{
    "date": {
        "ru": "19 ноября",
        "en": "November 19",
        "zh": "11月19日"
    },
    "time_utc": "2024-11-19 06:14",
    "weekday": 1,
    "issue_id": 654,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.10640",
            "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices",
            "url": "https://huggingface.co/papers/2411.10640",
            "abstract": "The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile phones, as essential daily companions, represent the most effective and accessible deployment platform for MLLMs, enabling seamless integration into everyday tasks. However, deploying MLLMs on mobile phones presents challenges due to limitations in memory size and computational capability, making it difficult to achieve smooth and real-time processing without extensive optimization. In this paper, we present BlueLM-V-3B, an algorithm and system co-design approach specifically tailored for the efficient deployment of MLLMs on mobile platforms. To be specific, we redesign the dynamic resolution scheme adopted by mainstream MLLMs and implement system optimization for hardware-aware deployment to optimize model inference on mobile phones. BlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B features a language model with 2.7B parameters and a vision encoder with 400M parameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4 token/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight quantization. (3) Strong Performance: BlueLM-V-3B has attained the highest average score of 66.1 on the OpenCompass benchmark among models with leq 4B parameters and surpassed a series of models with much larger parameter sizes (e.g., MiniCPM-V-2.6, InternVL2-8B).",
            "score": 10,
            "issue_id": 652,
            "pub_date": "2024-11-16",
            "pub_date_card": {
                "ru": "16 ноября",
                "en": "November 16",
                "zh": "11月16日"
            },
            "hash": "0366549d4347dfd2",
            "authors": [
                "Xudong Lu",
                "Yinghao Chen",
                "Cheng Chen",
                "Hui Tan",
                "Boheng Chen",
                "Yina Xie",
                "Rui Hu",
                "Guanxin Tan",
                "Renshou Wu",
                "Yan Hu",
                "Yi Zeng",
                "Lei Wu",
                "Liuyang Bian",
                "Zhaoxiong Wang",
                "Long Liu",
                "Yanzhou Yang",
                "Han Xiao",
                "Aojun Zhou",
                "Yafei Wen",
                "Xiaoxin Chen",
                "Shuai Ren",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10640.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#small_models",
                    "#multimodal",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "BlueLM-V-3B: Мощь больших языковых моделей в вашем кармане",
                    "desc": "BlueLM-V-3B - это новый подход к развертыванию мультимодальных больших языковых моделей (MLLM) на мобильных устройствах. Он решает проблемы ограниченной памяти и вычислительной мощности смартфонов, оптимизируя алгоритм и систему для эффективной работы. Модель имеет компактный размер (3,1 млрд параметров), высокую скорость генерации (24,4 токена/с) и превосходную производительность по сравнению с более крупными моделями. BlueLM-V-3B демонстрирует потенциал для интеграции продвинутых ИИ-технологий в повседневную жизнь через мобильные устройства."
                },
                "en": {
                    "title": "Efficient MLLMs for Mobile: BlueLM-V-3B Unleashed!",
                    "desc": "This paper introduces BlueLM-V-3B, a multimodal large language model (MLLM) designed for efficient deployment on mobile devices. It addresses the challenges of limited memory and computational power by optimizing model inference through a redesigned dynamic resolution scheme and hardware-aware system optimizations. BlueLM-V-3B features a compact architecture with 2.7 billion parameters for the language model and 400 million for the vision encoder, ensuring fast processing speeds. The model achieves impressive performance metrics, outperforming larger models on benchmarks while maintaining a generation speed of 24.4 tokens per second."
                },
                "zh": {
                    "title": "高效部署多模态大语言模型的创新方案",
                    "desc": "这篇论文介绍了一种名为BlueLM-V-3B的多模态大语言模型（MLLM），旨在高效地在移动平台上部署。该模型具有2.7亿参数的语言模型和4亿参数的视觉编码器，能够在移动设备上实现快速生成。通过重新设计动态分辨率方案和进行硬件优化，BlueLM-V-3B在MediaTek Dimensity 9300处理器上达到了每秒24.4个标记的生成速度。该模型在OpenCompass基准测试中获得了66.1的最高平均分，超越了许多参数更大的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11504",
            "title": "Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering",
            "url": "https://huggingface.co/papers/2411.11504",
            "abstract": "The evolution of machine learning has increasingly prioritized the development of powerful models and more scalable supervision signals. However, the emergence of foundation models presents significant challenges in providing effective supervision signals necessary for further enhancing their capabilities. Consequently, there is an urgent need to explore novel supervision signals and technical approaches. In this paper, we propose verifier engineering, a novel post-training paradigm specifically designed for the era of foundation models. The core of verifier engineering involves leveraging a suite of automated verifiers to perform verification tasks and deliver meaningful feedback to foundation models. We systematically categorize the verifier engineering process into three essential stages: search, verify, and feedback, and provide a comprehensive review of state-of-the-art research developments within each stage. We believe that verifier engineering constitutes a fundamental pathway toward achieving Artificial General Intelligence.",
            "score": 3,
            "issue_id": 654,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 ноября",
                "en": "November 18",
                "zh": "11月18日"
            },
            "hash": "a48ecb5e8a1da0ae",
            "authors": [
                "Xinyan Guan",
                "Yanjiang Liu",
                "Xinyu Lu",
                "Boxi Cao",
                "Ben He",
                "Xianpei Han",
                "Le Sun",
                "Jie Lou",
                "Bowen Yu",
                "Yaojie Lu",
                "Hongyu Lin"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11504.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#survey",
                    "#training",
                    "#agi"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Верификационная инженерия: новый путь к совершенствованию ИИ",
                    "desc": "В статье представлена концепция 'верификационной инженерии' - новой парадигмы для улучшения фундаментальных моделей машинного обучения. Авторы предлагают использовать автоматизированные верификаторы для проверки и обратной связи с моделями. Процесс разделен на три этапа: поиск, верификация и обратная связь. Исследователи считают, что этот подход может стать ключевым на пути к созданию искусственного общего интеллекта."
                },
                "en": {
                    "title": "Unlocking Foundation Models with Verifier Engineering",
                    "desc": "This paper discusses the challenges of providing effective supervision signals for foundation models in machine learning. It introduces a new approach called verifier engineering, which uses automated verifiers to enhance the capabilities of these models. The process is divided into three stages: search, verify, and feedback, each aimed at improving model performance. The authors argue that this method is crucial for advancing towards Artificial General Intelligence."
                },
                "zh": {
                    "title": "验证器工程：迈向人工通用智能的新路径",
                    "desc": "本论文探讨了在基础模型时代，如何提供有效的监督信号以提升模型能力。我们提出了一种新的后训练范式——验证器工程，旨在利用自动化验证器进行验证任务，并为基础模型提供有意义的反馈。验证器工程的过程分为三个关键阶段：搜索、验证和反馈，并对每个阶段的最新研究进展进行了系统性回顾。我们认为，验证器工程是实现人工通用智能的重要途径。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10669",
            "title": "Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts",
            "url": "https://huggingface.co/papers/2411.10669",
            "abstract": "As the research of Multimodal Large Language Models (MLLMs) becomes popular, an advancing MLLM model is typically required to handle various textual and visual tasks (e.g., VQA, Detection, OCR, and ChartQA) simultaneously for real-world applications. However, due to the significant differences in representation and distribution among data from various tasks, simply mixing data of all tasks together leads to the well-known``multi-task conflict\" issue, resulting in performance degradation across various tasks. To address this issue, we propose Awaker2.5-VL, a Mixture of Experts~(MoE) architecture suitable for MLLM, which acquires the multi-task capabilities through multiple sparsely activated experts. To speed up the training and inference of Awaker2.5-VL, each expert in our model is devised as a low-rank adaptation (LoRA) structure. Extensive experiments on multiple latest benchmarks demonstrate the effectiveness of Awaker2.5-VL. The code and model weight are released in our Project Page: https://github.com/MetabrainAGI/Awaker.",
            "score": 3,
            "issue_id": 654,
            "pub_date": "2024-11-16",
            "pub_date_card": {
                "ru": "16 ноября",
                "en": "November 16",
                "zh": "11月16日"
            },
            "hash": "f1319420ae85759e",
            "authors": [
                "Jinqiang Long",
                "Yanqi Dai",
                "Guoxing Yang",
                "Hongpeng Lin",
                "Nanyi Fei",
                "Yizhao Gao",
                "Zhiwu Lu"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Metabrain AGI Lab, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10669.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Awaker2.5-VL: Мультизадачная MLLM без конфликтов",
                    "desc": "Статья представляет Awaker2.5-VL - новую архитектуру мультимодальной большой языковой модели (MLLM), основанную на принципе смеси экспертов (MoE). Модель решает проблему конфликта между задачами при обучении на разнородных данных. Каждый эксперт в системе реализован с использованием низкоранговой адаптации (LoRA) для ускорения обучения и вывода. Эксперименты показывают эффективность Awaker2.5-VL на современных бенчмарках."
                },
                "en": {
                    "title": "Awaker2.5-VL: Mastering Multimodal Tasks with Expert Precision",
                    "desc": "This paper introduces Awaker2.5-VL, a new model designed to improve the performance of Multimodal Large Language Models (MLLMs) on various tasks like visual question answering and detection. The authors address the 'multi-task conflict' problem, which occurs when different tasks interfere with each other due to their diverse data representations. Awaker2.5-VL uses a Mixture of Experts (MoE) architecture, where only a subset of experts is activated for each task, allowing for better specialization and efficiency. Additionally, the model incorporates low-rank adaptation (LoRA) to enhance training speed and inference performance, showing promising results in extensive experiments."
                },
                "zh": {
                    "title": "多模态任务的专家混合解决方案",
                    "desc": "本研究提出了一种名为Awaker2.5-VL的多模态大语言模型（MLLM），旨在同时处理文本和视觉任务，如视觉问答（VQA）、检测、光学字符识别（OCR）和图表问答（ChartQA）。为了克服多任务冲突问题，Awaker2.5-VL采用了专家混合（MoE）架构，通过多个稀疏激活的专家来实现多任务能力。每个专家被设计为低秩适应（LoRA）结构，以加速模型的训练和推理。大量实验结果表明，Awaker2.5-VL在多个最新基准测试中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11767",
            "title": "Drowning in Documents: Consequences of Scaling Reranker Inference",
            "url": "https://huggingface.co/papers/2411.11767",
            "abstract": "Rerankers, typically cross-encoders, are often used to re-score the documents retrieved by cheaper initial IR systems. This is because, though expensive, rerankers are assumed to be more effective. We challenge this assumption by measuring reranker performance for full retrieval, not just re-scoring first-stage retrieval. Our experiments reveal a surprising trend: the best existing rerankers provide diminishing returns when scoring progressively more documents and actually degrade quality beyond a certain limit. In fact, in this setting, rerankers can frequently assign high scores to documents with no lexical or semantic overlap with the query. We hope that our findings will spur future research to improve reranking.",
            "score": 3,
            "issue_id": 652,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 ноября",
                "en": "November 18",
                "zh": "11月18日"
            },
            "hash": "3fa06087787bc8d4",
            "authors": [
                "Mathew Jacob",
                "Erik Lindgren",
                "Matei Zaharia",
                "Michael Carbin",
                "Omar Khattab",
                "Andrew Drozdov"
            ],
            "affiliations": [
                "Databricks",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11767.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Неожиданные ограничения ранжировщиков в информационном поиске",
                    "desc": "В статье исследуется эффективность ранжировщиков (rerankers) в информационном поиске. Авторы обнаружили, что при оценке большего количества документов качество ранжирования ухудшается. Более того, ранжировщики могут присваивать высокие оценки документам без лексического или семантического сходства с запросом. Исследование ставит под сомнение предположение о превосходстве ранжировщиков над более простыми системами поиска."
                },
                "en": {
                    "title": "Rethinking Rerankers: Diminishing Returns in Document Scoring",
                    "desc": "This paper investigates the effectiveness of rerankers, specifically cross-encoders, in the context of information retrieval (IR). Traditionally, rerankers are believed to enhance the quality of document scoring after an initial retrieval phase. However, the authors find that as more documents are scored, the performance of these rerankers diminishes and can even lead to poorer results. The study highlights that rerankers may assign high scores to irrelevant documents, suggesting a need for improved methods in reranking processes."
                },
                "zh": {
                    "title": "重排序器的有效性需重新审视",
                    "desc": "本文探讨了重排序器（通常是交叉编码器）在信息检索中的有效性。我们通过测量重排序器在完整检索中的表现，挑战了它们在初步检索后重新评分的假设。实验结果显示，现有的重排序器在评分越来越多的文档时，效果逐渐减弱，甚至在某个限度后质量下降。我们的发现希望能激励未来的研究，以改进重排序技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07641",
            "title": "Top-$nσ$: Not All Logits Are You Need",
            "url": "https://huggingface.co/papers/2411.07641",
            "abstract": "Large language models (LLMs) typically employ greedy decoding or low-temperature sampling for reasoning tasks, reflecting a perceived trade-off between diversity and accuracy. We challenge this convention by introducing top-nsigma, a novel sampling method that operates directly on pre-softmax logits by leveraging a statistical threshold. Our key insight is that logits naturally separate into a Gaussian-distributed noisy region and a distinct informative region, enabling efficient token filtering without complex probability manipulations. Unlike existing methods (e.g., top-p, min-p) that inadvertently include more noise tokens at higher temperatures, top-nsigma maintains a stable sampling space regardless of temperature scaling. We also provide a theoretical analysis of top-nsigma to better understand its behavior. The extensive experimental results across four reasoning-focused datasets demonstrate that our method not only outperforms existing sampling approaches but also surpasses greedy decoding, while maintaining consistent performance even at high temperatures.",
            "score": 3,
            "issue_id": 651,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "d3439bf0c336ac57",
            "authors": [
                "Chenxia Tang",
                "Jianchun Liu",
                "Hongli Xu",
                "Liusheng Huang"
            ],
            "affiliations": [
                "School of Computer Science and Technology, University of Science and Technology of China",
                "Suzhou Institute for Advanced Research, University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.07641.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Top-nsigma: эффективное сэмплирование для улучшения рассуждений языковых моделей",
                    "desc": "В статье представлен новый метод сэмплирования для больших языковых моделей под названием top-nsigma. Этот метод работает напрямую с логитами перед софтмаксом, используя статистический порог для разделения шумовых и информативных токенов. Top-nsigma позволяет эффективно фильтровать токены без сложных манипуляций с вероятностями и сохраняет стабильное пространство сэмплирования независимо от температуры. Экспериментальные результаты показывают, что данный метод превосходит существующие подходы к сэмплированию и жадное декодирование на задачах рассуждения."
                },
                "en": {
                    "title": "Top-NSigma: Enhancing Sampling for Better Reasoning in LLMs",
                    "desc": "This paper introduces a new sampling method called top-nsigma for large language models (LLMs) that improves reasoning tasks. Unlike traditional methods that use greedy decoding or low-temperature sampling, top-nsigma filters tokens based on their pre-softmax logits, which are divided into informative and noisy regions. This approach allows for better token selection without the complications of probability adjustments, ensuring a stable sampling process across different temperature settings. Experimental results show that top-nsigma outperforms existing methods and maintains high performance even when using higher temperatures."
                },
                "zh": {
                    "title": "突破传统，提升推理性能的top-nsigma方法",
                    "desc": "本文提出了一种新的采样方法top-nsigma，旨在改善大语言模型在推理任务中的表现。该方法直接在预软最大值的logits上操作，通过统计阈值来进行有效的令牌过滤。我们的研究表明，logits可以自然地分为高斯分布的噪声区域和信息丰富的区域，从而避免了复杂的概率操作。实验结果显示，top-nsigma在多个推理数据集上超越了现有的采样方法和贪婪解码，且在高温度下仍能保持稳定的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10510",
            "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2411.10510",
            "abstract": "Diffusion Transformers (DiT) have emerged as powerful generative models for various tasks, including image, video, and speech synthesis. However, their inference process remains computationally expensive due to the repeated evaluation of resource-intensive attention and feed-forward modules. To address this, we introduce SmoothCache, a model-agnostic inference acceleration technique for DiT architectures. SmoothCache leverages the observed high similarity between layer outputs across adjacent diffusion timesteps. By analyzing layer-wise representation errors from a small calibration set, SmoothCache adaptively caches and reuses key features during inference. Our experiments demonstrate that SmoothCache achieves 8% to 71% speed up while maintaining or even improving generation quality across diverse modalities. We showcase its effectiveness on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio, highlighting its potential to enable real-time applications and broaden the accessibility of powerful DiT models.",
            "score": 1,
            "issue_id": 654,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 ноября",
                "en": "November 15",
                "zh": "11月15日"
            },
            "hash": "991f548fec1ec8c9",
            "authors": [
                "Joseph Liu",
                "Joshua Geddes",
                "Ziyu Guo",
                "Haomiao Jiang",
                "Mahesh Kumar Nandwana"
            ],
            "affiliations": [
                "Queens University",
                "Roblox"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10510.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#video",
                    "#optimization",
                    "#audio",
                    "#diffusion"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "SmoothCache: Быстрее и лучше с умным кэшированием для DiT",
                    "desc": "Статья представляет SmoothCache - технику ускорения вывода для архитектур Diffusion Transformers (DiT). SmoothCache использует высокое сходство между выходными данными слоев на соседних временных шагах диффузии. Метод адаптивно кэширует и повторно использует ключевые признаки во время вывода, анализируя ошибки представления слоев на небольшом калибровочном наборе. Эксперименты показывают, что SmoothCache достигает ускорения от 8% до 71% при сохранении или даже улучшении качества генерации для различных модальностей."
                },
                "en": {
                    "title": "Accelerating Diffusion Transformers with SmoothCache",
                    "desc": "This paper presents SmoothCache, a technique designed to speed up the inference process of Diffusion Transformers (DiT), which are used for generating images, videos, and audio. The traditional inference method is slow because it requires many evaluations of complex attention and feed-forward layers. SmoothCache improves efficiency by caching and reusing similar outputs from adjacent diffusion timesteps, reducing the need for repeated calculations. Experiments show that this method can accelerate inference by 8% to 71% while maintaining or enhancing the quality of the generated content."
                },
                "zh": {
                    "title": "SmoothCache：加速扩散变换器的推理过程",
                    "desc": "扩散变换器（DiT）是一种强大的生成模型，广泛应用于图像、视频和语音合成等任务。然而，它们的推理过程计算开销较大，因为需要重复评估资源密集型的注意力和前馈模块。为了解决这个问题，我们提出了SmoothCache，这是一种与模型无关的推理加速技术，利用相邻扩散时间步之间层输出的高度相似性。通过分析小型校准集中的层级表示误差，SmoothCache自适应地缓存和重用关键特征，从而在保持或提高生成质量的同时，实现了8%到71%的速度提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10499",
            "title": "FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on",
            "url": "https://huggingface.co/papers/2411.10499",
            "abstract": "Although image-based virtual try-on has made considerable progress, emerging approaches still encounter challenges in producing high-fidelity and robust fitting images across diverse scenarios. These methods often struggle with issues such as texture-aware maintenance and size-aware fitting, which hinder their overall effectiveness. To address these limitations, we propose a novel garment perception enhancement technique, termed FitDiT, designed for high-fidelity virtual try-on using Diffusion Transformers (DiT) allocating more parameters and attention to high-resolution features. First, to further improve texture-aware maintenance, we introduce a garment texture extractor that incorporates garment priors evolution to fine-tune garment feature, facilitating to better capture rich details such as stripes, patterns, and text. Additionally, we introduce frequency-domain learning by customizing a frequency distance loss to enhance high-frequency garment details. To tackle the size-aware fitting issue, we employ a dilated-relaxed mask strategy that adapts to the correct length of garments, preventing the generation of garments that fill the entire mask area during cross-category try-on. Equipped with the above design, FitDiT surpasses all baselines in both qualitative and quantitative evaluations. It excels in producing well-fitting garments with photorealistic and intricate details, while also achieving competitive inference times of 4.57 seconds for a single 1024x768 image after DiT structure slimming, outperforming existing methods.",
            "score": 1,
            "issue_id": 654,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 ноября",
                "en": "November 15",
                "zh": "11月15日"
            },
            "hash": "b142b4be26ef6147",
            "authors": [
                "Boyuan Jiang",
                "Xiaobin Hu",
                "Donghao Luo",
                "Qingdong He",
                "Chengming Xu",
                "Jinlong Peng",
                "Jiangning Zhang",
                "Chengjie Wang",
                "Yunsheng Wu",
                "Yanwei Fu"
            ],
            "affiliations": [
                "Fudan University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10499.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "👚",
                "ru": {
                    "title": "FitDiT: Высококачественная виртуальная примерка с сохранением деталей",
                    "desc": "Статья представляет новый метод виртуальной примерки одежды под названием FitDiT, основанный на диффузионных трансформерах. FitDiT улучшает сохранение текстур одежды с помощью экстрактора текстур и обучения в частотной области. Для решения проблемы подгонки размера используется стратегия расширенной маски. Метод превосходит существующие подходы по качеству и реалистичности генерируемых изображений."
                },
                "en": {
                    "title": "FitDiT: Revolutionizing Virtual Try-On with High-Fidelity Garment Perception",
                    "desc": "This paper presents FitDiT, a new technique for improving virtual try-on systems using Diffusion Transformers. It addresses challenges in producing realistic and well-fitting images by enhancing garment perception through a specialized texture extractor and frequency-domain learning. The method also incorporates a dilated-relaxed mask strategy to ensure garments fit correctly without oversizing. FitDiT demonstrates superior performance in generating high-fidelity images with intricate details while maintaining efficient processing times."
                },
                "zh": {
                    "title": "FitDiT：高保真虚拟试穿的新突破",
                    "desc": "本文提出了一种新的服装感知增强技术，称为FitDiT，旨在提高虚拟试穿的高保真度。该方法利用扩散变换器（DiT）分配更多参数和注意力于高分辨率特征，以解决纹理感知维护和尺寸感知适配的问题。我们引入了服装纹理提取器和频域学习，增强了服装细节的捕捉能力，并采用扩张放松掩码策略来适应服装的正确长度。FitDiT在定性和定量评估中均超越了所有基线，能够生成具有真实感和复杂细节的合身服装。"
                }
            }
        }
    ],
    "link_prev": "2024-11-18.html",
    "link_next": "2024-11-20.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "18.11",
        "en": "11/18",
        "zh": "11月18日"
    },
    "short_date_next": {
        "ru": "20.11",
        "en": "11/20",
        "zh": "11月20日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一个新的视觉语言模型（VLM），称为LLaVA-o1，旨在进行自主多阶段推理。与链式思维提示不同，LLaVA-o1独立进行总结、视觉解释、逻辑推理和结论生成。这种结构化方法使其在推理密集型任务上取得显著精度提升。研究团队编制了LLaVA-o1-100k数据集，并提出了一种推理时阶段级束搜索方法，以实现有效的推理时扩展。结果显示，LLaVA-o1在多模态推理基准测试中表现出色，超越了多个大型和封闭源模型。",
        "title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
        "pinyin": "这篇文章介绍了一个新的视觉语言模型（VLM），称为LLaVA-o1，旨在进行自主多阶段推理。与链式思维提示不同，LLaVA-o1独立进行总结、视觉解释、逻辑推理和结论生成。这种结构化方法使其在推理密集型任务上取得显著精度提升。研究团队编制了LLaVA-o1-100k数据集，并提出了一种推理时阶段级束搜索方法，以实现有效的推理时扩展。结果显示，LLaVA-o1在多模态推理基准测试中表现出色，超越了多个大型和封闭源模型。\n\nzhè piān wén zhāng jiè shào le yī gè xīn de shì jué yǔ yán mó xìng (VLM)，chēng wéi LLaVA-o1，zhǐ yǐn jìn xíng zì zhǔ duō jiē duàn tuí lǐ. yǔ liàn shì sī wéi tí shì bù tóng，LLaVA-o1 dú lì jìn xíng zǒng jiě，shì jué jiě shì，luó ji tuí lǐ hé jié lùn shēng chéng. zhè zhǒng jiē gòu huà fǎ shǐ qí zài tuí lǐ mì jī xíng rèn wù shàng qǔ dé xiǎn zhù jīng dù tí shēng. yán jiū tuán duì biān zhì le LLaVA-o1-100k shù jù jí，bìng tí chū le yī zhǒng tuí lǐ shí jiē duàn jí shù sōu suǒ fǎ，yǐ shí xiàn yán jiū shí kuò zhǎn. jié guǒ xiǎn shì，LLaVA-o1 zài duō mó shuài tuí lǐ jī zhǔn cè shì zhōng biǎo xiàn chū sè，chāo yuè le duō gè dà xíng hé fēng bì yuán mó xìng.",
        "vocab": "[\n    {\"word\": \"视觉语言模型\", \"pinyin\": \"shìjué yǔyán móxíng\", \"trans\": \"visual language model\"},\n    {\"word\": \"自主\", \"pinyin\": \"zìzhǔ\", \"trans\": \"autonomous\"},\n    {\"word\": \"多阶段\", \"pinyin\": \"duō jiēduàn\", \"trans\": \"multi-stage\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"链式\", \"pinyin\": \"liànshì\", \"trans\": \"chain-like\"},\n    {\"word\": \"提示\", \"pinyin\": \"tíshì\", \"trans\": \"prompt\"},\n    {\"word\": \"独立\", \"pinyin\": \"dúlì\", \"trans\": \"independent\"},\n    {\"word\": \"总结\", \"pinyin\": \"zǒngjié\", \"trans\": \"summary\"},\n    {\"word\": \"视觉解释\", \"pinyin\": \"shìjué jiěshì\", \"trans\": \"visual explanation\"},\n    {\"word\": \"逻辑推理\", \"pinyin\": \"luóji tuīlǐ\", \"trans\": \"logical reasoning\"},\n    {\"word\": \"结论生成\", \"pinyin\": \"jiélùn shēngchéng\", \"trans\": \"conclusion generation\"},\n    {\"word\": \"结构化\", \"pinyin\": \"jiégòuhuà\", \"trans\": \"structured\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāngfǎ\", \"trans\": \"method\"},\n    {\"word\": \"精度\", \"pinyin\": \"jīngdù\", \"trans\": \"accuracy\"},\n    {\"word\": \"提升\", \"pinyin\": \"tíshēng\", \"trans\": \"improvement\"},\n    {\"word\": \"研究团队\", \"pinyin\": \"yánjiū tuánduì\", \"trans\": \"research team\"},\n    {\"word\": \"编制\", \"pinyin\": \"biānzhì\", \"trans\": \"compile\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"},\n    {\"word\": \"推理时\", \"pinyin\": \"tuīlǐ shí\", \"trans\": \"during reasoning\"},\n    {\"word\": \"阶段级\", \"pinyin\": \"jiēduàn jí\", \"trans\": \"stage-level\"},\n    {\"word\": \"束搜索\", \"pinyin\": \"shù sōusuǒ\", \"trans\": \"beam search\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuòzhǎn\", \"trans\": \"expansion\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō móshì\", \"trans\": \"multimodal\"},\n    {\"word\": \"基准测试\", \"pinyin\": \"jīzhǔn cèshì\", \"trans\": \"benchmark test\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chūsè\", \"trans\": \"outstanding\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāoyuè\", \"trans\": \"surpass\"},\n    {\"word\": \"大型\", \"pinyin\": \"dàxíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"封闭源\", \"pinyin\": \"fēngbì yuán\", \"trans\": \"closed-source\"}\n]",
        "trans": "This article introduces a new visual language model (VLM) called LLaVA-o1, designed for autonomous multi-stage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently performs summarization, visual explanation, logical reasoning, and conclusion generation. This structured approach achieves significant accuracy improvements in reasoning-intensive tasks. The research team compiled the LLaVA-o1-100k dataset and proposed a stage-level beam search method during inference to achieve effective reasoning-time expansion. The results show that LLaVA-o1 performs exceptionally well in multimodal reasoning benchmarks, outperforming several large and closed-source models.",
        "update_ts": "2024-11-18 09:12"
    }
}