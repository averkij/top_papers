{
    "date": {
        "ru": "19 ноября",
        "en": "November 19",
        "zh": "11月19日"
    },
    "time_utc": "2024-11-19 04:13",
    "weekday": 1,
    "issue_id": 652,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.10640",
            "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices",
            "url": "https://huggingface.co/papers/2411.10640",
            "abstract": "The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile phones, as essential daily companions, represent the most effective and accessible deployment platform for MLLMs, enabling seamless integration into everyday tasks. However, deploying MLLMs on mobile phones presents challenges due to limitations in memory size and computational capability, making it difficult to achieve smooth and real-time processing without extensive optimization. In this paper, we present BlueLM-V-3B, an algorithm and system co-design approach specifically tailored for the efficient deployment of MLLMs on mobile platforms. To be specific, we redesign the dynamic resolution scheme adopted by mainstream MLLMs and implement system optimization for hardware-aware deployment to optimize model inference on mobile phones. BlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B features a language model with 2.7B parameters and a vision encoder with 400M parameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4 token/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight quantization. (3) Strong Performance: BlueLM-V-3B has attained the highest average score of 66.1 on the OpenCompass benchmark among models with leq 4B parameters and surpassed a series of models with much larger parameter sizes (e.g., MiniCPM-V-2.6, InternVL2-8B).",
            "score": 3,
            "issue_id": 652,
            "pub_date": "2024-11-16",
            "pub_date_card": {
                "ru": "16 ноября",
                "en": "November 16",
                "zh": "11月16日"
            },
            "hash": "0366549d4347dfd2",
            "authors": [
                "Xudong Lu",
                "Yinghao Chen",
                "Cheng Chen",
                "Hui Tan",
                "Boheng Chen",
                "Yina Xie",
                "Rui Hu",
                "Guanxin Tan",
                "Renshou Wu",
                "Yan Hu",
                "Yi Zeng",
                "Lei Wu",
                "Liuyang Bian",
                "Zhaoxiong Wang",
                "Long Liu",
                "Yanzhou Yang",
                "Han Xiao",
                "Aojun Zhou",
                "Yafei Wen",
                "Xiaoxin Chen",
                "Shuai Ren",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10640.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#small_models",
                    "#multimodal",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "BlueLM-V-3B: Мощь больших языковых моделей в вашем кармане",
                    "desc": "BlueLM-V-3B - это новый подход к развертыванию мультимодальных больших языковых моделей (MLLM) на мобильных устройствах. Он решает проблемы ограниченной памяти и вычислительной мощности смартфонов, оптимизируя алгоритм и систему для эффективной работы. Модель имеет компактный размер (3,1 млрд параметров), высокую скорость генерации (24,4 токена/с) и превосходную производительность по сравнению с более крупными моделями. BlueLM-V-3B демонстрирует потенциал для интеграции продвинутых ИИ-технологий в повседневную жизнь через мобильные устройства."
                },
                "en": {
                    "title": "Efficient MLLMs for Mobile: BlueLM-V-3B Unleashed!",
                    "desc": "This paper introduces BlueLM-V-3B, a multimodal large language model (MLLM) designed for efficient deployment on mobile devices. It addresses the challenges of limited memory and computational power by optimizing model inference through a redesigned dynamic resolution scheme and hardware-aware system optimizations. BlueLM-V-3B features a compact architecture with 2.7 billion parameters for the language model and 400 million for the vision encoder, ensuring fast processing speeds. The model achieves impressive performance metrics, outperforming larger models on benchmarks while maintaining a generation speed of 24.4 tokens per second."
                },
                "zh": {
                    "title": "高效部署多模态大语言模型的创新方案",
                    "desc": "这篇论文介绍了一种名为BlueLM-V-3B的多模态大语言模型（MLLM），旨在高效地在移动平台上部署。该模型具有2.7亿参数的语言模型和4亿参数的视觉编码器，能够在移动设备上实现快速生成。通过重新设计动态分辨率方案和进行硬件优化，BlueLM-V-3B在MediaTek Dimensity 9300处理器上达到了每秒24.4个标记的生成速度。该模型在OpenCompass基准测试中获得了66.1的最高平均分，超越了许多参数更大的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11767",
            "title": "Drowning in Documents: Consequences of Scaling Reranker Inference",
            "url": "https://huggingface.co/papers/2411.11767",
            "abstract": "Rerankers, typically cross-encoders, are often used to re-score the documents retrieved by cheaper initial IR systems. This is because, though expensive, rerankers are assumed to be more effective. We challenge this assumption by measuring reranker performance for full retrieval, not just re-scoring first-stage retrieval. Our experiments reveal a surprising trend: the best existing rerankers provide diminishing returns when scoring progressively more documents and actually degrade quality beyond a certain limit. In fact, in this setting, rerankers can frequently assign high scores to documents with no lexical or semantic overlap with the query. We hope that our findings will spur future research to improve reranking.",
            "score": 0,
            "issue_id": 652,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 ноября",
                "en": "November 18",
                "zh": "11月18日"
            },
            "hash": "3fa06087787bc8d4",
            "authors": [
                "Mathew Jacob",
                "Erik Lindgren",
                "Matei Zaharia",
                "Michael Carbin",
                "Omar Khattab",
                "Andrew Drozdov"
            ],
            "affiliations": [
                "Databricks",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11767.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Неожиданные ограничения ранжировщиков в информационном поиске",
                    "desc": "В статье исследуется эффективность ранжировщиков (rerankers) в информационном поиске. Авторы обнаружили, что при оценке большего количества документов качество ранжирования ухудшается. Более того, ранжировщики могут присваивать высокие оценки документам без лексического или семантического сходства с запросом. Исследование ставит под сомнение предположение о превосходстве ранжировщиков над более простыми системами поиска."
                },
                "en": {
                    "title": "Rethinking Rerankers: Diminishing Returns in Document Scoring",
                    "desc": "This paper investigates the effectiveness of rerankers, specifically cross-encoders, in the context of information retrieval (IR). Traditionally, rerankers are believed to enhance the quality of document scoring after an initial retrieval phase. However, the authors find that as more documents are scored, the performance of these rerankers diminishes and can even lead to poorer results. The study highlights that rerankers may assign high scores to irrelevant documents, suggesting a need for improved methods in reranking processes."
                },
                "zh": {
                    "title": "重排序器的有效性需重新审视",
                    "desc": "本文探讨了重排序器（通常是交叉编码器）在信息检索中的有效性。我们通过测量重排序器在完整检索中的表现，挑战了它们在初步检索后重新评分的假设。实验结果显示，现有的重排序器在评分越来越多的文档时，效果逐渐减弱，甚至在某个限度后质量下降。我们的发现希望能激励未来的研究，以改进重排序技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07641",
            "title": "Top-$nσ$: Not All Logits Are You Need",
            "url": "https://huggingface.co/papers/2411.07641",
            "abstract": "Large language models (LLMs) typically employ greedy decoding or low-temperature sampling for reasoning tasks, reflecting a perceived trade-off between diversity and accuracy. We challenge this convention by introducing top-nsigma, a novel sampling method that operates directly on pre-softmax logits by leveraging a statistical threshold. Our key insight is that logits naturally separate into a Gaussian-distributed noisy region and a distinct informative region, enabling efficient token filtering without complex probability manipulations. Unlike existing methods (e.g., top-p, min-p) that inadvertently include more noise tokens at higher temperatures, top-nsigma maintains a stable sampling space regardless of temperature scaling. We also provide a theoretical analysis of top-nsigma to better understand its behavior. The extensive experimental results across four reasoning-focused datasets demonstrate that our method not only outperforms existing sampling approaches but also surpasses greedy decoding, while maintaining consistent performance even at high temperatures.",
            "score": 0,
            "issue_id": 651,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "d3439bf0c336ac57",
            "authors": [
                "Chenxia Tang",
                "Jianchun Liu",
                "Hongli Xu",
                "Liusheng Huang"
            ],
            "affiliations": [
                "School of Computer Science and Technology, University of Science and Technology of China",
                "Suzhou Institute for Advanced Research, University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.07641.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Top-nsigma: эффективное сэмплирование для улучшения рассуждений языковых моделей",
                    "desc": "В статье представлен новый метод сэмплирования для больших языковых моделей под названием top-nsigma. Этот метод работает напрямую с логитами перед софтмаксом, используя статистический порог для разделения шумовых и информативных токенов. Top-nsigma позволяет эффективно фильтровать токены без сложных манипуляций с вероятностями и сохраняет стабильное пространство сэмплирования независимо от температуры. Экспериментальные результаты показывают, что данный метод превосходит существующие подходы к сэмплированию и жадное декодирование на задачах рассуждения."
                },
                "en": {
                    "title": "Top-NSigma: Enhancing Sampling for Better Reasoning in LLMs",
                    "desc": "This paper introduces a new sampling method called top-nsigma for large language models (LLMs) that improves reasoning tasks. Unlike traditional methods that use greedy decoding or low-temperature sampling, top-nsigma filters tokens based on their pre-softmax logits, which are divided into informative and noisy regions. This approach allows for better token selection without the complications of probability adjustments, ensuring a stable sampling process across different temperature settings. Experimental results show that top-nsigma outperforms existing methods and maintains high performance even when using higher temperatures."
                },
                "zh": {
                    "title": "突破传统，提升推理性能的top-nsigma方法",
                    "desc": "本文提出了一种新的采样方法top-nsigma，旨在改善大语言模型在推理任务中的表现。该方法直接在预软最大值的logits上操作，通过统计阈值来进行有效的令牌过滤。我们的研究表明，logits可以自然地分为高斯分布的噪声区域和信息丰富的区域，从而避免了复杂的概率操作。实验结果显示，top-nsigma在多个推理数据集上超越了现有的采样方法和贪婪解码，且在高温度下仍能保持稳定的性能。"
                }
            }
        }
    ],
    "link_prev": "2024-11-18.html",
    "link_next": "2024-11-20.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "18.11",
        "en": "11/18",
        "zh": "11月18日"
    },
    "short_date_next": {
        "ru": "20.11",
        "en": "11/20",
        "zh": "11月20日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一个新的视觉语言模型（VLM），称为LLaVA-o1，旨在进行自主多阶段推理。与链式思维提示不同，LLaVA-o1独立进行总结、视觉解释、逻辑推理和结论生成。这种结构化方法使其在推理密集型任务上取得显著精度提升。研究团队编制了LLaVA-o1-100k数据集，并提出了一种推理时阶段级束搜索方法，以实现有效的推理时扩展。结果显示，LLaVA-o1在多模态推理基准测试中表现出色，超越了多个大型和封闭源模型。",
        "title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
        "pinyin": "这篇文章介绍了一个新的视觉语言模型（VLM），称为LLaVA-o1，旨在进行自主多阶段推理。与链式思维提示不同，LLaVA-o1独立进行总结、视觉解释、逻辑推理和结论生成。这种结构化方法使其在推理密集型任务上取得显著精度提升。研究团队编制了LLaVA-o1-100k数据集，并提出了一种推理时阶段级束搜索方法，以实现有效的推理时扩展。结果显示，LLaVA-o1在多模态推理基准测试中表现出色，超越了多个大型和封闭源模型。\n\nzhè piān wén zhāng jiè shào le yī gè xīn de shì jué yǔ yán mó xìng (VLM)，chēng wéi LLaVA-o1，zhǐ yǐn jìn xíng zì zhǔ duō jiē duàn tuí lǐ. yǔ liàn shì sī wéi tí shì bù tóng，LLaVA-o1 dú lì jìn xíng zǒng jiě，shì jué jiě shì，luó ji tuí lǐ hé jié lùn shēng chéng. zhè zhǒng jiē gòu huà fǎ shǐ qí zài tuí lǐ mì jī xíng rèn wù shàng qǔ dé xiǎn zhù jīng dù tí shēng. yán jiū tuán duì biān zhì le LLaVA-o1-100k shù jù jí，bìng tí chū le yī zhǒng tuí lǐ shí jiē duàn jí shù sōu suǒ fǎ，yǐ shí xiàn yán jiū shí kuò zhǎn. jié guǒ xiǎn shì，LLaVA-o1 zài duō mó shuài tuí lǐ jī zhǔn cè shì zhōng biǎo xiàn chū sè，chāo yuè le duō gè dà xíng hé fēng bì yuán mó xìng.",
        "vocab": "[\n    {\"word\": \"视觉语言模型\", \"pinyin\": \"shìjué yǔyán móxíng\", \"trans\": \"visual language model\"},\n    {\"word\": \"自主\", \"pinyin\": \"zìzhǔ\", \"trans\": \"autonomous\"},\n    {\"word\": \"多阶段\", \"pinyin\": \"duō jiēduàn\", \"trans\": \"multi-stage\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"链式\", \"pinyin\": \"liànshì\", \"trans\": \"chain-like\"},\n    {\"word\": \"提示\", \"pinyin\": \"tíshì\", \"trans\": \"prompt\"},\n    {\"word\": \"独立\", \"pinyin\": \"dúlì\", \"trans\": \"independent\"},\n    {\"word\": \"总结\", \"pinyin\": \"zǒngjié\", \"trans\": \"summary\"},\n    {\"word\": \"视觉解释\", \"pinyin\": \"shìjué jiěshì\", \"trans\": \"visual explanation\"},\n    {\"word\": \"逻辑推理\", \"pinyin\": \"luóji tuīlǐ\", \"trans\": \"logical reasoning\"},\n    {\"word\": \"结论生成\", \"pinyin\": \"jiélùn shēngchéng\", \"trans\": \"conclusion generation\"},\n    {\"word\": \"结构化\", \"pinyin\": \"jiégòuhuà\", \"trans\": \"structured\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāngfǎ\", \"trans\": \"method\"},\n    {\"word\": \"精度\", \"pinyin\": \"jīngdù\", \"trans\": \"accuracy\"},\n    {\"word\": \"提升\", \"pinyin\": \"tíshēng\", \"trans\": \"improvement\"},\n    {\"word\": \"研究团队\", \"pinyin\": \"yánjiū tuánduì\", \"trans\": \"research team\"},\n    {\"word\": \"编制\", \"pinyin\": \"biānzhì\", \"trans\": \"compile\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"},\n    {\"word\": \"推理时\", \"pinyin\": \"tuīlǐ shí\", \"trans\": \"during reasoning\"},\n    {\"word\": \"阶段级\", \"pinyin\": \"jiēduàn jí\", \"trans\": \"stage-level\"},\n    {\"word\": \"束搜索\", \"pinyin\": \"shù sōusuǒ\", \"trans\": \"beam search\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuòzhǎn\", \"trans\": \"expansion\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō móshì\", \"trans\": \"multimodal\"},\n    {\"word\": \"基准测试\", \"pinyin\": \"jīzhǔn cèshì\", \"trans\": \"benchmark test\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chūsè\", \"trans\": \"outstanding\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāoyuè\", \"trans\": \"surpass\"},\n    {\"word\": \"大型\", \"pinyin\": \"dàxíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"封闭源\", \"pinyin\": \"fēngbì yuán\", \"trans\": \"closed-source\"}\n]",
        "trans": "This article introduces a new visual language model (VLM) called LLaVA-o1, designed for autonomous multi-stage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently performs summarization, visual explanation, logical reasoning, and conclusion generation. This structured approach achieves significant accuracy improvements in reasoning-intensive tasks. The research team compiled the LLaVA-o1-100k dataset and proposed a stage-level beam search method during inference to achieve effective reasoning-time expansion. The results show that LLaVA-o1 performs exceptionally well in multimodal reasoning benchmarks, outperforming several large and closed-source models.",
        "update_ts": "2024-11-18 09:12"
    }
}