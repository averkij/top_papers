{
    "date": {
        "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 19",
        "zh": "11æœˆ19æ—¥"
    },
    "time_utc": "2024-11-19 21:09",
    "weekday": 1,
    "issue_id": 669,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.11844",
            "title": "Generative World Explorer",
            "url": "https://huggingface.co/papers/2411.11844",
            "abstract": "Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state.In contrast, humans can imagine unseen parts of the world through a mental exploration and revise their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions, without necessitating the physical exploration of the world at all times. To achieve this human-like ability, we introduce the Generative World Explorer (Genex), an egocentric world exploration framework that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train Genex, we create a synthetic urban scene dataset, Genex-DB. Our experimental results demonstrate that (1) Genex can generate high-quality and consistent observations during long-horizon exploration of a large virtual physical world and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans.",
            "score": 33,
            "issue_id": 655,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "ad359ab18e626959",
            "authors": [
                "Taiming Lu",
                "Tianmin Shu",
                "Alan Yuille",
                "Daniel Khashabi",
                "Jieneng Chen"
            ],
            "affiliations": [
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11844.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#3d",
                    "#synthetic",
                    "#games",
                    "#dataset",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœÑ‹ÑĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ² embodied AI. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¼Ñ‹ÑĞ»ĞµĞ½Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D Ğ¼Ğ¸Ñ€ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ Ğ½Ñ‘Ğ¼ Ğ±ĞµĞ· Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Generative World Explorer (Genex), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Genex Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Imagine to Explore: Enhancing Decision-Making with Mental Simulations",
                    "desc": "This paper addresses the challenge of planning in environments where an agent has incomplete information. Unlike traditional methods that rely on physical exploration, the authors propose a framework called Generative World Explorer (Genex) that enables agents to mentally simulate and explore their surroundings. By generating imagined observations, Genex allows agents to update their beliefs about the world without needing to physically navigate it. The results show that this approach leads to improved decision-making in complex environments, demonstrating the potential of mental exploration in embodied AI."
                },
                "zh": {
                    "title": "å¿ƒç†æ¢ç´¢ï¼Œæ™ºèƒ½å†³ç­–çš„æ–°æ–¹å¼",
                    "desc": "åœ¨å…·èº«äººå·¥æ™ºèƒ½ä¸­ï¼Œéƒ¨åˆ†è§‚å¯Ÿçš„è§„åˆ’æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚å¤§å¤šæ•°ç ”ç©¶é€šè¿‡è®©æ™ºèƒ½ä½“ç‰©ç†æ¢ç´¢ç¯å¢ƒæ¥æ›´æ–°å¯¹ä¸–ç•ŒçŠ¶æ€çš„ä¿¡å¿µï¼Œè€Œæˆ‘ä»¬æå‡ºçš„ç”Ÿæˆä¸–ç•Œæ¢ç´¢å™¨ï¼ˆGenexï¼‰åˆ™å…è®¸æ™ºèƒ½ä½“é€šè¿‡å¿ƒç†æ¢ç´¢æ¥æƒ³è±¡æœªè§çš„ä¸–ç•Œéƒ¨åˆ†ã€‚Genexèƒ½å¤Ÿåœ¨å¤§å‹3Dä¸–ç•Œä¸­ç”Ÿæˆæƒ³è±¡çš„è§‚å¯Ÿï¼Œä»è€Œæ›´æ–°ä¿¡å¿µï¼Œå¸®åŠ©æ™ºèƒ½ä½“åœ¨å½“å‰æ­¥éª¤åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåˆæˆåŸå¸‚åœºæ™¯æ•°æ®é›†Genex-DBï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†Genexåœ¨é•¿æ—¶é—´æ¢ç´¢ä¸­çš„é«˜è´¨é‡è§‚å¯Ÿç”Ÿæˆèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10640",
            "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices",
            "url": "https://huggingface.co/papers/2411.10640",
            "abstract": "The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile phones, as essential daily companions, represent the most effective and accessible deployment platform for MLLMs, enabling seamless integration into everyday tasks. However, deploying MLLMs on mobile phones presents challenges due to limitations in memory size and computational capability, making it difficult to achieve smooth and real-time processing without extensive optimization. In this paper, we present BlueLM-V-3B, an algorithm and system co-design approach specifically tailored for the efficient deployment of MLLMs on mobile platforms. To be specific, we redesign the dynamic resolution scheme adopted by mainstream MLLMs and implement system optimization for hardware-aware deployment to optimize model inference on mobile phones. BlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B features a language model with 2.7B parameters and a vision encoder with 400M parameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4 token/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight quantization. (3) Strong Performance: BlueLM-V-3B has attained the highest average score of 66.1 on the OpenCompass benchmark among models with leq 4B parameters and surpassed a series of models with much larger parameter sizes (e.g., MiniCPM-V-2.6, InternVL2-8B).",
            "score": 28,
            "issue_id": 652,
            "pub_date": "2024-11-16",
            "pub_date_card": {
                "ru": "16 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 16",
                "zh": "11æœˆ16æ—¥"
            },
            "hash": "0366549d4347dfd2",
            "authors": [
                "Xudong Lu",
                "Yinghao Chen",
                "Cheng Chen",
                "Hui Tan",
                "Boheng Chen",
                "Yina Xie",
                "Rui Hu",
                "Guanxin Tan",
                "Renshou Wu",
                "Yan Hu",
                "Yi Zeng",
                "Lei Wu",
                "Liuyang Bian",
                "Zhaoxiong Wang",
                "Long Liu",
                "Yanzhou Yang",
                "Han Xiao",
                "Aojun Zhou",
                "Yafei Wen",
                "Xiaoxin Chen",
                "Shuai Ren",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10640.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#small_models",
                    "#multimodal",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "BlueLM-V-3B: ĞœĞ¾Ñ‰ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ²Ğ°ÑˆĞµĞ¼ ĞºĞ°Ñ€Ğ¼Ğ°Ğ½Ğµ",
                    "desc": "BlueLM-V-3B - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ (3,1 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (24,4 Ñ‚Ğ¾ĞºĞµĞ½Ğ°/Ñ) Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. BlueLM-V-3B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ˜Ğ˜-Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½ÑƒÑ Ğ¶Ğ¸Ğ·Ğ½ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Efficient MLLMs for Mobile: BlueLM-V-3B Unleashed!",
                    "desc": "This paper introduces BlueLM-V-3B, a multimodal large language model (MLLM) designed for efficient deployment on mobile devices. It addresses the challenges of limited memory and computational power by optimizing model inference through a redesigned dynamic resolution scheme and hardware-aware system optimizations. BlueLM-V-3B features a compact architecture with 2.7 billion parameters for the language model and 400 million for the vision encoder, ensuring fast processing speeds. The model achieves impressive performance metrics, outperforming larger models on benchmarks while maintaining a generation speed of 24.4 tokens per second."
                },
                "zh": {
                    "title": "é«˜æ•ˆéƒ¨ç½²å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ¡ˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBlueLM-V-3Bçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œæ—¨åœ¨é«˜æ•ˆåœ°åœ¨ç§»åŠ¨å¹³å°ä¸Šéƒ¨ç½²ã€‚è¯¥æ¨¡å‹å…·æœ‰2.7äº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹å’Œ4äº¿å‚æ•°çš„è§†è§‰ç¼–ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°å¿«é€Ÿç”Ÿæˆã€‚é€šè¿‡é‡æ–°è®¾è®¡åŠ¨æ€åˆ†è¾¨ç‡æ–¹æ¡ˆå’Œè¿›è¡Œç¡¬ä»¶ä¼˜åŒ–ï¼ŒBlueLM-V-3Båœ¨MediaTek Dimensity 9300å¤„ç†å™¨ä¸Šè¾¾åˆ°äº†æ¯ç§’24.4ä¸ªæ ‡è®°çš„ç”Ÿæˆé€Ÿåº¦ã€‚è¯¥æ¨¡å‹åœ¨OpenCompassåŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†66.1çš„æœ€é«˜å¹³å‡åˆ†ï¼Œè¶…è¶Šäº†è®¸å¤šå‚æ•°æ›´å¤§çš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11504",
            "title": "Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering",
            "url": "https://huggingface.co/papers/2411.11504",
            "abstract": "The evolution of machine learning has increasingly prioritized the development of powerful models and more scalable supervision signals. However, the emergence of foundation models presents significant challenges in providing effective supervision signals necessary for further enhancing their capabilities. Consequently, there is an urgent need to explore novel supervision signals and technical approaches. In this paper, we propose verifier engineering, a novel post-training paradigm specifically designed for the era of foundation models. The core of verifier engineering involves leveraging a suite of automated verifiers to perform verification tasks and deliver meaningful feedback to foundation models. We systematically categorize the verifier engineering process into three essential stages: search, verify, and feedback, and provide a comprehensive review of state-of-the-art research developments within each stage. We believe that verifier engineering constitutes a fundamental pathway toward achieving Artificial General Intelligence.",
            "score": 12,
            "issue_id": 654,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "a48ecb5e8a1da0ae",
            "authors": [
                "Xinyan Guan",
                "Yanjiang Liu",
                "Xinyu Lu",
                "Boxi Cao",
                "Ben He",
                "Xianpei Han",
                "Le Sun",
                "Jie Lou",
                "Bowen Yu",
                "Yaojie Lu",
                "Hongyu Lin"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11504.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#survey",
                    "#training",
                    "#agi"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸' - Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¿Ğ¾Ğ¸ÑĞº, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑ‡Ğ¸Ñ‚Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ½Ğ° Ğ¿ÑƒÑ‚Ğ¸ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Unlocking Foundation Models with Verifier Engineering",
                    "desc": "This paper discusses the challenges of providing effective supervision signals for foundation models in machine learning. It introduces a new approach called verifier engineering, which uses automated verifiers to enhance the capabilities of these models. The process is divided into three stages: search, verify, and feedback, each aimed at improving model performance. The authors argue that this method is crucial for advancing towards Artificial General Intelligence."
                },
                "zh": {
                    "title": "éªŒè¯å™¨å·¥ç¨‹ï¼šè¿ˆå‘äººå·¥é€šç”¨æ™ºèƒ½çš„æ–°è·¯å¾„",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨åŸºç¡€æ¨¡å‹æ—¶ä»£ï¼Œå¦‚ä½•æä¾›æœ‰æ•ˆçš„ç›‘ç£ä¿¡å·ä»¥æå‡æ¨¡å‹èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åè®­ç»ƒèŒƒå¼â€”â€”éªŒè¯å™¨å·¥ç¨‹ï¼Œæ—¨åœ¨åˆ©ç”¨è‡ªåŠ¨åŒ–éªŒè¯å™¨è¿›è¡ŒéªŒè¯ä»»åŠ¡ï¼Œå¹¶ä¸ºåŸºç¡€æ¨¡å‹æä¾›æœ‰æ„ä¹‰çš„åé¦ˆã€‚éªŒè¯å™¨å·¥ç¨‹çš„è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªå…³é”®é˜¶æ®µï¼šæœç´¢ã€éªŒè¯å’Œåé¦ˆï¼Œå¹¶å¯¹æ¯ä¸ªé˜¶æ®µçš„æœ€æ–°ç ”ç©¶è¿›å±•è¿›è¡Œäº†ç³»ç»Ÿæ€§å›é¡¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒéªŒè¯å™¨å·¥ç¨‹æ˜¯å®ç°äººå·¥é€šç”¨æ™ºèƒ½çš„é‡è¦é€”å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.09944",
            "title": "SlimLM: An Efficient Small Language Model for On-Device Document Assistance",
            "url": "https://huggingface.co/papers/2411.09944",
            "abstract": "While small language models (SLMs) show promises for mobile deployment, their real-world performance and applications on smartphones remains underexplored. We present SlimLM, a series of SLMs optimized for document assistance tasks on mobile devices. Through extensive experiments on a Samsung Galaxy S24, we identify the optimal trade-offs between model size (ranging from 125M to 7B parameters), context length, and inference time for efficient on-device processing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on DocAssist, our constructed dataset for summarization, question answering and suggestion tasks. Our smallest model demonstrates efficient performance on S24, while larger variants offer enhanced capabilities within mobile constraints. We evaluate SlimLM against existing SLMs, showing comparable or superior performance and offering a benchmark for future research in on-device language models. We also provide an Android application, offering practical insights into SLM deployment. Our findings provide valuable insights and illuminate the capabilities of running advanced language models on high-end smartphones, potentially reducing server costs and enhancing privacy through on-device processing.",
            "score": 9,
            "issue_id": 655,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 15",
                "zh": "11æœˆ15æ—¥"
            },
            "hash": "ec53cf3813914219",
            "authors": [
                "Thang M. Pham",
                "Phat T. Nguyen",
                "Seunghyun Yoon",
                "Viet Dac Lai",
                "Franck Dernoncourt",
                "Trung Bui"
            ],
            "affiliations": [
                "Adobe Research",
                "Auburn University",
                "Georgia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.09944.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training",
                    "#small_models",
                    "#open_source",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ²Ğ°ÑˆĞµĞ¼ ĞºĞ°Ñ€Ğ¼Ğ°Ğ½Ğµ",
                    "desc": "SlimLM - ÑÑ‚Ğ¾ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¾Ñ‚ 125 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ¾ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Samsung Galaxy S24. SlimLM Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ SlimPajama-627B Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DocAssist Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑĞµÑ€Ğ²ĞµÑ€Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "SlimLM: Efficient Language Models for Mobile Document Assistance",
                    "desc": "This paper introduces SlimLM, a series of small language models designed specifically for mobile devices, focusing on document assistance tasks. The models are optimized for performance on smartphones, balancing size, context length, and inference time to ensure efficient on-device processing. SlimLM is pre-trained on a large dataset and fine-tuned for tasks like summarization and question answering, demonstrating effective performance even with the smallest model. The research highlights the potential of deploying advanced language models on smartphones, which can lower server costs and improve user privacy."
                },
                "zh": {
                    "title": "SlimLMï¼šç§»åŠ¨è®¾å¤‡ä¸Šçš„é«˜æ•ˆè¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSlimLMçš„å°å‹è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œä¸“ä¸ºç§»åŠ¨è®¾å¤‡ä¸Šçš„æ–‡æ¡£è¾…åŠ©ä»»åŠ¡ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨ä¸‰æ˜ŸGalaxy S24ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œæ‰¾åˆ°äº†æ¨¡å‹å¤§å°ã€ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ¨ç†æ—¶é—´ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ï¼Œä»¥å®ç°é«˜æ•ˆçš„æœ¬åœ°å¤„ç†ã€‚SlimLMåœ¨SlimPajama-627Bä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨æˆ‘ä»¬æ„å»ºçš„DocAssistæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ”¯æŒæ‘˜è¦ã€é—®ç­”å’Œå»ºè®®ç­‰ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒSlimLMåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé™ä½æœåŠ¡å™¨æˆæœ¬å¹¶å¢å¼ºéšç§ä¿æŠ¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10836",
            "title": "AnimateAnything: Consistent and Controllable Animation for Video Generation",
            "url": "https://huggingface.co/papers/2411.10836",
            "abstract": "We present a unified controllable video generation approach AnimateAnything that facilitates precise and consistent video manipulation across various conditions, including camera trajectories, text prompts, and user motion annotations. Specifically, we carefully design a multi-scale control feature fusion network to construct a common motion representation for different conditions. It explicitly converts all control information into frame-by-frame optical flows. Then we incorporate the optical flows as motion priors to guide final video generation. In addition, to reduce the flickering issues caused by large-scale motion, we propose a frequency-based stabilization module. It can enhance temporal coherence by ensuring the video's frequency domain consistency. Experiments demonstrate that our method outperforms the state-of-the-art approaches. For more details and videos, please refer to the webpage: https://yu-shaonian.github.io/Animate_Anything/.",
            "score": 9,
            "issue_id": 655,
            "pub_date": "2024-11-16",
            "pub_date_card": {
                "ru": "16 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 16",
                "zh": "11æœˆ16æ—¥"
            },
            "hash": "b979f7de4cf79a50",
            "authors": [
                "Guojun Lei",
                "Chi Wang",
                "Hong Li",
                "Rong Zhang",
                "Yikai Wang",
                "Weiwei Xu"
            ],
            "affiliations": [
                "Beihang University",
                "State Key Lab of CAD&CG, Zhejiang University",
                "Tsinghua University",
                "Zhejiang Gongshang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10836.jpg",
            "data": {
                "categories": [
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "AnimateAnything - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²ÑÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ€Ñ†Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "AnimateAnything: Mastering Video Generation with Precision Control",
                    "desc": "The paper introduces AnimateAnything, a method for generating videos that allows for detailed control over various aspects like camera movement and user inputs. It uses a multi-scale control feature fusion network to create a unified motion representation that can adapt to different conditions. By converting control information into optical flows, the method guides the video generation process effectively. Additionally, a frequency-based stabilization module is implemented to minimize flickering and improve the smoothness of the final video output."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¯æ§è§†é¢‘ç”Ÿæˆï¼Œç²¾å‡†æ“æ§æ¯ä¸€å¸§",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¯æ§è§†é¢‘ç”Ÿæˆæ–¹æ³•AnimateAnythingï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ¡ä»¶ä¸‹å®ç°ç²¾ç¡®å’Œä¸€è‡´çš„è§†é¢‘æ“æ§ï¼ŒåŒ…æ‹¬ç›¸æœºè½¨è¿¹ã€æ–‡æœ¬æç¤ºå’Œç”¨æˆ·è¿åŠ¨æ³¨é‡Šã€‚è¯¥æ–¹æ³•è®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦æ§åˆ¶ç‰¹å¾èåˆç½‘ç»œï¼Œä»¥æ„å»ºä¸åŒæ¡ä»¶ä¸‹çš„å…±åŒè¿åŠ¨è¡¨ç¤ºã€‚å®ƒå°†æ‰€æœ‰æ§åˆ¶ä¿¡æ¯æ˜¾å¼è½¬æ¢ä¸ºé€å¸§çš„å…‰æµï¼Œå¹¶å°†å…‰æµä½œä¸ºè¿åŠ¨å…ˆéªŒæ¥æŒ‡å¯¼æœ€ç»ˆçš„è§†é¢‘ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºé¢‘ç‡çš„ç¨³å®šæ¨¡å—ï¼Œä»¥å‡å°‘å¤§è§„æ¨¡è¿åŠ¨å¼•èµ·çš„é—ªçƒé—®é¢˜ï¼Œä»è€Œå¢å¼ºè§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07641",
            "title": "Top-$nÏƒ$: Not All Logits Are You Need",
            "url": "https://huggingface.co/papers/2411.07641",
            "abstract": "Large language models (LLMs) typically employ greedy decoding or low-temperature sampling for reasoning tasks, reflecting a perceived trade-off between diversity and accuracy. We challenge this convention by introducing top-nsigma, a novel sampling method that operates directly on pre-softmax logits by leveraging a statistical threshold. Our key insight is that logits naturally separate into a Gaussian-distributed noisy region and a distinct informative region, enabling efficient token filtering without complex probability manipulations. Unlike existing methods (e.g., top-p, min-p) that inadvertently include more noise tokens at higher temperatures, top-nsigma maintains a stable sampling space regardless of temperature scaling. We also provide a theoretical analysis of top-nsigma to better understand its behavior. The extensive experimental results across four reasoning-focused datasets demonstrate that our method not only outperforms existing sampling approaches but also surpasses greedy decoding, while maintaining consistent performance even at high temperatures.",
            "score": 9,
            "issue_id": 651,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 12",
                "zh": "11æœˆ12æ—¥"
            },
            "hash": "d3439bf0c336ac57",
            "authors": [
                "Chenxia Tang",
                "Jianchun Liu",
                "Hongli Xu",
                "Liusheng Huang"
            ],
            "affiliations": [
                "School of Computer Science and Technology, University of Science and Technology of China",
                "Suzhou Institute for Advanced Research, University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.07641.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Top-nsigma: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ top-nsigma. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑĞ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Top-nsigma Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¶Ğ°Ğ´Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Top-NSigma: Enhancing Sampling for Better Reasoning in LLMs",
                    "desc": "This paper introduces a new sampling method called top-nsigma for large language models (LLMs) that improves reasoning tasks. Unlike traditional methods that use greedy decoding or low-temperature sampling, top-nsigma filters tokens based on their pre-softmax logits, which are divided into informative and noisy regions. This approach allows for better token selection without the complications of probability adjustments, ensuring a stable sampling process across different temperature settings. Experimental results show that top-nsigma outperforms existing methods and maintains high performance even when using higher temperatures."
                },
                "zh": {
                    "title": "çªç ´ä¼ ç»Ÿï¼Œæå‡æ¨ç†æ€§èƒ½çš„top-nsigmaæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ³•top-nsigmaï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•ç›´æ¥åœ¨é¢„è½¯æœ€å¤§å€¼çš„logitsä¸Šæ“ä½œï¼Œé€šè¿‡ç»Ÿè®¡é˜ˆå€¼æ¥è¿›è¡Œæœ‰æ•ˆçš„ä»¤ç‰Œè¿‡æ»¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œlogitså¯ä»¥è‡ªç„¶åœ°åˆ†ä¸ºé«˜æ–¯åˆ†å¸ƒçš„å™ªå£°åŒºåŸŸå’Œä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸï¼Œä»è€Œé¿å…äº†å¤æ‚çš„æ¦‚ç‡æ“ä½œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œtop-nsigmaåœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„é‡‡æ ·æ–¹æ³•å’Œè´ªå©ªè§£ç ï¼Œä¸”åœ¨é«˜æ¸©åº¦ä¸‹ä»èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11171",
            "title": "LLÃ¤Mmlein: Compact and Competitive German-Only Language Models from Scratch",
            "url": "https://huggingface.co/papers/2411.11171",
            "abstract": "We create two German-only decoder models, LL\\\"aMmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use. The model training involved several key steps, including extensive data preprocessing, the creation of a custom German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks. Throughout the training process, multiple checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor the models' learning dynamics. Compared to state-of-the-art models on the SuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models' quality scales with size as expected, but performance improvements on some tasks plateaued early, offering valuable insights into resource allocation for future model development.",
            "score": 7,
            "issue_id": 656,
            "pub_date": "2024-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "3eea3fe6bac7ab51",
            "authors": [
                "Jan Pfister",
                "Julia Wunderle",
                "Andreas Hotho"
            ],
            "affiliations": [
                "Data Science Chair Center for Artificial Intelligence and Data Science (CAIDAS) Julius-Maximilians-UniversitÃ¤t WÃ¼rzburg (JMU)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11171.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#dataset",
                    "#low_resource",
                    "#open_source",
                    "#training",
                    "#multilingual"
                ],
                "emoji": "ğŸ‡©ğŸ‡ª",
                "ru": {
                    "title": "ĞĞµĞ¼ĞµÑ†ĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¾Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ñƒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ½Ğ° Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ: LL\"aMmlein 120M Ğ¸ 1B. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ» Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SuperGLEBer, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¾ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Empowering German NLP with LL\"aMmlein Models",
                    "desc": "This paper presents two German-only decoder models, LL\"aMmlein 120M and 1B, developed from scratch for the German NLP community. The training process included data preprocessing, a custom tokenizer, and evaluation against benchmarks like SuperGLEBer. The models demonstrated competitive performance, matching or exceeding state-of-the-art models of similar sizes. Insights from the training revealed that while model quality improves with size, some tasks show early performance plateaus, guiding future resource allocation in model development."
                },
                "zh": {
                    "title": "å¾·è¯­è§£ç å™¨æ¨¡å‹çš„åˆ›æ–°ä¸å…±äº«",
                    "desc": "æˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªä»…æ”¯æŒå¾·è¯­çš„è§£ç å™¨æ¨¡å‹ï¼ŒLL\"aMmlein 120Må’Œ1Bï¼Œå¹¶å°†å…¶è®­ç»ƒæ•°æ®å…¬å¼€ï¼Œä¾›å¾·è¯­è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶ç¤¾åŒºä½¿ç”¨ã€‚æ¨¡å‹è®­ç»ƒåŒ…æ‹¬å¤šä¸ªå…³é”®æ­¥éª¤ï¼Œå¦‚æ•°æ®é¢„å¤„ç†ã€è‡ªå®šä¹‰å¾·è¯­åˆ†è¯å™¨çš„åˆ›å»ºã€å®é™…è®­ç»ƒä»¥åŠåœ¨å„ç§åŸºå‡†ä¸Šçš„æœ€ç»ˆæ¨¡å‹è¯„ä¼°ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¿å­˜å¹¶åˆ†æäº†å¤šä¸ªæ£€æŸ¥ç‚¹ï¼Œä½¿ç”¨SuperGLEBeråŸºå‡†ç›‘æµ‹æ¨¡å‹çš„å­¦ä¹ åŠ¨æ€ã€‚ä¸SuperGLEBeråŸºå‡†ä¸Šçš„æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼Œä¸¤ä¸ªLL\"aMmleinæ¨¡å‹è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå§‹ç»ˆä¸ç›¸ä¼¼å‚æ•°å¤§å°çš„æ¨¡å‹ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10669",
            "title": "Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts",
            "url": "https://huggingface.co/papers/2411.10669",
            "abstract": "As the research of Multimodal Large Language Models (MLLMs) becomes popular, an advancing MLLM model is typically required to handle various textual and visual tasks (e.g., VQA, Detection, OCR, and ChartQA) simultaneously for real-world applications. However, due to the significant differences in representation and distribution among data from various tasks, simply mixing data of all tasks together leads to the well-known``multi-task conflict\" issue, resulting in performance degradation across various tasks. To address this issue, we propose Awaker2.5-VL, a Mixture of Experts~(MoE) architecture suitable for MLLM, which acquires the multi-task capabilities through multiple sparsely activated experts. To speed up the training and inference of Awaker2.5-VL, each expert in our model is devised as a low-rank adaptation (LoRA) structure. Extensive experiments on multiple latest benchmarks demonstrate the effectiveness of Awaker2.5-VL. The code and model weight are released in our Project Page: https://github.com/MetabrainAGI/Awaker.",
            "score": 7,
            "issue_id": 654,
            "pub_date": "2024-11-16",
            "pub_date_card": {
                "ru": "16 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 16",
                "zh": "11æœˆ16æ—¥"
            },
            "hash": "f1319420ae85759e",
            "authors": [
                "Jinqiang Long",
                "Yanqi Dai",
                "Guoxing Yang",
                "Hongpeng Lin",
                "Nanyi Fei",
                "Yizhao Gao",
                "Zhiwu Lu"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Metabrain AGI Lab, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10669.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Awaker2.5-VL: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ MLLM Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Awaker2.5-VL - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ (LoRA) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Awaker2.5-VL Ğ½Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Awaker2.5-VL: Mastering Multimodal Tasks with Expert Precision",
                    "desc": "This paper introduces Awaker2.5-VL, a new model designed to improve the performance of Multimodal Large Language Models (MLLMs) on various tasks like visual question answering and detection. The authors address the 'multi-task conflict' problem, which occurs when different tasks interfere with each other due to their diverse data representations. Awaker2.5-VL uses a Mixture of Experts (MoE) architecture, where only a subset of experts is activated for each task, allowing for better specialization and efficiency. Additionally, the model incorporates low-rank adaptation (LoRA) to enhance training speed and inference performance, showing promising results in extensive experiments."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€ä»»åŠ¡çš„ä¸“å®¶æ··åˆè§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºAwaker2.5-VLçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œæ—¨åœ¨åŒæ—¶å¤„ç†æ–‡æœ¬å’Œè§†è§‰ä»»åŠ¡ï¼Œå¦‚è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€æ£€æµ‹ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å’Œå›¾è¡¨é—®ç­”ï¼ˆChartQAï¼‰ã€‚ä¸ºäº†å…‹æœå¤šä»»åŠ¡å†²çªé—®é¢˜ï¼ŒAwaker2.5-VLé‡‡ç”¨äº†ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„ï¼Œé€šè¿‡å¤šä¸ªç¨€ç–æ¿€æ´»çš„ä¸“å®¶æ¥å®ç°å¤šä»»åŠ¡èƒ½åŠ›ã€‚æ¯ä¸ªä¸“å®¶è¢«è®¾è®¡ä¸ºä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ç»“æ„ï¼Œä»¥åŠ é€Ÿæ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒAwaker2.5-VLåœ¨å¤šä¸ªæœ€æ–°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10510",
            "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2411.10510",
            "abstract": "Diffusion Transformers (DiT) have emerged as powerful generative models for various tasks, including image, video, and speech synthesis. However, their inference process remains computationally expensive due to the repeated evaluation of resource-intensive attention and feed-forward modules. To address this, we introduce SmoothCache, a model-agnostic inference acceleration technique for DiT architectures. SmoothCache leverages the observed high similarity between layer outputs across adjacent diffusion timesteps. By analyzing layer-wise representation errors from a small calibration set, SmoothCache adaptively caches and reuses key features during inference. Our experiments demonstrate that SmoothCache achieves 8% to 71% speed up while maintaining or even improving generation quality across diverse modalities. We showcase its effectiveness on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio, highlighting its potential to enable real-time applications and broaden the accessibility of powerful DiT models.",
            "score": 7,
            "issue_id": 654,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 15",
                "zh": "11æœˆ15æ—¥"
            },
            "hash": "991f548fec1ec8c9",
            "authors": [
                "Joseph Liu",
                "Joshua Geddes",
                "Ziyu Guo",
                "Haomiao Jiang",
                "Mahesh Kumar Nandwana"
            ],
            "affiliations": [
                "Queens University",
                "Roblox"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10510.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#video",
                    "#optimization",
                    "#audio",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "SmoothCache: Ğ‘Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ ÑƒĞ¼Ğ½Ñ‹Ğ¼ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ DiT",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SmoothCache - Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Diffusion Transformers (DiT). SmoothCache Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾ĞµĞ² Ğ½Ğ° ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºÑÑˆĞ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SmoothCache Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ 8% Ğ´Ğ¾ 71% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Accelerating Diffusion Transformers with SmoothCache",
                    "desc": "This paper presents SmoothCache, a technique designed to speed up the inference process of Diffusion Transformers (DiT), which are used for generating images, videos, and audio. The traditional inference method is slow because it requires many evaluations of complex attention and feed-forward layers. SmoothCache improves efficiency by caching and reusing similar outputs from adjacent diffusion timesteps, reducing the need for repeated calculations. Experiments show that this method can accelerate inference by 8% to 71% while maintaining or enhancing the quality of the generated content."
                },
                "zh": {
                    "title": "SmoothCacheï¼šåŠ é€Ÿæ‰©æ•£å˜æ¢å™¨çš„æ¨ç†è¿‡ç¨‹",
                    "desc": "æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒã€è§†é¢‘å’Œè¯­éŸ³åˆæˆç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ¨ç†è¿‡ç¨‹è®¡ç®—å¼€é”€è¾ƒå¤§ï¼Œå› ä¸ºéœ€è¦é‡å¤è¯„ä¼°èµ„æºå¯†é›†å‹çš„æ³¨æ„åŠ›å’Œå‰é¦ˆæ¨¡å—ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SmoothCacheï¼Œè¿™æ˜¯ä¸€ç§ä¸æ¨¡å‹æ— å…³çš„æ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œåˆ©ç”¨ç›¸é‚»æ‰©æ•£æ—¶é—´æ­¥ä¹‹é—´å±‚è¾“å‡ºçš„é«˜åº¦ç›¸ä¼¼æ€§ã€‚é€šè¿‡åˆ†æå°å‹æ ¡å‡†é›†ä¸­çš„å±‚çº§è¡¨ç¤ºè¯¯å·®ï¼ŒSmoothCacheè‡ªé€‚åº”åœ°ç¼“å­˜å’Œé‡ç”¨å…³é”®ç‰¹å¾ï¼Œä»è€Œåœ¨ä¿æŒæˆ–æé«˜ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†8%åˆ°71%çš„é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11767",
            "title": "Drowning in Documents: Consequences of Scaling Reranker Inference",
            "url": "https://huggingface.co/papers/2411.11767",
            "abstract": "Rerankers, typically cross-encoders, are often used to re-score the documents retrieved by cheaper initial IR systems. This is because, though expensive, rerankers are assumed to be more effective. We challenge this assumption by measuring reranker performance for full retrieval, not just re-scoring first-stage retrieval. Our experiments reveal a surprising trend: the best existing rerankers provide diminishing returns when scoring progressively more documents and actually degrade quality beyond a certain limit. In fact, in this setting, rerankers can frequently assign high scores to documents with no lexical or semantic overlap with the query. We hope that our findings will spur future research to improve reranking.",
            "score": 7,
            "issue_id": 652,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "3fa06087787bc8d4",
            "authors": [
                "Mathew Jacob",
                "Erik Lindgren",
                "Matei Zaharia",
                "Michael Carbin",
                "Omar Khattab",
                "Andrew Drozdov"
            ],
            "affiliations": [
                "Databricks",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11767.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² (rerankers) Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ÑÑ. Ğ‘Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ³Ğ¾, Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ±ĞµĞ· Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ½Ğ°Ğ´ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Rethinking Rerankers: Diminishing Returns in Document Scoring",
                    "desc": "This paper investigates the effectiveness of rerankers, specifically cross-encoders, in the context of information retrieval (IR). Traditionally, rerankers are believed to enhance the quality of document scoring after an initial retrieval phase. However, the authors find that as more documents are scored, the performance of these rerankers diminishes and can even lead to poorer results. The study highlights that rerankers may assign high scores to irrelevant documents, suggesting a need for improved methods in reranking processes."
                },
                "zh": {
                    "title": "é‡æ’åºå™¨çš„æœ‰æ•ˆæ€§éœ€é‡æ–°å®¡è§†",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é‡æ’åºå™¨ï¼ˆé€šå¸¸æ˜¯äº¤å‰ç¼–ç å™¨ï¼‰åœ¨ä¿¡æ¯æ£€ç´¢ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬é€šè¿‡æµ‹é‡é‡æ’åºå™¨åœ¨å®Œæ•´æ£€ç´¢ä¸­çš„è¡¨ç°ï¼ŒæŒ‘æˆ˜äº†å®ƒä»¬åœ¨åˆæ­¥æ£€ç´¢åé‡æ–°è¯„åˆ†çš„å‡è®¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„é‡æ’åºå™¨åœ¨è¯„åˆ†è¶Šæ¥è¶Šå¤šçš„æ–‡æ¡£æ—¶ï¼Œæ•ˆæœé€æ¸å‡å¼±ï¼Œç”šè‡³åœ¨æŸä¸ªé™åº¦åè´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬çš„å‘ç°å¸Œæœ›èƒ½æ¿€åŠ±æœªæ¥çš„ç ”ç©¶ï¼Œä»¥æ”¹è¿›é‡æ’åºæŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.09213",
            "title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering",
            "url": "https://huggingface.co/papers/2411.09213",
            "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates a completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of a reliable medical system. This paper addresses this gap by providing a comprehensive evaluation framework for medical question-answering (QA) systems in a RAG setting for these situations, including sufficiency, integration, and robustness. We introduce Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to four medical QA datasets for testing LLMs' ability to handle these specific scenarios. Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results reveals current models' limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs' reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain.",
            "score": 6,
            "issue_id": 655,
            "pub_date": "2024-11-14",
            "pub_date_card": {
                "ru": "14 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 14",
                "zh": "11æœˆ14æ—¥"
            },
            "hash": "e99e85d88963aa4c",
            "authors": [
                "Nghia Trung Ngo",
                "Chien Van Nguyen",
                "Franck Dernoncourt",
                "Thien Huu Nguyen"
            ],
            "affiliations": [
                "Adobe Research, USA",
                "Department of Computer Science, University of Oregon, OR, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.09213.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#survey",
                    "#healthcare",
                    "#open_source",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… RAG-ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… retrieval-augmented generation (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MedRGB, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Medical QA with Robust Retrieval-Augmented Generation",
                    "desc": "This paper introduces a new evaluation framework called Medical Retrieval-Augmented Generation Benchmark (MedRGB) to improve the performance of large language models (LLMs) in medical question-answering tasks. It highlights the need for accurate and trustworthy systems in the sensitive medical domain, addressing gaps in existing benchmarks that do not consider practical scenarios. The authors conduct extensive evaluations of both commercial and open-source LLMs, revealing their limitations in dealing with noise and misinformation in retrieved documents. The study also analyzes the reasoning processes of these models, providing insights for future improvements in retrieval-augmented generation systems for medical applications."
                },
                "zh": {
                    "title": "æå‡åŒ»ç–—é—®ç­”ç³»ç»Ÿçš„å¯é æ€§",
                    "desc": "æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»ç–—é¢†åŸŸã€‚ç„¶è€Œï¼ŒåŒ»ç–—é¢†åŸŸçš„æ•æ„Ÿæ€§è¦æ±‚ç³»ç»Ÿå¿…é¡»å®Œå…¨å‡†ç¡®å’Œå¯ä¿¡ã€‚ç°æœ‰çš„RAGåŸºå‡†ä¸»è¦å…³æ³¨æ ‡å‡†çš„æ£€ç´¢-å›ç­”è®¾ç½®ï¼Œå¿½è§†äº†è®¸å¤šå®é™…åœºæ™¯ï¼Œè¿™äº›åœºæ™¯å¯¹å¯é åŒ»ç–—ç³»ç»Ÿçš„å…³é”®æ–¹é¢è¿›è¡Œäº†è¯„ä¼°ã€‚æœ¬æ–‡æå‡ºäº†åŒ»ç–—æ£€ç´¢å¢å¼ºç”ŸæˆåŸºå‡†ï¼ˆMedRGBï¼‰ï¼Œä¸ºå››ä¸ªåŒ»ç–—é—®ç­”æ•°æ®é›†æä¾›äº†å„ç§è¡¥å……å…ƒç´ ï¼Œä»¥æµ‹è¯•LLMsåœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„å¤„ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11024",
            "title": "VeGaS: Video Gaussian Splatting",
            "url": "https://huggingface.co/papers/2411.11024",
            "abstract": "Implicit Neural Representations (INRs) employ neural networks to approximate discrete data as continuous functions. In the context of video data, such models can be utilized to transform the coordinates of pixel locations along with frame occurrence times (or indices) into RGB color values. Although INRs facilitate effective compression, they are unsuitable for editing purposes. One potential solution is to use a 3D Gaussian Splatting (3DGS) based model, such as the Video Gaussian Representation (VGR), which is capable of encoding video as a multitude of 3D Gaussians and is applicable for numerous video processing operations, including editing. Nevertheless, in this case, the capacity for modification is constrained to a limited set of basic transformations. To address this issue, we introduce the Video Gaussian Splatting (VeGaS) model, which enables realistic modifications of video data. To construct VeGaS, we propose a novel family of Folded-Gaussian distributions designed to capture nonlinear dynamics in a video stream and model consecutive frames by 2D Gaussians obtained as respective conditional distributions. Our experiments demonstrate that VeGaS outperforms state-of-the-art solutions in frame reconstruction tasks and allows realistic modifications of video data. The code is available at: https://github.com/gmum/VeGaS.",
            "score": 5,
            "issue_id": 661,
            "pub_date": "2024-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "0c6ed02c1597f4ff",
            "authors": [
                "Weronika Smolak-DyÅ¼ewska",
                "Dawid Malarz",
                "Kornel Howil",
                "Jan Kaczmarczyk",
                "Marcin Mazur",
                "PrzemysÅ‚aw Spurek"
            ],
            "affiliations": [
                "Jagiellonian University Faculty of Mathematics and Computer Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11024.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "VeGaS: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Video Gaussian Splatting (VeGaS) Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. VeGaS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ÑĞ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². VeGaS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "VeGaS: Revolutionizing Video Editing with Gaussian Splatting",
                    "desc": "Implicit Neural Representations (INRs) use neural networks to represent discrete data as continuous functions, particularly in video processing. They convert pixel coordinates and frame indices into RGB values, enabling effective data compression but limiting editing capabilities. The Video Gaussian Splatting (VeGaS) model enhances this by utilizing a new family of Folded-Gaussian distributions, allowing for realistic video modifications while capturing nonlinear dynamics. Our experiments show that VeGaS surpasses existing methods in frame reconstruction and supports a wider range of editing operations."
                },
                "zh": {
                    "title": "è§†é¢‘æ•°æ®çš„çœŸå®ä¿®æ”¹æ–°æ–¹æ³•",
                    "desc": "éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRsï¼‰ä½¿ç”¨ç¥ç»ç½‘ç»œå°†ç¦»æ•£æ•°æ®è¿‘ä¼¼ä¸ºè¿ç»­å‡½æ•°ã€‚åœ¨è§†é¢‘æ•°æ®ä¸­ï¼Œè¿™ç§æ¨¡å‹å¯ä»¥å°†åƒç´ ä½ç½®çš„åæ ‡å’Œå¸§å‡ºç°æ—¶é—´è½¬æ¢ä¸ºRGBé¢œè‰²å€¼ã€‚è™½ç„¶INRsåœ¨å‹ç¼©æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸é€‚åˆç¼–è¾‘ã€‚æˆ‘ä»¬æå‡ºçš„è§†é¢‘é«˜æ–¯ç‚¹äº‘ï¼ˆVeGaSï¼‰æ¨¡å‹ï¼Œåˆ©ç”¨æ–°å‹çš„æŠ˜å é«˜æ–¯åˆ†å¸ƒï¼Œèƒ½å¤Ÿå®ç°è§†é¢‘æ•°æ®çš„çœŸå®ä¿®æ”¹ï¼Œå¹¶åœ¨å¸§é‡å»ºä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11045",
            "title": "StableV2V: Stablizing Shape Consistency in Video-to-Video Editing",
            "url": "https://huggingface.co/papers/2411.11045",
            "abstract": "Recent advancements of generative AI have significantly promoted content creation and editing, where prevailing studies further extend this exciting progress to video editing. In doing so, these studies mainly transfer the inherent motion patterns from the source videos to the edited ones, where results with inferior consistency to user prompts are often observed, due to the lack of particular alignments between the delivered motions and edited contents. To address this limitation, we present a shape-consistent video editing method, namely StableV2V, in this paper. Our method decomposes the entire editing pipeline into several sequential procedures, where it edits the first video frame, then establishes an alignment between the delivered motions and user prompts, and eventually propagates the edited contents to all other frames based on such alignment. Furthermore, we curate a testing benchmark, namely DAVIS-Edit, for a comprehensive evaluation of video editing, considering various types of prompts and difficulties. Experimental results and analyses illustrate the outperforming performance, visual consistency, and inference efficiency of our method compared to existing state-of-the-art studies.",
            "score": 5,
            "issue_id": 658,
            "pub_date": "2024-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "d4f742e7121f2322",
            "authors": [
                "Chang Liu",
                "Rui Li",
                "Kaidong Zhang",
                "Yunwei Lan",
                "Dong Liu"
            ],
            "affiliations": [
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11045.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#games",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "StableV2V: Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ StableV2V, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DAVIS-Edit Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ StableV2V Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "StableV2V: Aligning Motion with User Prompts for Consistent Video Editing",
                    "desc": "This paper introduces StableV2V, a novel method for video editing that enhances the consistency between user prompts and the resulting video content. The approach involves breaking down the editing process into sequential steps, starting with the first frame and aligning the motion patterns with user instructions. By propagating the edited content across all frames based on this alignment, StableV2V achieves better visual coherence and performance. Additionally, the authors present a new benchmark, DAVIS-Edit, to evaluate the effectiveness of video editing methods under various conditions."
                },
                "zh": {
                    "title": "ç¨³å®šä¸€è‡´çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºStableV2Vçš„å½¢çŠ¶ä¸€è‡´æ€§è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç¼–è¾‘çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•å°†æ•´ä¸ªç¼–è¾‘æµç¨‹åˆ†è§£ä¸ºå¤šä¸ªé¡ºåºæ­¥éª¤ï¼Œé¦–å…ˆç¼–è¾‘ç¬¬ä¸€å¸§è§†é¢‘ï¼Œç„¶ååœ¨ç”¨æˆ·æç¤ºå’Œä¼ é€’çš„è¿åŠ¨ä¹‹é—´å»ºç«‹å¯¹é½ï¼Œæœ€åæ ¹æ®è¿™ç§å¯¹é½å°†ç¼–è¾‘å†…å®¹ä¼ æ’­åˆ°å…¶ä»–å¸§ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªæµ‹è¯•åŸºå‡†DAVIS-Editï¼Œä»¥å…¨é¢è¯„ä¼°è§†é¢‘ç¼–è¾‘çš„æ•ˆæœï¼Œè€ƒè™‘äº†ä¸åŒç±»å‹çš„æç¤ºå’Œéš¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›ç ”ç©¶ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ã€è§†è§‰ä¸€è‡´æ€§å’Œæ¨ç†æ•ˆç‡ä¸Šå‡è¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.09661",
            "title": "Adaptive Decoding via Latent Preference Optimization",
            "url": "https://huggingface.co/papers/2411.09661",
            "abstract": "During language model decoding, it is known that using higher temperature sampling gives more creative responses, while lower temperatures are more factually accurate. However, such models are commonly applied to general instruction following, which involves both creative and fact seeking tasks, using a single fixed temperature across all examples and tokens. In this work, we introduce Adaptive Decoding, a layer added to the model to select the sampling temperature dynamically at inference time, at either the token or example level, in order to optimize performance. To learn its parameters we introduce Latent Preference Optimization (LPO) a general approach to train discrete latent variables such as choices of temperature. Our method outperforms all fixed decoding temperatures across a range of tasks that require different temperatures, including UltraFeedback, Creative Story Writing, and GSM8K.",
            "score": 4,
            "issue_id": 659,
            "pub_date": "2024-11-14",
            "pub_date_card": {
                "ru": "14 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 14",
                "zh": "11æœˆ14æ—¥"
            },
            "hash": "4bb5fff16280c4bc",
            "authors": [
                "Shehzaad Dhuliawala",
                "Ilia Kulikov",
                "Ping Yu",
                "Asli Celikyilmaz",
                "Jason Weston",
                "Sainbayar Sukhbaatar",
                "Jack Lanchantin"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.09661.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#story_generation",
                    "#inference"
                ],
                "emoji": "ğŸŒ¡ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñƒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (LPO) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… UltraFeedback, Creative Story Writing Ğ¸ GSM8K."
                },
                "en": {
                    "title": "Dynamic Temperature for Optimal Language Model Responses",
                    "desc": "This paper presents a new method called Adaptive Decoding, which allows language models to adjust their sampling temperature dynamically during inference. By varying the temperature, the model can balance creativity and factual accuracy based on the specific task at hand. The authors introduce Latent Preference Optimization (LPO) to effectively train the model's temperature selection process. Their approach shows improved performance over traditional fixed temperature methods across various tasks, demonstrating its versatility and effectiveness."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”è§£ç ï¼šåŠ¨æ€é€‰æ‹©æ¸©åº¦ä¼˜åŒ–æ¨¡å‹è¡¨ç°",
                    "desc": "åœ¨è¯­è¨€æ¨¡å‹è§£ç è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨è¾ƒé«˜çš„æ¸©åº¦é‡‡æ ·å¯ä»¥äº§ç”Ÿæ›´å…·åˆ›æ„çš„å“åº”ï¼Œè€Œè¾ƒä½çš„æ¸©åº¦åˆ™æ›´å‡†ç¡®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”è§£ç æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶åŠ¨æ€é€‰æ‹©é‡‡æ ·æ¸©åº¦ï¼Œä¼˜åŒ–æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å¼•å…¥äº†æ½œåœ¨åå¥½ä¼˜åŒ–ï¼ˆLPOï¼‰æ¥è®­ç»ƒæ¸©åº¦é€‰æ‹©çš„ç¦»æ•£æ½œå˜é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨éœ€è¦ä¸åŒæ¸©åº¦çš„å¤šç§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå›ºå®šè§£ç æ¸©åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10499",
            "title": "FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on",
            "url": "https://huggingface.co/papers/2411.10499",
            "abstract": "Although image-based virtual try-on has made considerable progress, emerging approaches still encounter challenges in producing high-fidelity and robust fitting images across diverse scenarios. These methods often struggle with issues such as texture-aware maintenance and size-aware fitting, which hinder their overall effectiveness. To address these limitations, we propose a novel garment perception enhancement technique, termed FitDiT, designed for high-fidelity virtual try-on using Diffusion Transformers (DiT) allocating more parameters and attention to high-resolution features. First, to further improve texture-aware maintenance, we introduce a garment texture extractor that incorporates garment priors evolution to fine-tune garment feature, facilitating to better capture rich details such as stripes, patterns, and text. Additionally, we introduce frequency-domain learning by customizing a frequency distance loss to enhance high-frequency garment details. To tackle the size-aware fitting issue, we employ a dilated-relaxed mask strategy that adapts to the correct length of garments, preventing the generation of garments that fill the entire mask area during cross-category try-on. Equipped with the above design, FitDiT surpasses all baselines in both qualitative and quantitative evaluations. It excels in producing well-fitting garments with photorealistic and intricate details, while also achieving competitive inference times of 4.57 seconds for a single 1024x768 image after DiT structure slimming, outperforming existing methods.",
            "score": 4,
            "issue_id": 654,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 15",
                "zh": "11æœˆ15æ—¥"
            },
            "hash": "b142b4be26ef6147",
            "authors": [
                "Boyuan Jiang",
                "Xiaobin Hu",
                "Donghao Luo",
                "Qingdong He",
                "Chengming Xu",
                "Jinlong Peng",
                "Jiangning Zhang",
                "Chengjie Wang",
                "Yunsheng Wu",
                "Yanwei Fu"
            ],
            "affiliations": [
                "Fudan University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10499.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#3d",
                    "#diffusion"
                ],
                "emoji": "ğŸ‘š",
                "ru": {
                    "title": "FitDiT: Ğ’Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FitDiT, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. FitDiT ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "FitDiT: Revolutionizing Virtual Try-On with High-Fidelity Garment Perception",
                    "desc": "This paper presents FitDiT, a new technique for improving virtual try-on systems using Diffusion Transformers. It addresses challenges in producing realistic and well-fitting images by enhancing garment perception through a specialized texture extractor and frequency-domain learning. The method also incorporates a dilated-relaxed mask strategy to ensure garments fit correctly without oversizing. FitDiT demonstrates superior performance in generating high-fidelity images with intricate details while maintaining efficient processing times."
                },
                "zh": {
                    "title": "FitDiTï¼šé«˜ä¿çœŸè™šæ‹Ÿè¯•ç©¿çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æœè£…æ„ŸçŸ¥å¢å¼ºæŠ€æœ¯ï¼Œç§°ä¸ºFitDiTï¼Œæ—¨åœ¨æé«˜è™šæ‹Ÿè¯•ç©¿çš„é«˜ä¿çœŸåº¦ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰åˆ†é…æ›´å¤šå‚æ•°å’Œæ³¨æ„åŠ›äºé«˜åˆ†è¾¨ç‡ç‰¹å¾ï¼Œä»¥è§£å†³çº¹ç†æ„ŸçŸ¥ç»´æŠ¤å’Œå°ºå¯¸æ„ŸçŸ¥é€‚é…çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†æœè£…çº¹ç†æå–å™¨å’Œé¢‘åŸŸå­¦ä¹ ï¼Œå¢å¼ºäº†æœè£…ç»†èŠ‚çš„æ•æ‰èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨æ‰©å¼ æ”¾æ¾æ©ç ç­–ç•¥æ¥é€‚åº”æœè£…çš„æ­£ç¡®é•¿åº¦ã€‚FitDiTåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å‡è¶…è¶Šäº†æ‰€æœ‰åŸºçº¿ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰çœŸå®æ„Ÿå’Œå¤æ‚ç»†èŠ‚çš„åˆèº«æœè£…ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10168",
            "title": "Evaluating the role of `Constitutions' for learning from AI feedback",
            "url": "https://huggingface.co/papers/2411.10168",
            "abstract": "The growing capabilities of large language models (LLMs) have led to their use as substitutes for human feedback for training and assessing other LLMs. These methods often rely on `constitutions', written guidelines which a critic model uses to provide feedback and improve generations. We investigate how the choice of constitution affects feedback quality by using four different constitutions to improve patient-centered communication in medical interviews. In pairwise comparisons conducted by 215 human raters, we found that detailed constitutions led to better results regarding emotive qualities. However, none of the constitutions outperformed the baseline in learning more practically-oriented skills related to information gathering and provision. Our findings indicate that while detailed constitutions should be prioritised, there are possible limitations to the effectiveness of AI feedback as a reward signal in certain areas.",
            "score": 3,
            "issue_id": 661,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 15",
                "zh": "11æœˆ15æ—¥"
            },
            "hash": "491ed277e2d6a217",
            "authors": [
                "Saskia Redgate",
                "Andrew M. Bean",
                "Adam Mahdi"
            ],
            "affiliations": [
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10168.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#interpretability",
                    "#alignment",
                    "#healthcare"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞšĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¸ LLM: Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… LLM Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° 'ĞºĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¸' (Ğ¿Ğ¸ÑÑŒĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹) Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ€Ğ°Ñ‡Ğ¾Ğ¼ Ğ¸ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ñ… ÑĞ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ˜Ğ˜ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Enhancing AI Feedback with Detailed Guidelines",
                    "desc": "This paper explores how large language models (LLMs) can be trained and evaluated using human-like feedback, specifically through the use of 'constitutions'â€”guidelines that help a critic model assess and improve LLM outputs. The study tests four different constitutions to enhance patient-centered communication during medical interviews. Results from 215 human raters show that more detailed constitutions improve the emotive qualities of the communication, but they do not significantly enhance practical skills like information gathering. The findings suggest that while detailed guidelines are beneficial, there are limitations to using AI feedback as a reward signal for certain competencies."
                },
                "zh": {
                    "title": "è¯¦ç»†æŒ‡å¯¼åŸåˆ™æå‡æƒ…æ„Ÿè´¨é‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®­ç»ƒå’Œè¯„ä¼°å…¶ä»–LLMsæ—¶ï¼Œå¦‚ä½•ä½œä¸ºäººç±»åé¦ˆçš„æ›¿ä»£å“ã€‚ç ”ç©¶ä¸­ä½¿ç”¨äº†å››ç§ä¸åŒçš„æŒ‡å¯¼åŸåˆ™ï¼ˆconstitutionï¼‰ï¼Œä»¥æ”¹å–„åŒ»ç–—è®¿è°ˆä¸­çš„ä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„æ²Ÿé€šã€‚é€šè¿‡215åäººç±»è¯„å®¡è€…çš„å¯¹æ¯”å®éªŒï¼Œæˆ‘ä»¬å‘ç°è¯¦ç»†çš„æŒ‡å¯¼åŸåˆ™åœ¨æƒ…æ„Ÿè´¨é‡æ–¹é¢çš„åé¦ˆæ•ˆæœæ›´å¥½ã€‚ç„¶è€Œï¼Œåœ¨ä¿¡æ¯æ”¶é›†å’Œæä¾›ç­‰å®é™…æŠ€èƒ½çš„å­¦ä¹ ä¸Šï¼Œæ²¡æœ‰ä»»ä½•æŒ‡å¯¼åŸåˆ™è¶…è¶Šäº†åŸºçº¿è¡¨ç°ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-18.html",
    "link_next": "2024-11-20.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "18.11",
        "en": "11/18",
        "zh": "11æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.11",
        "en": "11/20",
        "zh": "11æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 7,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 0,
        "#inference": 4,
        "#3d": 3,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 2,
        "#training": 6,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 2,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å…´èµ·åŠå…¶åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„æ½œåŠ›ã€‚æ‰‹æœºæ˜¯éƒ¨ç½²MLLMsçš„æœ€ä½³å¹³å°ï¼Œä½†ç”±äºå†…å­˜å’Œè®¡ç®—èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥å®ç°æµç•…çš„å®æ—¶å¤„ç†ã€‚æ–‡ç« æå‡ºäº†BlueLM-V-3Bï¼Œä¸€ç§ä¸“ä¸ºç§»åŠ¨å¹³å°è®¾è®¡çš„ç®—æ³•å’Œç³»ç»Ÿã€‚å®ƒé€šè¿‡é‡æ–°è®¾è®¡åŠ¨æ€åˆ†è¾¨ç‡æ–¹æ¡ˆå’Œç¡¬ä»¶æ„ŸçŸ¥éƒ¨ç½²ä¼˜åŒ–ï¼Œå®ç°äº†å°å°ºå¯¸ã€å¿«é€Ÿåº¦å’Œå¼ºæ€§èƒ½ã€‚\n\nTranslation:\nThis article discusses the rise of multimodal large language models (MLLMs) and their potential in daily life. Mobile phones are the best platform for deploying MLLMs, but due to memory and computational limitations, real-time processing is challenging. The article presents BlueLM-V-3B, an algorithm and system designed for mobile platforms. It achieves small size, fast speed, and strong performance through dynamic resolution scheme redesign and hardware-aware deployment optimization.",
        "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices",
        "pinyin": "Sure, here is the pinyin transcription for the given text:\n\nZhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le duÅ mÃ³ tÃ i dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (MLLMs) de xÄ«ng qÇ jÃ­ qÃ­ zÃ i rÃ¬ chÃ¡ng shÄ“ng huÃ³ zhÅng de qiÃ¡n lÃ¬. ShÇ’u jÄ« shÃ¬ bÃ¹ shÇ” MLLMs de zuÃ¬ jiÄ pÃ­ng tÃ i, dÃ n yÃ³u yÃº nÃ¨i cÃ¹n hÃ© suÃ n jÃ¬ nÃ©ng lÃ¬ yÇ’u xiÃ n, nÃ¡n yÇ shÃ­ xiÃ n liÃº chÃ ng de shÃ­ shÃ­ chÇ” lÇ. WÃ©n zhÄng tÃ­ chÅ« le BlueLM-V-3B, yÄ« zhÇ’ng zhuÄn wÃ¨i yÃ­ dÃ²ng pÃ­ng tÃ i shÃ¨ jÃ¬ de suÃ n fÇ hÃ© xÃ¬ tÇ’ng. TÄ tÅng guÃ² chÃ³ng xÄ«n shÃ¨ jÃ¬ dÃ²ng tÃ i fÄ“n biÃ© lÇœ fÄng Än hÃ© yÃ¬ng jiÃ n gÇn zhÄ« bÃ¹ shÇ” yÅu huÃ , shÃ­ xiÃ n le xiÇo chÇ cÃ¹, kuÃ i sÃ¹ dÃ¹ hÃ© qiÃ¡ng xÃ¬ng nÃ©ng.\n\nTranslation:\nThis article discusses the rise of multimodal large language models (MLLMs) and their potential in daily life. Mobile phones are the best platform for deploying MLLMs, but due to memory and computational limitations, real-time processing is challenging. The article presents BlueLM-V-3B, an algorithm and system designed for mobile platforms. It achieves small size, fast speed, and strong performance through dynamic resolution scheme redesign and hardware-aware deployment optimization.",
        "vocab": "[{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'},\n{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language models'},\n{'word': 'å…´èµ·', 'pinyin': 'xÄ«ng qÇ', 'trans': 'rise'},\n{'word': 'æ½œåŠ›', 'pinyin': 'qiÃ¡n lÃ¬', 'trans': 'potential'},\n{'word': 'éƒ¨ç½²', 'pinyin': 'bÃ¹ shÇ”', 'trans': 'deploy'},\n{'word': 'å¹³å°', 'pinyin': 'pÃ­ng tÃ¡i', 'trans': 'platform'},\n{'word': 'å†…å­˜', 'pinyin': 'nÃ¨i cÃºn', 'trans': 'memory'},\n{'word': 'è®¡ç®—', 'pinyin': 'jÃ¬ suÃ n', 'trans': 'computational'},\n{'word': 'æµç•…', 'pinyin': 'liÃº chÃ ng', 'trans': 'smooth'},\n{'word': 'å®æ—¶', 'pinyin': 'shÃ­ shÃ­', 'trans': 'real-time'},\n{'word': 'å¤„ç†', 'pinyin': 'chÇ” lÇ', 'trans': 'processing'},\n{'word': 'ç®—æ³•', 'pinyin': 'suÃ n fÇ', 'trans': 'algorithm'},\n{'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ng tÃ i', 'trans': 'dynamic'},\n{'word': 'åˆ†è¾¨ç‡', 'pinyin': 'fÄ“n biÃ n lÇœ', 'trans': 'resolution'},\n{'word': 'æ–¹æ¡ˆ', 'pinyin': 'fÄng Ã n', 'trans': 'scheme'},\n{'word': 'ç¡¬ä»¶', 'pinyin': 'yÃ¬ng jiÃ n', 'trans': 'hardware'},\n{'word': 'æ„ŸçŸ¥', 'pinyin': 'gÇn zhÄ«', 'trans': 'aware'},\n{'word': 'ä¼˜åŒ–', 'pinyin': 'yÅu huÃ ', 'trans': 'optimization'}]",
        "trans": "Here's a refined translation of the text:\n\n\"This article explores the emergence of multimodal large language models (MLLMs) and their potential applications in everyday life. While mobile phones serve as an ideal platform for deploying MLLMs, their limited memory and computational power present challenges for smooth real-time processing. To address these issues, the article introduces BlueLM-V-3B, an algorithm and system tailored for mobile platforms. By employing a redesigned dynamic resolution scheme and hardware-aware deployment optimizations, BlueLM-V-3B achieves a compact size, high speed, and robust performance.\"\n\nChanges made:\n1. Used \"explores the emergence\" instead of \"discusses the rise\" for a more formal tone.\n2. Added \"applications in\" for better flow.\n3. Changed \"difficult to achieve\" to \"present challenges for\" to improve phrasing.\n4. Used \"to address these issues\" to create a stronger connection between sentences.\n5. Changed \"it achieves\" to \"By employing... BlueLM-V-3B achieves\" for clarity.\n6. Used \"robust performance\" instead of \"strong performance\" for more natural English.",
        "update_ts": "2024-11-19 09:11"
    }
}