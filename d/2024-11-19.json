{
    "date": {
        "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 19",
        "zh": "11æœˆ19æ—¥"
    },
    "time_utc": "2024-11-19 04:13",
    "weekday": 1,
    "issue_id": 652,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.10640",
            "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices",
            "url": "https://huggingface.co/papers/2411.10640",
            "abstract": "The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile phones, as essential daily companions, represent the most effective and accessible deployment platform for MLLMs, enabling seamless integration into everyday tasks. However, deploying MLLMs on mobile phones presents challenges due to limitations in memory size and computational capability, making it difficult to achieve smooth and real-time processing without extensive optimization. In this paper, we present BlueLM-V-3B, an algorithm and system co-design approach specifically tailored for the efficient deployment of MLLMs on mobile platforms. To be specific, we redesign the dynamic resolution scheme adopted by mainstream MLLMs and implement system optimization for hardware-aware deployment to optimize model inference on mobile phones. BlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B features a language model with 2.7B parameters and a vision encoder with 400M parameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4 token/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight quantization. (3) Strong Performance: BlueLM-V-3B has attained the highest average score of 66.1 on the OpenCompass benchmark among models with leq 4B parameters and surpassed a series of models with much larger parameter sizes (e.g., MiniCPM-V-2.6, InternVL2-8B).",
            "score": 3,
            "issue_id": 652,
            "pub_date": "2024-11-16",
            "pub_date_card": {
                "ru": "16 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 16",
                "zh": "11æœˆ16æ—¥"
            },
            "hash": "0366549d4347dfd2",
            "authors": [
                "Xudong Lu",
                "Yinghao Chen",
                "Cheng Chen",
                "Hui Tan",
                "Boheng Chen",
                "Yina Xie",
                "Rui Hu",
                "Guanxin Tan",
                "Renshou Wu",
                "Yan Hu",
                "Yi Zeng",
                "Lei Wu",
                "Liuyang Bian",
                "Zhaoxiong Wang",
                "Long Liu",
                "Yanzhou Yang",
                "Han Xiao",
                "Aojun Zhou",
                "Yafei Wen",
                "Xiaoxin Chen",
                "Shuai Ren",
                "Hongsheng Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "vivo AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10640.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#small_models",
                    "#multimodal",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "BlueLM-V-3B: ĞœĞ¾Ñ‰ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ²Ğ°ÑˆĞµĞ¼ ĞºĞ°Ñ€Ğ¼Ğ°Ğ½Ğµ",
                    "desc": "BlueLM-V-3B - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ (3,1 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (24,4 Ñ‚Ğ¾ĞºĞµĞ½Ğ°/Ñ) Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. BlueLM-V-3B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ˜Ğ˜-Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½ÑƒÑ Ğ¶Ğ¸Ğ·Ğ½ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Efficient MLLMs for Mobile: BlueLM-V-3B Unleashed!",
                    "desc": "This paper introduces BlueLM-V-3B, a multimodal large language model (MLLM) designed for efficient deployment on mobile devices. It addresses the challenges of limited memory and computational power by optimizing model inference through a redesigned dynamic resolution scheme and hardware-aware system optimizations. BlueLM-V-3B features a compact architecture with 2.7 billion parameters for the language model and 400 million for the vision encoder, ensuring fast processing speeds. The model achieves impressive performance metrics, outperforming larger models on benchmarks while maintaining a generation speed of 24.4 tokens per second."
                },
                "zh": {
                    "title": "é«˜æ•ˆéƒ¨ç½²å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ¡ˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBlueLM-V-3Bçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œæ—¨åœ¨é«˜æ•ˆåœ°åœ¨ç§»åŠ¨å¹³å°ä¸Šéƒ¨ç½²ã€‚è¯¥æ¨¡å‹å…·æœ‰2.7äº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹å’Œ4äº¿å‚æ•°çš„è§†è§‰ç¼–ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°å¿«é€Ÿç”Ÿæˆã€‚é€šè¿‡é‡æ–°è®¾è®¡åŠ¨æ€åˆ†è¾¨ç‡æ–¹æ¡ˆå’Œè¿›è¡Œç¡¬ä»¶ä¼˜åŒ–ï¼ŒBlueLM-V-3Båœ¨MediaTek Dimensity 9300å¤„ç†å™¨ä¸Šè¾¾åˆ°äº†æ¯ç§’24.4ä¸ªæ ‡è®°çš„ç”Ÿæˆé€Ÿåº¦ã€‚è¯¥æ¨¡å‹åœ¨OpenCompassåŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†66.1çš„æœ€é«˜å¹³å‡åˆ†ï¼Œè¶…è¶Šäº†è®¸å¤šå‚æ•°æ›´å¤§çš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11767",
            "title": "Drowning in Documents: Consequences of Scaling Reranker Inference",
            "url": "https://huggingface.co/papers/2411.11767",
            "abstract": "Rerankers, typically cross-encoders, are often used to re-score the documents retrieved by cheaper initial IR systems. This is because, though expensive, rerankers are assumed to be more effective. We challenge this assumption by measuring reranker performance for full retrieval, not just re-scoring first-stage retrieval. Our experiments reveal a surprising trend: the best existing rerankers provide diminishing returns when scoring progressively more documents and actually degrade quality beyond a certain limit. In fact, in this setting, rerankers can frequently assign high scores to documents with no lexical or semantic overlap with the query. We hope that our findings will spur future research to improve reranking.",
            "score": 0,
            "issue_id": 652,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "3fa06087787bc8d4",
            "authors": [
                "Mathew Jacob",
                "Erik Lindgren",
                "Matei Zaharia",
                "Michael Carbin",
                "Omar Khattab",
                "Andrew Drozdov"
            ],
            "affiliations": [
                "Databricks",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11767.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² (rerankers) Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ÑÑ. Ğ‘Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ³Ğ¾, Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ±ĞµĞ· Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ½Ğ°Ğ´ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Rethinking Rerankers: Diminishing Returns in Document Scoring",
                    "desc": "This paper investigates the effectiveness of rerankers, specifically cross-encoders, in the context of information retrieval (IR). Traditionally, rerankers are believed to enhance the quality of document scoring after an initial retrieval phase. However, the authors find that as more documents are scored, the performance of these rerankers diminishes and can even lead to poorer results. The study highlights that rerankers may assign high scores to irrelevant documents, suggesting a need for improved methods in reranking processes."
                },
                "zh": {
                    "title": "é‡æ’åºå™¨çš„æœ‰æ•ˆæ€§éœ€é‡æ–°å®¡è§†",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é‡æ’åºå™¨ï¼ˆé€šå¸¸æ˜¯äº¤å‰ç¼–ç å™¨ï¼‰åœ¨ä¿¡æ¯æ£€ç´¢ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬é€šè¿‡æµ‹é‡é‡æ’åºå™¨åœ¨å®Œæ•´æ£€ç´¢ä¸­çš„è¡¨ç°ï¼ŒæŒ‘æˆ˜äº†å®ƒä»¬åœ¨åˆæ­¥æ£€ç´¢åé‡æ–°è¯„åˆ†çš„å‡è®¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„é‡æ’åºå™¨åœ¨è¯„åˆ†è¶Šæ¥è¶Šå¤šçš„æ–‡æ¡£æ—¶ï¼Œæ•ˆæœé€æ¸å‡å¼±ï¼Œç”šè‡³åœ¨æŸä¸ªé™åº¦åè´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬çš„å‘ç°å¸Œæœ›èƒ½æ¿€åŠ±æœªæ¥çš„ç ”ç©¶ï¼Œä»¥æ”¹è¿›é‡æ’åºæŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07641",
            "title": "Top-$nÏƒ$: Not All Logits Are You Need",
            "url": "https://huggingface.co/papers/2411.07641",
            "abstract": "Large language models (LLMs) typically employ greedy decoding or low-temperature sampling for reasoning tasks, reflecting a perceived trade-off between diversity and accuracy. We challenge this convention by introducing top-nsigma, a novel sampling method that operates directly on pre-softmax logits by leveraging a statistical threshold. Our key insight is that logits naturally separate into a Gaussian-distributed noisy region and a distinct informative region, enabling efficient token filtering without complex probability manipulations. Unlike existing methods (e.g., top-p, min-p) that inadvertently include more noise tokens at higher temperatures, top-nsigma maintains a stable sampling space regardless of temperature scaling. We also provide a theoretical analysis of top-nsigma to better understand its behavior. The extensive experimental results across four reasoning-focused datasets demonstrate that our method not only outperforms existing sampling approaches but also surpasses greedy decoding, while maintaining consistent performance even at high temperatures.",
            "score": 0,
            "issue_id": 651,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 12",
                "zh": "11æœˆ12æ—¥"
            },
            "hash": "d3439bf0c336ac57",
            "authors": [
                "Chenxia Tang",
                "Jianchun Liu",
                "Hongli Xu",
                "Liusheng Huang"
            ],
            "affiliations": [
                "School of Computer Science and Technology, University of Science and Technology of China",
                "Suzhou Institute for Advanced Research, University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.07641.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Top-nsigma: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ top-nsigma. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑĞ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Top-nsigma Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¶Ğ°Ğ´Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Top-NSigma: Enhancing Sampling for Better Reasoning in LLMs",
                    "desc": "This paper introduces a new sampling method called top-nsigma for large language models (LLMs) that improves reasoning tasks. Unlike traditional methods that use greedy decoding or low-temperature sampling, top-nsigma filters tokens based on their pre-softmax logits, which are divided into informative and noisy regions. This approach allows for better token selection without the complications of probability adjustments, ensuring a stable sampling process across different temperature settings. Experimental results show that top-nsigma outperforms existing methods and maintains high performance even when using higher temperatures."
                },
                "zh": {
                    "title": "çªç ´ä¼ ç»Ÿï¼Œæå‡æ¨ç†æ€§èƒ½çš„top-nsigmaæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ³•top-nsigmaï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•ç›´æ¥åœ¨é¢„è½¯æœ€å¤§å€¼çš„logitsä¸Šæ“ä½œï¼Œé€šè¿‡ç»Ÿè®¡é˜ˆå€¼æ¥è¿›è¡Œæœ‰æ•ˆçš„ä»¤ç‰Œè¿‡æ»¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œlogitså¯ä»¥è‡ªç„¶åœ°åˆ†ä¸ºé«˜æ–¯åˆ†å¸ƒçš„å™ªå£°åŒºåŸŸå’Œä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸï¼Œä»è€Œé¿å…äº†å¤æ‚çš„æ¦‚ç‡æ“ä½œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œtop-nsigmaåœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„é‡‡æ ·æ–¹æ³•å’Œè´ªå©ªè§£ç ï¼Œä¸”åœ¨é«˜æ¸©åº¦ä¸‹ä»èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-18.html",
    "link_next": "2024-11-20.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "18.11",
        "en": "11/18",
        "zh": "11æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.11",
        "en": "11/20",
        "zh": "11æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç§°ä¸ºLLaVA-o1ï¼Œæ—¨åœ¨è¿›è¡Œè‡ªä¸»å¤šé˜¶æ®µæ¨ç†ã€‚ä¸é“¾å¼æ€ç»´æç¤ºä¸åŒï¼ŒLLaVA-o1ç‹¬ç«‹è¿›è¡Œæ€»ç»“ã€è§†è§‰è§£é‡Šã€é€»è¾‘æ¨ç†å’Œç»“è®ºç”Ÿæˆã€‚è¿™ç§ç»“æ„åŒ–æ–¹æ³•ä½¿å…¶åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—ç²¾åº¦æå‡ã€‚ç ”ç©¶å›¢é˜Ÿç¼–åˆ¶äº†LLaVA-o1-100kæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é˜¶æ®µçº§æŸæœç´¢æ–¹æ³•ï¼Œä»¥å®ç°æœ‰æ•ˆçš„æ¨ç†æ—¶æ‰©å±•ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLaVA-o1åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å¤šä¸ªå¤§å‹å’Œå°é—­æºæ¨¡å‹ã€‚",
        "title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç§°ä¸ºLLaVA-o1ï¼Œæ—¨åœ¨è¿›è¡Œè‡ªä¸»å¤šé˜¶æ®µæ¨ç†ã€‚ä¸é“¾å¼æ€ç»´æç¤ºä¸åŒï¼ŒLLaVA-o1ç‹¬ç«‹è¿›è¡Œæ€»ç»“ã€è§†è§‰è§£é‡Šã€é€»è¾‘æ¨ç†å’Œç»“è®ºç”Ÿæˆã€‚è¿™ç§ç»“æ„åŒ–æ–¹æ³•ä½¿å…¶åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—ç²¾åº¦æå‡ã€‚ç ”ç©¶å›¢é˜Ÿç¼–åˆ¶äº†LLaVA-o1-100kæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é˜¶æ®µçº§æŸæœç´¢æ–¹æ³•ï¼Œä»¥å®ç°æœ‰æ•ˆçš„æ¨ç†æ—¶æ‰©å±•ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLaVA-o1åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å¤šä¸ªå¤§å‹å’Œå°é—­æºæ¨¡å‹ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« gÃ¨ xÄ«n de shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ¬ng (VLM)ï¼ŒchÄ“ng wÃ©i LLaVA-o1ï¼ŒzhÇ yÇn jÃ¬n xÃ­ng zÃ¬ zhÇ” duÅ jiÄ“ duÃ n tuÃ­ lÇ. yÇ” liÃ n shÃ¬ sÄ« wÃ©i tÃ­ shÃ¬ bÃ¹ tÃ³ngï¼ŒLLaVA-o1 dÃº lÃ¬ jÃ¬n xÃ­ng zÇ’ng jiÄ›ï¼ŒshÃ¬ juÃ© jiÄ› shÃ¬ï¼ŒluÃ³ ji tuÃ­ lÇ hÃ© jiÃ© lÃ¹n shÄ“ng chÃ©ng. zhÃ¨ zhÇ’ng jiÄ“ gÃ²u huÃ  fÇ shÇ qÃ­ zÃ i tuÃ­ lÇ mÃ¬ jÄ« xÃ­ng rÃ¨n wÃ¹ shÃ ng qÇ” dÃ© xiÇn zhÃ¹ jÄ«ng dÃ¹ tÃ­ shÄ“ng. yÃ¡n jiÅ« tuÃ¡n duÃ¬ biÄn zhÃ¬ le LLaVA-o1-100k shÃ¹ jÃ¹ jÃ­ï¼ŒbÃ¬ng tÃ­ chÅ« le yÄ« zhÇ’ng tuÃ­ lÇ shÃ­ jiÄ“ duÃ n jÃ­ shÃ¹ sÅu suÇ’ fÇï¼ŒyÇ shÃ­ xiÃ n yÃ¡n jiÅ« shÃ­ kuÃ² zhÇn. jiÃ© guÇ’ xiÇn shÃ¬ï¼ŒLLaVA-o1 zÃ i duÅ mÃ³ shuÃ i tuÃ­ lÇ jÄ« zhÇ”n cÃ¨ shÃ¬ zhÅng biÇo xiÃ n chÅ« sÃ¨ï¼ŒchÄo yuÃ¨ le duÅ gÃ¨ dÃ  xÃ­ng hÃ© fÄ“ng bÃ¬ yuÃ¡n mÃ³ xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"è§†è§‰è¯­è¨€æ¨¡å‹\", \"pinyin\": \"shÃ¬juÃ© yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"visual language model\"},\n    {\"word\": \"è‡ªä¸»\", \"pinyin\": \"zÃ¬zhÇ”\", \"trans\": \"autonomous\"},\n    {\"word\": \"å¤šé˜¶æ®µ\", \"pinyin\": \"duÅ jiÄ“duÃ n\", \"trans\": \"multi-stage\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ«lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"é“¾å¼\", \"pinyin\": \"liÃ nshÃ¬\", \"trans\": \"chain-like\"},\n    {\"word\": \"æç¤º\", \"pinyin\": \"tÃ­shÃ¬\", \"trans\": \"prompt\"},\n    {\"word\": \"ç‹¬ç«‹\", \"pinyin\": \"dÃºlÃ¬\", \"trans\": \"independent\"},\n    {\"word\": \"æ€»ç»“\", \"pinyin\": \"zÇ’ngjiÃ©\", \"trans\": \"summary\"},\n    {\"word\": \"è§†è§‰è§£é‡Š\", \"pinyin\": \"shÃ¬juÃ© jiÄ›shÃ¬\", \"trans\": \"visual explanation\"},\n    {\"word\": \"é€»è¾‘æ¨ç†\", \"pinyin\": \"luÃ³ji tuÄ«lÇ\", \"trans\": \"logical reasoning\"},\n    {\"word\": \"ç»“è®ºç”Ÿæˆ\", \"pinyin\": \"jiÃ©lÃ¹n shÄ“ngchÃ©ng\", \"trans\": \"conclusion generation\"},\n    {\"word\": \"ç»“æ„åŒ–\", \"pinyin\": \"jiÃ©gÃ²uhuÃ \", \"trans\": \"structured\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄngfÇ\", \"trans\": \"method\"},\n    {\"word\": \"ç²¾åº¦\", \"pinyin\": \"jÄ«ngdÃ¹\", \"trans\": \"accuracy\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­shÄ“ng\", \"trans\": \"improvement\"},\n    {\"word\": \"ç ”ç©¶å›¢é˜Ÿ\", \"pinyin\": \"yÃ¡njiÅ« tuÃ¡nduÃ¬\", \"trans\": \"research team\"},\n    {\"word\": \"ç¼–åˆ¶\", \"pinyin\": \"biÄnzhÃ¬\", \"trans\": \"compile\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹jÃ¹jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"æ¨ç†æ—¶\", \"pinyin\": \"tuÄ«lÇ shÃ­\", \"trans\": \"during reasoning\"},\n    {\"word\": \"é˜¶æ®µçº§\", \"pinyin\": \"jiÄ“duÃ n jÃ­\", \"trans\": \"stage-level\"},\n    {\"word\": \"æŸæœç´¢\", \"pinyin\": \"shÃ¹ sÅusuÇ’\", \"trans\": \"beam search\"},\n    {\"word\": \"æ‰©å±•\", \"pinyin\": \"kuÃ²zhÇn\", \"trans\": \"expansion\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³shÃ¬\", \"trans\": \"multimodal\"},\n    {\"word\": \"åŸºå‡†æµ‹è¯•\", \"pinyin\": \"jÄ«zhÇ”n cÃ¨shÃ¬\", \"trans\": \"benchmark test\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ«sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄoyuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"å¤§å‹\", \"pinyin\": \"dÃ xÃ­ng\", \"trans\": \"large-scale\"},\n    {\"word\": \"å°é—­æº\", \"pinyin\": \"fÄ“ngbÃ¬ yuÃ¡n\", \"trans\": \"closed-source\"}\n]",
        "trans": "This article introduces a new visual language model (VLM) called LLaVA-o1, designed for autonomous multi-stage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently performs summarization, visual explanation, logical reasoning, and conclusion generation. This structured approach achieves significant accuracy improvements in reasoning-intensive tasks. The research team compiled the LLaVA-o1-100k dataset and proposed a stage-level beam search method during inference to achieve effective reasoning-time expansion. The results show that LLaVA-o1 performs exceptionally well in multimodal reasoning benchmarks, outperforming several large and closed-source models.",
        "update_ts": "2024-11-18 09:12"
    }
}