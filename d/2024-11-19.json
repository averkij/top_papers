{
    "date": {
        "ru": "19 ноября",
        "en": "November 19",
        "zh": "11月19日"
    },
    "time_utc": "2024-11-19 03:23",
    "weekday": 1,
    "issue_id": 651,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.07641",
            "title": "Top-$nσ$: Not All Logits Are You Need",
            "url": "https://huggingface.co/papers/2411.07641",
            "abstract": "Large language models (LLMs) typically employ greedy decoding or low-temperature sampling for reasoning tasks, reflecting a perceived trade-off between diversity and accuracy. We challenge this convention by introducing top-nsigma, a novel sampling method that operates directly on pre-softmax logits by leveraging a statistical threshold. Our key insight is that logits naturally separate into a Gaussian-distributed noisy region and a distinct informative region, enabling efficient token filtering without complex probability manipulations. Unlike existing methods (e.g., top-p, min-p) that inadvertently include more noise tokens at higher temperatures, top-nsigma maintains a stable sampling space regardless of temperature scaling. We also provide a theoretical analysis of top-nsigma to better understand its behavior. The extensive experimental results across four reasoning-focused datasets demonstrate that our method not only outperforms existing sampling approaches but also surpasses greedy decoding, while maintaining consistent performance even at high temperatures.",
            "score": 0,
            "issue_id": 651,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "d3439bf0c336ac57",
            "authors": [
                "Chenxia Tang",
                "Jianchun Liu",
                "Hongli Xu",
                "Liusheng Huang"
            ],
            "affiliations": [
                "School of Computer Science and Technology, University of Science and Technology of China",
                "Suzhou Institute for Advanced Research, University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.07641.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Top-nsigma: эффективное сэмплирование для улучшения рассуждений языковых моделей",
                    "desc": "В статье представлен новый метод сэмплирования для больших языковых моделей под названием top-nsigma. Этот метод работает напрямую с логитами перед софтмаксом, используя статистический порог для разделения шумовых и информативных токенов. Top-nsigma позволяет эффективно фильтровать токены без сложных манипуляций с вероятностями и сохраняет стабильное пространство сэмплирования независимо от температуры. Экспериментальные результаты показывают, что данный метод превосходит существующие подходы к сэмплированию и жадное декодирование на задачах рассуждения."
                },
                "en": {
                    "title": "Top-NSigma: Enhancing Sampling for Better Reasoning in LLMs",
                    "desc": "This paper introduces a new sampling method called top-nsigma for large language models (LLMs) that improves reasoning tasks. Unlike traditional methods that use greedy decoding or low-temperature sampling, top-nsigma filters tokens based on their pre-softmax logits, which are divided into informative and noisy regions. This approach allows for better token selection without the complications of probability adjustments, ensuring a stable sampling process across different temperature settings. Experimental results show that top-nsigma outperforms existing methods and maintains high performance even when using higher temperatures."
                },
                "zh": {
                    "title": "突破传统，提升推理性能的top-nsigma方法",
                    "desc": "本文提出了一种新的采样方法top-nsigma，旨在改善大语言模型在推理任务中的表现。该方法直接在预软最大值的logits上操作，通过统计阈值来进行有效的令牌过滤。我们的研究表明，logits可以自然地分为高斯分布的噪声区域和信息丰富的区域，从而避免了复杂的概率操作。实验结果显示，top-nsigma在多个推理数据集上超越了现有的采样方法和贪婪解码，且在高温度下仍能保持稳定的性能。"
                }
            }
        }
    ],
    "link_prev": "2024-11-18.html",
    "link_next": "2024-11-20.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "18.11",
        "en": "11/18",
        "zh": "11月18日"
    },
    "short_date_next": {
        "ru": "20.11",
        "en": "11/20",
        "zh": "11月20日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一个新的视觉语言模型（VLM），称为LLaVA-o1，旨在进行自主多阶段推理。与链式思维提示不同，LLaVA-o1独立进行总结、视觉解释、逻辑推理和结论生成。这种结构化方法使其在推理密集型任务上取得显著精度提升。研究团队编制了LLaVA-o1-100k数据集，并提出了一种推理时阶段级束搜索方法，以实现有效的推理时扩展。结果显示，LLaVA-o1在多模态推理基准测试中表现出色，超越了多个大型和封闭源模型。",
        "title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
        "pinyin": "这篇文章介绍了一个新的视觉语言模型（VLM），称为LLaVA-o1，旨在进行自主多阶段推理。与链式思维提示不同，LLaVA-o1独立进行总结、视觉解释、逻辑推理和结论生成。这种结构化方法使其在推理密集型任务上取得显著精度提升。研究团队编制了LLaVA-o1-100k数据集，并提出了一种推理时阶段级束搜索方法，以实现有效的推理时扩展。结果显示，LLaVA-o1在多模态推理基准测试中表现出色，超越了多个大型和封闭源模型。\n\nzhè piān wén zhāng jiè shào le yī gè xīn de shì jué yǔ yán mó xìng (VLM)，chēng wéi LLaVA-o1，zhǐ yǐn jìn xíng zì zhǔ duō jiē duàn tuí lǐ. yǔ liàn shì sī wéi tí shì bù tóng，LLaVA-o1 dú lì jìn xíng zǒng jiě，shì jué jiě shì，luó ji tuí lǐ hé jié lùn shēng chéng. zhè zhǒng jiē gòu huà fǎ shǐ qí zài tuí lǐ mì jī xíng rèn wù shàng qǔ dé xiǎn zhù jīng dù tí shēng. yán jiū tuán duì biān zhì le LLaVA-o1-100k shù jù jí，bìng tí chū le yī zhǒng tuí lǐ shí jiē duàn jí shù sōu suǒ fǎ，yǐ shí xiàn yán jiū shí kuò zhǎn. jié guǒ xiǎn shì，LLaVA-o1 zài duō mó shuài tuí lǐ jī zhǔn cè shì zhōng biǎo xiàn chū sè，chāo yuè le duō gè dà xíng hé fēng bì yuán mó xìng.",
        "vocab": "[\n    {\"word\": \"视觉语言模型\", \"pinyin\": \"shìjué yǔyán móxíng\", \"trans\": \"visual language model\"},\n    {\"word\": \"自主\", \"pinyin\": \"zìzhǔ\", \"trans\": \"autonomous\"},\n    {\"word\": \"多阶段\", \"pinyin\": \"duō jiēduàn\", \"trans\": \"multi-stage\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"链式\", \"pinyin\": \"liànshì\", \"trans\": \"chain-like\"},\n    {\"word\": \"提示\", \"pinyin\": \"tíshì\", \"trans\": \"prompt\"},\n    {\"word\": \"独立\", \"pinyin\": \"dúlì\", \"trans\": \"independent\"},\n    {\"word\": \"总结\", \"pinyin\": \"zǒngjié\", \"trans\": \"summary\"},\n    {\"word\": \"视觉解释\", \"pinyin\": \"shìjué jiěshì\", \"trans\": \"visual explanation\"},\n    {\"word\": \"逻辑推理\", \"pinyin\": \"luóji tuīlǐ\", \"trans\": \"logical reasoning\"},\n    {\"word\": \"结论生成\", \"pinyin\": \"jiélùn shēngchéng\", \"trans\": \"conclusion generation\"},\n    {\"word\": \"结构化\", \"pinyin\": \"jiégòuhuà\", \"trans\": \"structured\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāngfǎ\", \"trans\": \"method\"},\n    {\"word\": \"精度\", \"pinyin\": \"jīngdù\", \"trans\": \"accuracy\"},\n    {\"word\": \"提升\", \"pinyin\": \"tíshēng\", \"trans\": \"improvement\"},\n    {\"word\": \"研究团队\", \"pinyin\": \"yánjiū tuánduì\", \"trans\": \"research team\"},\n    {\"word\": \"编制\", \"pinyin\": \"biānzhì\", \"trans\": \"compile\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"},\n    {\"word\": \"推理时\", \"pinyin\": \"tuīlǐ shí\", \"trans\": \"during reasoning\"},\n    {\"word\": \"阶段级\", \"pinyin\": \"jiēduàn jí\", \"trans\": \"stage-level\"},\n    {\"word\": \"束搜索\", \"pinyin\": \"shù sōusuǒ\", \"trans\": \"beam search\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuòzhǎn\", \"trans\": \"expansion\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō móshì\", \"trans\": \"multimodal\"},\n    {\"word\": \"基准测试\", \"pinyin\": \"jīzhǔn cèshì\", \"trans\": \"benchmark test\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎoxiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chūsè\", \"trans\": \"outstanding\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāoyuè\", \"trans\": \"surpass\"},\n    {\"word\": \"大型\", \"pinyin\": \"dàxíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"封闭源\", \"pinyin\": \"fēngbì yuán\", \"trans\": \"closed-source\"}\n]",
        "trans": "This article introduces a new visual language model (VLM) called LLaVA-o1, designed for autonomous multi-stage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently performs summarization, visual explanation, logical reasoning, and conclusion generation. This structured approach achieves significant accuracy improvements in reasoning-intensive tasks. The research team compiled the LLaVA-o1-100k dataset and proposed a stage-level beam search method during inference to achieve effective reasoning-time expansion. The results show that LLaVA-o1 performs exceptionally well in multimodal reasoning benchmarks, outperforming several large and closed-source models.",
        "update_ts": "2024-11-18 09:12"
    }
}