
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. August 13.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">13 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-12.html">â¬…ï¸ <span id="prev-date">12.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-08-14.html">â¡ï¸ <span id="next-date">14.08</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-08.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '13 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 13', 'zh': '8æœˆ13æ—¥'};
        let feedDateNext = {'ru': '14.08', 'en': '08/14', 'zh': '8æœˆ14æ—¥'};
        let feedDatePrev = {'ru': '12.08', 'en': '08/12', 'zh': '8æœˆ12æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.05748', 'title': 'WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent', 'url': 'https://huggingface.co/papers/2508.05748', 'abstract': 'WebWatcher, a multimodal agent with enhanced visual-language reasoning, outperforms existing agents in complex visual and textual information retrieval tasks using synthetic trajectories and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Web agents such as Deep Research have demonstrated superhuman cognitive abilities, capable of solving highly challenging information-seeking problems. However, most research remains primarily text-centric, overlooking visual information in the real world. This makes multimodal Deep Research highly challenging, as such agents require much stronger reasoning abilities in perception, logic, knowledge, and the use of more sophisticated tools compared to text-based agents. To address this limitation, we introduce WebWatcher, a multi-modal Agent for Deep Research equipped with enhanced visual-language reasoning capabilities. It leverages high-quality synthetic multimodal trajectories for efficient cold start training, utilizes various tools for deep reasoning, and further enhances generalization through reinforcement learning. To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark with BrowseComp-style that requires complex information retrieval involving both visual and textual information. Experimental results show that WebWatcher significantly outperforms proprietary baseline, RAG workflow and open-source agents in four challenging VQA benchmarks, which paves the way for solving complex multimodal information-seeking tasks.', 'score': 62, 'issue_id': 5318, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': '937594202a5b31b7', 'authors': ['Xinyu Geng', 'Peng Xia', 'Zhen Zhang', 'Xinyu Wang', 'Qiuchen Wang', 'Ruixue Ding', 'Chenxi Wang', 'Jialong Wu', 'Yida Zhao', 'Kuan Li', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.05748.jpg', 'data': {'categories': ['#reasoning', '#synthetic', '#multimodal', '#agents', '#rl', '#benchmark', '#open_source'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'WebWatcher: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'WebWatcher - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. WebWatcher Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ€Ñ‚Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº BrowseComp-VL, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'WebWatcher: Revolutionizing Multimodal Information Retrieval', 'desc': 'WebWatcher is a multimodal agent designed to improve the retrieval of complex visual and textual information. It utilizes synthetic trajectories and reinforcement learning to enhance its visual-language reasoning capabilities. Unlike traditional text-centric agents, WebWatcher effectively integrates visual data, allowing for better reasoning and problem-solving. The introduction of the BrowseComp-VL benchmark further validates its superior performance in multimodal tasks compared to existing agents.'}, 'zh': {'title': 'WebWatcherï¼šè¶…è¶Šæ–‡æœ¬çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“', 'desc': 'WebWatcheræ˜¯ä¸€ç§å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼Œå…·å¤‡å¢å¼ºçš„è§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­è¶…è¶Šç°æœ‰æ™ºèƒ½ä½“ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨é«˜è´¨é‡çš„åˆæˆå¤šæ¨¡æ€è½¨è¿¹è¿›è¡Œé«˜æ•ˆçš„å†·å¯åŠ¨è®­ç»ƒï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æå‡å…¶æ³›åŒ–èƒ½åŠ›ã€‚WebWatcheråœ¨å¤šä¸ªè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚ä¸ºè¯„ä¼°å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„èƒ½åŠ›ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†BrowseComp-VLåŸºå‡†ï¼Œä¸“æ³¨äºè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„å¤æ‚æ£€ç´¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.08086', 'title': 'Matrix-3D: Omnidirectional Explorable 3D World Generation', 'url': 'https://huggingface.co/papers/2508.08086', 'abstract': 'Matrix-3D generates wide-coverage 3D worlds from single images or text using panoramic video diffusion and reconstruction models.  \t\t\t\t\tAI-generated summary \t\t\t\t Explorable 3D world generation from a single image or text prompt forms a cornerstone of spatial intelligence. Recent works utilize video model to achieve wide-scope and generalizable 3D world generation. However, existing approaches often suffer from a limited scope in the generated scenes. In this work, we propose Matrix-3D, a framework that utilize panoramic representation for wide-coverage omnidirectional explorable 3D world generation that combines conditional video generation and panoramic 3D reconstruction. We first train a trajectory-guided panoramic video diffusion model that employs scene mesh renders as condition, to enable high-quality and geometrically consistent scene video generation. To lift the panorama scene video to 3D world, we propose two separate methods: (1) a feed-forward large panorama reconstruction model for rapid 3D scene reconstruction and (2) an optimization-based pipeline for accurate and detailed 3D scene reconstruction. To facilitate effective training, we also introduce the Matrix-Pano dataset, the first large-scale synthetic collection comprising 116K high-quality static panoramic video sequences with depth and trajectory annotations. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance in panoramic video generation and 3D world generation. See more in https://matrix-3d.github.io.', 'score': 37, 'issue_id': 5317, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 11', 'zh': '8æœˆ11æ—¥'}, 'hash': '70a9a4327de06f04', 'authors': ['Zhongqi Yang', 'Wenhang Ge', 'Yuqi Li', 'Jiaqi Chen', 'Haoyuan Li', 'Mengyin An', 'Fei Kang', 'Hua Xue', 'Baixin Xu', 'Yuyang Yin', 'Eric Li', 'Yang Liu', 'Yikai Wang', 'Hao-Xiang Guo', 'Yahui Zhou'], 'affiliations': ['Hong Kong University of Science and Technology (Guangzhou)', 'Institute of Computing Technology, Chinese Academy of Sciences', 'School of Artificial Intelligence, Beijing Normal University', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.08086.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#3d', '#synthetic'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Matrix-3D - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D: Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Matrix-Pano Ğ¸Ğ· 116 Ñ‚Ñ‹ÑÑÑ‡ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Transforming Images into Immersive 3D Worlds', 'desc': 'Matrix-3D is a novel framework designed to create expansive 3D environments from a single image or text input by leveraging advanced panoramic video diffusion and reconstruction techniques. It addresses the limitations of previous methods by employing a trajectory-guided video diffusion model that generates high-quality scene videos, ensuring geometric consistency. The framework includes two distinct approaches for converting panoramic videos into 3D worlds: a fast feed-forward model for quick reconstructions and an optimization-based method for detailed accuracy. Additionally, the introduction of the Matrix-Pano dataset, which contains a large collection of panoramic video sequences with depth and trajectory data, supports effective training and enhances the overall performance of the system.'}, 'zh': {'title': 'ä»å•å›¾åƒç”Ÿæˆå…¨æ™¯ 3D ä¸–ç•Œçš„åˆ›æ–°æ¡†æ¶', 'desc': 'Matrix-3D æ˜¯ä¸€ä¸ªç”Ÿæˆå¹¿æ³›è¦†ç›–çš„ 3D ä¸–ç•Œçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒæˆ–æ–‡æœ¬æç¤ºä¸­ç”Ÿæˆå¯æ¢ç´¢çš„ 3D ä¸–ç•Œã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ¡ä»¶è§†é¢‘ç”Ÿæˆå’Œå…¨æ™¯ 3D é‡å»ºï¼Œåˆ©ç”¨å…¨æ™¯è¡¨ç¤ºæ¥å®ç°å…¨æ–¹ä½çš„ 3D ä¸–ç•Œç”Ÿæˆã€‚æˆ‘ä»¬é¦–å…ˆè®­ç»ƒäº†ä¸€ä¸ªåŸºäºè½¨è¿¹å¼•å¯¼çš„å…¨æ™¯è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥åœºæ™¯ç½‘æ ¼æ¸²æŸ“ä½œä¸ºæ¡ä»¶ï¼Œä»è€Œå®ç°é«˜è´¨é‡å’Œå‡ ä½•ä¸€è‡´çš„åœºæ™¯è§†é¢‘ç”Ÿæˆã€‚ä¸ºäº†å°†å…¨æ™¯åœºæ™¯è§†é¢‘æå‡ä¸º 3D ä¸–ç•Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼šå¿«é€Ÿçš„å‰é¦ˆå…¨æ™¯é‡å»ºæ¨¡å‹å’ŒåŸºäºä¼˜åŒ–çš„ç²¾ç¡® 3D åœºæ™¯é‡å»ºæµç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.07976', 'title': 'Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL', 'url': 'https://huggingface.co/papers/2508.07976', 'abstract': 'ASearcher is an open-source project that uses scalable asynchronous RL training to enhance search agents, achieving high performance on QA tasks with long-horizon search capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.', 'score': 23, 'issue_id': 5318, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 11', 'zh': '8æœˆ11æ—¥'}, 'hash': '22a9c1c0433f2764', 'authors': ['Jiaxuan Gao', 'Wei Fu', 'Minyang Xie', 'Shusheng Xu', 'Chuyi He', 'Zhiyu Mei', 'Banghua Zhu', 'Yi Wu'], 'affiliations': ['Ant Research, RL Lab', 'IIIS, Tsinghua University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.07976.jpg', 'data': {'categories': ['#dataset', '#training', '#agents', '#rl', '#long_context', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'ASearcher: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ASearcher - ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ASearcher Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… xBench Ğ¸ GAIA.'}, 'en': {'title': 'Unlocking Expert-Level Search Intelligence with ASearcher', 'desc': 'ASearcher is an innovative open-source project that enhances search agents through scalable asynchronous reinforcement learning (RL) training. It addresses the limitations of existing methods by enabling long-horizon search capabilities, allowing agents to learn complex strategies over extended interactions. The project introduces a prompt-based large language model (LLM) agent that autonomously generates high-quality question-answer pairs, significantly improving the dataset for training. As a result, ASearcher achieves impressive performance metrics, outperforming previous open-source agents in various QA tasks.'}, 'zh': {'title': 'ASearcherï¼šæå‡æœç´¢æ™ºèƒ½çš„å¼€æºå¼ºåŒ–å­¦ä¹ é¡¹ç›®', 'desc': 'ASearcheræ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œåˆ©ç”¨å¯æ‰©å±•çš„å¼‚æ­¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ¥å¢å¼ºæœç´¢ä»£ç†çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æ—¶é—´æœç´¢çš„é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥é¡¹ç›®çš„å…³é”®è´¡çŒ®åŒ…æ‹¬å¯æ‰©å±•çš„å®Œå…¨å¼‚æ­¥RLè®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé«˜è®­ç»ƒæ•ˆç‡çš„åŒæ—¶è¿›è¡Œé•¿æ—¶é—´æœç´¢ã€‚é€šè¿‡åŸºäºæç¤ºçš„LLMä»£ç†ï¼ŒASearcherèƒ½å¤Ÿè‡ªä¸»åˆæˆé«˜è´¨é‡çš„é—®ç­”ï¼Œåˆ›å»ºå¤§è§„æ¨¡çš„é—®ç­”æ•°æ®é›†ã€‚ç»è¿‡RLè®­ç»ƒï¼ŒASearcherçš„ä»£ç†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚æœç´¢ä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.09138', 'title': 'Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language\n  Models', 'url': 'https://huggingface.co/papers/2508.09138', 'abstract': 'Two methods, Temporal Self-Consistency Voting and Temporal Consistency Reinforcement, improve diffusion large language models by leveraging temporal consistency in intermediate predictions.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them.', 'score': 19, 'issue_id': 5317, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 12', 'zh': '8æœˆ12æ—¥'}, 'hash': 'fc4661402e36332d', 'authors': ['Wen Wang', 'Bozhen Fang', 'Chenchen Jing', 'Yongliang Shen', 'Yangyi Shen', 'Qiuyu Wang', 'Hao Ouyang', 'Hao Chen', 'Chunhua Shen'], 'affiliations': ['Ant Group', 'Stanford University', 'Zhejiang University', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.09138.jpg', 'data': {'categories': ['#training', '#optimization', '#diffusion', '#benchmark', '#rl'], 'emoji': 'â³', 'ru': {'title': 'Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dLLM): Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑÑ†Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¹, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ğ¾ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ² ÑĞµÑ€ĞµĞ´Ğ¸Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… ÑˆĞ°Ğ³Ğ°Ñ…. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GSM8K, MATH500, SVAMP Ğ¸ Countdown.'}, 'en': {'title': 'Harnessing Temporal Consistency for Better Language Model Outputs', 'desc': 'This paper presents two innovative methods to enhance diffusion large language models (dLLMs) by focusing on the temporal consistency of their predictions. The authors identify a problem called temporal oscillation, where valuable intermediate outputs are lost during the final denoising process. To combat this, they propose Temporal Self-Consistency Voting, which aggregates predictions from various steps to find the most reliable output, and Temporal Consistency Reinforcement, which uses a reward based on Temporal Semantic Entropy to promote stable outputs. Their experiments show significant improvements in performance across several benchmarks, highlighting the importance of leveraging temporal dynamics in language model generation.'}, 'zh': {'title': 'åˆ©ç”¨æ—¶é—´ä¸€è‡´æ€§æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼Œæ—¶é—´è‡ªä¸€è‡´æ€§æŠ•ç¥¨å’Œæ—¶é—´ä¸€è‡´æ€§å¼ºåŒ–ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨ä¸­é—´é¢„æµ‹çš„æ—¶é—´ä¸€è‡´æ€§æ¥æ”¹è¿›æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å»å™ªçš„è¿‡ç¨‹ä¸­ï¼Œæ­£ç¡®ç­”æ¡ˆå¾€å¾€åœ¨ä¸­é—´æ­¥éª¤ä¸­å‡ºç°ï¼Œä½†åœ¨åç»­æ­¥éª¤ä¸­è¢«è¦†ç›–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºçš„æ—¶é—´è‡ªä¸€è‡´æ€§æŠ•ç¥¨æ–¹æ³•åœ¨æµ‹è¯•æ—¶èšåˆå¤šä¸ªå»å™ªæ­¥éª¤çš„é¢„æµ‹ï¼Œä»¥é€‰æ‹©æœ€ä¸€è‡´çš„è¾“å‡ºï¼›è€Œæ—¶é—´ä¸€è‡´æ€§å¼ºåŒ–åˆ™ä½¿ç”¨æ—¶é—´è¯­ä¹‰ç†µä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œé¼“åŠ±ç”Ÿæˆç¨³å®šçš„ç»“æœã€‚å®éªŒè¯æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.07409', 'title': 'CharacterShot: Controllable and Consistent 4D Character Animation', 'url': 'https://huggingface.co/papers/2508.07409', 'abstract': 'CharacterShot is a 4D character animation framework that uses a DiT-based model and dual-attention module to generate consistent 3D animations from a single image and 2D pose sequence.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we propose CharacterShot, a controllable and consistent 4D character animation framework that enables any individual designer to create dynamic 3D characters (i.e., 4D character animation) from a single reference character image and a 2D pose sequence. We begin by pretraining a powerful 2D character animation model based on a cutting-edge DiT-based image-to-video model, which allows for any 2D pose sequnce as controllable signal. We then lift the animation model from 2D to 3D through introducing dual-attention module together with camera prior to generate multi-view videos with spatial-temporal and spatial-view consistency. Finally, we employ a novel neighbor-constrained 4D gaussian splatting optimization on these multi-view videos, resulting in continuous and stable 4D character representations. Moreover, to improve character-centric performance, we construct a large-scale dataset Character4D, containing 13,115 unique characters with diverse appearances and motions, rendered from multiple viewpoints. Extensive experiments on our newly constructed benchmark, CharacterBench, demonstrate that our approach outperforms current state-of-the-art methods. Code, models, and datasets will be publicly available at https://github.com/Jeoyal/CharacterShot.', 'score': 19, 'issue_id': 5320, 'pub_date': '2025-08-10', 'pub_date_card': {'ru': '10 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 10', 'zh': '8æœˆ10æ—¥'}, 'hash': '9990661aef1ea53c', 'authors': ['Junyao Gao', 'Jiaxing Li', 'Wenran Liu', 'Yanhong Zeng', 'Fei Shen', 'Kai Chen', 'Yanan Sun', 'Cairong Zhao'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Shanghai AI Lab', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2508.07409.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#3d', '#optimization', '#open_source'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞĞ¶Ğ¸Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ² 3D Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'CharacterShot - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ 4D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ DiT Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 3D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 2D-Ğ¿Ğ¾Ğ·. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 2D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 2D Ğ² 3D Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Character4D Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CharacterBench.'}, 'en': {'title': 'Transforming 2D Images into Dynamic 4D Animations!', 'desc': 'CharacterShot is a novel framework for creating 4D character animations from a single image and a sequence of 2D poses. It utilizes a DiT-based model to pretrain a 2D animation model, allowing designers to control the animation process effectively. The framework incorporates a dual-attention module and camera prior to enhance the transition from 2D to 3D, ensuring consistency in both spatial and temporal dimensions. Additionally, it introduces a neighbor-constrained 4D Gaussian splatting optimization to produce stable and continuous character representations, supported by a large dataset of diverse characters.'}, 'zh': {'title': 'ä»å•å›¾åƒç”ŸæˆåŠ¨æ€3Dè§’è‰²åŠ¨ç”»çš„åˆ›æ–°æ¡†æ¶', 'desc': 'CharacterShotæ˜¯ä¸€ä¸ªåŸºäºDiTæ¨¡å‹å’ŒåŒé‡æ³¨æ„åŠ›æ¨¡å—çš„4Dè§’è‰²åŠ¨ç”»æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•ä¸€å›¾åƒå’Œ2Då§¿åŠ¿åºåˆ—ç”Ÿæˆä¸€è‡´çš„3DåŠ¨ç”»ã€‚è¯¥æ¡†æ¶å…è®¸è®¾è®¡å¸ˆæ§åˆ¶åŠ¨ç”»è¿‡ç¨‹ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„2Dè§’è‰²åŠ¨ç”»æ¨¡å‹æ¥å¤„ç†ä»»æ„2Då§¿åŠ¿åºåˆ—ã€‚é€šè¿‡å¼•å…¥åŒé‡æ³¨æ„åŠ›æ¨¡å—å’Œç›¸æœºå…ˆéªŒï¼Œæ¨¡å‹å°†åŠ¨ç”»ä»2Dæå‡åˆ°3Dï¼Œç”Ÿæˆå…·æœ‰æ—¶ç©ºå’Œè§†è§’ä¸€è‡´æ€§çš„å¤šè§†è§’è§†é¢‘ã€‚æœ€åï¼Œé‡‡ç”¨æ–°é¢–çš„é‚»åŸŸçº¦æŸ4Dé«˜æ–¯ç‚¹äº‘ä¼˜åŒ–ï¼Œç¡®ä¿ç”Ÿæˆçš„4Dè§’è‰²è¡¨ç°è¿ç»­ç¨³å®šã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.08088', 'title': 'HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating\n  Local and Web Searches', 'url': 'https://huggingface.co/papers/2508.08088', 'abstract': 'HierSearch, a hierarchical agentic deep search framework using hierarchical RL, improves performance in multi-source retrieval tasks by coordinating local and Web search agents and refining knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, large reasoning models have demonstrated strong mathematical and coding abilities, and deep search leverages their reasoning capabilities in challenging information retrieval tasks. Existing deep search works are generally limited to a single knowledge source, either local or the Web. However, enterprises often require private deep search systems that can leverage search tools over both local and the Web corpus. Simply training an agent equipped with multiple search tools using flat reinforcement learning (RL) is a straightforward idea, but it has problems such as low training data efficiency and poor mastery of complex tools. To address the above issue, we propose a hierarchical agentic deep search framework, HierSearch, trained with hierarchical RL. At the low level, a local deep search agent and a Web deep search agent are trained to retrieve evidence from their corresponding domains. At the high level, a planner agent coordinates low-level agents and provides the final answer. Moreover, to prevent direct answer copying and error propagation, we design a knowledge refiner that filters out hallucinations and irrelevant evidence returned by low-level agents. Experiments show that HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains.', 'score': 18, 'issue_id': 5317, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 11', 'zh': '8æœˆ11æ—¥'}, 'hash': '7e2ae00381bb3360', 'authors': ['Jiejun Tan', 'Zhicheng Dou', 'Yan Yu', 'Jiehan Cheng', 'Qiang Ju', 'Jian Xie', 'Ji-Rong Wen'], 'affiliations': ['Baichuan Intelligent Technology', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.08088.jpg', 'data': {'categories': ['#hallucinations', '#rag', '#reasoning', '#agents', '#benchmark', '#rl', '#healthcare'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'HierSearch - ÑÑ‚Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ½Ğ° ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. HierSearch Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ»Ğ¾ÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑˆĞµÑÑ‚Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¹, Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'HierSearch: Elevating Multi-Source Retrieval with Hierarchical RL', 'desc': 'HierSearch is a hierarchical framework that enhances multi-source information retrieval by using hierarchical reinforcement learning (RL). It consists of local and Web search agents that work together to gather relevant information from different sources. A planner agent oversees these agents to ensure accurate and coherent answers, while a knowledge refiner filters out incorrect or irrelevant data. This approach significantly improves performance over traditional flat RL methods and outperforms existing deep search systems across various domains.'}, 'zh': {'title': 'å±‚æ¬¡åŒ–æ™ºèƒ½æ·±åº¦æœç´¢ï¼Œæå‡å¤šæºæ£€ç´¢æ€§èƒ½', 'desc': 'HierSearchæ˜¯ä¸€ç§å±‚æ¬¡åŒ–çš„æ™ºèƒ½æ·±åº¦æœç´¢æ¡†æ¶ï¼Œåˆ©ç”¨å±‚æ¬¡åŒ–å¼ºåŒ–å­¦ä¹ æ¥æå‡å¤šæºæ£€ç´¢ä»»åŠ¡çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡åè°ƒæœ¬åœ°æœç´¢ä»£ç†å’Œç½‘ç»œæœç´¢ä»£ç†ï¼Œä¼˜åŒ–çŸ¥è¯†è·å–è¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„å•ä¸€çŸ¥è¯†æºæ·±åº¦æœç´¢ä¸åŒï¼ŒHierSearchèƒ½å¤ŸåŒæ—¶åˆ©ç”¨æœ¬åœ°å’Œç½‘ç»œæ•°æ®ï¼Œæ»¡è¶³ä¼ä¸šå¯¹ç§æœ‰æ·±åº¦æœç´¢ç³»ç»Ÿçš„éœ€æ±‚ã€‚æ­¤å¤–ï¼ŒHierSearchè¿˜è®¾è®¡äº†çŸ¥è¯†ç²¾ç‚¼å™¨ï¼Œä»¥è¿‡æ»¤ä½çº§ä»£ç†è¿”å›çš„é”™è¯¯ä¿¡æ¯å’Œæ— å…³è¯æ®ï¼Œä»è€Œæé«˜æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.05615', 'title': 'Test-Time Reinforcement Learning for GUI Grounding via Region\n  Consistency', 'url': 'https://huggingface.co/papers/2508.05615', 'abstract': 'GUI-RC and GUI-RCPO enhance GUI grounding accuracy by leveraging spatial consistency and reinforcement learning without additional training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), which transforms these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: GUI-RC boosts Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO further improves it to 85.14% through self-supervised optimization. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more robust and data-efficient GUI agents.', 'score': 10, 'issue_id': 5317, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': 'b571750771c4ba60', 'authors': ['Yong Du', 'Yuchen Yan', 'Fei Tang', 'Zhengxi Lu', 'Chang Zong', 'Weiming Lu', 'Shengpei Jiang', 'Yongliang Shen'], 'affiliations': ['Central South University', 'SF Technology', 'Zhejiang University', 'Zhejiang University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.05615.jpg', 'data': {'categories': ['#inference', '#optimization', '#agents', '#rlhf', '#rl'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ GUI Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ GUI-RC Ğ¸ GUI-RCPO Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. GUI-RC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. GUI-RCPO Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ScreenSpot Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing GUI Grounding with Spatial Consistency and Reinforcement Learning', 'desc': 'This paper presents GUI-RC and GUI-RCPO, two innovative methods that improve the accuracy of GUI grounding by utilizing spatial consistency and reinforcement learning without needing extra training data. GUI grounding involves translating natural language commands into specific screen coordinates, which is crucial for autonomous GUI agents. The authors propose a technique called GUI-RC that uses spatial voting grids to identify areas of agreement among multiple predictions, enhancing localization accuracy. Additionally, GUI-RCPO applies reinforcement learning to refine predictions based on how well they align with the consensus, leading to significant performance improvements on benchmark tests.'}, 'zh': {'title': 'æå‡GUIå®šä½å‡†ç¡®æ€§çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†GUI-RCå’ŒGUI-RCPOä¸¤ç§æ–¹æ³•ï¼Œä»¥æé«˜å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å®šä½çš„å‡†ç¡®æ€§ã€‚è¿™äº›æ–¹æ³•åˆ©ç”¨ç©ºé—´ä¸€è‡´æ€§å’Œå¼ºåŒ–å­¦ä¹ ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®ã€‚GUI-RCé€šè¿‡æ„å»ºç©ºé—´æŠ•ç¥¨ç½‘æ ¼ï¼Œä»å¤šä¸ªé¢„æµ‹ä¸­è¯†åˆ«å…±è¯†åŒºåŸŸï¼Œä»è€Œåœ¨æµ‹è¯•æ—¶æé«˜å‡†ç¡®æ€§ã€‚GUI-RCPOåˆ™å°†ä¸€è‡´æ€§æ¨¡å¼è½¬åŒ–ä¸ºå¥–åŠ±ï¼Œå…è®¸æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹æœªæ ‡è®°æ•°æ®è¿›è¡Œè‡ªæˆ‘ä¼˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.05399', 'title': 'UNCAGE: Contrastive Attention Guidance for Masked Generative\n  Transformers in Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2508.05399', 'abstract': 'UNCAGE, a training-free method using contrastive attention guidance, enhances compositional fidelity in text-to-image generation by prioritizing the unmasking of object-representing tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at https://github.com/furiosa-ai/uncage.', 'score': 9, 'issue_id': 5317, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'}, 'hash': '2e4f45b5a135f498', 'authors': ['Wonjun Kang', 'Byeongkeun Ahn', 'Minjae Lee', 'Kevin Galim', 'Seunghyuk Oh', 'Hyung Il Koo', 'Nam Ik Cho'], 'affiliations': ['Ajou University', 'FuriosaAI', 'Independent Researcher', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05399.jpg', 'data': {'categories': ['#training', '#optimization', '#diffusion', '#benchmark', '#cv'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'UNCAGE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğº Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ½ĞµĞµ Ğ½Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. UNCAGE Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Enhancing Text-to-Image Generation with UNCAGE', 'desc': 'The paper introduces UNCAGE, a novel method that enhances the quality of text-to-image (T2I) generation without requiring additional training. It utilizes contrastive attention guidance to focus on unmasking tokens that represent distinct objects, improving compositional fidelity in generated images. This approach addresses the limitations of existing models, particularly in accurately binding attributes and achieving text-image alignment. The results show that UNCAGE outperforms previous methods in both quantitative and qualitative assessments while maintaining low computational overhead.'}, 'zh': {'title': 'UNCAGEï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç»„åˆä¿çœŸåº¦', 'desc': 'UNCAGEæ˜¯ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨å¯¹æ¯”æ³¨æ„åŠ›å¼•å¯¼æ¥å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ç»„åˆä¿çœŸåº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜å…ˆè§£ç æ¸…æ™°è¡¨ç¤ºå•ä¸ªå¯¹è±¡çš„æ ‡è®°ï¼Œæ¥æ”¹å–„æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½ã€‚å°½ç®¡ç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨ç»„åˆç”Ÿæˆæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼ŒUNCAGEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæå‡äº†ç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨æ¨ç†æ—¶å‡ ä¹æ²¡æœ‰é¢å¤–å¼€é”€ï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.09062', 'title': 'VertexRegen: Mesh Generation with Continuous Level of Detail', 'url': 'https://huggingface.co/papers/2508.09062', 'abstract': 'VertexRegen generates meshes with continuous detail by reversing edge collapse through a generative model, offering anytime generation and flexibility in detail levels.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce VertexRegen, a novel mesh generation framework that enables generation at a continuous level of detail. Existing autoregressive methods generate meshes in a partial-to-complete manner and thus intermediate steps of generation represent incomplete structures. VertexRegen takes inspiration from progressive meshes and reformulates the process as the reversal of edge collapse, i.e. vertex split, learned through a generative model. Experimental results demonstrate that VertexRegen produces meshes of comparable quality to state-of-the-art methods while uniquely offering anytime generation with the flexibility to halt at any step to yield valid meshes with varying levels of detail.', 'score': 8, 'issue_id': 5320, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 12', 'zh': '8æœˆ12æ—¥'}, 'hash': '1881627a4d7d9e0c', 'authors': ['Xiang Zhang', 'Yawar Siddiqui', 'Armen Avetisyan', 'Chris Xie', 'Jakob Engel', 'Henry Howard-Jenkins'], 'affiliations': ['Meta Reality Labs Research', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2508.09062.jpg', 'data': {'categories': ['#games', '#optimization', '#3d'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-ÑĞµÑ‚Ğ¾Ğº Ñ Ğ»ÑĞ±Ñ‹Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'VertexRegen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑÑ…Ğ»Ğ¾Ğ¿Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ±ĞµÑ€, Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², VertexRegen Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµÑ‚ĞºĞ¸ Ğ½Ğ° Ğ»ÑĞ±Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'VertexRegen: Anytime Mesh Generation with Continuous Detail', 'desc': 'VertexRegen is a new framework for generating 3D meshes that allows for continuous detail levels. Unlike traditional methods that build meshes from incomplete structures, VertexRegen reverses the edge collapse process, effectively splitting vertices to create detailed meshes. This approach is guided by a generative model, which learns how to produce high-quality meshes. The framework also supports anytime generation, meaning users can stop the process at any point to obtain valid meshes with different levels of detail.'}, 'zh': {'title': 'VertexRegenï¼šçµæ´»çš„è¿ç»­ç»†èŠ‚ç½‘æ ¼ç”Ÿæˆ', 'desc': 'VertexRegenæ˜¯ä¸€ç§æ–°é¢–çš„ç½‘æ ¼ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿä»¥è¿ç»­çš„ç»†èŠ‚çº§åˆ«ç”Ÿæˆç½‘æ ¼ã€‚ä¸ç°æœ‰çš„è‡ªå›å½’æ–¹æ³•ä¸åŒï¼ŒVertexRegené€šè¿‡é€†å‘è¾¹ç¼˜åˆå¹¶çš„æ–¹å¼è¿›è¡Œç”Ÿæˆï¼Œå³é€šè¿‡é¡¶ç‚¹åˆ†è£‚æ¥å®ç°ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†æ¸è¿›å¼ç½‘æ ¼çš„æ€æƒ³ï¼Œå¹¶é€šè¿‡ç”Ÿæˆæ¨¡å‹è¿›è¡Œå­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVertexRegenç”Ÿæˆçš„ç½‘æ ¼è´¨é‡ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶æä¾›äº†éšæ—¶ç”Ÿæˆçš„çµæ´»æ€§ï¼Œå¯ä»¥åœ¨ä»»æ„æ­¥éª¤åœæ­¢ä»¥è·å¾—ä¸åŒç»†èŠ‚çº§åˆ«çš„æœ‰æ•ˆç½‘æ ¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.08665', 'title': 'Aryabhata: An exam-focused language model for JEE Math', 'url': 'https://huggingface.co/papers/2508.08665', 'abstract': 'Aryabhata 1.0, a compact math reasoning model, outperforms existing models on educational exams and benchmarks by using supervised fine-tuning, reinforcement learning with verifiable rewards, and novel exploration strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Aryabhata 1.0, a compact 7B parameter math reasoning model optimized for the Indian academic exam, the Joint Entrance Examination (JEE). Despite rapid progress in large language models (LLMs), current models often remain unsuitable for educational use. Aryabhata 1.0 is built by merging strong open-weight reasoning models, followed by supervised fine-tuning (SFT) with curriculum learning on verified chain-of-thought (CoT) traces curated through best-of-n rejection sampling. To further boost performance, we apply reinforcement learning with verifiable rewards (RLVR) using A2C objective with group-relative advantage estimation alongwith novel exploration strategies such as Adaptive Group Resizing and Temperature Scaling. Evaluated on both in-distribution (JEE Main 2025) and out-of-distribution (MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy and efficiency, while offering pedagogically useful step-by-step reasoning. We release Aryabhata as a foundation model to advance exam-centric, open-source small language models. This marks our first open release for community feedback (https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0{Aryabhata 1.0 on Hugging Face}); PW is actively training future models to further improve learning outcomes for students.', 'score': 7, 'issue_id': 5323, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 12', 'zh': '8æœˆ12æ—¥'}, 'hash': 'c9693974b3ce4cf2', 'authors': ['Ritvik Rastogi', 'Sachin Dharashivkar', 'Sandeep Varma'], 'affiliations': ['AthenaAgent', 'PhysicsWallah'], 'pdf_title_img': 'assets/pdf/title_img/2508.08665.jpg', 'data': {'categories': ['#small_models', '#dataset', '#reasoning', '#training', '#open_source', '#rl', '#math'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Aryabhata 1.0 - ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ²ÑÑ‚ÑƒĞ¿Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ° JEE Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. Aryabhata 1.0 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Math Reasoning for Education with Aryabhata 1.0', 'desc': 'Aryabhata 1.0 is a compact math reasoning model designed specifically for the Joint Entrance Examination (JEE) in India. It utilizes supervised fine-tuning and reinforcement learning with verifiable rewards to enhance its performance on educational tasks. The model incorporates innovative exploration strategies to improve its reasoning capabilities, allowing it to provide detailed step-by-step solutions. Evaluated against various benchmarks, Aryabhata 1.0 demonstrates superior accuracy and efficiency compared to existing models, making it a valuable tool for educational purposes.'}, 'zh': {'title': 'Aryabhata 1.0ï¼šæ•™è‚²è€ƒè¯•çš„æ•°å­¦æ¨ç†æ–°æ ‡æ†', 'desc': 'Aryabhata 1.0 æ˜¯ä¸€ä¸ªç´§å‡‘çš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œä¸“ä¸ºå°åº¦çš„è”åˆå…¥å­¦è€ƒè¯•ï¼ˆJEEï¼‰ä¼˜åŒ–ã€‚å®ƒé€šè¿‡ç›‘ç£å¾®è°ƒã€å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ å’Œæ–°é¢–çš„æ¢ç´¢ç­–ç•¥ï¼Œè¶…è¶Šäº†ç°æœ‰æ¨¡å‹çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å¼ºå¤§çš„å¼€æ”¾æƒé‡æ¨ç†æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨äº†è¯¾ç¨‹å­¦ä¹ å’Œé“¾å¼æ€ç»´çš„éªŒè¯æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚ç»è¿‡è¯„ä¼°ï¼ŒAryabhata åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒåŒæ—¶æä¾›äº†æœ‰åŠ©äºæ•™å­¦çš„é€æ­¥æ¨ç†è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.08940', 'title': 'Train Long, Think Short: Curriculum Learning for Efficient Reasoning', 'url': 'https://huggingface.co/papers/2508.08940', 'abstract': 'A curriculum learning strategy using Group Relative Policy Optimization (GRPO) enhances the reasoning abilities of large language models by progressively tightening token budgets, improving accuracy and token efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. In this work, we propose a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). Our method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. We augment GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. We further ablate the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. Our code and checkpoints are released at: https://github.com/hammoudhasan/curriculum_grpo.', 'score': 5, 'issue_id': 5323, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 12', 'zh': '8æœˆ12æ—¥'}, 'hash': '72998e00bb3562ea', 'authors': ['Hasan Abed Al Kader Hammoud', 'Kumail Alhamoud', 'Abed Hammoud', 'Elie Bou-Zeid', 'Marzyeh Ghassemi', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology (KAUST), Saudi Arabia', 'Massachusetts Institute of Technology (MIT), Cambridge, MA, USA', 'Princeton University, Princeton, NJ, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.08940.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#optimization', '#training', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Group Relative Policy Optimization (GRPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ¶ĞµÑÑ‚Ğ¾Ñ‡Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ GRPO Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¸ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Progressive Learning for Efficient Reasoning in LLMs', 'desc': 'This paper presents a novel curriculum learning strategy that utilizes Group Relative Policy Optimization (GRPO) to enhance the reasoning capabilities of large language models (LLMs). The approach involves starting with a generous token budget and progressively tightening it, allowing models to first explore various solution strategies before refining them into concise reasoning. By incorporating a reward function that balances task correctness, length efficiency, and formatting adherence, the method improves both accuracy and token efficiency. Experiments demonstrate that this curriculum-based training consistently outperforms traditional fixed-budget methods across multiple datasets, highlighting the effectiveness of progressive constraints in model training.'}, 'zh': {'title': 'è¯¾ç¨‹å­¦ä¹ æå‡æ¨ç†èƒ½åŠ›çš„ç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ ç­–ç•¥çš„é•¿åº¦æ§åˆ¶æ¨ç†æ–¹æ³•ï¼Œä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡é€æ­¥æ”¶ç´§ä»¤ç‰Œé¢„ç®—ï¼Œé¼“åŠ±æ¨¡å‹é¦–å…ˆæ¢ç´¢æœ‰æ•ˆçš„è§£å†³ç­–ç•¥ï¼Œç„¶åå°†å…¶æç‚¼ä¸ºæ›´ç®€æ´çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¥–åŠ±å‡½æ•°ï¼Œå¹³è¡¡ä»»åŠ¡æ­£ç¡®æ€§ã€é•¿åº¦æ•ˆç‡å’Œæ ¼å¼éµå¾ªä¸‰ä¸ªä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºè¯¾ç¨‹çš„è®­ç»ƒåœ¨ç›¸åŒçš„æœ€ç»ˆé¢„ç®—ä¸‹ï¼Œå§‹ç»ˆä¼˜äºå›ºå®šé¢„ç®—çš„åŸºçº¿ï¼Œè¾¾åˆ°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ˜¾è‘—æé«˜çš„ä»¤ç‰Œæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.06964', 'title': 'Adversarial Video Promotion Against Text-to-Video Retrieval', 'url': 'https://huggingface.co/papers/2508.06964', 'abstract': 'The Video Promotion attack (ViPro) enhances the robustness of text-to-video retrieval (T2VR) by promoting videos towards selected queries, demonstrating significant improvements over existing baselines in various attack scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Thanks to the development of cross-modal models, text-to-video retrieval (T2VR) is advancing rapidly, but its robustness remains largely unexamined. Existing attacks against T2VR are designed to push videos away from queries, i.e., suppressing the ranks of videos, while the attacks that pull videos towards selected queries, i.e., promoting the ranks of videos, remain largely unexplored. These attacks can be more impactful as attackers may gain more views/clicks for financial benefits and widespread (mis)information. To this end, we pioneer the first attack against T2VR to promote videos adversarially, dubbed the Video Promotion attack (ViPro). We further propose Modal Refinement (MoRe) to capture the finer-grained, intricate interaction between visual and textual modalities to enhance black-box transferability. Comprehensive experiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing datasets with over 10k videos, evaluated under 3 scenarios. All experiments are conducted in a multi-target setting to reflect realistic scenarios where attackers seek to promote the video regarding multiple queries simultaneously. We also evaluated our attacks for defences and imperceptibility. Overall, ViPro surpasses other baselines by over 30/10/4% for white/grey/black-box settings on average. Our work highlights an overlooked vulnerability, provides a qualitative analysis on the upper/lower bound of our attacks, and offers insights into potential counterplays. Code will be publicly available at https://github.com/michaeltian108/ViPro.', 'score': 5, 'issue_id': 5323, 'pub_date': '2025-08-09', 'pub_date_card': {'ru': '9 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 9', 'zh': '8æœˆ9æ—¥'}, 'hash': '25b0548ad7d6f59c', 'authors': ['Qiwei Tian', 'Chenhao Lin', 'Zhengyu Zhao', 'Qian Li', 'Shuai Liu', 'Chao Shen'], 'affiliations': ['Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.06964.jpg', 'data': {'categories': ['#multimodal', '#security', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ViPro: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Video Promotion (ViPro). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ViPro Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼, Ğ° Ğ½Ğµ Ğ½Ğ° Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Modal Refinement (MoRe) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ViPro Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ°Ñ‚Ğ°Ğº.'}, 'en': {'title': 'Promoting Videos: A New Threat to Text-to-Video Retrieval', 'desc': 'The Video Promotion attack (ViPro) introduces a novel approach to enhance the robustness of text-to-video retrieval (T2VR) systems by promoting videos towards specific queries. Unlike traditional attacks that aim to suppress video rankings, ViPro focuses on increasing the visibility of videos, which can lead to greater financial gains for attackers. The paper also presents Modal Refinement (MoRe), a technique that improves the interaction between visual and textual data, enhancing the effectiveness of the attack across different scenarios. Comprehensive experiments demonstrate that ViPro significantly outperforms existing methods, revealing a critical vulnerability in T2VR systems and suggesting avenues for future defenses.'}, 'zh': {'title': 'è§†é¢‘æ¨å¹¿æ”»å‡»ï¼šæå‡T2VRé²æ£’æ€§çš„åˆ›æ–°æ–¹æ³•', 'desc': 'è§†é¢‘æ¨å¹¿æ”»å‡»ï¼ˆViProï¼‰é€šè¿‡å°†è§†é¢‘å‘é€‰å®šæŸ¥è¯¢æ¨å¹¿ï¼Œå¢å¼ºäº†æ–‡æœ¬åˆ°è§†é¢‘æ£€ç´¢ï¼ˆT2VRï¼‰çš„é²æ£’æ€§ã€‚ä¸ç°æœ‰çš„æ”»å‡»æ–¹æ³•ä¸åŒï¼ŒViProä¸“æ³¨äºæå‡è§†é¢‘çš„æ’åï¼Œè€Œä¸æ˜¯å°†å…¶å‹åˆ¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†æ¨¡æ€ç²¾ç‚¼ï¼ˆMoReï¼‰æ–¹æ³•ï¼Œä»¥æ•æ‰è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´æ›´ç»†è‡´çš„äº¤äº’ï¼Œä»è€Œæé«˜é»‘ç®±è½¬ç§»æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒViProåœ¨å¤šç§æ”»å‡»åœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå±•ç¤ºäº†è¿™ä¸€é¢†åŸŸè¢«å¿½è§†çš„è„†å¼±æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.09101', 'title': 'AutoCodeBench: Large Language Models are Automatic Code Benchmark\n  Generators', 'url': 'https://huggingface.co/papers/2508.09101', 'abstract': 'AutoCodeGen creates a large-scale, multilingual code generation benchmark, AutoCodeBench, to evaluate LLMs on diverse and complex tasks without manual annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through a multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, a large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as a valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios.', 'score': 4, 'issue_id': 5318, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 12', 'zh': '8æœˆ12æ—¥'}, 'hash': '6dd7cffc5f881c6a', 'authors': ['Jason Chou', 'Ao Liu', 'Yuchi Deng', 'Zhiying Zeng', 'Tao Zhang', 'Haotian Zhu', 'Jianwei Cai', 'Yue Mao', 'Chenchen Zhang', 'Lingyun Tan', 'Ziyan Xu', 'Bohui Zhai', 'Hengyi Liu', 'Speed Zhu', 'Wiggin Zhou', 'Fengzong Lian'], 'affiliations': ['Hunyuan Team, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2508.09101.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#games', '#benchmark', '#open_source'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'AutoCodeBench: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ˜Ğ˜', 'desc': 'AutoCodeGen Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… AutoCodeBench, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 3920 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 20 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. AutoCodeBench Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Revolutionizing Code Generation Evaluation with AutoCodeBench', 'desc': 'The paper introduces AutoCodeGen, a method for creating a large-scale, multilingual benchmark called AutoCodeBench to evaluate the code generation capabilities of Large Language Models (LLMs). Unlike existing benchmarks that rely on manual annotations and focus mainly on Python, AutoCodeBench offers a diverse set of 3,920 problems across 20 programming languages, ensuring a balanced difficulty level. The benchmark is generated automatically, ensuring high data quality through advanced techniques like reverse-order problem generation and multiple filtering steps. The evaluation of over 30 LLMs on this benchmark reveals that even the most advanced models face challenges with the complexity and diversity of the tasks, highlighting the need for more rigorous testing in multilingual code generation.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–å¤šè¯­è¨€ä»£ç ç”ŸæˆåŸºå‡†çš„åˆ›æ–°', 'desc': 'AutoCodeGen æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆé«˜éš¾åº¦çš„å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ•°æ®é›†ï¼Œè€Œæ— éœ€äººå·¥æ³¨é‡Šã€‚å®ƒé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆæµ‹è¯•è¾“å…¥ï¼Œå¹¶é€šè¿‡å¤šè¯­è¨€æ²™ç®±è·å–æµ‹è¯•è¾“å‡ºï¼Œä»è€Œç¡®ä¿æµ‹è¯•ç”¨ä¾‹çš„æ­£ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚æˆ‘ä»¬æ¨å‡ºçš„ AutoCodeBench æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ä»£ç ç”ŸæˆåŸºå‡†ï¼ŒåŒ…å«3920ä¸ªé—®é¢˜ï¼Œå‡åŒ€åˆ†å¸ƒåœ¨20ç§ç¼–ç¨‹è¯­è¨€ä¸­ï¼Œæ—¨åœ¨è¯„ä¼° LLMs åœ¨å¤æ‚ã€å¤šæ ·å’Œå®é™…çš„å¤šè¯­è¨€ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„ LLMs åœ¨è¿™äº›ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§é¢å‰ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.08244', 'title': 'Cut2Next: Generating Next Shot via In-Context Tuning', 'url': 'https://huggingface.co/papers/2508.08244', 'abstract': 'Cut2Next, a framework using a Diffusion Transformer with in-context tuning and hierarchical prompting, generates high-quality, cinematically coherent shots that adhere to professional editing patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective multi-shot generation demands purposeful, film-like transitions and strict cinematic continuity. Current methods, however, often prioritize basic visual consistency, neglecting crucial editing patterns (e.g., shot/reverse shot, cutaways) that drive narrative flow for compelling storytelling. This yields outputs that may be visually coherent but lack narrative sophistication and true cinematic integrity. To bridge this, we introduce Next Shot Generation (NSG): synthesizing a subsequent, high-quality shot that critically conforms to professional editing patterns while upholding rigorous cinematic continuity. Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This strategy uses Relational Prompts to define overall context and inter-shot editing styles. Individual Prompts then specify per-shot content and cinematographic attributes. Together, these guide Cut2Next to generate cinematically appropriate next shots. Architectural innovations, Context-Aware Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further integrate these diverse signals without introducing new parameters. We construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with hierarchical prompts, and introduce CutBench for evaluation. Experiments show Cut2Next excels in visual consistency and text fidelity. Crucially, user studies reveal a strong preference for Cut2Next, particularly for its adherence to intended editing patterns and overall cinematic continuity, validating its ability to generate high-quality, narratively expressive, and cinematically coherent subsequent shots.', 'score': 4, 'issue_id': 5320, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 11', 'zh': '8æœˆ11æ—¥'}, 'hash': '172314ad94a655f4', 'authors': ['Jingwen He', 'Hongbo Liu', 'Jiajun Li', 'Ziqi Huang', 'Yu Qiao', 'Wanli Ouyang', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.08244.jpg', 'data': {'categories': ['#cv', '#diffusion', '#benchmark', '#story_generation', '#dataset', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ¾ÑĞ²Ğ¾Ğ¸Ğ» Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¸Ğ½Ğ¾Ğ¼Ğ¾Ğ½Ñ‚Ğ°Ğ¶', 'desc': 'Cut2Next - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ in-context tuning Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ Ğ¼Ğ¾Ğ½Ñ‚Ğ°Ğ¶Ğ°. Cut2Next Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ğ¼Ğ¾Ğ½Ñ‚Ğ°Ğ¶Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Cut2Next Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Cinematic Continuity in AI-Generated Shots', 'desc': "Cut2Next is a novel framework that utilizes a Diffusion Transformer to generate high-quality video shots that follow professional editing patterns. It addresses the limitations of existing methods that focus mainly on visual consistency, often overlooking essential cinematic techniques that enhance storytelling. By implementing Next Shot Generation (NSG) with in-context tuning and hierarchical prompting, Cut2Next ensures that each generated shot maintains narrative flow and cinematic integrity. The framework's architectural innovations and the creation of specialized datasets enable it to produce visually coherent and narratively rich outputs, as confirmed by user studies favoring its performance."}, 'zh': {'title': 'ç”Ÿæˆé«˜è´¨é‡ç”µå½±é•œå¤´çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'Cut2Nextæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨å’Œä¸Šä¸‹æ–‡è°ƒä¼˜ç”Ÿæˆé«˜è´¨é‡çš„é•œå¤´ï¼Œç¬¦åˆä¸“ä¸šçš„å‰ªè¾‘æ¨¡å¼ã€‚è¯¥æ–¹æ³•è§£å†³äº†å½“å‰ç”ŸæˆæŠ€æœ¯åœ¨å™äº‹æµç•…æ€§å’Œå‰ªè¾‘æ¨¡å¼ä¸Šçš„ä¸è¶³ï¼Œç¡®ä¿ç”Ÿæˆçš„é•œå¤´ä¸ä»…è§†è§‰ä¸€è‡´ï¼Œè¿˜å…·å¤‡å™äº‹çš„å¤æ‚æ€§ã€‚é€šè¿‡å¼•å…¥å±‚æ¬¡åŒ–å¤šæç¤ºç­–ç•¥ï¼ŒCut2Nextèƒ½å¤Ÿå®šä¹‰æ•´ä½“ä¸Šä¸‹æ–‡å’Œé•œå¤´é—´çš„ç¼–è¾‘é£æ ¼ï¼Œä»è€Œç”Ÿæˆç¬¦åˆç”µå½±è¿ç»­æ€§çš„åç»­é•œå¤´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCut2Nextåœ¨è§†è§‰ä¸€è‡´æ€§å’Œæ–‡æœ¬ä¿çœŸåº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œç”¨æˆ·ç ”ç©¶ä¹Ÿæ˜¾ç¤ºå‡ºå¯¹å…¶å‰ªè¾‘æ¨¡å¼çš„å¼ºçƒˆåå¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.09123', 'title': 'OpenCUA: Open Foundations for Computer-Use Agents', 'url': 'https://huggingface.co/papers/2508.09123', 'abstract': 'OpenCUA is an open-source framework for vision-language models as computer-use agents, featuring an annotation infrastructure, a large-scale dataset, and a scalable pipeline that achieves state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models have demonstrated impressive capabilities as computer-use agents (CUAs) capable of automating diverse computer tasks. As their commercial potential grows, critical details of the most capable CUA systems remain closed. As these agents will increasingly mediate digital interactions and execute consequential decisions on our behalf, the research community needs access to open CUA frameworks to study their capabilities, limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive open-source framework for scaling CUA data and foundation models. Our framework consists of: (1) an annotation infrastructure that seamlessly captures human computer-use demonstrations; (2) AgentNet, the first large-scale computer-use task dataset spanning 3 operating systems and 200+ applications and websites; (3) a scalable pipeline that transforms demonstrations into state-action pairs with reflective long Chain-of-Thought reasoning that sustain robust performance gains as data scales. Our end-to-end agent models demonstrate strong performance across CUA benchmarks. In particular, OpenCUA-32B achieves an average success rate of 34.8% on OSWorld-Verified, establishing a new state-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA (GPT-4o). Further analysis confirms that our approach generalizes well across domains and benefits significantly from increased test-time computation. We release our annotation tool, datasets, code, and models to build open foundations for further CUA research.', 'score': 3, 'issue_id': 5323, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 12', 'zh': '8æœˆ12æ—¥'}, 'hash': '111b2e6f3de484b8', 'authors': ['Xinyuan Wang', 'Bowen Wang', 'Dunjie Lu', 'Junlin Yang', 'Tianbao Xie', 'Junli Wang', 'Jiaqi Deng', 'Xiaole Guo', 'Yiheng Xu', 'Chen Henry Wu', 'Zhennan Shen', 'Zhuokai Li', 'Ryan Li', 'Xiaochuan Li', 'Junda Chen', 'Boyuan Zheng', 'Peihang Li', 'Fangyu Lei', 'Ruisheng Cao', 'Yeqiao Fu', 'Dongchan Shin', 'Martin Shin', 'Jiarui Hu', 'Yuyan Wang', 'Jixuan Chen', 'Yuxiao Ye', 'Danyang Zhang', 'Dikang Du', 'Hao Hu', 'Huarong Chen', 'Zaida Zhou', 'Yipu Wang', 'Heng Wang', 'Diyi Yang', 'Victor Zhong', 'Flood Sung', 'Y. Charles', 'Zhilin Yang', 'Tao Yu'], 'affiliations': ['Carnegie Mellon University', 'Moonshot AI', 'Stanford University', 'University of Waterloo', 'XLANG Lab, University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.09123.jpg', 'data': {'categories': ['#agents', '#dataset', '#reasoning', '#multimodal', '#agi', '#benchmark', '#open_source'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'OpenCUA: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'OpenCUA - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ°Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ°. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğµ, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° AgentNet Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ 3 Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 200 Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ pipeline Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¿Ğ°Ñ€Ñ‹ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ OpenCUA-32B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OSWorld-Verified.'}, 'en': {'title': 'Empowering Research with OpenCUA: The Future of Computer-Use Agents', 'desc': 'OpenCUA is an innovative open-source framework designed for vision-language models that function as computer-use agents (CUAs). It includes a robust annotation infrastructure, a large-scale dataset called AgentNet, and a scalable pipeline that enhances performance through reflective reasoning. The framework allows researchers to access and study the capabilities and limitations of CUAs, which are becoming increasingly important in automating digital tasks. With state-of-the-art results, OpenCUA-32B sets a new benchmark in the field, promoting further research and development in open CUA systems.'}, 'zh': {'title': 'å¼€æ”¾æºä»£ç ï¼Œæ¨åŠ¨è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„æœªæ¥', 'desc': 'OpenCUAæ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œä¸“ä¸ºè§†è§‰-è¯­è¨€æ¨¡å‹ä½œä¸ºè®¡ç®—æœºä½¿ç”¨ä»£ç†è€Œè®¾è®¡ã€‚å®ƒæä¾›äº†ä¸€ä¸ªæ³¨é‡ŠåŸºç¡€è®¾æ–½ã€å¤§è§„æ¨¡æ•°æ®é›†å’Œå¯æ‰©å±•çš„ç®¡é“ï¼Œèƒ½å¤Ÿå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æ•æ‰äººç±»è®¡ç®—æœºä½¿ç”¨æ¼”ç¤ºçš„æ³¨é‡Šå·¥å…·ï¼Œä»¥åŠæ¶µç›–å¤šä¸ªæ“ä½œç³»ç»Ÿå’Œåº”ç”¨ç¨‹åºçš„å¤§è§„æ¨¡è®¡ç®—æœºä½¿ç”¨ä»»åŠ¡æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒOpenCUAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†å¼€æ”¾æºä»£ç æ¨¡å‹çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.08791', 'title': 'Feedback-Driven Tool-Use Improvements in Large Language Models via\n  Automated Build Environments', 'url': 'https://huggingface.co/papers/2508.08791', 'abstract': "An automated pipeline for constructing training environments and a verifiable reward mechanism enhance large language models' tool-use performance without compromising general capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective tool use is essential for large language models (LLMs) to interact meaningfully with their environment. However, progress is limited by the lack of efficient reinforcement learning (RL) frameworks specifically designed for tool use, due to challenges in constructing stable training environments and designing verifiable reward mechanisms. To address this, we propose an automated environment construction pipeline, incorporating scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. This enables the creation of high-quality training environments that provide detailed and measurable feedback without relying on external tools. Additionally, we introduce a verifiable reward mechanism that evaluates both the precision of tool use and the completeness of task execution. When combined with trajectory data collected from the constructed environments, this mechanism integrates seamlessly with standard RL algorithms to facilitate feedback-driven model training. Experiments on LLMs of varying scales demonstrate that our approach significantly enhances the models' tool-use performance without degrading their general capabilities, regardless of inference modes or training algorithms. Our analysis suggests that these gains result from improved context understanding and reasoning, driven by updates to the lower-layer MLP parameters in models.", 'score': 3, 'issue_id': 5319, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 12', 'zh': '8æœˆ12æ—¥'}, 'hash': 'd6c6aa0f12a1c869', 'authors': ['Junjie Ye', 'Changhao Jiang', 'Zhengyin Du', 'Yufei Xu', 'Xuesong Yao', 'Zhiheng Xi', 'Xiaoran Fan', 'Qi Zhang', 'Xuanjing Huang', 'Jiecao Chen'], 'affiliations': ['ByteDance Seed', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2508.08791.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rl'], 'emoji': 'ğŸ› ï¸', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… ÑÑ€ĞµĞ´ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’Ğ²ĞµĞ´ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² LLM Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Enhancing Tool Use in LLMs with Automated Training Environments', 'desc': 'This paper presents a new method to improve how large language models (LLMs) use tools effectively. It introduces an automated system for creating training environments that are stable and provide clear feedback, which is crucial for reinforcement learning (RL). Additionally, the authors propose a verifiable reward mechanism that assesses how well the models use tools and complete tasks. The results show that this approach enhances the tool-use abilities of LLMs without harming their overall performance.'}, 'zh': {'title': 'æå‡å·¥å…·ä½¿ç”¨æ€§èƒ½çš„è‡ªåŠ¨åŒ–è®­ç»ƒç¯å¢ƒ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„è®­ç»ƒç¯å¢ƒæ„å»ºç®¡é“ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å·¥å…·ä½¿ç”¨æ–¹é¢çš„è¡¨ç°ï¼ŒåŒæ—¶ä¿æŒå…¶é€šç”¨èƒ½åŠ›ä¸å—å½±å“ã€‚è¯¥ç®¡é“é€šè¿‡åœºæ™¯åˆ†è§£ã€æ–‡æ¡£ç”Ÿæˆã€åŠŸèƒ½é›†æˆã€å¤æ‚åº¦æ‰©å±•å’Œæœ¬åœ°éƒ¨ç½²ç­‰æ­¥éª¤ï¼Œåˆ›å»ºé«˜è´¨é‡çš„è®­ç»ƒç¯å¢ƒï¼Œå¹¶æä¾›è¯¦ç»†çš„å¯æµ‹é‡åé¦ˆã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶ï¼Œè¯„ä¼°å·¥å…·ä½¿ç”¨çš„å‡†ç¡®æ€§å’Œä»»åŠ¡æ‰§è¡Œçš„å®Œæ•´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å·¥å…·ä½¿ç”¨æ€§èƒ½ï¼Œä¸”ä¸å½±å“å…¶æ•´ä½“èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.09050', 'title': 'Bridging Theory and Practice in Quantum Game Theory: Optimized\n  Implementation of the Battle of the Sexes with Error Mitigation on NISQ\n  Hardware', 'url': 'https://huggingface.co/papers/2508.09050', 'abstract': "Quantum game theory demonstrated on IBM Quantum hardware using the Eisert-Wilkens-Lewenstein framework shows persistent quantum advantages in strategic coordination despite noise and decoherence.  \t\t\t\t\tAI-generated summary \t\t\t\t Implementing quantum game theory on real hardware is challenging due to noise, decoherence, and limited qubit connectivity, yet such demonstrations are essential to validate theoretical predictions. We present one of the first full experimental realizations of the Battle of the Sexes game under the Eisert-Wilkens-Lewenstein (EWL) framework on IBM Quantum's ibm sherbrooke superconducting processor. Four quantum strategies (I, H, R(pi/4), R(pi)) were evaluated across 31 entanglement values gamma in [0, pi] using 2048 shots per configuration, enabling a direct comparison between analytical predictions and hardware execution. To mitigate noise and variability, we introduce a Guided Circuit Mapping (GCM) method that dynamically selects qubit pairs and optimizes routing based on real-time topology and calibration data. The analytical model forecasts up to 108% payoff improvement over the classical equilibrium, and despite hardware-induced deviations, experimental results with GCM preserve the expected payoff trends within 3.5%-12% relative error. These findings show that quantum advantages in strategic coordination can persist under realistic NISQ conditions, providing a pathway toward practical applications of quantum game theory in multi-agent, economic, and distributed decision-making systems.", 'score': 1, 'issue_id': 5322, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 12', 'zh': '8æœˆ12æ—¥'}, 'hash': '107a4a0916cff223', 'authors': ['GermÃ¡n DÃ­az Agreda', 'Carlos Andres Duran Paredes', 'Mateo Buenaventura Samboni', 'Jhon Alejandro Andrade', 'SebastiÃ¡n AndrÃ©s Cajas OrdoÃ±ez'], 'affiliations': ['Facultad de Ciencias Naturales, Exactas de la Educacion Universidad del Cauca Popayan, Colombia', 'National Irish Centre for AI (CeADAR) University College Dublin (UCD) Dublin, Ireland'], 'pdf_title_img': 'assets/pdf/title_img/2508.09050.jpg', 'data': {'categories': ['#agents', '#games', '#optimization', '#math'], 'emoji': 'ğŸ®', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ¸Ğ³Ñ€ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ ÑˆÑƒĞ¼: ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚ Ğ½Ğ° IBM Quantum', 'desc': "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¾Ğ´Ğ½Ğ° Ğ¸Ğ· Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ³Ñ€ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ IBM. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ­Ğ¹Ğ·ĞµÑ€Ñ‚Ğ°-Ğ’Ğ¸Ğ»ĞºĞµĞ½ÑĞ°-Ğ›ĞµĞ²ĞµĞ½ÑˆÑ‚ĞµĞ¹Ğ½Ğ° Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€Ñ‹ 'Ğ‘Ğ¸Ñ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»Ğ¾Ğ²', Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ·Ğ°Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ´ĞµĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Guided Circuit Mapping, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºÑƒĞ±Ğ¸Ñ‚Ğ¾Ğ². ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ, ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ğµ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹ÑˆĞ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ² ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… NISQ."}, 'en': {'title': 'Harnessing Quantum Strategies for Real-World Decision Making', 'desc': 'This paper explores the implementation of quantum game theory on IBM Quantum hardware, specifically using the Eisert-Wilkens-Lewenstein framework. It presents an experimental realization of the Battle of the Sexes game, evaluating various quantum strategies while addressing challenges like noise and decoherence. The authors introduce a Guided Circuit Mapping method to optimize qubit routing and mitigate errors, achieving results that align closely with theoretical predictions. The findings suggest that quantum advantages in strategic coordination can be maintained even in real-world conditions, paving the way for future applications in complex decision-making scenarios.'}, 'zh': {'title': 'é‡å­åšå¼ˆç†è®ºçš„ä¼˜åŠ¿åœ¨ç°å®æ¡ä»¶ä¸‹ä¾ç„¶å­˜åœ¨', 'desc': 'è¿™ç¯‡è®ºæ–‡å±•ç¤ºäº†åœ¨IBMé‡å­ç¡¬ä»¶ä¸Šå®ç°é‡å­åšå¼ˆç†è®ºçš„å®éªŒï¼Œä½¿ç”¨äº†Eisert-Wilkens-Lewensteinæ¡†æ¶ã€‚å°½ç®¡å­˜åœ¨å™ªå£°å’Œé€€ç›¸å¹²çš„æŒ‘æˆ˜ï¼Œå®éªŒç»“æœè¡¨æ˜é‡å­ç­–ç•¥åœ¨åè°ƒå†³ç­–ä¸­ä»ç„¶å…·æœ‰ä¼˜åŠ¿ã€‚ç ”ç©¶ä¸­é‡‡ç”¨äº†å¼•å¯¼ç”µè·¯æ˜ å°„æ–¹æ³•ï¼Œä»¥ä¼˜åŒ–é‡å­æ¯”ç‰¹çš„è¿æ¥å’Œè·¯ç”±ï¼Œä»è€Œå‡å°‘å™ªå£°å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨å®é™…çš„NISQæ¡ä»¶ä¸‹ï¼Œé‡å­åšå¼ˆç†è®ºçš„ä¼˜åŠ¿ä¾ç„¶å­˜åœ¨ï¼Œä¸ºå¤šæ™ºèƒ½ä½“å’Œç»æµå†³ç­–ç³»ç»Ÿçš„å®é™…åº”ç”¨æä¾›äº†å¯èƒ½çš„è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04195', 'title': 'NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech\n  Modeling with Paralinguistic Vocalizations', 'url': 'https://huggingface.co/papers/2508.04195', 'abstract': 'NVSpeech is a pipeline that integrates the recognition and synthesis of paralinguistic vocalizations in Mandarin, using a large annotated dataset and models that treat these cues as decodable tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Paralinguistic vocalizations-including non-verbal sounds like laughter and breathing, as well as lexicalized interjections such as "uhm" and "oh"-are integral to natural spoken communication. Despite their importance in conveying affect, intent, and interactional cues, such cues remain largely overlooked in conventional automatic speech recognition (ASR) and text-to-speech (TTS) systems. We present NVSpeech, an integrated and scalable pipeline that bridges the recognition and synthesis of paralinguistic vocalizations, encompassing dataset construction, ASR modeling, and controllable TTS. (1) We introduce a manually annotated dataset of 48,430 human-spoken utterances with 18 word-level paralinguistic categories. (2) We develop the paralinguistic-aware ASR model, which treats paralinguistic cues as inline decodable tokens (e.g., "You\'re so funny [Laughter]"), enabling joint lexical and non-verbal transcription. This model is then used to automatically annotate a large corpus, the first large-scale Chinese dataset of 174,179 utterances (573 hours) with word-level alignment and paralingustic cues. (3) We finetune zero-shot TTS models on both human- and auto-labeled data to enable explicit control over paralinguistic vocalizations, allowing context-aware insertion at arbitrary token positions for human-like speech synthesis. By unifying the recognition and generation of paralinguistic vocalizations, NVSpeech offers the first open, large-scale, word-level annotated pipeline for expressive speech modeling in Mandarin, integrating recognition and synthesis in a scalable and controllable manner. Dataset and audio demos are available at https://nvspeech170k.github.io/.', 'score': 0, 'issue_id': 5322, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': 'b4eb7f9d017ec3fd', 'authors': ['Huan Liao', 'Qinke Ni', 'Yuancheng Wang', 'Yiheng Lu', 'Haoyue Zhan', 'Pengyuan Xie', 'Qiang Zhang', 'Zhizheng Wu'], 'affiliations': ['Guangzhou Quwan Network Technology', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2508.04195.jpg', 'data': {'categories': ['#dataset', '#open_source', '#data', '#audio'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ² Ñ€ĞµÑ‡Ğ¸', 'desc': 'NVSpeech - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ğ½Ğ´Ğ°Ñ€Ğ¸Ğ½ÑĞºĞ¾Ğ¼ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ 18 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR) Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ (TTS), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ĞºĞ°Ğº Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. NVSpeech Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· ÑĞºÑĞ¿Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑÑ….'}, 'en': {'title': 'Bridging Speech Recognition and Synthesis with Paralinguistic Cues', 'desc': 'NVSpeech is a novel pipeline designed to enhance the recognition and synthesis of paralinguistic vocalizations in Mandarin, which include non-verbal sounds and interjections. It utilizes a large, manually annotated dataset of over 48,000 utterances categorized into 18 paralinguistic types, enabling a more nuanced understanding of spoken communication. The system features a paralinguistic-aware automatic speech recognition (ASR) model that treats these vocal cues as decodable tokens, allowing for joint transcription of verbal and non-verbal elements. Additionally, it incorporates a controllable text-to-speech (TTS) model that can insert these vocalizations contextually, resulting in more expressive and human-like speech synthesis.'}, 'zh': {'title': 'NVSpeechï¼šæ™®é€šè¯å‰¯è¯­è¨€å£°éŸ³çš„è¯†åˆ«ä¸åˆæˆæ–°çªç ´', 'desc': 'NVSpeechæ˜¯ä¸€ä¸ªé›†æˆäº†æ™®é€šè¯ä¸­å‰¯è¯­è¨€å£°éŸ³è¯†åˆ«å’Œåˆæˆçš„ç®¡é“ï¼Œä½¿ç”¨äº†ä¸€ä¸ªå¤§å‹æ ‡æ³¨æ•°æ®é›†å’Œå°†è¿™äº›çº¿ç´¢è§†ä¸ºå¯è§£ç æ ‡è®°çš„æ¨¡å‹ã€‚å‰¯è¯­è¨€å£°éŸ³åŒ…æ‹¬éè¯­è¨€å£°éŸ³ï¼ˆå¦‚ç¬‘å£°å’Œå‘¼å¸ï¼‰ä»¥åŠè¯æ±‡åŒ–çš„æ’å…¥è¯­ï¼ˆå¦‚â€œå—¯â€å’Œâ€œå“¦â€ï¼‰ï¼Œåœ¨è‡ªç„¶å£è¯­äº¤æµä¸­è‡³å…³é‡è¦ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«48,430ä¸ªäººå·¥å‘éŸ³çš„æ ‡æ³¨æ•°æ®é›†ï¼Œå¼€å‘äº†ä¸€ä¸ªå‰¯è¯­è¨€æ„ŸçŸ¥çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šç”Ÿæˆäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„ä¸­æ–‡æ•°æ®é›†ã€‚NVSpeechå®ç°äº†å‰¯è¯­è¨€å£°éŸ³çš„è¯†åˆ«ä¸åˆæˆçš„ç»Ÿä¸€ï¼Œä¸ºæ™®é€šè¯çš„è¡¨è¾¾æ€§è¯­éŸ³å»ºæ¨¡æä¾›äº†é¦–ä¸ªå¼€æ”¾çš„å¤§è§„æ¨¡æ ‡æ³¨ç®¡é“ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (6)', '#agi (1)', '#alignment', '#architecture (1)', '#audio (1)', '#benchmark (8)', '#cv (2)', '#data (1)', '#dataset (8)', '#diffusion (4)', '#ethics', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (1)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (3)', '#multilingual (1)', '#multimodal (3)', '#open_source (7)', '#optimization (8)', '#plp', '#rag (1)', '#reasoning (6)', '#rl (7)', '#rlhf (2)', '#robotics', '#science', '#security (1)', '#small_models (1)', '#story_generation (1)', '#survey', '#synthetic (2)', '#training (6)', '#transfer_learning', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-08-13 08:16',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-13 08:16')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-13 08:16')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    