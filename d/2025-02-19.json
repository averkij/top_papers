{
    "date": {
        "ru": "19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 19",
        "zh": "2æœˆ19æ—¥"
    },
    "time_utc": "2025-02-19 06:14",
    "weekday": 2,
    "issue_id": 2289,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.12900",
            "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
            "url": "https://huggingface.co/papers/2502.12900",
            "abstract": "Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency. We propose Soundwave, which utilizes an efficient training strategy and a novel architecture to address these issues. Results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks, using only one-fiftieth of the training data. Further analysis shows that Soundwave still retains its intelligence during conversation. The project is available at https://github.com/FreedomIntelligence/Soundwave.",
            "score": 31,
            "issue_id": 2289,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "95780ecdf251cffd",
            "authors": [
                "Yuhao Zhang",
                "Zhiheng Liu",
                "Fan Bu",
                "Ruiyu Zhang",
                "Benyou Wang",
                "Haizhou Li"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12900.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#audio",
                    "#transfer_learning",
                    "#open_source",
                    "#optimization",
                    "#data",
                    "#architecture"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Soundwave - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµÑ‡Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµÑ‡ÑŒÑ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ. Soundwave Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2-Audio Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ñ… AIR-Bench, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 1/50 Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°."
                },
                "en": {
                    "title": "Soundwave: Efficient Speech Translation with Minimal Data",
                    "desc": "This paper introduces Soundwave, a new model designed for speech translation that requires significantly less training data compared to existing large language models. It addresses two key challenges: the differences in how speech and text are represented and the varying lengths of sequences in speech data. By employing an efficient training strategy and a unique architecture, Soundwave achieves superior performance on speech tasks while using only 2% of the data needed by its competitors. The findings indicate that Soundwave maintains high conversational intelligence, making it a promising approach for data-efficient speech processing."
                },
                "zh": {
                    "title": "é«˜æ•ˆè®­ç»ƒï¼Œè¯­éŸ³ç¿»è¯‘æ–°çªç ´ï¼",
                    "desc": "ç°æœ‰çš„ç«¯åˆ°ç«¯è¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè€Œæ•°æ®é«˜æ•ˆè®­ç»ƒå°šæœªæ·±å…¥æ¢è®¨ã€‚æˆ‘ä»¬å…³æ³¨è¯­éŸ³å’Œæ–‡æœ¬ä¹‹é—´çš„ä¸¤ä¸ªåŸºæœ¬é—®é¢˜ï¼šè¡¨ç¤ºç©ºé—´å·®è·å’Œåºåˆ—é•¿åº¦ä¸ä¸€è‡´ã€‚æˆ‘ä»¬æå‡ºäº†Soundwaveï¼Œå®ƒåˆ©ç”¨é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥å’Œæ–°é¢–çš„æ¶æ„æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç»“æœè¡¨æ˜ï¼ŒSoundwaveåœ¨è¯­éŸ³ç¿»è¯‘å’ŒAIR-Benchè¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå…ˆè¿›çš„Qwen2-Audioï¼Œä»…ä½¿ç”¨äº†äº”ååˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11564",
            "title": "Continuous Diffusion Model for Language Modeling",
            "url": "https://huggingface.co/papers/2502.11564",
            "abstract": "Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose a continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish a connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce a simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at https://github.com/harryjo97/RDLM{https://github.com/harryjo97/RDLM}.",
            "score": 23,
            "issue_id": 2287,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "f5889d3a88d24b7c",
            "authors": [
                "Jaehyeong Jo",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "Korea Advanced Institute of Science and Technology (KAIST)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11564.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸ĞµĞ¹ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ±ĞµĞ·ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Language Modeling with Continuous Diffusion",
                    "desc": "This paper presents a new continuous diffusion model designed for language modeling, which effectively handles discrete categorical data. The authors highlight the limitations of existing discrete diffusion models and propose a method that leverages the geometry of categorical distributions to enhance performance. By establishing a connection between discrete diffusion and continuous flow on a statistical manifold, they introduce a novel design that improves iterative refinement. Their experiments demonstrate that this approach not only surpasses traditional discrete models but also approaches the performance of autoregressive models."
                },
                "zh": {
                    "title": "è¿ç»­æ‰©æ•£æ¨¡å‹ï¼šæå‡ç¦»æ•£æ•°æ®å»ºæ¨¡çš„æ€§èƒ½",
                    "desc": "æ‰©æ•£æ¨¡å‹ä½œä¸ºä¸€ç§æ–°å…´çš„æ›¿ä»£è‡ªå›å½’æ¨¡å‹çš„æ–¹æ³•ï¼Œåœ¨å»ºæ¨¡ç¦»æ•£åˆ†ç±»æ•°æ®æ–¹é¢å±•ç°äº†è‰¯å¥½çš„å‰æ™¯ã€‚ç°æœ‰çš„è¿ç»­æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†ç¦»æ•£æ•°æ®æ—¶æ€§èƒ½æœ‰é™ï¼Œä¸”äºŒè€…ä¹‹é—´çš„è”ç³»ä¸æ˜ç¡®ï¼Œé™åˆ¶äº†æ‰©æ•£æ¨¡å‹çš„å‘å±•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿ç»­æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†åŸºç¡€åˆ†ç±»åˆ†å¸ƒçš„å‡ ä½•ç‰¹æ€§ï¼Œå¹¶å»ºç«‹äº†ç¦»æ•£æ‰©æ•£ä¸è¿ç»­æµåŠ¨ä¹‹é—´çš„è”ç³»ã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œæ¥è¿‘è‡ªå›å½’æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11079",
            "title": "Phantom: Subject-consistent video generation via cross-modal alignment",
            "url": "https://huggingface.co/papers/2502.11079",
            "abstract": "The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/.",
            "score": 21,
            "issue_id": 2286,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "e443a635e58b164f",
            "authors": [
                "Lijie Liu",
                "Tianxiang Ma",
                "Bingchuan Li",
                "Zhuowei Chen",
                "Jiawei Liu",
                "Qian He",
                "Xinglong Wu"
            ],
            "affiliations": [
                "Intelligent Creation Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11079.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Phantom: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Phantom - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Phantom Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ°Ñ… Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²Ğ¸Ğ´ĞµĞ¾. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Phantom: Consistent Video Generation from Text and Images",
                    "desc": "This paper introduces a new approach called Subject-to-Video, which focuses on generating videos that maintain consistency with specific subjects extracted from reference images. The authors present Phantom, a unified framework that integrates both text and image inputs to create videos, ensuring that the generated content aligns well with the provided prompts. By utilizing a triplet data structure of text, image, and video, Phantom enhances the learning of cross-modal relationships, improving the quality of video generation. The framework particularly excels in generating videos of humans while preserving their identity across frames, marking a significant advancement in video generation technology."
                },
                "zh": {
                    "title": "å®ç°ä¸»é¢˜ä¸€è‡´æ€§çš„è§†é¢‘ç”Ÿæˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œç§°ä¸ºPhantomï¼Œæ—¨åœ¨å®ç°ä¸»é¢˜ä¸€è‡´æ€§çš„è§†é¢‘ç”Ÿæˆã€‚è¯¥æ¨¡å‹é€šè¿‡æå–å‚è€ƒå›¾åƒä¸­çš„ä¸»é¢˜å…ƒç´ ï¼Œå¹¶ç»“åˆæ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆè§†é¢‘ã€‚Phantomæ¡†æ¶èƒ½å¤Ÿå¤„ç†å•ä¸€å’Œå¤šä¸ªä¸»é¢˜çš„å‚è€ƒï¼Œå¼ºè°ƒæ–‡æœ¬å’Œå›¾åƒçš„åŒæ¨¡æ€æç¤ºçš„å¹³è¡¡ã€‚ç ”ç©¶è€…ä»¬é€šè¿‡æ–‡æœ¬-å›¾åƒ-è§†é¢‘ä¸‰å…ƒç»„æ•°æ®æ¥å­¦ä¹ è·¨æ¨¡æ€å¯¹é½ï¼Œä»è€Œæå‡äººç±»ç”Ÿæˆè§†é¢‘çš„ä¸»é¢˜ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12464",
            "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models",
            "url": "https://huggingface.co/papers/2502.12464",
            "abstract": "Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on \"hard\" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model's capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines.",
            "score": 20,
            "issue_id": 2288,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "5edcf5af6c8edecb",
            "authors": [
                "Seanie Lee",
                "Dong Bok Lee",
                "Dominik Wagner",
                "Minki Kang",
                "Haebin Seong",
                "Tobias Bocklet",
                "Juho Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST",
                "Technische Hochschule NÃ¼rnberg Georg Simon Ohm"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12464.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SafeRoute Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). SafeRoute Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° 'Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ' Ğ¸ 'ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ'. Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ - Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ SafeRoute Ğ¿ĞµÑ€ĞµĞ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Smart Routing for Safer AI: Balancing Efficiency and Accuracy",
                    "desc": "This paper introduces SafeRoute, a binary router designed to improve the efficiency of safety guard models used with large language models (LLMs). The router identifies which inputs are 'hard' and require the computational power of a larger safety model, while 'easy' inputs can be handled by smaller, more efficient models. By selectively applying the larger model only to challenging cases, SafeRoute reduces overall computational costs without sacrificing safety performance. Experimental results show that this adaptive approach significantly enhances the balance between resource usage and accuracy compared to using only the larger model."
                },
                "zh": {
                    "title": "æ™ºèƒ½è·¯ç”±ï¼Œæå‡å®‰å…¨ä¸æ•ˆç‡ï¼",
                    "desc": "åœ¨å®é™…åº”ç”¨ä¸­éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éœ€è¦å¼ºå¤§çš„å®‰å…¨é˜²æŠ¤æ¨¡å‹æ¥æ£€æµ‹å’Œé˜»æ­¢æœ‰å®³çš„ç”¨æˆ·æç¤ºã€‚è™½ç„¶å¤§å‹å®‰å…¨é˜²æŠ¤æ¨¡å‹çš„æ€§èƒ½å¾ˆå¼ºï¼Œä½†å…¶è®¡ç®—æˆæœ¬ä¹Ÿå¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…ä»¬ä½¿ç”¨äº†è¾ƒå°çš„è’¸é¦æ¨¡å‹ï¼Œä½†åœ¨å¤„ç†â€œå›°éš¾â€ç¤ºä¾‹æ—¶ï¼Œå®ƒä»¬çš„è¡¨ç°å¾€å¾€ä¸å¦‚å¤§å‹æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†SafeRouteï¼Œä¸€ä¸ªäºŒå…ƒè·¯ç”±å™¨ï¼Œå¯ä»¥åŒºåˆ†å›°éš¾ç¤ºä¾‹å’Œç®€å•ç¤ºä¾‹ï¼Œä»è€Œæé«˜æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13131",
            "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis",
            "url": "https://huggingface.co/papers/2502.13131",
            "abstract": "Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment.",
            "score": 20,
            "issue_id": 2286,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "b3376cde29e0b44f",
            "authors": [
                "Feng Luo",
                "Rui Yang",
                "Hao Sun",
                "Chunyuan Deng",
                "Jiarui Yao",
                "Jingyan Shen",
                "Huan Zhang",
                "Hanjie Chen"
            ],
            "affiliations": [
                "Columbia University",
                "Rice University",
                "University of Cambridge",
                "University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13131.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rlhf",
                    "#training",
                    "#dataset",
                    "#interpretability"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ… - Decomposed Reward Models (DRMs). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ (PCA) Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¸ÑĞ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. DRMs Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DRMs ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Decomposed Reward Models: Unlocking Diverse Human Preferences for Personalized AI",
                    "desc": "This paper presents Decomposed Reward Models (DRMs), a new method for understanding human preferences in AI systems. Instead of relying on detailed preference data, DRMs use binary comparisons to extract diverse preferences, making the process more scalable and cost-effective. By representing preferences as vectors and applying Principal Component Analysis (PCA), the model identifies key dimensions of preference that can be combined to meet different user needs. The results show that DRMs can effectively adapt to new users and provide interpretable insights into preferences like helpfulness and safety."
                },
                "zh": {
                    "title": "åˆ†è§£å¥–åŠ±æ¨¡å‹ï¼šä¸ªæ€§åŒ–AIçš„æ–°æ–¹æ³•",
                    "desc": "ç†è§£äººç±»åå¥½å¯¹äºæ”¹å–„åŸºç¡€æ¨¡å‹å’Œæ„å»ºä¸ªæ€§åŒ–AIç³»ç»Ÿè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹éš¾ä»¥æ•æ‰åå¥½çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºåˆ†è§£å¥–åŠ±æ¨¡å‹ï¼ˆDRMsï¼‰ï¼Œå®ƒé€šè¿‡äºŒå…ƒæ¯”è¾ƒæå–äººç±»åå¥½ï¼Œè€Œæ— éœ€ç»†ç²’åº¦çš„æ³¨é‡Šã€‚DRMsåˆ©ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰åˆ†æåå¥½å‘é‡ï¼Œèƒ½å¤Ÿçµæ´»ç»„åˆä»¥æ»¡è¶³ä¸åŒç”¨æˆ·éœ€æ±‚ï¼Œæä¾›äº†ä¸€ç§å¯è§£é‡Šä¸”å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13143",
            "title": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation",
            "url": "https://huggingface.co/papers/2502.13143",
            "abstract": "Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientations-a key requirement for tasks involving fine-grained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers a more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a knife). To support this, we construct OrienText300K, a large-scale dataset of 3D models annotated with semantic orientations that link geometric understanding to functional semantics. By integrating semantic orientation into a VLM system, we enable robots to generate manipulation actions with both positional and orientational constraints. Extensive experiments in simulation and real world demonstrate that our approach significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER.",
            "score": 20,
            "issue_id": 2286,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "7aab95a55286d8b2",
            "authors": [
                "Zekun Qi",
                "Wenyao Zhang",
                "Yufei Ding",
                "Runpei Dong",
                "Xinqiang Yu",
                "Jingwen Li",
                "Lingyun Xu",
                "Baoyu Li",
                "Xialin He",
                "Guofan Fan",
                "Jiazhao Zhang",
                "Jiawei He",
                "Jiayuan Gu",
                "Xin Jin",
                "Kaisheng Ma",
                "Zhizheng Zhang",
                "He Wang",
                "Li Yi"
            ],
            "affiliations": [
                "Eastern Institute of Technology",
                "Galbot",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Shanghai Qi Zhi Institute",
                "ShanghaiTech University",
                "Tsinghua University",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13143.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#games",
                    "#dataset",
                    "#reasoning",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ OrienText300K Ñ 3D Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ (VLM) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑÑ… Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering Robots with Semantic Orientation for Enhanced Manipulation",
                    "desc": "This paper addresses the challenge of teaching robots to understand object orientations, which is essential for precise manipulation tasks. It introduces the concept of semantic orientation, allowing robots to interpret orientations using natural language instead of traditional geometric frames. The authors present OrienText300K, a dataset of 3D models annotated with these semantic orientations, linking geometric understanding to functional semantics. By incorporating this approach into vision-language models (VLMs), the paper demonstrates improved accuracy in robotic manipulation tasks, showcasing the effectiveness of using natural language for orientation representation."
                },
                "zh": {
                    "title": "ç”¨è‡ªç„¶è¯­è¨€æå‡æœºå™¨äººçš„ç©ºé—´æ™ºèƒ½",
                    "desc": "ç©ºé—´æ™ºèƒ½æ˜¯å…·èº«äººå·¥æ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¸®åŠ©æœºå™¨äººç†è§£å’Œä¸ç¯å¢ƒäº’åŠ¨ã€‚å°½ç®¡æœ€è¿‘çš„è¿›å±•æé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹ç‰©ä½“ä½ç½®å’Œå…³ç³»çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶ç¼ºä¹ç²¾ç¡®ç†è§£ç‰©ä½“æ–¹å‘çš„èƒ½åŠ›ï¼Œè¿™å¯¹äºç»†è‡´æ“ä½œä»»åŠ¡è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰æ–¹å‘çš„æ¦‚å¿µï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€ä»¥æ— å‚è€ƒæ¡†æ¶çš„æ–¹å¼å®šä¹‰ç‰©ä½“æ–¹å‘ã€‚é€šè¿‡æ„å»ºOrienText300Kæ•°æ®é›†ï¼Œæˆ‘ä»¬å°†å‡ ä½•ç†è§£ä¸åŠŸèƒ½è¯­ä¹‰è”ç³»èµ·æ¥ï¼Œä»è€Œæå‡æœºå™¨äººåœ¨æ“ä½œä¸­çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11433",
            "title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading",
            "url": "https://huggingface.co/papers/2502.11433",
            "abstract": "Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose FLAG-Trader, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.",
            "score": 16,
            "issue_id": 2286,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "0731051fe5131889",
            "authors": [
                "Guojun Xiong",
                "Zhiyang Deng",
                "Keyi Wang",
                "Yupeng Cao",
                "Haohang Li",
                "Yangyang Yu",
                "Xueqing Peng",
                "Mingquan Lin",
                "Kaleb E Smith",
                "Xiao-Yang Liu",
                "Jimin Huang",
                "Sophia Ananiadou",
                "Qianqian Xie"
            ],
            "affiliations": [
                "Columbia University",
                "Harvard University",
                "NVIDIA",
                "Rensselaer Polytechnic Institute",
                "Stevens Institute of Technology",
                "TheFinAI",
                "University of Manchester",
                "University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11433.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#architecture",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#multimodal"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° FLAG-Trader, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹. LLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑÑŒ Ğº Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ğµ, Ğ½Ğ¾ Ğ¸ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering Financial Trading with FLAG-Trader: Merging Language and Reinforcement Learning",
                    "desc": "This paper introduces FLAG-Trader, a new architecture that combines large language models (LLMs) with reinforcement learning (RL) to improve decision-making in financial trading. The approach uses a partially fine-tuned LLM as a policy network, allowing it to utilize its pre-trained knowledge while adapting specifically to financial tasks. By applying policy gradient optimization based on trading rewards, FLAG-Trader enhances the performance of LLMs in trading scenarios and other financial applications. The authors provide extensive empirical evidence demonstrating the effectiveness of their proposed method."
                },
                "zh": {
                    "title": "FLAG-Traderï¼šæå‡é‡‘èå†³ç­–çš„æ™ºèƒ½äº¤æ˜“æ¶æ„",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFLAG-Traderçš„ç»Ÿä¸€æ¶æ„ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é‡‘èå¸‚åœºä¸­çš„å†³ç­–èƒ½åŠ›ã€‚è¯¥æ¶æ„ç»“åˆäº†è¯­è¨€å¤„ç†å’ŒåŸºäºæ¢¯åº¦çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­–ç•¥ä¼˜åŒ–ï¼Œä½¿å¾—éƒ¨åˆ†å¾®è°ƒçš„LLMå¯ä»¥ä½œä¸ºç­–ç•¥ç½‘ç»œï¼Œåˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†å¹¶é€‚åº”é‡‘èé¢†åŸŸã€‚é€šè¿‡äº¤æ˜“å¥–åŠ±é©±åŠ¨çš„ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼ŒFLAG-Traderä¸ä»…æé«˜äº†LLMåœ¨äº¤æ˜“ä¸­çš„è¡¨ç°ï¼Œè¿˜æ”¹å–„äº†å…¶ä»–é‡‘èé¢†åŸŸä»»åŠ¡çš„ç»“æœã€‚æˆ‘ä»¬æä¾›äº†å¤§é‡å®è¯è¯æ®æ¥éªŒè¯è¿™äº›æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13145",
            "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation",
            "url": "https://huggingface.co/papers/2502.13145",
            "abstract": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6times speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5times speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba",
            "score": 14,
            "issue_id": 2286,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "6810113d41bfd26d",
            "authors": [
                "Bencheng Liao",
                "Hongyuan Tao",
                "Qian Zhang",
                "Tianheng Cheng",
                "Yingyue Li",
                "Haoran Yin",
                "Wenyu Liu",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Horizon Robotics",
                "Institute of Artificial Intelligence, Huazhong University of Science & Technology",
                "School of EIC, Huazhong University of Science & Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13145.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#transfer_learning",
                    "#architecture",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "mmMamba: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ mmMamba - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ state space models. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… RNN-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ»Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Mamba Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Transformer Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ mmMamba Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Efficient Multimodal Models with mmMamba",
                    "desc": "The paper introduces mmMamba, a new framework designed to create efficient multimodal large language models (MLLMs) with linear computational complexity. It utilizes a progressive distillation process to convert existing decoder-only MLLMs into more efficient architectures without needing separate vision encoders or pre-trained RNNs. The proposed seeding strategy and three-stage distillation recipe allow for effective knowledge transfer from Transformer models while maintaining multimodal capabilities. The results show that mmMamba-linear and mmMamba-hybrid significantly outperform traditional models in terms of speed and memory usage, making them suitable for practical deployment."
                },
                "zh": {
                    "title": "mmMambaï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨¡å‹æ¶æ„",
                    "desc": "æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºå…¶è®¡ç®—å¤æ‚åº¦å‘ˆå¹³æ–¹å¢é•¿ã€å¯¹é”®å€¼ç¼“å­˜çš„éœ€æ±‚å¢åŠ ä»¥åŠä¾èµ–äºç‹¬ç«‹çš„è§†è§‰ç¼–ç å™¨ï¼Œé¢ä¸´éƒ¨ç½²æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†mmMambaæ¡†æ¶ï¼Œé€šè¿‡ä»ç°æœ‰çš„MLLMsè¿›è¡Œæ¸è¿›è’¸é¦ï¼Œå¼€å‘çº¿æ€§å¤æ‚åº¦çš„æœ¬åœ°å¤šæ¨¡æ€çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œä½¿ç”¨é€‚åº¦çš„å­¦æœ¯è®¡ç®—èµ„æºã€‚è¯¥æ–¹æ³•å…è®¸å°†è®­ç»ƒå¥½çš„ä»…è§£ç å™¨MLLMsç›´æ¥è½¬æ¢ä¸ºçº¿æ€§å¤æ‚åº¦æ¶æ„ï¼Œè€Œæ— éœ€é¢„è®­ç»ƒçš„åŸºäºRNNçš„LLMæˆ–è§†è§‰ç¼–ç å™¨ã€‚æˆ‘ä»¬çš„è’¸é¦ç­–ç•¥æœ‰æ•ˆåœ°å°†çŸ¥è¯†ä»Transformerè½¬ç§»åˆ°Mambaï¼ŒåŒæ—¶ä¿ç•™å¤šæ¨¡æ€èƒ½åŠ›ï¼Œå¹¶æ”¯æŒçµæ´»çš„æ··åˆæ¶æ„ï¼Œä»¥å®ç°å¯å®šåˆ¶çš„æ•ˆç‡ä¸æ€§èƒ½æƒè¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12513",
            "title": "RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm",
            "url": "https://huggingface.co/papers/2502.12513",
            "abstract": "After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn achieve state-of-the-art performance on multiple downstream tasks. To facilitate future research, the RealSyn dataset and pre-trained model weights are released at https://github.com/deepglint/RealSyn.",
            "score": 9,
            "issue_id": 2286,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "1ed14365f683281c",
            "authors": [
                "Tiancheng Gu",
                "Kaicheng Yang",
                "Chaoyi Zhang",
                "Yin Xie",
                "Xiang An",
                "Ziyong Feng",
                "Dongnan Liu",
                "Weidong Cai",
                "Jiankang Deng"
            ],
            "affiliations": [
                "DeepGlint",
                "Imperial College London",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12513.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#data",
                    "#synthetic",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "RealSyn: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ RealSyn, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ RealSyn, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Unlocking Vision-Language Learning with RealSyn Dataset",
                    "desc": "This paper introduces RealSyn, a new dataset designed to improve vision-language representation learning by utilizing both realistic and synthetic texts. The authors develop a data extraction pipeline to gather high-quality images and texts from unpaired multimodal documents. They also implement a hierarchical retrieval method to link images with relevant texts and propose an image semantic augmented generation module to create synthetic text. The resulting dataset, RealSyn, shows significant improvements in model performance on various tasks, demonstrating its effectiveness and scalability in the field of machine learning."
                },
                "zh": {
                    "title": "åˆ©ç”¨æœªé…å¯¹æ•°æ®æå‡è§†è§‰-è¯­è¨€å­¦ä¹ çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ•°æ®é›†RealSynï¼Œç”¨äºè§†è§‰-è¯­è¨€è¡¨ç¤ºå­¦ä¹ ã€‚ç ”ç©¶è€…ä»¬é€šè¿‡æå–é«˜è´¨é‡çš„å›¾åƒå’Œæ–‡æœ¬ï¼Œåˆ©ç”¨æœªé…å¯¹çš„æ•°æ®æ¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºäº†æ›´å¥½åœ°å…³è”å›¾åƒå’Œæ–‡æœ¬ï¼Œè®¾è®¡äº†ä¸€ç§å±‚æ¬¡æ£€ç´¢æ–¹æ³•ï¼Œå¹¶æå‡ºäº†å›¾åƒè¯­ä¹‰å¢å¼ºç”Ÿæˆæ¨¡å—æ¥ç”Ÿæˆåˆæˆæ–‡æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºRealSyné¢„è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12501",
            "title": "Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge",
            "url": "https://huggingface.co/papers/2502.12501",
            "abstract": "LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly rely on majority voting or criteria expansion, which is insufficient to address the limitation in CoT. We propose Crowd-based Comparative Evaluation, which introduces additional crowd responses to compare with the candidate responses, thereby exposing deeper and more comprehensive details within the candidate responses. This process effectively guides LLM-as-a-Judge to provide a more detailed CoT judgment. Extensive experiments demonstrate that our approach enhances evaluation reliability, achieving an average accuracy gain of 6.7% across five benchmarks. Moreover, our method produces higher-quality CoTs that facilitate judge distillation and exhibit superior performance in rejection sampling for supervised fine-tuning (SFT), referred to as crowd rejection sampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs generated by ours are more comprehensive and of higher quality, and evaluation accuracy improves as inference scales.",
            "score": 4,
            "issue_id": 2286,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "1c26233f72f504ee",
            "authors": [
                "Qiyuan Zhang",
                "Yufei Wang",
                "Yuxin Jiang",
                "Liangyou Li",
                "Chuhan Wu",
                "Yasheng Wang",
                "Xin Jiang",
                "Lifeng Shang",
                "Ruiming Tang",
                "Fuyuan Lyu",
                "Chen Ma"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "Huawei Noahs Ark Lab",
                "McGill University & MILA",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12501.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#inference",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»Ğ¿Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Crowd-based Comparative Evaluation'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ LLM-as-a-Judge, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ñ‚Ğ¾Ğ»Ğ¿Ñ‹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 6.7% Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought) Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing LLM Evaluations with Crowd Insights",
                    "desc": "This paper addresses the limitations of the LLM-as-a-Judge method, which generates chain-of-thought (CoT) judgments for auto-evaluation. The authors identify that CoT reasoning often lacks depth and comprehensiveness, leading to incomplete evaluations. To overcome this, they propose a new method called Crowd-based Comparative Evaluation, which incorporates additional crowd responses to enhance the evaluation process. Their experiments show that this approach improves the reliability of evaluations, increases accuracy by 6.7%, and produces higher-quality CoTs that benefit supervised fine-tuning."
                },
                "zh": {
                    "title": "æå‡LLMè¯„ä¼°å¯é æ€§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºäººç¾¤çš„æ¯”è¾ƒè¯„ä¼°ï¼ˆCrowd-based Comparative Evaluationï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„åˆ¤è€…çš„å¯é æ€§ã€‚ä¼ ç»Ÿçš„é“¾å¼æ¨ç†ï¼ˆCoTï¼‰æ–¹æ³•å¸¸å¸¸æ— æ³•æ•æ‰åˆ°å…¨é¢å’Œæ·±å…¥çš„ç»†èŠ‚ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å®Œæ•´ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å…¥é¢å¤–çš„äººç¾¤åé¦ˆï¼Œä¸å€™é€‰å“åº”è¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œæ­ç¤ºå€™é€‰å“åº”ä¸­çš„æ›´æ·±å±‚æ¬¡å’Œæ›´å…¨é¢çš„ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†6.7%çš„è¯„ä¼°å‡†ç¡®æ€§ï¼Œå¹¶ç”Ÿæˆäº†æ›´é«˜è´¨é‡çš„CoTï¼Œä¿ƒè¿›äº†ç›‘ç£å¾®è°ƒçš„æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12215",
            "title": "Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?",
            "url": "https://huggingface.co/papers/2502.12215",
            "abstract": "The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose Shortest Majority Vote, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches.",
            "score": 3,
            "issue_id": 2288,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "a02df3e32ba854de",
            "authors": [
                "Zhiyuan Zeng",
                "Qinyuan Cheng",
                "Zhangyue Yin",
                "Yunhua Zhou",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "School of Computer Science, Fudan University, Shanghai, China",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12215.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ¾Ñ€Ğ¾Ñ‡Ğµ - Ğ»ÑƒÑ‡ÑˆĞµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€Ğ¾Ñ‡Ğµ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ ÑĞ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑ…ÑƒĞ´ÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Shortest Majority Vote, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing LLM Performance with Shorter, Smarter Reasoning",
                    "desc": "This paper investigates the concept of test-time scaling in large language models (LLMs), particularly focusing on the o1 series by OpenAI and its successors. It reveals that longer chains of thought (CoTs) do not always lead to better accuracy, as shorter CoTs can yield correct answers more frequently. The study highlights that the presence of self-revisions in longer CoTs can negatively impact performance. To enhance test-time scalability, the authors propose a new method called Shortest Majority Vote, which integrates parallel scaling strategies with CoT length characteristics, outperforming traditional majority voting methods."
                },
                "zh": {
                    "title": "æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯OpenAIçš„o1ç³»åˆ—ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ä¸€äº›åç»­æ¨¡å‹å¦‚QwQå’ŒDeepseek-R1æ¨¡ä»¿äº†è¿™äº›è¿›å±•ï¼Œä½†å®ƒä»¬çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†éªŒè¯ã€‚æ›´é•¿çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å¹¶ä¸æ€»æ˜¯æé«˜å‡†ç¡®æ€§ï¼Œåè€Œæ­£ç¡®ç­”æ¡ˆå¾€å¾€æ¯”é”™è¯¯ç­”æ¡ˆæ›´çŸ­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æœ€çŸ­å¤šæ•°æŠ•ç¥¨ï¼Œç»“åˆå¹¶è¡Œç¼©æ”¾ç­–ç•¥å’ŒCoTé•¿åº¦ç‰¹å¾ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æµ‹è¯•æ—¶é—´å¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12170",
            "title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections",
            "url": "https://huggingface.co/papers/2502.12170",
            "abstract": "We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer .",
            "score": 3,
            "issue_id": 2287,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "f06bcb1b611b7c39",
            "authors": [
                "Da Xiao",
                "Qingye Meng",
                "Shengping Li",
                "Xingyuan Yuan"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications, Beijing, China",
                "ColorfulClouds Technology Co., Ltd., Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12170.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MUDD (MUltiway Dynamic Dense) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Transformer. MUDD Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑĞ° ÑĞ²ÑĞ·ĞµĞ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ±Ğ»Ğ¾ĞºĞ° Transformer. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MUDDFormer Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Transformer-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² 1.8-2.4 Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ MUDDPythia-2.8B ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ° Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Pythia-6.9B Ğ¸ Ğ´Ğ°Ğ¶Ğµ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Pythia-12B Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ²ÑĞµĞ³Ğ¾ 0.23% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ 0.4% Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Dynamic Connections for Enhanced Transformer Performance",
                    "desc": "The paper introduces MUDD connections, which improve the flow of information between layers in Transformer models. Unlike traditional residual connections that use fixed weights, MUDD connections adaptively generate weights based on the hidden states of the input at each position. This dynamic approach allows for better integration of information from different input streams, enhancing the overall performance of the model. The proposed MUDDFormer architecture shows significant improvements in language modeling tasks, achieving results comparable to larger models while maintaining a smaller parameter count."
                },
                "zh": {
                    "title": "åŠ¨æ€è¿æ¥ï¼Œæå‡Transformeræ€§èƒ½ï¼",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå¤šå‘åŠ¨æ€ç¨ å¯†è¿æ¥ï¼ˆMUDDï¼‰çš„ç®€å•æœ‰æ•ˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ®‹å·®è¿æ¥çš„å±€é™æ€§ï¼Œå¹¶å¢å¼ºTransformerä¸­è·¨å±‚ä¿¡æ¯æµåŠ¨ã€‚ä¸ç°æœ‰çš„é™æ€å…±äº«è¿æ¥æƒé‡çš„ç¨ å¯†è¿æ¥æ–¹æ³•ä¸åŒï¼ŒMUDDæ ¹æ®æ¯ä¸ªåºåˆ—ä½ç½®çš„éšè—çŠ¶æ€åŠ¨æ€ç”Ÿæˆè¿æ¥æƒé‡ï¼Œå¹¶é’ˆå¯¹Transformerå—çš„æ¯ä¸ªè§£è€¦è¾“å…¥æµï¼ˆæŸ¥è¯¢ã€é”®ã€å€¼æˆ–æ®‹å·®ï¼‰ã€‚MUDDè¿æ¥å¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•Transformeræ¶æ„ä¸­ï¼Œå½¢æˆMUDDFormerã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMUDDFormeråœ¨è¯­è¨€å»ºæ¨¡ä¸­æ˜¾è‘—è¶…è¶Šäº†å„ç§æ¨¡å‹æ¶æ„å’Œè§„æ¨¡çš„Transformerï¼Œè¡¨ç°å‡ºä¸è®­ç»ƒæ—¶è®¡ç®—é‡ä¸º1.8X-2.4Xçš„Transformerç›¸å½“çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13130",
            "title": "Magma: A Foundation Model for Multimodal AI Agents",
            "url": "https://huggingface.co/papers/2502.13130",
            "abstract": "We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pretrained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to a wide range of tasks as shown in Fig.1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility at https://microsoft.github.io/Magma.",
            "score": 2,
            "issue_id": 2288,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "1851b242ac65c88f",
            "authors": [
                "Jianwei Yang",
                "Reuben Tan",
                "Qianhui Wu",
                "Ruijie Zheng",
                "Baolin Peng",
                "Yongyuan Liang",
                "Yu Gu",
                "Mu Cai",
                "Seonghyeon Ye",
                "Joel Jang",
                "Yuquan Deng",
                "Lars Liden",
                "Jianfeng Gao"
            ],
            "affiliations": [
                "KAIST",
                "Microsoft Research",
                "University of Maryland",
                "University of Washington",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13130.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#robotics",
                    "#agi",
                    "#multimodal",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Magma: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Magma - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. Magma Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Set-of-Mark Ğ¸ Trace-of-Mark. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Magma Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑƒ Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Magma: Bridging Digital and Physical Worlds with Multimodal Intelligence",
                    "desc": "Magma is a foundation model designed for multimodal AI tasks that operate in both digital and physical environments. It enhances traditional vision-language models by integrating spatial-temporal intelligence, allowing it to plan and execute actions in real-world scenarios. The model is pretrained on diverse datasets, utilizing Set-of-Mark (SoM) for action grounding and Trace-of-Mark (ToM) for action planning, which together improve its ability to understand and interact with visual-spatial elements. Magma achieves state-of-the-art performance in UI navigation and robotic manipulation, surpassing specialized models and competing effectively with larger multimodal models."
                },
                "zh": {
                    "title": "Magmaï¼šå¤šæ¨¡æ€æ™ºèƒ½çš„æœªæ¥",
                    "desc": "Magmaæ˜¯ä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æ•°å­—å’Œç‰©ç†ä¸–ç•Œä¸­çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ä»»åŠ¡ã€‚å®ƒä¸ä»…ä¿ç•™äº†è§†è§‰-è¯­è¨€æ¨¡å‹çš„ç†è§£èƒ½åŠ›ï¼Œè¿˜å…·å¤‡åœ¨è§†è§‰ç©ºé—´ä¸­è§„åˆ’å’Œè¡ŒåŠ¨çš„èƒ½åŠ›ã€‚Magmaé€šè¿‡å¤§é‡å¼‚æ„æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™äº›æ•°æ®é›†åŒ…æ‹¬å›¾åƒã€è§†é¢‘å’Œæœºå™¨äººæ•°æ®ï¼Œä½¿ç”¨Set-of-Markå’ŒTrace-of-Markè¿›è¡ŒåŠ¨ä½œæ ‡å®šã€‚å®éªŒè¡¨æ˜ï¼ŒMagmaåœ¨ç”¨æˆ·ç•Œé¢å¯¼èˆªå’Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šåˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œè¶…è¶Šäº†ä¸“é—¨ä¸ºè¿™äº›ä»»åŠ¡è®¾è®¡çš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09838",
            "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
            "url": "https://huggingface.co/papers/2502.09838",
            "abstract": "We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.",
            "score": 2,
            "issue_id": 2287,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "ce920c7d40d11dad",
            "authors": [
                "Tianwei Lin",
                "Wenqiao Zhang",
                "Sijing Li",
                "Yuqian Yuan",
                "Binhe Yu",
                "Haoyuan Li",
                "Wanggui He",
                "Hao Jiang",
                "Mengze Li",
                "Xiaohui Song",
                "Siliang Tang",
                "Jun Xiao",
                "Hui Lin",
                "Yueting Zhuang",
                "Beng Chin Ooi"
            ],
            "affiliations": [
                "Alibaba",
                "National University of Singapore",
                "The Hong Kong University of Science and Technology",
                "University of Electronic Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09838.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#cv",
                    "#dataset",
                    "#healthcare"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "HealthGPT: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "HealthGPT - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ H-LoRA Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VL-Health. HealthGPT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Medical AI with HealthGPT",
                    "desc": "HealthGPT is a Medical Large Vision-Language Model that combines understanding and generating medical images and text in one system. It uses a unique method called heterogeneous low-rank adaptation (H-LoRA) to enhance pre-trained large language models with diverse medical knowledge. The model is trained on a specialized dataset named VL-Health, which focuses on medical comprehension and generation tasks. Results show that HealthGPT performs exceptionally well in various medical visual tasks, demonstrating its effectiveness and scalability."
                },
                "zh": {
                    "title": "HealthGPTï¼šåŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ•´åˆ",
                    "desc": "æˆ‘ä»¬æå‡ºäº†HealthGPTï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§çš„åŒ»ç–—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMed-LVLMï¼‰ï¼Œå®ƒå°†åŒ»ç–—è§†è§‰ç†è§£å’Œç”Ÿæˆèƒ½åŠ›æ•´åˆåœ¨ä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’æ¡†æ¶ä¸­ã€‚æˆ‘ä»¬çš„è‡ªä¸¾ç†å¿µæ˜¯é€æ­¥å°†å¼‚æ„çš„ç†è§£å’Œç”ŸæˆçŸ¥è¯†é€‚åº”äºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚è¿™é€šè¿‡ä¸€ç§æ–°é¢–çš„å¼‚æ„ä½ç§©é€‚åº”ï¼ˆH-LoRAï¼‰æŠ€æœ¯å®ç°ï¼Œå¹¶è¾…ä»¥å®šåˆ¶çš„åˆ†å±‚è§†è§‰æ„ŸçŸ¥æ–¹æ³•å’Œä¸‰é˜¶æ®µå­¦ä¹ ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHealthGPTåœ¨åŒ»ç–—è§†è§‰ç»Ÿä¸€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12574",
            "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
            "url": "https://huggingface.co/papers/2502.12574",
            "abstract": "Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods.",
            "score": 1,
            "issue_id": 2286,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "6e5de7c584198857",
            "authors": [
                "Cheng Luo",
                "Zefan Cai",
                "Hanshi Sun",
                "Jinqi Xiao",
                "Bo Yuan",
                "Wen Xiao",
                "Junjie Hu",
                "Jiawei Zhao",
                "Beidi Chen",
                "Anima Anandkumar"
            ],
            "affiliations": [
                "California Institute of Technology",
                "Carnegie Mellon University",
                "Microsoft",
                "Rutgers University",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12574.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#inference",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "HEADINFER: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ LLM Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "HEADINFER - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ³Ñ€ÑƒĞ¶Ğ°Ñ‚ÑŒ ĞºÑÑˆ ĞºĞ»ÑÑ‡ĞµĞ¹-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ (KV cache) Ğ² Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ CPU, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ½Ğ° GPU. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, HEADINFER ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama-3-8B Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ² 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ½Ğ° 92% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼."
                },
                "en": {
                    "title": "HEADINFER: Efficient Memory Management for Long Contexts in LLMs",
                    "desc": "This paper introduces HEADINFER, a novel approach to manage the memory usage of large language models (LLMs) during long context generation. By offloading the key-value (KV) cache to CPU RAM, HEADINFER reduces the reliance on GPU memory, which is crucial for efficient inference. The method employs a fine-grained, head-wise offloading strategy, allowing selective storage of attention heads on the GPU while dynamically computing attention outputs. Evaluation on the Llama-3-8B model shows a remarkable reduction in GPU memory usage, enabling efficient processing of extremely long sequences without compromising performance."
                },
                "zh": {
                    "title": "HEADINFERï¼šä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„å†…å­˜ä½¿ç”¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºHEADINFERçš„æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆä¸­çš„å†…å­˜ä½¿ç”¨ã€‚é€šè¿‡å°†å…³é”®å€¼ç¼“å­˜ï¼ˆKVç¼“å­˜ï¼‰è½¬ç§»åˆ°CPU RAMï¼ŒHEADINFERé¿å…äº†åœ¨GPUä¸Šå®Œå…¨å­˜å‚¨KVç¼“å­˜çš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ç»†ç²’åº¦çš„å¤´éƒ¨çº§åˆ«å¸è½½ç­–ç•¥ï¼Œä»…åœ¨GPUä¸Šä¿ç•™é€‰æ‹©æ€§çš„æ³¨æ„åŠ›å¤´KVç¼“å­˜ï¼ŒåŒæ—¶åŠ¨æ€è®¡ç®—æ³¨æ„åŠ›è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHEADINFERåœ¨æ˜¾è‘—å‡å°‘å†…å­˜å ç”¨çš„åŒæ—¶ï¼Œä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼Œä½¿å¾—åœ¨å•ä¸ªæ¶ˆè´¹çº§GPUä¸Šå®ç°äº†å¯¹é•¿è¾¾400ä¸‡æ ‡è®°çš„æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10852",
            "title": "Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages",
            "url": "https://huggingface.co/papers/2502.10852",
            "abstract": "While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.",
            "score": 0,
            "issue_id": 2287,
            "pub_date": "2025-02-15",
            "pub_date_card": {
                "ru": "15 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 15",
                "zh": "2æœˆ15æ—¥"
            },
            "hash": "11a782268b627ec1",
            "authors": [
                "Zeli Su",
                "Ziyin Zhang",
                "Guixian Xu",
                "Jianing Liu",
                "XU Han",
                "Ting Zhang",
                "Yushuang Dong"
            ],
            "affiliations": [
                "Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of MOE",
                "Minzu University of China",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10852.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ°: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ°Ñ… Ñ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ğ¼ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ², Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ XLM-SWCM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ XLM-SWCM Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ°Ğ¶Ğµ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering Low-Resource Languages with Efficient Multilingual Models",
                    "desc": "This paper addresses the challenges faced by multilingual language models in generating text for extremely low-resource languages. It introduces a new framework that adapts multilingual encoders for text generation by reusing weights between the encoder and decoder. This approach allows the model to utilize the semantic knowledge learned by the encoder, leading to better performance in low-resource settings. The authors demonstrate the effectiveness of their framework, named XLM-SWCM, on four Chinese minority languages, showing that it outperforms larger models in various tasks."
                },
                "zh": {
                    "title": "ä¸ºæä½èµ„æºè¯­è¨€èµ‹èƒ½çš„å¤šè¯­è¨€æ–‡æœ¬ç”Ÿæˆæ¡†æ¶",
                    "desc": "å¤šè¯­è¨€æ¨¡å‹å¦‚XLM-Råœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¤šè¯­è¨€èƒ½åŠ›æœ‰æ‰€æå‡ï¼Œä½†åœ¨æä½èµ„æºè¯­è¨€ä¸Šè¡¨ç°ä»ç„¶è¾ƒå·®ã€‚ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹å¦‚LLaMAå’ŒQwenæ”¯æŒçš„è¯­è¨€æ•°é‡è¿œå°‘äºXLM-Rï¼Œå¯¼è‡´è®¸å¤šè¯­è¨€ç¼ºä¹æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œå°†å¤šè¯­è¨€ç¼–ç å™¨é€‚åº”äºæä½èµ„æºè¯­è¨€çš„æ–‡æœ¬ç”Ÿæˆã€‚é€šè¿‡é‡ç”¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„æƒé‡ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨ç¼–ç å™¨å­¦ä¹ åˆ°çš„è¯­ä¹‰ç©ºé—´ï¼Œä»è€Œåœ¨ä½èµ„æºè¯­è¨€ä¸­å®ç°é«˜æ•ˆå­¦ä¹ å’Œæœ‰æ•ˆæ³›åŒ–ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-18.html",
    "link_next": "2025-02-20.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "18.02",
        "en": "02/18",
        "zh": "2æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.02",
        "en": "02/20",
        "zh": "2æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 1,
        "#training": 11,
        "#robotics": 2,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†äººå½¢æœºå™¨äººè‡ªåŠ¨è·Œå€’æ¢å¤çš„é‡è¦æ€§ã€‚è®¾è®¡æ§åˆ¶å™¨è®©æœºå™¨äººç«™èµ·æ¥å¾ˆéš¾ï¼Œå› ä¸ºè·Œå€’åæœºå™¨äººå¯èƒ½å¤„äºå„ç§å§¿æ€ï¼Œä¸”åœ°å½¢å¤æ‚ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªå­¦ä¹ æ¡†æ¶ï¼Œè®©æœºå™¨äººåœ¨ä¸åŒå§¿æ€å’Œåœ°å½¢ä¸‹ç«™èµ·æ¥ã€‚æ¡†æ¶åˆ†ä¸¤é˜¶æ®µï¼Œå…ˆæ‰¾åˆ°ç«™èµ·æ¥çš„è·¯å¾„ï¼Œå†ä¼˜åŒ–åŠ¨ä½œä½¿å…¶å¹³ç¨³å¯é ã€‚å®éªŒä¸­ï¼Œæœºå™¨äººåœ¨ä¸åŒåœ°é¢å’Œå§¿æ€ä¸‹æˆåŠŸç«™èµ·æ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨çœŸå®ç¯å¢ƒä¸­æˆåŠŸåº”ç”¨çš„å­¦ä¹ æ–¹æ³•ã€‚",
        "title": "Learning Getting-Up Policies for Real-World Humanoid Robots",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†äººå½¢æœºå™¨äººè‡ªåŠ¨è·Œå€’æ¢å¤çš„é‡è¦æ€§ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le rÃ©nxÃ­ng jÄ«qÃ¬rÃ©n zÃ¬dÃ²ng diÄ“dÇo huÄ«fÃ¹ de zhÃ²ngyÃ oxÃ¬ng.\n\nè®¾è®¡æ§åˆ¶å™¨è®©æœºå™¨äººç«™èµ·æ¥å¾ˆéš¾ï¼Œå› ä¸ºè·Œå€’åæœºå™¨äººå¯èƒ½å¤„äºå„ç§å§¿æ€ï¼Œä¸”åœ°å½¢å¤æ‚ã€‚\nShÃ¨jÃ¬ kÃ²ngzhÃ¬qÃ¬ rÃ ng jÄ«qÃ¬rÃ©n zhÃ n qÇlÃ¡i hÄ›n nÃ¡n, yÄ«nwÃ¨i diÄ“dÇo hÃ²u jÄ«qÃ¬rÃ©n kÄ›nÃ©ng chÇ”yÃº gÃ¨zhÇ’ng zÄ«tÃ i, qiÄ› dÃ¬xÃ­ng fÃ¹zÃ¡.\n\næ–‡ç« æå‡ºäº†ä¸€ä¸ªå­¦ä¹ æ¡†æ¶ï¼Œè®©æœºå™¨äººåœ¨ä¸åŒå§¿æ€å’Œåœ°å½¢ä¸‹ç«™èµ·æ¥ã€‚\nWÃ©nzhÄng tÃ­chÅ« le yÄ«gÃ¨ xuÃ©xÃ­ kuÃ ngjiÃ , rÃ ng jÄ«qÃ¬rÃ©n zÃ i bÃ¹tÃ³ng zÄ«tÃ i hÃ© dÃ¬xÃ­ng xiÃ  zhÃ n qÇlÃ¡i.\n\næ¡†æ¶åˆ†ä¸¤é˜¶æ®µï¼Œå…ˆæ‰¾åˆ°ç«™èµ·æ¥çš„è·¯å¾„ï¼Œå†ä¼˜åŒ–åŠ¨ä½œä½¿å…¶å¹³ç¨³å¯é ã€‚\nKuÃ ngjiÃ  fÄ“n liÇng jiÄ“duÃ n, xiÄn zhÇo dÃ o zhÃ n qÇlÃ¡i de lÃ¹jÃ¬ng, zÃ i yÅuhuÃ  dÃ²ngzuÃ² shÇ qÃ­ pÃ­ngwÄ›n kÄ›kÃ o.\n\nå®éªŒä¸­ï¼Œæœºå™¨äººåœ¨ä¸åŒåœ°é¢å’Œå§¿æ€ä¸‹æˆåŠŸç«™èµ·æ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨çœŸå®ç¯å¢ƒä¸­æˆåŠŸåº”ç”¨çš„å­¦ä¹ æ–¹æ³•ã€‚\nShÃ­yÃ n zhÅng, jÄ«qÃ¬rÃ©n zÃ i bÃ¹tÃ³ng dÃ¬miÃ n hÃ© zÄ«tÃ i xiÃ  chÃ©nggÅng zhÃ n qÇlÃ¡i, zhÃ¨ shÃ¬ shÇ’ucÃ¬ zÃ i zhÄ“nshÃ­ huÃ¡njÃ¬ng zhÅng chÃ©nggÅng yÃ¬ngyÃ²ng de xuÃ©xÃ­ fÄngfÇ.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},\n{'word': 'äººå½¢æœºå™¨äºº', 'pinyin': 'rÃ©n xÃ­ng jÄ« qÃ¬ rÃ©n', 'trans': 'humanoid robot'},\n{'word': 'è‡ªåŠ¨', 'pinyin': 'zÃ¬ dÃ²ng', 'trans': 'automatic'},\n{'word': 'è·Œå€’', 'pinyin': 'diÄ“ dÇo', 'trans': 'fall down'},\n{'word': 'æ¢å¤', 'pinyin': 'huÄ« fÃ¹', 'trans': 'recover'},\n{'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'},\n{'word': 'è®¾è®¡', 'pinyin': 'shÃ¨ jÃ¬', 'trans': 'design'},\n{'word': 'æ§åˆ¶å™¨', 'pinyin': 'kÃ²ng zhÃ¬ qÃ¬', 'trans': 'controller'},\n{'word': 'ç«™èµ·æ¥', 'pinyin': 'zhÃ n qÇ lÃ¡i', 'trans': 'stand up'},\n{'word': 'å§¿æ€', 'pinyin': 'zÄ« tÃ i', 'trans': 'posture'},\n{'word': 'åœ°å½¢', 'pinyin': 'dÃ¬ xÃ­ng', 'trans': 'terrain'},\n{'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'},\n{'word': 'å­¦ä¹ ', 'pinyin': 'xuÃ© xÃ­', 'trans': 'learn'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},\n{'word': 'é˜¶æ®µ', 'pinyin': 'jiÄ“ duÃ n', 'trans': 'stage'},\n{'word': 'è·¯å¾„', 'pinyin': 'lÃ¹ jÃ¬ng', 'trans': 'path'},\n{'word': 'ä¼˜åŒ–', 'pinyin': 'yÅu huÃ ', 'trans': 'optimize'},\n{'word': 'åŠ¨ä½œ', 'pinyin': 'dÃ²ng zuÃ²', 'trans': 'action'},\n{'word': 'å¹³ç¨³', 'pinyin': 'pÃ­ng wÄ›n', 'trans': 'stable'},\n{'word': 'å¯é ', 'pinyin': 'kÄ› kÃ o', 'trans': 'reliable'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'åœ°é¢', 'pinyin': 'dÃ¬ miÃ n', 'trans': 'ground'},\n{'word': 'æˆåŠŸ', 'pinyin': 'chÃ©ng gÅng', 'trans': 'success'},\n{'word': 'é¦–æ¬¡', 'pinyin': 'shÇ’u cÃ¬', 'trans': 'first time'},\n{'word': 'çœŸå®', 'pinyin': 'zhÄ“n shÃ­', 'trans': 'real'},\n{'word': 'ç¯å¢ƒ', 'pinyin': 'huÃ¡n jÃ¬ng', 'trans': 'environment'},\n{'word': 'åº”ç”¨', 'pinyin': 'yÃ¬ng yÃ²ng', 'trans': 'apply'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}]",
        "trans": "This article discusses the importance of automatic fall recovery for humanoid robots. Designing a controller to make a robot stand up after a fall is challenging because the robot may be in various postures and the terrain can be complex. The article proposes a learning framework that enables the robot to stand up from different postures and terrains. The framework consists of two stages: first, finding a path to stand up, and then optimizing the actions to make them smooth and reliable. In experiments, the robot successfully stood up from various surfaces and postures, marking the first successful application of a learning method in a real-world environment.",
        "update_ts": "2025-02-18 09:11"
    }
}