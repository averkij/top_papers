{
    "date": {
        "ru": "19 февраля",
        "en": "February 19",
        "zh": "2月19日"
    },
    "time_utc": "2025-02-19 06:14",
    "weekday": 2,
    "issue_id": 2289,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.12900",
            "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
            "url": "https://huggingface.co/papers/2502.12900",
            "abstract": "Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency. We propose Soundwave, which utilizes an efficient training strategy and a novel architecture to address these issues. Results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks, using only one-fiftieth of the training data. Further analysis shows that Soundwave still retains its intelligence during conversation. The project is available at https://github.com/FreedomIntelligence/Soundwave.",
            "score": 31,
            "issue_id": 2289,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "95780ecdf251cffd",
            "authors": [
                "Yuhao Zhang",
                "Zhiheng Liu",
                "Fan Bu",
                "Ruiyu Zhang",
                "Benyou Wang",
                "Haizhou Li"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12900.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#audio",
                    "#transfer_learning",
                    "#open_source",
                    "#optimization",
                    "#data",
                    "#architecture"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "Эффективное обучение речевых моделей с минимумом данных",
                    "desc": "Исследователи представили Soundwave - новую модель обработки речи, которая решает проблемы разрыва пространства представлений и несоответствия длины последовательностей между речью и текстом. Модель использует эффективную стратегию обучения и инновационную архитектуру. Soundwave превосходит передовую модель Qwen2-Audio в задачах перевода речи и тестах AIR-Bench, используя всего 1/50 часть обучающих данных. При этом модель сохраняет свой интеллект во время диалога."
                },
                "en": {
                    "title": "Soundwave: Efficient Speech Translation with Minimal Data",
                    "desc": "This paper introduces Soundwave, a new model designed for speech translation that requires significantly less training data compared to existing large language models. It addresses two key challenges: the differences in how speech and text are represented and the varying lengths of sequences in speech data. By employing an efficient training strategy and a unique architecture, Soundwave achieves superior performance on speech tasks while using only 2% of the data needed by its competitors. The findings indicate that Soundwave maintains high conversational intelligence, making it a promising approach for data-efficient speech processing."
                },
                "zh": {
                    "title": "高效训练，语音翻译新突破！",
                    "desc": "现有的端到端语音大型语言模型通常依赖于大规模标注数据进行训练，而数据高效训练尚未深入探讨。我们关注语音和文本之间的两个基本问题：表示空间差距和序列长度不一致。我们提出了Soundwave，它利用高效的训练策略和新颖的架构来解决这些问题。结果表明，Soundwave在语音翻译和AIR-Bench语音任务中表现优于先进的Qwen2-Audio，仅使用了五十分之一的训练数据。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11564",
            "title": "Continuous Diffusion Model for Language Modeling",
            "url": "https://huggingface.co/papers/2502.11564",
            "abstract": "Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose a continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish a connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce a simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at https://github.com/harryjo97/RDLM{https://github.com/harryjo97/RDLM}.",
            "score": 23,
            "issue_id": 2287,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "f5889d3a88d24b7c",
            "authors": [
                "Jaehyeong Jo",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "Korea Advanced Institute of Science and Technology (KAIST)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11564.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Непрерывная диффузия на статистическом многообразии для языкового моделирования",
                    "desc": "Статья представляет новый подход к моделированию естественного языка с использованием непрерывных диффузионных моделей. Авторы устанавливают связь между дискретной диффузией и непрерывным потоком на статистическом многообразии. Предложенный метод обобщает предыдущие дискретные диффузионные модели и вводит безсимуляционную схему обучения. Эксперименты показывают, что данный подход превосходит существующие дискретные диффузионные модели и приближается к производительности авторегрессионных моделей в задачах языкового моделирования."
                },
                "en": {
                    "title": "Revolutionizing Language Modeling with Continuous Diffusion",
                    "desc": "This paper presents a new continuous diffusion model designed for language modeling, which effectively handles discrete categorical data. The authors highlight the limitations of existing discrete diffusion models and propose a method that leverages the geometry of categorical distributions to enhance performance. By establishing a connection between discrete diffusion and continuous flow on a statistical manifold, they introduce a novel design that improves iterative refinement. Their experiments demonstrate that this approach not only surpasses traditional discrete models but also approaches the performance of autoregressive models."
                },
                "zh": {
                    "title": "连续扩散模型：提升离散数据建模的性能",
                    "desc": "扩散模型作为一种新兴的替代自回归模型的方法，在建模离散分类数据方面展现了良好的前景。现有的连续扩散模型在处理离散数据时性能有限，且二者之间的联系不明确，限制了扩散模型的发展。本文提出了一种新的连续扩散模型，结合了基础分类分布的几何特性，并建立了离散扩散与连续流动之间的联系。通过全面的实验，我们的方法在语言建模基准测试中超越了现有的离散扩散模型，接近自回归模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11079",
            "title": "Phantom: Subject-consistent video generation via cross-modal alignment",
            "url": "https://huggingface.co/papers/2502.11079",
            "abstract": "The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/.",
            "score": 21,
            "issue_id": 2286,
            "pub_date": "2025-02-16",
            "pub_date_card": {
                "ru": "16 февраля",
                "en": "February 16",
                "zh": "2月16日"
            },
            "hash": "e443a635e58b164f",
            "authors": [
                "Lijie Liu",
                "Tianxiang Ma",
                "Bingchuan Li",
                "Zhuowei Chen",
                "Jiawei Liu",
                "Qian He",
                "Xinglong Wu"
            ],
            "affiliations": [
                "Intelligent Creation Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11079.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Phantom: баланс текста и изображения для создания согласованного видео",
                    "desc": "Эта статья представляет Phantom - унифицированную систему генерации видео на основе текстовых и визуальных подсказок. Авторы предлагают новый подход к созданию видео с сохранением характеристик субъектов из опорных изображений. Phantom переосмысливает модель совместной обработки текста и изображений, обучаясь на триплетах текст-изображение-видео. Особое внимание уделяется сохранению идентичности людей при генерации видео."
                },
                "en": {
                    "title": "Phantom: Consistent Video Generation from Text and Images",
                    "desc": "This paper introduces a new approach called Subject-to-Video, which focuses on generating videos that maintain consistency with specific subjects extracted from reference images. The authors present Phantom, a unified framework that integrates both text and image inputs to create videos, ensuring that the generated content aligns well with the provided prompts. By utilizing a triplet data structure of text, image, and video, Phantom enhances the learning of cross-modal relationships, improving the quality of video generation. The framework particularly excels in generating videos of humans while preserving their identity across frames, marking a significant advancement in video generation technology."
                },
                "zh": {
                    "title": "实现主题一致性的视频生成",
                    "desc": "这篇论文介绍了一种新的视频生成模型，称为Phantom，旨在实现主题一致性的视频生成。该模型通过提取参考图像中的主题元素，并结合文本指令生成视频。Phantom框架能够处理单一和多个主题的参考，强调文本和图像的双模态提示的平衡。研究者们通过文本-图像-视频三元组数据来学习跨模态对齐，从而提升人类生成视频的主题一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12464",
            "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models",
            "url": "https://huggingface.co/papers/2502.12464",
            "abstract": "Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on \"hard\" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model's capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines.",
            "score": 20,
            "issue_id": 2288,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "5edcf5af6c8edecb",
            "authors": [
                "Seanie Lee",
                "Dong Bok Lee",
                "Dominik Wagner",
                "Minki Kang",
                "Haebin Seong",
                "Tobias Bocklet",
                "Juho Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST",
                "Technische Hochschule Nürnberg Georg Simon Ohm"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12464.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Умная маршрутизация для эффективной защиты языковых моделей",
                    "desc": "Статья предлагает метод SafeRoute для повышения эффективности моделей безопасности в крупных языковых моделях (LLM). SafeRoute использует бинарный маршрутизатор для разделения входных данных на 'простые' и 'сложные'. Сложные примеры обрабатываются большой моделью безопасности, а простые - меньшей дистиллированной моделью. Этот подход позволяет сохранить точность крупной модели при значительном снижении вычислительных затрат. Эксперименты на нескольких наборах данных показывают преимущество SafeRoute перед базовыми методами."
                },
                "en": {
                    "title": "Smart Routing for Safer AI: Balancing Efficiency and Accuracy",
                    "desc": "This paper introduces SafeRoute, a binary router designed to improve the efficiency of safety guard models used with large language models (LLMs). The router identifies which inputs are 'hard' and require the computational power of a larger safety model, while 'easy' inputs can be handled by smaller, more efficient models. By selectively applying the larger model only to challenging cases, SafeRoute reduces overall computational costs without sacrificing safety performance. Experimental results show that this adaptive approach significantly enhances the balance between resource usage and accuracy compared to using only the larger model."
                },
                "zh": {
                    "title": "智能路由，提升安全与效率！",
                    "desc": "在实际应用中部署大型语言模型（LLMs）需要强大的安全防护模型来检测和阻止有害的用户提示。虽然大型安全防护模型的性能很强，但其计算成本也很高。为了解决这个问题，研究者们使用了较小的蒸馏模型，但在处理“困难”示例时，它们的表现往往不如大型模型。我们提出了SafeRoute，一个二元路由器，可以区分困难示例和简单示例，从而提高效率，同时保持准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13131",
            "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis",
            "url": "https://huggingface.co/papers/2502.13131",
            "abstract": "Understanding human preferences is crucial for improving foundation models and building personalized AI systems. However, preferences are inherently diverse and complex, making it difficult for traditional reward models to capture their full range. While fine-grained preference data can help, collecting it is expensive and hard to scale. In this paper, we introduce Decomposed Reward Models (DRMs), a novel approach that extracts diverse human preferences from binary comparisons without requiring fine-grained annotations. Our key insight is to represent human preferences as vectors and analyze them using Principal Component Analysis (PCA). By constructing a dataset of embedding differences between preferred and rejected responses, DRMs identify orthogonal basis vectors that capture distinct aspects of preference. These decomposed rewards can be flexibly combined to align with different user needs, offering an interpretable and scalable alternative to traditional reward models. We demonstrate that DRMs effectively extract meaningful preference dimensions (e.g., helpfulness, safety, humor) and adapt to new users without additional training. Our results highlight DRMs as a powerful framework for personalized and interpretable LLM alignment.",
            "score": 20,
            "issue_id": 2286,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "b3376cde29e0b44f",
            "authors": [
                "Feng Luo",
                "Rui Yang",
                "Hao Sun",
                "Chunyuan Deng",
                "Jiarui Yao",
                "Jingyan Shen",
                "Huan Zhang",
                "Hanjie Chen"
            ],
            "affiliations": [
                "Columbia University",
                "Rice University",
                "University of Cambridge",
                "University of Illinois at Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13131.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rlhf",
                    "#training",
                    "#dataset",
                    "#interpretability"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Разложение предпочтений для персонализированного обучения языковых моделей",
                    "desc": "Статья представляет новый подход к извлечению разнообразных человеческих предпочтений из бинарных сравнений без необходимости в детальных аннотациях - Decomposed Reward Models (DRMs). Метод использует векторное представление предпочтений и анализ методом главных компонент (PCA) для выявления ортогональных базисных векторов, отражающих различные аспекты предпочтений. DRMs позволяют гибко комбинировать разложенные награды для адаптации к потребностям разных пользователей, предлагая интерпретируемую и масштабируемую альтернативу традиционным моделям вознаграждения. Эксперименты показывают, что DRMs эффективно извлекают значимые измерения предпочтений и адаптируются к новым пользователям без дополнительного обучения."
                },
                "en": {
                    "title": "Decomposed Reward Models: Unlocking Diverse Human Preferences for Personalized AI",
                    "desc": "This paper presents Decomposed Reward Models (DRMs), a new method for understanding human preferences in AI systems. Instead of relying on detailed preference data, DRMs use binary comparisons to extract diverse preferences, making the process more scalable and cost-effective. By representing preferences as vectors and applying Principal Component Analysis (PCA), the model identifies key dimensions of preference that can be combined to meet different user needs. The results show that DRMs can effectively adapt to new users and provide interpretable insights into preferences like helpfulness and safety."
                },
                "zh": {
                    "title": "分解奖励模型：个性化AI的新方法",
                    "desc": "理解人类偏好对于改善基础模型和构建个性化AI系统至关重要。传统的奖励模型难以捕捉偏好的多样性和复杂性。我们提出了一种新方法，称为分解奖励模型（DRMs），它通过二元比较提取人类偏好，而无需细粒度的注释。DRMs利用主成分分析（PCA）分析偏好向量，能够灵活组合以满足不同用户需求，提供了一种可解释且可扩展的替代方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13143",
            "title": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation",
            "url": "https://huggingface.co/papers/2502.13143",
            "abstract": "Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientations-a key requirement for tasks involving fine-grained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers a more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a knife). To support this, we construct OrienText300K, a large-scale dataset of 3D models annotated with semantic orientations that link geometric understanding to functional semantics. By integrating semantic orientation into a VLM system, we enable robots to generate manipulation actions with both positional and orientational constraints. Extensive experiments in simulation and real world demonstrate that our approach significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER.",
            "score": 20,
            "issue_id": 2286,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "7aab95a55286d8b2",
            "authors": [
                "Zekun Qi",
                "Wenyao Zhang",
                "Yufei Ding",
                "Runpei Dong",
                "Xinqiang Yu",
                "Jingwen Li",
                "Lingyun Xu",
                "Baoyu Li",
                "Xialin He",
                "Guofan Fan",
                "Jiazhao Zhang",
                "Jiawei He",
                "Jiayuan Gu",
                "Xin Jin",
                "Kaisheng Ma",
                "Zhizheng Zhang",
                "He Wang",
                "Li Yi"
            ],
            "affiliations": [
                "Eastern Institute of Technology",
                "Galbot",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Shanghai Qi Zhi Institute",
                "ShanghaiTech University",
                "Tsinghua University",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13143.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#games",
                    "#dataset",
                    "#reasoning",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Семантическая ориентация: новый подход к пространственному интеллекту роботов",
                    "desc": "Статья представляет концепцию семантической ориентации для улучшения пространственного интеллекта роботов. Авторы создали датасет OrienText300K с 3D моделями, аннотированными семантическими ориентациями на естественном языке. Интеграция семантической ориентации в систему с визуально-языковой моделью (VLM) позволяет роботам генерировать действия с учетом позиционных и ориентационных ограничений. Эксперименты показали значительное улучшение возможностей роботов в манипуляциях с объектами."
                },
                "en": {
                    "title": "Empowering Robots with Semantic Orientation for Enhanced Manipulation",
                    "desc": "This paper addresses the challenge of teaching robots to understand object orientations, which is essential for precise manipulation tasks. It introduces the concept of semantic orientation, allowing robots to interpret orientations using natural language instead of traditional geometric frames. The authors present OrienText300K, a dataset of 3D models annotated with these semantic orientations, linking geometric understanding to functional semantics. By incorporating this approach into vision-language models (VLMs), the paper demonstrates improved accuracy in robotic manipulation tasks, showcasing the effectiveness of using natural language for orientation representation."
                },
                "zh": {
                    "title": "用自然语言提升机器人的空间智能",
                    "desc": "空间智能是具身人工智能的重要组成部分，帮助机器人理解和与环境互动。尽管最近的进展提高了视觉语言模型（VLMs）对物体位置和关系的感知能力，但它们仍然缺乏精确理解物体方向的能力，这对于细致操作任务至关重要。为了解决这个问题，我们提出了语义方向的概念，使用自然语言以无参考框架的方式定义物体方向。通过构建OrienText300K数据集，我们将几何理解与功能语义联系起来，从而提升机器人在操作中的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11433",
            "title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading",
            "url": "https://huggingface.co/papers/2502.11433",
            "abstract": "Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose FLAG-Trader, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.",
            "score": 16,
            "issue_id": 2286,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "0731051fe5131889",
            "authors": [
                "Guojun Xiong",
                "Zhiyang Deng",
                "Keyi Wang",
                "Yupeng Cao",
                "Haohang Li",
                "Yangyang Yu",
                "Xueqing Peng",
                "Mingquan Lin",
                "Kaleb E Smith",
                "Xiao-Yang Liu",
                "Jimin Huang",
                "Sophia Ananiadou",
                "Qianqian Xie"
            ],
            "affiliations": [
                "Columbia University",
                "Harvard University",
                "NVIDIA",
                "Rensselaer Polytechnic Institute",
                "Stevens Institute of Technology",
                "TheFinAI",
                "University of Manchester",
                "University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11433.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#architecture",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#multimodal"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "Усиление торговых стратегий с помощью языковых моделей и обучения с подкреплением",
                    "desc": "В статье представлена архитектура FLAG-Trader, объединяющая языковую обработку с помощью больших языковых моделей (LLM) и обучение с подкреплением для оптимизации торговых стратегий. LLM используется в качестве политики, адаптируясь к финансовой области через эффективную дообучение. Метод улучшает принятие решений в интерактивных финансовых сценариях, таких как торговля. Эмпирические результаты подтверждают эффективность подхода не только в торговле, но и в других финансовых задачах."
                },
                "en": {
                    "title": "Empowering Financial Trading with FLAG-Trader: Merging Language and Reinforcement Learning",
                    "desc": "This paper introduces FLAG-Trader, a new architecture that combines large language models (LLMs) with reinforcement learning (RL) to improve decision-making in financial trading. The approach uses a partially fine-tuned LLM as a policy network, allowing it to utilize its pre-trained knowledge while adapting specifically to financial tasks. By applying policy gradient optimization based on trading rewards, FLAG-Trader enhances the performance of LLMs in trading scenarios and other financial applications. The authors provide extensive empirical evidence demonstrating the effectiveness of their proposed method."
                },
                "zh": {
                    "title": "FLAG-Trader：提升金融决策的智能交易架构",
                    "desc": "本文提出了一种名为FLAG-Trader的统一架构，旨在提升大型语言模型（LLMs）在金融市场中的决策能力。该架构结合了语言处理和基于梯度的强化学习（RL）策略优化，使得部分微调的LLM可以作为策略网络，利用预训练知识并适应金融领域。通过交易奖励驱动的策略梯度优化，FLAG-Trader不仅提高了LLM在交易中的表现，还改善了其他金融领域任务的结果。我们提供了大量实证证据来验证这些改进。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13145",
            "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation",
            "url": "https://huggingface.co/papers/2502.13145",
            "abstract": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6times speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5times speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba",
            "score": 14,
            "issue_id": 2286,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "6810113d41bfd26d",
            "authors": [
                "Bencheng Liao",
                "Hongyuan Tao",
                "Qian Zhang",
                "Tianheng Cheng",
                "Yingyue Li",
                "Haoran Yin",
                "Wenyu Liu",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Horizon Robotics",
                "Institute of Artificial Intelligence, Huazhong University of Science & Technology",
                "School of EIC, Huazhong University of Science & Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13145.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#transfer_learning",
                    "#architecture",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "mmMamba: эффективные мультимодальные модели с линейной сложностью",
                    "desc": "Статья представляет mmMamba - фреймворк для разработки мультимодальных моделей с линейной сложностью на основе state space models. Авторы предлагают метод прогрессивной дистилляции знаний от существующих мультимодальных языковых моделей (MLLM) к линейным архитектурам без использования предобученных RNN-моделей или энкодеров изображений. Предложенный подход включает стратегию инициализации Mamba из обученного Transformer и трехэтапный рецепт дистилляции. Результаты показывают, что mmMamba достигает конкурентоспособной производительности по сравнению с существующими моделями, обеспечивая значительное ускорение и экономию памяти."
                },
                "en": {
                    "title": "Efficient Multimodal Models with mmMamba",
                    "desc": "The paper introduces mmMamba, a new framework designed to create efficient multimodal large language models (MLLMs) with linear computational complexity. It utilizes a progressive distillation process to convert existing decoder-only MLLMs into more efficient architectures without needing separate vision encoders or pre-trained RNNs. The proposed seeding strategy and three-stage distillation recipe allow for effective knowledge transfer from Transformer models while maintaining multimodal capabilities. The results show that mmMamba-linear and mmMamba-hybrid significantly outperform traditional models in terms of speed and memory usage, making them suitable for practical deployment."
                },
                "zh": {
                    "title": "mmMamba：高效的多模态模型架构",
                    "desc": "最近的多模态大型语言模型（MLLMs）在性能上取得了显著进展，但由于其计算复杂度呈平方增长、对键值缓存的需求增加以及依赖于独立的视觉编码器，面临部署挑战。我们提出了mmMamba框架，通过从现有的MLLMs进行渐进蒸馏，开发线性复杂度的本地多模态状态空间模型，使用适度的学术计算资源。该方法允许将训练好的仅解码器MLLMs直接转换为线性复杂度架构，而无需预训练的基于RNN的LLM或视觉编码器。我们的蒸馏策略有效地将知识从Transformer转移到Mamba，同时保留多模态能力，并支持灵活的混合架构，以实现可定制的效率与性能权衡。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12513",
            "title": "RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm",
            "url": "https://huggingface.co/papers/2502.12513",
            "abstract": "After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn achieve state-of-the-art performance on multiple downstream tasks. To facilitate future research, the RealSyn dataset and pre-trained model weights are released at https://github.com/deepglint/RealSyn.",
            "score": 9,
            "issue_id": 2286,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "1ed14365f683281c",
            "authors": [
                "Tiancheng Gu",
                "Kaicheng Yang",
                "Chaoyi Zhang",
                "Yin Xie",
                "Xiang An",
                "Ziyong Feng",
                "Dongnan Liu",
                "Weidong Cai",
                "Jiankang Deng"
            ],
            "affiliations": [
                "DeepGlint",
                "Imperial College London",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12513.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#data",
                    "#synthetic",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "RealSyn: Улучшение мультимодального обучения с помощью реальных и синтетических данных",
                    "desc": "Статья представляет новый подход к обучению мультимодальных моделей, использующих изображения и текст. Авторы разработали метод RealSyn, который извлекает высококачественные данные из непарных документов и создает синтетические тексты для улучшения визуальной информации. Они также применили стратегию семантически сбалансированной выборки для повышения разнообразия данных. Эксперименты показали, что модели, предобученные на датасете RealSyn, достигают наилучших результатов в различных задачах компьютерного зрения и обработки естественного языка."
                },
                "en": {
                    "title": "Unlocking Vision-Language Learning with RealSyn Dataset",
                    "desc": "This paper introduces RealSyn, a new dataset designed to improve vision-language representation learning by utilizing both realistic and synthetic texts. The authors develop a data extraction pipeline to gather high-quality images and texts from unpaired multimodal documents. They also implement a hierarchical retrieval method to link images with relevant texts and propose an image semantic augmented generation module to create synthetic text. The resulting dataset, RealSyn, shows significant improvements in model performance on various tasks, demonstrating its effectiveness and scalability in the field of machine learning."
                },
                "zh": {
                    "title": "利用未配对数据提升视觉-语言学习的创新方法",
                    "desc": "这篇论文介绍了一种新的数据集RealSyn，用于视觉-语言表示学习。研究者们通过提取高质量的图像和文本，利用未配对的数据来提升模型的性能。为了更好地关联图像和文本，设计了一种层次检索方法，并提出了图像语义增强生成模块来生成合成文本。实验结果表明，基于RealSyn预训练的模型在多个下游任务上达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12501",
            "title": "Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge",
            "url": "https://huggingface.co/papers/2502.12501",
            "abstract": "LLM-as-a-Judge, which generates chain-of-thought (CoT) judgments, has become a widely adopted auto-evaluation method. However, its reliability is compromised by the CoT reasoning's inability to capture comprehensive and deeper details, often leading to incomplete outcomes. Existing methods mainly rely on majority voting or criteria expansion, which is insufficient to address the limitation in CoT. We propose Crowd-based Comparative Evaluation, which introduces additional crowd responses to compare with the candidate responses, thereby exposing deeper and more comprehensive details within the candidate responses. This process effectively guides LLM-as-a-Judge to provide a more detailed CoT judgment. Extensive experiments demonstrate that our approach enhances evaluation reliability, achieving an average accuracy gain of 6.7% across five benchmarks. Moreover, our method produces higher-quality CoTs that facilitate judge distillation and exhibit superior performance in rejection sampling for supervised fine-tuning (SFT), referred to as crowd rejection sampling, thereby enabling more efficient SFT. Our analysis confirms that CoTs generated by ours are more comprehensive and of higher quality, and evaluation accuracy improves as inference scales.",
            "score": 4,
            "issue_id": 2286,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "1c26233f72f504ee",
            "authors": [
                "Qiyuan Zhang",
                "Yufei Wang",
                "Yuxin Jiang",
                "Liangyou Li",
                "Chuhan Wu",
                "Yasheng Wang",
                "Xin Jiang",
                "Lifeng Shang",
                "Ruiming Tang",
                "Fuyuan Lyu",
                "Chen Ma"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "Huawei Noahs Ark Lab",
                "McGill University & MILA",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12501.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#inference",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение оценки ИИ через сравнение с мнением толпы",
                    "desc": "Статья представляет новый метод оценки моделей машинного обучения под названием 'Crowd-based Comparative Evaluation'. Этот подход улучшает традиционный метод LLM-as-a-Judge, добавляя сравнение с ответами толпы для более глубокого анализа. Эксперименты показали повышение точности оценки в среднем на 6.7% на пяти бенчмарках. Метод также демонстрирует улучшенное качество рассуждений (Chain-of-Thought) и эффективность в отборе данных для обучения с учителем."
                },
                "en": {
                    "title": "Enhancing LLM Evaluations with Crowd Insights",
                    "desc": "This paper addresses the limitations of the LLM-as-a-Judge method, which generates chain-of-thought (CoT) judgments for auto-evaluation. The authors identify that CoT reasoning often lacks depth and comprehensiveness, leading to incomplete evaluations. To overcome this, they propose a new method called Crowd-based Comparative Evaluation, which incorporates additional crowd responses to enhance the evaluation process. Their experiments show that this approach improves the reliability of evaluations, increases accuracy by 6.7%, and produces higher-quality CoTs that benefit supervised fine-tuning."
                },
                "zh": {
                    "title": "提升LLM评估可靠性的创新方法",
                    "desc": "本文提出了一种新的评估方法，称为基于人群的比较评估（Crowd-based Comparative Evaluation），旨在提高大型语言模型（LLM）作为评判者的可靠性。传统的链式推理（CoT）方法常常无法捕捉到全面和深入的细节，导致评估结果不完整。我们的方法通过引入额外的人群反馈，与候选响应进行比较，从而揭示候选响应中的更深层次和更全面的细节。实验结果表明，该方法在五个基准测试中平均提高了6.7%的评估准确性，并生成了更高质量的CoT，促进了监督微调的效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12215",
            "title": "Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?",
            "url": "https://huggingface.co/papers/2502.12215",
            "abstract": "The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose Shortest Majority Vote, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches.",
            "score": 3,
            "issue_id": 2288,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 февраля",
                "en": "February 17",
                "zh": "2月17日"
            },
            "hash": "a02df3e32ba854de",
            "authors": [
                "Zhiyuan Zeng",
                "Qinyuan Cheng",
                "Zhangyue Yin",
                "Yunhua Zhou",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "School of Computer Science, Fudan University, Shanghai, China",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12215.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Короче - лучше: новый взгляд на масштабирование языковых моделей",
                    "desc": "Это исследование посвящено масштабированию во время вывода в больших языковых моделях (LLM). Авторы обнаружили, что более длинные цепочки рассуждений не всегда повышают точность, и часто правильные решения короче неправильных. Исследование показывает, что это связано со способностью моделей к самокоррекции, которая может ухудшать производительность. На основе этих наблюдений предложен метод Shortest Majority Vote, сочетающий параллельное масштабирование со свойствами длины цепочек рассуждений."
                },
                "en": {
                    "title": "Enhancing LLM Performance with Shorter, Smarter Reasoning",
                    "desc": "This paper investigates the concept of test-time scaling in large language models (LLMs), particularly focusing on the o1 series by OpenAI and its successors. It reveals that longer chains of thought (CoTs) do not always lead to better accuracy, as shorter CoTs can yield correct answers more frequently. The study highlights that the presence of self-revisions in longer CoTs can negatively impact performance. To enhance test-time scalability, the authors propose a new method called Shortest Majority Vote, which integrates parallel scaling strategies with CoT length characteristics, outperforming traditional majority voting methods."
                },
                "zh": {
                    "title": "提升模型推理能力的新方法",
                    "desc": "本文探讨了大型语言模型（LLMs）在推理时的测试时间缩放能力，特别是OpenAI的o1系列。研究发现，虽然一些后续模型如QwQ和Deepseek-R1模仿了这些进展，但它们的测试时间缩放能力仍未得到充分验证。更长的链式思维（CoT）并不总是提高准确性，反而正确答案往往比错误答案更短。我们提出了一种新的方法——最短多数投票，结合并行缩放策略和CoT长度特征，显著提升了模型的测试时间可扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12170",
            "title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections",
            "url": "https://huggingface.co/papers/2502.12170",
            "abstract": "We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer .",
            "score": 3,
            "issue_id": 2287,
            "pub_date": "2025-02-13",
            "pub_date_card": {
                "ru": "13 февраля",
                "en": "February 13",
                "zh": "2月13日"
            },
            "hash": "f06bcb1b611b7c39",
            "authors": [
                "Da Xiao",
                "Qingye Meng",
                "Shengping Li",
                "Xingyuan Yuan"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications, Beijing, China",
                "ColorfulClouds Technology Co., Ltd., Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12170.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Динамические связи для эффективных трансформеров",
                    "desc": "Исследователи предлагают новый метод MUDD (MUltiway Dynamic Dense) для улучшения связей между слоями в архитектуре Transformer. MUDD генерирует веса связей динамически в зависимости от скрытых состояний на каждой позиции последовательности и для каждого входного потока блока Transformer. Эксперименты показывают, что MUDDFormer значительно превосходит стандартные Transformer-модели в задачах языкового моделирования, достигая производительности моделей, обученных с использованием в 1.8-2.4 раза больше вычислительных ресурсов. Примечательно, что MUDDPythia-2.8B сопоставима по эффективности с Pythia-6.9B и даже конкурирует с Pythia-12B в некоторых задачах, добавляя всего 0.23% параметров и 0.4% вычислений."
                },
                "en": {
                    "title": "Dynamic Connections for Enhanced Transformer Performance",
                    "desc": "The paper introduces MUDD connections, which improve the flow of information between layers in Transformer models. Unlike traditional residual connections that use fixed weights, MUDD connections adaptively generate weights based on the hidden states of the input at each position. This dynamic approach allows for better integration of information from different input streams, enhancing the overall performance of the model. The proposed MUDDFormer architecture shows significant improvements in language modeling tasks, achieving results comparable to larger models while maintaining a smaller parameter count."
                },
                "zh": {
                    "title": "动态连接，提升Transformer性能！",
                    "desc": "我们提出了一种名为多向动态稠密连接（MUDD）的简单有效方法，旨在解决残差连接的局限性，并增强Transformer中跨层信息流动。与现有的静态共享连接权重的稠密连接方法不同，MUDD根据每个序列位置的隐藏状态动态生成连接权重，并针对Transformer块的每个解耦输入流（查询、键、值或残差）。MUDD连接可以无缝集成到任何Transformer架构中，形成MUDDFormer。大量实验表明，MUDDFormer在语言建模中显著超越了各种模型架构和规模的Transformer，表现出与训练时计算量为1.8X-2.4X的Transformer相当的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13130",
            "title": "Magma: A Foundation Model for Multimodal AI Agents",
            "url": "https://huggingface.co/papers/2502.13130",
            "abstract": "We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pretrained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to a wide range of tasks as shown in Fig.1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility at https://microsoft.github.io/Magma.",
            "score": 2,
            "issue_id": 2288,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "1851b242ac65c88f",
            "authors": [
                "Jianwei Yang",
                "Reuben Tan",
                "Qianhui Wu",
                "Ruijie Zheng",
                "Baolin Peng",
                "Yongyuan Liang",
                "Yu Gu",
                "Mu Cai",
                "Seonghyeon Ye",
                "Joel Jang",
                "Yuquan Deng",
                "Lars Liden",
                "Jianfeng Gao"
            ],
            "affiliations": [
                "KAIST",
                "Microsoft Research",
                "University of Maryland",
                "University of Washington",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13130.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#robotics",
                    "#agi",
                    "#multimodal",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Magma: мультимодальный ИИ-агент для цифрового и физического мира",
                    "desc": "Статья представляет Magma - новую мультимодальную модель искусственного интеллекта, способную выполнять агентные задачи как в цифровом, так и в физическом мире. Модель расширяет возможности существующих vision-language моделей, добавляя способность планировать и действовать в визуально-пространственном мире. Magma обучена на больших гетерогенных наборах данных, включая изображения, видео и робототехнические данные, с использованием специальных методов разметки Set-of-Mark и Trace-of-Mark. Эксперименты показывают, что Magma достигает новых передовых результатов в задачах навигации по пользовательскому интерфейсу и манипуляции роботами."
                },
                "en": {
                    "title": "Magma: Bridging Digital and Physical Worlds with Multimodal Intelligence",
                    "desc": "Magma is a foundation model designed for multimodal AI tasks that operate in both digital and physical environments. It enhances traditional vision-language models by integrating spatial-temporal intelligence, allowing it to plan and execute actions in real-world scenarios. The model is pretrained on diverse datasets, utilizing Set-of-Mark (SoM) for action grounding and Trace-of-Mark (ToM) for action planning, which together improve its ability to understand and interact with visual-spatial elements. Magma achieves state-of-the-art performance in UI navigation and robotic manipulation, surpassing specialized models and competing effectively with larger multimodal models."
                },
                "zh": {
                    "title": "Magma：多模态智能的未来",
                    "desc": "Magma是一个基础模型，能够处理数字和物理世界中的多模态人工智能任务。它不仅保留了视觉-语言模型的理解能力，还具备在视觉空间中规划和行动的能力。Magma通过大量异构数据集进行预训练，这些数据集包括图像、视频和机器人数据，使用Set-of-Mark和Trace-of-Mark进行动作标定。实验表明，Magma在用户界面导航和机器人操作任务上创造了新的最先进结果，超越了专门为这些任务设计的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.09838",
            "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
            "url": "https://huggingface.co/papers/2502.09838",
            "abstract": "We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.",
            "score": 2,
            "issue_id": 2287,
            "pub_date": "2025-02-14",
            "pub_date_card": {
                "ru": "14 февраля",
                "en": "February 14",
                "zh": "2月14日"
            },
            "hash": "ce920c7d40d11dad",
            "authors": [
                "Tianwei Lin",
                "Wenqiao Zhang",
                "Sijing Li",
                "Yuqian Yuan",
                "Binhe Yu",
                "Haoyuan Li",
                "Wanggui He",
                "Hao Jiang",
                "Mengze Li",
                "Xiaohui Song",
                "Siliang Tang",
                "Jun Xiao",
                "Hui Lin",
                "Yueting Zhuang",
                "Beng Chin Ooi"
            ],
            "affiliations": [
                "Alibaba",
                "National University of Singapore",
                "The Hong Kong University of Science and Technology",
                "University of Electronic Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.09838.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#cv",
                    "#dataset",
                    "#healthcare"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "HealthGPT: Единая модель для понимания и генерации медицинских изображений",
                    "desc": "HealthGPT - это мощная мультимодальная модель для медицинской визуальной обработки и генерации. Она использует новую технику адаптации H-LoRA и иерархический подход к визуальному восприятию. Модель обучена на специально созданном наборе данных VL-Health. HealthGPT демонстрирует исключительную производительность в медицинских визуальных задачах."
                },
                "en": {
                    "title": "Revolutionizing Medical AI with HealthGPT",
                    "desc": "HealthGPT is a Medical Large Vision-Language Model that combines understanding and generating medical images and text in one system. It uses a unique method called heterogeneous low-rank adaptation (H-LoRA) to enhance pre-trained large language models with diverse medical knowledge. The model is trained on a specialized dataset named VL-Health, which focuses on medical comprehension and generation tasks. Results show that HealthGPT performs exceptionally well in various medical visual tasks, demonstrating its effectiveness and scalability."
                },
                "zh": {
                    "title": "HealthGPT：医疗视觉语言模型的创新整合",
                    "desc": "我们提出了HealthGPT，这是一种强大的医疗大型视觉语言模型（Med-LVLM），它将医疗视觉理解和生成能力整合在一个统一的自回归框架中。我们的自举理念是逐步将异构的理解和生成知识适应于预训练的大型语言模型（LLMs）。这通过一种新颖的异构低秩适应（H-LoRA）技术实现，并辅以定制的分层视觉感知方法和三阶段学习策略。实验结果表明，HealthGPT在医疗视觉统一任务中表现出色，具有良好的可扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12574",
            "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
            "url": "https://huggingface.co/papers/2502.12574",
            "abstract": "Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods.",
            "score": 1,
            "issue_id": 2286,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 февраля",
                "en": "February 18",
                "zh": "2月18日"
            },
            "hash": "6e5de7c584198857",
            "authors": [
                "Cheng Luo",
                "Zefan Cai",
                "Hanshi Sun",
                "Jinqi Xiao",
                "Bo Yuan",
                "Wen Xiao",
                "Junjie Hu",
                "Jiawei Zhao",
                "Beidi Chen",
                "Anima Anandkumar"
            ],
            "affiliations": [
                "California Institute of Technology",
                "Carnegie Mellon University",
                "Microsoft",
                "Rutgers University",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12574.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#inference",
                    "#training",
                    "#long_context"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "HEADINFER: эффективное использование памяти для LLM с длинным контекстом",
                    "desc": "HEADINFER - это новый метод для оптимизации памяти при работе с большими языковыми моделями (LLM). Он позволяет выгружать кэш ключей-значений (KV cache) в оперативную память CPU, значительно снижая нагрузку на GPU. Используя стратегию выборочного хранения данных для отдельных голов внимания, HEADINFER сохраняет вычислительную эффективность при существенном уменьшении объема используемой памяти. Метод был успешно протестирован на модели Llama-3-8B с последовательностью в 1 миллион токенов, сократив использование памяти GPU на 92% по сравнению с базовым методом."
                },
                "en": {
                    "title": "HEADINFER: Efficient Memory Management for Long Contexts in LLMs",
                    "desc": "This paper introduces HEADINFER, a novel approach to manage the memory usage of large language models (LLMs) during long context generation. By offloading the key-value (KV) cache to CPU RAM, HEADINFER reduces the reliance on GPU memory, which is crucial for efficient inference. The method employs a fine-grained, head-wise offloading strategy, allowing selective storage of attention heads on the GPU while dynamically computing attention outputs. Evaluation on the Llama-3-8B model shows a remarkable reduction in GPU memory usage, enabling efficient processing of extremely long sequences without compromising performance."
                },
                "zh": {
                    "title": "HEADINFER：优化大语言模型的内存使用",
                    "desc": "本文提出了一种名为HEADINFER的方法，旨在优化大语言模型（LLM）在长上下文生成中的内存使用。通过将关键值缓存（KV缓存）转移到CPU RAM，HEADINFER避免了在GPU上完全存储KV缓存的需求。该方法采用细粒度的头部级别卸载策略，仅在GPU上保留选择性的注意力头KV缓存，同时动态计算注意力输出。实验结果表明，HEADINFER在显著减少内存占用的同时，保持了计算效率，使得在单个消费级GPU上实现了对长达400万标记的推理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.10852",
            "title": "Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages",
            "url": "https://huggingface.co/papers/2502.10852",
            "abstract": "While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.",
            "score": 0,
            "issue_id": 2287,
            "pub_date": "2025-02-15",
            "pub_date_card": {
                "ru": "15 февраля",
                "en": "February 15",
                "zh": "2月15日"
            },
            "hash": "11a782268b627ec1",
            "authors": [
                "Zeli Su",
                "Ziyin Zhang",
                "Guixian Xu",
                "Jianing Liu",
                "XU Han",
                "Ting Zhang",
                "Yushuang Dong"
            ],
            "affiliations": [
                "Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of MOE",
                "Minzu University of China",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.10852.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#low_resource"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Преодоление языкового барьера: эффективная генерация текста для малоресурсных языков",
                    "desc": "Статья представляет новый подход к адаптации многоязычных энкодеров для генерации текста на языках с крайне ограниченными ресурсами. Авторы предлагают метод повторного использования весов между энкодером и декодером, что позволяет модели эффективно использовать семантическое пространство энкодера. Применяя этот подход к четырем китайским языкам меньшинств, исследователи разработали модель XLM-SWCM. Эксперименты показали превосходные результаты XLM-SWCM на различных задачах даже в сравнении с гораздо более крупными моделями."
                },
                "en": {
                    "title": "Empowering Low-Resource Languages with Efficient Multilingual Models",
                    "desc": "This paper addresses the challenges faced by multilingual language models in generating text for extremely low-resource languages. It introduces a new framework that adapts multilingual encoders for text generation by reusing weights between the encoder and decoder. This approach allows the model to utilize the semantic knowledge learned by the encoder, leading to better performance in low-resource settings. The authors demonstrate the effectiveness of their framework, named XLM-SWCM, on four Chinese minority languages, showing that it outperforms larger models in various tasks."
                },
                "zh": {
                    "title": "为极低资源语言赋能的多语言文本生成框架",
                    "desc": "多语言模型如XLM-R在自然语言处理中的多语言能力有所提升，但在极低资源语言上表现仍然较差。现代大型语言模型如LLaMA和Qwen支持的语言数量远少于XLM-R，导致许多语言缺乏文本生成模型。为了解决这个问题，我们提出了一种新框架，将多语言编码器适应于极低资源语言的文本生成。通过重用编码器和解码器之间的权重，我们的框架能够利用编码器学习到的语义空间，从而在低资源语言中实现高效学习和有效泛化。"
                }
            }
        }
    ],
    "link_prev": "2025-02-18.html",
    "link_next": "2025-02-20.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "18.02",
        "en": "02/18",
        "zh": "2月18日"
    },
    "short_date_next": {
        "ru": "20.02",
        "en": "02/20",
        "zh": "2月20日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 1,
        "#training": 11,
        "#robotics": 2,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了人形机器人自动跌倒恢复的重要性。设计控制器让机器人站起来很难，因为跌倒后机器人可能处于各种姿态，且地形复杂。文章提出了一个学习框架，让机器人在不同姿态和地形下站起来。框架分两阶段，先找到站起来的路径，再优化动作使其平稳可靠。实验中，机器人在不同地面和姿态下成功站起来，这是首次在真实环境中成功应用的学习方法。",
        "title": "Learning Getting-Up Policies for Real-World Humanoid Robots",
        "pinyin": "这篇文章讨论了人形机器人自动跌倒恢复的重要性。\nZhè piān wénzhāng tǎolùn le rénxíng jīqìrén zìdòng diēdǎo huīfù de zhòngyàoxìng.\n\n设计控制器让机器人站起来很难，因为跌倒后机器人可能处于各种姿态，且地形复杂。\nShèjì kòngzhìqì ràng jīqìrén zhàn qǐlái hěn nán, yīnwèi diēdǎo hòu jīqìrén kěnéng chǔyú gèzhǒng zītài, qiě dìxíng fùzá.\n\n文章提出了一个学习框架，让机器人在不同姿态和地形下站起来。\nWénzhāng tíchū le yīgè xuéxí kuàngjià, ràng jīqìrén zài bùtóng zītài hé dìxíng xià zhàn qǐlái.\n\n框架分两阶段，先找到站起来的路径，再优化动作使其平稳可靠。\nKuàngjià fēn liǎng jiēduàn, xiān zhǎo dào zhàn qǐlái de lùjìng, zài yōuhuà dòngzuò shǐ qí píngwěn kěkào.\n\n实验中，机器人在不同地面和姿态下成功站起来，这是首次在真实环境中成功应用的学习方法。\nShíyàn zhōng, jīqìrén zài bùtóng dìmiàn hé zītài xià chénggōng zhàn qǐlái, zhè shì shǒucì zài zhēnshí huánjìng zhōng chénggōng yìngyòng de xuéxí fāngfǎ.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '人形机器人', 'pinyin': 'rén xíng jī qì rén', 'trans': 'humanoid robot'},\n{'word': '自动', 'pinyin': 'zì dòng', 'trans': 'automatic'},\n{'word': '跌倒', 'pinyin': 'diē dǎo', 'trans': 'fall down'},\n{'word': '恢复', 'pinyin': 'huī fù', 'trans': 'recover'},\n{'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'},\n{'word': '设计', 'pinyin': 'shè jì', 'trans': 'design'},\n{'word': '控制器', 'pinyin': 'kòng zhì qì', 'trans': 'controller'},\n{'word': '站起来', 'pinyin': 'zhàn qǐ lái', 'trans': 'stand up'},\n{'word': '姿态', 'pinyin': 'zī tài', 'trans': 'posture'},\n{'word': '地形', 'pinyin': 'dì xíng', 'trans': 'terrain'},\n{'word': '复杂', 'pinyin': 'fù zá', 'trans': 'complex'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '学习', 'pinyin': 'xué xí', 'trans': 'learn'},\n{'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'},\n{'word': '阶段', 'pinyin': 'jiē duàn', 'trans': 'stage'},\n{'word': '路径', 'pinyin': 'lù jìng', 'trans': 'path'},\n{'word': '优化', 'pinyin': 'yōu huà', 'trans': 'optimize'},\n{'word': '动作', 'pinyin': 'dòng zuò', 'trans': 'action'},\n{'word': '平稳', 'pinyin': 'píng wěn', 'trans': 'stable'},\n{'word': '可靠', 'pinyin': 'kě kào', 'trans': 'reliable'},\n{'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'},\n{'word': '地面', 'pinyin': 'dì miàn', 'trans': 'ground'},\n{'word': '成功', 'pinyin': 'chéng gōng', 'trans': 'success'},\n{'word': '首次', 'pinyin': 'shǒu cì', 'trans': 'first time'},\n{'word': '真实', 'pinyin': 'zhēn shí', 'trans': 'real'},\n{'word': '环境', 'pinyin': 'huán jìng', 'trans': 'environment'},\n{'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'apply'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}]",
        "trans": "This article discusses the importance of automatic fall recovery for humanoid robots. Designing a controller to make a robot stand up after a fall is challenging because the robot may be in various postures and the terrain can be complex. The article proposes a learning framework that enables the robot to stand up from different postures and terrains. The framework consists of two stages: first, finding a path to stand up, and then optimizing the actions to make them smooth and reliable. In experiments, the robot successfully stood up from various surfaces and postures, marking the first successful application of a learning method in a real-world environment.",
        "update_ts": "2025-02-18 09:11"
    }
}