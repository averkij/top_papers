{
    "date": {
        "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 8",
        "zh": "10æœˆ8æ—¥"
    },
    "time_utc": "2025-10-08 02:16",
    "weekday": 2,
    "issue_id": 6298,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.06217",
            "title": "TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.06217",
            "abstract": "TaTToo, a novel table-grounded Process Reward Model, enhances tabular reasoning by explicitly addressing table-specific operations and integrating tool-based verification, leading to significant performance improvements over existing PRMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have recently emerged as a powerful framework for enhancing the reasoning capabilities of large reasoning models (LRMs), particularly in the context of test-time scaling (TTS). However, their potential for supervising LRMs on tabular reasoning domains remains underexplored. Through detailed empirical analyses, we identify that existing PRMs, though widely adopted for supervising text-only reasoning steps, struggle with table-specific operations such as sub-table retrieval and schema interaction, leading to critical performance bottlenecks. To address this limitation, we propose TaTToo, a novel table-grounded PRM framework that (i) reasons explicitly over tabular reasoning steps and (ii) integrates tool-based verification to provide precise reward supervision. Concretely, we first design a scalable data curation pipeline that constructs over 60k high-quality step-level annotations by integrating table verification rationales with tool-based executions. Building on the collected data, we train TaTToo with a dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use reasoning patterns, followed by reinforcement learning with tool-grounded reward shaping to align our model with table-based verification. We provide a comprehensive evaluation of the policy improvement induced by our newly designed PRM. Across 5 challenging tabular reasoning benchmarks covering numerical reasoning, fact-checking, and data analysis, TaTToo improves downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong generalizability across diverse TTS strategies.",
            "score": 13,
            "issue_id": 6298,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "bdefddb943fa9266",
            "authors": [
                "Jiaru Zou",
                "Soumya Roy",
                "Vinay Kumar Verma",
                "Ziyi Wang",
                "David Wipf",
                "Pan Lu",
                "Sumit Negi",
                "James Zou",
                "Jingrui He"
            ],
            "affiliations": [
                "Amazon",
                "Purdue University",
                "Stanford University",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06217.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#training",
                    "#data",
                    "#benchmark",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "TaTToo: Process Reward Model Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ TaTToo â€” Ğ½Ğ¾Ğ²ÑƒÑ Process Reward Model Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ² LLM. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ PRM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¾ ÑÑ…ĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. TaTToo Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ PRM Ñ 72B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 30.9% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, fact-checking Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Tabular Reasoning with TaTToo!",
                    "desc": "TaTToo is a new framework designed to improve how large reasoning models handle tables in machine learning. It focuses on specific operations related to tables, like retrieving sub-tables and interacting with schemas, which previous models struggled with. By using a combination of supervised learning and reinforcement learning, TaTToo provides better guidance for reasoning tasks involving tables. The results show that TaTToo significantly enhances performance on various tabular reasoning challenges, outperforming existing models with fewer parameters."
                },
                "zh": {
                    "title": "TaTTooï¼šæå‡è¡¨æ ¼æ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "TaTTooæ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºè¡¨æ ¼çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨æå‡è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡æ˜ç¡®å¤„ç†è¡¨æ ¼ç‰¹å®šæ“ä½œå’Œæ•´åˆå·¥å…·éªŒè¯ï¼Œæ˜¾è‘—æ”¹å–„äº†ç°æœ‰è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹åœ¨å¤„ç†è¡¨æ ¼æ¨ç†æ—¶å­˜åœ¨ç“¶é¢ˆï¼Œè€ŒTaTTooé€šè¿‡è®¾è®¡å¯æ‰©å±•çš„æ•°æ®æ•´ç†ç®¡é“å’ŒåŒé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œå…‹æœäº†è¿™äº›é™åˆ¶ã€‚ç»è¿‡è¯„ä¼°ï¼ŒTaTTooåœ¨å¤šä¸ªè¡¨æ ¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæå‡äº†ä¸‹æ¸¸å¤§è§„æ¨¡æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03270",
            "title": "CoDA: Coding LM via Diffusion Adaptation",
            "url": "https://huggingface.co/papers/2510.03270",
            "abstract": "CoDA, a 1.7B-parameter diffusion coder, achieves competitive performance with smaller models through confidence-guided sampling and is released with open-source tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models promise bidirectional context and infilling capabilities that autoregressive coders lack, yet practical systems remain heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU with a fully open-source training pipeline. CoDA pairs large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling that keeps inference latency competitive. On Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters. Our release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants.",
            "score": 5,
            "issue_id": 6298,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "49ba9b2131a56dc8",
            "authors": [
                "Haolin Chen",
                "Shiyu Wang",
                "Can Qin",
                "Bo Pang",
                "Zuxin Liu",
                "Jielin Qiu",
                "Jianguo Zhang",
                "Yingbo Zhou",
                "Zeyuan Chen",
                "Ran Xu",
                "Shelby Heinecke",
                "Silvio Savarese",
                "Caiming Xiong",
                "Huan Wang",
                "Weiran Yao"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03270.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#open_source",
                    "#diffusion",
                    "#small_models",
                    "#dataset"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹",
                    "desc": "CoDA â€” ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ 1.7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ² ĞºĞ¾Ğ´Ğµ. CoDA Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° TPU Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ confidence-guided sampling Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… HumanEval Ğ¸ MBPP Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ¾ 7B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹."
                },
                "en": {
                    "title": "CoDA: Lightweight Diffusion Coding with Competitive Performance",
                    "desc": "CoDA is a diffusion coder with 1.7 billion parameters that competes effectively with larger models by using confidence-guided sampling. It leverages a unique training approach that combines large-scale diffusion pre-training with code-focused mid-training and instruction tuning. This allows CoDA to maintain low inference latency while providing advanced capabilities like bidirectional context and infilling. The model is open-source, including tools and checkpoints to support further research in lightweight diffusion-based coding assistants."
                },
                "zh": {
                    "title": "CoDAï¼šè½»é‡çº§æ‰©æ•£ç¼–ç çš„æœªæ¥",
                    "desc": "CoDAæ˜¯ä¸€ç§å…·æœ‰17äº¿å‚æ•°çš„æ‰©æ•£ç¼–ç å™¨ï¼Œé€šè¿‡ä¿¡å¿ƒå¼•å¯¼é‡‡æ ·å®ç°äº†ä¸æ›´å°æ¨¡å‹çš„ç«äº‰æ€§èƒ½ã€‚å®ƒç»“åˆäº†å¤§è§„æ¨¡çš„æ‰©æ•£é¢„è®­ç»ƒå’Œä»¥ä»£ç ä¸ºä¸­å¿ƒçš„ä¸­æœŸè®­ç»ƒï¼Œä»¥åŠæŒ‡ä»¤è°ƒä¼˜ï¼Œä»è€Œä¿æŒäº†æ¨ç†å»¶è¿Ÿçš„ç«äº‰åŠ›ã€‚CoDAåœ¨Humanevalã€MBPPå’ŒEvalPlusç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¡¨ç°ä¸é«˜è¾¾70äº¿å‚æ•°çš„æ‰©æ•£æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ã€‚æˆ‘ä»¬å‘å¸ƒäº†æ¨¡å‹æ£€æŸ¥ç‚¹ã€è¯„ä¼°å·¥å…·å’ŒTPUè®­ç»ƒç®¡é“ï¼Œä»¥åŠ é€Ÿè½»é‡çº§æ‰©æ•£ç¼–ç åŠ©æ‰‹çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26328",
            "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
            "url": "https://huggingface.co/papers/2509.26328",
            "abstract": "Fast-dLLM v2, a block diffusion language model, efficiently converts pretrained autoregressive models for parallel text generation, achieving significant speedup without compromising accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.",
            "score": 4,
            "issue_id": 6298,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 30",
                "zh": "9æœˆ30æ—¥"
            },
            "hash": "d003eca6c18f4d37",
            "authors": [
                "Chengyue Wu",
                "Hao Zhang",
                "Shuchen Xue",
                "Shizhe Diao",
                "Yonggan Fu",
                "Zhijian Liu",
                "Pavlo Molchanov",
                "Ping Luo",
                "Song Han",
                "Enze Xie"
            ],
            "affiliations": [
                "MIT",
                "NVIDIA",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26328.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#open_source",
                    "#diffusion",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Fast-dLLM v2 â€” Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ LLM Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ²ÑĞµĞ³Ğ¾ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Ğ² 500 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ñƒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ². Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 2.5 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Speed Meets Accuracy: Fast-dLLM v2 Revolutionizes Text Generation",
                    "desc": "Fast-dLLM v2 is a block diffusion language model that transforms pretrained autoregressive models for faster text generation. It achieves this by using a novel block diffusion mechanism and a complementary attention mask, allowing for efficient parallel processing while maintaining the model's accuracy. The model requires significantly less fine-tuning data, only about 1 billion tokens, compared to traditional methods that need hundreds of billions. With a hierarchical caching system, Fast-dLLM v2 can generate text up to 2.5 times faster than standard autoregressive decoding, making it a powerful tool for natural language tasks."
                },
                "zh": {
                    "title": "å¿«é€Ÿé«˜æ•ˆçš„å—æ‰©æ•£è¯­è¨€æ¨¡å‹",
                    "desc": "Fast-dLLM v2æ˜¯ä¸€ç§å—æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å°†é¢„è®­ç»ƒçš„è‡ªå›å½’æ¨¡å‹è½¬æ¢ä¸ºå¹¶è¡Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹ä»…éœ€çº¦10äº¿ä¸ªæ ‡è®°è¿›è¡Œå¾®è°ƒï¼Œç›¸æ¯”äºå…¨æ³¨æ„åŠ›æ‰©æ•£æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®å‡å°‘äº†500å€ï¼ŒåŒæ—¶ä¿æŒäº†åŸå§‹æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥å—æ‰©æ•£æœºåˆ¶å’Œäº’è¡¥æ³¨æ„åŠ›æ©ç ï¼ŒFast-dLLM v2å®ç°äº†å—çº§åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œå¹¶è®¾è®¡äº†åˆ†å±‚ç¼“å­˜æœºåˆ¶ä»¥åŠ é€Ÿè§£ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFast-dLLM v2åœ¨å‡†ç¡®æ€§ä¸Šä¸è‡ªå›å½’åŸºçº¿ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶åœ¨æ•ˆç‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06218",
            "title": "EgoNight: Towards Egocentric Vision Understanding at Night with a\n  Challenging Benchmark",
            "url": "https://huggingface.co/papers/2510.06218",
            "abstract": "EgoNight is a comprehensive benchmark for nighttime egocentric vision, focusing on visual question answering and revealing performance gaps between day and night conditions for multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance.",
            "score": 1,
            "issue_id": 6298,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "a863fc0993d7f2f6",
            "authors": [
                "Deheng Zhang",
                "Yuqian Fu",
                "Runyi Yang",
                "Yang Miao",
                "Tianwen Qian",
                "Xu Zheng",
                "Guolei Sun",
                "Ajad Chhatkuli",
                "Xuanjing Huang",
                "Yu-Gang Jiang",
                "Luc Van Gool",
                "Danda Pani Paudel"
            ],
            "affiliations": [
                "East China Normal University",
                "Fudan University",
                "HKUST(GZ)",
                "INSAIT, Sofia University St. Kliment Ohridski",
                "Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06218.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#long_context",
                    "#benchmark",
                    "#games",
                    "#transfer_learning",
                    "#synthetic",
                    "#cv"
                ],
                "emoji": "ğŸŒ™",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° AI-Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞ¼Ğ½Ğ¾Ñ‚Ğµ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°",
                    "desc": "EgoNight â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ egocentric-Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ Ñ„Ğ¾ĞºÑƒÑĞ¾Ğ¼ Ğ½Ğ° visual question answering. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 3658 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° 90 Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ğº Ğ² Ğ´Ğ½ĞµĞ²Ğ½Ğ¾Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ñ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… multimodal LLM Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ´Ğ½ĞµĞ²Ğ½Ñ‹Ñ… Ğº Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼. ĞŸĞ¾Ğ¼Ğ¸Ğ¼Ğ¾ VQA, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ğ¾Ğ¸ÑĞº ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ-Ğ½Ğ¾Ñ‡ÑŒ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ² Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging the Gap: Nighttime Vision for AI",
                    "desc": "EgoNight is a new benchmark designed to improve nighttime egocentric vision, particularly in visual question answering (VQA). It highlights the performance differences of multimodal large language models (MLLMs) when operating in low-light conditions compared to daytime scenarios. The benchmark includes day-night aligned videos to enhance the quality of night annotations and reveals significant performance drops in models when transitioning from day to night. Additionally, EgoNight introduces tasks like day-night correspondence retrieval and egocentric depth estimation to further challenge and advance current models in this field."
                },
                "zh": {
                    "title": "å¤œé—´è§†è§‰é—®ç­”çš„æ–°åŸºå‡†ï¼šEgoNight",
                    "desc": "EgoNightæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºå¤œé—´è‡ªæˆ‘ä¸­å¿ƒè§†è§‰ï¼Œç‰¹åˆ«æ˜¯è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ã€‚ç°æœ‰çš„è‡ªæˆ‘ä¸­å¿ƒè§†è§‰åŸºå‡†ä¸»è¦é›†ä¸­åœ¨ç™½å¤©åœºæ™¯ï¼Œå¿½è§†äº†ä½å…‰ç…§æ¡ä»¶ä¸‹çš„åº”ç”¨éœ€æ±‚ã€‚EgoNighté€šè¿‡å¼•å…¥æ—¥å¤œå¯¹é½çš„è§†é¢‘ï¼Œæå‡äº†å¤œé—´æ ‡æ³¨çš„è´¨é‡ï¼Œå¹¶æ­ç¤ºäº†ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„æ€§èƒ½å·®è·ã€‚è¯¥åŸºå‡†åŒ…å«3658ä¸ªé—®ç­”å¯¹ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ï¼Œæ—¨åœ¨æ¨åŠ¨è‡ªæˆ‘ä¸­å¿ƒè§†è§‰ç ”ç©¶çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06139",
            "title": "Deforming Videos to Masks: Flow Matching for Referring Video\n  Segmentation",
            "url": "https://huggingface.co/papers/2510.06139",
            "abstract": "FlowRVS addresses the challenges of Referring Video Object Segmentation by reformulating the task as a continuous flow problem, leveraging pretrained T2V models and achieving state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a J&F of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.",
            "score": 1,
            "issue_id": 6298,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 7",
                "zh": "10æœˆ7æ—¥"
            },
            "hash": "27bd729b0bb095f1",
            "authors": [
                "Zanyi Wang",
                "Dengyang Jiang",
                "Liuzhuozheng Li",
                "Sizhe Dang",
                "Chengzu Li",
                "Harry Yang",
                "Guang Dai",
                "Mengmeng Wang",
                "Jingdong Wang"
            ],
            "affiliations": [
                "Baidu",
                "SGIT AI Lab, State Grid Corporation of China",
                "The Hong Kong University of Science and Technology",
                "The University of Tokyo",
                "University of California, San Diego",
                "University of Cambridge",
                "Zhejiang University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06139.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#games",
                    "#alignment"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "FlowRVS Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ (RVOS), Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ ĞµÑ‘ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Â«Ğ½Ğ°Ğ¹Ñ‚Ğ¸-Ğ·Ğ°Ñ‚ĞµĞ¼-ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÂ», Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ pretrained text-to-video Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ğ¼Ğ°ÑĞºÑƒ Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… RVOS, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MeViS Ğ¸ Ref-DAVIS17."
                },
                "en": {
                    "title": "Revolutionizing Video Segmentation with Continuous Flow",
                    "desc": "FlowRVS introduces a new way to tackle Referring Video Object Segmentation (RVOS) by treating it as a continuous flow problem. This approach allows the model to better connect language descriptions to specific video pixels, improving the segmentation process. Unlike previous methods that separate locating and segmenting tasks, FlowRVS maintains a unified framework that enhances temporal consistency and semantic understanding. By leveraging pretrained T2V models, it achieves state-of-the-art performance on major RVOS benchmarks, showcasing the effectiveness of continuous deformation in video understanding."
                },
                "zh": {
                    "title": "FlowRVSï¼šè§†é¢‘ç‰©ä½“åˆ†å‰²çš„æ–°æ€è·¯",
                    "desc": "FlowRVSæå‡ºäº†ä¸€ç§æ–°æ–¹æ³•æ¥è§£å†³è§†é¢‘ç‰©ä½“åˆ†å‰²ä¸­çš„å¼•ç”¨é—®é¢˜ï¼Œå°†å…¶é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªè¿ç»­æµåŠ¨é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ï¼Œå®ç°äº†å¯¹è§†é¢‘ä¸­ç›®æ ‡çš„ç²¾ç»†åƒç´ æ§åˆ¶å’Œè¯­ä¹‰å¯¹é½ã€‚ä¸ä¼ ç»Ÿçš„â€œå®šä½-å†åˆ†å‰²â€æµç¨‹ä¸åŒï¼ŒFlowRVSé€šè¿‡ç›´æ¥å­¦ä¹ è¯­è¨€å¼•å¯¼çš„å˜å½¢ï¼Œä»è§†é¢‘çš„æ•´ä½“è¡¨ç¤ºåˆ°ç›®æ ‡æ©è†œï¼Œä¿æŒäº†æ—¶é—´ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶åœ¨ä¸»è¦çš„å¼•ç”¨è§†é¢‘ç‰©ä½“åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œå±•ç¤ºäº†å°†è§†é¢‘ç†è§£ä»»åŠ¡å»ºæ¨¡ä¸ºè¿ç»­å˜å½¢è¿‡ç¨‹çš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05432",
            "title": "AInstein: Assessing the Feasibility of AI-Generated Approaches to\n  Research Problems",
            "url": "https://huggingface.co/papers/2510.05432",
            "abstract": "AInstein evaluates the problem-solving capabilities of large language models by testing their ability to generate valid solutions to AI research problems using only pretrained knowledge, revealing both their potential and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.",
            "score": 1,
            "issue_id": 6298,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "effef15175939fca",
            "authors": [
                "Shambhavi Mishra",
                "Gaurav Sahu",
                "Marco Pedersoli",
                "Laurent Charlin",
                "Jose Dolz",
                "Christopher Pal"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "HEC Montreal",
                "International Laboratory on Learning Systems (ILLS)",
                "LIVIA, ETS Montreal",
                "Mila Quebec AI Institute",
                "Polytechnique Montreal",
                "ServiceNow Research",
                "Universite de Montreal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05432.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#benchmark",
                    "#rlhf",
                    "#agents"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "ĞœĞ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ AI ÑÑ‚Ğ°Ñ‚ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸?",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AInstein â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ AI, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ĞµĞ¹ ICLR 2025 Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾ Ñ‚Ñ€Ñ‘Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼: ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹, Ğ½Ğ¾ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ñ…Ñ€ÑƒĞ¿ĞºĞ¸Ğ¼Ğ¸ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "AInstein: Unveiling the Problem-Solving Power of LLMs",
                    "desc": "The paper introduces AInstein, a framework designed to evaluate the problem-solving abilities of large language models (LLMs) in generating valid solutions to AI research problems using only their pretrained knowledge. It tests LLMs without any fine-tuning or external aids, focusing on their capacity to produce solutions through iterative critique loops similar to scientific review processes. The evaluation is based on 1,214 ICLR papers and uses metrics like Success Rate, Rediscovery, and Novelty to assess the LLMs' performance. The findings indicate that while LLMs can rediscover existing solutions and occasionally suggest novel ideas, their problem-solving capabilities are fragile and highly dependent on how problems are framed."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç§‘å­¦é—®é¢˜è§£å†³èƒ½åŠ›",
                    "desc": "AInsteinæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³é—®é¢˜èƒ½åŠ›çš„æ¡†æ¶ã€‚å®ƒæµ‹è¯•è¿™äº›æ¨¡å‹åœ¨æ²¡æœ‰é¢†åŸŸç‰¹å®šå¾®è°ƒæˆ–å¤–éƒ¨å¸®åŠ©çš„æƒ…å†µä¸‹ï¼Œæ˜¯å¦èƒ½å¤Ÿç”Ÿæˆæœ‰æ•ˆçš„AIç ”ç©¶é—®é¢˜è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æå–é«˜è´¨é‡ICLR 2025æäº¤çš„ç²¾ç‚¼é—®é¢˜é™ˆè¿°ï¼ŒAInsteinæ¨¡æ‹Ÿç§‘å­¦ç ”ç©¶ä¸­çš„ææ¡ˆã€å®¡æŸ¥å’Œä¿®è®¢å¾ªç¯ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMsèƒ½å¤Ÿé‡æ–°å‘ç°å¯è¡Œçš„è§£å†³æ–¹æ¡ˆå¹¶å¶å°”æå‡ºåˆ›é€ æ€§çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å®ƒä»¬çš„è§£å†³é—®é¢˜èƒ½åŠ›ä»ç„¶è„†å¼±ï¼Œä¸”å¯¹é—®é¢˜çš„è¡¨è¿°éå¸¸æ•æ„Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05342",
            "title": "Margin Adaptive DPO: Leveraging Reward Model for Granular Control in\n  Preference Optimization",
            "url": "https://huggingface.co/papers/2510.05342",
            "abstract": "MADPO, a margin-adaptive method, enhances preference alignment in large language models by providing instance-level adaptive weighting to the DPO loss, improving performance across datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of beta-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative beta values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\\% on High Quality data and +10.5\\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.",
            "score": 0,
            "issue_id": 6298,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "1a257d25fefce283",
            "authors": [
                "Hyung Gyu Rho"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2510.05342.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ´ĞµĞ»Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ",
                    "desc": "MADPO â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ DPO Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ reward model Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğº loss-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ DPO Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ sample. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ…, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ Ğ±Ğ°Ñ‚Ñ‡-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ uniformĞ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ +33.3% Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ baseline Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Preference Alignment with Adaptive Weighting",
                    "desc": "MADPO, or Margin-Adaptive Direct Preference Optimization, is a novel method designed to improve the alignment of large language models by adapting the weighting of the DPO loss at the instance level. This approach addresses the limitations of previous methods by providing a continuous and adaptive weight based on the estimated preference margins for each training sample. By amplifying the learning signal for difficult examples and reducing it for easier ones, MADPO enhances the model's ability to learn from diverse datasets effectively. Experimental results demonstrate that MADPO significantly outperforms existing methods, achieving notable performance improvements across various data quality levels."
                },
                "zh": {
                    "title": "è¾¹é™…è‡ªé€‚åº”ä¼˜åŒ–ï¼Œæå‡æ¨¡å‹åå¥½å¯¹é½",
                    "desc": "MADPOæ˜¯ä¸€ç§è¾¹é™…è‡ªé€‚åº”æ–¹æ³•ï¼Œé€šè¿‡ä¸ºDPOæŸå¤±æä¾›å®ä¾‹çº§è‡ªé€‚åº”åŠ æƒï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„åå¥½å¯¹é½èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹æ¥ä¼°è®¡åå¥½è¾¹é™…ï¼Œç„¶åæ ¹æ®è¿™äº›è¾¹é™…ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬åº”ç”¨è¿ç»­çš„è‡ªé€‚åº”æƒé‡ã€‚MADPOçš„é‡åŠ æƒæ–¹æ¡ˆå¯¹å›°éš¾æ ·æœ¬å¢å¼ºä¿¡å·ï¼Œå¯¹ç®€å•æ ·æœ¬å‡å¼±ä¿¡å·ï¼Œä»è€Œå®ç°äº†å¯¹å­¦ä¹ ä¿¡å·çš„ç»†è‡´æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMADPOåœ¨æƒ…æ„Ÿç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–å¼ºåŸºçº¿ï¼Œè¯æ˜äº†å…¶åœ¨åå¥½å¯¹é½æ–¹é¢çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04087",
            "title": "A Contextual Quality Reward Model for Reliable and Efficient Best-of-N\n  Sampling",
            "url": "https://huggingface.co/papers/2510.04087",
            "abstract": "A new framework using an outside option in preference data collection and modeling improves reliability and efficiency in preference alignment techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is better, but what is good enough. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70\\%, and when tuned as an inference accelerator, it improves average inference speed by over 22\\% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.",
            "score": 0,
            "issue_id": 6298,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 5",
                "zh": "10æœˆ5æ—¥"
            },
            "hash": "77a7f937000c3b18",
            "authors": [
                "Hyung Gyu Rho"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2510.04087.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#data",
                    "#rlhf",
                    "#alignment"
                ],
                "emoji": "ğŸš¦",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Â«Ğ²Ğ½ĞµÑˆĞ½ÑÑ Ğ¾Ğ¿Ñ†Ğ¸ÑÂ» Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ reward models ÑƒĞ¼ĞµÑÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, ĞºĞ°ĞºĞ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ, Ğ½Ğ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ²Ğ¾Ğ¾Ğ±Ñ‰Ğµ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ğ¼. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ½Ğ¾ Ğ¸ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° 70% Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ inference Ğ½Ğ° 22%."
                },
                "en": {
                    "title": "Enhancing Preference Alignment with Outside Options",
                    "desc": "This paper presents a new framework for improving preference alignment techniques in machine learning by incorporating an outside option in data collection. Traditional methods, like Best-of-N sampling, often fail to identify acceptable responses, leading to poor choices among suboptimal options. The proposed framework enhances reward models by allowing them to recognize not only better options but also those that are sufficiently good. Experimental results demonstrate significant improvements in reliability and efficiency, reducing failures by 70% and increasing inference speed by over 22%."
                },
                "zh": {
                    "title": "æå‡åå¥½å¯¹é½çš„å¯é æ€§ä¸æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨åå¥½æ•°æ®æ”¶é›†å’Œå»ºæ¨¡ä¸­å¼•å…¥å¤–éƒ¨é€‰é¡¹ï¼Œæå‡äº†åå¥½å¯¹é½æŠ€æœ¯çš„å¯é æ€§å’Œæ•ˆç‡ã€‚ç°ä»£çš„åå¥½å¯¹é½æŠ€æœ¯ï¼Œå¦‚æœ€ä½³Nï¼ˆBoNï¼‰é‡‡æ ·ï¼Œä¾èµ–äºé€šè¿‡æˆå¯¹æ¯”è¾ƒæ•°æ®è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œè™½ç„¶èƒ½æœ‰æ•ˆå­¦ä¹ ç›¸å¯¹åå¥½ï¼Œä½†æœªèƒ½æ•æ‰å“åº”å¯æ¥å—æ€§çš„ä¿¡å·ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥å¤–éƒ¨é€‰é¡¹ï¼Œè®­ç»ƒå‡ºèƒ½å¤ŸåŒºåˆ†ä¸ä»…æ˜¯æ›´å¥½è€Œæ˜¯è¶³å¤Ÿå¥½çš„å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œè§£å†³äº†å¯é æ€§ç¼ºå£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¯¹é½å’Œæ¨ç†åŠ é€Ÿæ–¹é¢å‡æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œæä¾›äº†ä¸€ä¸ªçµæ´»çš„ç®¡ç†å¯é æ€§ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´æƒè¡¡çš„å·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05137",
            "title": "Demystifying deep search: a holistic evaluation with hint-free multi-hop\n  questions and factorised metrics",
            "url": "https://huggingface.co/papers/2510.05137",
            "abstract": "WebDetective is a benchmark for evaluating multi-hop reasoning in RAG systems and web agents, addressing issues of reasoning path leakage and single-pass evaluation, and introducing a framework to improve knowledge utilization and refusal behavior.  \t\t\t\t\tAI-generated summary \t\t\t\t RAG (Retrieval-Augmented Generation) systems and web agents are increasingly evaluated on multi-hop deep search tasks, yet current practice suffers from two major limitations. First, most benchmarks leak the reasoning path in the question text, allowing models to follow surface cues rather than discover reasoning chains autonomously. Second, evaluation is typically reduced to a single pass rate, which collapses diverse behaviours into one score and obscures whether failures stem from inadequate search, poor knowledge use, or inappropriate refusal. To address these issues, we present WebDetective, a benchmark of hint-free multi-hop questions paired with a controlled Wikipedia sandbox that ensures full traceability of model actions, and a holistic evaluation framework that separates search sufficiency, knowledge utilisation, and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals systematic weaknesses across all architectures: models struggle with knowledge utilisation despite having sufficient evidence and demonstrate near-absent appropriate refusal when evidence is lacking. These patterns expose a fundamental gap: today's systems excel at executing given reasoning paths but fail when required to discover them. We develop an agentic workflow, EvidenceLoop, that explicitly targets the challenges our benchmark identifies, incorporating verification loops and systematic evidence tracking that improve both search and synthesis capabilities. This baseline demonstrates that WebDetective's diagnostic framework can guide concrete architectural improvements, establishing our benchmark as a critical tool for developing genuinely autonomous reasoning systems rather than pattern-following agents.",
            "score": 0,
            "issue_id": 6298,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 1",
                "zh": "10æœˆ1æ—¥"
            },
            "hash": "d780a77899923141",
            "authors": [
                "Maojia Song",
                "Renhang Liu",
                "Xinyu Wang",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Soujanya Poria",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Nanyang Technological University (NTU)",
                "Singapore University of Technology and Design (SUTD)",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05137.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#rag",
                    "#benchmark",
                    "#leakage",
                    "#architecture",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "WebDetective: ĞšĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ° Ğ½Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼",
                    "desc": "WebDetective â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Â«ÑƒÑ‚ĞµÑ‡ĞºÑƒÂ» Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞµ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Wikipedia Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 25 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ: ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ."
                },
                "en": {
                    "title": "WebDetective: Enhancing Multi-Hop Reasoning in AI Systems",
                    "desc": "WebDetective is a new benchmark designed to evaluate how well RAG systems and web agents can perform multi-hop reasoning without leaking reasoning paths in the questions. It addresses the limitations of current evaluation methods that oversimplify model performance into a single score, which can hide specific weaknesses in knowledge utilization and refusal behavior. The benchmark includes hint-free questions and a controlled environment to track model actions, allowing for a more detailed analysis of how models search for information and use knowledge. The findings reveal that many models struggle with effectively utilizing available evidence and often fail to refuse when they lack sufficient information, highlighting the need for improvements in autonomous reasoning capabilities."
                },
                "zh": {
                    "title": "WebDetectiveï¼šæå‡å¤šè·³æ¨ç†çš„è¯„ä¼°æ ‡å‡†",
                    "desc": "WebDetectiveæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°RAGç³»ç»Ÿå’Œç½‘ç»œä»£ç†çš„å¤šè·³æ¨ç†åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³æ¨ç†è·¯å¾„æ³„æ¼å’Œå•æ¬¡è¯„ä¼°çš„é—®é¢˜ã€‚è¯¥åŸºå‡†æä¾›æ— æç¤ºçš„å¤šè·³é—®é¢˜ï¼Œå¹¶é…å¤‡ä¸€ä¸ªå—æ§çš„ç»´åŸºç™¾ç§‘æ²™ç®±ï¼Œä»¥ç¡®ä¿æ¨¡å‹è¡Œä¸ºçš„å¯è¿½æº¯æ€§ã€‚é€šè¿‡å¯¹25ä¸ªæœ€å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹åœ¨çŸ¥è¯†åˆ©ç”¨æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§å¼±ç‚¹ï¼Œå°½ç®¡æœ‰è¶³å¤Ÿçš„è¯æ®ï¼Œä½†åœ¨ç¼ºä¹è¯æ®æ—¶å‡ ä¹æ²¡æœ‰é€‚å½“çš„æ‹’ç»è¡Œä¸ºã€‚æˆ‘ä»¬å¼€å‘äº†EvidenceLoopå·¥ä½œæµç¨‹ï¼Œä¸“é—¨é’ˆå¯¹åŸºå‡†è¯†åˆ«çš„æŒ‘æˆ˜ï¼Œæ”¹è¿›äº†æœç´¢å’Œç»¼åˆèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02341",
            "title": "DRIFT: Learning from Abundant User Dissatisfaction in Real-World\n  Preference Learning",
            "url": "https://huggingface.co/papers/2510.02341",
            "abstract": "DRIFT, a dissatisfaction-refined iterative preference training method, improves large language models using implicit user dissatisfaction signals, achieving better performance than existing methods on real-world datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce DRIFT (Dissatisfaction-Refined Iterative preFerence Training), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world WildFeedback datasets and synthetic UltraFeedback datasets achieve up to +6.23\\% (7B) / +7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.",
            "score": 0,
            "issue_id": 6298,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "1bde94710320cd61",
            "authors": [
                "Yifan Wang",
                "Bolian Li",
                "Junlin Wu",
                "Zhaoxuan Tan",
                "Zheli Liu",
                "Ruqi Zhang",
                "Ananth Grama",
                "Qingkai Zeng"
            ],
            "affiliations": [
                "College of Computer Science, Nankai University",
                "Department of Computer Science and Engineering, University of Notre Dame",
                "Department of Computer Science, Purdue University",
                "Department of Computer Science, Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02341.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ½Ğ° Ğ½ĞµĞ´Ğ¾Ğ²Ğ¾Ğ»ÑŒÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹: Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DRIFT Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½ĞµĞ´Ğ¾Ğ²Ğ¾Ğ»ÑŒÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ’ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑĞ²Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ÑÑ Ñ€ĞµĞ´ĞºĞ¾. DRIFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½ĞµĞ´Ğ¾Ğ²Ğ¾Ğ»ÑŒÑÑ‚Ğ²Ğ° ĞºĞ°Ğº ÑĞºĞ¾Ñ€ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… WildFeedback Ğ¸ UltraFeedback Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ +12.29% Ğ¿Ğ¾ win rate Ğ½Ğ° AlpacaEval2, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº DPO Ğ¸ SPIN, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Harnessing User Dissatisfaction for Better Language Models",
                    "desc": "The paper introduces DRIFT, a novel training method for large language models that utilizes implicit user dissatisfaction signals to enhance performance. Unlike traditional methods that depend on explicit positive feedback, DRIFT focuses on refining preferences based on real-world user interactions, which often include dissatisfaction. This approach allows for dynamic sampling of positive responses, leading to improved model performance on various tasks. Empirical results demonstrate that DRIFT significantly outperforms existing methods, particularly in larger models, while also maintaining diversity in generated outputs."
                },
                "zh": {
                    "title": "åˆ©ç”¨ç”¨æˆ·ä¸æ»¡ä¿¡å·æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "DRIFTæ˜¯ä¸€ç§åŸºäºç”¨æˆ·ä¸æ»¡ä¿¡å·çš„è¿­ä»£åå¥½è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨çœŸå®ä¸–ç•Œä¸­çš„ç”¨æˆ·ä¸æ»¡ä¿¡å·ï¼ŒåŠ¨æ€é‡‡æ ·æ­£åé¦ˆï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ç”¨æˆ·çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨DRIFTè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚DRIFTä¸ä»…æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿˜ä¿æŒäº†æ¢ç´¢èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å¤šæ ·åŒ–çš„é«˜å¥–åŠ±è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-07.html",
    "link_next": "2025-10-09.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "07.10",
        "en": "10/07",
        "zh": "10æœˆ7æ—¥"
    },
    "short_date_next": {
        "ru": "09.10",
        "en": "10/09",
        "zh": "10æœˆ9æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 2,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 4,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 0
    }
}