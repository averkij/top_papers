{
    "date": {
        "ru": "8 октября",
        "en": "October 8",
        "zh": "10月8日"
    },
    "time_utc": "2025-10-08 03:24",
    "weekday": 2,
    "issue_id": 6299,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.06217",
            "title": "TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.06217",
            "abstract": "TaTToo, a novel table-grounded Process Reward Model, enhances tabular reasoning by explicitly addressing table-specific operations and integrating tool-based verification, leading to significant performance improvements over existing PRMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have recently emerged as a powerful framework for enhancing the reasoning capabilities of large reasoning models (LRMs), particularly in the context of test-time scaling (TTS). However, their potential for supervising LRMs on tabular reasoning domains remains underexplored. Through detailed empirical analyses, we identify that existing PRMs, though widely adopted for supervising text-only reasoning steps, struggle with table-specific operations such as sub-table retrieval and schema interaction, leading to critical performance bottlenecks. To address this limitation, we propose TaTToo, a novel table-grounded PRM framework that (i) reasons explicitly over tabular reasoning steps and (ii) integrates tool-based verification to provide precise reward supervision. Concretely, we first design a scalable data curation pipeline that constructs over 60k high-quality step-level annotations by integrating table verification rationales with tool-based executions. Building on the collected data, we train TaTToo with a dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use reasoning patterns, followed by reinforcement learning with tool-grounded reward shaping to align our model with table-based verification. We provide a comprehensive evaluation of the policy improvement induced by our newly designed PRM. Across 5 challenging tabular reasoning benchmarks covering numerical reasoning, fact-checking, and data analysis, TaTToo improves downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong generalizability across diverse TTS strategies.",
            "score": 21,
            "issue_id": 6298,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 октября",
                "en": "October 7",
                "zh": "10月7日"
            },
            "hash": "bdefddb943fa9266",
            "authors": [
                "Jiaru Zou",
                "Soumya Roy",
                "Vinay Kumar Verma",
                "Ziyi Wang",
                "David Wipf",
                "Pan Lu",
                "Sumit Negi",
                "James Zou",
                "Jingrui He"
            ],
            "affiliations": [
                "Amazon",
                "Purdue University",
                "Stanford University",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06217.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#training",
                    "#data",
                    "#benchmark",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "TaTToo: Process Reward Model с инструментами для работы с таблицами",
                    "desc": "Исследователи представили TaTToo — новую Process Reward Model для улучшения рассуждений над табличными данными в LLM. Существующие PRM плохо справляются с табличными операциями вроде извлечения подтаблиц и работы со схемой данных. TaTToo решает эту проблему через явное моделирование табличных операций и интеграцию инструментов для верификации шагов рассуждений. Модель с 8 миллиардами параметров превосходит базовые PRM с 72B параметрами и улучшает точность на 30.9% в задачах численного анализа, fact-checking и работы с данными."
                },
                "en": {
                    "title": "Revolutionizing Tabular Reasoning with TaTToo!",
                    "desc": "TaTToo is a new framework designed to improve how large reasoning models handle tables in machine learning. It focuses on specific operations related to tables, like retrieving sub-tables and interacting with schemas, which previous models struggled with. By using a combination of supervised learning and reinforcement learning, TaTToo provides better guidance for reasoning tasks involving tables. The results show that TaTToo significantly enhances performance on various tabular reasoning challenges, outperforming existing models with fewer parameters."
                },
                "zh": {
                    "title": "TaTToo：提升表格推理的新方法",
                    "desc": "TaTToo是一种新颖的基于表格的过程奖励模型，旨在提升表格推理能力。它通过明确处理表格特定操作和整合工具验证，显著改善了现有过程奖励模型的性能。研究表明，现有的过程奖励模型在处理表格推理时存在瓶颈，而TaTToo通过设计可扩展的数据整理管道和双阶段训练方法，克服了这些限制。经过评估，TaTToo在多个表格推理基准测试中表现优异，提升了下游大规模推理模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.26328",
            "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
            "url": "https://huggingface.co/papers/2509.26328",
            "abstract": "Fast-dLLM v2, a block diffusion language model, efficiently converts pretrained autoregressive models for parallel text generation, achieving significant speedup without compromising accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.",
            "score": 16,
            "issue_id": 6298,
            "pub_date": "2025-09-30",
            "pub_date_card": {
                "ru": "30 сентября",
                "en": "September 30",
                "zh": "9月30日"
            },
            "hash": "d003eca6c18f4d37",
            "authors": [
                "Chengyue Wu",
                "Hao Zhang",
                "Shuchen Xue",
                "Shizhe Diao",
                "Yonggan Fu",
                "Zhijian Liu",
                "Pavlo Molchanov",
                "Ping Luo",
                "Song Han",
                "Enze Xie"
            ],
            "affiliations": [
                "MIT",
                "NVIDIA",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.26328.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#open_source",
                    "#diffusion",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Быстрая параллельная генерация текста через блочную диффузию",
                    "desc": "Статья представляет Fast-dLLM v2 — блочную диффузионную языковую модель, которая эффективно преобразует предобученные авторегрессионные LLM для параллельной генерации текста. Ключевое преимущество подхода в том, что требуется всего 1 миллиард токенов для дообучения (в 500 раз меньше, чем у полноценных диффузионных моделей). Модель использует блочный механизм диффузии с иерархической системой кэширования, что позволяет сохранять контекст и генерировать текст параллельно внутри блоков. В результате достигается ускорение в 2.5 раза по сравнению со стандартной авторегрессионной генерацией без потери качества."
                },
                "en": {
                    "title": "Speed Meets Accuracy: Fast-dLLM v2 Revolutionizes Text Generation",
                    "desc": "Fast-dLLM v2 is a block diffusion language model that transforms pretrained autoregressive models for faster text generation. It achieves this by using a novel block diffusion mechanism and a complementary attention mask, allowing for efficient parallel processing while maintaining the model's accuracy. The model requires significantly less fine-tuning data, only about 1 billion tokens, compared to traditional methods that need hundreds of billions. With a hierarchical caching system, Fast-dLLM v2 can generate text up to 2.5 times faster than standard autoregressive decoding, making it a powerful tool for natural language tasks."
                },
                "zh": {
                    "title": "快速高效的块扩散语言模型",
                    "desc": "Fast-dLLM v2是一种块扩散语言模型，能够高效地将预训练的自回归模型转换为并行文本生成模型。该模型仅需约10亿个标记进行微调，相比于全注意力扩散模型，训练数据减少了500倍，同时保持了原始模型的性能。通过引入块扩散机制和互补注意力掩码，Fast-dLLM v2实现了块级双向上下文建模，并设计了分层缓存机制以加速解码。实验结果表明，Fast-dLLM v2在准确性上与自回归基线相当或更优，同时在效率上达到了最先进的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03270",
            "title": "CoDA: Coding LM via Diffusion Adaptation",
            "url": "https://huggingface.co/papers/2510.03270",
            "abstract": "CoDA, a 1.7B-parameter diffusion coder, achieves competitive performance with smaller models through confidence-guided sampling and is released with open-source tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models promise bidirectional context and infilling capabilities that autoregressive coders lack, yet practical systems remain heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU with a fully open-source training pipeline. CoDA pairs large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling that keeps inference latency competitive. On Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters. Our release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants.",
            "score": 10,
            "issue_id": 6298,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "49ba9b2131a56dc8",
            "authors": [
                "Haolin Chen",
                "Shiyu Wang",
                "Can Qin",
                "Bo Pang",
                "Zuxin Liu",
                "Jielin Qiu",
                "Jianguo Zhang",
                "Yingbo Zhou",
                "Zeyuan Chen",
                "Ran Xu",
                "Shelby Heinecke",
                "Silvio Savarese",
                "Caiming Xiong",
                "Huan Wang",
                "Weiran Yao"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03270.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#open_source",
                    "#diffusion",
                    "#small_models",
                    "#dataset"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Легковесный диффузионный кодер с направленной генерацией",
                    "desc": "CoDA — это языковая модель на основе диффузии с 1.7 миллиардами параметров, специально обученная для генерации кода. В отличие от авторегрессивных моделей, диффузионные модели используют двунаправленный контекст и лучше справляются с заполнением пропусков в коде. CoDA обучалась на TPU с использованием открытого пайплайна и применяет confidence-guided sampling для ускорения инференса. На бенчмарках HumanEval и MBPP модель показывает результаты, сопоставимые с диффузионными моделями размером до 7B параметров, при этом оставаясь компактной."
                },
                "en": {
                    "title": "CoDA: Lightweight Diffusion Coding with Competitive Performance",
                    "desc": "CoDA is a diffusion coder with 1.7 billion parameters that competes effectively with larger models by using confidence-guided sampling. It leverages a unique training approach that combines large-scale diffusion pre-training with code-focused mid-training and instruction tuning. This allows CoDA to maintain low inference latency while providing advanced capabilities like bidirectional context and infilling. The model is open-source, including tools and checkpoints to support further research in lightweight diffusion-based coding assistants."
                },
                "zh": {
                    "title": "CoDA：轻量级扩散编码的未来",
                    "desc": "CoDA是一种具有17亿参数的扩散编码器，通过信心引导采样实现了与更小模型的竞争性能。它结合了大规模的扩散预训练和以代码为中心的中期训练，以及指令调优，从而保持了推理延迟的竞争力。CoDA在Humaneval、MBPP和EvalPlus等基准测试中，表现与高达70亿参数的扩散模型相当或更好。我们发布了模型检查点、评估工具和TPU训练管道，以加速轻量级扩散编码助手的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05560",
            "title": "HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video",
            "url": "https://huggingface.co/papers/2510.05560",
            "abstract": "HoloScene is an interactive 3D reconstruction framework that achieves geometry completeness, object interactivity, physical plausibility, photorealistic rendering, and realistic physical properties through an energy-based optimization problem.  \t\t\t\t\tAI-generated summary \t\t\t\t Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in a variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and scene-understanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, a novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages a comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and inter-object relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into a unified, coherent objective. Optimization is efficiently performed via a hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScene's broad applicability and effectiveness. Project page: https://xiahongchi.github.io/HoloScene.",
            "score": 4,
            "issue_id": 6299,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 октября",
                "en": "October 7",
                "zh": "10月7日"
            },
            "hash": "fcf790fe9803a797",
            "authors": [
                "Hongchi Xia",
                "Chih-Hao Lin",
                "Hao-Yu Hsu",
                "Quentin Leboutet",
                "Katelyn Gao",
                "Michael Paulitsch",
                "Benjamin Ummenhofer",
                "Shenlong Wang"
            ],
            "affiliations": [
                "Intel",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05560.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "🏗️",
                "ru": {
                    "title": "Цифровые двойники с физикой и фотореализмом",
                    "desc": "HoloScene — это фреймворк для интерактивной 3D-реконструкции физического мира в виртуальные среды, готовые к симуляции. Система использует граф сцены, который кодирует геометрию объектов, их внешний вид, физические свойства и взаимосвязи между ними. Реконструкция формулируется как задача энергетической оптимизации, объединяющая данные наблюдений, физические ограничения и генеративные prior'ы через гибридный подход с sampling и градиентными методами. Результат — полные цифровые двойники с точной геометрией, физической стабильностью и фотореалистичным рендерингом для AR/VR, игр и робототехники."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction with HoloScene",
                    "desc": "HoloScene is a cutting-edge framework for creating interactive 3D reconstructions that meet essential criteria for realistic simulations. It addresses common shortcomings in existing methods by ensuring geometry completeness, object interactivity, physical plausibility, and photorealistic rendering. The framework utilizes an energy-based optimization approach that combines observational data and physical constraints to produce accurate digital twins. Its effectiveness is demonstrated through superior performance on benchmark datasets and practical applications in gaming and digital-twin manipulation."
                },
                "zh": {
                    "title": "HoloScene：实现真实感的交互式3D重建",
                    "desc": "HoloScene是一个交互式3D重建框架，旨在实现几何完整性、物体交互性、物理合理性、照片级渲染和真实的物理属性。该框架通过能量优化问题来整合观察数据、物理约束和生成先验，形成一个统一的目标。HoloScene利用全面的交互场景图表示，编码物体的几何形状、外观和物理属性，同时考虑层次和物体间的关系。通过结合基于采样的探索和基于梯度的细化，优化过程高效进行，最终生成的数字双胞胎在新视角下展现出完整精确的几何形状和真实的渲染效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05432",
            "title": "AInstein: Assessing the Feasibility of AI-Generated Approaches to\n  Research Problems",
            "url": "https://huggingface.co/papers/2510.05432",
            "abstract": "AInstein evaluates the problem-solving capabilities of large language models by testing their ability to generate valid solutions to AI research problems using only pretrained knowledge, revealing both their potential and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.",
            "score": 4,
            "issue_id": 6298,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 октября",
                "en": "October 6",
                "zh": "10月6日"
            },
            "hash": "effef15175939fca",
            "authors": [
                "Shambhavi Mishra",
                "Gaurav Sahu",
                "Marco Pedersoli",
                "Laurent Charlin",
                "Jose Dolz",
                "Christopher Pal"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "HEC Montreal",
                "International Laboratory on Learning Systems (ILLS)",
                "LIVIA, ETS Montreal",
                "Mila Quebec AI Institute",
                "Polytechnique Montreal",
                "ServiceNow Research",
                "Universite de Montreal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05432.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#benchmark",
                    "#rlhf",
                    "#agents"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Может ли AI стать самостоятельным исследователем в машинном обучении?",
                    "desc": "Исследование представляет AInstein — фреймворк для оценки способности больших языковых моделей (LLM) решать исследовательские задачи в области AI, используя только предобученные знания без файнтюнинга или внешних источников. Модели анализируют реальные задачи из статей ICLR 2025 и предлагают решения через итеративные циклы генерации и критики, имитируя научный процесс. Оценка проводится по трём метрикам: успешность решения, способность переоткрыть существующие методы и генерация новых валидных подходов. Результаты показывают, что LLM могут находить осмысленные решения и иногда предлагать креативные альтернативы, но их способности остаются хрупкими и сильно зависят от формулировки задачи."
                },
                "en": {
                    "title": "AInstein: Unveiling the Problem-Solving Power of LLMs",
                    "desc": "The paper introduces AInstein, a framework designed to evaluate the problem-solving abilities of large language models (LLMs) in generating valid solutions to AI research problems using only their pretrained knowledge. It tests LLMs without any fine-tuning or external aids, focusing on their capacity to produce solutions through iterative critique loops similar to scientific review processes. The evaluation is based on 1,214 ICLR papers and uses metrics like Success Rate, Rediscovery, and Novelty to assess the LLMs' performance. The findings indicate that while LLMs can rediscover existing solutions and occasionally suggest novel ideas, their problem-solving capabilities are fragile and highly dependent on how problems are framed."
                },
                "zh": {
                    "title": "评估大型语言模型的科学问题解决能力",
                    "desc": "AInstein是一个评估大型语言模型（LLMs）解决问题能力的框架。它测试这些模型在没有领域特定微调或外部帮助的情况下，是否能够生成有效的AI研究问题解决方案。通过提取高质量ICLR 2025提交的精炼问题陈述，AInstein模拟科学研究中的提案、审查和修订循环。研究结果表明，尽管LLMs能够重新发现可行的解决方案并偶尔提出创造性的替代方案，但它们的解决问题能力仍然脆弱，且对问题的表述非常敏感。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05367",
            "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation",
            "url": "https://huggingface.co/papers/2510.05367",
            "abstract": "The paper proposes stage-specific strategies to accelerate diffusion model inference in video generation, reducing memory usage and maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at https://github.com/NKUShaw/LightCache .",
            "score": 2,
            "issue_id": 6299,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 октября",
                "en": "October 6",
                "zh": "10月6日"
            },
            "hash": "da5c54f2d1fce894",
            "authors": [
                "Yang Xiao",
                "Gen Li",
                "Kaiyuan Deng",
                "Yushu Wu",
                "Zheng Zhan",
                "Yanzhi Wang",
                "Xiaolong Ma",
                "Bo Hui"
            ],
            "affiliations": [
                "Clemson University",
                "Microsoft Research",
                "Northeastern University",
                "The University of Arizona",
                "University of Tulsa"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05367.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#open_source",
                    "#video",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Ускорение видеогенерации через управление памятью на разных стадиях",
                    "desc": "Исследователи предложили метод ускорения генерации видео с помощью диффузионных моделей без дополнительного обучения. Они разделили процесс инференса на три стадии (кодирование, шумоподавление и декодирование) и обнаружили проблему резкого роста потребления памяти. Для каждой стадии разработаны специфические стратегии: асинхронный обмен кэша, разбиение признаков на части и нарезка латентных представлений при декодировании. В результате достигнуто ускорение работы модели при снижении потребления памяти с сохранением приемлемого качества генерируемого видео."
                },
                "en": {
                    "title": "Accelerating Video Generation with Stage-Specific Strategies",
                    "desc": "This paper focuses on improving the efficiency of video generation using diffusion models by introducing stage-specific strategies. It identifies that the inference process can be broken down into three stages: encoding, denoising, and decoding, and highlights the memory issues that arise during the latter two stages. The authors propose methods such as Asynchronous Cache Swapping, Feature Chunking, and Slicing Latents to optimize memory usage without significantly increasing processing time. Overall, their approach results in faster inference speeds and reduced memory consumption while keeping quality loss minimal."
                },
                "zh": {
                    "title": "阶段特定策略加速视频生成推理",
                    "desc": "本文提出了针对扩散模型在视频生成中的推理加速的阶段特定策略，旨在减少内存使用并保持生成质量。我们将推理过程分解为编码、去噪和解码三个阶段，并发现基于缓存的加速方法在后两个阶段常常导致内存激增。为了解决这个问题，我们分析了不同阶段推理的特征，并提出了三种减少内存消耗的策略：异步缓存交换、特征块和切片潜变量解码。同时，我们确保这三种策略引入的时间开销低于加速带来的收益。与基线相比，我们的方法实现了更快的推理速度和更低的内存使用，同时保持了可接受范围内的质量下降。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05342",
            "title": "Margin Adaptive DPO: Leveraging Reward Model for Granular Control in\n  Preference Optimization",
            "url": "https://huggingface.co/papers/2510.05342",
            "abstract": "MADPO, a margin-adaptive method, enhances preference alignment in large language models by providing instance-level adaptive weighting to the DPO loss, improving performance across datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of beta-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative beta values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\\% on High Quality data and +10.5\\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.",
            "score": 2,
            "issue_id": 6298,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 октября",
                "en": "October 6",
                "zh": "10月6日"
            },
            "hash": "1a257d25fefce283",
            "authors": [
                "Hyung Gyu Rho"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2510.05342.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Адаптивные веса для каждого примера делают обучение по предпочтениям эффективнее",
                    "desc": "MADPO — это новый метод выравнивания больших языковых моделей по предпочтениям, который решает проблему DPO с фиксированной температурой. Метод сначала обучает reward model для оценки сложности каждой пары примеров, а затем применяет индивидуальные адаптивные веса к loss-функции DPO для каждого sample. Это позволяет модели больше учиться на сложных примерах и меньше переобучаться на простых, в отличие от предыдущих методов с батч-уровневой или uniformной регуляризацией. Эксперименты показывают улучшение до +33.3% на качественных данных по сравнению с baseline методами."
                },
                "en": {
                    "title": "Enhancing Preference Alignment with Adaptive Weighting",
                    "desc": "MADPO, or Margin-Adaptive Direct Preference Optimization, is a novel method designed to improve the alignment of large language models by adapting the weighting of the DPO loss at the instance level. This approach addresses the limitations of previous methods by providing a continuous and adaptive weight based on the estimated preference margins for each training sample. By amplifying the learning signal for difficult examples and reducing it for easier ones, MADPO enhances the model's ability to learn from diverse datasets effectively. Experimental results demonstrate that MADPO significantly outperforms existing methods, achieving notable performance improvements across various data quality levels."
                },
                "zh": {
                    "title": "边际自适应优化，提升模型偏好对齐",
                    "desc": "MADPO是一种边际自适应方法，通过为DPO损失提供实例级自适应加权，增强了大型语言模型的偏好对齐能力。该方法首先训练一个奖励模型来估计偏好边际，然后根据这些边际为每个训练样本应用连续的自适应权重。MADPO的重加权方案对困难样本增强信号，对简单样本减弱信号，从而实现了对学习信号的细致控制。实验结果表明，MADPO在情感生成任务中显著优于其他强基线，证明了其在偏好对齐方面的稳健性和有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04081",
            "title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model\n  Reasoning",
            "url": "https://huggingface.co/papers/2510.04081",
            "abstract": "Caco, a code-assisted chain-of-thought framework, automates the generation of high-quality, verifiable, and diverse reasoning data, enhancing the performance of large language models on mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose Caco (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, Caco first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created Caco-1.3M dataset demonstrate that Caco-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that Caco's code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention.",
            "score": 2,
            "issue_id": 6299,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 октября",
                "en": "October 5",
                "zh": "10月5日"
            },
            "hash": "4f2356e6d1057b79",
            "authors": [
                "Honglin Lin",
                "Qizhi Pei",
                "Xin Gao",
                "Zhuoshi Pan",
                "Yu Li",
                "Juntao Li",
                "Conghui He",
                "Lijun Wu"
            ],
            "affiliations": [
                "OpenDataLab, Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04081.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#math",
                    "#data",
                    "#training"
                ],
                "emoji": "🔢",
                "ru": {
                    "title": "Код как основа для автоматической генерации качественных рассуждений",
                    "desc": "Caco — это фреймворк для автоматической генерации высококачественных данных для обучения математическим рассуждениям в LLM с использованием кода. Система создает цепочки рассуждений (chain-of-thought) в виде исполняемого кода, автоматически проверяет их корректность через выполнение, а затем преобразует обратно в естественный язык. Благодаря этому подходу был создан датасет Caco-1.3M, который обеспечивает разнообразие и верифицируемость обучающих примеров без участия человека. Эксперименты показали, что модели, обученные на этих данных, превосходят существующие решения на математических бенчмарках и лучше генерализуются на новые задачи."
                },
                "en": {
                    "title": "Caco: Automating High-Quality Reasoning for LLMs",
                    "desc": "Caco is a new framework designed to improve the reasoning abilities of large language models (LLMs) in solving mathematical problems. It automates the creation of high-quality reasoning data by using code to generate diverse and verifiable reasoning paths. This approach addresses the limitations of traditional Chain-of-Thought (CoT) methods, which often struggle with quality and scalability. By incorporating automated validation and a closed-loop synthesis process, Caco ensures that the generated reasoning data is both executable and adaptable to various tasks, leading to better performance on mathematical reasoning benchmarks."
                },
                "zh": {
                    "title": "Caco：自动化高质量推理数据生成的创新框架",
                    "desc": "Caco是一个代码辅助的思维链框架，旨在自动生成高质量、可验证和多样化的推理数据，从而提升大型语言模型在数学推理任务上的表现。该框架通过代码驱动的增强方法，解决了现有思维链方法在生成控制、质量不足和推理路径有限等问题。Caco首先在统一的代码格式上微调代码基础的思维链生成器，然后扩展数据生成以获得大量多样化的推理轨迹。通过代码执行和基于规则的过滤，Caco确保了逻辑正确性和结构多样性，从而实现了完全自动化和可扩展的推理数据合成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06218",
            "title": "EgoNight: Towards Egocentric Vision Understanding at Night with a\n  Challenging Benchmark",
            "url": "https://huggingface.co/papers/2510.06218",
            "abstract": "EgoNight is a comprehensive benchmark for nighttime egocentric vision, focusing on visual question answering and revealing performance gaps between day and night conditions for multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance.",
            "score": 1,
            "issue_id": 6298,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 октября",
                "en": "October 7",
                "zh": "10月7日"
            },
            "hash": "a863fc0993d7f2f6",
            "authors": [
                "Deheng Zhang",
                "Yuqian Fu",
                "Runyi Yang",
                "Yang Miao",
                "Tianwen Qian",
                "Xu Zheng",
                "Guolei Sun",
                "Ajad Chhatkuli",
                "Xuanjing Huang",
                "Yu-Gang Jiang",
                "Luc Van Gool",
                "Danda Pani Paudel"
            ],
            "affiliations": [
                "East China Normal University",
                "Fudan University",
                "HKUST(GZ)",
                "INSAIT, Sofia University St. Kliment Ohridski",
                "Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06218.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#long_context",
                    "#benchmark",
                    "#games",
                    "#transfer_learning",
                    "#synthetic",
                    "#cv"
                ],
                "emoji": "🌙",
                "ru": {
                    "title": "Проверка AI-зрения в темноте от первого лица",
                    "desc": "EgoNight — это первый комплексный бенчмарк для оценки egocentric-видения в ночных условиях с фокусом на visual question answering. Датасет включает 3658 пар вопрос-ответ на 90 видео, записанных как в дневное, так и в ночное время с выровненными сценами и действиями. Исследование показало значительное падение производительности современных multimodal LLM при переходе от дневных к ночным условиям. Помимо VQA, бенчмарк включает дополнительные задачи: поиск соответствий день-ночь и оценку глубины в ночных условиях для более полного тестирования моделей."
                },
                "en": {
                    "title": "Bridging the Gap: Nighttime Vision for AI",
                    "desc": "EgoNight is a new benchmark designed to improve nighttime egocentric vision, particularly in visual question answering (VQA). It highlights the performance differences of multimodal large language models (MLLMs) when operating in low-light conditions compared to daytime scenarios. The benchmark includes day-night aligned videos to enhance the quality of night annotations and reveals significant performance drops in models when transitioning from day to night. Additionally, EgoNight introduces tasks like day-night correspondence retrieval and egocentric depth estimation to further challenge and advance current models in this field."
                },
                "zh": {
                    "title": "夜间视觉问答的新基准：EgoNight",
                    "desc": "EgoNight是一个全面的基准测试，专注于夜间自我中心视觉，特别是视觉问答（VQA）任务。现有的自我中心视觉基准主要集中在白天场景，忽视了低光照条件下的应用需求。EgoNight通过引入日夜对齐的视频，提升了夜间标注的质量，并揭示了不同光照条件下的性能差距。该基准包含3658个问答对，支持多种任务，旨在推动自我中心视觉研究的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06139",
            "title": "Deforming Videos to Masks: Flow Matching for Referring Video\n  Segmentation",
            "url": "https://huggingface.co/papers/2510.06139",
            "abstract": "FlowRVS addresses the challenges of Referring Video Object Segmentation by reformulating the task as a continuous flow problem, leveraging pretrained T2V models and achieving state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a J&F of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.",
            "score": 1,
            "issue_id": 6298,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 октября",
                "en": "October 7",
                "zh": "10月7日"
            },
            "hash": "27bd729b0bb095f1",
            "authors": [
                "Zanyi Wang",
                "Dengyang Jiang",
                "Liuzhuozheng Li",
                "Sizhe Dang",
                "Chengzu Li",
                "Harry Yang",
                "Guang Dai",
                "Mengmeng Wang",
                "Jingdong Wang"
            ],
            "affiliations": [
                "Baidu",
                "SGIT AI Lab, State Grid Corporation of China",
                "The Hong Kong University of Science and Technology",
                "The University of Tokyo",
                "University of California, San Diego",
                "University of Cambridge",
                "Zhejiang University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06139.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#games",
                    "#alignment"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Сегментация видео через непрерывную деформацию под управлением текста",
                    "desc": "FlowRVS решает задачу сегментации объектов в видео по текстовому описанию (RVOS), переформулируя её как проблему непрерывного потока. Вместо традиционного подхода «найти-затем-сегментировать», метод использует pretrained text-to-video модели для прямой деформации видео-представления в целевую маску под управлением языкового описания. Это позволяет обеспечить точный контроль на уровне пикселей, семантическое выравнивание текста и видео, а также временную согласованность. Подход достигает state-of-the-art результатов на всех основных бенчмарках RVOS, включая MeViS и Ref-DAVIS17."
                },
                "en": {
                    "title": "Revolutionizing Video Segmentation with Continuous Flow",
                    "desc": "FlowRVS introduces a new way to tackle Referring Video Object Segmentation (RVOS) by treating it as a continuous flow problem. This approach allows the model to better connect language descriptions to specific video pixels, improving the segmentation process. Unlike previous methods that separate locating and segmenting tasks, FlowRVS maintains a unified framework that enhances temporal consistency and semantic understanding. By leveraging pretrained T2V models, it achieves state-of-the-art performance on major RVOS benchmarks, showcasing the effectiveness of continuous deformation in video understanding."
                },
                "zh": {
                    "title": "FlowRVS：视频物体分割的新思路",
                    "desc": "FlowRVS提出了一种新方法来解决视频物体分割中的引用问题，将其重新定义为一个连续流动问题。该方法利用预训练的文本到视频模型，实现了对视频中目标的精细像素控制和语义对齐。与传统的“定位-再分割”流程不同，FlowRVS通过直接学习语言引导的变形，从视频的整体表示到目标掩膜，保持了时间一致性。该框架在主要的引用视频物体分割基准测试中取得了新的最先进结果，展示了将视频理解任务建模为连续变形过程的巨大潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06131",
            "title": "Discrete Diffusion Models with MLLMs for Unified Medical Multimodal\n  Generation",
            "url": "https://huggingface.co/papers/2510.06131",
            "abstract": "MeDiM, a medical discrete diffusion model, integrates multimodal biomedical data by learning shared distributions across images, text, and clinical notes, achieving high-fidelity generation and enhanced downstream performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.",
            "score": 1,
            "issue_id": 6299,
            "pub_date": "2025-10-07",
            "pub_date_card": {
                "ru": "7 октября",
                "en": "October 7",
                "zh": "10月7日"
            },
            "hash": "0641a3ba669f441d",
            "authors": [
                "Jiawei Mao",
                "Yuhan Wang",
                "Lifeng Chen",
                "Can Zhao",
                "Yucheng Tang",
                "Dong Yang",
                "Liangqiong Qu",
                "Daguang Xu",
                "Yuyin Zhou"
            ],
            "affiliations": [
                "NVIDIA",
                "UC Santa Cruz",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06131.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#science",
                    "#healthcare",
                    "#multimodal"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Единая диффузионная модель для всех медицинских модальностей",
                    "desc": "MeDiM — это первая медицинская модель дискретной диффузии, которая объединяет разные типы биомедицинских данных (изображения, текст и клинические записи) в едином вероятностном пространстве без модально-специфичных компонентов. В основе модели лежит мультимодальная LLM с модифицированной архитектурой: убрана каузальная маска внимания для двунаправленного контекста и добавлены непрерывные timestep embeddings для диффузионного процесса. Модель способна переводить между изображениями и текстом, а также генерировать согласованные пары медицинских изображений и отчётов по текстовым запросам. Эксперименты показывают высокое качество генерации и улучшение результатов на downstream-задачах при использовании совместно сгенерированных пар изображение-отчёт."
                },
                "en": {
                    "title": "Unifying Biomedical Data with MeDiM: A Multimodal Diffusion Revolution",
                    "desc": "MeDiM is a novel medical discrete diffusion model designed to integrate various types of biomedical data, such as images, text, and clinical notes. It overcomes the limitations of traditional models that operate within specific modalities by learning shared distributions across all data types. By utilizing a multimodal large language model as its backbone, MeDiM can generate high-quality medical outputs and translate between different modalities effectively. The model's innovative design features, like bidirectional context and continuous timestep embeddings, contribute to its superior performance in generating coherent and clinically relevant multimodal outputs."
                },
                "zh": {
                    "title": "MeDiM：医学多模态生成的创新桥梁",
                    "desc": "MeDiM是一种医学离散扩散模型，能够通过学习图像、文本和临床记录之间的共享分布来整合多模态生物医学数据。该模型克服了传统生成医学模型在特定模态场景下的局限性，实现了高保真度的生成和增强的下游性能。MeDiM统一了多种生成任务，包括图像与文本之间的翻译，以及根据提示共同生成跨领域的图像-报告对。通过使用多模态大型语言模型作为扩散骨干，MeDiM在一个共享的概率空间中桥接了视觉和语言表示。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04087",
            "title": "A Contextual Quality Reward Model for Reliable and Efficient Best-of-N\n  Sampling",
            "url": "https://huggingface.co/papers/2510.04087",
            "abstract": "A new framework using an outside option in preference data collection and modeling improves reliability and efficiency in preference alignment techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is better, but what is good enough. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70\\%, and when tuned as an inference accelerator, it improves average inference speed by over 22\\% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.",
            "score": 1,
            "issue_id": 6298,
            "pub_date": "2025-10-05",
            "pub_date_card": {
                "ru": "5 октября",
                "en": "October 5",
                "zh": "10月5日"
            },
            "hash": "77a7f937000c3b18",
            "authors": [
                "Hyung Gyu Rho"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2510.04087.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#data",
                    "#rlhf",
                    "#alignment"
                ],
                "emoji": "🚦",
                "ru": {
                    "title": "Научить AI понимать, что хорошо, а не только что лучше",
                    "desc": "Исследователи предлагают новый подход к обучению моделей предпочтений, добавляя «внешнюю опцию» в данные сравнений. Традиционные reward models умеют определять, какой ответ лучше, но не могут понять, является ли ответ вообще приемлемым. Новый метод позволяет модели различать не только относительное качество, но и абсолютную приемлемость ответов, что особенно важно для сложных запросов. Адаптивная стратегия генерации с ранним выходом снижает количество неприемлемых ответов на 70% и ускоряет inference на 22%."
                },
                "en": {
                    "title": "Enhancing Preference Alignment with Outside Options",
                    "desc": "This paper presents a new framework for improving preference alignment techniques in machine learning by incorporating an outside option in data collection. Traditional methods, like Best-of-N sampling, often fail to identify acceptable responses, leading to poor choices among suboptimal options. The proposed framework enhances reward models by allowing them to recognize not only better options but also those that are sufficiently good. Experimental results demonstrate significant improvements in reliability and efficiency, reducing failures by 70% and increasing inference speed by over 22%."
                },
                "zh": {
                    "title": "提升偏好对齐的可靠性与效率",
                    "desc": "本文提出了一种新的框架，通过在偏好数据收集和建模中引入外部选项，提升了偏好对齐技术的可靠性和效率。现代的偏好对齐技术，如最佳N（BoN）采样，依赖于通过成对比较数据训练的奖励模型，虽然能有效学习相对偏好，但未能捕捉响应可接受性的信号。我们通过引入外部选项，训练出能够区分不仅是更好而是足够好的奖励模型，从而解决了可靠性缺口。实验结果表明，该框架在对齐和推理加速方面均显著提高了性能，提供了一个灵活的管理可靠性与计算效率之间权衡的工具。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.05137",
            "title": "Demystifying deep search: a holistic evaluation with hint-free multi-hop\n  questions and factorised metrics",
            "url": "https://huggingface.co/papers/2510.05137",
            "abstract": "WebDetective is a benchmark for evaluating multi-hop reasoning in RAG systems and web agents, addressing issues of reasoning path leakage and single-pass evaluation, and introducing a framework to improve knowledge utilization and refusal behavior.  \t\t\t\t\tAI-generated summary \t\t\t\t RAG (Retrieval-Augmented Generation) systems and web agents are increasingly evaluated on multi-hop deep search tasks, yet current practice suffers from two major limitations. First, most benchmarks leak the reasoning path in the question text, allowing models to follow surface cues rather than discover reasoning chains autonomously. Second, evaluation is typically reduced to a single pass rate, which collapses diverse behaviours into one score and obscures whether failures stem from inadequate search, poor knowledge use, or inappropriate refusal. To address these issues, we present WebDetective, a benchmark of hint-free multi-hop questions paired with a controlled Wikipedia sandbox that ensures full traceability of model actions, and a holistic evaluation framework that separates search sufficiency, knowledge utilisation, and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals systematic weaknesses across all architectures: models struggle with knowledge utilisation despite having sufficient evidence and demonstrate near-absent appropriate refusal when evidence is lacking. These patterns expose a fundamental gap: today's systems excel at executing given reasoning paths but fail when required to discover them. We develop an agentic workflow, EvidenceLoop, that explicitly targets the challenges our benchmark identifies, incorporating verification loops and systematic evidence tracking that improve both search and synthesis capabilities. This baseline demonstrates that WebDetective's diagnostic framework can guide concrete architectural improvements, establishing our benchmark as a critical tool for developing genuinely autonomous reasoning systems rather than pattern-following agents.",
            "score": 1,
            "issue_id": 6298,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "d780a77899923141",
            "authors": [
                "Maojia Song",
                "Renhang Liu",
                "Xinyu Wang",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Soujanya Poria",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Nanyang Technological University (NTU)",
                "Singapore University of Technology and Design (SUTD)",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.05137.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#rag",
                    "#benchmark",
                    "#leakage",
                    "#architecture",
                    "#agents"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "WebDetective: Как научить AI-агентов думать самостоятельно, а не следовать подсказкам",
                    "desc": "WebDetective — это новый бенчмарк для оценки многошаговых рассуждений в RAG-системах и веб-агентах. Существующие тесты содержат «утечку» пути рассуждений прямо в вопросе, позволяя моделям просто следовать поверхностным подсказкам вместо самостоятельного построения цепочки мыслей. Авторы создали контролируемую среду на основе Wikipedia и детальную систему оценки, которая раздельно измеряет качество поиска информации, использование знаний и способность отказаться от ответа при недостатке данных. Тестирование 25 современных моделей выявило критическую проблему: системы хорошо выполняют заданные пути рассуждений, но проваливаются, когда нужно самостоятельно их обнаружить."
                },
                "en": {
                    "title": "WebDetective: Enhancing Multi-Hop Reasoning in AI Systems",
                    "desc": "WebDetective is a new benchmark designed to evaluate how well RAG systems and web agents can perform multi-hop reasoning without leaking reasoning paths in the questions. It addresses the limitations of current evaluation methods that oversimplify model performance into a single score, which can hide specific weaknesses in knowledge utilization and refusal behavior. The benchmark includes hint-free questions and a controlled environment to track model actions, allowing for a more detailed analysis of how models search for information and use knowledge. The findings reveal that many models struggle with effectively utilizing available evidence and often fail to refuse when they lack sufficient information, highlighting the need for improvements in autonomous reasoning capabilities."
                },
                "zh": {
                    "title": "WebDetective：提升多跳推理的评估标准",
                    "desc": "WebDetective是一个用于评估RAG系统和网络代理的多跳推理基准，旨在解决推理路径泄漏和单次评估的问题。该基准提供无提示的多跳问题，并配备一个受控的维基百科沙箱，以确保模型行为的可追溯性。通过对25个最先进模型的评估，我们发现这些模型在知识利用方面存在系统性弱点，尽管有足够的证据，但在缺乏证据时几乎没有适当的拒绝行为。我们开发了EvidenceLoop工作流程，专门针对基准识别的挑战，改进了搜索和综合能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.02341",
            "title": "DRIFT: Learning from Abundant User Dissatisfaction in Real-World\n  Preference Learning",
            "url": "https://huggingface.co/papers/2510.02341",
            "abstract": "DRIFT, a dissatisfaction-refined iterative preference training method, improves large language models using implicit user dissatisfaction signals, achieving better performance than existing methods on real-world datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce DRIFT (Dissatisfaction-Refined Iterative preFerence Training), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world WildFeedback datasets and synthetic UltraFeedback datasets achieve up to +6.23\\% (7B) / +7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.",
            "score": 1,
            "issue_id": 6298,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "1bde94710320cd61",
            "authors": [
                "Yifan Wang",
                "Bolian Li",
                "Junlin Wu",
                "Zhaoxuan Tan",
                "Zheli Liu",
                "Ruqi Zhang",
                "Ananth Grama",
                "Qingkai Zeng"
            ],
            "affiliations": [
                "College of Computer Science, Nankai University",
                "Department of Computer Science and Engineering, University of Notre Dame",
                "Department of Computer Science, Purdue University",
                "Department of Computer Science, Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.02341.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#alignment",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Обучение LLM на недовольстве пользователей: превращаем негатив в качество",
                    "desc": "В статье представлен метод DRIFT для обучения больших языковых моделей на основе сигналов недовольства пользователей. В реальных системах пользователи часто итеративно улучшают ответы через уточнения и исправления, создавая неявные сигналы неудовлетворённости, в то время как явная положительная обратная связь встречается редко. DRIFT использует эти сигналы недовольства как якорь для обучения и динамически генерирует положительные примеры из развивающейся политики модели. На датасетах WildFeedback и UltraFeedback метод показывает улучшение до +12.29% по win rate на AlpacaEval2, превосходя базовые методы как DPO и SPIN, при этом сохраняя разнообразие генерируемых решений."
                },
                "en": {
                    "title": "Harnessing User Dissatisfaction for Better Language Models",
                    "desc": "The paper introduces DRIFT, a novel training method for large language models that utilizes implicit user dissatisfaction signals to enhance performance. Unlike traditional methods that depend on explicit positive feedback, DRIFT focuses on refining preferences based on real-world user interactions, which often include dissatisfaction. This approach allows for dynamic sampling of positive responses, leading to improved model performance on various tasks. Empirical results demonstrate that DRIFT significantly outperforms existing methods, particularly in larger models, while also maintaining diversity in generated outputs."
                },
                "zh": {
                    "title": "利用用户不满信号提升语言模型性能",
                    "desc": "DRIFT是一种基于用户不满信号的迭代偏好训练方法，旨在提升大型语言模型的性能。该方法利用真实世界中的用户不满信号，动态采样正反馈，从而更好地适应用户的需求。实验结果表明，使用DRIFT训练的模型在多个任务上显著超越了传统方法，尤其是在大规模模型上表现尤为突出。DRIFT不仅提高了模型的性能，还保持了探索能力，能够生成更多样化的高奖励解决方案。"
                }
            }
        }
    ],
    "link_prev": "2025-10-07.html",
    "link_next": "2025-10-09.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "07.10",
        "en": "10/07",
        "zh": "10月7日"
    },
    "short_date_next": {
        "ru": "09.10",
        "en": "10/09",
        "zh": "10月9日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 7,
        "#agents": 2,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 4,
        "#rag": 1,
        "#plp": 0,
        "#inference": 4,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    }
}