{
    "date": {
        "ru": "11 июня",
        "en": "June 11",
        "zh": "6月11日"
    },
    "time_utc": "2025-06-11 10:13",
    "weekday": 2,
    "issue_id": 4239,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.06751",
            "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
            "url": "https://huggingface.co/papers/2506.06751",
            "abstract": "LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper evaluates geopolitical biases in LLMs with respect to various countries though an analysis of their interpretation of historical events with conflicting national perspectives (USA, UK, USSR, and China). We introduce a novel dataset with neutral event descriptions and contrasting viewpoints from different countries. Our findings show significant geopolitical biases, with models favoring specific national narratives. Additionally, simple debiasing prompts had a limited effect in reducing these biases. Experiments with manipulated participant labels reveal models' sensitivity to attribution, sometimes amplifying biases or recognizing inconsistencies, especially with swapped labels. This work highlights national narrative biases in LLMs, challenges the effectiveness of simple debiasing methods, and offers a framework and dataset for future geopolitical bias research.",
            "score": 33,
            "issue_id": 4234,
            "pub_date": "2025-06-07",
            "pub_date_card": {
                "ru": "7 июня",
                "en": "June 7",
                "zh": "6月7日"
            },
            "hash": "87a1fbaf018382d4",
            "authors": [
                "Mikhail Salnikov",
                "Dmitrii Korzh",
                "Ivan Lazichny",
                "Elvir Karimov",
                "Artyom Iudin",
                "Ivan Oseledets",
                "Oleg Y. Rogov",
                "Alexander Panchenko",
                "Natalia Loukachevitch",
                "Elena Tutubalina"
            ],
            "affiliations": [
                "AIRI",
                "Kazan Federal University",
                "Lomonosov MSU",
                "MIPT",
                "MTUCI",
                "Sber AI",
                "Skoltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06751.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#alignment",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "Геополитические предубеждения в LLM: вызов объективности искусственного интеллекта",
                    "desc": "Исследование оценивает геополитические предубеждения в больших языковых моделях (LLM) при интерпретации исторических событий с конфликтующими национальными перспективами. Авторы представляют новый набор данных с нейтральными описаниями событий и противоречивыми точками зрения разных стран. Результаты показывают значительные геополитические предубеждения, причем модели отдают предпочтение определенным национальным нарративам. Простые методы дебиасинга оказались малоэффективными в снижении этих предубеждений."
                },
                "en": {
                    "title": "Uncovering Geopolitical Biases in Language Models",
                    "desc": "This paper investigates the presence of geopolitical biases in large language models (LLMs) by analyzing how they interpret historical events from different national perspectives, specifically focusing on the USA, UK, USSR, and China. It introduces a new dataset that contains neutral descriptions of events alongside varying viewpoints from these countries to facilitate further research. The results indicate that LLMs tend to favor certain national narratives, demonstrating significant biases in their outputs. Additionally, the study finds that basic debiasing techniques are not very effective in mitigating these biases, suggesting a need for more robust methods in future research."
                },
                "zh": {
                    "title": "揭示大型语言模型的地缘政治偏见",
                    "desc": "本论文评估了大型语言模型（LLMs）在解释历史事件时的地缘政治偏见，特别是针对美国、英国、苏联和中国等国家的不同视角。我们引入了一个新数据集，包含中立的事件描述和来自不同国家的对立观点。研究结果显示，模型倾向于支持特定国家的叙事，且简单的去偏见方法效果有限。通过操控参与者标签的实验，我们发现模型对归因非常敏感，有时会放大偏见或识别不一致，尤其是在标签交换的情况下。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08672",
            "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic\n  Sampling",
            "url": "https://huggingface.co/papers/2506.08672",
            "abstract": "RuleReasoner enhances rule-based reasoning in small models through dynamic domain sampling, achieving superior performance and efficiency compared to large models.  \t\t\t\t\tAI-generated summary \t\t\t\t Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin (Delta4.1% average points on eight ID tasks and Delta10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL.",
            "score": 14,
            "issue_id": 4239,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "f9f7805577b3b091",
            "authors": [
                "Yang Liu",
                "Jiaqi Li",
                "Zilong Zheng"
            ],
            "affiliations": [
                "NLCo Lab, Beijing Institute for General Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08672.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#small_models",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное рассуждение на основе правил для малых моделей",
                    "desc": "RuleReasoner - это новый метод для улучшения рассуждений на основе правил в небольших моделях машинного обучения. Он использует динамическую выборку доменов и подкрепляющее обучение для повышения производительности. RuleReasoner превосходит крупные модели рассуждений по точности на задачах в распределении и вне распределения. Метод также демонстрирует более высокую вычислительную эффективность по сравнению с существующими подходами."
                },
                "en": {
                    "title": "Boosting Small Models with Dynamic Domain Sampling",
                    "desc": "RuleReasoner is a method that improves rule-based reasoning in small models by using dynamic domain sampling. It addresses the challenges posed by varying rule formats and complexities in real-world applications. By updating sampling weights based on historical rewards, RuleReasoner enhances the learning process and generalization across different tasks. Empirical results show that it significantly outperforms large reasoning models while being more computationally efficient."
                },
                "zh": {
                    "title": "小模型的规则推理新突破",
                    "desc": "RuleReasoner是一种增强小型模型规则推理的方法，通过动态领域采样实现了优越的性能和效率。该方法利用强化学习（RL）来优化规则推理，解决了现实应用中规则格式和复杂性带来的挑战。通过更新不同领域的采样权重，RuleReasoner能够灵活地进行在线学习，避免了传统方法中需要人工设计的混合训练方案。实验证明，RuleReasoner在多个任务上显著超越了大型推理模型，且计算效率更高。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09040",
            "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better",
            "url": "https://huggingface.co/papers/2506.09040",
            "abstract": "Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.",
            "score": 12,
            "issue_id": 4237,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "09d042607d92f156",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#optimization",
                    "#interpretability",
                    "#benchmark",
                    "#games",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Семантическая реконструкция изображений для улучшения мультимодального понимания",
                    "desc": "Авторегрессивная семантическая визуальная реконструкция (ASVR) улучшает мультимодальное понимание, фокусируясь на семантической реконструкции, а не на восстановлении исходного визуального образа. Этот подход позволяет эффективно использовать изображения без сопроводительных подписей и учитывать важные визуальные детали, которые могут быть упущены в текстовых описаниях. ASVR объединяет обучение визуальной и текстовой модальностей в единой авторегрессивной структуре, что приводит к значительному улучшению производительности на различных мультимодальных тестах. Метод показывает стабильные улучшения при различных масштабах данных и типах базовых языковых моделей."
                },
                "en": {
                    "title": "Revolutionizing Multimodal Understanding with Semantic Focus",
                    "desc": "The paper introduces Autoregressive Semantic Visual Reconstruction (ASVR), a method that enhances multimodal understanding by focusing on reconstructing the semantic content of images rather than their raw visual appearance. This approach addresses limitations in existing large vision-language models (LVLMs) that primarily rely on textual sequences, which can lead to missing critical visual details. By enabling joint learning of visual and textual modalities, ASVR allows models to effectively reconstruct discrete semantic tokens from continuous image features, leading to improved comprehension. The results show significant performance gains across various benchmarks, demonstrating the effectiveness of semantic reconstruction in multimodal tasks."
                },
                "zh": {
                    "title": "自回归语义重建，提升多模态理解！",
                    "desc": "自回归语义视觉重建（ASVR）通过关注语义重建而非原始视觉外观，提升了多模态理解的能力。传统的大型视觉语言模型（LVLM）仅对文本序列应用自回归监督，未能充分整合视觉模态，导致无法利用没有配套说明的图像。ASVR 通过在统一的自回归框架内实现视觉和文本模态的联合学习，克服了这一限制。我们的研究表明，重建图像的语义表示能够显著提高多模态理解的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08009",
            "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
            "url": "https://huggingface.co/papers/2506.08009",
            "abstract": "Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/",
            "score": 9,
            "issue_id": 4235,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 июня",
                "en": "June 9",
                "zh": "6月9日"
            },
            "hash": "e63130d065bba1fd",
            "authors": [
                "Xun Huang",
                "Zhengqi Li",
                "Guande He",
                "Mingyuan Zhou",
                "Eli Shechtman"
            ],
            "affiliations": [
                "Adobe Research",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08009.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#video",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Self Forcing: реалистичная генерация видео в реальном времени",
                    "desc": "Представлен новый метод обучения авторегрессионных видео-диффузионных моделей под названием Self Forcing. Он решает проблему смещения экспозиции, обучая модель на собственных сгенерированных выходных данных вместо истинных. Self Forcing использует кэширование ключ-значение и целостную функцию потерь на уровне всего видео. Метод позволяет генерировать видео в реальном времени с низкой задержкой на одном GPU, сохраняя высокое качество."
                },
                "en": {
                    "title": "Self Forcing: Enhancing Video Generation with Autoregressive Training",
                    "desc": "The paper presents Self Forcing, a new training method for autoregressive video diffusion models that aims to reduce exposure bias and enhance video generation quality. It allows models to generate video frames based on their own previously generated outputs instead of relying solely on ground-truth frames, which helps in better training. By using a holistic video-level supervision approach, the method evaluates the quality of the entire video sequence rather than just individual frames. Additionally, it incorporates efficient caching mechanisms and a few-step diffusion model to optimize performance and ensure real-time video generation on a single GPU."
                },
                "zh": {
                    "title": "自我强制：提升视频生成质量的新方法",
                    "desc": "Self Forcing是一种新颖的自回归视频扩散模型训练方法，旨在减少曝光偏差并提高生成质量。该方法通过整体视频级监督和高效缓存机制，解决了模型在推理时必须依赖自身不完美输出生成序列的问题。与以往方法不同，Self Forcing在训练过程中使用自生成输出进行条件生成，从而实现视频级的整体损失评估。实验表明，该方法能够在单个GPU上实现实时视频生成，且生成质量优于传统的非因果扩散模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07927",
            "title": "Solving Inequality Proofs with Large Language Models",
            "url": "https://huggingface.co/papers/2506.07927",
            "abstract": "The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.",
            "score": 9,
            "issue_id": 4235,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 июня",
                "en": "June 9",
                "zh": "6月9日"
            },
            "hash": "0171dcd88ad3a14f",
            "authors": [
                "Jiayi Sheng",
                "Luna Lyu",
                "Jikai Jin",
                "Tony Xia",
                "Alex Gu",
                "James Zou",
                "Pan Lu"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Stanford University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07927.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "LLM могут найти ответ, но не могут доказать",
                    "desc": "Исследование способности больших языковых моделей (LLM) доказывать неравенства выявило значительные трудности в построении строгих доказательств. Эксперты создали набор данных IneqMath с олимпиадными задачами и разработали новую систему оценки с использованием LLM в качестве судьи. Результаты показали, что даже лучшие модели достигают менее 10% точности при пошаговой проверке, что значительно ниже точности при учете только конечного ответа. Это исследование выявило существенный разрыв между способностью LLM находить ответ и строить корректное доказательство."
                },
                "en": {
                    "title": "Bridging the Gap: From Answers to Rigorous Proofs in Inequality Proving",
                    "desc": "This paper explores the challenges faced by large language models (LLMs) in the domain of inequality proving, which is essential for advanced reasoning in mathematics and science. It identifies a significant gap between generating answers and producing valid, step-by-step proofs, highlighting the limitations of current datasets that are often inadequate. The authors propose a new task formulation that breaks down inequality proving into two checkable subtasks: bound estimation and relation prediction, and introduce the IneqMath dataset for training and evaluation. Their evaluation of 29 leading LLMs reveals that even the best models struggle with rigorous proof construction, achieving less than 10% accuracy when assessed on step-wise reasoning, indicating a need for improved methodologies in theorem-guided reasoning and self-refinement."
                },
                "zh": {
                    "title": "揭示不等式证明中的推理挑战",
                    "desc": "这篇论文探讨了使用大型语言模型进行不等式证明的挑战，揭示了找到答案与生成有效逐步解决方案之间的差距。不等式证明在科学和数学领域至关重要，考验着高级推理能力，如发现紧界和战略性定理应用。为了应对现有数据集稀缺和形式化的问题，作者提出了一种非正式但可验证的任务形式，将不等式证明重构为两个可自动检查的子任务：界限估计和关系预测。此外，研究还发布了IneqMath数据集，并开发了一种新的评估框架，以检测常见的推理缺陷。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04614",
            "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation",
            "url": "https://huggingface.co/papers/2506.04614",
            "abstract": "A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency.",
            "score": 9,
            "issue_id": 4236,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "9eb1f2457722a2cd",
            "authors": [
                "Yuyang Wanyan",
                "Xi Zhang",
                "Haiyang Xu",
                "Haowei Liu",
                "Junyang Wang",
                "Jiabo Ye",
                "Yutong Kou",
                "Ming Yan",
                "Fei Huang",
                "Xiaoshan Yang",
                "Weiming Dong",
                "Changsheng Xu"
            ],
            "affiliations": [
                "Alibaba Group",
                "Beijing Jiaotong University",
                "MAIS, Institute of Automation, Chinese Academy of Sciences, China",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04614.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#multimodal",
                    "#optimization",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Повышение надежности автоматизации GUI с помощью предоперационной критики",
                    "desc": "Статья представляет новый механизм предоперационной критики для повышения надежности задач мультимодального рассуждения в автоматизации графического интерфейса пользователя (GUI). Авторы предлагают стратегию оптимизации политики с учетом градиента и предложений (S-GRPO) для создания модели GUI-Critic-R1. Исследование включает разработку процесса сбора данных на основе бутстрэппинга рассуждений для создания наборов данных GUI-Critic-Train и GUI-Critic-Test. Эксперименты показывают, что GUI-Critic-R1 превосходит существующие мультимодальные большие языковые модели (MLLM) в точности критики и эффективности автоматизации GUI."
                },
                "en": {
                    "title": "Enhancing GUI Automation Reliability with Pre-operative Critique",
                    "desc": "This paper presents a new method to improve the reliability of GUI automation using a pre-operative critic mechanism. The mechanism provides feedback before actions are executed, helping to prevent errors that could lead to serious issues like unwanted deletions. The authors introduce a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to enhance the feedback process, making it more effective. Experiments show that their model, GUI-Critic-R1, significantly outperforms existing models in both accuracy and operational efficiency during GUI automation tasks."
                },
                "zh": {
                    "title": "提升GUI自动化可靠性的预操作评估机制",
                    "desc": "本文提出了一种预操作评估机制，旨在提高图形用户界面（GUI）自动化中的多模态推理任务的可靠性。我们引入了一种名为建议感知梯度相对策略优化（S-GRPO）的策略，以构建预操作评估模型GUI-Critic-R1，并通过引入新颖的建议奖励来增强模型反馈的可靠性。该机制在实际执行之前提供有效反馈，帮助推理潜在结果和行动的正确性，从而减少决策错误的风险。通过在移动和网页领域的静态实验，我们的模型在评估准确性上显著优于现有的多模态大语言模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08002",
            "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
            "url": "https://huggingface.co/papers/2506.08002",
            "abstract": "A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/",
            "score": 8,
            "issue_id": 4232,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 июня",
                "en": "June 9",
                "zh": "6月9日"
            },
            "hash": "ab2da61a8a27783d",
            "authors": [
                "Aadarsh Sahoo",
                "Vansh Tibrewal",
                "Georgia Gkioxari"
            ],
            "affiliations": [
                "California Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08002.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#synthetic",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Единая мультимодальная модель для понимания 3D-мира",
                    "desc": "Предложена унифицированная модель для языка, изображений и 3D-сцен, достигающая оптимальной производительности в различных 3D-задачах. Модель основана на авторегрессионных методах и использует выравнивание модальностей. Авторы предоставляют детальное руководство по ключевым аспектам обучения, включая представление данных и специфические для модальностей целевые функции. Модель оценивается на четырех основных 3D-задачах и четырех наборах данных, демонстрируя эффективность в реконструкции сложных 3D-форм объектов."
                },
                "en": {
                    "title": "Unifying Language, Image, and 3D Understanding for Enhanced Machine Interaction",
                    "desc": "This paper presents a unified framework that integrates language, images, and 3D scenes to enhance machine understanding of three-dimensional environments. By leveraging autoregressive models, the authors explore how to effectively represent and process structured 3D scenes alongside traditional modalities. The framework includes a comprehensive guide for optimal training and performance, addressing critical aspects like data representation and modality-specific objectives. The model is evaluated on various 3D tasks, demonstrating its capability in rendering, recognition, instruction-following, and question-answering across both synthetic and real-world datasets."
                },
                "zh": {
                    "title": "统一三维场景模型，提升AI理解能力",
                    "desc": "本文提出了一种统一的语言、图像和三维场景模型框架，旨在实现各种三维任务和数据集的最佳训练和性能。该框架利用自回归模型，探索了结构化三维场景的新模式，帮助设计师构建和编辑三维环境。我们提供了一份详细的“食谱”，阐述了实现最佳训练和性能的关键设计选择，并评估了在四个核心三维任务和数据集上的表现。通过丰富三维模态的量化形状编码，我们的模型在复杂三维物体识别任务中展现了有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07177",
            "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models",
            "url": "https://huggingface.co/papers/2506.07177",
            "abstract": "Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancements in diffusion models have significantly improved video quality, directing attention to fine-grained controllability. However, many existing methods depend on fine-tuning large-scale video models for specific tasks, which becomes increasingly impractical as model sizes continue to grow. In this work, we present Frame Guidance, a training-free guidance for controllable video generation based on frame-level signals, such as keyframes, style reference images, sketches, or depth maps. For practical training-free guidance, we propose a simple latent processing method that dramatically reduces memory usage, and apply a novel latent optimization strategy designed for globally coherent video generation. Frame Guidance enables effective control across diverse tasks, including keyframe guidance, stylization, and looping, without any training, compatible with any video models. Experimental results show that Frame Guidance can produce high-quality controlled videos for a wide range of tasks and input signals.",
            "score": 8,
            "issue_id": 4233,
            "pub_date": "2025-06-08",
            "pub_date_card": {
                "ru": "8 июня",
                "en": "June 8",
                "zh": "6月8日"
            },
            "hash": "7d83fcff01c3595a",
            "authors": [
                "Sangwon Jang",
                "Taekyung Ki",
                "Jaehyeong Jo",
                "Jaehong Yoon",
                "Soo Ye Kim",
                "Zhe Lin",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "Adobe Research",
                "DeepAuto.ai",
                "KAIST",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07177.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Управление генерацией видео без переобучения модели",
                    "desc": "Статья представляет метод Frame Guidance для управления генерацией видео без дополнительного обучения моделей. Этот подход использует покадровые сигналы, такие как ключевые кадры, эталонные изображения стиля или карты глубины. Frame Guidance значительно снижает использование памяти благодаря обработке латентного пространства. Метод применяет новую стратегию оптимизации латентного пространства для создания глобально согласованных видео."
                },
                "en": {
                    "title": "Effortless Control in Video Generation with Frame Guidance",
                    "desc": "Frame Guidance introduces a novel approach to video generation that does not require any training, allowing for effective control using frame-level signals. This method significantly reduces memory usage while ensuring that the generated videos maintain global coherence. By utilizing a simple latent processing technique and a unique latent optimization strategy, Frame Guidance can adapt to various tasks such as keyframe guidance and stylization. The results demonstrate that this approach can produce high-quality videos across different input types without the need for fine-tuning large models."
                },
                "zh": {
                    "title": "无训练的视频生成控制新方法",
                    "desc": "本论文提出了一种名为Frame Guidance的方法，用于控制视频生成，且无需训练。该方法利用帧级信号，如关键帧和风格参考图像，来实现对视频生成的精细控制。Frame Guidance显著降低了内存使用，并增强了视频输出的全局一致性。实验结果表明，该方法能够在多种任务中生成高质量的可控视频，且与任何视频模型兼容。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05167",
            "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
            "url": "https://huggingface.co/papers/2506.05167",
            "abstract": "ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or ECoRAG framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at https://github.com/ldilab/ECoRAG.",
            "score": 6,
            "issue_id": 4231,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "d979315df3a92206",
            "authors": [
                "Yeonseok Jeong",
                "Jinsu Kim",
                "Dohyeon Lee",
                "Seung-won Hwang"
            ],
            "affiliations": [
                "IPAI, Seoul National University",
                "Korea University",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05167.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#alignment",
                    "#long_context"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ECoRAG: Умное сжатие для точных ответов",
                    "desc": "ECoRAG - это новый фреймворк для улучшения производительности больших языковых моделей (LLM) в задачах открытого вопросно-ответного поиска (ODQA). Он использует сжатие полученных документов на основе доказательности, что позволяет снизить задержку и использование токенов. ECoRAG превосходит существующие методы сжатия и повышает эффективность LLM в задачах ODQA. Этот подход не только улучшает производительность, но и является экономически эффективным, сохраняя только необходимую информацию для генерации правильного ответа."
                },
                "en": {
                    "title": "ECoRAG: Elevating LLMs with Evidence-Based Compression",
                    "desc": "The ECoRAG framework enhances the performance of Large Language Models (LLMs) in Open-Domain Question Answering (ODQA) by focusing on evidentiality during document retrieval and compression. By filtering out non-evidential information, ECoRAG ensures that the generated answers are supported by relevant evidence, improving the overall accuracy of responses. Additionally, the framework optimizes resource usage by reducing latency and minimizing token consumption, making it more efficient than previous methods. Experiments demonstrate that ECoRAG significantly outperforms existing compression techniques in ODQA tasks."
                },
                "zh": {
                    "title": "ECoRAG：提升问答性能的证据性压缩框架",
                    "desc": "ECoRAG框架通过基于证据性压缩检索到的文档，提升了大型语言模型（LLM）在开放领域问答（ODQA）中的表现。该方法解决了以往压缩技术未能有效过滤非证据性信息的问题，从而提高了生成答案的准确性。ECoRAG确保生成的答案有足够的证据支持，并在必要时进行额外的文档检索。实验结果表明，ECoRAG在ODQA任务中优于现有的压缩方法，同时降低了延迟和令牌使用，具有很高的成本效益。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08887",
            "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval",
            "url": "https://huggingface.co/papers/2506.08887",
            "abstract": "The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on image-level vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from image-level to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Discrepancy Reduction in Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is available at https://github.com/LunarShen/DsicoVLA.",
            "score": 4,
            "issue_id": 4232,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "068c2f58bc5049d2",
            "authors": [
                "Leqi Shen",
                "Guoqiang Gong",
                "Tianxiang Hao",
                "Tao He",
                "Yifeng Zhang",
                "Pengzhang Liu",
                "Sicheng Zhao",
                "Jungong Han",
                "Guiguang Ding"
            ],
            "affiliations": [
                "BNRist",
                "Department of Automation, Tsinghua University",
                "GRG Banking Equipment Co., Ltd.",
                "Hangzhou Zhuoxi Institute of Brain and Intelligence",
                "JD.com",
                "School of Software",
                "South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08887.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#alignment",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Преодоление разрыва между изображениями и видео в мультимодальном поиске",
                    "desc": "Статья представляет DiscoVLA - метод для улучшения поиска видео по текстовым запросам с использованием модели CLIP. Авторы решают проблемы несоответствия в зрении, языке и выравнивании при переходе от изображений к видео. DiscoVLA объединяет признаки изображений и видео, генерирует псевдо-подписи к изображениям и применяет дистилляцию знаний для улучшения выравнивания видео и текста. Эксперименты показывают превосходство DiscoVLA над существующими методами в задаче поиска видео по текстовым запросам."
                },
                "en": {
                    "title": "Bridging Gaps in Video-Text Retrieval with DiscoVLA",
                    "desc": "The paper introduces DiscoVLA, a method designed to enhance video-text retrieval by addressing three main discrepancies: vision, language, and alignment. Unlike existing approaches that primarily focus on visual aspects, DiscoVLA integrates both image and video features to improve understanding at the video level. It also generates pseudo image captions to refine image-level alignment and employs Image-to-Video Alignment Distillation to strengthen video-level alignment using knowledge from image-level data. Experimental results show that DiscoVLA significantly outperforms previous methods, achieving a notable improvement in retrieval accuracy."
                },
                "zh": {
                    "title": "提升视频-文本检索的DiscoVLA方法",
                    "desc": "本文提出了一种名为DiscoVLA的方法，旨在通过解决视觉、语言和对齐的差异来改善视频-文本检索。传统的CLIP模型主要关注图像级别的视觉-语言匹配，而视频-文本检索需要在视频级别上进行全面理解。我们的方法通过图像-视频特征融合来整合图像和视频的特征，有效应对视觉和语言的差异。同时，我们生成伪图像标题以学习细粒度的图像级别对齐，从而减轻对齐差异。实验结果表明，DiscoVLA在视频-文本检索任务中表现优越，尤其在MSRVTT数据集上取得了50.5%的R@1成绩。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07932",
            "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor",
            "url": "https://huggingface.co/papers/2506.07932",
            "abstract": "A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Squeeze3D, a novel framework that leverages implicit prior knowledge learnt by existing pre-trained 3D generative models to compress 3D data at extremely high compression ratios. Our approach bridges the latent spaces between a pre-trained encoder and a pre-trained generation model through trainable mapping networks. Any 3D model represented as a mesh, point cloud, or a radiance field is first encoded by the pre-trained encoder and then transformed (i.e. compressed) into a highly compact latent code. This latent code can effectively be used as an extremely compressed representation of the mesh or point cloud. A mapping network transforms the compressed latent code into the latent space of a powerful generative model, which is then conditioned to recreate the original 3D model (i.e. decompression). Squeeze3D is trained entirely on generated synthetic data and does not require any 3D datasets. The Squeeze3D architecture can be flexibly used with existing pre-trained 3D encoders and existing generative models. It can flexibly support different formats, including meshes, point clouds, and radiance fields. Our experiments demonstrate that Squeeze3D achieves compression ratios of up to 2187x for textured meshes, 55x for point clouds, and 619x for radiance fields while maintaining visual quality comparable to many existing methods. Squeeze3D only incurs a small compression and decompression latency since it does not involve training object-specific networks to compress an object.",
            "score": 2,
            "issue_id": 4233,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 июня",
                "en": "June 9",
                "zh": "6月9日"
            },
            "hash": "a13860cb07518cb2",
            "authors": [
                "Rishit Dagli",
                "Yushi Guan",
                "Sankeerth Durvasula",
                "Mohammadreza Mofayezi",
                "Nandita Vijaykumar"
            ],
            "affiliations": [
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07932.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "Эффективное сжатие 3D-данных с помощью нейросетей",
                    "desc": "Squeeze3D - это новая система для сжатия 3D-данных, использующая предобученные модели. Она позволяет достичь высоких степеней сжатия при сохранении визуального качества для сеток, облаков точек и полей излучения. Система работает путем преобразования входных данных в компактное латентное представление с помощью энкодера, а затем восстановления 3D-модели генеративной моделью. Squeeze3D не требует специальных 3D-датасетов для обучения и обеспечивает быстрое сжатие и распаковку данных."
                },
                "en": {
                    "title": "Efficient 3D Data Compression with Squeeze3D",
                    "desc": "Squeeze3D is a new framework designed to compress 3D data efficiently using pre-trained models. It connects the latent spaces of a pre-trained encoder and a generative model through trainable mapping networks, allowing for high compression ratios while preserving visual quality. The framework can handle various 3D representations, such as meshes and point clouds, and is trained solely on synthetic data, eliminating the need for specific 3D datasets. Experiments show that Squeeze3D achieves impressive compression rates, making it a versatile tool for 3D data compression."
                },
                "zh": {
                    "title": "Squeeze3D：高效压缩3D数据的新方法",
                    "desc": "Squeeze3D是一个新颖的框架，利用预训练模型高效压缩3D数据，达到高压缩比并保持视觉质量。该方法通过可训练的映射网络连接预训练编码器和生成模型之间的潜在空间。3D模型首先被编码为紧凑的潜在代码，然后通过映射网络转换为生成模型的潜在空间，以重建原始3D模型。实验表明，Squeeze3D在不同格式的3D数据上实现了显著的压缩效果，同时延迟较小。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05928",
            "title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models",
            "url": "https://huggingface.co/papers/2506.05928",
            "abstract": "A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to further enhance the performance of parameter-efficient fine-tuning (PEFT) methods in Large Language Model (LLM) applications. Existing methods employ homogeneous MoE-LoRA architectures composed of LoRA experts with either similar or identical structures and capacities. However, these approaches often suffer from representation collapse and expert load imbalance, which negatively impact the potential of LLMs. To address these challenges, we propose a heterogeneous Mixture-of-Adapters (MoA) approach. This method dynamically integrates PEFT adapter experts with diverse structures, leveraging their complementary representational capabilities to foster expert specialization, thereby enhancing the effective transfer of pre-trained knowledge to downstream tasks. MoA supports two variants: (i) Soft MoA achieves fine-grained integration by performing a weighted fusion of all expert outputs; (ii) Sparse MoA activates adapter experts sparsely based on their contribution, achieving this with negligible performance degradation. Experimental results demonstrate that heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance and parameter efficiency. Our project is available at https://github.com/DCDmllm/MoA.",
            "score": 2,
            "issue_id": 4232,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 июня",
                "en": "June 6",
                "zh": "6月6日"
            },
            "hash": "f78e6f70ebb69ac2",
            "authors": [
                "Jie Cao",
                "Tianwei Lin",
                "Hongyang He",
                "Rolan Yan",
                "Wenqiao Zhang",
                "Juncheng Li",
                "Dongping Zhang",
                "Siliang Tang",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Tencent",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05928.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#training",
                    "#optimization",
                    "#small_models"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Гетерогенная смесь адаптеров: новый шаг в эффективной настройке языковых моделей",
                    "desc": "Статья представляет новый подход к эффективной настройке больших языковых моделей - гетерогенную Mixture-of-Adapters (MoA). MoA интегрирует различные адаптеры-эксперты, что позволяет преодолеть проблемы коллапса представлений и дисбаланса нагрузки экспертов, характерные для гомогенных методов MoE-LoRA. Предложены два варианта MoA: Soft MoA с взвешенным слиянием выходов всех экспертов и Sparse MoA с активацией только наиболее значимых экспертов. Эксперименты показывают, что MoA превосходит гомогенные методы MoE-LoRA как по производительности, так и по эффективности использования параметров."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Diverse Adapter Experts",
                    "desc": "This paper introduces a new approach called heterogeneous Mixture-of-Adapters (MoA) for fine-tuning Large Language Models (LLMs) more efficiently. Unlike traditional homogeneous MoE-LoRA methods that use similar adapter experts, MoA combines diverse adapter structures to improve performance and prevent issues like representation collapse. The method includes two variants: Soft MoA, which fuses outputs from all experts, and Sparse MoA, which selectively activates experts based on their effectiveness. Experimental results show that MoA significantly outperforms existing methods in both performance and parameter efficiency."
                },
                "zh": {
                    "title": "异构适配器混合：提升大语言模型微调效率的创新方法",
                    "desc": "本文提出了一种异构的适配器混合（MoA）方法，以增强大语言模型（LLM）中参数高效微调的效果。与传统的同质MoE-LoRA方法不同，MoA集成了具有不同结构的适配器专家，从而克服了表示崩溃和专家负载不平衡的问题。该方法通过动态整合适配器专家的互补表示能力，促进了专家的专业化，提升了预训练知识向下游任务的有效转移。实验结果表明，异构MoA在性能和参数效率上均优于同质MoE-LoRA方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08300",
            "title": "Institutional Books 1.0: A 242B token dataset from Harvard Library's\n  collections, refined for accuracy and usability",
            "url": "https://huggingface.co/papers/2506.08300",
            "abstract": "Institutional Books 1.0 provides a large dataset of public domain books from Harvard Library for training and inference of large language models, enhancing data accessibility and sustainability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have a direct impact on their quality. The rapid development and adoption of LLMs of varying quality has brought into focus the scarcity of publicly available, high-quality training data and revealed an urgent need to ground the stewardship of these datasets in sustainable practices with clear provenance chains. To that end, this technical report introduces Institutional Books 1.0, a large collection of public domain books originally digitized through Harvard Library's participation in the Google Books project, beginning in 2006. Working with Harvard Library, we extracted, analyzed, and processed these volumes into an extensively-documented dataset of historic texts. This analysis covers the entirety of Harvard Library's collection scanned as part of that project, originally spanning 1,075,899 volumes written in over 250 different languages for a total of approximately 250 billion tokens. As part of this initial release, the OCR-extracted text (original and post-processed) as well as the metadata (bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens, identified as being in the public domain have been made available. This report describes this project's goals and methods as well as the results of the analyses we performed, all in service of making this historical collection more accessible and easier for humans and machines alike to filter, read and use.",
            "score": 1,
            "issue_id": 4239,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 июня",
                "en": "June 10",
                "zh": "6月10日"
            },
            "hash": "8e42e765c2efbe2a",
            "authors": [
                "Matteo Cargnelutti",
                "Catherine Brobston",
                "John Hess",
                "Jack Cushman",
                "Kristi Mukk",
                "Aristana Scourtas",
                "Kyle Courtney",
                "Greg Leppert",
                "Amanda Watson",
                "Martha Whitehead",
                "Jonathan Zittrain"
            ],
            "affiliations": [
                "Harvard Law School Library",
                "Harvard Law School, Harvard School of Engineering and Applied Sciences, Harvard Kennedy School",
                "Harvard Library",
                "Institutional Data Initiative, Harvard Law School Library",
                "Library Innovation Lab, Harvard Law School Library"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08300.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#synthetic",
                    "#data"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "Большие данные для больших моделей: историческая библиотека в цифровом формате",
                    "desc": "Датасет Institutional Books 1.0 предоставляет обширную коллекцию книг из публичного достояния Гарвардской библиотеки для обучения и использования крупных языковых моделей. Он включает 983,004 тома на более чем 250 языках, содержащих около 242 миллиардов токенов. Датасет содержит OCR-извлеченный текст и метаданные книг, находящихся в открытом доступе. Проект направлен на повышение доступности исторической коллекции и облегчение ее использования как людьми, так и машинами."
                },
                "en": {
                    "title": "Unlocking Knowledge: A Sustainable Dataset for Language Models",
                    "desc": "Institutional Books 1.0 is a comprehensive dataset of public domain books sourced from Harvard Library, aimed at improving the training and inference processes of large language models (LLMs). The dataset includes over 983,000 volumes and approximately 242 billion tokens, providing a rich resource for enhancing the quality and diversity of training data. By ensuring clear provenance and sustainable practices in data stewardship, this initiative addresses the critical need for high-quality, publicly available datasets in the rapidly evolving field of machine learning. The project not only facilitates better model performance but also promotes accessibility to historical texts for both human and machine use."
                },
                "zh": {
                    "title": "提升语言模型的数据可用性与可持续性",
                    "desc": "Institutional Books 1.0 是一个大型数据集，包含来自哈佛图书馆的公共领域书籍，旨在为大型语言模型的训练和推理提供数据支持。这些书籍的多样性和质量直接影响到语言模型的表现，因此需要高质量的训练数据。该项目从哈佛图书馆提取和处理了超过983,000本书籍的文本和元数据，确保数据的可访问性和可持续性。通过这个数据集，研究人员和开发者可以更方便地使用历史文本，推动机器学习的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07047",
            "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
            "url": "https://huggingface.co/papers/2506.07047",
            "abstract": "Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.",
            "score": 1,
            "issue_id": 4237,
            "pub_date": "2025-06-08",
            "pub_date_card": {
                "ru": "8 июня",
                "en": "June 8",
                "zh": "6月8日"
            },
            "hash": "d3bc82dde4f2b8bc",
            "authors": [
                "Yu Xuejun",
                "Jianyuan Zhong",
                "Zijin Feng",
                "Pengyi Zhai",
                "Roozbeh Yousefzadeh",
                "Wei Chong Ng",
                "Haoxiong Liu",
                "Ziyi Shou",
                "Jing Xiong",
                "Yudong Zhou",
                "Claudia Beth Ong",
                "Austen Jeremy Sugiarto",
                "Yaoxi Zhang",
                "Wai Ming Tai",
                "Huan Cao",
                "Dongcai Lu",
                "Jiacheng Sun",
                "Qiang Xu",
                "Shen Xin",
                "Zhenguo Li"
            ],
            "affiliations": [
                "Huawei Celia Team",
                "Huawei Noahs Ark Lab",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07047.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#dataset",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Автоматизация формального доказательства теорем от естественного языка до математической логики",
                    "desc": "Исследователи представили Mathesis - первый сквозной конвейер для доказательства теорем, обрабатывающий неформальные формулировки задач. Ключевым компонентом является Mathesis-Autoformalizer, использующий обучение с подкреплением для улучшения формализации естественно-языковых задач. Система также включает Mathesis-Prover для генерации формальных доказательств. Для оценки применимости подхода авторы создали набор данных Gaokao-Formal из 488 сложных задач китайского вступительного экзамена в вузы."
                },
                "en": {
                    "title": "Bridging Natural Language and Formal Reasoning with Mathesis",
                    "desc": "This paper presents Mathesis, a novel end-to-end theorem proving system that processes informal problem statements, addressing the limitations of existing LLM-based theorem provers. It introduces Mathesis-Autoformalizer, which utilizes reinforcement learning to automatically convert natural language problems into formal statements, supported by the LeanScorer framework for assessing formalization quality. Additionally, the Mathesis-Prover generates formal proofs from these formalized statements. The system's effectiveness is validated through experiments on the Gaokao-Formal benchmark, demonstrating significant improvements in accuracy and pass rates compared to existing methods."
                },
                "zh": {
                    "title": "Mathesis：非正式问题的定理证明新突破",
                    "desc": "本论文介绍了Mathesis，这是第一个能够处理非正式问题陈述的端到端定理证明管道。它包括Mathesis-Autoformalizer，这是一个使用强化学习的自动形式化工具，能够提高自然语言问题的形式化能力，并通过LeanScorer框架评估形式化质量。论文还提出了Mathesis-Prover，能够从形式化的陈述中生成正式证明。通过在中国高考的488个复杂问题上进行评估，实验结果表明Mathesis在通过率和准确性上均优于现有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05700",
            "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model",
            "url": "https://huggingface.co/papers/2506.05700",
            "abstract": "RKEFino1, a knowledge-enhanced financial reasoning model, addresses accuracy and compliance challenges in Digital Regulatory Reporting through fine-tuning with domain knowledge from XBRL, CDM, and MOF, and introduces a novel Numerical NER task.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) hold great promise for financial applications but introduce critical accuracy and compliance challenges in Digital Regulatory Reporting (DRR). To address these issues, we propose RKEFino1, a regulation knowledge-enhanced financial reasoning model built upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We formulate two QA tasks-knowledge-based and mathematical reasoning-and introduce a novel Numerical NER task covering financial entities in both sentences and tables. Experimental results demonstrate the effectiveness and generalization capacity of RKEFino1 in compliance-critical financial tasks. We have released our model on Hugging Face.",
            "score": 1,
            "issue_id": 4231,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 июня",
                "en": "June 6",
                "zh": "6月6日"
            },
            "hash": "a23a28bb68811316",
            "authors": [
                "Yan Wang",
                "Yueru He",
                "Ruoyu Xiang",
                "Jeff Zhao"
            ],
            "affiliations": [
                "Columbia University",
                "New York University",
                "The University of Texas at Austin",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05700.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#reasoning",
                    "#training",
                    "#healthcare",
                    "#open_source"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Умная финансовая модель для точной регуляторной отчетности",
                    "desc": "RKEFino1 - это модель финансового рассуждения, улучшенная знаниями о регулировании, построенная на основе Fino1. Она решает проблемы точности и соответствия нормам в цифровой регуляторной отчетности путем дообучения на доменных знаниях из XBRL, CDM и MOF. Модель сформулирована для решения двух задач вопросно-ответной системы: на основе знаний и математических рассуждений. RKEFino1 также вводит новую задачу числового распознавания именованных сущностей (NER) для финансовых объектов в предложениях и таблицах."
                },
                "en": {
                    "title": "Enhancing Financial Compliance with RKEFino1",
                    "desc": "RKEFino1 is a financial reasoning model designed to improve accuracy and compliance in Digital Regulatory Reporting (DRR). It enhances the Fino1 model by incorporating domain knowledge from XBRL, CDM, and MOF, which are essential for understanding financial regulations. The model introduces a new task called Numerical Named Entity Recognition (NER) to identify financial entities in both text and tabular formats. Experimental results show that RKEFino1 effectively addresses compliance challenges and generalizes well to various financial tasks."
                },
                "zh": {
                    "title": "知识增强的金融推理，提升合规性与准确性",
                    "desc": "RKEFino1是一种增强知识的金融推理模型，旨在解决数字监管报告中的准确性和合规性挑战。该模型基于Fino1，并通过XBRL、CDM和MOF等领域知识进行微调。我们提出了两个问答任务——基于知识的问答和数学推理，并引入了一种新的数值命名实体识别任务，涵盖了句子和表格中的金融实体。实验结果表明，RKEFino1在合规性关键的金融任务中表现出色，具有良好的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07976",
            "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
            "url": "https://huggingface.co/papers/2506.07976",
            "abstract": "Test-Time Interaction (TTI) improves web agent performance by scaling interaction, enabling adaptive behavior and balancing exploration and exploitation without adding per-step compute.  \t\t\t\t\tAI-generated summary \t\t\t\t The current paradigm of test-time scaling relies on generating long reasoning traces (\"thinking\" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents.",
            "score": 0,
            "issue_id": 4239,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 июня",
                "en": "June 9",
                "zh": "6月9日"
            },
            "hash": "b35082cb7ca5bb65",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#agents",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Адаптивные веб-агенты: новые горизонты с масштабированием взаимодействия",
                    "desc": "Статья представляет новый подход к улучшению производительности веб-агентов с использованием масштабирования взаимодействия во время тестирования (Test-Time Interaction, TTI). TTI позволяет агентам адаптивно корректировать свое поведение, балансируя между исследованием и эксплуатацией без увеличения вычислительных затрат на каждом шаге. Авторы демонстрируют эффективность TTI на бенчмарках WebVoyager и WebArena, используя языковую модель Gemma 3 12B. Результаты показывают, что масштабирование взаимодействия является мощным дополнением к масштабированию вычислений на каждом шаге для обучения адаптивных агентов."
                },
                "en": {
                    "title": "Empowering Web Agents with Test-Time Interaction",
                    "desc": "This paper introduces Test-Time Interaction (TTI), a novel approach that enhances the performance of web agents by allowing them to interact more effectively with their environment. Unlike traditional methods that focus on generating long reasoning traces before acting, TTI enables agents to adapt their behavior in real-time by increasing their interaction horizon. This method supports complex behaviors such as exploration, backtracking, and dynamic re-planning within a single decision-making process. The authors demonstrate that TTI significantly improves task success rates on web benchmarks, showcasing its potential as a complementary strategy to existing scaling techniques in reinforcement learning."
                },
                "zh": {
                    "title": "测试时交互：提升代理智能的新维度",
                    "desc": "本文提出了一种新的方法，称为测试时交互（TTI），旨在提高网络代理的性能。TTI通过扩展代理的交互范围，使其能够在单次执行中进行探索、回溯和动态重新规划。与传统的依赖长推理轨迹的方式不同，TTI允许代理在与环境互动时获取新信息并适应其行为。我们的实验表明，TTI在WebVoyager和WebArena基准测试中表现出色，展示了交互扩展作为一种强大的补充方法。"
                }
            }
        }
    ],
    "link_prev": "2025-06-10.html",
    "link_next": "2025-06-12.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "10.06",
        "en": "06/10",
        "zh": "6月10日"
    },
    "short_date_next": {
        "ru": "12.06",
        "en": "06/12",
        "zh": "6月12日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 4,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 1,
        "#rl": 4,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 6,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 0
    }
}