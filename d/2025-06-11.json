{
    "date": {
        "ru": "11 Ğ¸ÑĞ½Ñ",
        "en": "June 11",
        "zh": "6æœˆ11æ—¥"
    },
    "time_utc": "2025-06-11 02:42",
    "weekday": 2,
    "issue_id": 4231,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.05167",
            "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
            "url": "https://huggingface.co/papers/2506.05167",
            "abstract": "ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or ECoRAG framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at https://github.com/ldilab/ECoRAG.",
            "score": 2,
            "issue_id": 4231,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 Ğ¸ÑĞ½Ñ",
                "en": "June 5",
                "zh": "6æœˆ5æ—¥"
            },
            "hash": "d979315df3a92206",
            "authors": [
                "Yeonseok Jeong",
                "Jinsu Kim",
                "Dohyeon Lee",
                "Seung-won Hwang"
            ],
            "affiliations": [
                "IPAI, Seoul National University",
                "Korea University",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05167.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#alignment",
                    "#long_context"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ECoRAG: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²",
                    "desc": "ECoRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° (ODQA). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ECoRAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ODQA. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°."
                },
                "en": {
                    "title": "ECoRAG: Elevating LLMs with Evidence-Based Compression",
                    "desc": "The ECoRAG framework enhances the performance of Large Language Models (LLMs) in Open-Domain Question Answering (ODQA) by focusing on evidentiality during document retrieval and compression. By filtering out non-evidential information, ECoRAG ensures that the generated answers are supported by relevant evidence, improving the overall accuracy of responses. Additionally, the framework optimizes resource usage by reducing latency and minimizing token consumption, making it more efficient than previous methods. Experiments demonstrate that ECoRAG significantly outperforms existing compression techniques in ODQA tasks."
                },
                "zh": {
                    "title": "ECoRAGï¼šæå‡é—®ç­”æ€§èƒ½çš„è¯æ®æ€§å‹ç¼©æ¡†æ¶",
                    "desc": "ECoRAGæ¡†æ¶é€šè¿‡åŸºäºè¯æ®æ€§å‹ç¼©æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼€æ”¾é¢†åŸŸé—®ç­”ï¼ˆODQAï¼‰ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä»¥å¾€å‹ç¼©æŠ€æœ¯æœªèƒ½æœ‰æ•ˆè¿‡æ»¤éè¯æ®æ€§ä¿¡æ¯çš„é—®é¢˜ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚ECoRAGç¡®ä¿ç”Ÿæˆçš„ç­”æ¡ˆæœ‰è¶³å¤Ÿçš„è¯æ®æ”¯æŒï¼Œå¹¶åœ¨å¿…è¦æ—¶è¿›è¡Œé¢å¤–çš„æ–‡æ¡£æ£€ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECoRAGåœ¨ODQAä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„å‹ç¼©æ–¹æ³•ï¼ŒåŒæ—¶é™ä½äº†å»¶è¿Ÿå’Œä»¤ç‰Œä½¿ç”¨ï¼Œå…·æœ‰å¾ˆé«˜çš„æˆæœ¬æ•ˆç›Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05700",
            "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model",
            "url": "https://huggingface.co/papers/2506.05700",
            "abstract": "RKEFino1, a knowledge-enhanced financial reasoning model, addresses accuracy and compliance challenges in Digital Regulatory Reporting through fine-tuning with domain knowledge from XBRL, CDM, and MOF, and introduces a novel Numerical NER task.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) hold great promise for financial applications but introduce critical accuracy and compliance challenges in Digital Regulatory Reporting (DRR). To address these issues, we propose RKEFino1, a regulation knowledge-enhanced financial reasoning model built upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We formulate two QA tasks-knowledge-based and mathematical reasoning-and introduce a novel Numerical NER task covering financial entities in both sentences and tables. Experimental results demonstrate the effectiveness and generalization capacity of RKEFino1 in compliance-critical financial tasks. We have released our model on Hugging Face.",
            "score": 1,
            "issue_id": 4231,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "a23a28bb68811316",
            "authors": [
                "Yan Wang",
                "Yueru He",
                "Ruoyu Xiang",
                "Jeff Zhao"
            ],
            "affiliations": [
                "Columbia University",
                "New York University",
                "The University of Texas at Austin",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05700.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#reasoning",
                    "#training",
                    "#healthcare",
                    "#open_source"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "RKEFino1 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Fino1. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ¼ Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¸Ğ· XBRL, CDM Ğ¸ MOF. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹: Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. RKEFino1 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ (NER) Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Financial Compliance with RKEFino1",
                    "desc": "RKEFino1 is a financial reasoning model designed to improve accuracy and compliance in Digital Regulatory Reporting (DRR). It enhances the Fino1 model by incorporating domain knowledge from XBRL, CDM, and MOF, which are essential for understanding financial regulations. The model introduces a new task called Numerical Named Entity Recognition (NER) to identify financial entities in both text and tabular formats. Experimental results show that RKEFino1 effectively addresses compliance challenges and generalizes well to various financial tasks."
                },
                "zh": {
                    "title": "çŸ¥è¯†å¢å¼ºçš„é‡‘èæ¨ç†ï¼Œæå‡åˆè§„æ€§ä¸å‡†ç¡®æ€§",
                    "desc": "RKEFino1æ˜¯ä¸€ç§å¢å¼ºçŸ¥è¯†çš„é‡‘èæ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ•°å­—ç›‘ç®¡æŠ¥å‘Šä¸­çš„å‡†ç¡®æ€§å’Œåˆè§„æ€§æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åŸºäºFino1ï¼Œå¹¶é€šè¿‡XBRLã€CDMå’ŒMOFç­‰é¢†åŸŸçŸ¥è¯†è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªé—®ç­”ä»»åŠ¡â€”â€”åŸºäºçŸ¥è¯†çš„é—®ç­”å’Œæ•°å­¦æ¨ç†ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°å€¼å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ï¼Œæ¶µç›–äº†å¥å­å’Œè¡¨æ ¼ä¸­çš„é‡‘èå®ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRKEFino1åœ¨åˆè§„æ€§å…³é”®çš„é‡‘èä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-10.html",
    "link_next": "2025-06-12.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "10.06",
        "en": "06/10",
        "zh": "6æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.06",
        "en": "06/12",
        "zh": "6æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§å«åšå¼ºåŒ–é¢„è®­ç»ƒï¼ˆRPTï¼‰çš„æ–°æ–¹æ³•ã€‚å®ƒæŠŠä¸‹ä¸€ä¸ªè¯é¢„æµ‹å½“ä½œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚è¿™æ ·å¯ä»¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„å¾®è°ƒæä¾›åšå®åŸºç¡€ã€‚RPTåˆ©ç”¨å¤§é‡æ–‡æœ¬æ•°æ®ï¼Œä¸ä¾èµ–ç‰¹å®šé¢†åŸŸçš„æ ‡æ³¨ç­”æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¢åŠ è®­ç»ƒè®¡ç®—é‡å¯ä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚",
        "title": "Reinforcement Pre-Training",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§å«åšå¼ºåŒ–é¢„è®­ç»ƒï¼ˆRPTï¼‰çš„æ–°æ–¹æ³•ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ« zhÇ’ng jiÃ ozuÃ² qiÃ¡ng huÃ  yÃ¹ xÃ¹nliÃ n (RPT) de xÄ«n fÄngfÇ.\n\nå®ƒæŠŠä¸‹ä¸€ä¸ªè¯é¢„æµ‹å½“ä½œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚\nTÄ bÇ xiÃ  yÄ« gÃ¨ cÃ­ yÃ¹cÃ¨ dÄngzuÃ² qiÃ¡ng huÃ  xuÃ©xÃ­ rÃ¨nwÃ¹ lÃ¡i xÃ¹nliÃ n yÇ”yÃ¡n mÃ³xÃ­ng.\n\nè¿™æ ·å¯ä»¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„å¾®è°ƒæä¾›åšå®åŸºç¡€ã€‚\nZhÃ¨yÃ ng kÄ›yÇ tÃ­gÄo yÃ¹cÃ¨ de zhÇ”nquÃ¨xÃ¬ng, bÃ¬ng wÃ¨i jÃ¬n yÄ« bÃ¹ de wÄ“i tiÃ¡o tÃ­gÅng jiÄnshÃ­ jÄ«chÇ”.\n\nRPTåˆ©ç”¨å¤§é‡æ–‡æœ¬æ•°æ®ï¼Œä¸ä¾èµ–ç‰¹å®šé¢†åŸŸçš„æ ‡æ³¨ç­”æ¡ˆã€‚\nRPT lÃ¬yÃ²ng dÃ liÃ ng wÃ©nbÄ›n shÃ¹jÃ¹, bÃ¹ yÄ«lÃ i tÃ¨dÃ¬ng lÇngyÃ¹ de biÄozhÃ¹ dÃ¡'Ã n.\n\nå®éªŒç»“æœæ˜¾ç¤ºï¼Œå¢åŠ è®­ç»ƒè®¡ç®—é‡å¯ä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚\nShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, zÄ“ngjiÄ xÃ¹nliÃ n jÃ¬suÃ n liÃ ng kÄ›yÇ tÃ­gÄo yÃ¹cÃ¨ zhÇ”nquÃ¨xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"å¼ºåŒ–\", \"pinyin\": \"qiÃ¡ng huÃ \", \"trans\": \"reinforcement\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pre-training\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"é¢„æµ‹\", \"pinyin\": \"yÃ¹ cÃ¨\", \"trans\": \"prediction\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wu\", \"trans\": \"task\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å‡†ç¡®æ€§\", \"pinyin\": \"zhÇ”n quÃ¨ xÃ¬ng\", \"trans\": \"accuracy\"},\n    {\"word\": \"å¾®è°ƒ\", \"pinyin\": \"wÄ“i tiÃ¡o\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"åšå®\", \"pinyin\": \"jiÄn shÃ­\", \"trans\": \"solid\"},\n    {\"word\": \"åŸºç¡€\", \"pinyin\": \"jÄ« chÇ”\", \"trans\": \"foundation\"},\n    {\"word\": \"åˆ©ç”¨\", \"pinyin\": \"lÃ¬ yÃ²ng\", \"trans\": \"utilize\"},\n    {\"word\": \"æ–‡æœ¬\", \"pinyin\": \"wÃ©n bÄ›n\", \"trans\": \"text\"},\n    {\"word\": \"æ•°æ®\", \"pinyin\": \"shÃ¹ jÃ¹\", \"trans\": \"data\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ« lÃ i\", \"trans\": \"depend on\"},\n    {\"word\": \"ç‰¹å®š\", \"pinyin\": \"tÃ¨ dÃ¬ng\", \"trans\": \"specific\"},\n    {\"word\": \"é¢†åŸŸ\", \"pinyin\": \"lÇng yÃ¹\", \"trans\": \"field\"},\n    {\"word\": \"æ ‡æ³¨\", \"pinyin\": \"biÄo zhÃ¹\", \"trans\": \"annotation\"},\n    {\"word\": \"ç­”æ¡ˆ\", \"pinyin\": \"dÃ¡ Ã n\", \"trans\": \"answer\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"æ˜¾ç¤º\", \"pinyin\": \"xiÇn shÃ¬\", \"trans\": \"show\"},\n    {\"word\": \"å¢åŠ \", \"pinyin\": \"zÄ“ng jiÄ\", \"trans\": \"increase\"},\n    {\"word\": \"è®¡ç®—é‡\", \"pinyin\": \"jÃ¬ suÃ n liÃ ng\", \"trans\": \"computational amount\"}\n]",
        "trans": "This article introduces a new method called Reinforced Pre-Training (RPT). It trains language models by treating next-word prediction as a reinforcement learning task. This approach improves the accuracy of predictions and provides a solid foundation for further fine-tuning. RPT leverages a large amount of text data and does not rely on domain-specific annotated answers. Experimental results show that increasing the amount of training computation can enhance prediction accuracy.",
        "update_ts": "2025-06-10 09:13"
    }
}