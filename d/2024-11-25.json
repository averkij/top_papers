{
    "date": {
        "ru": "25 ноября",
        "en": "November 25",
        "zh": "11月25日"
    },
    "time_utc": "2024-11-25 11:09",
    "weekday": 0,
    "issue_id": 761,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.14793",
            "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
            "url": "https://huggingface.co/papers/2411.14793",
            "abstract": "Recent large-scale diffusion models generate high-quality images but struggle to learn new, personalized artistic styles, which limits the creation of unique style templates. Fine-tuning with reference images is the most promising approach, but it often blindly utilizes objectives and noise level distributions used for pre-training, leading to suboptimal style alignment. We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge. This enables models to better capture unique styles and generate images with higher style alignment. Our method allows diffusion models to learn and share new \"style templates\", enhancing personalized content creation. We demonstrate the ability to generate styles such as personal watercolor paintings, minimal flat cartoons, 3D renderings, multi-panel images, and memes with text, thereby broadening the scope of style-driven generation.",
            "score": 19,
            "issue_id": 752,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "03859b57f29683ab",
            "authors": [
                "Jooyoung Choi",
                "Chaehun Shin",
                "Yeongtak Oh",
                "Heeseung Kim",
                "Sungroh Yoon"
            ],
            "affiliations": [
                "AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University",
                "Data Science and AI Laboratory, ECE, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14793.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#multimodal",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Улучшение стилизации изображений с помощью оптимизации шума в диффузионных моделях",
                    "desc": "Статья представляет новый метод улучшения генерации изображений в определенном стиле с помощью диффузионных моделей. Авторы предлагают использовать Style-friendly SNR sampler, который смещает распределение соотношения сигнал-шум в сторону более высоких уровней шума при дообучении модели. Это позволяет лучше захватывать уникальные стилистические особенности и генерировать изображения с более высоким соответствием заданному стилю. Метод демонстрирует способность генерировать различные стили, включая акварельные рисунки, минималистичные мультфильмы, 3D-рендеры и мемы с текстом."
                },
                "en": {
                    "title": "Unlocking Unique Artistic Styles with Style-friendly SNR Sampler",
                    "desc": "This paper addresses the challenge of adapting large-scale diffusion models to generate personalized artistic styles. The authors introduce the Style-friendly SNR sampler, which modifies the signal-to-noise ratio (SNR) during fine-tuning to emphasize higher noise levels where stylistic features are more prominent. By doing so, the model improves its ability to capture unique styles, resulting in images that align better with the desired artistic expression. The proposed method expands the creative possibilities for generating diverse styles, including watercolor paintings and cartoons, thus enhancing personalized content creation."
                },
                "zh": {
                    "title": "提升个性化艺术风格生成的信噪比方法",
                    "desc": "最近的大规模扩散模型能够生成高质量的图像，但在学习新的个性化艺术风格方面存在困难，这限制了独特风格模板的创建。微调参考图像是最有前景的方法，但通常盲目使用预训练时的目标和噪声水平分布，导致风格对齐不理想。我们提出了风格友好的信噪比（SNR）采样器，在微调过程中积极将信噪比分布向更高的噪声水平转移，以专注于风格特征出现的噪声水平。这使得模型能够更好地捕捉独特风格，并生成具有更高风格对齐的图像。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15124",
            "title": "TÜLU 3: Pushing Frontiers in Open Language Model Post-Training",
            "url": "https://huggingface.co/papers/2411.15124",
            "abstract": "Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce T\\\"ULU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. T\\\"ULU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With T\\\"ULU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.   In addition to the T\\\"ULU 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the T\\\"ULU 3 approach to more domains.",
            "score": 17,
            "issue_id": 755,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "44809ea81d71ef97",
            "authors": [
                "Nathan Lambert",
                "Jacob Morrison",
                "Valentina Pyatkin",
                "Shengyi Huang",
                "Hamish Ivison",
                "Faeze Brahman",
                "Lester James V. Miranda",
                "Alisa Liu",
                "Nouha Dziri",
                "Shane Lyu",
                "Yuling Gu",
                "Saumya Malik",
                "Victoria Graf",
                "Jena D. Hwang",
                "Jiangjiang Yang",
                "Ronan Le Bras",
                "Oyvind Tafjord",
                "Chris Wilhelm",
                "Luca Soldaini",
                "Noah A. Smith",
                "Yizhong Wang",
                "Pradeep Dasigi",
                "Hannaneh Hajishirzi"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15124.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#data",
                    "#open_source",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Открытый рецепт пост-тренировки языковых моделей",
                    "desc": "Статья представляет T\"ULU 3 - семейство открытых моделей, обученных с помощью пост-тренировки. Авторы описывают методы обучения, включая SFT, DPO и новый метод RLVR. T\"ULU 3 превосходит многие современные модели, включая инструктированные версии Llama 3.1 и GPT-4o-mini. Исследователи предоставляют полные рецепты обучения, наборы данных и инструменты для воспроизведения результатов."
                },
                "en": {
                    "title": "Unlocking Language Model Potential with T\"ULU 3",
                    "desc": "This paper presents T\"ULU 3, a set of fully-open post-trained language models that enhance performance and capabilities beyond existing models. It emphasizes the importance of transparency in training data and methodologies, providing a comprehensive guide for implementing modern post-training techniques. The models utilize advanced training algorithms, including supervised finetuning and a novel reinforcement learning method, achieving superior results compared to both open and closed counterparts. Additionally, the paper offers extensive resources for replication and adaptation, including datasets, training code, and evaluation tools."
                },
                "zh": {
                    "title": "T\"ULU 3：开放的后训练模型新纪元",
                    "desc": "本文介绍了T\"ULU 3，这是一个完全开放的最新后训练模型系列，旨在提高语言模型的行为和技能。我们提供了模型的训练数据、代码和训练配方，填补了开放技术与专有技术之间的透明度差距。T\"ULU 3在多个基准测试中超越了现有的语言模型，包括Llama 3.1和GPT-4o-mini等。我们还引入了一种多任务评估方案，并提供了详细的报告，以便于在更多领域中复现和适应T\"ULU 3的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12946",
            "title": "A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection",
            "url": "https://huggingface.co/papers/2411.12946",
            "abstract": "Large Language Models are prone to off-topic misuse, where users may prompt these models to perform tasks beyond their intended scope. Current guardrails, which often rely on curated examples or custom classifiers, suffer from high false-positive rates, limited adaptability, and the impracticality of requiring real-world data that is not available in pre-production. In this paper, we introduce a flexible, data-free guardrail development methodology that addresses these challenges. By thoroughly defining the problem space qualitatively and passing this to an LLM to generate diverse prompts, we construct a synthetic dataset to benchmark and train off-topic guardrails that outperform heuristic approaches. Additionally, by framing the task as classifying whether the user prompt is relevant with respect to the system prompt, our guardrails effectively generalize to other misuse categories, including jailbreak and harmful prompts. Lastly, we further contribute to the field by open-sourcing both the synthetic dataset and the off-topic guardrail models, providing valuable resources for developing guardrails in pre-production environments and supporting future research and development in LLM safety.",
            "score": 9,
            "issue_id": 756,
            "pub_date": "2024-11-20",
            "pub_date_card": {
                "ru": "20 ноября",
                "en": "November 20",
                "zh": "11月20日"
            },
            "hash": "de5ca9118a5cab35",
            "authors": [
                "Gabriel Chua",
                "Shing Yee Chan",
                "Shaun Khoo"
            ],
            "affiliations": [
                "Government Technology Agency Singapore",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12946.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#hallucinations",
                    "#synthetic",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Гибкая защита языковых моделей без реальных данных",
                    "desc": "Статья представляет новую методологию разработки защитных механизмов для больших языковых моделей (LLM) от нецелевого использования. Авторы предлагают гибкий подход, не требующий реальных данных, основанный на генерации синтетического датасета с помощью LLM. Разработанные защитные механизмы превосходят эвристические подходы и могут обобщаться на другие категории злоупотреблений. Авторы также открывают доступ к синтетическому датасету и моделям защиты для поддержки будущих исследований в области безопасности LLM."
                },
                "en": {
                    "title": "Building Better Guardrails for Large Language Models",
                    "desc": "This paper addresses the issue of off-topic misuse in Large Language Models (LLMs) by proposing a new methodology for developing guardrails without relying on real-world data. The authors create a synthetic dataset by using LLMs to generate diverse prompts based on a well-defined problem space, which helps in training more effective off-topic guardrails. Their approach not only reduces false positives but also enhances adaptability to various misuse scenarios, such as harmful prompts and jailbreak attempts. Furthermore, the authors contribute to the community by open-sourcing their synthetic dataset and guardrail models, promoting further research in LLM safety."
                },
                "zh": {
                    "title": "构建灵活的防护措施，提升大型语言模型安全性",
                    "desc": "本论文探讨了大型语言模型在使用中可能出现的偏离主题的误用问题。我们提出了一种灵活的、无数据的防护措施开发方法，旨在解决现有方法的高误报率和适应性不足的问题。通过对问题空间的定性定义，并利用大型语言模型生成多样化的提示，我们构建了一个合成数据集，用于基准测试和训练防护措施。最后，我们开源了合成数据集和防护模型，为大型语言模型的安全性研究和开发提供了重要资源。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15098",
            "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
            "url": "https://huggingface.co/papers/2411.15098",
            "abstract": "In this paper, we introduce OminiControl, a highly versatile and parameter-efficient framework that integrates image conditions into pre-trained Diffusion Transformer (DiT) models. At its core, OminiControl leverages a parameter reuse mechanism, enabling the DiT to encode image conditions using itself as a powerful backbone and process them with its flexible multi-modal attention processors. Unlike existing methods, which rely heavily on additional encoder modules with complex architectures, OminiControl (1) effectively and efficiently incorporates injected image conditions with only ~0.1% additional parameters, and (2) addresses a wide range of image conditioning tasks in a unified manner, including subject-driven generation and spatially-aligned conditions such as edges, depth, and more. Remarkably, these capabilities are achieved by training on images generated by the DiT itself, which is particularly beneficial for subject-driven generation. Extensive evaluations demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted models in both subject-driven and spatially-aligned conditional generation. Additionally, we release our training dataset, Subjects200K, a diverse collection of over 200,000 identity-consistent images, along with an efficient data synthesis pipeline to advance research in subject-consistent generation.",
            "score": 8,
            "issue_id": 754,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "9cd668db99ed0902",
            "authors": [
                "Zhenxiong Tan",
                "Songhua Liu",
                "Xingyi Yang",
                "Qiaochu Xue",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15098.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#synthetic",
                    "#data",
                    "#diffusion",
                    "#dataset",
                    "#architecture",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "OminiControl: Универсальное управление генерацией изображений с минимальными затратами",
                    "desc": "OminiControl - это новая эффективная система для интеграции изображений в предобученные модели Diffusion Transformer (DiT). Она использует механизм повторного использования параметров, позволяя DiT кодировать условия изображений с помощью собственной архитектуры. OminiControl требует всего 0,1% дополнительных параметров и может решать широкий спектр задач условной генерации изображений. Система превосходит существующие модели на основе UNet и адаптированные DiT как в генерации, управляемой субъектом, так и в пространственно-согласованной условной генерации."
                },
                "en": {
                    "title": "Efficient Image Conditioning with OminiControl",
                    "desc": "OminiControl is a new framework that enhances pre-trained Diffusion Transformer (DiT) models by integrating image conditions efficiently. It uses a parameter reuse mechanism, allowing the DiT to process image conditions with minimal additional parameters, specifically around 0.1%. This framework can handle various image conditioning tasks, such as generating images based on specific subjects or aligning them with spatial features like edges and depth. OminiControl has shown superior performance compared to traditional UNet-based models and other DiT adaptations, and it comes with a large dataset, Subjects200K, to support further research."
                },
                "zh": {
                    "title": "OminiControl：高效整合图像条件的创新框架",
                    "desc": "本文介绍了OminiControl，这是一个高度灵活且参数高效的框架，能够将图像条件集成到预训练的扩散变换器（DiT）模型中。OminiControl利用参数重用机制，使DiT能够使用自身作为强大的基础，编码图像条件，并通过灵活的多模态注意力处理器进行处理。与现有方法不同，OminiControl仅需约0.1%的额外参数，就能有效地整合注入的图像条件，并以统一的方式处理多种图像条件任务。通过在DiT自身生成的图像上进行训练，OminiControl在主题驱动生成和空间对齐条件生成方面的表现优于现有的UNet和DiT适应模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14982",
            "title": "Large Multi-modal Models Can Interpret Features in Large Multi-modal Models",
            "url": "https://huggingface.co/papers/2411.14982",
            "abstract": "Recent advances in Large Multimodal Models (LMMs) lead to significant breakthroughs in both academia and industry. One question that arises is how we, as humans, can understand their internal neural representations. This paper takes an initial step towards addressing this question by presenting a versatile framework to identify and interpret the semantics within LMMs. Specifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the representations into human understandable features. 2) We then present an automatic interpretation framework to interpreted the open-semantic features learned in SAE by the LMMs themselves. We employ this framework to analyze the LLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these features can effectively steer the model's behavior. Our results contribute to a deeper understanding of why LMMs excel in specific tasks, including EQ tests, and illuminate the nature of their mistakes along with potential strategies for their rectification. These findings offer new insights into the internal mechanisms of LMMs and suggest parallels with the cognitive processes of the human brain.",
            "score": 6,
            "issue_id": 761,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "7d4fae86425adb6c",
            "authors": [
                "Kaichen Zhang",
                "Yifei Shen",
                "Bo Li",
                "Ziwei Liu"
            ],
            "affiliations": [
                "LMMs-Lab Team, S-Lab, NTU, Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14982.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрывая тайны мышления больших мультимодальных моделей",
                    "desc": "Статья представляет новый подход к интерпретации внутренних нейронных представлений в больших мультимодальных моделях (LMM). Авторы применяют разреженный автоэнкодер для выделения понятных человеку признаков, а затем используют автоматическую систему интерпретации для анализа этих признаков. Исследование проводится на модели LLaVA-NeXT-8B с помощью LLaVA-OV-72B, демонстрируя, как выделенные признаки влияют на поведение модели. Результаты помогают лучше понять принципы работы LMM и проводят параллели с когнитивными процессами человеческого мозга."
                },
                "en": {
                    "title": "Unlocking the Secrets of Large Multimodal Models",
                    "desc": "This paper explores how we can understand the internal workings of Large Multimodal Models (LMMs) by using a Sparse Autoencoder (SAE) to break down their representations into features that humans can comprehend. It introduces a framework for automatically interpreting these features, which are learned by the LMMs themselves. The study specifically analyzes the LLaVA-NeXT-8B model in relation to the LLaVA-OV-72B model, showing that the identified features can influence the model's performance on various tasks. The findings enhance our understanding of LMMs' strengths and weaknesses, drawing parallels to human cognitive processes."
                },
                "zh": {
                    "title": "揭示大型多模态模型的内部机制",
                    "desc": "本文探讨了大型多模态模型（LMMs）的内部神经表示如何被理解。我们首先使用稀疏自编码器（SAE）将表示解耦为人类可理解的特征。接着，我们提出了一个自动解释框架，用于解释LMMs自身学习的开放语义特征。我们的研究结果加深了对LMMs在特定任务中表现优异原因的理解，并揭示了它们错误的性质及可能的纠正策略。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14794",
            "title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection",
            "url": "https://huggingface.co/papers/2411.14794",
            "abstract": "The advancement of Large Vision Language Models (LVLMs) has significantly improved multimodal understanding, yet challenges remain in video reasoning tasks due to the scarcity of high-quality, large-scale datasets. Existing video question-answering (VideoQA) datasets often rely on costly manual annotations with insufficient granularity or automatic construction methods with redundant frame-by-frame analysis, limiting their scalability and effectiveness for complex reasoning. To address these challenges, we introduce VideoEspresso, a novel dataset that features VideoQA pairs preserving essential spatial details and temporal coherence, along with multimodal annotations of intermediate reasoning steps. Our construction pipeline employs a semantic-aware method to reduce redundancy, followed by generating QA pairs using GPT-4o. We further develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes, guiding GPT-4o in extracting logical relationships from QA pairs and video content. To exploit the potential of high-quality VideoQA pairs, we propose a Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a two-stage instruction fine-tuned reasoning LVLM. This framework adaptively selects core frames and performs CoT reasoning using multimodal evidence. Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our method outperforms existing baselines on most tasks, demonstrating superior video reasoning capabilities. Our code and dataset will be released at: https://github.com/hshjerry/VideoEspresso",
            "score": 5,
            "issue_id": 757,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "f03437948b77773f",
            "authors": [
                "Songhao Han",
                "Wei Huang",
                "Hairong Shi",
                "Le Zhuo",
                "Xiu Su",
                "Shifeng Zhang",
                "Xu Zhou",
                "Xiaojuan Qi",
                "Yue Liao",
                "Si Liu"
            ],
            "affiliations": [
                "Beihang University",
                "CUHK",
                "Central South University",
                "Sangfor Technologies Inc.",
                "Shanghai AI Lab",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14794.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#cv",
                    "#games",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VideoEspresso: новый подход к созданию данных и рассуждению для задач видеопонимания",
                    "desc": "Статья представляет новый набор данных VideoEspresso для задач видеопонимания и видео-вопросно-ответных систем. Авторы разработали метод создания высококачественных пар вопрос-ответ с сохранением пространственно-временной информации и промежуточных шагов рассуждения. Предложена архитектура Hybrid LVLMs Collaboration, включающая селектор ключевых кадров и двухэтапную модель рассуждений на основе больших мультимодальных языковых моделей. Эксперименты показали превосходство предложенного подхода над существующими методами на большинстве задач видеопонимания."
                },
                "en": {
                    "title": "Enhancing VideoQA with VideoEspresso: A New Era of Multimodal Reasoning",
                    "desc": "This paper presents VideoEspresso, a new dataset designed to enhance video question-answering (VideoQA) by providing high-quality pairs that maintain spatial and temporal coherence. It addresses the limitations of existing datasets, which often rely on expensive manual annotations or ineffective automatic methods. The authors introduce a semantic-aware construction pipeline that reduces redundancy and generates QA pairs using GPT-4o, along with video Chain-of-Thought (CoT) annotations to improve reasoning. Additionally, they propose a Hybrid LVLMs Collaboration framework that optimally selects frames and performs reasoning, achieving superior performance on various tasks compared to existing models."
                },
                "zh": {
                    "title": "VideoEspresso：提升视频推理的新数据集与框架",
                    "desc": "本论文介绍了一种新的视频问答数据集VideoEspresso，旨在解决现有数据集在视频推理任务中的不足。该数据集保留了重要的空间细节和时间连贯性，并提供了中间推理步骤的多模态注释。我们提出了一种混合的大型视觉语言模型协作框架，能够自适应选择核心帧并进行连锁思维推理。实验结果表明，我们的方法在多个任务上优于现有基线，展示了更强的视频推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14762",
            "title": "Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction",
            "url": "https://huggingface.co/papers/2411.14762",
            "abstract": "Efficient tokenization of videos remains a challenge in training vision models that can process long videos. One promising direction is to develop a tokenizer that can encode long video clips, as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization. However, training existing tokenizers on long videos often incurs a huge training cost as they are trained to reconstruct all the frames at once. In this paper, we introduce CoordTok, a video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos, inspired by recent advances in 3D generative models. In particular, CoordTok encodes a video into factorized triplane representations and reconstructs patches that correspond to randomly sampled (x,y,t) coordinates. This allows for training large tokenizer models directly on long videos without requiring excessive training resources. Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips. For instance, CoordTok can encode a 128-frame video with 128times128 resolution into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality. We further show that this efficient video tokenization enables memory-efficient training of a diffusion transformer that can generate 128 frames at once.",
            "score": 4,
            "issue_id": 756,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "9490a40884583735",
            "authors": [
                "Huiwon Jang",
                "Sihyun Yu",
                "Jinwoo Shin",
                "Pieter Abbeel",
                "Younggyo Seo"
            ],
            "affiliations": [
                "KAIST",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14762.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#3d",
                    "#diffusion",
                    "#video",
                    "#training"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "CoordTok: Эффективная токенизация длинных видео с помощью координатного представления",
                    "desc": "CoordTok - это новый токенизатор для видео, который использует координатное представление для кодирования длинных видеоклипов. Он обучается восстанавливать патчи, соответствующие случайно выбранным координатам (x,y,t), что позволяет эффективно обрабатывать длинные видео. CoordTok значительно сокращает количество токенов, необходимых для кодирования видео, по сравнению с базовыми методами. Это делает возможным эффективное по памяти обучение диффузионного трансформера для генерации длинных видео."
                },
                "en": {
                    "title": "Efficient Video Tokenization with CoordTok",
                    "desc": "This paper presents CoordTok, a novel video tokenizer designed to efficiently encode long video clips by leveraging coordinate-based representations. Unlike traditional tokenizers that reconstruct all frames simultaneously, CoordTok uses a factorized triplane representation to map (x,y,t) coordinates to video patches, significantly reducing the number of tokens needed. The approach allows for training large models on long videos without incurring high computational costs. Experimental results demonstrate that CoordTok can encode a 128-frame video into just 1280 tokens, outperforming existing methods that require thousands of tokens for similar quality."
                },
                "zh": {
                    "title": "高效视频标记化，降低训练成本！",
                    "desc": "本论文提出了一种名为CoordTok的视频标记器，旨在高效处理长视频的标记化问题。CoordTok通过学习坐标表示与输入视频补丁之间的映射，利用了视频的时间一致性。与传统方法相比，CoordTok显著减少了编码长视频所需的标记数量，从而降低了训练成本。实验表明，CoordTok能够在保持重建质量的同时，将128帧视频的标记数量减少到1280个。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14208",
            "title": "Novel View Extrapolation with Video Diffusion Priors",
            "url": "https://huggingface.co/papers/2411.14208",
            "abstract": "The field of novel view synthesis has made significant strides thanks to the development of radiance field methods. However, most radiance field techniques are far better at novel view interpolation than novel view extrapolation where the synthesis novel views are far beyond the observed training views. We design ViewExtrapolator, a novel view synthesis approach that leverages the generative priors of Stable Video Diffusion (SVD) for realistic novel view extrapolation. By redesigning the SVD denoising process, ViewExtrapolator refines the artifact-prone views rendered by radiance fields, greatly enhancing the clarity and realism of the synthesized novel views. ViewExtrapolator is a generic novel view extrapolator that can work with different types of 3D rendering such as views rendered from point clouds when only a single view or monocular video is available. Additionally, ViewExtrapolator requires no fine-tuning of SVD, making it both data-efficient and computation-efficient. Extensive experiments demonstrate the superiority of ViewExtrapolator in novel view extrapolation. Project page: https://kunhao-liu.github.io/ViewExtrapolator/.",
            "score": 2,
            "issue_id": 755,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 ноября",
                "en": "November 21",
                "zh": "11月21日"
            },
            "hash": "dddb7a1ecc9850f3",
            "authors": [
                "Kunhao Liu",
                "Ling Shao",
                "Shijian Lu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "UCAS-Terminus AI Lab, UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14208.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#video"
                ],
                "emoji": "🔭",
                "ru": {
                    "title": "Реалистичная экстраполяция новых ракурсов с помощью генеративных моделей",
                    "desc": "ViewExtrapolator - это новый подход к синтезу видов, использующий генеративные возможности Stable Video Diffusion (SVD) для реалистичной экстраполяции новых ракурсов. Метод улучшает качество изображений, полученных с помощью радиационных полей, значительно повышая четкость и реалистичность синтезированных видов. ViewExtrapolator может работать с различными типами 3D-рендеринга, включая облака точек, и не требует дополнительного обучения SVD. Эксперименты подтверждают превосходство ViewExtrapolator в задаче экстраполяции новых ракурсов."
                },
                "en": {
                    "title": "Enhancing Novel View Extrapolation with ViewExtrapolator",
                    "desc": "This paper introduces ViewExtrapolator, a new method for synthesizing novel views that go beyond the original training views. It utilizes the generative capabilities of Stable Video Diffusion (SVD) to improve the quality of these extrapolated views. By modifying the SVD denoising process, the method reduces artifacts and enhances the realism of the generated images. ViewExtrapolator is versatile, working with various 3D rendering types and requiring no fine-tuning, making it efficient in both data and computation."
                },
                "zh": {
                    "title": "提升新视图外推的清晰度与真实感",
                    "desc": "本论文介绍了一种新的视图合成方法，称为ViewExtrapolator，旨在改善新视图外推的效果。传统的辐射场技术在新视图插值方面表现良好，但在新视图外推时效果较差。ViewExtrapolator利用稳定视频扩散（SVD）的生成先验，通过重新设计去噪过程，显著提高了合成新视图的清晰度和真实感。该方法适用于不同类型的3D渲染，并且无需对SVD进行微调，具有数据和计算效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14521",
            "title": "MyTimeMachine: Personalized Facial Age Transformation",
            "url": "https://huggingface.co/papers/2411.14521",
            "abstract": "Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the person's appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to a personal photo collection of the user depicting aging in a small time interval (20sim40 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), which combines a global aging prior with a personal photo collection (using as few as 50 images) to learn a personalized age transformation. We introduce a novel Adapter Network that combines personalized aging features with global aging features and generates a re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. Our approach can also be extended to videos, achieving high-quality, identity-preserving, and temporally consistent aging effects that resemble actual appearances at target ages, demonstrating its superiority over state-of-the-art approaches.",
            "score": 2,
            "issue_id": 755,
            "pub_date": "2024-11-21",
            "pub_date_card": {
                "ru": "21 ноября",
                "en": "November 21",
                "zh": "11月21日"
            },
            "hash": "d104e8a00be886bb",
            "authors": [
                "Luchao Qi",
                "Jiaye Wu",
                "Bang Gong",
                "Annie N. Wang",
                "David W. Jacobs",
                "Roni Sengupta"
            ],
            "affiliations": [
                "University of Maryland, College Park",
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14521.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#multimodal",
                    "#cv",
                    "#video"
                ],
                "emoji": "👴",
                "ru": {
                    "title": "Персонализированное старение лиц с помощью глубокого обучения",
                    "desc": "Статья представляет новый метод персонализированного старения лиц на изображениях, называемый MyTimeMachine (MyTM). Авторы комбинируют глобальную модель старения с личной коллекцией фотографий для создания индивидуализированной трансформации возраста. В работе вводится Адаптерная Сеть, объединяющая персонализированные и глобальные признаки старения для генерации состаренного изображения с помощью StyleGAN2. Метод также применим к видео, обеспечивая качественные, сохраняющие идентичность и темпорально согласованные эффекты старения."
                },
                "en": {
                    "title": "Personalized Aging: Your Face, Your Time",
                    "desc": "This paper presents MyTimeMachine (MyTM), a novel approach to facial aging that combines global aging knowledge with personalized photo collections. It addresses the challenge of accurately predicting an individual's appearance at a target age by utilizing as few as 50 personal images. The method employs an Adapter Network that integrates personalized and global aging features, generating realistic re-aged images using StyleGAN2. Additionally, the authors introduce three specialized loss functions to enhance the personalization of the aging process, resulting in high-quality, identity-preserving, and temporally consistent aging effects."
                },
                "zh": {
                    "title": "个性化老化，真实再现",
                    "desc": "面部老化是一个复杂的过程，受到性别、种族、生活方式等多种因素的影响，因此很难学习到一个通用的老化模型来准确预测个体的老化情况。现有技术虽然能够生成逼真的老化效果，但重塑的图像往往与目标年龄的外貌不符，因此需要个性化处理。我们提出了MyTimeMachine（MyTM），它结合了全球老化模型和个人照片集（仅需50张图像）来学习个性化的年龄转换。我们的创新在于引入了适配器网络和三种损失函数，使得生成的老化图像能够保持身份一致性，并在视频中实现高质量的老化效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15033",
            "title": "One to rule them all: natural language to bind communication, perception and action",
            "url": "https://huggingface.co/papers/2411.15033",
            "abstract": "In recent years, research in the area of human-robot interaction has focused on developing robots capable of understanding complex human instructions and performing tasks in dynamic and diverse environments. These systems have a wide range of applications, from personal assistance to industrial robotics, emphasizing the importance of robots interacting flexibly, naturally and safely with humans. This paper presents an advanced architecture for robotic action planning that integrates communication, perception, and planning with Large Language Models (LLMs). Our system is designed to translate commands expressed in natural language into executable robot actions, incorporating environmental information and dynamically updating plans based on real-time feedback. The Planner Module is the core of the system where LLMs embedded in a modified ReAct framework are employed to interpret and carry out user commands. By leveraging their extensive pre-trained knowledge, LLMs can effectively process user requests without the need to introduce new knowledge on the changing environment. The modified ReAct framework further enhances the execution space by providing real-time environmental perception and the outcomes of physical actions. By combining robust and dynamic semantic map representations as graphs with control components and failure explanations, this architecture enhances a robot adaptability, task execution, and seamless collaboration with human users in shared and dynamic environments. Through the integration of continuous feedback loops with the environment the system can dynamically adjusts the plan to accommodate unexpected changes, optimizing the robot ability to perform tasks. Using a dataset of previous experience is possible to provide detailed feedback about the failure. Updating the LLMs context of the next iteration with suggestion on how to overcame the issue.",
            "score": 1,
            "issue_id": 758,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "f76bbfe3f8854ad7",
            "authors": [
                "Simone Colombani",
                "Dimitri Ognibene",
                "Giuseppe Boccignone"
            ],
            "affiliations": [
                "Oversonic Robotics, Carate Brianza, Italy",
                "University of Milan, Italy",
                "University of Milano-Bicocca, Milan, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15033.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#architecture",
                    "#agents",
                    "#robotics",
                    "#optimization",
                    "#agi",
                    "#interpretability"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Интеллектуальные роботы: взаимодействие с человеком на новом уровне",
                    "desc": "Статья представляет передовую архитектуру для планирования действий роботов, интегрирующую коммуникацию, восприятие и планирование с использованием больших языковых моделей (LLM). Система переводит команды на естественном языке в исполняемые действия робота, учитывая информацию об окружающей среде и динамически обновляя планы на основе обратной связи в реальном времени. Ключевым компонентом является Модуль Планировщика, где LLM, встроенные в модифицированную структуру ReAct, интерпретируют и выполняют команды пользователя. Архитектура повышает адаптивность робота, выполнение задач и взаимодействие с пользователями в динамических средах."
                },
                "en": {
                    "title": "Empowering Robots with Natural Language Understanding and Dynamic Adaptability",
                    "desc": "This paper discusses a new architecture for robots that helps them understand and follow human instructions in real-time. It combines Large Language Models (LLMs) with a planning system that allows robots to interpret natural language commands and adapt to changing environments. The core of this system is a Planner Module that uses LLMs to translate user requests into actions while considering the current state of the environment. By incorporating feedback loops and a dynamic semantic map, the architecture improves the robot's ability to work alongside humans and adjust its plans as needed."
                },
                "zh": {
                    "title": "智能机器人：自然语言与动态环境的完美结合",
                    "desc": "近年来，人机交互领域的研究集中在开发能够理解复杂人类指令并在动态多样环境中执行任务的机器人。这些系统在个人助理和工业机器人等多个应用中具有广泛的应用，强调机器人与人类灵活、自然和安全的互动。本文提出了一种先进的机器人行动规划架构，结合了通信、感知和规划，利用大型语言模型（LLMs）将自然语言表达的命令转化为可执行的机器人动作。通过实时反馈动态更新计划，该系统提高了机器人在共享和动态环境中适应性、任务执行和与人类用户的无缝协作能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15131",
            "title": "WildLMa: Long Horizon Loco-Manipulation in the Wild",
            "url": "https://huggingface.co/papers/2411.15131",
            "abstract": "`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world environments, which requires the robot to (1) have skills that generalize across object configurations; (2) be capable of long-horizon task execution in diverse environments; and (3) perform complex manipulation beyond pick-and-place. Quadruped robots with manipulators hold promise for extending the workspace and enabling robust locomotion, but existing results do not investigate such a capability. This paper proposes WildLMa with three components to address these issues: (1) adaptation of learned low-level controller for VR-enabled whole-body teleoperation and traversability; (2) WildLMa-Skill -- a library of generalizable visuomotor skills acquired via imitation learning or heuristics and (3) WildLMa-Planner -- an interface of learned skills that allow LLM planners to coordinate skills for long-horizon tasks. We demonstrate the importance of high-quality training data by achieving higher grasping success rate over existing RL baselines using only tens of demonstrations. WildLMa exploits CLIP for language-conditioned imitation learning that empirically generalizes to objects unseen in training demonstrations. Besides extensive quantitative evaluation, we qualitatively demonstrate practical robot applications, such as cleaning up trash in university hallways or outdoor terrains, operating articulated objects, and rearranging items on a bookshelf.",
            "score": 0,
            "issue_id": 755,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "c38df92e9599db09",
            "authors": [
                "Ri-Zhao Qiu",
                "Yuchen Song",
                "Xuanbin Peng",
                "Sai Aneesh Suryadevara",
                "Ge Yang",
                "Minghuan Liu",
                "Mazeyu Ji",
                "Chengzhe Jia",
                "Ruihan Yang",
                "Xueyan Zou",
                "Xiaolong Wang"
            ],
            "affiliations": [
                "MIT",
                "NVIDIA",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15131.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#robotics",
                    "#training",
                    "#agi",
                    "#transfer_learning",
                    "#long_context"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "WildLMa: Универсальный робот-манипулятор для реального мира",
                    "desc": "Статья представляет систему WildLMa для мобильной манипуляции роботов в реальных условиях. Система включает адаптивный низкоуровневый контроллер для телеоперации, библиотеку обобщаемых визуомоторных навыков и планировщик на основе языковых моделей для выполнения долгосрочных задач. WildLMa использует имитационное обучение и CLIP для генерализации на новые объекты, демонстрируя высокую точность захвата при небольшом количестве обучающих примеров. Система показала свою эффективность в практических задачах, таких как уборка мусора и перестановка предметов."
                },
                "en": {
                    "title": "Empowering Robots for Real-World Manipulation with WildLMa",
                    "desc": "This paper presents WildLMa, a framework designed for mobile manipulation robots to operate effectively in varied real-world settings. It includes a low-level controller for teleoperation, a library of generalizable visuomotor skills learned through imitation, and a planner that coordinates these skills for complex tasks. The approach emphasizes the significance of high-quality training data, achieving better performance in grasping tasks compared to existing reinforcement learning methods. Additionally, WildLMa utilizes CLIP for language-conditioned imitation learning, enabling the robot to adapt to new objects not seen during training."
                },
                "zh": {
                    "title": "WildLMa：提升机器人在真实环境中的操控能力",
                    "desc": "本论文提出了一种名为WildLMa的移动操控机器人系统，旨在解决在多样化真实环境中执行复杂任务的挑战。该系统包括三个主要组件：适应性低级控制器、通用视觉运动技能库和长时间任务规划接口。通过模仿学习和高质量训练数据，WildLMa在抓取成功率上超越了现有的强化学习基线。该研究展示了机器人在实际应用中的潜力，如清理校园走廊垃圾和操作复杂物体。"
                }
            }
        }
    ],
    "link_prev": "2024-11-22.html",
    "link_next": "2024-11-26.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "22.11",
        "en": "11/22",
        "zh": "11月22日"
    },
    "short_date_next": {
        "ru": "26.11",
        "en": "11/26",
        "zh": "11月26日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 2,
        "#agi": 2,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了大型扩散模型生成高质量图像，但难以学习新的个性化艺术风格。现有方法在微调时盲目使用预训练目标和噪声水平分布，导致风格对齐不佳。作者提出了风格友好的信噪比采样器，在微调时将信噪比分布偏向更高的噪声水平，以捕捉独特风格。这使模型能更好地生成具有高风格一致性的图像，并允许创建和共享新的风格模板。作者展示了生成水彩画、简笔画、3D渲染等多种风格的能力。",
        "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
        "pinyin": "这篇文章讨论了大型扩散模型生成高质量图像，但难以学习新的个性化艺术风格。现有方法在微调时盲目使用预训练目标和噪声水平分布，导致风格对齐不佳。作者提出了风格友好的信噪比采样器，在微调时将信噪比分布偏向更高的噪声水平，以捕捉独特风格。这使模型能更好地生成具有高风格一致性的图像，并允许创建和共享新的风格模板。作者展示了生成水彩画、简笔画、3D渲染等多种风格的能力。\n\nzhè piān wén zhāng tǎo lùn le dà xíng kuò sàn mó xíng shēng chéng gāo zhì liàng tú xiàng, dàn nán yǐ xué xí xīn de gè xìng huà yì shù fēng gé. xiàn yǒu fāng fǎ zài wēi tiáo shí māng mù shǐ yòng yù xùn liàn mù biāo hé zào shēng shuǐ píng fēn bù, dǎo zhì fēng gé duì qí bù jiā. zuò zhě tí chū le fēng gé yǒu hǎo de xìn zào bǐ cǎi yǎng qì, zài wēi tiáo shí jiāng xìn zào bǐ fēn bù piān xiàng gèng gāo de zào shēng shuǐ píng, yǐ bǔ zhuō dú tè fēng gé. zhè shǐ mó xíng néng gèng hǎo de shēng chéng jù yǒu gāo fēng gé yī zhì xìng de tú xiàng, bìng yǔn xǔ chuàng jiàn hé gòng xiǎng xīn de fēng gé mú bǎn. zuò zhě zhǎn shì le shēng chéng shuǐ cǎi huà, jiǎn bǐ huà, 3D xuàn rán děng duō zhǒng fēng gé de néng lì.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'}, {'word': '个性化', 'pinyin': 'gè xìng huà', 'trans': 'personalized'}, {'word': '艺术', 'pinyin': 'yì shù', 'trans': 'art'}, {'word': '风格', 'pinyin': 'fēng gé', 'trans': 'style'}, {'word': '现有', 'pinyin': 'xiàn yǒu', 'trans': 'existing'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tune'}, {'word': '盲目', 'pinyin': 'máng mù', 'trans': 'blindly'}, {'word': '预训练', 'pinyin': 'yù xùn liàn', 'trans': 'pre-trained'}, {'word': '目标', 'pinyin': 'mù biāo', 'trans': 'target'}, {'word': '噪声', 'pinyin': 'zào shēng', 'trans': 'noise'}, {'word': '水平', 'pinyin': 'shuǐ píng', 'trans': 'level'}, {'word': '分布', 'pinyin': 'fēn bù', 'trans': 'distribution'}, {'word': '导致', 'pinyin': 'dǎo zhì', 'trans': 'lead to'}, {'word': '对齐', 'pinyin': 'duì qí', 'trans': 'alignment'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '信噪比', 'pinyin': 'xìn zào bǐ', 'trans': 'signal-to-noise ratio'}, {'word': '采样器', 'pinyin': 'cǎi yàng qì', 'trans': 'sampler'}, {'word': '偏向', 'pinyin': 'piān xiàng', 'trans': 'bias towards'}, {'word': '捕捉', 'pinyin': 'bǔ zhuō', 'trans': 'capture'}, {'word': '独特', 'pinyin': 'dú tè', 'trans': 'unique'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '模板', 'pinyin': 'mú bǎn', 'trans': 'template'}, {'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'}, {'word': '水彩画', 'pinyin': 'shuǐ cǎi huà', 'trans': 'watercolor painting'}, {'word': '简笔画', 'pinyin': 'jiǎn bǐ huà', 'trans': 'line drawing'}, {'word': '3D渲染', 'pinyin': '3D xuàn rán', 'trans': '3D rendering'}]",
        "trans": "This article discusses the ability of large diffusion models to generate high-quality images but their difficulty in learning new personalized artistic styles. Existing methods blindly use pre-trained objectives and noise level distributions during fine-tuning, leading to poor style alignment. The authors propose a style-friendly signal-to-noise ratio sampler that biases the signal-to-noise ratio distribution towards higher noise levels during fine-tuning to capture unique styles. This allows the model to generate images with higher style consistency and enables the creation and sharing of new style templates. The authors demonstrate the capability to generate various styles such as watercolor, sketch, and 3D rendering.",
        "update_ts": "2024-11-25 09:06"
    }
}