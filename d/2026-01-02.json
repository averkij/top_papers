{
    "date": {
        "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 2",
        "zh": "1æœˆ2æ—¥"
    },
    "time_utc": "2026-01-02 12:43",
    "weekday": 4,
    "issue_id": 384,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.24617",
            "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
            "url": "https://huggingface.co/papers/2512.24617",
            "abstract": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled Î¼P parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.",
            "score": 17,
            "issue_id": 375,
            "pub_date": "2026-12-31",
            "pub_date_card": {
                "ru": "31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 31",
                "zh": "12æœˆ31æ—¥"
            },
            "hash": "14b3c28c67b8e772",
            "authors": [
                "Xingwei Qu",
                "Shaowen Wang",
                "Zihao Huang",
                "Kai Hua",
                "Fan Yin",
                "Rui-Jie Zhu",
                "Jundong Zhou",
                "Qiyang Min",
                "Zihao Wang",
                "Yizhi Li",
                "Tianyu Zhang",
                "He Xing",
                "Zheng Zhang",
                "Yuxuan Song",
                "Tianyu Zheng",
                "Zhiyuan Zeng",
                "Chenghua Lin",
                "Ge Zhang",
                "Wenhao Huang"
            ],
            "affiliations": [
                "ByteDance",
                "M-A-P",
                "Mila - Quebec AI Institute",
                "Tsinghua University",
                "University of Manchester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.24617.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¼Ğ°Ñ€Ñ‚-ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ: ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ² (DLCM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµÑ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ³Ğ´Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ²ĞµĞ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 2.69% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Language Models with Dynamic Concept Learning",
                    "desc": "This paper introduces Dynamic Large Concept Models (DLCM), which improve how language models process information by focusing on important semantic transitions instead of treating all tokens equally. By learning semantic boundaries and shifting computation to a compressed concept space, DLCM enhances reasoning efficiency. The framework allows for the discovery of variable-length concepts without relying on fixed linguistic units, fundamentally changing how models scale. Additionally, it presents a new scaling law that optimizes compute allocation, leading to significant performance improvements in zero-shot tasks."
                },
                "zh": {
                    "title": "åŠ¨æ€å¤§å‹æ¦‚å¿µæ¨¡å‹ï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„åˆ›æ–°",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æ‰€æœ‰æ ‡è®°åº”ç”¨ç»Ÿä¸€è®¡ç®—ï¼Œä½†è¯­è¨€çš„ä¿¡æ¯å¯†åº¦å¹¶ä¸å‡åŒ€ã€‚è¿™ç§ç»Ÿä¸€çš„è®¡ç®—æ–¹å¼åœ¨å¯é¢„æµ‹çš„åŒºåŸŸæµªè´¹äº†è®¡ç®—èƒ½åŠ›ï¼Œè€Œåœ¨è¯­ä¹‰å…³é”®çš„è½¬å˜ä¸Šåˆ™åˆ†é…ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†åŠ¨æ€å¤§å‹æ¦‚å¿µæ¨¡å‹ï¼ˆDLCMï¼‰ï¼Œå®ƒé€šè¿‡å­¦ä¹ æ½œåœ¨è¡¨ç¤ºçš„è¯­ä¹‰è¾¹ç•Œï¼Œå°†è®¡ç®—ä»æ ‡è®°è½¬ç§»åˆ°å‹ç¼©çš„æ¦‚å¿µç©ºé—´ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚DLCMèƒ½å¤Ÿç«¯åˆ°ç«¯åœ°å‘ç°å¯å˜é•¿åº¦çš„æ¦‚å¿µï¼Œå¹¶å¼•å…¥äº†é¦–ä¸ªè€ƒè™‘å‹ç¼©çš„æ‰©å±•æ³•åˆ™ï¼Œä¼˜åŒ–äº†è®¡ç®—èµ„æºçš„åˆ†é…ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.24165",
            "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
            "url": "https://huggingface.co/papers/2512.24165",
            "abstract": "While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\\%) and Gemini-3-Flash (+111.6\\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.",
            "score": 9,
            "issue_id": 378,
            "pub_date": "2026-12-30",
            "pub_date_card": {
                "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 30",
                "zh": "12æœˆ30æ—¥"
            },
            "hash": "7e8814af6b3a30c5",
            "authors": [
                "Zefeng He",
                "Xiaoye Qu",
                "Yafu Li",
                "Tong Zhu",
                "Siyuan Huang",
                "Yu Cheng"
            ],
            "affiliations": [
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.24165.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#reasoning",
                    "#diffusion",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ°Ñ DiffThinker. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DiffThinker Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-5 Ğ¸ Gemini-3-Flash."
                },
                "en": {
                    "title": "Revolutionizing Vision-Centric Reasoning with DiffThinker",
                    "desc": "This paper introduces DiffThinker, a new framework for Generative Multimodal Reasoning that enhances the reasoning capabilities of Multimodal Large Language Models (MLLMs) in vision-centric tasks. Unlike traditional MLLMs that focus primarily on text, DiffThinker treats multimodal reasoning as a generative image-to-image task, improving logical consistency and spatial accuracy. The authors highlight four key advantages of this approach: efficiency, controllability, native parallelism, and collaboration. Through extensive experiments, DiffThinker demonstrates significant performance improvements over existing models, establishing it as a leading method for complex reasoning in visual contexts."
                },
                "zh": {
                    "title": "ç”Ÿæˆå¤šæ¨¡æ€æ¨ç†çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆå¤šæ¨¡æ€æ¨ç†èŒƒå¼ï¼Œå¹¶å¼•å…¥äº†DiffThinkerï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¨ç†æ¡†æ¶ã€‚DiffThinkerå°†å¤šæ¨¡æ€æ¨ç†é‡æ–°å®šä¹‰ä¸ºä¸€ç§ç”Ÿæˆçš„å›¾åƒåˆ°å›¾åƒä»»åŠ¡ï¼Œä»è€Œåœ¨è§†è§‰ä¸­å¿ƒä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜çš„é€»è¾‘ä¸€è‡´æ€§å’Œç©ºé—´ç²¾åº¦ã€‚é€šè¿‡ä¸ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œç³»ç»Ÿæ¯”è¾ƒï¼Œæ­ç¤ºäº†DiffThinkerçš„å››ä¸ªæ ¸å¿ƒç‰¹æ€§ï¼šæ•ˆç‡ã€å¯æ§æ€§ã€åŸç”Ÿå¹¶è¡Œæ€§å’Œåä½œæ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDiffThinkeråœ¨å¤šä¸ªé¢†åŸŸçš„è¡¨ç°æ˜¾è‘—ä¼˜äºé¢†å…ˆçš„é—­æºæ¨¡å‹ï¼Œå±•ç¤ºäº†ç”Ÿæˆå¤šæ¨¡æ€æ¨ç†åœ¨è§†è§‰æ¨ç†ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22630",
            "title": "On the Role of Discreteness in Diffusion LLMs",
            "url": "https://huggingface.co/papers/2512.22630",
            "abstract": "Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.",
            "score": 5,
            "issue_id": 375,
            "pub_date": "2026-12-27",
            "pub_date_card": {
                "ru": "27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 27",
                "zh": "12æœˆ27æ—¥"
            },
            "hash": "8ee34765599976ba",
            "authors": [
                "Ziqi Jin",
                "Bin Wang",
                "Xiang Lin",
                "Lidong Bing",
                "Aixin Sun"
            ],
            "affiliations": [
                "MiroMind AI",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22630.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ°: Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ², Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ĞºÑƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¾Ñ‚ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¸Ñ… Ğ½Ğ° Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ½Ğ°Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑĞ¼, Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ°Ñ€Ğ¶Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Language Generation with Structured Diffusion Models",
                    "desc": "This paper explores the challenges of applying diffusion models to language generation due to the structured nature of text. It categorizes existing methods into two types: continuous diffusion in embedding space and discrete diffusion over tokens, highlighting their limitations. The authors identify two main issues: the uniform corruption of information and the inability to capture dependencies between multiple tokens during decoding. They propose that future diffusion models should better align with the inherent structure of language to improve coherence in generated text."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ä»¥é€‚åº”è¯­è¨€ç»“æ„",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨è¯­è¨€ç”Ÿæˆä¸­å…·æœ‰å¸å¼•äººçš„ç‰¹æ€§ï¼Œå¦‚å¹¶è¡Œè§£ç å’Œè¿­ä»£ä¼˜åŒ–ï¼Œä½†æ–‡æœ¬çš„ç¦»æ•£æ€§å’Œé«˜åº¦ç»“æ„åŒ–ç‰¹å¾ä½¿å¾—ç›´æ¥åº”ç”¨æ‰©æ•£åŸç†é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»æ‰©æ•£è¿‡ç¨‹å’Œè¯­è¨€å»ºæ¨¡çš„è§’åº¦é‡æ–°å®¡è§†æ‰©æ•£è¯­è¨€å»ºæ¨¡ï¼Œå¹¶æ¦‚è¿°äº†äº”ä¸ªå°†æ‰©æ•£æœºåˆ¶ä¸è¯­è¨€ç‰¹å®šè¦æ±‚åŒºåˆ†å¼€æ¥çš„ç‰¹æ€§ã€‚æˆ‘ä»¬å°†ç°æœ‰æ–¹æ³•åˆ†ä¸ºåµŒå…¥ç©ºé—´ä¸­çš„è¿ç»­æ‰©æ•£å’Œæ ‡è®°ä¸Šçš„ç¦»æ•£æ‰©æ•£ï¼Œå¹¶æŒ‡å‡ºæ¯ç§æ–¹æ³•ä»…æ»¡è¶³äº”ä¸ªåŸºæœ¬ç‰¹æ€§ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤åæ˜ å‡ºç»“æ„ä¸Šçš„æƒè¡¡ã€‚é€šè¿‡å¯¹æœ€è¿‘å¤§å‹æ‰©æ•£è¯­è¨€æ¨¡å‹çš„åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šå‡åŒ€è…èš€ä¸å°Šé‡ä¿¡æ¯åœ¨ä½ç½®ä¸Šçš„åˆ†å¸ƒï¼Œä»¥åŠé€æ ‡è®°è¾¹é™…è®­ç»ƒæ— æ³•åœ¨å¹¶è¡Œè§£ç ä¸­æ•æ‰å¤šæ ‡è®°ä¾èµ–å…³ç³»ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-01.html",
    "link_next": "2026-01-05.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "01.01",
        "en": "01/01",
        "zh": "1æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "05.01",
        "en": "01/05",
        "zh": "1æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}