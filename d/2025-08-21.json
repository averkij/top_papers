{
    "date": {
        "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 21",
        "zh": "8æœˆ21æ—¥"
    },
    "time_utc": "2025-08-21 03:35",
    "weekday": 3,
    "issue_id": 5463,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.11987",
            "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction",
            "url": "https://huggingface.co/papers/2508.11987",
            "abstract": "FutureX is a dynamic, live benchmark for evaluating LLM agents in future prediction tasks, addressing challenges in real-time updates and data contamination.  \t\t\t\t\tAI-generated summary \t\t\t\t Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information gathering, contextual understanding, and decision-making under uncertainty. Agents must not only gather and interpret vast amounts of dynamic information but also integrate diverse data sources, weigh uncertainties, and adapt predictions based on emerging trends, just as human experts do in fields like politics, economics, and finance. Despite its importance, no large-scale benchmark exists for evaluating agents on future prediction, largely due to challenges in handling real-time updates and retrieving timely, accurate answers. To address this, we introduce FutureX, a dynamic and live evaluation benchmark specifically designed for LLM agents performing future prediction tasks. FutureX is the largest and most diverse live benchmark for future prediction, supporting real-time daily updates and eliminating data contamination through an automated pipeline for question gathering and answer collection. We evaluate 25 LLM/agent models, including those with reasoning, search capabilities, and integration of external tools such as the open-source Deep Research Agent and closed-source Deep Research models. This comprehensive evaluation assesses agents' adaptive reasoning and performance in dynamic environments. Additionally, we provide in-depth analyses of agents' failure modes and performance pitfalls in future-oriented tasks, including the vulnerability to fake web pages and the temporal validity. Our goal is to establish a dynamic, contamination-free evaluation standard that drives the development of LLM agents capable of performing at the level of professional human analysts in complex reasoning and predictive thinking.",
            "score": 14,
            "issue_id": 5463,
            "pub_date": "2025-08-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 16",
                "zh": "8æœˆ16æ—¥"
            },
            "hash": "122aab80ab15c823",
            "authors": [
                "Zhiyuan Zeng",
                "Jiashuo Liu",
                "Siyuan Chen",
                "Tianci He",
                "Yali Liao",
                "Jinpeng Wang",
                "Zaiyuan Wang",
                "Yang Yang",
                "Lingyue Yin",
                "Mingren Yin",
                "Zhenwei Zhu",
                "Tianle Cai",
                "Zehui Chen",
                "Jiecao Chen",
                "Yantao Du",
                "Xiang Gao",
                "Jiacheng Guo",
                "Liang Hu",
                "Jianpeng Jiao",
                "Xiangsheng Li",
                "Jingkai Liu",
                "Shuang Ni",
                "Zhoufutu Wen",
                "Ge Zhang",
                "Kaiyuan Zhang",
                "Xin Zhou",
                "Jose Blanchet",
                "Xipeng Qiu",
                "Mengdi Wang",
                "Wenhao Huang"
            ],
            "affiliations": [
                "ByteDance",
                "Fudan University",
                "Princeton University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11987.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#survey",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "FutureX: Ğ–Ğ¸Ğ²Ğ¾Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "FutureX - ÑÑ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¶Ğ¸Ğ²Ğ¾Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. FutureX Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ 25 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LLM/Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ."
                },
                "en": {
                    "title": "FutureX: Elevating LLM Agents to Expert Predictors",
                    "desc": "FutureX is a novel benchmark designed to evaluate large language model (LLM) agents on their ability to predict future events. It addresses the challenges of real-time data updates and the risk of data contamination, which have hindered previous evaluation efforts. By providing a dynamic and automated pipeline for question and answer collection, FutureX allows for daily updates and ensures the integrity of the data used for assessments. This benchmark not only evaluates the reasoning and adaptability of various LLM models but also highlights their weaknesses in handling complex predictive tasks, aiming to enhance their performance to match that of human experts."
                },
                "zh": {
                    "title": "FutureXï¼šæœªæ¥é¢„æµ‹çš„åŠ¨æ€è¯„ä¼°åŸºå‡†",
                    "desc": "FutureXæ˜¯ä¸€ä¸ªåŠ¨æ€çš„å®æ—¶åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨æœªæ¥é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®ƒè§£å†³äº†å®æ—¶æ›´æ–°å’Œæ•°æ®æ±¡æŸ“ç­‰æŒ‘æˆ˜ï¼Œæ”¯æŒæ¯æ—¥æ›´æ–°å¹¶é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹æ¶ˆé™¤æ•°æ®æ±¡æŸ“ã€‚è¯¥åŸºå‡†è¯„ä¼°äº†25ç§LLM/ä»£ç†æ¨¡å‹ï¼Œé‡ç‚¹è€ƒå¯Ÿå®ƒä»¬åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”æ€§æ¨ç†å’Œæ€§èƒ½ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªåŠ¨æ€ã€æ— æ±¡æŸ“çš„è¯„ä¼°æ ‡å‡†ï¼Œæ¨åŠ¨LLMä»£ç†åœ¨å¤æ‚æ¨ç†å’Œé¢„æµ‹æ€ç»´æ–¹é¢çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.14811",
            "title": "Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From\n  Sparse Inputs without Per-Scene Optimization",
            "url": "https://huggingface.co/papers/2508.14811",
            "abstract": "Tinker is a framework for high-fidelity 3D editing using pretrained diffusion models, enabling multi-view consistency with minimal per-scene training.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Tinker, a versatile framework for high-fidelity 3D editing that operates in both one-shot and few-shot regimes without any per-scene finetuning. Unlike prior techniques that demand extensive per-scene optimization to ensure multi-view consistency or to produce dozens of consistent edited input views, Tinker delivers robust, multi-view consistent edits from as few as one or two images. This capability stems from repurposing pretrained diffusion models, which unlocks their latent 3D awareness. To drive research in this space, we curate the first large-scale multi-view editing dataset and data pipeline, spanning diverse scenes and styles. Building on this dataset, we develop our framework capable of generating multi-view consistent edited views without per-scene training, which consists of two novel components: (1) Referring multi-view editor: Enables precise, reference-driven edits that remain coherent across all viewpoints. (2) Any-view-to-video synthesizer: Leverages spatial-temporal priors from video diffusion to perform high-quality scene completion and novel-view generation even from sparse inputs. Through extensive experiments, Tinker significantly reduces the barrier to generalizable 3D content creation, achieving state-of-the-art performance on editing, novel-view synthesis, and rendering enhancement tasks. We believe that Tinker represents a key step towards truly scalable, zero-shot 3D editing. Project webpage: https://aim-uofa.github.io/Tinker",
            "score": 9,
            "issue_id": 5463,
            "pub_date": "2025-08-20",
            "pub_date_card": {
                "ru": "20 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 20",
                "zh": "8æœˆ20æ—¥"
            },
            "hash": "148e6ade70f23987",
            "authors": [
                "Canyu Zhao",
                "Xiaoman Li",
                "Tianjian Feng",
                "Zhiyue Zhao",
                "Hao Chen",
                "Chunhua Shen"
            ],
            "affiliations": [
                "Zhejiang University of Technology, China",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.14811.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Tinker: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Tinker - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ñ‹. Tinker Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ 3D-Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾-Ğ´Ğ²ÑƒÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€ Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¾Ğ¹ Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing 3D Editing with Minimal Training",
                    "desc": "Tinker is a framework designed for high-quality 3D editing that utilizes pretrained diffusion models to achieve multi-view consistency with minimal training. It allows users to make edits based on just one or two images, eliminating the need for extensive scene-specific optimization. The framework includes innovative components like a referring multi-view editor for coherent edits across different perspectives and an any-view-to-video synthesizer for generating new views and completing scenes. Tinker aims to simplify the process of creating 3D content, making it more accessible and efficient for users."
                },
                "zh": {
                    "title": "Tinkerï¼šç®€åŒ–3Dç¼–è¾‘çš„é©å‘½æ€§æ¡†æ¶",
                    "desc": "Tinkeræ˜¯ä¸€ä¸ªé«˜ä¿çœŸ3Dç¼–è¾‘æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å®ç°å¤šè§†è§’ä¸€è‡´æ€§ï¼Œä¸”åªéœ€æœ€å°‘çš„åœºæ™¯è®­ç»ƒã€‚ä¸ä»¥å¾€éœ€è¦å¤§é‡åœºæ™¯ä¼˜åŒ–çš„æŠ€æœ¯ä¸åŒï¼ŒTinkerèƒ½å¤Ÿä»ä¸€åˆ°ä¸¤å¼ å›¾åƒä¸­ç”Ÿæˆç¨³å¥çš„å¤šè§†è§’ä¸€è‡´ç¼–è¾‘ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºé‡æ–°åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œè§£é”å…¶æ½œåœ¨çš„3Dæ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºé¦–ä¸ªå¤§è§„æ¨¡å¤šè§†è§’ç¼–è¾‘æ•°æ®é›†ï¼ŒTinkeræ˜¾è‘—é™ä½äº†é€šç”¨3Då†…å®¹åˆ›ä½œçš„é—¨æ§›ï¼Œæ¨åŠ¨äº†é›¶-shot 3Dç¼–è¾‘çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.14879",
            "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
            "url": "https://huggingface.co/papers/2508.14879",
            "abstract": "MeshCoder reconstructs complex 3D objects from point clouds into editable Blender Python scripts, enhancing shape-to-code reconstruction and 3D shape understanding through a multimodal large language model.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing 3D objects into editable programs is pivotal for applications like reverse engineering and shape editing. However, existing methods often rely on limited domain-specific languages (DSLs) and small-scale datasets, restricting their ability to model complex geometries and structures. To address these challenges, we introduce MeshCoder, a novel framework that reconstructs complex 3D objects from point clouds into editable Blender Python scripts. We develop a comprehensive set of expressive Blender Python APIs capable of synthesizing intricate geometries. Leveraging these APIs, we construct a large-scale paired object-code dataset, where the code for each object is decomposed into distinct semantic parts. Subsequently, we train a multimodal large language model (LLM) that translates 3D point cloud into executable Blender Python scripts. Our approach not only achieves superior performance in shape-to-code reconstruction tasks but also facilitates intuitive geometric and topological editing through convenient code modifications. Furthermore, our code-based representation enhances the reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these contributions establish MeshCoder as a powerful and flexible solution for programmatic 3D shape reconstruction and understanding.",
            "score": 5,
            "issue_id": 5463,
            "pub_date": "2025-08-20",
            "pub_date_card": {
                "ru": "20 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 20",
                "zh": "8æœˆ20æ—¥"
            },
            "hash": "78801534603cc68a",
            "authors": [
                "Bingquan Dai",
                "Li Ray Luo",
                "Qihong Tang",
                "Jie Wang",
                "Xinyu Lian",
                "Hao Xu",
                "Minghan Qin",
                "Xudong Xu",
                "Bo Dai",
                "Haoqian Wang",
                "Zhaoyang Lyu",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "AI Thrust, HKUST(GZ), Guangzhou, China",
                "Beijing Institute of Technology, Beijing, China",
                "Harbin Institute of Technology, Shenzhen, China",
                "Shanghai Artificial Intelligence Laboratory, Shanghai, China",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.14879.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#synthetic",
                    "#3d",
                    "#reasoning"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğº ĞºĞ¾Ğ´Ñƒ: MeshCoder Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸",
                    "desc": "MeshCoder - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸Ğ· Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Python Ğ´Ğ»Ñ Blender. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM) Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° 3D Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´ Blender Python. MeshCoder Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ² ĞºĞ¾Ğ´ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Transforming 3D Shapes into Editable Code with MeshCoder",
                    "desc": "MeshCoder is a framework that converts complex 3D objects from point clouds into editable Blender Python scripts, improving the process of shape-to-code reconstruction. It addresses limitations of existing methods that use narrow domain-specific languages and small datasets, which hinder the modeling of intricate geometries. By creating a large-scale dataset that pairs 3D objects with their corresponding code, MeshCoder trains a multimodal large language model to effectively translate point clouds into executable scripts. This innovative approach not only enhances performance in reconstruction tasks but also allows for intuitive editing of 3D shapes through code, thereby improving the understanding of 3D structures."
                },
                "zh": {
                    "title": "MeshCoderï¼šå°†3Dç‰©ä½“è½¬åŒ–ä¸ºå¯ç¼–è¾‘ä»£ç çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "MeshCoder æ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œå¯ä»¥å°†å¤æ‚çš„ 3D ç‰©ä½“ä»ç‚¹äº‘é‡å»ºä¸ºå¯ç¼–è¾‘çš„ Blender Python è„šæœ¬ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å¼€å‘ä¸€å¥—å…¨é¢çš„ Blender Python APIï¼Œèƒ½å¤Ÿåˆæˆå¤æ‚çš„å‡ ä½•å½¢çŠ¶ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„é…å¯¹ç‰©ä½“-ä»£ç æ•°æ®é›†ï¼Œä½¿å¾—æ¯ä¸ªç‰©ä½“çš„ä»£ç å¯ä»¥åˆ†è§£ä¸ºä¸åŒçš„è¯­ä¹‰éƒ¨åˆ†ã€‚é€šè¿‡è®­ç»ƒå¤šæ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒMeshCoder åœ¨å½¢çŠ¶åˆ°ä»£ç çš„é‡å»ºä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”é€šè¿‡ä»£ç ä¿®æ”¹å®ç°äº†ç›´è§‚çš„å‡ ä½•å’Œæ‹“æ‰‘ç¼–è¾‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.14160",
            "title": "RynnEC: Bringing MLLMs into Embodied World",
            "url": "https://huggingface.co/papers/2508.14160",
            "abstract": "RynnEC, a video multimodal large language model with a region-centric approach, achieves state-of-the-art performance in object property understanding, segmentation, and spatial reasoning, using an egocentric video pipeline and a region-centered benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce RynnEC, a video multimodal large language model designed for embodied cognition. Built upon a general-purpose vision-language foundation model, RynnEC incorporates a region encoder and a mask decoder, enabling flexible region-level video interaction. Despite its compact architecture, RynnEC achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning. Conceptually, it offers a region-centric video paradigm for the brain of embodied agents, providing fine-grained perception of the physical world and enabling more precise interactions. To mitigate the scarcity of annotated 3D datasets, we propose an egocentric video based pipeline for generating embodied cognition data. Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for evaluating embodied cognitive capabilities. We anticipate that RynnEC will advance the development of general-purpose cognitive cores for embodied agents and facilitate generalization across diverse embodied tasks. The code, model checkpoints, and benchmark are available at: https://github.com/alibaba-damo-academy/RynnEC",
            "score": 3,
            "issue_id": 5463,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 19",
                "zh": "8æœˆ19æ—¥"
            },
            "hash": "258b579ad840bfef",
            "authors": [
                "Ronghao Dang",
                "Yuqian Yuan",
                "Yunxuan Mao",
                "Kehan Li",
                "Jiangpin Liu",
                "Zhikai Wang",
                "Xin Li",
                "Fan Wang",
                "Deli Zhao"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.14160.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#agents",
                    "#video",
                    "#3d",
                    "#games",
                    "#agi",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "RynnEC: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "RynnEC - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñƒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. RynnEC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¼Ğ°ÑĞ¾Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² RynnEC-Bench."
                },
                "en": {
                    "title": "RynnEC: Revolutionizing Embodied Cognition in Video Understanding",
                    "desc": "RynnEC is a video multimodal large language model that focuses on understanding and interacting with objects in a spatial context. It uses a region-centric approach, which allows it to analyze video content at a detailed level, enhancing its ability to understand object properties and perform segmentation tasks. The model is built on a vision-language foundation and employs a region encoder and mask decoder for effective video interaction. By introducing an egocentric video pipeline and a dedicated benchmark, RynnEC aims to improve embodied cognition in AI, making it more adept at navigating and interpreting the physical world."
                },
                "zh": {
                    "title": "RynnECï¼šå…·èº«è®¤çŸ¥çš„æ–°è§†è§’",
                    "desc": "RynnECæ˜¯ä¸€ç§è§†é¢‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨åŒºåŸŸä¸­å¿ƒçš„æ–¹æ³•ï¼Œä¸“æ³¨äºç‰©ä½“å±æ€§ç†è§£ã€åˆ†å‰²å’Œç©ºé—´æ¨ç†ã€‚å®ƒåŸºäºé€šç”¨çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œç»“åˆåŒºåŸŸç¼–ç å™¨å’Œæ©ç è§£ç å™¨ï¼Œå®ç°çµæ´»çš„åŒºåŸŸçº§è§†é¢‘äº¤äº’ã€‚å°½ç®¡æ¶æ„ç´§å‡‘ï¼ŒRynnECåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†å¯¹ç‰©ç†ä¸–ç•Œçš„ç»†è‡´æ„ŸçŸ¥ã€‚è¯¥æ¨¡å‹è¿˜å¼•å…¥äº†åŸºäºè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘çš„æ•°æ®ç”Ÿæˆç®¡é“ï¼Œä»¥è§£å†³æ ‡æ³¨3Dæ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ï¼Œå¹¶æ¨å‡ºäº†RynnEC-BenchåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å…·èº«è®¤çŸ¥èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13491",
            "title": "From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating\n  Financial Large Language Models",
            "url": "https://huggingface.co/papers/2508.13491",
            "abstract": "FinCDM, a cognitive diagnosis framework, evaluates financial LLMs at the knowledge-skill level using a comprehensive dataset, revealing hidden knowledge gaps and supporting more trustworthy model development.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown promise for financial applications, yet their suitability for this high-stakes domain remains largely unproven due to inadequacies in existing benchmarks. Existing benchmarks solely rely on score-level evaluation, summarizing performance with a single score that obscures the nuanced understanding of what models truly know and their precise limitations. They also rely on datasets that cover only a narrow subset of financial concepts, while overlooking other essentials for real-world applications. To address these gaps, we introduce FinCDM, the first cognitive diagnosis evaluation framework tailored for financial LLMs, enabling the evaluation of LLMs at the knowledge-skill level, identifying what financial skills and knowledge they have or lack based on their response patterns across skill-tagged tasks, rather than a single aggregated number. We construct CPA-QKA, the first cognitively informed financial evaluation dataset derived from the Certified Public Accountant (CPA) examination, with comprehensive coverage of real-world accounting and financial skills. It is rigorously annotated by domain experts, who author, validate, and annotate questions with high inter-annotator agreement and fine-grained knowledge labels. Our extensive experiments on 30 proprietary, open-source, and domain-specific LLMs show that FinCDM reveals hidden knowledge gaps, identifies under-tested areas such as tax and regulatory reasoning overlooked by traditional benchmarks, and uncovers behavioral clusters among models. FinCDM introduces a new paradigm for financial LLM evaluation by enabling interpretable, skill-aware diagnosis that supports more trustworthy and targeted model development, and all datasets and evaluation scripts will be publicly released to support further research.",
            "score": 3,
            "issue_id": 5463,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 19",
                "zh": "8æœˆ19æ—¥"
            },
            "hash": "467cc61595abe622",
            "authors": [
                "Ziyan Kuang",
                "Feiyu Zhu",
                "Maowei Jiang",
                "Yanzhao Lai",
                "Zelin Wang",
                "Zhitong Wang",
                "Meikang Qiu",
                "Jiajia Huang",
                "Min Peng",
                "Qianqian Xie",
                "Sophia Ananiadou"
            ],
            "affiliations": [
                "Beijing University of Financial Technology",
                "Center for the Study of Language and Information, Wuhan University",
                "Computer Science, University of Manchester",
                "Computer and Cyber Sciences, Augusta University",
                "School of Artificial Intelligence, Wuhan University",
                "School of Computer Science, Nanjing Audit University",
                "Southwest Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13491.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#interpretability",
                    "#science"
                ],
                "emoji": "ğŸ’¹",
                "ru": {
                    "title": "FinCDM: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "FinCDM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CPA-QKA, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğµ Ğ´Ğ»Ñ ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ÑƒÑ…Ğ³Ğ°Ğ»Ñ‚ĞµÑ€Ğ¾Ğ². FinCDM Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¸ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ñ€Ğ°ÑĞ»Ğ¸."
                },
                "en": {
                    "title": "Unlocking Financial Knowledge Gaps in LLMs with FinCDM",
                    "desc": "FinCDM is a novel cognitive diagnosis framework designed to evaluate financial Large Language Models (LLMs) at a detailed knowledge-skill level. Unlike traditional benchmarks that provide a single performance score, FinCDM analyzes response patterns across skill-tagged tasks to identify specific financial skills and knowledge gaps in models. It utilizes the CPA-QKA dataset, which is meticulously annotated by experts to cover a wide range of real-world financial concepts. This approach not only reveals hidden deficiencies in LLMs but also promotes more reliable and targeted development of financial AI systems."
                },
                "zh": {
                    "title": "é‡‘èæ¨¡å‹è¯„ä¼°çš„æ–°è§†è§’",
                    "desc": "FinCDMæ˜¯ä¸€ä¸ªè®¤çŸ¥è¯Šæ–­æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°é‡‘èé¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œé€šè¿‡çŸ¥è¯†-æŠ€èƒ½å±‚é¢è¿›è¡Œè¯„ä¼°ã€‚å®ƒåˆ©ç”¨CPA-QKAæ•°æ®é›†ï¼Œæ¶µç›–çœŸå®ä¸–ç•Œçš„ä¼šè®¡å’Œé‡‘èæŠ€èƒ½ï¼Œå¸®åŠ©è¯†åˆ«æ¨¡å‹çš„çŸ¥è¯†ç¼ºå£å’Œä¸è¶³ä¹‹å¤„ã€‚ä¸ä¼ ç»Ÿçš„å•ä¸€è¯„åˆ†è¯„ä¼°ä¸åŒï¼ŒFinCDMèƒ½å¤Ÿæ­ç¤ºæ¨¡å‹åœ¨ç‰¹å®šé‡‘èæŠ€èƒ½ä¸Šçš„è¡¨ç°ï¼Œæä¾›æ›´ç»†è‡´çš„åˆ†æã€‚è¯¥æ¡†æ¶çš„æ¨å‡ºä¸ºé‡‘èLLMçš„è¯„ä¼°å¼€è¾Ÿäº†æ–°çš„æ€è·¯ï¼Œä¿ƒè¿›äº†æ›´å¯é çš„æ¨¡å‹å¼€å‘ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-20.html",
    "link_next": "2025-08-22.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "20.08",
        "en": "08/20",
        "zh": "8æœˆ20æ—¥"
    },
    "short_date_next": {
        "ru": "22.08",
        "en": "08/22",
        "zh": "8æœˆ22æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}