{
    "date": {
        "ru": "5 –Ω–æ—è–±—Ä—è",
        "en": "November 5",
        "zh": "11Êúà5Êó•"
    },
    "time_utc": "2024-11-05 04:15",
    "weekday": 1,
    "issue_id": 421,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.00836",
            "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models",
            "url": "https://huggingface.co/papers/2411.00836",
            "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.",
            "score": 8,
            "issue_id": 420,
            "pub_date": "2024-10-29",
            "pub_date_card": {
                "ru": "29 –æ–∫—Ç—è–±—Ä—è",
                "en": "October 29",
                "zh": "10Êúà29Êó•"
            },
            "hash": "e73ae00a5621a2b9",
            "data": {
                "categories": [
                    "#benchmark",
                    "#math",
                    "#cv"
                ],
                "emoji": "üßÆ",
                "ru": {
                    "title": "DynaMath: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DynaMath - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π Vision-Language Models (VLM) –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ VLM, —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-4V, —á–∞—Å—Ç–æ –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –ø—Ä–∏–º–µ–Ω—è—Ç—å —à–∞–≥–∏ —Ä–µ—à–µ–Ω–∏—è –∫ –ø–æ—Ö–æ–∂–∏–º –∑–∞–¥–∞—á–∞–º —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏. DynaMath –≤–∫–ª—é—á–∞–µ—Ç 501 –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å –≤ –≤–∏–¥–µ Python-–ø—Ä–æ–≥—Ä–∞–º–º, –ø–æ–∑–≤–æ–ª—è—é—â–∏—Ö –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞—Ü–∏–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ 14 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö VLM –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∏—Ö —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —Ö—É–¥—à–µ–º —Å–ª—É—á–∞–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ —Å—Ä–µ–¥–Ω–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Enhancing VLMs: Unveiling Mathematical Reasoning Limitations with DynaMath",
                    "desc": "This paper explores the limitations of Vision-Language Models (VLMs) in performing mathematical reasoning tasks that involve visual elements. It highlights that while humans can adapt their problem-solving strategies to slight changes in problems, current state-of-the-art VLMs like GPT-4o struggle with this adaptability. To address this issue, the authors introduce DynaMath, a dynamic benchmark that generates a wide variety of mathematical questions to rigorously test VLMs' reasoning capabilities. The findings reveal that VLMs exhibit significantly lower accuracy in worst-case scenarios compared to average cases, underscoring the importance of evaluating their robustness in mathematical reasoning."
                },
                "zh": {
                    "title": "ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËßÜËßâ‰∏ä‰∏ãÊñáÁöÑÂΩ±Âìç‰∏ã„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°‰∫∫Á±ªËÉΩÂ§üÁÅµÊ¥ªÂ∫îÂØπÁõ∏‰ººÈóÆÈ¢òÁöÑÂèòÂåñÔºåÂΩìÂâçÁöÑÊúÄÂÖàËøõÊ®°ÂûãÂ¶ÇGPT-4oÂú®Èù¢ÂØπËøô‰∫õÂèòÂåñÊó∂Âç¥Ë°®Áé∞‰∏ç‰Ω≥ÔºåÊòæÁ§∫Âá∫ÂÖ∂Êï∞Â≠¶Êé®ÁêÜËÉΩÂäõÁöÑÂ±ÄÈôêÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜDynaMathÔºå‰∏Ä‰∏™Âä®ÊÄÅËßÜËßâÊï∞Â≠¶Âü∫ÂáÜÔºåÊó®Âú®Ê∑±ÂÖ•ËØÑ‰º∞VLMsÁöÑÊé®ÁêÜÁ®≥ÂÅ•ÊÄß„ÄÇÈÄöËøáÂØπ501‰∏™È´òË¥®ÈáèÁßçÂ≠êÈóÆÈ¢òÁöÑËá™Âä®ÁîüÊàêÔºåDynaMathËÉΩÂ§üËØÑ‰º∞Ê®°ÂûãÂú®‰∏çÂêåËæìÂÖ•Êù°‰ª∂‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÁªìÊûúÊòæÁ§∫Ê®°ÂûãÂú®ÊúÄÂùèÊÉÖÂÜµ‰∏ãÁöÑÂáÜÁ°ÆÁéáÊòæËëó‰Ωé‰∫éÂπ≥ÂùáÊÉÖÂÜµ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02385",
            "title": "How Far is Video Generation from World Model: A Physical Law Perspective",
            "url": "https://huggingface.co/papers/2411.02385",
            "abstract": "OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit \"case-based\" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color > size > velocity > shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora's broader success. See our project page at https://phyworld.github.io",
            "score": 3,
            "issue_id": 421,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 –Ω–æ—è–±—Ä—è",
                "en": "November 4",
                "zh": "11Êúà4Êó•"
            },
            "hash": "771395452deb397f",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#reasoning"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–∏ –Ω–µ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω—ã –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑—É—á–∞—Ç—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω—ã –∏–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ 2D —Å–∏–º—É–ª—è—Ü–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ, —É–ø—Ä–∞–≤–ª—è–µ–º—ã—Ö –∑–∞–∫–æ–Ω–∞–º–∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –º–µ—Ö–∞–Ω–∏–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –∏–¥–µ–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ —Ä–∞–º–∫–∞—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –Ω–æ –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–µ–π –Ω–∞ –Ω–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –Ω–µ –∞–±—Å—Ç—Ä–∞–≥–∏—Ä—É—é—Ç –æ–±—â–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞, –∞ —Å–∫–æ—Ä–µ–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤."
                },
                "en": {
                    "title": "Unlocking Video Generation: Beyond Scaling to Understand Physics",
                    "desc": "This paper investigates the ability of video generation models to learn and predict physical laws from visual data without human input. The authors created a 2D simulation environment to generate videos based on classical mechanics, allowing for extensive testing of model performance. They found that while the models excelled in familiar scenarios, they struggled with new, unseen situations, indicating a reliance on specific examples rather than generalizing physical principles. The study concludes that simply increasing model size is not enough for these systems to grasp fundamental laws of physics, highlighting the need for improved generalization techniques."
                },
                "zh": {
                    "title": "ËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏éÁâ©ÁêÜÊ≥ïÂàôÁöÑÊé¢Á¥¢",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®Â≠¶‰π†Áâ©ÁêÜÊ≥ïÂàôÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™‰∫åÁª¥Ê®°ÊãüÊµãËØïÂπ≥Âè∞ÔºåÁî®‰∫éÁîüÊàêÂèóÁªèÂÖ∏ÂäõÂ≠¶Ê≥ïÂàôÊîØÈÖçÁöÑËßÜÈ¢ëÊï∞ÊçÆ„ÄÇÈÄöËøáÂØπÊ®°ÂûãÂú®‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑË°®Áé∞ËøõË°åËØÑ‰º∞ÔºåÊàë‰ª¨ÂèëÁé∞Ê®°ÂûãÂú®Â∑≤Áü•ÂàÜÂ∏ÉÂÜÖË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Êú™Áü•ÂàÜÂ∏É‰∏≠ÂàôÂá∫Áé∞Â§±Ë¥•„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊ®°ÂûãÂú®Êé®ÂπøÊñ∞Ê°à‰æãÊó∂Ôºå‰ºòÂÖàËÄÉËôëÁöÑÂõ†Á¥†‰æùÊ¨°‰∏∫È¢úËâ≤„ÄÅÂ§ßÂ∞è„ÄÅÈÄüÂ∫¶ÂíåÂΩ¢Áä∂ÔºåËÄå‰∏çÊòØÊäΩË±°Âá∫‰∏ÄËà¨ÁöÑÁâ©ÁêÜËßÑÂàô„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00918",
            "title": "LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models",
            "url": "https://huggingface.co/papers/2411.00918",
            "abstract": "Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops LibMoE, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms. Built upon three core principles: (i) modular design, (ii) efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs more accessible to a wide range of researchers by standardizing the training and evaluation pipelines. Using LibMoE, we extensively benchmarked five state-of-the-art MoE algorithms over three different LLMs and 11 datasets under the zero-shot setting. The results show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks. With the modular design and extensive evaluation, we believe LibMoE will be invaluable for researchers to make meaningful progress towards the next generation of MoE and LLMs. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.",
            "score": 3,
            "issue_id": 420,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 –Ω–æ—è–±—Ä—è",
                "en": "November 1",
                "zh": "11Êúà1Êó•"
            },
            "hash": "a406640433a3de34",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "LibMoE: –î–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π Mixture of Experts –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LibMoE - –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –º–æ–¥—É–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ Mixture of Experts (MoE) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). LibMoE –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ç—Ä–µ—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö: –º–æ–¥—É–ª—å–Ω—ã–π –¥–∏–∑–∞–π–Ω, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ—Ü–µ–Ω–∫–∞. –ò—Å–ø–æ–ª—å–∑—É—è LibMoE, –∞–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—è—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ MoE –Ω–∞ —Ç—Ä–µ—Ö —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –∏ 11 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–∂–∏–º–µ zero-shot. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –≤—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã MoE –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–∏ –ø–æ —à–∏—Ä–æ–∫–æ–º—É —Å–ø–µ–∫—Ç—Ä—É –∑–∞–¥–∞—á."
                },
                "en": {
                    "title": "LibMoE: Streamlining Mixture of Experts for Large Language Models",
                    "desc": "This paper introduces LibMoE, a framework designed to facilitate research on Mixture of Experts (MoE) algorithms for large language models (LLMs). It emphasizes a modular design, efficient training processes, and comprehensive evaluation methods to make MoE more accessible to researchers. The authors benchmark five leading MoE algorithms across three LLMs and 11 datasets, revealing that their performance is generally similar across various tasks. LibMoE aims to standardize the training and evaluation of MoE, helping researchers advance the development of future LLMs."
                },
                "zh": {
                    "title": "LibMoEÔºöËÆ©Ê∑∑Âêà‰∏ìÂÆ∂ÁÆóÊ≥ïÊõ¥Êòì‰∫éÁ†îÁ©∂ÂíåÂ∫îÁî®",
                    "desc": "Ê∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÈ´òÊïàÂíåÊúâÊïàÂèëÂ±ï‰∏≠ÊâÆÊºîÁùÄÈáçË¶ÅËßíËâ≤„ÄÇÁî±‰∫éËµÑÊ∫êÈúÄÊ±ÇÂ∑®Â§ßÔºåËÆ∏Â§öÁ†îÁ©∂ËÄÖÈöæ‰ª•Á†îÁ©∂Â§ßËßÑÊ®°ÁöÑMoEÁÆóÊ≥ï„ÄÇÊú¨ÊñáÂºÄÂèë‰∫ÜLibMoEÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢‰∏îÊ®°ÂùóÂåñÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÁÆÄÂåñMoEÁÆóÊ≥ïÁöÑÁ†îÁ©∂„ÄÅËÆ≠ÁªÉÂíåËØÑ‰º∞„ÄÇÈÄöËøáÊ®°ÂùóÂåñËÆæËÆ°„ÄÅÈ´òÊïàËÆ≠ÁªÉÂíåÂÖ®Èù¢ËØÑ‰º∞ÔºåLibMoE‰ΩøÂæóMoEÂú®LLMs‰∏≠ÁöÑÂ∫îÁî®ÂØπÊõ¥Â§öÁ†îÁ©∂ËÄÖÂèòÂæóÂèØÂèä„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02397",
            "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
            "url": "https://huggingface.co/papers/2411.02397",
            "abstract": "Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs) -- despite making significant headway in this context -- have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a training-free method to accelerate video DiTs, termed Adaptive Caching (AdaCache), which is motivated by the fact that \"not all videos are created equal\": meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization (MoReg) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines.",
            "score": 2,
            "issue_id": 421,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 –Ω–æ—è–±—Ä—è",
                "en": "November 4",
                "zh": "11Êúà4Êó•"
            },
            "hash": "d7fa22d791789900",
            "data": {
                "categories": [
                    "#video",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "üöÄ",
                "ru": {
                    "title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ AdaCache –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ (DiT) –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. AdaCache –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –∫—ç—à–∏—Ä—É–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏, —É—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ —Ä–∞–∑–Ω—ã–º –≤–∏–¥–µ–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è. –¢–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ö–µ–º–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è (MoReg) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ (–¥–æ 4.7 —Ä–∞–∑ –Ω–∞ Open-Sora 720p) –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞."
                },
                "en": {
                    "title": "Accelerating Video Generation with Adaptive Caching!",
                    "desc": "This paper presents a method called Adaptive Caching (AdaCache) to improve the speed of video generation using Diffusion Transformers (DiTs). The authors argue that different videos have varying requirements for denoising steps, allowing for a more efficient computation process. By caching computations and creating a tailored caching schedule for each video, they enhance the balance between quality and speed. Additionally, they introduce a Motion Regularization (MoReg) technique to optimize resource allocation based on the motion content of the video, achieving significant speed improvements without compromising quality."
                },
                "zh": {
                    "title": "Âä†ÈÄüËßÜÈ¢ëÁîüÊàêÔºåÊèêÂçáË¥®Èáè‰∏éÊïàÁéáÔºÅ",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ëá™ÈÄÇÂ∫îÁºìÂ≠òÔºàAdaCacheÔºâÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂä†ÈÄüËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊâ©Êï£ÂèòÊç¢Âô®ÔºàDiTsÔºâ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÁºìÂ≠òËÆ°ÁÆóËøáÁ®ãÔºåÈíàÂØπÊØè‰∏™ËßÜÈ¢ëÁîüÊàêÂà∂ÂÆöÁºìÂ≠òËÆ°ÂàíÔºå‰ªéËÄå‰ºòÂåñË¥®Èáè‰∏éÂª∂ËøüÁöÑÂπ≥Ë°°„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜËøêÂä®Ê≠£ÂàôÂåñÔºàMoRegÔºâÊñπÊ°àÔºåÊ†πÊçÆËßÜÈ¢ë‰∏≠ÁöÑËøêÂä®ÂÜÖÂÆπÊù•ÊéßÂà∂ËÆ°ÁÆóÂàÜÈÖç„ÄÇÊï¥‰ΩìËÄåË®ÄÔºåËøô‰∫õÂàõÊñ∞ÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÈÄüÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÁîüÊàêË¥®Èáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00743",
            "title": "Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models",
            "url": "https://huggingface.co/papers/2411.00743",
            "abstract": "Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. We introduce Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. We present a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. Our evaluation of SSAEs on standard metrics, such as downstream perplexity and L_0 sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. We showcase the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5\\% increase in worst-group classification accuracy when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains.",
            "score": 0,
            "issue_id": 421,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 –Ω–æ—è–±—Ä—è",
                "en": "November 1",
                "zh": "11Êúà1Êó•"
            },
            "hash": "af234e3c99f935ec",
            "data": {
                "categories": [
                    "#interpretability",
                    "#dataset",
                    "#training"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Å–∫—Ä—ã—Ç—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã (SSAE). SSAE —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø–æ–¥–¥–æ–º–µ–Ω–∞—Ö –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—è–≤–ª—è—é—Ç —Ä–µ–¥–∫–∏–µ, –Ω–æ –≤–∞–∂–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤ –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–ª–æ—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Tilted Empirical Risk Minimization –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å SSAE –ø–æ–∫–∞–∑–∞–Ω–∞ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫–∞—Ö –∏ –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö Bias in Bios."
                },
                "en": {
                    "title": "Illuminating Hidden Concepts in Foundation Models with SSAEs",
                    "desc": "This paper discusses the challenges of understanding and mitigating risks in foundation models (FMs) through effective interpretability methods. It introduces Specialized Sparse Autoencoders (SSAEs), which are designed to better identify rare but important concepts in data by focusing on specific subdomains. The authors provide a training approach that utilizes dense retrieval for data selection and Tilted Empirical Risk Minimization to enhance concept recall. The results show that SSAEs outperform traditional Sparse Autoencoders in capturing these critical features, as demonstrated in a case study that improved classification accuracy by addressing bias in gender information."
                },
                "zh": {
                    "title": "‰∏ìÊ≥®Â≠êÈ¢ÜÂüüÁöÑÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºöÊè≠Á§∫Âü∫Á°ÄÊ®°ÂûãÁöÑÊΩúÂú®ÁâπÂæÅ",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂü∫Á°ÄÊ®°ÂûãÔºàFMsÔºâÊΩúÂú®È£éÈô©ÁöÑÁêÜËß£‰∏éÁºìËß£ÔºåÂº∫Ë∞É‰∫ÜÊúâÊïàÁöÑÂèØËß£ÈáäÊÄßÊñπÊ≥ïÁöÑÈáçË¶ÅÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSSAEsÔºâÔºåÊó®Âú®Êè≠Á§∫Êï∞ÊçÆ‰∏≠Á®ÄÊúâ‰ΩÜÈáçË¶ÅÁöÑÊ¶ÇÂøµÔºåÁâπÂà´ÂÖ≥Ê≥®ÁâπÂÆöÂ≠êÈ¢ÜÂüü„ÄÇÈÄöËøáÂØÜÈõÜÊ£ÄÁ¥¢ÂíåÂÄæÊñúÁªèÈ™åÈ£éÈô©ÊúÄÂ∞èÂåñÁ≠âÊñπÊ≥ïÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜSSAEsÂú®ÊçïÊçâÂ≠êÈ¢ÜÂüüÂ∞æÈÉ®Ê¶ÇÂøµÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇÊ°à‰æãÁ†îÁ©∂Ë°®ÊòéÔºåSSAEsÂú®ÂéªÈô§ËôöÂÅáÊÄßÂà´‰ø°ÊÅØÊó∂ÔºåÂàÜÁ±ªÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü12.5%„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2024-11-04.html",
    "link_next": "2024-11-06.html",
    "short_date_prev": {
        "ru": "04.11",
        "en": "11/04",
        "zh": "11Êúà4Êó•"
    },
    "short_date_next": {
        "ru": "06.11",
        "en": "11/06",
        "zh": "11Êúà6Êó•"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#medicine": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÊûÑÂª∫ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºàGUIÔºâ‰ª£ÁêÜÁöÑÁé∞ÊúâÂä™Âäõ„ÄÇÁõÆÂâç‰∏ªË¶Å‰æùËµñ‰∫éÂº∫Â§ßÁöÑÂïÜ‰∏öËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÔºåÂ¶ÇGPT-4oÂíåGeminiProVision„ÄÇÂºÄÊ∫êVLMsÂú®GUIÁêÜËß£ÂíåÊú™ËßÅÂàÜÂ∏ÉÔºàOODÔºâÂú∫ÊôØ‰∏≠Ë°®Áé∞ËæÉÂ∑ÆÔºåÂõ†Ê≠§ÂÆûË∑µËÄÖ‰∏çÂ§™ÊÑøÊÑè‰ΩøÁî®„ÄÇ‰∏∫‰∫ÜÊé®Âä®Ëøô‰∏ÄÈ¢ÜÂüüÁöÑÁ†îÁ©∂Ôºå‰ΩúËÄÖÂºÄÂèë‰∫ÜOS-AtlasÔºåËøôÊòØ‰∏Ä‰∏™ÊìÖÈïøGUIÁêÜËß£ÂíåOOD‰ªªÂä°ÁöÑÂü∫Á°ÄÊ®°Âûã„ÄÇ‰ªñ‰ª¨ËøòÂèëÂ∏É‰∫Ü‰∏Ä‰∏™ÂåÖÂê´1300Â§ö‰∏áGUIÂÖÉÁ¥†ÁöÑË∑®Âπ≥Âè∞Êï∞ÊçÆÈõÜÔºåÂπ∂ËØÅÊòé‰∫ÜOS-AtlasÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÁöÑÊòæËëóÊÄßËÉΩÊèêÂçá„ÄÇ",
        "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
        "pinyin": "Zh√® piƒÅn w√©nzhƒÅng t«éol√πn le g√≤uji√†n t√∫x√≠ng y√≤ngh√π jiƒìmi√†n (GUI) d√†il«ê de xi√†n y«íu n«îl√¨. DƒÅngqi√°n zh«îy√†o yƒ´l√†i z√†i qi√°ngd√† de shƒÅngy√® sh√¨ju√© y«îy√°n m√≥x√≠ng (VLMs) sh√†ng, r√∫ GPT-4o h√© GeminiProVision. KƒÅiyu√°n VLMs z√†i GUI l«êjiƒõ h√© w√®i ji√†n fƒìnb√π (OOD) ch«éngj«êng zh≈çng bi«éoxi«én ji√†och«é, yƒ´nc«ê sh√≠ji√†nzhƒõ b√π t√†i yu√†ny√¨ sh«êy√≤ng. W√®ile tuƒ´d√≤ng zh√® yƒ´ l«êngy√π de y√°nji≈´, zu√≤zhƒõ kƒÅifƒÅle OS-Atlas, zh√® sh√¨ yƒ´g√® sh√†nch√°ng GUI l«êjiƒõ h√© OOD r√®nw√π de jƒ´ch«î m√≥x√≠ng. TƒÅmen h√°i fƒÅb√πle yƒ´g√® bƒÅoh√°n 1300 du≈ç w√†n GUI yu√°ns√π de ku√† p√≠ngt√°i sh√πj√πj√≠, b√¨ng zh√®ngm√≠ngle OS-Atlas z√†i du≈çg√® jƒ´zh«în c√®sh√¨ zh≈çng de xi«énzh√π xi√†on√©ng t«êshƒìng.",
        "vocab": "[\n    {\"word\": \"ÊûÑÂª∫\", \"pinyin\": \"g√≤u ji√†n\", \"trans\": \"construct\"},\n    {\"word\": \"ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢\", \"pinyin\": \"t√∫ x√≠ng y√≤ng h√π ji√® mi√†n\", \"trans\": \"graphical user interface\"},\n    {\"word\": \"‰ª£ÁêÜ\", \"pinyin\": \"d√†i l«ê\", \"trans\": \"agent\"},\n    {\"word\": \"Áé∞Êúâ\", \"pinyin\": \"xi√†n y«íu\", \"trans\": \"existing\"},\n    {\"word\": \"Âä™Âäõ\", \"pinyin\": \"n«î l√¨\", \"trans\": \"efforts\"},\n    {\"word\": \"‰æùËµñ\", \"pinyin\": \"yƒ´ l√†i\", \"trans\": \"rely on\"},\n    {\"word\": \"Âº∫Â§ß\", \"pinyin\": \"qi√°ng d√†\", \"trans\": \"powerful\"},\n    {\"word\": \"ÂïÜ‰∏ö\", \"pinyin\": \"shƒÅng y√®\", \"trans\": \"commercial\"},\n    {\"word\": \"ËßÜËßâËØ≠Ë®ÄÊ®°Âûã\", \"pinyin\": \"sh√¨ ju√© y«î y√°n m√≥ x√≠ng\", \"trans\": \"visual language model\"},\n    {\"word\": \"ÂºÄÊ∫ê\", \"pinyin\": \"kƒÅi yu√°n\", \"trans\": \"open-source\"},\n    {\"word\": \"ÁêÜËß£\", \"pinyin\": \"l«ê jiƒõ\", \"trans\": \"understanding\"},\n    {\"word\": \"Êú™ËßÅÂàÜÂ∏É\", \"pinyin\": \"w√®i ji√†n fƒìn b√π\", \"trans\": \"out-of-distribution\"},\n    {\"word\": \"Âú∫ÊôØ\", \"pinyin\": \"ch«éng j«êng\", \"trans\": \"scenario\"},\n    {\"word\": \"Ë°®Áé∞\", \"pinyin\": \"bi«éo xi√†n\", \"trans\": \"performance\"},\n    {\"word\": \"ËæÉÂ∑Æ\", \"pinyin\": \"ji√†o ch√†\", \"trans\": \"poor\"},\n    {\"word\": \"ÂÆûË∑µËÄÖ\", \"pinyin\": \"sh√≠ ji√†n zhƒõ\", \"trans\": \"practitioner\"},\n    {\"word\": \"‰∏çÂ§™ÊÑøÊÑè\", \"pinyin\": \"b√π t√†i yu√†n y√¨\", \"trans\": \"not very willing\"},\n    {\"word\": \"Êé®Âä®\", \"pinyin\": \"tuƒ´ d√≤ng\", \"trans\": \"promote\"},\n    {\"word\": \"È¢ÜÂüü\", \"pinyin\": \"l«êng y√π\", \"trans\": \"field\"},\n    {\"word\": \"Á†îÁ©∂\", \"pinyin\": \"y√°n ji≈´\", \"trans\": \"research\"},\n    {\"word\": \"ÂºÄÂèë\", \"pinyin\": \"kƒÅi fƒÅ\", \"trans\": \"develop\"},\n    {\"word\": \"Âü∫Á°ÄÊ®°Âûã\", \"pinyin\": \"jƒ´ ch«î m√≥ x√≠ng\", \"trans\": \"foundation model\"},\n    {\"word\": \"ÊìÖÈïø\", \"pinyin\": \"sh√†n ch√°ng\", \"trans\": \"proficient\"},\n    {\"word\": \"‰ªªÂä°\", \"pinyin\": \"r√®n w√π\", \"trans\": \"task\"},\n    {\"word\": \"ÂèëÂ∏É\", \"pinyin\": \"fƒÅ b√π\", \"trans\": \"release\"},\n    {\"word\": \"Êï∞ÊçÆÈõÜ\", \"pinyin\": \"sh√π j√π j√≠\", \"trans\": \"dataset\"},\n    {\"word\": \"Ë∑®Âπ≥Âè∞\", \"pinyin\": \"ku√† p√≠ng t√°i\", \"trans\": \"cross-platform\"},\n    {\"word\": \"ÂÖÉÁ¥†\", \"pinyin\": \"yu√°n s√π\", \"trans\": \"element\"},\n    {\"word\": \"ËØÅÊòé\", \"pinyin\": \"zh√®ng m√≠ng\", \"trans\": \"demonstrate\"},\n    {\"word\": \"Âü∫ÂáÜÊµãËØï\", \"pinyin\": \"jƒ´ zh«în c√® sh√¨\", \"trans\": \"benchmark test\"},\n    {\"word\": \"ÊòæËëó\", \"pinyin\": \"xi«én zh√π\", \"trans\": \"significant\"},\n    {\"word\": \"ÊÄßËÉΩ\", \"pinyin\": \"x√¨ng n√©ng\", \"trans\": \"performance\"},\n    {\"word\": \"ÊèêÂçá\", \"pinyin\": \"t√≠ shƒìng\", \"trans\": \"improvement\"}\n]",
        "trans": "This article discusses current efforts in building graphical user interface (GUI) agents. Currently, these efforts primarily rely on powerful commercial vision language models (VLMs) such as GPT-4o and GeminiProVision. Open-source VLMs perform poorly in GUI understanding and out-of-distribution (OOD) scenarios, making practitioners reluctant to use them. To advance research in this field, the authors developed OS-Atlas, a foundational model adept at GUI understanding and OOD tasks. They also released a cross-platform dataset containing over 13 million GUI elements and demonstrated significant performance improvements of OS-Atlas across multiple benchmark tests.",
        "update_ts": "2024-11-04 10:14"
    }
}