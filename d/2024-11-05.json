{
    "date": {
        "ru": "5 ноября",
        "en": "November 5",
        "zh": "11月5日"
    },
    "time_utc": "2024-11-05 07:18",
    "weekday": 1,
    "issue_id": 423,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.24024",
            "title": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents",
            "url": "https://huggingface.co/papers/2410.24024",
            "abstract": "Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been recently a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and closed-source models. In this work, we propose AndroidLab as a systematic Android agent framework. It includes an operation environment with different modalities, action space, and a reproducible benchmark. It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space. AndroidLab benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. By using the AndroidLab environment, we develop an Android Instruction dataset and train six open-source LLMs and LMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at https://github.com/THUDM/Android-Lab.",
            "score": 26,
            "issue_id": 422,
            "pub_date": "2024-10-31",
            "pub_date_card": {
                "ru": "31 октября",
                "en": "October 31",
                "zh": "10月31日"
            },
            "hash": "4ba16ad433c7511f",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "AndroidLab: Революция в обучении Android-агентов",
                    "desc": "Статья представляет AndroidLab - систематическую среду для разработки и оценки агентов на базе Android. Эта среда включает в себя операционное окружение с различными модальностями, пространством действий и воспроизводимым эталоном. AndroidLab поддерживает как большие языковые модели (LLM), так и мультимодальные модели (LMM) в одном пространстве действий. С помощью AndroidLab авторы разработали набор данных Android Instruction и обучили шесть моделей с открытым исходным кодом, значительно повысив их эффективность."
                },
                "en": {
                    "title": "Empowering Android Agents with AndroidLab: A New Benchmark Framework",
                    "desc": "This paper introduces AndroidLab, a comprehensive framework designed for training and evaluating Android agents. It provides a structured environment that includes various modalities and a defined action space, allowing for systematic testing of both open-source and closed-source models. The framework supports large language models (LLMs) and multimodal models (LMMs), facilitating their development and benchmarking across 138 tasks in nine different applications. By utilizing AndroidLab, the authors significantly improved the success rates of LLMs and LMMs, demonstrating its effectiveness in enhancing the performance of Android agents."
                },
                "zh": {
                    "title": "AndroidLab：提升安卓代理的训练与评估",
                    "desc": "自主代理在与现实世界互动中变得越来越重要，尤其是安卓代理。现有的安卓代理训练和评估研究缺乏系统性，无法全面比较开源和闭源模型。我们提出了AndroidLab，这是一个系统化的安卓代理框架，支持多种操作环境和动作空间。通过AndroidLab，我们开发了安卓指令数据集，并显著提高了大语言模型和多模态模型的成功率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02337",
            "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning",
            "url": "https://huggingface.co/papers/2411.02337",
            "abstract": "Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents heavily rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. This paper introduces WebRL, a self-evolving online curriculum reinforcement learning framework designed to train high-performance web agents using open LLMs. WebRL addresses three key challenges in building LLM web agents, including the scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4 models into proficient web agents. On WebArena-Lite, WebRL improves the success rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B. These open models significantly surpass the performance of GPT-4-Turbo (17.6%) and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's effectiveness in bridging the gap between open and proprietary LLM-based web agents, paving the way for more accessible and powerful autonomous web interaction systems.",
            "score": 13,
            "issue_id": 422,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 ноября",
                "en": "November 4",
                "zh": "11月4日"
            },
            "hash": "a9af7d20e52c1f39",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "WebRL: Открытые языковые модели превосходят проприетарные в веб-задачах",
                    "desc": "Статья представляет WebRL - фреймворк обучения с подкреплением для создания веб-агентов на основе открытых языковых моделей. WebRL решает проблемы нехватки обучающих задач, разреженной обратной связи и смещения распределения политики в онлайн-обучении. Фреймворк включает самоэволюционирующий учебный план, модель вознаграждения на основе результатов и адаптивные стратегии обучения с подкреплением. Применение WebRL значительно улучшило производительность открытых моделей Llama-3.1 и GLM-4 в задачах веб-взаимодействия, превзойдя даже проприетарные модели GPT-4."
                },
                "en": {
                    "title": "Empowering Open LLMs for Superior Web Performance with WebRL",
                    "desc": "This paper presents WebRL, a novel framework that enhances open large language models (LLMs) for web-based tasks through reinforcement learning. It tackles challenges such as limited training tasks, sparse feedback, and policy drift by implementing a self-evolving curriculum that generates new tasks from failures, a robust outcome-supervised reward model, and adaptive learning strategies. The framework significantly improves the performance of open models like Llama-3.1 and GLM-4, achieving success rates that surpass proprietary models like GPT-4-Turbo. Overall, WebRL demonstrates a promising approach to making powerful web agents more accessible by leveraging open-source LLMs."
                },
                "zh": {
                    "title": "WebRL：开放LLM的自我进化网络代理训练框架",
                    "desc": "大型语言模型（LLMs）在网络任务中展现了出色的潜力，但现有的LLM网络代理依赖昂贵的专有API，而开放的LLM缺乏决策能力。本文提出了WebRL，一个自我进化的在线课程强化学习框架，旨在利用开放的LLM训练高性能的网络代理。WebRL解决了构建LLM网络代理的三个关键挑战，包括训练任务稀缺、反馈信号稀疏和在线学习中的策略分布漂移。通过WebRL，我们将开放的Llama-3.1和GLM-4模型转变为高效的网络代理，显著提高了它们的成功率，超越了现有的专有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00836",
            "title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models",
            "url": "https://huggingface.co/papers/2411.00836",
            "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.",
            "score": 9,
            "issue_id": 420,
            "pub_date": "2024-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "e73ae00a5621a2b9",
            "data": {
                "categories": [
                    "#benchmark",
                    "#math",
                    "#cv"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "DynaMath: Новый подход к оценке математических способностей ИИ",
                    "desc": "Статья представляет DynaMath - динамический визуальный математический бенчмарк для оценки робастности рассуждений Vision-Language Models (VLM) в задачах математики. Исследователи обнаружили, что современные VLM, такие как GPT-4V, часто не способны применять шаги решения к похожим задачам с небольшими изменениями. DynaMath включает 501 исходный вопрос в виде Python-программ, позволяющих генерировать множество вариаций для тестирования обобщающей способности моделей. Результаты оценки 14 современных VLM показали, что их точность в худшем случае значительно ниже средней точности."
                },
                "en": {
                    "title": "Enhancing VLMs: Unveiling Mathematical Reasoning Limitations with DynaMath",
                    "desc": "This paper explores the limitations of Vision-Language Models (VLMs) in performing mathematical reasoning tasks that involve visual elements. It highlights that while humans can adapt their problem-solving strategies to slight changes in problems, current state-of-the-art VLMs like GPT-4o struggle with this adaptability. To address this issue, the authors introduce DynaMath, a dynamic benchmark that generates a wide variety of mathematical questions to rigorously test VLMs' reasoning capabilities. The findings reveal that VLMs exhibit significantly lower accuracy in worst-case scenarios compared to average cases, underscoring the importance of evaluating their robustness in mathematical reasoning."
                },
                "zh": {
                    "title": "提升视觉语言模型的数学推理能力",
                    "desc": "本文探讨了视觉语言模型（VLMs）在数学推理任务中的表现，尤其是在视觉上下文的影响下。研究发现，尽管人类能够灵活应对相似问题的变化，当前的最先进模型如GPT-4o在面对这些变化时却表现不佳，显示出其数学推理能力的局限性。为了解决这一问题，本文提出了DynaMath，一个动态视觉数学基准，旨在深入评估VLMs的推理稳健性。通过对501个高质量种子问题的自动生成，DynaMath能够评估模型在不同输入条件下的泛化能力，结果显示模型在最坏情况下的准确率显著低于平均情况。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02395",
            "title": "Training-free Regional Prompting for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2411.02395",
            "abstract": "Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text prompts, especially when the text prompts contain various objects with numerous attributes and interrelated spatial relationships. While many regional prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but there are still no implementations based on the recent Diffusion Transformer (DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. Code is available at https://github.com/antonioo-c/Regional-Prompting-FLUX.",
            "score": 7,
            "issue_id": 423,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 ноября",
                "en": "November 4",
                "zh": "11月4日"
            },
            "hash": "2a0401bfd2cb136b",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#long_context"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Точная генерация изображений по сложным текстам: новый метод для DiT",
                    "desc": "Статья представляет новый метод регионального промптинга для архитектуры Diffusion Transformer (DiT), применимый к моделям FLUX.1. Этот подход позволяет улучшить генерацию изображений по сложным текстовым описаниям без дополнительного обучения модели. Авторы реализовали метод манипуляции вниманием, что позволяет DiT более точно следовать детальным композиционным промптам. Работа направлена на преодоление ограничений существующих моделей в обработке длинных текстовых описаний с множеством объектов и пространственных отношений."
                },
                "en": {
                    "title": "Enhancing Text-to-Image Generation with Regional Prompting in Diffusion Transformers",
                    "desc": "This paper discusses the limitations of current diffusion models in generating images from long and complex text prompts. It highlights the challenges these models face when dealing with multiple objects and their attributes in a spatial context. The authors propose a new method called regional prompting for the Diffusion Transformer (DiT) architecture, specifically for the FLUX.1 model, which enhances its ability to generate images based on detailed text descriptions without requiring additional training. This approach utilizes attention manipulation to achieve fine-grained compositional generation."
                },
                "zh": {
                    "title": "区域提示提升扩散模型的文本到图像生成能力",
                    "desc": "扩散模型在文本到图像生成方面表现出色，尤其是在理解语义方面得到了很大提升。尽管已有许多区域提示方法被提出，但现有模型仍无法完美处理长且复杂的文本提示。本文提出了一种基于注意力操作的区域提示方法，专门针对FLUX.1架构进行实现。该方法使得扩散变换器能够在无需训练的情况下，实现细粒度的组合文本到图像生成能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02335",
            "title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity",
            "url": "https://huggingface.co/papers/2411.02335",
            "abstract": "Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-p% sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., 1-sparsity ratio) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable.",
            "score": 4,
            "issue_id": 422,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 ноября",
                "en": "November 4",
                "zh": "11月4日"
            },
            "hash": "3c9152f4d267fc7b",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#interpretability"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Разреженность активаций в LLM: ключ к эффективности и интерпретируемости",
                    "desc": "Статья исследует разреженность активаций в больших языковых моделях (LLM). Авторы предлагают новую метрику PPL-p% для измерения разреженности активаций. Исследование показывает, что различные функции активации демонстрируют противоположные тренды разреженности во время обучения. Обнаружено, что соотношение ширины и глубины модели влияет на разреженность активаций, а масштаб параметров модели оказывает слабое влияние."
                },
                "en": {
                    "title": "Unlocking Efficiency: The Power of Activation Sparsity in LLMs",
                    "desc": "This paper investigates activation sparsity in decoder-only Transformer-based large language models (LLMs), focusing on how weakly-contributed activation outputs can be reduced for efficiency. The authors introduce a new metric called PPL-p% sparsity to quantitatively measure activation sparsity across different activation functions. Their experiments reveal that while ReLU and SiLU functions perform similarly, they exhibit opposite trends in training-time sparsity, with ReLU being more efficient. Additionally, the study finds that activation sparsity is largely unaffected by the parameter scale, suggesting that deeper architectures may enhance efficiency without requiring more parameters."
                },
                "zh": {
                    "title": "激活稀疏性：提升大型语言模型效率的关键",
                    "desc": "激活稀疏性指的是在激活输出中存在大量贡献较弱的元素，这些元素可以被消除，从而对大型语言模型（LLMs）的许多重要应用有益。本文对基于解码器的Transformer LLM中的激活稀疏性进行了全面的定量研究，提出了一种新的激活稀疏性度量标准PPL-p%稀疏性，适用于任何激活函数。研究发现，不同的激活函数在性能上相似，但在训练时间的稀疏性趋势上却相反，ReLU激活函数在利用更多训练数据方面更为高效。最后，研究表明，激活稀疏性的极限值与参数规模的变化关系不大，这为提高LLMs的效率和可解释性提供了重要的启示。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02385",
            "title": "How Far is Video Generation from World Model: A Physical Law Perspective",
            "url": "https://huggingface.co/papers/2411.02385",
            "abstract": "OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit \"case-based\" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color > size > velocity > shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora's broader success. See our project page at https://phyworld.github.io",
            "score": 4,
            "issue_id": 421,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 ноября",
                "en": "November 4",
                "zh": "11月4日"
            },
            "hash": "771395452deb397f",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#reasoning"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Генеративные видеомодели не раскрывают физические законы при масштабировании",
                    "desc": "Исследование оценивает способность моделей генерации видео изучать фундаментальные физические законы из визуальных данных. Авторы разработали 2D симуляцию для создания видео, управляемых законами классической механики. Эксперименты показали, что модели достигают идеальной генерализации в рамках распределения, но не справляются с экстраполяцией на новые сценарии. Результаты указывают на то, что модели не абстрагируют общие физические правила, а скорее демонстрируют обобщение на основе конкретных примеров."
                },
                "en": {
                    "title": "Unlocking Video Generation: Beyond Scaling to Understand Physics",
                    "desc": "This paper investigates the ability of video generation models to learn and predict physical laws from visual data without human input. The authors created a 2D simulation environment to generate videos based on classical mechanics, allowing for extensive testing of model performance. They found that while the models excelled in familiar scenarios, they struggled with new, unseen situations, indicating a reliance on specific examples rather than generalizing physical principles. The study concludes that simply increasing model size is not enough for these systems to grasp fundamental laws of physics, highlighting the need for improved generalization techniques."
                },
                "zh": {
                    "title": "视频生成模型与物理法则的探索",
                    "desc": "本文探讨了视频生成模型在学习物理法则方面的潜力。我们开发了一个二维模拟测试平台，用于生成受经典力学法则支配的视频数据。通过对模型在不同场景下的表现进行评估，我们发现模型在已知分布内表现良好，但在未知分布中则出现失败。研究表明，模型在推广新案例时，优先考虑的因素依次为颜色、大小、速度和形状，而不是抽象出一般的物理规则。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00918",
            "title": "LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models",
            "url": "https://huggingface.co/papers/2411.00918",
            "abstract": "Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops LibMoE, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms. Built upon three core principles: (i) modular design, (ii) efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs more accessible to a wide range of researchers by standardizing the training and evaluation pipelines. Using LibMoE, we extensively benchmarked five state-of-the-art MoE algorithms over three different LLMs and 11 datasets under the zero-shot setting. The results show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks. With the modular design and extensive evaluation, we believe LibMoE will be invaluable for researchers to make meaningful progress towards the next generation of MoE and LLMs. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.",
            "score": 4,
            "issue_id": 420,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 ноября",
                "en": "November 1",
                "zh": "11月1日"
            },
            "hash": "a406640433a3de34",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LibMoE: Демократизация исследований Mixture of Experts в больших языковых моделях",
                    "desc": "Статья представляет LibMoE - комплексную модульную систему для исследования, обучения и оценки алгоритмов Mixture of Experts (MoE) в больших языковых моделях (LLM). LibMoE основан на трех принципах: модульный дизайн, эффективное обучение и всесторонняя оценка. Используя LibMoE, авторы провели обширное сравнение пяти современных алгоритмов MoE на трех различных LLM и 11 наборах данных в режиме zero-shot. Результаты показали, что, несмотря на уникальные характеристики, все алгоритмы MoE показывают примерно одинаковые результаты при усреднении по широкому спектру задач."
                },
                "en": {
                    "title": "LibMoE: Streamlining Mixture of Experts for Large Language Models",
                    "desc": "This paper introduces LibMoE, a framework designed to facilitate research on Mixture of Experts (MoE) algorithms for large language models (LLMs). It emphasizes a modular design, efficient training processes, and comprehensive evaluation methods to make MoE more accessible to researchers. The authors benchmark five leading MoE algorithms across three LLMs and 11 datasets, revealing that their performance is generally similar across various tasks. LibMoE aims to standardize the training and evaluation of MoE, helping researchers advance the development of future LLMs."
                },
                "zh": {
                    "title": "LibMoE：让混合专家算法更易于研究和应用",
                    "desc": "混合专家（MoE）在大型语言模型（LLMs）的高效和有效发展中扮演着重要角色。由于资源需求巨大，许多研究者难以研究大规模的MoE算法。本文开发了LibMoE，这是一个全面且模块化的框架，旨在简化MoE算法的研究、训练和评估。通过模块化设计、高效训练和全面评估，LibMoE使得MoE在LLMs中的应用对更多研究者变得可及。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02319",
            "title": "GenXD: Generating Any 3D and 4D Scenes",
            "url": "https://huggingface.co/papers/2411.02319",
            "abstract": "Recent developments in 2D visual generation have been remarkably successful. However, 3D and 4D generation remain challenging in real-world applications due to the lack of large-scale 4D data and effective model design. In this paper, we propose to jointly investigate general 3D and 4D generation by leveraging camera and object movements commonly observed in daily life. Due to the lack of real-world 4D data in the community, we first propose a data curation pipeline to obtain camera poses and object motion strength from videos. Based on this pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K. By leveraging all the 3D and 4D data, we develop our framework, GenXD, which allows us to produce any 3D or 4D scene. We propose multiview-temporal modules, which disentangle camera and object movements, to seamlessly learn from both 3D and 4D data. Additionally, GenXD employs masked latent conditions to support a variety of conditioning views. GenXD can generate videos that follow the camera trajectory as well as consistent 3D views that can be lifted into 3D representations. We perform extensive evaluations across various real-world and synthetic datasets, demonstrating GenXD's effectiveness and versatility compared to previous methods in 3D and 4D generation.",
            "score": 3,
            "issue_id": 422,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 ноября",
                "en": "November 4",
                "zh": "11月4日"
            },
            "hash": "51444eddaf6bbbe7",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#cv",
                    "#3d",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "GenXD: Универсальный генератор 3D и 4D сцен",
                    "desc": "Исследователи представили новый подход к генерации 3D и 4D сцен под названием GenXD. Они создали большой набор данных реальных 4D сцен CamVid-30K, используя специальный конвейер для извлечения информации о движении камеры и объектов из видео. GenXD использует мультиракурсно-временные модули для разделения движений камеры и объектов, а также маскированные латентные условия для гибкого задания входных ракурсов. Метод позволяет генерировать видео с заданной траекторией камеры и согласованные 3D виды, которые можно преобразовать в 3D-представления."
                },
                "en": {
                    "title": "Unlocking 3D and 4D Generation with GenXD",
                    "desc": "This paper addresses the challenges of generating 3D and 4D visual content, which are more complex than 2D generation due to limited data and model design issues. The authors introduce a new dataset called CamVid-30K, created by extracting camera poses and object movements from videos, which helps in training models effectively. They present a framework named GenXD that utilizes multiview-temporal modules to differentiate between camera and object movements, enabling the generation of realistic 3D and 4D scenes. Extensive evaluations show that GenXD outperforms existing methods, demonstrating its capability to create coherent videos and 3D representations from diverse inputs."
                },
                "zh": {
                    "title": "突破3D与4D生成的瓶颈",
                    "desc": "本文探讨了3D和4D视觉生成的挑战，尤其是在缺乏大规模4D数据的情况下。我们提出了一种数据整理流程，从视频中获取相机姿态和物体运动强度，并引入了一个大型的4D场景数据集CamVid-30K。基于这些数据，我们开发了GenXD框架，能够生成任意3D或4D场景。通过多视角时间模块，GenXD能够有效地学习相机和物体的运动，并生成与相机轨迹一致的视频和可提升为3D表示的视图。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02265",
            "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent",
            "url": "https://huggingface.co/papers/2411.02265",
            "abstract": "In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.   Codes: https://github.com/Tencent/Hunyuan-Large   Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
            "score": 3,
            "issue_id": 422,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 ноября",
                "en": "November 4",
                "zh": "11月4日"
            },
            "hash": "775afc5ff4d7fbdf",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#long_context",
                    "#synthetic",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Hunyuan-Large: Гигантский шаг вперед в области моделей смеси экспертов",
                    "desc": "Статья представляет Hunyuan-Large - крупнейшую открытую модель на основе трансформеров с архитектурой смеси экспертов, содержащую 389 миллиардов параметров. Модель демонстрирует превосходную производительность в различных задачах, включая понимание и генерацию языка, логические рассуждения и программирование. Ключевые особенности Hunyuan-Large включают использование масштабных синтетических данных и смешанную стратегию маршрутизации экспертов. Авторы также исследуют законы масштабирования и графики скорости обучения для моделей смеси экспертов."
                },
                "en": {
                    "title": "Unlocking New Frontiers in AI with Hunyuan-Large",
                    "desc": "Hunyuan-Large is a groundbreaking Transformer-based mixture of experts model with an impressive 389 billion parameters, designed to process up to 256K tokens. It demonstrates exceptional performance in various tasks such as language understanding, logical reasoning, and coding, surpassing the capabilities of LLama3.1-70B and rivaling the larger LLama3.1-405B. The model employs innovative techniques like large-scale synthetic data generation, a mixed expert routing strategy, and expert-specific learning rates to enhance its efficiency. Furthermore, the paper explores scaling laws and learning rate schedules, offering insights for future advancements in model optimization."
                },
                "zh": {
                    "title": "Hunyuan-Large：超大规模专家混合模型的创新之路",
                    "desc": "本文介绍了Hunyuan-Large，这是目前最大的开源基于Transformer的专家混合模型，拥有3890亿个参数和520亿个激活参数，能够处理多达256K的token。我们对Hunyuan-Large在语言理解与生成、逻辑推理、数学问题解决、编码、长上下文和聚合任务等多个基准上的优越性能进行了全面评估，结果显示其优于LLama3.1-70B，并且在与更大模型LLama3.1-405B的比较中表现相当。Hunyuan-Large的关键实践包括大规模合成数据、混合专家路由策略、键值缓存压缩技术和专家特定学习率策略。此外，我们还研究了专家混合模型的扩展规律和学习率调度，为未来模型的开发和优化提供了宝贵的见解和指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.02397",
            "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
            "url": "https://huggingface.co/papers/2411.02397",
            "abstract": "Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs) -- despite making significant headway in this context -- have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a training-free method to accelerate video DiTs, termed Adaptive Caching (AdaCache), which is motivated by the fact that \"not all videos are created equal\": meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization (MoReg) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines.",
            "score": 2,
            "issue_id": 421,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 ноября",
                "en": "November 4",
                "zh": "11月4日"
            },
            "hash": "d7fa22d791789900",
            "data": {
                "categories": [
                    "#video",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Адаптивное кэширование ускоряет генерацию видео без потери качества",
                    "desc": "Статья представляет метод AdaCache для ускорения видео-диффузионных трансформеров (DiT) без переобучения. AdaCache адаптивно кэширует вычисления в процессе диффузии, учитывая, что разным видео требуется разное количество шагов шумоподавления. Также предложена схема регуляризации движения (MoReg) для оптимизации распределения вычислений на основе содержания движения в видео. Метод значительно ускоряет генерацию видео (до 4.7 раз на Open-Sora 720p) без потери качества."
                },
                "en": {
                    "title": "Accelerating Video Generation with Adaptive Caching!",
                    "desc": "This paper presents a method called Adaptive Caching (AdaCache) to improve the speed of video generation using Diffusion Transformers (DiTs). The authors argue that different videos have varying requirements for denoising steps, allowing for a more efficient computation process. By caching computations and creating a tailored caching schedule for each video, they enhance the balance between quality and speed. Additionally, they introduce a Motion Regularization (MoReg) technique to optimize resource allocation based on the motion content of the video, achieving significant speed improvements without compromising quality."
                },
                "zh": {
                    "title": "加速视频生成，提升质量与效率！",
                    "desc": "本论文提出了一种名为自适应缓存（AdaCache）的方法，用于加速视频生成中的扩散变换器（DiTs）。该方法通过缓存计算过程，针对每个视频生成制定缓存计划，从而优化质量与延迟的平衡。我们还引入了运动正则化（MoReg）方案，根据视频中的运动内容来控制计算分配。整体而言，这些创新显著提高了推理速度，同时保持了生成质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00743",
            "title": "Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models",
            "url": "https://huggingface.co/papers/2411.00743",
            "abstract": "Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. We introduce Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. We present a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. Our evaluation of SSAEs on standard metrics, such as downstream perplexity and L_0 sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. We showcase the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5\\% increase in worst-group classification accuracy when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains.",
            "score": 2,
            "issue_id": 421,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 ноября",
                "en": "November 1",
                "zh": "11月1日"
            },
            "hash": "af234e3c99f935ec",
            "data": {
                "categories": [
                    "#interpretability",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Специализированные автоэнкодеры раскрывают скрытые концепты в фундаментальных моделях",
                    "desc": "Статья представляет новый метод интерпретации фундаментальных моделей - Специализированные разреженные автоэнкодеры (SSAE). SSAE фокусируются на конкретных поддоменах и эффективно выявляют редкие, но важные концепты в данных. Авторы демонстрируют преимущества плотного поиска для выбора данных и использования Tilted Empirical Risk Minimization в качестве целевой функции обучения. Эффективность SSAE показана на стандартных метриках и в практическом исследовании на наборе данных Bias in Bios."
                },
                "en": {
                    "title": "Illuminating Hidden Concepts in Foundation Models with SSAEs",
                    "desc": "This paper discusses the challenges of understanding and mitigating risks in foundation models (FMs) through effective interpretability methods. It introduces Specialized Sparse Autoencoders (SSAEs), which are designed to better identify rare but important concepts in data by focusing on specific subdomains. The authors provide a training approach that utilizes dense retrieval for data selection and Tilted Empirical Risk Minimization to enhance concept recall. The results show that SSAEs outperform traditional Sparse Autoencoders in capturing these critical features, as demonstrated in a case study that improved classification accuracy by addressing bias in gender information."
                },
                "zh": {
                    "title": "专注子领域的稀疏自编码器：揭示基础模型的潜在特征",
                    "desc": "本论文探讨了基础模型（FMs）潜在风险的理解与缓解，强调了有效的可解释性方法的重要性。我们提出了一种新的稀疏自编码器（SSAEs），旨在揭示数据中稀有但重要的概念，特别关注特定子领域。通过密集检索和倾斜经验风险最小化等方法，我们展示了SSAEs在捕捉子领域尾部概念方面的优势。案例研究表明，SSAEs在去除虚假性别信息时，分类准确率提高了12.5%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.01747",
            "title": "DynaSaur: Large Language Agents Beyond Predefined Actions",
            "url": "https://huggingface.co/papers/2411.01747",
            "abstract": "Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed set of actions significantly restricts the planning and acting capabilities of LLM agents, and (2) this approach requires substantial human effort to enumerate and implement all possible actions, which becomes impractical in complex environments with a vast number of potential actions. In this work, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with the environment by generating and executing programs written in a general-purpose programming language at each step. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments on the GAIA benchmark demonstrate that this framework offers significantly greater flexibility and outperforms previous methods. Notably, it allows an LLM agent to recover in scenarios where no relevant action exists in the predefined set or when existing actions fail due to unforeseen edge cases. At the time of writing, we hold the top position on the GAIA public leaderboard. Our code can be found in https://github.com/adobe-research/dynasaur{https://github.com/adobe-research/dynasaur}.",
            "score": 1,
            "issue_id": 423,
            "pub_date": "2024-11-04",
            "pub_date_card": {
                "ru": "4 ноября",
                "en": "November 4",
                "zh": "11月4日"
            },
            "hash": "772b11b15cab80a0",
            "data": {
                "categories": [
                    "#agents",
                    "#plp",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Динамическое создание действий для агентов на базе LLM: шаг к адаптивному ИИ",
                    "desc": "Эта статья представляет новую структуру агентов на основе больших языковых моделей (LLM), которая позволяет динамически создавать и комбинировать действия в режиме реального времени. В отличие от существующих систем с фиксированным набором действий, предлагаемый подход генерирует и выполняет программы на языке общего назначения на каждом шаге взаимодействия с окружающей средой. Авторы демонстрируют, что их метод обеспечивает большую гибкость и превосходит предыдущие подходы в экспериментах на бенчмарке GAIA. Особенно эффективно система работает в сценариях, где предопределенные действия отсутствуют или неприменимы."
                },
                "en": {
                    "title": "Empowering LLM Agents with Dynamic Action Generation",
                    "desc": "This paper introduces a new framework for large language model (LLM) agents that allows them to dynamically create and execute actions in real-time, rather than relying on a fixed set of predefined actions. This approach enhances the planning and acting capabilities of LLM agents, making them more adaptable to complex and unpredictable environments. By generating programs in a general-purpose programming language, the agents can respond to unforeseen situations and accumulate useful actions for future tasks. The experiments conducted on the GAIA benchmark show that this framework significantly outperforms traditional methods, providing greater flexibility and effectiveness in real-world applications."
                },
                "zh": {
                    "title": "动态创建与组合动作的LLM代理框架",
                    "desc": "现有的大型语言模型（LLM）代理系统通常在每一步从固定的预定义动作集中选择动作。这种方法在封闭的、狭窄的环境中有效，但在现实世界中应用时面临两个主要挑战：一是从固定动作集中选择限制了LLM代理的规划和执行能力，二是需要大量人力来列举和实现所有可能的动作，这在复杂环境中变得不切实际。我们提出了一种LLM代理框架，允许在线动态创建和组合动作，代理通过生成和执行通用编程语言编写的程序与环境互动。我们的实验表明，该框架提供了更大的灵活性，并在GAIA基准测试中表现优于以往方法。"
                }
            }
        }
    ],
    "link_prev": "2024-11-04.html",
    "link_next": "2024-11-06.html",
    "short_date_prev": {
        "ru": "04.11",
        "en": "11/04",
        "zh": "11月4日"
    },
    "short_date_next": {
        "ru": "06.11",
        "en": "11/06",
        "zh": "11月6日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#medicine": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#translation": 0
    },
    "zh": {
        "text": "这篇文章讨论了构建图形用户界面（GUI）代理的现有努力。目前主要依赖于强大的商业视觉语言模型（VLMs），如GPT-4o和GeminiProVision。开源VLMs在GUI理解和未见分布（OOD）场景中表现较差，因此实践者不太愿意使用。为了推动这一领域的研究，作者开发了OS-Atlas，这是一个擅长GUI理解和OOD任务的基础模型。他们还发布了一个包含1300多万GUI元素的跨平台数据集，并证明了OS-Atlas在多个基准测试中的显著性能提升。",
        "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
        "pinyin": "Zhè piān wénzhāng tǎolùn le gòujiàn túxíng yònghù jiēmiàn (GUI) dàilǐ de xiàn yǒu nǔlì. Dāngqián zhǔyào yīlài zài qiángdà de shāngyè shìjué yǔyán móxíng (VLMs) shàng, rú GPT-4o hé GeminiProVision. Kāiyuán VLMs zài GUI lǐjiě hé wèi jiàn fēnbù (OOD) chǎngjǐng zhōng biǎoxiǎn jiàochǎ, yīncǐ shíjiànzhě bù tài yuànyì shǐyòng. Wèile tuīdòng zhè yī lǐngyù de yánjiū, zuòzhě kāifāle OS-Atlas, zhè shì yīgè shàncháng GUI lǐjiě hé OOD rènwù de jīchǔ móxíng. Tāmen hái fābùle yīgè bāohán 1300 duō wàn GUI yuánsù de kuà píngtái shùjùjí, bìng zhèngmíngle OS-Atlas zài duōgè jīzhǔn cèshì zhōng de xiǎnzhù xiàonéng tǐshēng.",
        "vocab": "[\n    {\"word\": \"构建\", \"pinyin\": \"gòu jiàn\", \"trans\": \"construct\"},\n    {\"word\": \"图形用户界面\", \"pinyin\": \"tú xíng yòng hù jiè miàn\", \"trans\": \"graphical user interface\"},\n    {\"word\": \"代理\", \"pinyin\": \"dài lǐ\", \"trans\": \"agent\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiàn yǒu\", \"trans\": \"existing\"},\n    {\"word\": \"努力\", \"pinyin\": \"nǔ lì\", \"trans\": \"efforts\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"rely on\"},\n    {\"word\": \"强大\", \"pinyin\": \"qiáng dà\", \"trans\": \"powerful\"},\n    {\"word\": \"商业\", \"pinyin\": \"shāng yè\", \"trans\": \"commercial\"},\n    {\"word\": \"视觉语言模型\", \"pinyin\": \"shì jué yǔ yán mó xíng\", \"trans\": \"visual language model\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāi yuán\", \"trans\": \"open-source\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understanding\"},\n    {\"word\": \"未见分布\", \"pinyin\": \"wèi jiàn fēn bù\", \"trans\": \"out-of-distribution\"},\n    {\"word\": \"场景\", \"pinyin\": \"chǎng jǐng\", \"trans\": \"scenario\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"较差\", \"pinyin\": \"jiào chà\", \"trans\": \"poor\"},\n    {\"word\": \"实践者\", \"pinyin\": \"shí jiàn zhě\", \"trans\": \"practitioner\"},\n    {\"word\": \"不太愿意\", \"pinyin\": \"bù tài yuàn yì\", \"trans\": \"not very willing\"},\n    {\"word\": \"推动\", \"pinyin\": \"tuī dòng\", \"trans\": \"promote\"},\n    {\"word\": \"领域\", \"pinyin\": \"lǐng yù\", \"trans\": \"field\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"开发\", \"pinyin\": \"kāi fā\", \"trans\": \"develop\"},\n    {\"word\": \"基础模型\", \"pinyin\": \"jī chǔ mó xíng\", \"trans\": \"foundation model\"},\n    {\"word\": \"擅长\", \"pinyin\": \"shàn cháng\", \"trans\": \"proficient\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wù\", \"trans\": \"task\"},\n    {\"word\": \"发布\", \"pinyin\": \"fā bù\", \"trans\": \"release\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"跨平台\", \"pinyin\": \"kuà píng tái\", \"trans\": \"cross-platform\"},\n    {\"word\": \"元素\", \"pinyin\": \"yuán sù\", \"trans\": \"element\"},\n    {\"word\": \"证明\", \"pinyin\": \"zhèng míng\", \"trans\": \"demonstrate\"},\n    {\"word\": \"基准测试\", \"pinyin\": \"jī zhǔn cè shì\", \"trans\": \"benchmark test\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"提升\", \"pinyin\": \"tí shēng\", \"trans\": \"improvement\"}\n]",
        "trans": "This article discusses current efforts in building graphical user interface (GUI) agents. Currently, these efforts primarily rely on powerful commercial vision language models (VLMs) such as GPT-4o and GeminiProVision. Open-source VLMs perform poorly in GUI understanding and out-of-distribution (OOD) scenarios, making practitioners reluctant to use them. To advance research in this field, the authors developed OS-Atlas, a foundational model adept at GUI understanding and OOD tasks. They also released a cross-platform dataset containing over 13 million GUI elements and demonstrated significant performance improvements of OS-Atlas across multiple benchmark tests.",
        "update_ts": "2024-11-04 10:14"
    },
    "link_month": "2024-11.html"
}