
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. September 12.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            margin-top: 10px;
            margin-bottom: 10px;
            display: block;
            border-radius: 5px;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">12 сентября</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-09-11.html">⬅️ <span id="prev-date">11.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-09-13.html">➡️ <span id="next-date">13.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '12 сентября', 'en': 'September 12', 'zh': '9月12日'};
        let feedDateNext = {'ru': '13.09', 'en': '09/13', 'zh': '9月13日'};
        let feedDatePrev = {'ru': '11.09', 'en': '09/11', 'zh': '9月11日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2409.06820', 'title': 'PingPong: A Benchmark for Role-Playing Language Models with User Emulation and Multi-Model Evaluation', 'url': 'https://huggingface.co/papers/2409.06820', 'abstract': 'We introduce a novel benchmark for evaluating the role-playing capabilities of language models. Our approach leverages language models themselves to emulate users in dynamic, multi-turn conversations and to assess the resulting dialogues. The framework consists of three main components: a player model assuming a specific character role, an interrogator model simulating user behavior, and a judge model evaluating conversation quality. We conducted experiments comparing automated evaluations with human annotations to validate our approach, demonstrating strong correlations across multiple criteria. This work provides a foundation for a robust and dynamic evaluation of model capabilities in interactive scenarios.', 'score': 62, 'issue_id': 1, 'pub_date': '2024-09-10', 'pub_date_card': {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'}, 'hash': '1c586f12e00722af', 'data': {'categories': ['#reasoning', '#interpretability', '#agents', '#benchmark', '#games', '#architecture'], 'emoji': '🎭', 'ru': {'title': 'Языковые модели оценивают сами себя в ролевых играх', 'desc': 'Представлен новый подход к оценке способностей языковых моделей к ролевой игре. Метод использует сами языковые модели для эмуляции пользователей в динамических многоходовых диалогах и оценки результатов. Framework состоит из трёх компонентов: модели игрока, принимающей определённую роль, модели собеседника, имитирующей поведение пользователя, и модели судьи, оценивающей качество беседы. Эксперименты показали сильную корреляцию между автоматизированными оценками и аннотациями людей.'}, 'en': {'title': 'Evaluating Language Models in Role-Playing Scenarios', 'desc': 'This paper presents a new benchmark designed to evaluate how well language models can perform in role-playing scenarios. It uses a framework that includes three key components: a player model that takes on a character, an interrogator model that mimics user interactions, and a judge model that assesses the quality of the conversations. The authors conducted experiments to compare automated evaluations with human assessments, showing that their method produces reliable results. This research lays the groundwork for more effective evaluations of language models in interactive and dynamic settings.'}, 'zh': {'title': '动态对话评估：语言模型的新基准', 'desc': '我们提出了一种新的基准，用于评估语言模型的角色扮演能力。我们的方法利用语言模型本身模拟用户在动态的多轮对话中的行为，并评估生成的对话。该框架由三个主要组件组成：扮演特定角色的玩家模型、模拟用户行为的询问者模型，以及评估对话质量的评判模型。通过与人工标注的比较实验，我们验证了该方法的有效性，显示出在多个标准下的强相关性。'}}}, {'id': 'https://huggingface.co/papers/2409.07314', 'title': 'MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications', 'url': 'https://huggingface.co/papers/2409.07314', 'abstract': "The rapid development of Large Language Models (LLMs) for healthcare applications has spurred calls for holistic evaluation beyond frequently-cited benchmarks like USMLE, to better reflect real-world performance. While real-world assessments are valuable indicators of utility, they often lag behind the pace of LLM evolution, likely rendering findings obsolete upon deployment. This temporal disconnect necessitates a comprehensive upfront evaluation that can guide model selection for specific clinical applications. We introduce MEDIC, a framework assessing LLMs across five critical dimensions of clinical competence: medical reasoning, ethics and bias, data and language understanding, in-context learning, and clinical safety. MEDIC features a novel cross-examination framework quantifying LLM performance across areas like coverage and hallucination detection, without requiring reference outputs. We apply MEDIC to evaluate LLMs on medical question-answering, safety, summarization, note generation, and other tasks. Our results show performance disparities across model sizes, baseline vs medically finetuned models, and have implications on model selection for applications requiring specific model strengths, such as low hallucination or lower cost of inference. MEDIC's multifaceted evaluation reveals these performance trade-offs, bridging the gap between theoretical capabilities and practical implementation in healthcare settings, ensuring that the most promising models are identified and adapted for diverse healthcare applications.", 'score': 50, 'issue_id': 1, 'pub_date': '2024-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': 'd38ebd585fc1c68a', 'data': {'categories': ['#reasoning', '#evaluation', '#hallucinations', '#training', '#healthcare', '#inference', '#ethics', '#benchmark', '#alignment', '#architecture'], 'emoji': '🩺', 'ru': {'title': 'MEDIC: комплексная оценка языковых моделей для здравоохранения', 'desc': 'Статья представляет MEDIC - новую систему оценки больших языковых моделей (LLM) для применения в здравоохранении. MEDIC оценивает модели по пяти ключевым аспектам клинической компетентности, включая медицинское мышление, этику и предвзятость, понимание данных и языка, обучение в контексте и клиническую безопасность. Система использует инновационный метод перекрестной проверки для оценки производительности LLM без необходимости в эталонных ответах. Результаты применения MEDIC показывают различия в производительности между моделями разных размеров и специализаций, что важно для выбора оптимальной модели для конкретных медицинских задач.'}, 'en': {'title': 'MEDIC: A Comprehensive Evaluation Framework for Healthcare LLMs', 'desc': 'This paper discusses the need for a comprehensive evaluation framework for Large Language Models (LLMs) used in healthcare, as traditional benchmarks may not accurately reflect real-world performance. The authors introduce MEDIC, a framework that assesses LLMs based on five key dimensions: medical reasoning, ethics and bias, data understanding, in-context learning, and clinical safety. MEDIC employs a unique cross-examination method to evaluate model performance without needing reference outputs, focusing on aspects like coverage and hallucination detection. The findings highlight significant performance differences among various models, guiding the selection of LLMs for specific clinical tasks and ensuring their effective application in healthcare.'}, 'zh': {'title': 'MEDIC：提升医疗语言模型评估的全面性', 'desc': '本论文介绍了MEDIC框架，用于全面评估大型语言模型（LLMs）在医疗应用中的表现。该框架从医学推理、伦理与偏见、数据与语言理解、上下文学习和临床安全五个关键维度进行评估。通过MEDIC，我们能够量化LLMs在医疗问答、安全性、摘要生成等任务中的表现差异，帮助选择适合特定临床应用的模型。研究结果显示，不同模型大小和微调策略对性能有显著影响，强调了在医疗环境中实施时的实际能力与理论能力之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2409.07429', 'title': 'Agent Workflow Memory', 'url': 'https://huggingface.co/papers/2409.07429', 'abstract': 'Despite the potential of language model-based agents to solve real-world tasks such as web navigation, current methods still struggle with long-horizon tasks with complex action trajectories. In contrast, humans can flexibly solve complex tasks by learning reusable task workflows from past experiences and using them to guide future actions. To build agents that can similarly benefit from this process, we introduce Agent Workflow Memory (AWM), a method for inducing commonly reused routines, i.e., workflows, and selectively providing workflows to the agent to guide subsequent generations. AWM flexibly applies to both offline and online scenarios, where agents induce workflows from training examples beforehand or from test queries on the fly. We experiment on two major web navigation benchmarks -- Mind2Web and WebArena -- that collectively cover 1000+ tasks from 200+ domains across travel, shopping, and social media, among others. AWM substantially improves the baseline results by 24.6% and 51.1% relative success rate on Mind2Web and WebArena while reducing the number of steps taken to solve WebArena tasks successfully. Furthermore, online AWM robustly generalizes in cross-task, website, and domain evaluations, surpassing baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps widen.', 'score': 27, 'issue_id': 1, 'pub_date': '2024-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '42c395b13379e4ef', 'data': {'categories': ['#reasoning', '#long_context', '#agents', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Обучение агентов на основе рабочих процессов для эффективной веб-навигации', 'desc': 'Статья представляет метод Agent Workflow Memory (AWM) для улучшения работы языковых моделей в задачах веб-навигации. AWM позволяет агентам извлекать и использовать повторяющиеся рабочие процессы из прошлого опыта. Метод применим как в офлайн, так и в онлайн сценариях. Эксперименты на двух крупных бенчмарках веб-навигации показали значительное улучшение результатов и сокращение числа шагов для решения задач.'}, 'en': {'title': 'Empowering Agents with Reusable Workflows for Complex Tasks', 'desc': 'This paper presents Agent Workflow Memory (AWM), a novel approach designed to enhance language model-based agents in performing long-horizon tasks with complex action sequences. AWM enables agents to learn and reuse workflows from past experiences, allowing them to efficiently tackle new tasks by leveraging these learned routines. The method is applicable in both offline and online settings, adapting to workflows derived from training data or generated in real-time during task execution. Experimental results demonstrate that AWM significantly improves task success rates and reduces the number of steps needed to complete tasks across various web navigation benchmarks.'}, 'zh': {'title': '提升代理任务解决能力的工作流记忆', 'desc': '本文提出了一种名为代理工作流记忆（AWM）的方法，旨在帮助语言模型代理更好地解决复杂的长期任务。AWM通过从过去的经验中学习可重用的任务工作流，来指导代理的后续行动。该方法适用于离线和在线场景，能够根据训练示例或实时查询生成工作流。实验结果表明，AWM在多个网络导航基准测试中显著提高了成功率，并减少了解决任务所需的步骤。'}}}, {'id': 'https://huggingface.co/papers/2409.07146', 'title': 'Gated Slot Attention for Efficient Linear-Time Sequence Modeling', 'url': 'https://huggingface.co/papers/2409.07146', 'abstract': 'Linear attention Transformers and their gated variants, celebrated for enabling parallel training and efficient recurrent inference, still fall short in recall-intensive tasks compared to traditional Transformers and demand significant resources for training from scratch. This paper introduces Gated Slot Attention (GSA), which enhances Attention with Bounded-memory-Control (ABC) by incorporating a gating mechanism inspired by Gated Linear Attention (GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size. This design greatly enhances both training and inference efficiency through GLA\'s hardware-efficient training algorithm and reduced state size. Additionally, retaining the softmax operation is particularly beneficial in "finetuning pretrained Transformers to RNNs" (T2R) settings, reducing the need for extensive training from scratch. Extensive experiments confirm GSA\'s superior performance in scenarios requiring in-context recall and in T2R settings.', 'score': 19, 'issue_id': 1, 'pub_date': '2024-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '94f5d7f9f1710481', 'data': {'categories': ['#training', '#inference', '#optimization', '#transfer_learning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'GSA: Эффективное улучшение памяти для линейных трансформеров', 'desc': 'Статья представляет новый метод под названием Gated Slot Attention (GSA), который улучшает модель Attention with Bounded-memory-Control (ABC). GSA использует механизм гейтинга, вдохновленный Gated Linear Attention (GLA), и состоит из двухслойного GLA, связанного через софтмакс. Этот метод улучшает емкость памяти, сохраняя компактный размер рекуррентного состояния, что повышает эффективность обучения и вывода. GSA показывает превосходные результаты в задачах, требующих контекстного запоминания, и в настройке предобученных трансформеров на RNN.'}, 'en': {'title': 'Enhancing Memory Efficiency in Transformers with Gated Slot Attention', 'desc': 'This paper presents Gated Slot Attention (GSA), a novel approach that improves the efficiency of linear attention Transformers by integrating a gating mechanism. GSA enhances the Attention with Bounded-memory-Control (ABC) framework, allowing for better memory management through context-aware reading and adaptive forgetting. The architecture consists of a two-layer Gated Linear Attention (GLA) that optimizes both training and inference processes while keeping the memory footprint small. Experimental results demonstrate that GSA outperforms traditional Transformers in recall-intensive tasks and reduces the training burden in fine-tuning scenarios.'}, 'zh': {'title': '提升记忆与效率的门控槽注意力机制', 'desc': '本文提出了一种新的注意力机制，称为门控槽注意力（GSA），旨在提高记忆能力和训练效率。GSA结合了门控线性注意力（GLA）和有界记忆控制（ABC），通过引入门控机制来优化信息的读取和遗忘。该方法通过软最大化连接的两层GLA，增强了上下文感知的记忆读取能力，同时保持了紧凑的递归状态大小。实验结果表明，GSA在需要上下文回忆的任务和微调预训练变换器到递归神经网络的设置中表现优越。'}}}, {'id': 'https://huggingface.co/papers/2409.07452', 'title': 'Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models', 'url': 'https://huggingface.co/papers/2409.07452', 'abstract': 'Despite having tremendous progress in image-to-3D generation, existing methods still struggle to produce multi-view consistent images with high-resolution textures in detail, especially in the paradigm of 2D diffusion that lacks 3D awareness. In this work, we present High-resolution Image-to-3D model (Hi3D), a new video diffusion based paradigm that redefines a single image to multi-view images as 3D-aware sequential image generation (i.e., orbital video generation). This methodology delves into the underlying temporal consistency knowledge in video diffusion model that generalizes well to geometry consistency across multiple views in 3D generation. Technically, Hi3D first empowers the pre-trained video diffusion model with 3D-aware prior (camera pose condition), yielding multi-view images with low-resolution texture details. A 3D-aware video-to-video refiner is learnt to further scale up the multi-view images with high-resolution texture details. Such high-resolution multi-view images are further augmented with novel views through 3D Gaussian Splatting, which are finally leveraged to obtain high-fidelity meshes via 3D reconstruction. Extensive experiments on both novel view synthesis and single view reconstruction demonstrate that our Hi3D manages to produce superior multi-view consistency images with highly-detailed textures. Source code and data are available at https://github.com/yanghb22-fdu/Hi3D-Official.', 'score': 18, 'issue_id': 1, 'pub_date': '2024-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': 'fa11c9f3b70cbc47', 'data': {'categories': ['#video', '#dataset', '#cv', '#open_source', '#diffusion', '#3d'], 'emoji': '🎥', 'ru': {'title': 'От 2D к 3D: видеодиффузия для создания реалистичных трехмерных моделей', 'desc': 'Статья представляет новую модель Hi3D для генерации трехмерных объектов из изображений. Модель использует видеодиффузию для создания орбитальных видео, что обеспечивает согласованность между разными ракурсами. Hi3D сначала генерирует многоракурсные изображения низкого разрешения, затем улучшает их детализацию с помощью специального рефайнера. Полученные изображения используются для создания высококачественных 3D-моделей с детальными текстурами.'}, 'en': {'title': 'Transforming Single Images into High-Resolution 3D Views with Hi3D', 'desc': 'This paper introduces the High-resolution Image-to-3D model (Hi3D), which enhances the generation of multi-view images from a single image using a video diffusion approach. Hi3D incorporates 3D awareness by utilizing camera pose conditions, allowing it to generate images with improved geometric consistency across different views. The model first produces low-resolution multi-view images, which are then refined to high-resolution textures through a dedicated video-to-video refinement process. The final output includes high-fidelity 3D meshes, demonstrating significant advancements in multi-view consistency and detail in image generation.'}, 'zh': {'title': '高分辨率图像到3D生成的新突破', 'desc': '尽管在图像到3D生成方面取得了巨大进展，但现有方法在生成多视角一致的高分辨率纹理图像时仍然面临挑战。本文提出了一种新的视频扩散基础的高分辨率图像到3D模型（Hi3D），将单幅图像重新定义为多视角图像，形成3D感知的序列图像生成。该方法利用视频扩散模型中的时间一致性知识，能够在3D生成中实现几何一致性。Hi3D通过引入3D感知先验，生成低分辨率纹理的多视角图像，并通过3D高斯点云进一步增强图像的分辨率，最终实现高保真网格的重建。'}}}, {'id': 'https://huggingface.co/papers/2409.04057', 'title': 'Self-Harmonized Chain of Thought', 'url': 'https://huggingface.co/papers/2409.04057', 'abstract': "Chain-of-Thought (CoT) prompting reveals that large language models are capable of performing complex reasoning via intermediate steps. CoT prompting is primarily categorized into three approaches. The first approach utilizes straightforward prompts like ``Let's think step by step'' to generate a sequential thought process before yielding an answer. The second approach makes use of human-crafted, step-by-step demonstrations to guide the model's reasoning process. The third automates the generation of reasoned demonstrations with the 'Let's think step by step'.This approach sometimes leads to reasoning errors, highlighting the need to diversify demonstrations to mitigate its misleading effects. However, diverse demonstrations pose challenges for effective representations. In this work, we propose ECHO, a self-harmonized chain-of-thought prompting method. It consolidates diverse solution paths into a uniform and effective solution pattern.ECHO demonstrates the best overall performance across three reasoning domains.", 'score': 16, 'issue_id': 1, 'pub_date': '2024-09-06', 'pub_date_card': {'ru': '6 сентября', 'en': 'September 6', 'zh': '9月6日'}, 'hash': '88a4d1900f46ba34', 'data': {'categories': ['#reasoning', '#training', '#interpretability', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'ECHO: Самогармонизация для улучшения цепочек рассуждений в языковых моделях', 'desc': 'Это исследование посвящено методу Chain-of-Thought (CoT) в больших языковых моделях, который позволяет им выполнять сложные рассуждения через промежуточные шаги. Авторы выделяют три основных подхода к CoT-промптингу и обсуждают их преимущества и недостатки. В работе предлагается новый метод ECHO - самогармонизирующийся подход к CoT-промптингу, который объединяет разнообразные пути решения в единый эффективный шаблон. ECHO демонстрирует наилучшую общую производительность в трех областях рассуждений.'}, 'en': {'title': 'ECHO: Harmonizing Reasoning for Better AI Performance', 'desc': 'This paper discusses Chain-of-Thought (CoT) prompting, which helps large language models perform complex reasoning by breaking down tasks into intermediate steps. It identifies three main approaches to CoT prompting: simple prompts, human-crafted demonstrations, and automated demonstrations. The authors introduce ECHO, a new method that harmonizes diverse reasoning paths into a cohesive solution pattern, addressing the challenges of representation in diverse demonstrations. ECHO shows superior performance in various reasoning tasks compared to existing methods.'}, 'zh': {'title': 'ECHO：统一多样化推理路径的创新方法', 'desc': '本文探讨了链式思维（CoT）提示在大型语言模型中如何通过中间步骤进行复杂推理。CoT提示主要分为三种方法：第一种是使用简单的提示语，如“让我们一步一步思考”，以生成顺序思维过程；第二种是利用人类设计的逐步示范来引导模型的推理过程；第三种是自动生成带有“让我们一步一步思考”的推理示范。本文提出了ECHO方法，它将多样化的解决路径整合为统一有效的解决模式，并在三个推理领域中表现出最佳的整体性能。'}}}, {'id': 'https://huggingface.co/papers/2409.06185', 'title': 'Can Large Language Models Unlock Novel Scientific Research Ideas?', 'url': 'https://huggingface.co/papers/2409.06185', 'abstract': '"An idea is nothing more nor less than a new combination of old elements" (Young, J.W.). The widespread adoption of Large Language Models (LLMs) and publicly available ChatGPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people\'s everyday lives. This study explores the capability of LLMs in generating novel research ideas based on information from research papers. We conduct a thorough examination of 4 LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and Physics). We found that the future research ideas generated by Claude-2 and GPT-4 are more aligned with the author\'s perspective than GPT-3.5 and Gemini. We also found that Claude-2 generates more diverse future research ideas than GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the novelty, relevancy, and feasibility of the generated future research ideas. This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations. Our work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas. We make our datasets and codes publicly available.', 'score': 12, 'issue_id': 1, 'pub_date': '2024-09-10', 'pub_date_card': {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'}, 'hash': '6d390735db62f961', 'data': {'categories': ['#science', '#dataset', '#multilingual', '#training', '#agi', '#rag', '#benchmark', '#alignment', '#open_source'], 'emoji': '💡', 'ru': {'title': 'LLM как генератор научных идей: потенциал и ограничения', 'desc': 'Исследование посвящено способности больших языковых моделей (LLM) генерировать новые идеи для исследований на основе информации из научных статей. Авторы провели сравнительный анализ четырех LLM в пяти областях науки. Результаты показали, что идеи, генерируемые Claude-2 и GPT-4, лучше соответствуют авторской перспективе, чем идеи GPT-3.5 и Gemini. Также было обнаружено, что Claude-2 генерирует более разнообразные идеи по сравнению с другими моделями.'}, 'en': {'title': 'Harnessing LLMs for Innovative Research Ideas', 'desc': "This paper investigates how Large Language Models (LLMs) can generate innovative research ideas by analyzing existing research papers. The study evaluates four different LLMs across five fields, including Chemistry and Medicine, to assess their effectiveness in idea generation. Results indicate that Claude-2 and GPT-4 produce ideas that better reflect the authors' perspectives compared to GPT-3.5 and Gemini, with Claude-2 also showing greater diversity in the generated ideas. The research emphasizes the potential and limitations of LLMs in contributing to the research community, and the authors provide their datasets and codes for public use."}, 'zh': {'title': '大型语言模型助力新研究想法的生成', 'desc': '本研究探讨了大型语言模型（LLMs）在生成新研究想法方面的能力。我们对四种LLMs在五个领域（如化学、计算机、经济学、医学和物理学）进行了全面评估。结果显示，Claude-2和GPT-4生成的未来研究想法更符合作者的观点，而Claude-2的多样性优于其他模型。我们的研究为理解LLMs在创意生成中的作用提供了见解，并公开了数据集和代码。'}}}, {'id': 'https://huggingface.co/papers/2409.06765', 'title': 'gsplat: An Open-Source Library for Gaussian Splatting', 'url': 'https://huggingface.co/papers/2409.06765', 'abstract': 'gsplat is an open-source library designed for training and developing Gaussian Splatting methods. It features a front-end with Python bindings compatible with the PyTorch library and a back-end with highly optimized CUDA kernels. gsplat offers numerous features that enhance the optimization of Gaussian Splatting models, which include optimization improvements for speed, memory, and convergence times. Experimental results demonstrate that gsplat achieves up to 10% less training time and 4x less memory than the original implementation. Utilized in several research projects, gsplat is actively maintained on GitHub. Source code is available at https://github.com/nerfstudio-project/gsplat under Apache License 2.0. We welcome contributions from the open-source community.', 'score': 11, 'issue_id': 1, 'pub_date': '2024-09-10', 'pub_date_card': {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'}, 'hash': '564acf9a51deee5d', 'data': {'categories': ['#training', '#inference', '#optimization', '#open_source', '#architecture', '#3d'], 'emoji': '🎨', 'ru': {'title': 'gsplat: Эффективная библиотека для Gaussian Splatting', 'desc': 'gsplat - это открытая библиотека для обучения и разработки методов Gaussian Splatting. Она предлагает фронтенд с Python-привязками, совместимыми с PyTorch, и бэкенд с оптимизированными CUDA-ядрами. gsplat обеспечивает улучшения в оптимизации моделей Gaussian Splatting, включая ускорение, экономию памяти и сокращение времени сходимости. Экспериментальные результаты показывают, что gsplat достигает до 10% меньшего времени обучения и в 4 раза меньшего использования памяти по сравнению с оригинальной реализацией.'}, 'en': {'title': 'Accelerate Gaussian Splatting with gsplat!', 'desc': 'gsplat is an open-source library that facilitates the training and development of Gaussian Splatting techniques. It integrates seamlessly with PyTorch through Python bindings and utilizes optimized CUDA kernels for enhanced performance. The library boasts significant improvements in optimization, resulting in faster training times and reduced memory usage compared to previous implementations. With its active maintenance and community contributions, gsplat is a valuable tool for researchers working with Gaussian Splatting methods.'}, 'zh': {'title': '高效训练高斯点云的开源工具', 'desc': 'gsplat是一个开源库，专门用于训练和开发高斯点云方法。它具有与PyTorch库兼容的Python绑定前端和高度优化的CUDA内核后端。gsplat提供了许多功能，提升了高斯点云模型的优化效果，包括速度、内存和收敛时间的改进。实验结果表明，gsplat的训练时间比原始实现减少了10%，内存使用减少了4倍。'}}}, {'id': 'https://huggingface.co/papers/2409.07450', 'title': 'VMAS: Video-to-Music Generation via Semantic Alignment in Web Music Videos', 'url': 'https://huggingface.co/papers/2409.07450', 'abstract': 'We present a framework for learning to generate background music from video inputs. Unlike existing works that rely on symbolic musical annotations, which are limited in quantity and diversity, our method leverages large-scale web videos accompanied by background music. This enables our model to learn to generate realistic and diverse music. To accomplish this goal, we develop a generative video-music Transformer with a novel semantic video-music alignment scheme. Our model uses a joint autoregressive and contrastive learning objective, which encourages the generation of music aligned with high-level video content. We also introduce a novel video-beat alignment scheme to match the generated music beats with the low-level motions in the video. Lastly, to capture fine-grained visual cues in a video needed for realistic background music generation, we introduce a new temporal video encoder architecture, allowing us to efficiently process videos consisting of many densely sampled frames. We train our framework on our newly curated DISCO-MV dataset, consisting of 2.2M video-music samples, which is orders of magnitude larger than any prior datasets used for video music generation. Our method outperforms existing approaches on the DISCO-MV and MusicCaps datasets according to various music generation evaluation metrics, including human evaluation. Results are available at https://genjib.github.io/project_page/VMAs/index.html', 'score': 10, 'issue_id': 1, 'pub_date': '2024-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '49f4a99bdddf4b22', 'data': {'categories': ['#video', '#audio', '#dataset', '#training', '#transfer_learning', '#games', '#diffusion', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🎵', 'ru': {'title': 'Генерация фоновой музыки по видео с помощью ИИ', 'desc': 'Представлена новая система для генерации фоновой музыки на основе видеоряда. В отличие от существующих подходов, использующих символьные музыкальные аннотации, данный метод обучается на масштабном датасете веб-видео с фоновой музыкой. Разработана генеративная видео-музыкальная модель на основе трансформера с новой схемой семантического выравнивания видео и музыки. Модель использует совместную авторегрессионную и контрастивную функцию потерь для генерации музыки, соответствующей содержанию видео. Также предложена новая архитектура временного видеокодировщика для эффективной обработки видео с большим количеством кадров.'}, 'en': {'title': 'Transforming Video into Music: A New Era of Generative Sound', 'desc': "This paper introduces a new framework for generating background music from video inputs using a generative video-music Transformer. Unlike previous methods that depend on limited symbolic musical annotations, this approach utilizes a large dataset of web videos with accompanying music, allowing for more realistic and diverse music generation. The model employs a joint autoregressive and contrastive learning objective to ensure that the generated music aligns with the high-level content of the video, while a novel video-beat alignment scheme synchronizes music beats with the video's motion. The framework is trained on the DISCO-MV dataset, which contains 2.2 million video-music pairs, significantly enhancing the model's performance over existing methods."}, 'zh': {'title': '从视频生成多样化背景音乐的创新框架', 'desc': '本文提出了一种从视频输入生成背景音乐的学习框架。与依赖于有限符号音乐注释的现有方法不同，我们的方法利用了大量带有背景音乐的网络视频。我们开发了一种生成性视频音乐Transformer，并引入了新颖的语义视频音乐对齐方案，以生成真实且多样的音乐。通过在DISCO-MV数据集上训练，我们的方法在音乐生成评估指标上超越了现有的技术。'}}}, {'id': 'https://huggingface.co/papers/2409.07441', 'title': 'Instant Facial Gaussians Translator for Relightable and Interactable Facial Rendering', 'url': 'https://huggingface.co/papers/2409.07441', 'abstract': 'We propose GauFace, a novel Gaussian Splatting representation, tailored for efficient animation and rendering of physically-based facial assets. Leveraging strong geometric priors and constrained optimization, GauFace ensures a neat and structured Gaussian representation, delivering high fidelity and real-time facial interaction of 30fps@1440p on a Snapdragon 8 Gen 2 mobile platform.   Then, we introduce TransGS, a diffusion transformer that instantly translates physically-based facial assets into the corresponding GauFace representations. Specifically, we adopt a patch-based pipeline to handle the vast number of Gaussians effectively. We also introduce a novel pixel-aligned sampling scheme with UV positional encoding to ensure the throughput and rendering quality of GauFace assets generated by our TransGS. Once trained, TransGS can instantly translate facial assets with lighting conditions to GauFace representation, With the rich conditioning modalities, it also enables editing and animation capabilities reminiscent of traditional CG pipelines.   We conduct extensive evaluations and user studies, compared to traditional offline and online renderers, as well as recent neural rendering methods, which demonstrate the superior performance of our approach for facial asset rendering. We also showcase diverse immersive applications of facial assets using our TransGS approach and GauFace representation, across various platforms like PCs, phones and even VR headsets.', 'score': 10, 'issue_id': 1, 'pub_date': '2024-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '85cf4cf691dd0f42', 'data': {'categories': ['#video', '#cv', '#inference', '#optimization', '#games', '#diffusion', '#architecture', '#synthetic', '#3d'], 'emoji': '🎭', 'ru': {'title': 'Революция в реалистичной анимации лиц на мобильных устройствах', 'desc': 'GauFace - это новая модель представления лиц на основе гауссовского спаттинга для эффективной анимации и рендеринга. Модель обеспечивает высокое качество и рендеринг в реальном времени на мобильных устройствах. TransGS - это трансформер на основе диффузии, который мгновенно переводит физически-обоснованные лицевые ассеты в представление GauFace. Авторы провели обширные оценки и исследования, демонстрирующие превосходную производительность их подхода для рендеринга лицевых ассетов.'}, 'en': {'title': 'Revolutionizing Facial Animation with GauFace and TransGS', 'desc': 'GauFace is a new method for efficiently animating and rendering facial assets using a Gaussian representation. It combines geometric principles and optimization techniques to achieve high-quality facial interactions at 30 frames per second and 1440p resolution on mobile devices. The TransGS model translates traditional facial assets into the GauFace format quickly, utilizing a patch-based approach to manage numerous Gaussians effectively. This system not only enhances rendering quality but also allows for real-time editing and animation, making it suitable for various platforms including PCs and VR headsets.'}, 'zh': {'title': '高效面部资产渲染的新方法', 'desc': '本文提出了一种新颖的高斯点云表示方法GauFace，旨在高效地动画和渲染基于物理的面部资产。通过利用强大的几何先验和约束优化，GauFace能够提供整洁的高斯表示，实现1440p@30fps的实时面部交互。我们还引入了TransGS，这是一种扩散变换器，可以快速将物理基础的面部资产转换为相应的GauFace表示。经过训练后，TransGS能够即时翻译面部资产，并支持丰富的编辑和动画功能，展现出优于传统渲染方法的性能。'}}}, {'id': 'https://huggingface.co/papers/2409.07440', 'title': 'SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories', 'url': 'https://huggingface.co/papers/2409.07440', 'abstract': 'Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPERaims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.', 'score': 6, 'issue_id': 1, 'pub_date': '2024-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': 'b39cd75f241daad2', 'data': {'categories': ['#science', '#survey', '#training', '#optimization', '#plp', '#benchmark'], 'emoji': '🧪', 'ru': {'title': 'SUPER: Новый вызов для языковых моделей в воспроизведении научных результатов', 'desc': 'Статья представляет SUPER - первый бенчмарк для оценки способности языковых моделей (LLM) воспроизводить результаты из исследовательских репозиториев. Бенчмарк включает 45 комплексных задач, 152 подзадачи и 602 автоматически сгенерированные проблемы в области машинного обучения и обработки естественного языка. Авторы вводят различные метрики для оценки успешности выполнения задач и прогресса моделей. Результаты показывают, что даже лучшая модель (GPT-4) решает только 16.3% комплексных задач, что подчеркивает сложность этой задачи и потенциал SUPER для дальнейших исследований.'}, 'en': {'title': 'SUPER: Benchmarking LLMs for Research Reproducibility', 'desc': 'This paper introduces SUPER, a benchmark designed to evaluate the ability of Large Language Models (LLMs) to autonomously reproduce results from research repositories in Machine Learning (ML) and Natural Language Processing (NLP). SUPER consists of three problem sets: end-to-end problems with expert solutions, sub-problems focusing on specific challenges, and automatically generated problems for larger-scale development. The evaluation measures assess task success and progress, revealing that even the best models, like GPT-4o, struggle with these tasks, achieving only 16.3% success on end-to-end problems. This highlights the complexity of the task and positions SUPER as a crucial tool for the research community to track advancements in LLM capabilities.'}, 'zh': {'title': 'SUPER：评估大型语言模型在研究中的能力', 'desc': '本文介绍了SUPER，这是第一个旨在评估大型语言模型（LLMs）在研究库中设置和执行任务能力的基准。SUPER包含三个不同的问题集，涵盖了从完整问题到特定挑战的子问题，以及自动生成的大规模问题。研究表明，当前最先进的模型在解决这些问题时表现不佳，最好的模型仅解决了16.3%的完整问题集。通过这些挑战，SUPER为研究社区提供了一个有价值的资源，以便衡量和推动进展。'}}}, {'id': 'https://huggingface.co/papers/2409.06744', 'title': 'ProteinBench: A Holistic Evaluation of Protein Foundation Models', 'url': 'https://huggingface.co/papers/2409.06744', 'abstract': 'Recent years have witnessed a surge in the development of protein foundation models, significantly improving performance in protein prediction and generative tasks ranging from 3D structure prediction and protein design to conformational dynamics. However, the capabilities and limitations associated with these models remain poorly understood due to the absence of a unified evaluation framework. To fill this gap, we introduce ProteinBench, a holistic evaluation framework designed to enhance the transparency of protein foundation models. Our approach consists of three key components: (i) A taxonomic classification of tasks that broadly encompass the main challenges in the protein domain, based on the relationships between different protein modalities; (ii) A multi-metric evaluation approach that assesses performance across four key dimensions: quality, novelty, diversity, and robustness; and (iii) In-depth analyses from various user objectives, providing a holistic view of model performance. Our comprehensive evaluation of protein foundation models reveals several key findings that shed light on their current capabilities and limitations. To promote transparency and facilitate further research, we release the evaluation dataset, code, and a public leaderboard publicly for further analysis and a general modular toolkit. We intend for ProteinBench to be a living benchmark for establishing a standardized, in-depth evaluation framework for protein foundation models, driving their development and application while fostering collaboration within the field.', 'score': 6, 'issue_id': 1, 'pub_date': '2024-09-10', 'pub_date_card': {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'}, 'hash': 'd848de93bbe4fa3d', 'data': {'categories': ['#science', '#dataset', '#training', '#healthcare', '#graphs', '#benchmark', '#open_source', '#multimodal', '#3d'], 'emoji': '🧬', 'ru': {'title': 'ProteinBench: новый стандарт оценки моделей белков', 'desc': 'В статье представлен ProteinBench - комплексная система оценки моделей белков. Она включает таксономическую классификацию задач, многомерный подход к оценке качества и глубокий анализ с разных точек зрения. ProteinBench позволяет лучше понять возможности и ограничения современных фундаментальных моделей белков. Авторы публикуют датасет, код и таблицу лидеров для дальнейших исследований в этой области.'}, 'en': {'title': 'ProteinBench: A New Standard for Evaluating Protein Models', 'desc': 'This paper introduces ProteinBench, a new evaluation framework for protein foundation models that enhances understanding of their capabilities and limitations. It includes a classification of protein tasks, a multi-metric evaluation system focusing on quality, novelty, diversity, and robustness, and detailed analyses based on user objectives. The framework aims to provide a comprehensive assessment of model performance in various protein-related tasks. By releasing the evaluation dataset and tools, the authors hope to promote transparency and collaboration in the field of protein modeling.'}, 'zh': {'title': 'ProteinBench：提升蛋白质模型透明度的评估框架', 'desc': '近年来，蛋白质基础模型的发展显著提升了蛋白质预测和生成任务的性能，包括三维结构预测和蛋白质设计等。然而，由于缺乏统一的评估框架，这些模型的能力和局限性仍然不够清晰。为了解决这个问题，我们提出了ProteinBench，这是一个全面的评估框架，旨在提高蛋白质基础模型的透明度。我们的框架包括任务分类、多指标评估和用户目标分析，帮助研究人员更好地理解模型的表现。'}}}, {'id': 'https://huggingface.co/papers/2409.06762', 'title': 'Generative Hierarchical Materials Search', 'url': 'https://huggingface.co/papers/2409.06762', 'abstract': 'Generative models trained at scale can now produce text, video, and more recently, scientific data such as crystal structures. In applications of generative approaches to materials science, and in particular to crystal structures, the guidance from the domain expert in the form of high-level instructions can be essential for an automated system to output candidate crystals that are viable for downstream research. In this work, we formulate end-to-end language-to-structure generation as a multi-objective optimization problem, and propose Generative Hierarchical Materials Search (GenMS) for controllable generation of crystal structures. GenMS consists of (1) a language model that takes high-level natural language as input and generates intermediate textual information about a crystal (e.g., chemical formulae), and (2) a diffusion model that takes intermediate information as input and generates low-level continuous value crystal structures. GenMS additionally uses a graph neural network to predict properties (e.g., formation energy) from the generated crystal structures. During inference, GenMS leverages all three components to conduct a forward tree search over the space of possible structures. Experiments show that GenMS outperforms other alternatives of directly using language models to generate structures both in satisfying user request and in generating low-energy structures. We confirm that GenMS is able to generate common crystal structures such as double perovskites, or spinels, solely from natural language input, and hence can form the foundation for more complex structure generation in near future.', 'score': 6, 'issue_id': 1, 'pub_date': '2024-09-10', 'pub_date_card': {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'}, 'hash': 'b81886f8007854f3', 'data': {'categories': ['#science', '#cv', '#training', '#graphs', '#inference', '#optimization', '#diffusion', '#architecture', '#multimodal', '#3d'], 'emoji': '💎', 'ru': {'title': 'GenMS: от слов к кристаллам с помощью ИИ', 'desc': 'Статья представляет GenMS - новый подход к генерации кристаллических структур на основе естественного языка. GenMS использует языковую модель для создания промежуточной текстовой информации о кристалле и диффузионную модель для генерации самой структуры. Система также включает графовую нейронную сеть для предсказания свойств сгенерированных кристаллов. Эксперименты показывают, что GenMS превосходит альтернативные методы в удовлетворении пользовательских запросов и генерации низкоэнергетических структур.'}, 'en': {'title': 'Transforming Language into Crystal Structures with GenMS', 'desc': 'This paper presents a novel approach called Generative Hierarchical Materials Search (GenMS) for generating crystal structures from natural language descriptions. It treats the generation process as a multi-objective optimization problem, integrating a language model, a diffusion model, and a graph neural network to produce viable crystal candidates. The language model interprets high-level instructions to create intermediate textual data, while the diffusion model translates this data into detailed crystal structures. GenMS demonstrates superior performance in generating low-energy structures and fulfilling user requests compared to traditional methods, paving the way for advanced materials discovery.'}, 'zh': {'title': '从语言到晶体结构的智能生成', 'desc': '本研究提出了一种名为生成层次材料搜索（GenMS）的新方法，用于从自然语言生成晶体结构。该方法将语言到结构的生成视为一个多目标优化问题，结合了语言模型、扩散模型和图神经网络。通过高层次的自然语言输入，GenMS能够生成晶体的中间文本信息，并进一步生成低级的连续值晶体结构。实验结果表明，GenMS在满足用户需求和生成低能量结构方面优于直接使用语言模型的其他方法。'}}}, {'id': 'https://huggingface.co/papers/2409.07129', 'title': 'MVLLaVA: An Intelligent Agent for Unified and Flexible Novel View Synthesis', 'url': 'https://huggingface.co/papers/2409.07129', 'abstract': 'This paper introduces MVLLaVA, an intelligent agent designed for novel view synthesis tasks. MVLLaVA integrates multiple multi-view diffusion models with a large multimodal model, LLaVA, enabling it to handle a wide range of tasks efficiently. MVLLaVA represents a versatile and unified platform that adapts to diverse input types, including a single image, a descriptive caption, or a specific change in viewing azimuth, guided by language instructions for viewpoint generation. We carefully craft task-specific instruction templates, which are subsequently used to fine-tune LLaVA. As a result, MVLLaVA acquires the capability to generate novel view images based on user instructions, demonstrating its flexibility across diverse tasks. Experiments are conducted to validate the effectiveness of MVLLaVA, demonstrating its robust performance and versatility in tackling diverse novel view synthesis challenges.', 'score': 6, 'issue_id': 1, 'pub_date': '2024-09-11', 'pub_date_card': {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'}, 'hash': '305ecdca1f5f10c0', 'data': {'categories': ['#cv', '#training', '#agents', '#diffusion', '#synthetic', '#multimodal', '#3d'], 'emoji': '🔄', 'ru': {'title': 'Умный синтез ракурсов: MVLLaVA объединяет мультимодальный ИИ и диффузионные модели', 'desc': 'MVLLaVA - это интеллектуальный агент для синтеза новых ракурсов изображений. Он объединяет мультимодальную языковую модель LLaVA с несколькими моделями диффузии для работы с множественными ракурсами. MVLLaVA может генерировать новые ракурсы на основе одного изображения, текстового описания или указания на изменение угла обзора. Система демонстрирует гибкость и эффективность в решении разнообразных задач синтеза новых ракурсов.'}, 'en': {'title': 'MVLLaVA: Your Versatile Agent for Novel View Generation', 'desc': 'MVLLaVA is an advanced intelligent agent that focuses on generating new views of images using multiple diffusion models combined with a large multimodal model called LLaVA. It can process various types of inputs, such as images, captions, or specific changes in viewpoint, all guided by user instructions. The system is fine-tuned with specially designed instruction templates to enhance its ability to create novel view images. Experiments show that MVLLaVA performs well across different tasks, proving its adaptability and effectiveness in novel view synthesis.'}, 'zh': {'title': 'MVLLaVA：灵活的新视角合成智能代理', 'desc': '本文介绍了MVLLaVA，一个用于新视角合成任务的智能代理。MVLLaVA结合了多个多视角扩散模型和大型多模态模型LLaVA，使其能够高效处理各种任务。它能够适应多种输入类型，包括单张图像、描述性标题或特定的视角变化，并通过语言指令生成视角。通过精心设计的任务特定指令模板，MVLLaVA能够根据用户指令生成新视角图像，展示了其在多样化任务中的灵活性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (6)', '#agents (3)', '#agi (1)', '#alignment (2)', '#architecture (9)', '#audio (1)', '#benchmark (6)', '#cv (4)', '#data', '#dataset (4)', '#diffusion (5)', '#ethics (1)', '#games (3)', '#graphs (2)', '#hallucinations (1)', '#healthcare (2)', '#inference (5)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (4)', '#open_source (4)', '#optimization (5)', '#plp (1)', '#rag (1)', '#reasoning (4)', '#rl', '#rlhf', '#robotics', '#science (4)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (3)', '#training (10)', '#transfer_learning (2)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <img class="article-pdf-title-img" src="${pdfImg}" />
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-09-12 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-09-12 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-09-12 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    