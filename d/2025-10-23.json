{
    "date": {
        "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 23",
        "zh": "10æœˆ23æ—¥"
    },
    "time_utc": "2025-10-23 09:14",
    "weekday": 3,
    "issue_id": 6574,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.19338",
            "title": "Every Attention Matters: An Efficient Hybrid Architecture for\n  Long-Context Reasoning",
            "url": "https://huggingface.co/papers/2510.19338",
            "abstract": "The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks.",
            "score": 55,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "bf307f4447578ac2",
            "authors": [
                "Ling Team",
                "Bin Han",
                "Caizhi Tang",
                "Chen Liang",
                "Donghao Zhang",
                "Fan Yuan",
                "Feng Zhu",
                "Jie Gao",
                "Jingyu Hu",
                "Longfei Li",
                "Meng Li",
                "Mingyang Zhang",
                "Peijie Jiang",
                "Peng Jiao",
                "Qian Zhao",
                "Qingyuan Yang",
                "Wenbo Shen",
                "Xinxing Yang",
                "Yalin Zhang",
                "Yankun Ren",
                "Yao Zhao",
                "Yibo Cao",
                "Yixuan Sun",
                "Yue Zhang",
                "Yuchen Fang",
                "Zibin Lin",
                "Zixuan Cheng",
                "Jun Zhou"
            ],
            "affiliations": [
                "Ling Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19338.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#long_context",
                    "#benchmark",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ’",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…",
                    "desc": "Ğ¡ĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ring-linear Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¸ softmax Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ring-mini-linear-2.0 (16B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¸ Ring-flash-linear-2.0 (104B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ inference Ğ² 10 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ dense Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ FP8 Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ¾ÑĞ»Ğ° Ğ½Ğ° 50%. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ SOTA Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… reasoning Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ„Ğ°Ğ·Ğµ reinforcement learning."
                },
                "en": {
                    "title": "Efficient Inference with Hybrid Attention Models",
                    "desc": "The Ring-linear model series introduces two advanced models, Ring-mini-linear-2.0 and Ring-flash-linear-2.0, which utilize a hybrid architecture that combines linear and softmax attention mechanisms. This innovative approach significantly reduces inference costs and enhances training efficiency, making it suitable for long-context scenarios. With 16B and 104B parameters respectively, these models achieve a remarkable reduction in computational overhead, cutting inference costs to one-tenth of a comparable dense model. Additionally, the integration of a high-performance FP8 operator library has led to a 50% improvement in training efficiency, ensuring that the models maintain state-of-the-art performance across various complex reasoning tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ä¸è®­ç»ƒçš„æ··åˆæ¶æ„",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†Ring-linearæ¨¡å‹ç³»åˆ—ï¼ŒåŒ…æ‹¬Ring-mini-linear-2.0å’ŒRing-flash-linear-2.0ã€‚è¿™äº›æ¨¡å‹é‡‡ç”¨æ··åˆæ¶æ„ï¼Œç»“åˆäº†çº¿æ€§æ³¨æ„åŠ›å’Œsoftmaxæ³¨æ„åŠ›ï¼Œæ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬å¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚Ring-mini-linear-2.0æ‹¥æœ‰160äº¿å‚æ•°ï¼Œè€ŒRing-flash-linear-2.0åˆ™æœ‰1040äº¿å‚æ•°ï¼ŒäºŒè€…åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡ä¼˜åŒ–ä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„æ¯”ä¾‹ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†å½“å‰æœ€ä½³çš„æ¨¡å‹ç»“æ„ï¼Œå¹¶åˆ©ç”¨è‡ªç ”çš„é«˜æ€§èƒ½FP8è¿ç®—åº“æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18927",
            "title": "BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via\n  Balanced Policy Optimization with Adaptive Clipping",
            "url": "https://huggingface.co/papers/2510.18927",
            "abstract": "BAlanced Policy Optimization with Adaptive Clipping (BAPO) addresses challenges in off-policy reinforcement learning by dynamically adjusting clipping bounds to improve sample efficiency, stability, and performance in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settings--where stale data from past policies are used for training--improves sample efficiency, but remains challenging: policy entropy declines sharply, optimization often becomes unstable and may even collapse. Through theoretical and empirical analysis, we identify two key insights: (i) an imbalance in optimization, where negative-advantage samples dominate the policy gradient, suppressing useful behaviors and risking gradient explosions; and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping mechanism in PPO-like objectives systematically blocks entropy-increasing updates, thereby driving the policy toward over-exploitation at the expense of exploration. Building on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), a simple yet effective method that dynamically adjusts clipping bounds to adaptively re-balance positive and negative contributions, preserve entropy, and stabilize RL optimization. Across diverse off-policy scenarios--including sample replay and partial rollout--BAPO achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025 benchmarks, our 7B BAPO model surpasses open-source counterparts such as SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art results among models of the same scale but also outperforms leading proprietary systems like o3-mini and Gemini-2.5-Flash-Thinking.",
            "score": 53,
            "issue_id": 6569,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "968dccb2af4a1217",
            "authors": [
                "Zhiheng Xi",
                "Xin Guo",
                "Yang Nan",
                "Enyu Zhou",
                "Junrui Shen",
                "Wenxiang Chen",
                "Jiaqi Liu",
                "Jixuan Huang",
                "Zhihao Zhang",
                "Honglin Guo",
                "Xun Deng",
                "Zhikai Lei",
                "Miao Zheng",
                "Guoteng Wang",
                "Shuo Zhang",
                "Peng Sun",
                "Rui Zheng",
                "Hang Yan",
                "Tao Gui",
                "Qi Zhang",
                "Xuanjing Huang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Innovation Institute",
                "Shanghai Qiji Zhifeng Co., Ltd."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18927.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· RL",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ BAPO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ğ² off-policy Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ clipping Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ² PPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ entropy Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ exploration. BAPO Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ clipping bounds, Ñ‡Ñ‚Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞºĞ»Ğ°Ğ´Ñ‹ Ğ² Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… AIME 2024/2025, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ ĞºĞ°Ğº open-source Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ o3-mini."
                },
                "en": {
                    "title": "Dynamic Clipping for Enhanced Reinforcement Learning Stability",
                    "desc": "BAlanced Policy Optimization with Adaptive Clipping (BAPO) enhances off-policy reinforcement learning by dynamically modifying clipping bounds to improve training efficiency and stability. The paper identifies issues like negative-advantage samples dominating the policy gradient, which can lead to poor performance and instability. It introduces the Entropy-Clip Rule, highlighting how fixed clipping mechanisms can hinder exploration by favoring over-exploitation. BAPO effectively addresses these challenges, resulting in faster and more efficient training across various scenarios, outperforming existing models on benchmark tests."
                },
                "zh": {
                    "title": "åŠ¨æ€è£å‰ªï¼Œå¹³è¡¡ä¼˜åŒ–ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡",
                    "desc": "BAPOï¼ˆå¹³è¡¡ç­–ç•¥ä¼˜åŒ–ä¸è‡ªé€‚åº”è£å‰ªï¼‰è§£å†³äº†ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸­çš„æŒ‘æˆ˜ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´è£å‰ªè¾¹ç•Œæ¥æé«˜æ ·æœ¬æ•ˆç‡ã€ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚è¯¥æ–¹æ³•è¯†åˆ«å‡ºä¼˜åŒ–ä¸­çš„ä¸å¹³è¡¡ç°è±¡ï¼Œè´Ÿä¼˜åŠ¿æ ·æœ¬ä¸»å¯¼ç­–ç•¥æ¢¯åº¦ï¼ŒæŠ‘åˆ¶æœ‰ç”¨è¡Œä¸ºå¹¶å¯èƒ½å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ã€‚åŒæ—¶ï¼ŒBAPOå¼•å…¥äº†ç†µè£å‰ªè§„åˆ™ï¼Œæ­ç¤ºäº†å›ºå®šè£å‰ªæœºåˆ¶å¦‚ä½•é˜»ç¢ç†µå¢åŠ çš„æ›´æ–°ï¼Œå¯¼è‡´ç­–ç•¥è¿‡åº¦å¼€å‘è€Œå¿½è§†æ¢ç´¢ã€‚é€šè¿‡åœ¨å¤šç§ç¦»çº¿åœºæ™¯ä¸­åº”ç”¨ï¼ŒBAPOå®ç°äº†å¿«é€Ÿã€ç¨³å®šå’Œæ•°æ®é«˜æ•ˆçš„è®­ç»ƒï¼Œè¶…è¶Šäº†å¤šä¸ªå¼€æºå’Œå•†ä¸šæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19363",
            "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts",
            "url": "https://huggingface.co/papers/2510.19363",
            "abstract": "LoongRL, a data-driven reinforcement learning method, enhances long-context reasoning by transforming short multi-hop QA into high-difficulty tasks, improving accuracy and generalization in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning over long contexts is essential for large language models. While reinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\" moments in chain-of-thought, the advanced thinking patterns required for long-context reasoning remain largely unexplored, and high-difficulty RL data are scarce. In this paper, we introduce LoongRL, a data-driven RL method for advanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis approach that transforms short multi-hop QA into high-difficulty long-context tasks by inserting UUID chains that hide the true question among large collections of distracting documents. Solving these tasks requires the model to trace the correct chain step-by-step, identify the true question, retrieve relevant facts and reason over them to answer correctly. RL training on KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning pattern that generalizes far beyond training length. Models trained at 16K effectively solve 128K tasks without prohibitive full-length RL rollout costs. On Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA accuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches a score of 74.2, rivaling much larger frontier models such as o3-mini (74.5) and DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all 128K needle-in-a-haystack stress tests, and preserves short-context reasoning capabilities.",
            "score": 30,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "c9f40dde34631067",
            "authors": [
                "Siyuan Wang",
                "Gaokai Zhang",
                "Li Lyna Zhang",
                "Ning Shang",
                "Fan Yang",
                "Dongyao Chen",
                "Mao Yang"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Microsoft Research Asia",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19363.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ UUID",
                    "desc": "LoongRL â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ reinforcement learning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ â€” Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ multi-hop Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ UUID ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½ Â«Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞ¹-Ğ¸Ñ‰Ğ¸-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ğ¹-Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞ¹Â», ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ´Ğ°Ğ»ĞµĞºĞ¾ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen2.5 Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ°Ñ‘Ñ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ +23.5% Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ 14B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ frontier Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ o3-mini Ğ¸ DeepSeek-R1."
                },
                "en": {
                    "title": "Unlocking Long-Context Reasoning with LoongRL",
                    "desc": "LoongRL is a novel reinforcement learning method designed to enhance long-context reasoning in large language models. It transforms short multi-hop question-answering tasks into more complex challenges by using UUID chains to obscure the actual question among distracting information. This approach encourages models to develop a systematic reasoning pattern that involves planning, retrieving, and verifying information, which significantly improves their performance on long-context tasks. The results show that models trained with LoongRL achieve substantial accuracy gains and can handle a much larger set of tasks than previously possible, while still maintaining their short-context reasoning abilities."
                },
                "zh": {
                    "title": "LoongRLï¼šæå‡é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•",
                    "desc": "LoongRLæ˜¯ä¸€ç§æ•°æ®é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºé•¿ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡å°†çŸ­å¤šè·³é—®ç­”è½¬åŒ–ä¸ºé«˜éš¾åº¦ä»»åŠ¡ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚LoongRLçš„æ ¸å¿ƒæ˜¯KeyChainæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ’å…¥UUIDé“¾æ¥éšè—çœŸå®é—®é¢˜ï¼Œä»è€Œåœ¨å¤§é‡å¹²æ‰°æ–‡æ¡£ä¸­ç”Ÿæˆé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ã€‚ç»è¿‡KeyChainæ•°æ®çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿå½¢æˆæœ‰æ•ˆçš„æ¨ç†æ¨¡å¼ï¼Œæ˜¾è‘—æé«˜äº†å¤šè·³é—®ç­”çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19430",
            "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
            "url": "https://huggingface.co/papers/2510.19430",
            "abstract": "GigaBrain-0, a VLA foundation model, uses world model-generated data to enhance cross-task generalization and policy robustness, improving real-world performance on complex manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Training Vision-Language-Action (VLA) models for generalist robots typically requires large-scale real-world robot data, which is expensive and time-consuming to collect. The inefficiency of physical data collection severely limits the scalability, and generalization capacity of current VLA systems. To address this challenge, we introduce GigaBrain-0, a novel VLA foundation model empowered by world model-generated data (e.g., video generation, real2real transfer, human transfer, view transfer, sim2real transfer data). By leveraging world models to generate diverse data at scale, GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization. Our approach further improves policy robustness through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision, enabling the model to reason about spatial geometry, object states, and long-horizon dependencies during task execution. This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves superior generalization across variations in appearances (e.g., textures, colors), object placements, and camera viewpoints. Additionally, we present GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently on devices such as the NVIDIA Jetson AGX Orin.",
            "score": 24,
            "issue_id": 6570,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "3079040be10dd49e",
            "authors": [
                "GigaBrain Team",
                "Angen Ye",
                "Boyuan Wang",
                "Chaojun Ni",
                "Guan Huang",
                "Guosheng Zhao",
                "Haoyun Li",
                "Jie Li",
                "Jiagang Zhu",
                "Lv Feng",
                "Peng Li",
                "Qiuping Deng",
                "Runqi Ouyang",
                "Wenkang Qin",
                "Xinze Chen",
                "Xiaofeng Wang",
                "Yang Wang",
                "Yifan Li",
                "Yilong Li",
                "Yiran Ding",
                "Yuan Xu",
                "Yun Ye",
                "Yukun Zhou",
                "Zhehao Dong",
                "Zhenan Wang",
                "Zhichao Liu",
                "Zheng Zhu"
            ],
            "affiliations": [
                "GigaAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19430.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#agi",
                    "#transfer_learning",
                    "#agents",
                    "#optimization",
                    "#3d",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· world model",
                    "desc": "GigaBrain-0 - ÑÑ‚Ğ¾ VLA-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (Vision-Language-Action) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ world model, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ€ĞµĞ´Ğ°Ğ¼Ğ¸ (sim2real, real2real) Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑ‹ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ RGBD-Ğ²Ñ…Ğ¾Ğ´ Ğ¸ embodied Chain-of-Thought reasoning, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° ĞºĞ°Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "GigaBrain-0: Revolutionizing Robot Learning with World Model Data",
                    "desc": "GigaBrain-0 is a new foundation model for Vision-Language-Action (VLA) that enhances the performance of robots in complex tasks by using data generated from world models. This approach reduces the need for expensive real-world data collection, allowing for better scalability and generalization across different tasks. By incorporating techniques like RGBD input modeling and Chain-of-Thought supervision, GigaBrain-0 improves the model's ability to understand spatial relationships and long-term dependencies. The model has shown significant improvements in real-world manipulation tasks, demonstrating its effectiveness in various scenarios and conditions."
                },
                "zh": {
                    "title": "GigaBrain-0ï¼šæå‡æœºå™¨äººä»»åŠ¡æ³›åŒ–ä¸é²æ£’æ€§çš„åˆ›æ–°æ¨¡å‹",
                    "desc": "GigaBrain-0æ˜¯ä¸€ç§æ–°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„æ•°æ®æ¥å¢å¼ºè·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›å’Œç­–ç•¥çš„é²æ£’æ€§ï¼Œä»è€Œæé«˜å¤æ‚æ“ä½œä»»åŠ¡çš„å®é™…è¡¨ç°ã€‚è¯¥æ¨¡å‹é€šè¿‡ç”Ÿæˆå¤šæ ·åŒ–çš„æ•°æ®ï¼Œæ˜¾è‘—å‡å°‘äº†å¯¹çœŸå®æœºå™¨äººæ•°æ®çš„ä¾èµ–ï¼ŒåŒæ—¶æ”¹å–„äº†æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡é—´çš„æ³›åŒ–èƒ½åŠ›ã€‚GigaBrain-0è¿˜é€šè¿‡RGBDè¾“å…¥å»ºæ¨¡å’Œå…·èº«çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ç›‘ç£ï¼Œæå‡äº†ç­–ç•¥çš„é²æ£’æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä»»åŠ¡æ‰§è¡Œä¸­æ¨ç†ç©ºé—´å‡ ä½•ã€ç‰©ä½“çŠ¶æ€å’Œé•¿æœŸä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGigaBrain-0åœ¨å¤–è§‚å˜åŒ–ã€ç‰©ä½“æ”¾ç½®å’Œç›¸æœºè§†è§’ç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19488",
            "title": "VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos",
            "url": "https://huggingface.co/papers/2510.19488",
            "abstract": "VideoAgentTrek automatically extracts GUI interaction data from YouTube videos using Video2Action, an inverse dynamics module, improving task success rates and step accuracy for computer-use agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Training computer-use agents requires massive amounts of GUI interaction data, but manually annotating action trajectories at scale is prohibitively expensive. We present VideoAgentTrek, a scalable pipeline that automatically mines training data from publicly available screen-recorded videos at web scale, eliminating the need for manual annotation. Our approach addresses a key challenge: raw videos contain implicit demonstrations but lack explicit action labels. To solve this, we develop Video2Action, an inverse dynamics module (IDM) with two components: (1) a video grounding model that detects and localizes GUI actions with precise temporal boundaries and context, and (2) an action-content recognizer that extracts structured parameters like click coordinates and typed text with high fidelity. Applied to 39,000 YouTube tutorial videos, our pipeline generates 1.52 million interaction steps automatically. We leverage this data through continued pretraining followed by supervised fine-tuning. On OSWorld-Verified, our approach improves task success rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results demonstrate that passive internet videos can be transformed into high-quality supervision for computer-use agents, providing a scalable alternative to expensive manual annotation.",
            "score": 14,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "4237f40e97c8b930",
            "authors": [
                "Dunjie Lu",
                "Yiheng Xu",
                "Junli Wang",
                "Haoyuan Wu",
                "Xinyuan Wang",
                "Zekun Wang",
                "Junlin Yang",
                "Hongjin Su",
                "Jixuan Chen",
                "Junda Chen",
                "Yuchen Mao",
                "Jingren Zhou",
                "Junyang Lin",
                "Binyuan Hui",
                "Tao Yu"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Group",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19488.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#video",
                    "#data",
                    "#agents",
                    "#optimization",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° YouTube Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ VideoAgentTrek â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ GUI Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… YouTube-Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Video2Action, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ ĞºĞ»Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ²Ğ²ĞµĞ´Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ² 39 Ñ‚Ñ‹ÑÑÑ‡ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° 1.52 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 70% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transforming YouTube Videos into Training Gold for AI Agents",
                    "desc": "VideoAgentTrek is a novel system that automatically extracts graphical user interface (GUI) interaction data from YouTube videos, addressing the challenge of obtaining large-scale training data for computer-use agents without manual annotation. It utilizes an inverse dynamics module called Video2Action, which includes a video grounding model to identify and time-stamp GUI actions, and an action-content recognizer to capture detailed parameters like click locations and text input. By applying this method to 39,000 tutorial videos, the system generated over 1.5 million interaction steps, significantly enhancing the training dataset. The results show a marked improvement in task success rates and step accuracy, demonstrating the effectiveness of using passive video content for training AI agents."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–æå–è§†é¢‘äº¤äº’æ•°æ®ï¼Œæå‡è®¡ç®—æœºä»£ç†æ€§èƒ½",
                    "desc": "VideoAgentTrek æ˜¯ä¸€ä¸ªè‡ªåŠ¨ä» YouTube è§†é¢‘ä¸­æå– GUI äº¤äº’æ•°æ®çš„ç³»ç»Ÿï¼Œä½¿ç”¨äº†é€†åŠ¨æ€æ¨¡å— Video2Actionï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„ä»»åŠ¡æˆåŠŸç‡å’Œæ­¥éª¤å‡†ç¡®æ€§ã€‚è¯¥ç³»ç»Ÿè§£å†³äº†æ‰‹åŠ¨æ ‡æ³¨å¤§é‡äº¤äº’æ•°æ®çš„é«˜æˆæœ¬é—®é¢˜ï¼Œé€šè¿‡ä»å…¬å¼€çš„å±å¹•å½•åˆ¶è§†é¢‘ä¸­è‡ªåŠ¨æŒ–æ˜è®­ç»ƒæ•°æ®ã€‚Video2Action åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šè§†é¢‘å®šä½æ¨¡å‹å’ŒåŠ¨ä½œå†…å®¹è¯†åˆ«å™¨ï¼Œèƒ½å¤Ÿç²¾ç¡®æ£€æµ‹å’Œæå– GUI æ“ä½œçš„æ—¶é—´è¾¹ç•Œå’Œç»“æ„åŒ–å‚æ•°ã€‚é€šè¿‡å¯¹ 39,000 ä¸ª YouTube æ•™ç¨‹è§†é¢‘çš„åº”ç”¨ï¼Œæˆ‘ä»¬çš„ç®¡é“è‡ªåŠ¨ç”Ÿæˆäº† 152 ä¸‡ä¸ªäº¤äº’æ­¥éª¤ï¼Œå±•ç¤ºäº†è¢«åŠ¨äº’è”ç½‘è§†é¢‘å¯ä»¥è½¬åŒ–ä¸ºé«˜è´¨é‡çš„è®¡ç®—æœºä½¿ç”¨ä»£ç†ç›‘ç£æ•°æ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19336",
            "title": "DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile\n  Phone Agents",
            "url": "https://huggingface.co/papers/2510.19336",
            "abstract": "DaMo, a trainable network optimizing data mixtures for Multimodal Large Language Models, enhances performance across various mobile phone tasks and benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Mobile Phone Agents (MPAs) have emerged as a promising research direction due to their broad applicability across diverse scenarios. While Multimodal Large Language Models (MLLMs) serve as the foundation for MPAs, their effectiveness in handling multiple mobile phone tasks simultaneously remains limited. Although multitask supervised fine-tuning (SFT) is widely adopted for multitask learning, existing approaches struggle to determine optimal training data compositions for peak performance. To address this challenge, we propose DaMo (Data Mixture Optimizer) - a novel solution employing a trainable network that predicts optimal data mixtures by forecasting downstream task performance for any given dataset ratio. To support comprehensive evaluation, we introduce PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse real-world industrial mobile application scenarios. Demonstrating strong predictive capability (R^2=0.81) in small-scale pilot experiments, DaMo efficiently extrapolates optimal data mixing configurations. Our results show DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to alternative methods. Furthermore, extensive experiments across established benchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench reveal DaMo's superior generalization, outperforming other approaches by 2.57% in terms of average score. When used solely for MLLM optimization on the BFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably, DaMo maintains robust scalability, preserving its effectiveness when applied to other model architectures. The code and dataset are available at https://github.com/OPPO-Mente-Lab/DaMo.git",
            "score": 14,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "4d330d1d597c2031",
            "authors": [
                "Kai Shi",
                "Jun Yang",
                "Ni Yang",
                "Binqiang Pan",
                "Qingsong Xie",
                "Chao Zhang",
                "Zhenyu Yang",
                "Tianhuang Su",
                "Haonan Lu"
            ],
            "affiliations": [
                "OPPO AI Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19336.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#training",
                    "#architecture",
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "DaMo: ÑƒĞ¼Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ°Ñ…",
                    "desc": "DaMo - ÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾ÑÑ‚Ğ°Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ multitask Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ RÂ²=0.81 Ğ² ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ PhoneAgentBench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ 1235 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. DaMo Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 3.38% Ğ½Ğ° PhoneAgentBench Ğ¸ Ğ½Ğ° 2.57% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Optimizing Data Mixtures for Enhanced Mobile Task Performance",
                    "desc": "DaMo is a novel trainable network designed to optimize data mixtures for Multimodal Large Language Models (MLLMs), specifically enhancing their performance on mobile phone tasks. It addresses the challenge of determining the best training data compositions for multitask learning, which traditional methods struggle with. By predicting optimal data mixtures based on expected task performance, DaMo demonstrates significant improvements in various benchmarks, including a 3.38% increase on PhoneAgentBench. Additionally, it shows strong generalization capabilities across established benchmarks, outperforming existing methods and maintaining effectiveness across different model architectures."
                },
                "zh": {
                    "title": "DaMoï¼šä¼˜åŒ–å¤šæ¨¡æ€ä»»åŠ¡çš„æ•°æ®ç»„åˆ",
                    "desc": "DaMoæ˜¯ä¸€ç§å¯è®­ç»ƒçš„ç½‘ç»œï¼Œæ—¨åœ¨ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„æ•°æ®ç»„åˆï¼Œä»è€Œæå‡åœ¨æ‰‹æœºä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œç¡®å®šæœ€ä½³çš„æ•°æ®æ··åˆæ¯”ä¾‹ï¼Œè§£å†³äº†å¤šä»»åŠ¡å­¦ä¹ ä¸­æ•°æ®ç»„åˆé€‰æ‹©çš„éš¾é¢˜ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†PhoneAgentBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è¯„ä¼°å¤šæ¨¡æ€æ‰‹æœºä»»åŠ¡çš„åŸºå‡†ï¼ŒåŒ…å«1235ä¸ªé—®ç­”å¯¹ï¼Œè¦†ç›–å¤šç§çœŸå®å·¥ä¸šåº”ç”¨åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDaMoåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨BFCL-v3ä»»åŠ¡ä¸Šæå‡äº†12.47%çš„æŒ‡æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19808",
            "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
            "url": "https://huggingface.co/papers/2510.19808",
            "abstract": "Pico-Banana-400K is a large-scale, high-quality dataset for instruction-based image editing, featuring diverse edit pairs, multi-turn editing, preference subsets, and long-short instruction pairs, enabling comprehensive research and benchmarking.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal models have demonstrated remarkable text-guided image editing capabilities, with systems like GPT-4o and Nano-Banana setting new benchmarks. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built from real images. We introduce Pico-Banana-400K, a comprehensive 400K-image dataset for instruction-based image editing. Our dataset is constructed by leveraging Nano-Banana to generate diverse edit pairs from real photographs in the OpenImages collection. What distinguishes Pico-Banana-400K from previous synthetic datasets is our systematic approach to quality and diversity. We employ a fine-grained image editing taxonomy to ensure comprehensive coverage of edit types while maintaining precise content preservation and instruction faithfulness through MLLM-based quality scoring and careful curation. Beyond single turn editing, Pico-Banana-400K enables research into complex editing scenarios. The dataset includes three specialized subsets: (1) a 72K-example multi-turn collection for studying sequential editing, reasoning, and planning across consecutive modifications; (2) a 56K-example preference subset for alignment research and reward model training; and (3) paired long-short editing instructions for developing instruction rewriting and summarization capabilities. By providing this large-scale, high-quality, and task-rich resource, Pico-Banana-400K establishes a robust foundation for training and benchmarking the next generation of text-guided image editing models.",
            "score": 11,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "19f1485a76e3707f",
            "authors": [
                "Yusu Qian",
                "Eli Bocek-Rivele",
                "Liangchen Song",
                "Jialing Tong",
                "Yinfei Yang",
                "Jiasen Lu",
                "Wenze Hu",
                "Zhe Gan"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19808.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#synthetic",
                    "#alignment",
                    "#reasoning"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Pico-Banana-400K: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Pico-Banana-400K â€” Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 400 Ñ‚Ñ‹ÑÑÑ‡ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½Ğ¾Ğº Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°Ğ¼. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğ¸Ğ· OpenImages Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Nano-Banana Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· MLLM. ĞŸĞ¾Ğ¼Ğ¸Ğ¼Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ‚Ñ€Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°: Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (72K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²), Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ reward-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (56K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²), Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ñ€ĞµÑÑƒÑ€Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ğ° ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ."
                },
                "en": {
                    "title": "Unlocking Advanced Image Editing with Pico-Banana-400K",
                    "desc": "Pico-Banana-400K is a new dataset designed for instruction-based image editing, containing 400,000 high-quality images. It features diverse edit pairs and supports complex editing tasks through multi-turn editing and preference subsets. The dataset is built from real images, ensuring high quality and relevance, and includes specialized subsets for various research needs. This resource aims to enhance the development and evaluation of advanced text-guided image editing models."
                },
                "zh": {
                    "title": "Pico-Banana-400Kï¼šå›¾åƒç¼–è¾‘çš„æ–°åŸºçŸ³",
                    "desc": "Pico-Banana-400Kæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å›¾åƒç¼–è¾‘æ•°æ®é›†ï¼Œä¸“æ³¨äºåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ã€‚è¯¥æ•°æ®é›†åŒ…å«å¤šæ ·çš„ç¼–è¾‘å¯¹ã€å¤šè½®ç¼–è¾‘å’Œåå¥½å­é›†ï¼Œæ”¯æŒå¤æ‚çš„ç¼–è¾‘åœºæ™¯ç ”ç©¶ã€‚é€šè¿‡åˆ©ç”¨Nano-Bananaç”ŸæˆçœŸå®ç…§ç‰‡çš„ç¼–è¾‘å¯¹ï¼ŒPico-Banana-400Kç¡®ä¿äº†ç¼–è¾‘ç±»å‹çš„å…¨é¢è¦†ç›–å’Œå†…å®¹çš„ç²¾ç¡®ä¿ç•™ã€‚è¿™ä¸ªæ•°æ®é›†ä¸ºä¸‹ä¸€ä»£æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ¨¡å‹çš„è®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•æä¾›äº†åšå®çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19307",
            "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models",
            "url": "https://huggingface.co/papers/2510.19307",
            "abstract": "A unified reinforcement and imitation learning algorithm creates efficient, lightweight vision-language models that match or exceed leading VLMs in performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have achieved remarkable progress, yet their large scale often renders them impractical for resource-constrained environments. This paper introduces Unified Reinforcement and Imitation Learning (RIL), a novel and efficient training algorithm designed to create powerful, lightweight VLMs. RIL distinctively combines the strengths of reinforcement learning with adversarial imitation learning. This enables smaller student VLMs not only to mimic the sophisticated text generation of large teacher models but also to systematically improve their generative capabilities through reinforcement signals. Key to our imitation framework is an LLM-based discriminator that adeptly distinguishes between student and teacher outputs, complemented by guidance from multiple large teacher VLMs to ensure diverse learning. This unified learning strategy, leveraging both reinforcement and imitation, empowers student models to achieve significant performance gains, making them competitive with leading closed-source VLMs. Extensive experiments on diverse vision-language benchmarks demonstrate that RIL significantly narrows the performance gap with state-of-the-art open- and closed-source VLMs and, in several instances, surpasses them.",
            "score": 9,
            "issue_id": 6568,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "9ddac8d5d67d2842",
            "authors": [
                "Byung-Kwan Lee",
                "Ryo Hachiuma",
                "Yong Man Ro",
                "Yu-Chiang Frank Wang",
                "Yueh-Hua Wu"
            ],
            "affiliations": [
                "KAIST",
                "NVIDIA",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19307.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#optimization",
                    "#rlhf",
                    "#cv",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ VLM ÑƒÑ‡Ğ°Ñ‚ÑÑ Ñƒ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· reinforcement Ğ¸ imitation learning",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Unified Reinforcement and Imitation Learning (RIL) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ reinforcement learning Ñ adversarial imitation learning, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğ¼ VLM Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ»ĞµĞ¶Ğ¸Ñ‚ LLM-Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ° Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… VLM Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RIL, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… open-source Ğ¸ closed-source VLM, Ğ° Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¸Ñ…."
                },
                "en": {
                    "title": "Efficient Learning for Powerful Vision-Language Models",
                    "desc": "This paper presents a new training algorithm called Unified Reinforcement and Imitation Learning (RIL) for creating efficient vision-language models (VLMs). RIL combines reinforcement learning and adversarial imitation learning to enable smaller models to learn from larger, more complex teacher models. By using a discriminator based on large language models (LLMs), the student models can effectively mimic and improve upon the text generation capabilities of their teachers. The results show that these lightweight models can perform comparably to, or even better than, existing state-of-the-art VLMs, making them suitable for environments with limited resources."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¼ºåŒ–ä¸æ¨¡ä»¿å­¦ä¹ ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ ä¸æ¨¡ä»¿å­¦ä¹ ç®—æ³•ï¼ˆRILï¼‰ï¼Œæ—¨åœ¨åˆ›å»ºé«˜æ•ˆä¸”è½»é‡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚è¯¥ç®—æ³•ç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’Œå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œä½¿å¾—å°å‹å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæ¨¡ä»¿å¤§å‹æ•™å¸ˆæ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¼ºåŒ–ä¿¡å·ç³»ç»Ÿæ€§åœ°æå‡å…¶ç”Ÿæˆèƒ½åŠ›ã€‚RILçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ¤åˆ«å™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†å­¦ç”Ÿå’Œæ•™å¸ˆçš„è¾“å‡ºï¼Œå¹¶é€šè¿‡å¤šä¸ªå¤§å‹æ•™å¸ˆVLMçš„æŒ‡å¯¼ç¡®ä¿å¤šæ ·åŒ–å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRILæ˜¾è‘—ç¼©å°äº†ä¸æœ€å…ˆè¿›çš„å¼€æºå’Œé—­æºVLMsä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†å®ƒä»¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.16844",
            "title": "FinSight: Towards Real-World Financial Deep Research",
            "url": "https://huggingface.co/papers/2510.16844",
            "abstract": "FinSight, a multi-agent framework using CAVM architecture and iterative vision-enhanced mechanism, generates high-quality, multimodal financial reports with superior accuracy and presentation quality compared to existing systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating professional financial reports is a labor-intensive and intellectually demanding process that current AI systems struggle to fully automate. To address this challenge, we introduce FinSight (Financial InSight), a novel multi agent framework for producing high-quality, multimodal financial reports. The foundation of FinSight is the Code Agent with Variable Memory (CAVM) architecture, which unifies external data, designed tools, and agents into a programmable variable space, enabling flexible data collection, analysis and report generation through executable code. To ensure professional-grade visualization, we propose an Iterative Vision-Enhanced Mechanism that progressively refines raw visual outputs into polished financial charts. Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis segments into coherent, citation-aware, and multimodal reports, ensuring both analytical depth and structural consistency. Experiments on various company and industry-level tasks demonstrate that FinSight significantly outperforms all baselines, including leading deep research systems in terms of factual accuracy, analytical depth, and presentation quality, demonstrating a clear path toward generating reports that approach human-expert quality.",
            "score": 7,
            "issue_id": 6567,
            "pub_date": "2025-10-19",
            "pub_date_card": {
                "ru": "19 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 19",
                "zh": "10æœˆ19æ—¥"
            },
            "hash": "4316f8c0ef12ddf6",
            "authors": [
                "Jiajie Jin",
                "Yuyao Zhang",
                "Yimeng Xu",
                "Hongjin Qian",
                "Yutao Zhu",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "BAAI",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.16844.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#architecture"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "AI-Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ",
                    "desc": "FinSight â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ² Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ CAVM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´. Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹. Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ² ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹ ÑĞ¾ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "FinSight: Revolutionizing Financial Reporting with AI",
                    "desc": "FinSight is a multi-agent framework designed to automate the generation of high-quality financial reports. It utilizes the Code Agent with Variable Memory (CAVM) architecture, which allows for flexible data integration and analysis through programmable code. The framework also includes an Iterative Vision-Enhanced Mechanism that improves visual outputs into professional-grade financial charts. Overall, FinSight outperforms existing AI systems in accuracy and presentation, making strides towards achieving human-expert quality in financial reporting."
                },
                "zh": {
                    "title": "FinSightï¼šæ™ºèƒ½ç”Ÿæˆé«˜è´¨é‡é‡‘èæŠ¥å‘Šçš„æœªæ¥",
                    "desc": "FinSightæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé‡‡ç”¨å¯å˜å†…å­˜çš„ä»£ç ä»£ç†æ¶æ„ï¼ˆCAVMï¼‰ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€é‡‘èæŠ¥å‘Šã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¯æ‰§è¡Œä»£ç çµæ´»åœ°æ”¶é›†å’Œåˆ†ææ•°æ®ï¼Œç¡®ä¿æŠ¥å‘Šçš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§ã€‚ä¸ºäº†æå‡å¯è§†åŒ–æ•ˆæœï¼ŒFinSightå¼•å…¥äº†è¿­ä»£è§†è§‰å¢å¼ºæœºåˆ¶ï¼Œé€æ­¥ä¼˜åŒ–åŸå§‹è§†è§‰è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFinSightåœ¨å‡†ç¡®æ€§ã€åˆ†ææ·±åº¦å’Œå±•ç¤ºè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œæ¥è¿‘äººç±»ä¸“å®¶çš„æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19817",
            "title": "olmOCR 2: Unit Test Rewards for Document OCR",
            "url": "https://huggingface.co/papers/2510.19817",
            "abstract": "olmOCR 2, a vision language model trained with reinforcement learning and verifiable rewards, achieves state-of-the-art performance in OCR tasks, particularly in math formula conversion, table parsing, and multi-column layouts.  \t\t\t\t\tAI-generated summary \t\t\t\t We present olmOCR 2, the latest in our family of powerful OCR systems for converting digitized print documents, like PDFs, into clean, naturally ordered plain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision language model (VLM) trained using reinforcement learning with verifiable rewards (RLVR), where our rewards are a diverse set of binary unit tests. To scale unit test creation, we develop a pipeline for generating synthetic documents with diverse and challenging layouts, known ground-truth HTML source code, and extracted test cases. We show that RL training on these test cases results in state-of-the-art performance on olmOCR-Bench, our English-language OCR benchmark, with the largest improvements in math formula conversion, table parsing, and multi-column layouts compared to previous versions. We release our model, data and code under permissive open licenses.",
            "score": 5,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "f414d75457c4f9ee",
            "authors": [
                "Jake Poznanski",
                "Luca Soldaini",
                "Kyle Lo"
            ],
            "affiliations": [
                "Allen Institute for AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19817.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#synthetic",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "OCR Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° unit-Ñ‚ĞµÑÑ‚Ğ°Ñ…",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ olmOCR 2 Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ vision language model Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ PDF-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ‚ĞµĞºÑÑ‚. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR), Ğ³Ğ´Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ unit-Ñ‚ĞµÑÑ‚Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ… Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ olmOCR-Bench, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ», Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… layout'Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ´ Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½Ñ‹ Ğ¿Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing OCR with Reinforcement Learning!",
                    "desc": "olmOCR 2 is an advanced optical character recognition (OCR) system that excels in converting printed documents into structured text. It utilizes a vision language model (VLM) with 7 billion parameters, trained through reinforcement learning with verifiable rewards to ensure high accuracy. The model is particularly effective in handling complex tasks such as math formula conversion, table parsing, and multi-column layouts. By generating synthetic documents for training, olmOCR 2 achieves state-of-the-art results on the olmOCR-Bench benchmark, demonstrating significant improvements over its predecessors."
                },
                "zh": {
                    "title": "olmOCR 2ï¼šOCRä»»åŠ¡çš„æœ€ä¼˜è§£",
                    "desc": "olmOCR 2 æ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å…¬å¼è½¬æ¢ã€è¡¨æ ¼è§£æå’Œå¤šåˆ—å¸ƒå±€æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œå¯éªŒè¯å¥–åŠ±è¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿äº†å…¶åœ¨å¤„ç†å¤æ‚æ–‡æ¡£æ—¶çš„é«˜æ•ˆæ€§ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç”Ÿæˆåˆæˆæ–‡æ¡£çš„ç®¡é“ï¼Œä»¥åˆ›å»ºå¤šæ ·åŒ–å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„å¸ƒå±€ï¼Œå¹¶é€šè¿‡äºŒå…ƒå•å…ƒæµ‹è¯•æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æœ€ç»ˆï¼ŒolmOCR 2 åœ¨æˆ‘ä»¬çš„è‹±è¯­OCRåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ•°å­¦å…¬å¼å’Œè¡¨æ ¼çš„å¤„ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19028",
            "title": "Are they lovers or friends? Evaluating LLMs' Social Reasoning in English\n  and Korean Dialogues",
            "url": "https://huggingface.co/papers/2510.19028",
            "abstract": "Current large language models exhibit significant limitations in social reasoning, particularly in inferring interpersonal relationships across different languages, and thinking models or chain-of-thought prompting offer minimal improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) are increasingly used in human-AI interactions, their social reasoning capabilities in interpersonal contexts are critical. We introduce SCRIPTS, a 1k-dialogue dataset in English and Korean, sourced from movie scripts. The task involves evaluating models' social reasoning capability to infer the interpersonal relationships (e.g., friends, sisters, lovers) between speakers in each dialogue. Each dialogue is annotated with probabilistic relational labels (Highly Likely, Less Likely, Unlikely) by native (or equivalent) Korean and English speakers from Korea and the U.S. Evaluating nine models on our task, current proprietary LLMs achieve around 75-80% on the English dataset, whereas their performance on Korean drops to 58-69%. More strikingly, models select Unlikely relationships in 10-25% of their responses. Furthermore, we find that thinking models and chain-of-thought prompting, effective for general reasoning, provide minimal benefits for social reasoning and occasionally amplify social biases. Our findings reveal significant limitations in current LLMs' social reasoning capabilities, highlighting the need for efforts to develop socially-aware language models.",
            "score": 5,
            "issue_id": 6570,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "8920c08609cd558d",
            "authors": [
                "Eunsu Kim",
                "Junyeong Park",
                "Juhyun Oh",
                "Kiwoong Park",
                "Seyoung Song",
                "A. Seza Dogruoz",
                "Najoung Kim",
                "Alice Oh"
            ],
            "affiliations": [
                "Boston University",
                "KAIST",
                "University of Ghent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19028.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#alignment",
                    "#dataset",
                    "#multilingual",
                    "#reasoning"
                ],
                "emoji": "ğŸ‘¥",
                "ru": {
                    "title": "LLM Ğ½Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ°Ğº Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾, ĞºĞ°Ğº Ğ¼Ñ‹ Ğ´ÑƒĞ¼Ğ°Ğ»Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SCRIPTS Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ… Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ (75-80%), Ğ½Ğ¾ Ğ¸Ñ… Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ¼ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ´Ğ¾ 58-69%, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ². Ğ£Ğ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ thinking models Ğ¸ chain-of-thought prompting Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ reasoning Ğ¸ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ´Ğ°Ğ¶Ğµ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… LLM Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Social Reasoning in Language Models",
                    "desc": "This paper discusses the limitations of current large language models (LLMs) in social reasoning, especially in understanding interpersonal relationships across languages. The authors introduce a new dataset called SCRIPTS, which consists of dialogues in English and Korean, to evaluate how well models can infer relationships like friends or lovers between speakers. They found that while LLMs perform reasonably well in English, their accuracy drops significantly in Korean, and they often misclassify relationships. The study highlights that traditional reasoning techniques, such as chain-of-thought prompting, do not improve social reasoning and may even exacerbate biases, indicating a need for more socially-aware AI models."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹çš„ç¤¾ä¼šæ¨ç†èƒ½åŠ›",
                    "desc": "å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šæ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œå°¤å…¶æ˜¯åœ¨è·¨è¯­è¨€æ¨æ–­äººé™…å…³ç³»æ–¹é¢ã€‚æˆ‘ä»¬æå‡ºäº†SCRIPTSï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è‹±è¯­å’ŒéŸ©è¯­çš„å¯¹è¯æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨æ¨æ–­å¯¹è¯ä¸­è¯´è¯è€…ä¹‹é—´äººé™…å…³ç³»çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä¹ä¸ªæ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°å½“å‰çš„ä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‹±è¯­æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸º75-80%ï¼Œè€Œåœ¨éŸ©è¯­æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸‹é™è‡³58-69%ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨ç¤¾ä¼šæ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨é‡å¤§ä¸è¶³ï¼ŒäºŸéœ€å¼€å‘æ›´å…·ç¤¾ä¼šæ„è¯†çš„è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.17932",
            "title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models",
            "url": "https://huggingface.co/papers/2510.17932",
            "abstract": "Chart2Code is a hierarchical benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models, featuring three levels of increasing complexity and diverse real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Chart2Code, a new benchmark for evaluating the chart understanding and code generation capabilities of large multimodal models (LMMs). Chart2Code is explicitly designed from a user-driven perspective, capturing diverse real-world scenarios and progressively increasing task difficulty. It consists of three levels: Level 1 (Chart Reproduction) reproduces charts from a reference figure and user query; Level 2 (Chart Editing) involves complex modifications such as changing chart types or adding elements; and Level 3 (Long-Table to Chart Generation) requires models to transform long, information-dense tables into faithful charts following user instructions. To our knowledge, this is the first hierarchical benchmark that reflects practical chart2code usage while systematically scaling task complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics that assess both code correctness and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art (SoTA) LMMs, including both proprietary and the latest open-source models such as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental results demonstrate that even the SoTA model GPT-5 averages only 0.57 on code-based evaluation and 0.22 on chart-quality assessment across the editing tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark will drive advances in multimodal reasoning and foster the development of more robust and general-purpose LMMs. Our code and data are available on Chart2Code.",
            "score": 5,
            "issue_id": 6571,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 20",
                "zh": "10æœˆ20æ—¥"
            },
            "hash": "515c35b7e783d482",
            "authors": [
                "Jiahao Tang",
                "Henry Hengyuan Zhao",
                "Lijian Wu",
                "Yifei Tao",
                "Dongxing Mao",
                "Yang Wan",
                "Jingru Tan",
                "Min Zeng",
                "Min Li",
                "Alex Jinpeng Wang"
            ],
            "affiliations": [
                "CSU-JPG, Central South University",
                "Nanyang Technological University",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.17932.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#multimodal",
                    "#interpretability",
                    "#optimization",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ â€” ÑĞ»Ğ¾Ğ¶Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ ĞºĞ°Ğ¶ĞµÑ‚ÑÑ",
                    "desc": "Chart2Code â€” ÑÑ‚Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°ÑÑ‰ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ², Ğ¸Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†. Ğ’ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ»Ğ¸ 25 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-5, Ğ½Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ½ĞµĞ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ â€” GPT-5 Ğ½Ğ°Ğ±Ñ€Ğ°Ğ» Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 0.57 Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ ĞºĞ¾Ğ´Ğ° Ğ¸ 0.22 Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 2023 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ 22 Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Chart2Code: Elevating Chart Understanding in AI",
                    "desc": "Chart2Code is a new benchmark designed to evaluate how well large multimodal models (LMMs) can understand charts and generate corresponding code. It features three levels of tasks that increase in complexity, starting from simple chart reproduction to more complex tasks like editing charts and generating charts from detailed tables. The benchmark includes 2,023 tasks across 22 different chart types, with metrics to assess both the accuracy of the generated code and the quality of the charts produced. Initial tests on state-of-the-art models, including GPT-5, show that these tasks are quite challenging, highlighting the need for further advancements in multimodal reasoning capabilities."
                },
                "zh": {
                    "title": "Chart2Codeï¼šå›¾è¡¨ç†è§£ä¸ä»£ç ç”Ÿæˆçš„æ–°åŸºå‡†",
                    "desc": "Chart2Codeæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨å›¾è¡¨ç†è§£å’Œä»£ç ç”Ÿæˆèƒ½åŠ›çš„åˆ†å±‚åŸºå‡†ã€‚å®ƒè®¾è®¡äº†ä¸‰ä¸ªå¤æ‚åº¦é€æ¸å¢åŠ çš„ä»»åŠ¡ï¼Œæ¶µç›–äº†å¤šç§çœŸå®åœºæ™¯ã€‚ç¬¬ä¸€çº§ä»»åŠ¡æ˜¯ä»å‚è€ƒå›¾å½¢å’Œç”¨æˆ·æŸ¥è¯¢ä¸­é‡ç°å›¾è¡¨ï¼›ç¬¬äºŒçº§ä»»åŠ¡æ¶‰åŠå¤æ‚çš„ä¿®æ”¹ï¼Œå¦‚æ›´æ”¹å›¾è¡¨ç±»å‹æˆ–æ·»åŠ å…ƒç´ ï¼›ç¬¬ä¸‰çº§ä»»åŠ¡åˆ™è¦æ±‚æ¨¡å‹å°†ä¿¡æ¯å¯†é›†çš„é•¿è¡¨æ ¼è½¬æ¢ä¸ºç¬¦åˆç”¨æˆ·æŒ‡ä»¤çš„å›¾è¡¨ã€‚é€šè¿‡å¯¹25ä¸ªæœ€å…ˆè¿›çš„LMMè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹GPT-5åœ¨ä»»åŠ¡ä¸­çš„è¡¨ç°ä¹Ÿç›¸å¯¹è¾ƒä½ï¼Œè¡¨æ˜Chart2Codeçš„æŒ‘æˆ˜æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15731",
            "title": "Attention Sinks in Diffusion Language Models",
            "url": "https://huggingface.co/papers/2510.15731",
            "abstract": "Empirical analysis of Masked Diffusion Language Models (DLMs) reveals distinct attention sinking phenomena and robustness compared to Autoregressive Models (ARMs).  \t\t\t\t\tAI-generated summary \t\t\t\t Masked Diffusion Language Models (DLMs) have recently emerged as a promising alternative to traditional Autoregressive Models (ARMs). DLMs employ transformer encoders with bidirectional attention, enabling parallel token generation while maintaining competitive performance. Although their efficiency and effectiveness have been extensively studied, the internal mechanisms that govern DLMs remain largely unexplored. In this work, we conduct an empirical analysis of DLM attention patterns, focusing on the attention sinking phenomenon, an effect previously observed in various transformer-based architectures. Our findings reveal that DLMs also exhibit attention sinks, but with distinct characteristics. First, unlike in ARMs, the sink positions in DLMs tend to shift throughout the generation process, displaying a dynamic behaviour. Second, while ARMs are highly sensitive to the removal of attention sinks, DLMs remain robust: masking sinks leads to only a minor degradation in performance. These results provide new insights into the inner workings of diffusion-based language models and highlight fundamental differences in how they allocate and utilize attention compared to autoregressive models.",
            "score": 5,
            "issue_id": 6571,
            "pub_date": "2025-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "97abd15d347fface",
            "authors": [
                "Maximo Eduardo Rulli",
                "Simone Petruzzi",
                "Edoardo Michielon",
                "Fabrizio Silvestri",
                "Simone Scardapane",
                "Alessio Devoto"
            ],
            "affiliations": [
                "Fastweb",
                "Sapienza University of Rome"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15731.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#diffusion",
                    "#interpretability"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Masked Diffusion Language Models (DLM) â€” Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ĞºĞ»Ğ°ÑÑĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… autoregressive Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ DLM Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚ attention sinking (ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑÑ…), Ğ½Ğ¾ Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸: Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Â«ÑÑ‚Ğ¾ĞºĞ¾Ğ²Â» Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ autoregressive Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, DLM Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… attention sinks â€” Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ñ‚Ğ¾Ğ¼, ĞºĞ°Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ autoregressive Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unveiling the Dynamic Attention of Masked Diffusion Models",
                    "desc": "This paper investigates Masked Diffusion Language Models (DLMs) and their attention mechanisms compared to Autoregressive Models (ARMs). It identifies a phenomenon called attention sinking, where certain tokens receive less attention during generation. The study finds that DLMs exhibit dynamic attention sinks that shift over time, unlike the static sinks in ARMs. Additionally, DLMs show robustness to the removal of these sinks, indicating a more resilient attention allocation strategy."
                },
                "zh": {
                    "title": "æ©è”½æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶æ–°å‘ç°",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†æ©è”½æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMsï¼‰ä¸è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸Šçš„å·®å¼‚ã€‚DLMsä½¿ç”¨åŒå‘æ³¨æ„åŠ›çš„å˜æ¢å™¨ç¼–ç å™¨ï¼Œèƒ½å¤Ÿå¹¶è¡Œç”Ÿæˆæ ‡è®°ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šä¸ARMsç›¸å½“ã€‚æˆ‘ä»¬å‘ç°DLMså­˜åœ¨æ³¨æ„åŠ›æ²‰æ²¡ç°è±¡ï¼Œä½†å…¶ç‰¹å¾ä¸ARMsä¸åŒï¼ŒDLMsçš„æ²‰æ²¡ä½ç½®åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¼šåŠ¨æ€å˜åŒ–ã€‚å°½ç®¡ARMså¯¹æ³¨æ„åŠ›æ²‰æ²¡çš„å»é™¤éå¸¸æ•æ„Ÿï¼Œä½†DLMsè¡¨ç°å‡ºè¾ƒå¼ºçš„é²æ£’æ€§ï¼Œæ©è”½æ²‰æ²¡å¯¹æ€§èƒ½çš„å½±å“è¾ƒå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19386",
            "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
            "url": "https://huggingface.co/papers/2510.19386",
            "abstract": "ColorAgent, an OS agent using step-wise reinforcement learning and a multi-agent framework, achieves high success rates in long-horizon interactions and personalized user engagement on Android benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t With the advancements in hardware, software, and large language model technologies, the interaction between humans and operating systems has evolved from the command-line interface to the rapidly emerging AI agent interactions. Building an operating system (OS) agent capable of executing user instructions and faithfully following user desires is becoming a reality. In this technical report, we present ColorAgent, an OS agent designed to engage in long-horizon, robust interactions with the environment while also enabling personalized and proactive user interaction. To enable long-horizon interactions with the environment, we enhance the model's capabilities through step-wise reinforcement learning and self-evolving training, while also developing a tailored multi-agent framework that ensures generality, consistency, and robustness. In terms of user interaction, we explore personalized user intent recognition and proactive engagement, positioning the OS agent not merely as an automation tool but as a warm, collaborative partner. We evaluate ColorAgent on the AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2% and 50.7%, respectively, establishing a new state of the art. Nonetheless, we note that current benchmarks are insufficient for a comprehensive evaluation of OS agents and propose further exploring directions in future work, particularly in the areas of evaluation paradigms, agent collaboration, and security. Our code is available at https://github.com/MadeAgents/mobile-use.",
            "score": 4,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "671d46e29b93bd8f",
            "authors": [
                "Ning Li",
                "Qiqiang Lin",
                "Zheng Wu",
                "Xiaoyun Mo",
                "Weiming Zhang",
                "Yin Zhao",
                "Xiangmou Qu",
                "Jiamu Zhou",
                "Jun Wang",
                "Congmin Zheng",
                "Yuanyi Song",
                "Hongjiang Chen",
                "Heyuan Huang",
                "Jihong Wang",
                "Jiaxin Yin",
                "Jingwei Yu",
                "Junwei Liao",
                "Qiuying Peng",
                "Xingyu Lou",
                "Jun Wang",
                "Weiwen Liu",
                "Zhuosheng Zhang",
                "Weinan Zhang"
            ],
            "affiliations": [
                "OPPO Research Institute",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19386.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#games",
                    "#benchmark",
                    "#security",
                    "#agents",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ColorAgent: ÑƒĞ¼Ğ½Ñ‹Ğ¹ OS-Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ",
                    "desc": "ColorAgent â€” ÑÑ‚Ğ¾ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Android, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ reinforcement learning Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ ÑĞ¾ ÑÑ€ĞµĞ´Ğ¾Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ multi-agent Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… AndroidWorld Ğ¸ AndroidLab Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² 77.2% Ğ¸ 50.7% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ² Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ ColorAgent Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° ĞºĞ°Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ½Ğ¸Ğ¼."
                },
                "en": {
                    "title": "ColorAgent: Your Personalized OS Partner for Intelligent Interaction",
                    "desc": "ColorAgent is an operating system agent that utilizes step-wise reinforcement learning within a multi-agent framework to enhance user interactions over long periods. It focuses on personalized user engagement, allowing the agent to understand and anticipate user needs, making it more than just an automation tool. The agent has been tested on Android benchmarks, achieving impressive success rates of 77.2% and 50.7%, setting a new standard in the field. The paper also highlights the need for improved evaluation methods for OS agents and suggests future research directions in collaboration and security."
                },
                "zh": {
                    "title": "ColorAgentï¼šæ™ºèƒ½æ“ä½œç³»ç»Ÿçš„ä¸ªæ€§åŒ–äº¤äº’æ–°çºªå…ƒ",
                    "desc": "ColorAgentæ˜¯ä¸€ç§æ“ä½œç³»ç»Ÿä»£ç†ï¼Œé‡‡ç”¨é€æ­¥å¼ºåŒ–å­¦ä¹ å’Œå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨é•¿æ—¶é—´äº¤äº’ä¸­å®ç°é«˜æˆåŠŸç‡ï¼Œå¹¶æä¾›ä¸ªæ€§åŒ–çš„ç”¨æˆ·å‚ä¸ä½“éªŒã€‚è¯¥ä»£ç†é€šè¿‡å¢å¼ºæ¨¡å‹èƒ½åŠ›ï¼Œæ”¯æŒä¸ç¯å¢ƒçš„é•¿æ—¶é—´äº¤äº’ï¼Œå¹¶è¿›è¡Œè‡ªæˆ‘è¿›åŒ–è®­ç»ƒã€‚ColorAgentä¸ä»…æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–å·¥å…·ï¼Œæ›´æ˜¯ä¸€ä¸ªæ¸©æš–çš„åˆä½œä¼™ä¼´ï¼Œèƒ½å¤Ÿè¯†åˆ«ç”¨æˆ·æ„å›¾å¹¶ä¸»åŠ¨å‚ä¸ã€‚æˆ‘ä»¬åœ¨AndroidWorldå’ŒAndroidLabåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†ColorAgentï¼Œåˆ†åˆ«å–å¾—äº†77.2%å’Œ50.7%çš„æˆåŠŸç‡ï¼Œåˆ›é€ äº†æ–°çš„æŠ€æœ¯æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19316",
            "title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via\n  Knowledge-Oriented Augmentations and Constraints",
            "url": "https://huggingface.co/papers/2510.19316",
            "abstract": "KORE is a method for injecting new knowledge into large multimodal models while preserving old knowledge, using structured augmentations and covariance matrix constraints to minimize catastrophic forgetting.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models encode extensive factual knowledge in their pre-trained weights. However, its knowledge remains static and limited, unable to keep pace with real-world developments, which hinders continuous knowledge acquisition. Effective knowledge injection thus becomes critical, involving two goals: knowledge adaptation (injecting new knowledge) and knowledge retention (preserving old knowledge). Existing methods often struggle to learn new knowledge and suffer from catastrophic forgetting. To address this, we propose KORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints for injecting new knowledge into large multimodal models while preserving old knowledge. Unlike general text or image data augmentation, KORE automatically converts individual knowledge items into structured and comprehensive knowledge to ensure that the model accurately learns new knowledge, enabling accurate adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix of LMM's linear layer activations and initializes the adapter by projecting the original weights into the matrix's null space, defining a fine-tuning direction that minimizes interference with previous knowledge, enabling powerful retention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B, LLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new knowledge injection performance and effectively mitigates catastrophic forgetting.",
            "score": 4,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "97796b9efcce99e8",
            "authors": [
                "Kailin Jiang",
                "Hongbo Jiang",
                "Ning Jiang",
                "Zhi Gao",
                "Jinhe Bi",
                "Yuchen Ren",
                "Bin Li",
                "Yuntao Du",
                "Lei Liu",
                "Qing Li"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Ludwig Maximilian University of Munich",
                "Northeast Forestry University",
                "Shandong University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "University of Science and Technology of China",
                "University of Sydney",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19316.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "KORE: Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ñ€Ğ¾Ğ³Ğ¾",
                    "desc": "KORE â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ½ĞµĞµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ KORE ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² LLM Ğ¸ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸ĞµĞ¹ Ğ² Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… LLaVA Ğ¸ Qwen2.5-VL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ€Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "KORE: Retain the Old, Embrace the New in Multimodal Learning!",
                    "desc": "KORE is a novel method designed to enhance large multimodal models by injecting new knowledge while ensuring that previously learned information is retained. It utilizes structured augmentations and covariance matrix constraints to minimize the risk of catastrophic forgetting, which is when a model loses old knowledge while learning new information. By converting knowledge items into structured formats, KORE allows models to accurately learn and adapt to new data. Experimental results demonstrate that KORE significantly improves the performance of knowledge injection in various multimodal models without compromising their existing knowledge."
                },
                "zh": {
                    "title": "KOREï¼šçŸ¥è¯†æ³¨å…¥ä¸ä¿ç•™çš„å®Œç¾å¹³è¡¡",
                    "desc": "KOREæ˜¯ä¸€ç§å‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ³¨å…¥æ–°çŸ¥è¯†çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿ç•™æ—§çŸ¥è¯†ã€‚å®ƒé€šè¿‡ç»“æ„åŒ–å¢å¼ºå’Œåæ–¹å·®çŸ©é˜µçº¦æŸæ¥æœ€å°åŒ–ç¾éš¾æ€§é—å¿˜ã€‚KOREèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†æ–°çŸ¥è¯†é€‚åº”åˆ°æ¨¡å‹ä¸­ï¼ŒåŒæ—¶ç¡®ä¿æ—§çŸ¥è¯†çš„ä¿ç•™ã€‚å®éªŒè¡¨æ˜ï¼ŒKOREåœ¨å¤šç§å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†æ–°çŸ¥è¯†æ³¨å…¥çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19286",
            "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools",
            "url": "https://huggingface.co/papers/2510.19286",
            "abstract": "TheMCPCompany evaluates tool-calling agents using REST APIs for interacting with real-world services, showing that advanced models perform well in simpler environments but struggle with complex enterprise environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Since the introduction of the Model Context Protocol (MCP), the number of available tools for Large Language Models (LLMs) has increased significantly. These task-specific tool sets offer an alternative to general-purpose tools such as web browsers, while being easier to develop and maintain than GUIs. However, current general-purpose agents predominantly rely on web browsers for interacting with the environment. Here, we introduce TheMCPCompany, a benchmark for evaluating tool-calling agents on tasks that involve interacting with various real-world services. We use the REST APIs of these services to create MCP servers, which include over 18,000 tools. We also provide manually annotated ground-truth tools for each task. In our experiments, we use the ground truth tools to show the potential of tool-calling agents for both improving performance and reducing costs assuming perfect tool retrieval. Next, we explore agent performance using tool retrieval to study the real-world practicality of tool-based agents. While all models with tool retrieval perform similarly or better than browser-based agents, smaller models cannot take full advantage of the available tools through retrieval. On the other hand, GPT-5's performance with tool retrieval is very close to its performance with ground-truth tools. Overall, our work shows that the most advanced reasoning models are effective at discovering tools in simpler environments, but seriously struggle with navigating complex enterprise environments. TheMCPCompany reveals that navigating tens of thousands of tools and combining them in non-trivial ways to solve complex problems is still a challenging task for current models and requires both better reasoning and better retrieval models.",
            "score": 4,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "d7d7013ab05a9068",
            "authors": [
                "Reza Esfandiarpoor",
                "Vishwas Suryanarayanan",
                "Stephen H. Bach",
                "Vishal Chowdhary",
                "Anthony Aue"
            ],
            "affiliations": [
                "Brown University",
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19286.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ¢Ñ‹ÑÑÑ‡Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² â€” Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ TheMCPCompany â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· REST API Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞµÑ€Ğ²Ğ¸ÑĞ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 18,000 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Model Context Protocol (MCP), Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ performance Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° GPT-5 Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼Ñƒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº reasoning, Ñ‚Ğ°Ğº Ğ¸ retrieval Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Navigating Complexity: Evaluating Tool-Calling Agents in Real-World Environments",
                    "desc": "The paper introduces TheMCPCompany, a benchmark designed to evaluate tool-calling agents that interact with real-world services through REST APIs. It highlights the performance of advanced models in simpler environments, where they excel, but reveals their limitations in complex enterprise settings. The study demonstrates that while tool retrieval enhances agent performance, smaller models struggle to utilize the available tools effectively. Ultimately, the findings suggest that improving reasoning and retrieval capabilities is essential for navigating intricate tasks involving numerous tools."
                },
                "zh": {
                    "title": "è¯„ä¼°å·¥å…·è°ƒç”¨ä»£ç†çš„æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "TheMCPCompanyæ˜¯ä¸€ä¸ªè¯„ä¼°å·¥å…·è°ƒç”¨ä»£ç†çš„åŸºå‡†ï¼Œä½¿ç”¨REST APIä¸çœŸå®ä¸–ç•ŒæœåŠ¡è¿›è¡Œäº¤äº’ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å…ˆè¿›çš„æ¨¡å‹åœ¨ç®€å•ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚çš„ä¼ä¸šç¯å¢ƒä¸­å´é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡18,000ä¸ªå·¥å…·çš„MCPæœåŠ¡å™¨ï¼Œå¹¶æä¾›äº†æ¯ä¸ªä»»åŠ¡çš„æ‰‹åŠ¨æ ‡æ³¨çœŸå®å·¥å…·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æ‰€æœ‰æ¨¡å‹åœ¨å·¥å…·æ£€ç´¢æ–¹é¢çš„è¡¨ç°ä¼˜äºåŸºäºæµè§ˆå™¨çš„ä»£ç†ï¼Œä½†è¾ƒå°çš„æ¨¡å‹æ— æ³•å……åˆ†åˆ©ç”¨å¯ç”¨å·¥å…·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18313",
            "title": "OmniNWM: Omniscient Driving Navigation World Models",
            "url": "https://huggingface.co/papers/2510.18313",
            "abstract": "OmniNWM is a unified world model for autonomous driving that generates panoramic videos, encodes actions using Plucker ray-maps, and defines dense rewards based on 3D occupancy, achieving top performance in video generation, control, and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous driving world models are expected to work effectively across three core dimensions: state, action, and reward. Existing models, however, are typically restricted to limited state modalities, short video sequences, imprecise action control, and a lack of reward awareness. In this paper, we introduce OmniNWM, an omniscient panoramic navigation world model that addresses all three dimensions within a unified framework. For state, OmniNWM jointly generates panoramic videos of RGB, semantics, metric depth, and 3D occupancy. A flexible forcing strategy enables high-quality long-horizon auto-regressive generation. For action, we introduce a normalized panoramic Plucker ray-map representation that encodes input trajectories into pixel-level signals, enabling highly precise and generalizable control over panoramic video generation. Regarding reward, we move beyond learning reward functions with external image-based models: instead, we leverage the generated 3D occupancy to directly define rule-based dense rewards for driving compliance and safety. Extensive experiments demonstrate that OmniNWM achieves state-of-the-art performance in video generation, control accuracy, and long-horizon stability, while providing a reliable closed-loop evaluation framework through occupancy-grounded rewards. Project page is available at https://github.com/Arlo0o/OmniNWM.",
            "score": 4,
            "issue_id": 6567,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "234f9481e9fb3cf2",
            "authors": [
                "Bohan Li",
                "Zhuang Ma",
                "Dalong Du",
                "Baorui Peng",
                "Zhujin Liang",
                "Zhenqiang Liu",
                "Chao Ma",
                "Yueming Jin",
                "Hao Zhao",
                "Wenjun Zeng",
                "Xin Jin"
            ],
            "affiliations": [
                "Eastern Institute of Technology, Ningbo",
                "National University of Singapore",
                "PhiGent",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18313.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#games",
                    "#agents",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ’ÑĞµĞ²Ğ¸Ğ´ÑÑ‰Ğ°Ñ world model Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 3D-Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸",
                    "desc": "OmniNWM â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ world model Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ RGB, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾Ğ¹, Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ğ¸ 3D occupancy. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Plucker ray-maps Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ reward Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ 3D occupancy Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "OmniNWM: The Future of Autonomous Driving World Models",
                    "desc": "OmniNWM is a comprehensive world model designed for autonomous driving that excels in generating panoramic videos and encoding actions with Plucker ray-maps. It effectively integrates state, action, and reward dimensions into a single framework, overcoming limitations of previous models. The model generates high-quality panoramic videos that include RGB, semantics, metric depth, and 3D occupancy, while also providing precise control through pixel-level action encoding. By utilizing 3D occupancy for defining dense rewards, OmniNWM ensures compliance and safety in driving, achieving top performance in video generation and control stability."
                },
                "zh": {
                    "title": "OmniNWMï¼šè‡ªåŠ¨é©¾é©¶çš„å…¨æ™¯å¯¼èˆªæ–°æ¨¡å‹",
                    "desc": "OmniNWMæ˜¯ä¸€ç§ç»Ÿä¸€çš„ä¸–ç•Œæ¨¡å‹ï¼Œä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è®¾è®¡ï¼Œèƒ½å¤Ÿç”Ÿæˆå…¨æ™¯è§†é¢‘ï¼Œå¹¶ä½¿ç”¨Pluckerå…‰çº¿å›¾ç¼–ç åŠ¨ä½œï¼ŒåŒæ—¶åŸºäº3Då ç”¨å®šä¹‰å¯†é›†å¥–åŠ±ã€‚è¯¥æ¨¡å‹åœ¨çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…‹æœäº†ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ã€‚OmniNWMç”ŸæˆRGBã€è¯­ä¹‰ã€åº¦é‡æ·±åº¦å’Œ3Då ç”¨çš„å…¨æ™¯è§†é¢‘ï¼Œé‡‡ç”¨çµæ´»çš„å¼ºåˆ¶ç­–ç•¥å®ç°é«˜è´¨é‡çš„é•¿æ—¶é—´è‡ªå›å½’ç”Ÿæˆã€‚é€šè¿‡ç›´æ¥åˆ©ç”¨ç”Ÿæˆçš„3Då ç”¨å®šä¹‰åŸºäºè§„åˆ™çš„å¯†é›†å¥–åŠ±ï¼ŒOmniNWMåœ¨è§†é¢‘ç”Ÿæˆã€æ§åˆ¶ç²¾åº¦å’Œé•¿æœŸç¨³å®šæ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19457",
            "title": "MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for\n  Large Multimodal Models",
            "url": "https://huggingface.co/papers/2510.19457",
            "abstract": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs' ability to understand time-sensitive knowledge. To address this gap, we propose MINED, a comprehensive benchmark that evaluates temporal awareness along 6 key dimensions and 11 challenging tasks: cognition, awareness, trustworthiness, understanding, reasoning, and robustness. MINED is constructed from Wikipedia by two professional annotators, containing 2,104 time-sensitive knowledge samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07, while most open-source LMMs still lack time understanding ability. Meanwhile, LMMs perform best on organization knowledge, whereas their performance is weakest on sport. To address these challenges, we investigate the feasibility of updating time-sensitive knowledge in LMMs through knowledge editing methods and observe that LMMs can effectively update knowledge via knowledge editing methods in single editing scenarios.",
            "score": 3,
            "issue_id": 6567,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "016313b1f4c1861b",
            "authors": [
                "Kailin Jiang",
                "Ning Jiang",
                "Yuchen Ren",
                "Yuchen Li",
                "Yifan Gao",
                "Jinhe Bi",
                "Yunpu Ma",
                "Qingqing Liu",
                "Xianhao Wang",
                "Yifan Jia",
                "Hongbo Jiang",
                "Yaocong Hu",
                "Bin Li",
                "Lei Liu",
                "Yuntao Du"
            ],
            "affiliations": [
                "Anhui Polytechnic University",
                "Beijing Institute of Technology",
                "Ludwig Maximilian University of Munich",
                "Northeast Forestry University",
                "Shandong University",
                "University of Science and Technology of China",
                "University of Sydney",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19457.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "â°",
                "ru": {
                    "title": "MINED: ÑƒÑ‡Ğ¸Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MINED â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2104 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ° Ğ¿Ğ¾ 6 Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ 11 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ² 6 Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ…: ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 15 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… LMM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Gemini-2.5-Pro Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° (63.07%), Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ open-source Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹ÌĞ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ knowledge editing Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹ÌÑ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² LMM Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Temporal Awareness in Large Multimodal Models with MINED",
                    "desc": "This paper discusses the limitations of Large Multimodal Models (LMMs) in understanding time-sensitive factual knowledge due to their static representations. The authors introduce MINED, a new benchmark designed to assess LMMs' temporal awareness across six dimensions and eleven tasks, including cognition and reasoning. The benchmark is built from Wikipedia and includes 2,104 samples of time-sensitive knowledge. The evaluation reveals that while some LMMs, like Gemini-2.5-Pro, perform well, many open-source models struggle with temporal understanding, particularly in areas like sports."
                },
                "zh": {
                    "title": "æå‡æ—¶é—´æ•æ„ŸçŸ¥è¯†ç†è§£çš„åŸºå‡†è¯„ä¼°",
                    "desc": "å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰é€šè¿‡è·¨æ¨¡æ€é¢„è®­ç»ƒç¼–ç ä¸°å¯Œçš„äº‹å®çŸ¥è¯†ï¼Œä½†å®ƒä»¬çš„é™æ€è¡¨ç¤ºåœ¨ç†è§£æ—¶é—´æ•æ„Ÿçš„äº‹å®çŸ¥è¯†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ç”±äºè®¾è®¡é™æ€ï¼Œæ— æ³•å……åˆ†è¯„ä¼°LMMså¯¹æ—¶é—´æ•æ„ŸçŸ¥è¯†çš„ç†è§£èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MINEDï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œè¯„ä¼°æ—¶é—´æ„è¯†çš„å…­ä¸ªå…³é”®ç»´åº¦å’Œåä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚é€šè¿‡å¯¹15ä¸ªå¹¿æ³›ä½¿ç”¨çš„LMMsè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°Gemini-2.5-Proåœ¨æ—¶é—´æ•æ„ŸçŸ¥è¯†çš„ç†è§£ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€Œå¤§å¤šæ•°å¼€æºLMMsä»ç„¶ç¼ºä¹è¿™ç§èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18909",
            "title": "Learning from the Best, Differently: A Diversity-Driven Rethinking on\n  Data Selection",
            "url": "https://huggingface.co/papers/2510.18909",
            "abstract": "The Orthogonal Diversity-Aware Selection (ODiS) algorithm enhances large language model performance by ensuring both quality and diversity in training data through orthogonal decomposition of evaluation dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality pre-training data is crutial for large language models, where quality captures factual reliability and semantic value, and diversity ensures broad coverage and distributional heterogeneity. Existing approaches typically rely on single or multiple-dimensional score-based selection. However, directly selecting top-scored data often degrades performance, and sampling from a broader range is required to recover results. The above non-monotonicity between dataset scores and downstream benchmark results reveals a fundamental bias: score-based methods collapse correlated dimensions, causing top-scored data to appear high-quality while systematically overlooking diversity. We argue that ensuring diversity requires decomposing correlated metrics into orthogonal feature dimensions, from which the top-scored data can be directly selected. Therefore, we proposed the Orthogonal Diversity-Aware Selection (ODiS) algorithm, which preserves both quality and diversity during data selection. First, ODiS evaluates data from multiple dimensions, covering language quality, knowledge quality, and comprehension difficulty. The multi-dimensional scores are then decorrelated via Principal Component Analysis (PCA), yielding orthogonal evaluation dimensions. For each dimension, a Roberta-based scorer is trained to regress the data onto PCA-projected scores, enabling scalable inference on large corpora. Finally, ODiS constructs the training dataset by selecting top-scored data within each orthogonal dimension, thereby ensuring both quality and diversity. Empirical results show that ODiS-selected data exhibit less than 2\\% inter-dimension overlap, confirming orthogonality between dimensions. More importantly, models trained with ODiS-selected data significantly outperform other baselines on downstream benchmarks, highlighting the necessity of orthogonal, diversity-aware data selection for LLMs.",
            "score": 3,
            "issue_id": 6570,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "f33b553748178ffe",
            "authors": [
                "Hongyi He",
                "Xiao Liu",
                "Zhenghao Lin",
                "Mingni Tang",
                "Yi Cheng",
                "Jintao Wang",
                "Wenjie Li",
                "Peng Cheng",
                "Yeyun Gong"
            ],
            "affiliations": [
                "Microsoft Research",
                "The Hong Kong Polytechnic University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18909.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#transfer_learning",
                    "#optimization",
                    "#dataset",
                    "#data",
                    "#training"
                ],
                "emoji": "âŠ¥",
                "ru": {
                    "title": "ĞÑ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LLM",
                    "desc": "ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ODiS ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¸Ğ½Ğ³Ñƒ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ñ„ÑƒĞ½Ğ´amentĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ°: Ğ¾Ğ½Ğ¸ ÑÑ…Ğ»Ğ¾Ğ¿Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ñ‚Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ODiS Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ PCA, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¸Ñ… Ğ½Ğ° Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ°, Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ODiS, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° benchmark'Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Language Models with Quality and Diversity through ODiS",
                    "desc": "The Orthogonal Diversity-Aware Selection (ODiS) algorithm improves the performance of large language models by focusing on both the quality and diversity of training data. It uses orthogonal decomposition to separate evaluation metrics, ensuring that selected data is not only high-quality but also covers a wide range of topics and styles. Traditional methods often fail because they prioritize high scores without considering the diversity of the data, leading to suboptimal model performance. ODiS addresses this issue by evaluating data across multiple dimensions and selecting the best samples from each, resulting in a more effective training dataset."
                },
                "zh": {
                    "title": "æ­£äº¤å¤šæ ·æ€§æ„è¯†é€‰æ‹©ç®—æ³•æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "ODiSç®—æ³•é€šè¿‡å¯¹è¯„ä¼°ç»´åº¦çš„æ­£äº¤åˆ†è§£ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚é«˜è´¨é‡çš„é¢„è®­ç»ƒæ•°æ®ä¸ä»…è¦å…·å¤‡äº‹å®å¯é æ€§å’Œè¯­ä¹‰ä»·å€¼ï¼Œè¿˜éœ€æ¶µç›–å¹¿æ³›çš„å†…å®¹å’Œåˆ†å¸ƒå¼‚è´¨æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºå•ä¸€æˆ–å¤šç»´è¯„åˆ†é€‰æ‹©ï¼Œä½†ç›´æ¥é€‰æ‹©é«˜åˆ†æ•°æ®å¾€å¾€ä¼šé™ä½æ€§èƒ½ï¼Œå› æ­¤éœ€è¦ä»æ›´å¹¿æ³›çš„èŒƒå›´è¿›è¡Œé‡‡æ ·ã€‚ODiSç®—æ³•é€šè¿‡å¤šç»´è¯„ä¼°å’Œä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰å»ç›¸å…³åŒ–ï¼Œç¡®ä¿åœ¨æ¯ä¸ªæ­£äº¤ç»´åº¦ä¸­é€‰æ‹©é«˜åˆ†æ•°æ®ï¼Œä»è€Œå®ç°è´¨é‡ä¸å¤šæ ·æ€§çš„å¹³è¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18941",
            "title": "ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to\n  Answer and Judge",
            "url": "https://huggingface.co/papers/2510.18941",
            "abstract": "ProfBench evaluates large language models in professional domains using human-expert criteria, revealing challenges and performance disparities between proprietary and open-weight models.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating progress in large language models (LLMs) is often constrained by the challenge of verifying responses, limiting assessments to tasks like mathematics, programming, and short-form question-answering. However, many real-world applications require evaluating LLMs in processing professional documents, synthesizing information, and generating comprehensive reports in response to user queries. We introduce ProfBench: a set of over 7000 response-criterion pairs as evaluated by human-experts with professional knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by mitigating self-enhancement bias and reducing the cost of evaluation by 2-3 orders of magnitude, to make it fair and accessible to the broader community. Our findings reveal that ProfBench poses significant challenges even for state-of-the-art LLMs, with top-performing models like GPT-5-high achieving only 65.9\\% overall performance. Furthermore, we identify notable performance disparities between proprietary and open-weight models and provide insights into the role that extended thinking plays in addressing complex, professional-domain tasks. Data: https://huggingface.co/datasets/nvidia/ProfBench and Code: https://github.com/NVlabs/ProfBench",
            "score": 2,
            "issue_id": 6569,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "5e5633a00c2a8ce8",
            "authors": [
                "Zhilin Wang",
                "Jaehun Jung",
                "Ximing Lu",
                "Shizhe Diao",
                "Ellie Evans",
                "Jiaqi Zeng",
                "Pavlo Molchanov",
                "Yejin Choi",
                "Jan Kautz",
                "Yi Dong"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18941.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#science",
                    "#open_source",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ProfBench: Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ LLM ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° 66% Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ProfBench - Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 7000 Ğ¿Ğ°Ñ€ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ PhD Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ Ğ¸ Ñ…Ğ¸Ğ¼Ğ¸Ğ¸, MBA Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ°Ñ… Ğ¸ ĞºĞ¾Ğ½ÑĞ°Ğ»Ñ‚Ğ¸Ğ½Ğ³Ğµ. Ğ”Ğ»Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM-ÑÑƒĞ´ÑŒĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ¾Ñ‚Ğ½Ğ¸ Ñ€Ğ°Ğ· Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ·Ğ°Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-5-high Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 65.9% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "ProfBench: Bridging the Gap in LLM Evaluation for Professional Domains",
                    "desc": "ProfBench is a benchmark designed to evaluate large language models (LLMs) specifically in professional domains using criteria set by human experts. It includes over 7000 response-criterion pairs evaluated by professionals in fields like Physics, Chemistry, Finance, and Consulting. The study highlights the limitations of current LLMs, revealing that even the best models struggle with complex tasks, achieving only 65.9% performance on average. Additionally, it uncovers significant performance differences between proprietary models and those with open weights, emphasizing the importance of extended reasoning in professional applications."
                },
                "zh": {
                    "title": "ProfBenchï¼šè¯„ä¼°ä¸“ä¸šé¢†åŸŸå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜ä¸å·®å¼‚",
                    "desc": "ProfBenchæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸè¡¨ç°çš„å·¥å…·ï¼Œä½¿ç”¨äººç±»ä¸“å®¶çš„æ ‡å‡†è¿›è¡Œè¯„ä¼°ã€‚å®ƒåŒ…å«è¶…è¿‡7000å¯¹å“åº”æ ‡å‡†ï¼Œç”±ç‰©ç†ã€åŒ–å­¦ã€é‡‘èå’Œå’¨è¯¢ç­‰é¢†åŸŸçš„ä¸“å®¶è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚GPT-5-highï¼Œåœ¨æ•´ä½“è¡¨ç°ä¸Šä¹Ÿä»…è¾¾åˆ°65.9%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†ä¸“æœ‰æ¨¡å‹ä¸å¼€æ”¾æƒé‡æ¨¡å‹ä¹‹é—´çš„æ˜¾è‘—æ€§èƒ½å·®å¼‚ï¼Œä»¥åŠæ‰©å±•æ€ç»´åœ¨å¤„ç†å¤æ‚ä¸“ä¸šä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18940",
            "title": "NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient\n  Fine-Tuning",
            "url": "https://huggingface.co/papers/2510.18940",
            "abstract": "NeuroAda is a parameter-efficient fine-tuning method that combines selective adaptation with bypass connections to achieve high performance with minimal trainable parameters and reduced memory usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into two categories: addition-based and selective in-situ adaptation. The former, such as LoRA, introduce additional modules to adapt the model to downstream tasks, offering strong memory efficiency. However, their representational capacity is often limited, making them less suitable for fine-grained adaptation. In contrast, the latter directly fine-tunes a carefully chosen subset of the original model parameters, allowing for more precise and effective adaptation, but at the cost of significantly increased memory consumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT method that enables fine-grained model finetuning while maintaining high memory efficiency. Our approach first identifies important parameters (i.e., connections within the network) as in selective adaptation, and then introduces bypass connections for these selected parameters. During finetuning, only the bypass connections are updated, leaving the original model parameters frozen. Empirical results on 23+ tasks spanning both natural language generation and understanding demonstrate that NeuroAda achieves state-of-the-art performance with as little as leq 0.02% trainable parameters, while reducing CUDA memory usage by up to 60%. We release our code here: https://github.com/FightingFighting/NeuroAda.git.",
            "score": 2,
            "issue_id": 6568,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "fcfd58e5ced254c9",
            "authors": [
                "Zhi Zhang",
                "Yixian Shen",
                "Congfeng Cao",
                "Ekaterina Shutova"
            ],
            "affiliations": [
                "ILLC, University of Amsterdam",
                "PCS, University of Amsterdam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18940.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#small_models",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "NeuroAda: Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²",
                    "desc": "NeuroAda â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ (PEFT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ² ÑĞµÑ‚Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ´Ğ»Ñ Ğ½Ğ¸Ñ… bypass-ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ selective adaptation, Ğ½Ğ¾ Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ĞºĞ°Ğº Ñƒ LoRA. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 23+ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 0.02% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ CUDA Ğ´Ğ¾ 60%."
                },
                "en": {
                    "title": "Efficient Fine-Tuning with NeuroAda: Less is More!",
                    "desc": "NeuroAda is a new method for fine-tuning machine learning models that focuses on being efficient with parameters and memory. It combines two techniques: selective adaptation, which targets important model parameters, and bypass connections, which allow for updates without changing the entire model. This approach enables precise adjustments to the model while keeping most of it unchanged, leading to significant memory savings. Tests show that NeuroAda performs exceptionally well on various tasks, using only a tiny fraction of trainable parameters and reducing memory usage significantly."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¾®è°ƒï¼Œæ€§èƒ½å“è¶Šçš„NeuroAda",
                    "desc": "NeuroAdaæ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå®ƒç»“åˆäº†é€‰æ‹©æ€§é€‚åº”å’Œæ—è·¯è¿æ¥ï¼Œä»¥å®ç°é«˜æ€§èƒ½å’Œæœ€å°çš„å¯è®­ç»ƒå‚æ•°ã€‚è¯¥æ–¹æ³•é¦–å…ˆè¯†åˆ«å‡ºé‡è¦çš„å‚æ•°ï¼Œç„¶åä¸ºè¿™äº›å‚æ•°å¼•å…¥æ—è·¯è¿æ¥ã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä»…æ›´æ–°æ—è·¯è¿æ¥ï¼Œä¿æŒåŸå§‹æ¨¡å‹å‚æ•°ä¸å˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNeuroAdaåœ¨23ä¸ªä»¥ä¸Šçš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½¿ç”¨çš„å¯è®­ç»ƒå‚æ•°å°‘äº0.02%ï¼Œå¹¶ä¸”CUDAå†…å­˜ä½¿ç”¨é‡å‡å°‘äº†60%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18428",
            "title": "AlphaOPT: Formulating Optimization Programs with Self-Improving LLM\n  Experience Library",
            "url": "https://huggingface.co/papers/2510.18428",
            "abstract": "AlphaOPT is a self-improving library that enables an LLM to learn from limited demonstrations and solver feedback, improving optimization modeling across industries without costly retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Optimization modeling enables critical decisions across industries but remains difficult to automate: informal language must be mapped to precise mathematical formulations and executable solver code. Prior LLM approaches either rely on brittle prompting or costly retraining with limited generalization. We present AlphaOPT, a self-improving experience library that enables an LLM to learn from limited demonstrations (even answers alone, without gold-standard programs) and solver feedback - without annotated reasoning traces or parameter updates. AlphaOPT operates in a continual two-phase cycle: (i) a Library Learning phase that reflects on failed attempts, extracting solver-verified, structured insights as {taxonomy, condition, explanation, example}; and (ii) a Library Evolution phase that diagnoses retrieval misalignments and refines the applicability conditions of stored insights, improving transfer across tasks. This design (1) learns efficiently from limited demonstrations without curated rationales, (2) expands continually without costly retraining by updating the library rather than model weights, and (3) makes knowledge explicit and interpretable for human inspection and intervention. Experiments show that AlphaOPT steadily improves with more data (65% to 72% from 100 to 300 training items) and surpasses the strongest baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only on answers. Code and data are available at: https://github.com/Minw913/AlphaOPT.",
            "score": 2,
            "issue_id": 6568,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "6d876f8d11b4ea83",
            "authors": [
                "Minwei Kong",
                "Ao Qu",
                "Xiaotong Guo",
                "Wenbin Ouyang",
                "Chonghe Jiang",
                "Han Zheng",
                "Yining Ma",
                "Dingyi Zhuang",
                "Yuhan Tang",
                "Junyi Li",
                "Hai Wang",
                "Cathy Wu",
                "Jinhua Zhao"
            ],
            "affiliations": [
                "London School of Economics and Political Science",
                "Massachusetts Institute of Technology",
                "Singapore Management University",
                "Singapore-MIT Alliance for Research and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18428.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#dataset",
                    "#transfer_learning",
                    "#optimization",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "AlphaOPT â€” ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ°ÑÑÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹ Ğ¸Ğ· Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. AlphaOPT Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ²Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ²Ğ¸Ğ´Ğµ. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ OptiBench ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ 65% Ğ´Ğ¾ 72% Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ baseline Ğ½Ğ° 7.7%."
                },
                "en": {
                    "title": "Empowering LLMs to Optimize Without Costly Retraining",
                    "desc": "AlphaOPT is a novel library designed to enhance the capabilities of large language models (LLMs) in optimization modeling by learning from limited examples and solver feedback. It operates through a two-phase cycle: first, it learns from past failures to extract structured insights, and second, it refines these insights to improve their applicability across different tasks. This approach allows AlphaOPT to continuously evolve without the need for expensive retraining, as it updates its knowledge library instead of the model parameters. Experimental results demonstrate that AlphaOPT improves performance significantly with more data and outperforms existing methods on challenging datasets."
                },
                "zh": {
                    "title": "AlphaOPTï¼šè‡ªæˆ‘æ”¹è¿›çš„ä¼˜åŒ–å»ºæ¨¡åº“",
                    "desc": "AlphaOPTæ˜¯ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›çš„åº“ï¼Œèƒ½å¤Ÿè®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»æœ‰é™çš„ç¤ºèŒƒå’Œæ±‚è§£å™¨åé¦ˆä¸­å­¦ä¹ ï¼Œä»è€Œæé«˜ä¼˜åŒ–å»ºæ¨¡çš„èƒ½åŠ›ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„å¾ªç¯å·¥ä½œï¼šé¦–å…ˆæ˜¯åº“å­¦ä¹ é˜¶æ®µï¼Œä»å¤±è´¥çš„å°è¯•ä¸­æå–ç»è¿‡æ±‚è§£å™¨éªŒè¯çš„ç»“æ„åŒ–è§è§£ï¼›å…¶æ¬¡æ˜¯åº“æ¼”åŒ–é˜¶æ®µï¼Œè¯Šæ–­æ£€ç´¢ä¸åŒ¹é…å¹¶ä¼˜åŒ–å­˜å‚¨è§è§£çš„é€‚ç”¨æ¡ä»¶ã€‚AlphaOPTçš„è®¾è®¡ä½¿å¾—å®ƒèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜‚è´µé‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒæŒç»­æ‰©å±•å’Œæé«˜æ€§èƒ½ï¼Œå¹¶ä¸”ä½¿çŸ¥è¯†å¯¹äººç±»å¯è§£é‡Šå’Œå¯å¹²é¢„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlphaOPTåœ¨æ•°æ®é‡å¢åŠ æ—¶è¡¨ç°å‡ºæŒç»­çš„æ”¹è¿›ï¼Œä¸”åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¶…è¶Šäº†æœ€å¼ºåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19753",
            "title": "When Do Transformers Learn Heuristics for Graph Connectivity?",
            "url": "https://huggingface.co/papers/2510.19753",
            "abstract": "Transformers struggle with generalizable algorithms, preferring heuristics; a disentangled Transformer can learn graph algorithms within its capacity but resorts to heuristics otherwise.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformers often fail to learn generalizable algorithms, instead relying on brittle heuristics. Using graph connectivity as a testbed, we explain this phenomenon both theoretically and empirically. We consider a simplified Transformer architecture, the disentangled Transformer, and prove that an L-layer model has capacity to solve for graphs with diameters up to exactly 3^L, implementing an algorithm equivalent to computing powers of the adjacency matrix. We analyze the training-dynamics, and show that the learned strategy hinges on whether most training instances are within this model capacity. Within-capacity graphs (diameter leq 3^L) drive the learning of a correct algorithmic solution while beyond-capacity graphs drive the learning of a simple heuristic based on node degrees. Finally, we empirically demonstrate that restricting training data within a model's capacity leads to both standard and disentangled transformers learning the exact algorithm rather than the degree-based heuristic.",
            "score": 1,
            "issue_id": 6570,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "fea19a943f96f863",
            "authors": [
                "Qilin Ye",
                "Deqing Fu",
                "Robin Jia",
                "Vatsal Sharan"
            ],
            "affiliations": [
                "Duke University",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19753.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#graphs",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ ÑƒÑ‡Ğ°Ñ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ñ… ÑĞ²Ğ¾Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Transformer-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ñ‹ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¹ disentangled Transformer Ñ L ÑĞ»Ğ¾ÑĞ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ñ Ğ´Ğ¸Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ¼ Ğ´Ğ¾ 3^L, Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²Ğ¾Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ ÑĞ¼ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´: ĞµÑĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ñ… capacity Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ¾ Transformer ÑƒÑ‡Ğ¸Ñ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, Ğ° ĞµÑĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ â€” Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºÑƒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ÑÑ‚ĞµĞ¿ĞµĞ½ÑÑ… Ğ²ĞµÑ€ÑˆĞ¸Ğ½. ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ñ‹ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»."
                },
                "en": {
                    "title": "Unlocking Algorithm Learning in Transformers",
                    "desc": "This paper investigates the limitations of Transformers in learning generalizable algorithms, particularly in the context of graph connectivity. It introduces the disentangled Transformer, which can theoretically solve graph problems up to a certain complexity defined by its layers. The study shows that when training data is within the model's capacity, the Transformer learns the correct algorithm, while data beyond this capacity leads to reliance on simpler heuristics. The findings emphasize the importance of aligning training data complexity with model capacity to achieve effective learning outcomes."
                },
                "zh": {
                    "title": "è§£è€¦å˜æ¢å™¨ï¼šä»å¯å‘å¼åˆ°ç®—æ³•çš„è½¬å˜",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å˜æ¢å™¨ï¼ˆTransformersï¼‰åœ¨å­¦ä¹ å¯æ¨å¹¿ç®—æ³•æ—¶çš„å›°éš¾ï¼Œé€šå¸¸ä¾èµ–äºè„†å¼±çš„å¯å‘å¼æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨å›¾çš„è¿é€šæ€§ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œç†è®ºå’Œå®è¯åˆ†æäº†è¿™ä¸€ç°è±¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§£è€¦å˜æ¢å™¨ï¼ˆdisentangled Transformerï¼‰èƒ½å¤Ÿåœ¨å…¶èƒ½åŠ›èŒƒå›´å†…å­¦ä¹ å›¾ç®—æ³•ï¼Œä½†åœ¨è¶…å‡ºèƒ½åŠ›èŒƒå›´æ—¶åˆ™é€€åŒ–ä¸ºå¯å‘å¼æ–¹æ³•ã€‚é€šè¿‡é™åˆ¶è®­ç»ƒæ•°æ®åœ¨æ¨¡å‹èƒ½åŠ›èŒƒå›´å†…ï¼Œå¯ä»¥ä½¿å˜æ¢å™¨å­¦ä¹ åˆ°å‡†ç¡®çš„ç®—æ³•ï¼Œè€Œä¸æ˜¯åŸºäºèŠ‚ç‚¹åº¦çš„å¯å‘å¼æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19492",
            "title": "Machine Text Detectors are Membership Inference Attacks",
            "url": "https://huggingface.co/papers/2510.19492",
            "abstract": "Theoretical and empirical investigation shows strong transferability between membership inference attacks and machine-generated text detection, highlighting the need for cross-task collaboration and introducing MINT for unified evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Although membership inference attacks (MIAs) and machine-generated text detection target different goals, identifying training samples and synthetic texts, their methods often exploit similar signals based on a language model's probability distribution. Despite this shared methodological foundation, the two tasks have been independently studied, which may lead to conclusions that overlook stronger methods and valuable insights developed in the other task. In this work, we theoretically and empirically investigate the transferability, i.e., how well a method originally developed for one task performs on the other, between MIAs and machine text detection. For our theoretical contribution, we prove that the metric that achieves the asymptotically highest performance on both tasks is the same. We unify a large proportion of the existing literature in the context of this optimal metric and hypothesize that the accuracy with which a given method approximates this metric is directly correlated with its transferability. Our large-scale empirical experiments, including 7 state-of-the-art MIA methods and 5 state-of-the-art machine text detectors across 13 domains and 10 generators, demonstrate very strong rank correlation (rho > 0.6) in cross-task performance. We notably find that Binoculars, originally designed for machine text detection, achieves state-of-the-art performance on MIA benchmarks as well, demonstrating the practical impact of the transferability. Our findings highlight the need for greater cross-task awareness and collaboration between the two research communities. To facilitate cross-task developments and fair evaluations, we introduce MINT, a unified evaluation suite for MIAs and machine-generated text detection, with implementation of 15 recent methods from both tasks.",
            "score": 1,
            "issue_id": 6570,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            },
            "hash": "a43f47a5cd4d15c3",
            "authors": [
                "Ryuto Koike",
                "Liam Dugan",
                "Masahiro Kaneko",
                "Chris Callison-Burch",
                "Naoaki Okazaki"
            ],
            "affiliations": [
                "Institute of Science Tokyo",
                "MBZUAI",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19492.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#transfer_learning",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ”Ğ²Ğ° Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼: Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ AI-Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ´Ğ½Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‡Ğ»ĞµĞ½ÑÑ‚Ğ²Ğ° Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ (MIA) Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¾Ğ±Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸Ğ· Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ°, Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Binoculars, Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ AI-Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ Ğ² MIA Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ”Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MINT â€” ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 15 Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ±ĞµĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging the Gap: Unifying Membership Inference and Text Detection",
                    "desc": "This paper explores the connection between membership inference attacks (MIAs) and machine-generated text detection, revealing that methods from one task can effectively apply to the other. The authors demonstrate that both tasks share a common metric that maximizes performance, suggesting that insights from one area can enhance the other. Through extensive experiments, they show a strong correlation in performance across different methods, indicating that techniques like Binoculars can excel in both domains. To support collaboration and evaluation, the paper introduces MINT, a unified framework for assessing both MIAs and machine-generated text detection methods."
                },
                "zh": {
                    "title": "è·¨ä»»åŠ¡åˆä½œï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æˆå‘˜æ¨æ–­æ”»å‡»ï¼ˆMIAï¼‰ä¸æœºå™¨ç”Ÿæˆæ–‡æœ¬æ£€æµ‹ä¹‹é—´çš„å¼ºè½¬ç§»æ€§ï¼Œå¼ºè°ƒäº†è·¨ä»»åŠ¡åˆä½œçš„é‡è¦æ€§ã€‚å°½ç®¡è¿™ä¸¤é¡¹ä»»åŠ¡çš„ç›®æ ‡ä¸åŒï¼Œä½†å®ƒä»¬çš„æ–¹æ³•å¸¸å¸¸åŸºäºè¯­è¨€æ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒï¼Œåˆ©ç”¨ç›¸ä¼¼çš„ä¿¡å·ã€‚æˆ‘ä»¬è¯æ˜äº†åœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸­ï¼Œè¾¾åˆ°æœ€é«˜æ€§èƒ½çš„åº¦é‡æ˜¯ç›¸åŒçš„ï¼Œå¹¶ä¸”æå‡ºäº†MINTï¼Œä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°å·¥å…·ï¼Œä»¥ä¿ƒè¿›è¿™ä¸¤ä¸ªé¢†åŸŸçš„äº¤å‰å‘å±•ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†ä¸åŒä»»åŠ¡ä¹‹é—´çš„æ€§èƒ½æ’åå…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œæ˜¾ç¤ºäº†è½¬ç§»æ€§çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.18917",
            "title": "RIR-Mega: a large-scale simulated room impulse response dataset for\n  machine learning and room acoustics modeling",
            "url": "https://huggingface.co/papers/2510.18917",
            "abstract": "RIR-Mega is a large dataset of simulated room impulse responses with tools for validation and a baseline model for predicting RT60 from waveforms.  \t\t\t\t\tAI-generated summary \t\t\t\t Room impulse responses are a core resource for dereverberation, robust speech recognition, source localization, and room acoustics estimation. We present RIR-Mega, a large collection of simulated RIRs described by a compact, machine friendly metadata schema and distributed with simple tools for validation and reuse. The dataset ships with a Hugging Face Datasets loader, scripts for metadata checks and checksums, and a reference regression baseline that predicts RT60 like targets from waveforms. On a train and validation split of 36,000 and 4,000 examples, a small Random Forest on lightweight time and spectral features reaches a mean absolute error near 0.013 s and a root mean square error near 0.022 s. We host a subset with 1,000 linear array RIRs and 3,000 circular array RIRs on Hugging Face for streaming and quick tests, and preserve the complete 50,000 RIR archive on Zenodo. The dataset and code are public to support reproducible studies.",
            "score": 1,
            "issue_id": 6568,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "2355986738e20368",
            "authors": [
                "Mandip Goswami"
            ],
            "affiliations": [
                "Amazon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.18917.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#open_source",
                    "#science"
                ],
                "emoji": "ğŸ”Š",
                "ru": {
                    "title": "ĞœĞµĞ³Ğ°-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ñ… Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ RIR-Mega â€” ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ñ… Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ (room impulse responses), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´ĞµÑ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°ĞºÑƒÑÑ‚Ğ¸ĞºĞ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 50,000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RT60 Ğ¸Ğ· Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Random Forest Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 0.013 ÑĞµĞºÑƒĞ½Ğ´ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ½Ğ° Hugging Face Ğ¸ Zenodo Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "RIR-Mega: A Comprehensive Dataset for Room Impulse Response Analysis",
                    "desc": "RIR-Mega is a comprehensive dataset designed for simulating room impulse responses (RIRs), which are essential for various audio processing tasks like dereverberation and speech recognition. It includes a structured metadata schema and tools for easy validation and reuse, making it accessible for researchers. The dataset features a baseline model that utilizes a Random Forest algorithm to predict RT60 values from audio waveforms, achieving impressive accuracy metrics. Additionally, RIR-Mega is available on platforms like Hugging Face and Zenodo, promoting reproducibility in research."
                },
                "zh": {
                    "title": "RIR-Megaï¼šæˆ¿é—´è„‰å†²å“åº”æ•°æ®é›†çš„åˆ›æ–°ä¸åº”ç”¨",
                    "desc": "RIR-Megaæ˜¯ä¸€ä¸ªå¤§å‹çš„æ¨¡æ‹Ÿæˆ¿é—´è„‰å†²å“åº”æ•°æ®é›†ï¼Œæ—¨åœ¨ä¸ºå»æ··å“ã€ç¨³å¥çš„è¯­éŸ³è¯†åˆ«ã€å£°æºå®šä½å’Œæˆ¿é—´å£°å­¦ä¼°è®¡æä¾›æ ¸å¿ƒèµ„æºã€‚è¯¥æ•°æ®é›†é‡‡ç”¨ç´§å‡‘çš„å…ƒæ•°æ®æ¶æ„ï¼Œä¾¿äºæœºå™¨å¤„ç†ï¼Œå¹¶é…å¤‡ç®€å•çš„éªŒè¯å’Œé‡ç”¨å·¥å…·ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºå‡†æ¨¡å‹ï¼Œä½¿ç”¨è½»é‡çº§çš„æ—¶é—´å’Œé¢‘è°±ç‰¹å¾ï¼Œé€šè¿‡éšæœºæ£®æ—ç®—æ³•é¢„æµ‹RT60ï¼Œå–å¾—äº†è¾ƒä½çš„å¹³å‡ç»å¯¹è¯¯å·®å’Œå‡æ–¹æ ¹è¯¯å·®ã€‚æ•°æ®é›†å’Œä»£ç éƒ½æ˜¯å…¬å¼€çš„ï¼Œä»¥æ”¯æŒå¯é‡å¤çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.15050",
            "title": "Directional Reasoning Injection for Fine-Tuning MLLMs",
            "url": "https://huggingface.co/papers/2510.15050",
            "abstract": "DRIFT, a lightweight method, enhances multimodal large language models' reasoning ability by transferring knowledge in gradient space, outperforming naive merging and supervised fine-tuning with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) are rapidly advancing, yet their reasoning ability often lags behind that of strong text-only counterparts. Existing methods to bridge this gap rely on supervised fine-tuning over large-scale multimodal reasoning data or reinforcement learning, both of which are resource-intensive. A promising alternative is model merging, which interpolates parameters between reasoning-enhanced LLMs and multimodal variants. However, our analysis shows that naive merging is not always a \"free lunch\": its effectiveness varies drastically across model families, with some (e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance degradation. To address this, we propose Directional Reasoning Injection for Fine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning knowledge in the gradient space, without destabilizing multimodal alignment. DRIFT precomputes a reasoning prior as the parameter-space difference between reasoning and multimodal variants, then uses it to bias gradients during multimodal fine-tuning. This approach preserves the simplicity of standard supervised fine-tuning pipelines while enabling efficient reasoning transfer. Extensive experiments on multimodal reasoning benchmarks, including MathVista and MathVerse, demonstrate that DRIFT consistently improves reasoning performance over naive merging and supervised fine-tuning, while matching or surpassing training-heavy methods at a fraction of the cost.",
            "score": 1,
            "issue_id": 6574,
            "pub_date": "2025-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "9c13d4d86aef12dc",
            "authors": [
                "Chao Huang",
                "Zeliang Zhang",
                "Jiang Liu",
                "Ximeng Sun",
                "Jialian Wu",
                "Xiaodong Yu",
                "Ze Wang",
                "Chenliang Xu",
                "Emad Barsoum",
                "Zicheng Liu"
            ],
            "affiliations": [
                "Advanced Micro Devices, Inc.",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.15050.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ½Ğ¾Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DRIFT â€” ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ñ‘Ğ¼ĞºĞ¾Ğ³Ğ¾ supervised fine-tuning Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². DRIFT Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµÑ‘ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DRIFT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ°Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ fine-tuning Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "DRIFT: Efficient Reasoning Enhancement for Multimodal Models",
                    "desc": "The paper introduces DRIFT, a novel method designed to enhance the reasoning capabilities of multimodal large language models (MLLMs) by transferring knowledge in the gradient space. Unlike traditional approaches that require extensive resources for supervised fine-tuning or reinforcement learning, DRIFT offers a lightweight alternative that maintains multimodal alignment. It precomputes a reasoning prior, which is the difference in parameters between reasoning-enhanced and multimodal models, and uses this to adjust gradients during fine-tuning. Experimental results show that DRIFT outperforms naive merging and standard fine-tuning methods, achieving better reasoning performance with significantly lower computational costs."
                },
                "zh": {
                    "title": "DRIFTï¼šé«˜æ•ˆæ¨ç†èƒ½åŠ›çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆ",
                    "desc": "DRIFTæ˜¯ä¸€ç§è½»é‡çº§çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¢¯åº¦ç©ºé—´ä¸­è½¬ç§»çŸ¥è¯†ï¼Œå¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„ç®€å•åˆå¹¶å’Œç›‘ç£å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒDRIFTåœ¨è®¡ç®—æˆæœ¬ä¸Šæ˜¾è‘—é™ä½ï¼ŒåŒæ—¶æå‡äº†æ¨ç†æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„è®¡ç®—æ¨ç†å…ˆéªŒï¼Œåˆ©ç”¨å‚æ•°ç©ºé—´çš„å·®å¼‚æ¥åç½®å¤šæ¨¡æ€å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦ï¼Œä»è€Œä¿æŒå¤šæ¨¡æ€å¯¹é½çš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRIFTåœ¨å¤šä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜äºç®€å•åˆå¹¶å’Œç›‘ç£å¾®è°ƒï¼Œä¸”åœ¨æˆæœ¬ä¸Šè¿œä½äºè®­ç»ƒå¯†é›†å‹çš„æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-22.html",
    "link_next": "2025-10-24.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "22.10",
        "en": "10/22",
        "zh": "10æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "24.10",
        "en": "10/24",
        "zh": "10æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 9,
        "#data": 6,
        "#benchmark": 12,
        "#agents": 6,
        "#cv": 2,
        "#rl": 5,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 8,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 14,
        "#robotics": 0,
        "#agi": 1,
        "#games": 4,
        "#interpretability": 3,
        "#reasoning": 8,
        "#transfer_learning": 7,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 16,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 7,
        "#small_models": 2,
        "#science": 2,
        "#low_resource": 1
    }
}