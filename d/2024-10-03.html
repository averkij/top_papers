
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 24 papers. October 3.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">24 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-02.html">â¬…ï¸ <span id="prev-date">02.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-04.html">â¡ï¸ <span id="next-date">04.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'};
        let feedDateNext = {'ru': '04.10', 'en': '10/04', 'zh': '10æœˆ4æ—¥'};
        let feedDatePrev = {'ru': '02.10', 'en': '10/02', 'zh': '10æœˆ2æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'Published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.01044', 'title': 'RATIONALYST: Pre-training Process-Supervision for Improving Reasoning', 'url': 'https://huggingface.co/papers/2410.01044', 'abstract': 'The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce RATIONALYST, a model for process-supervision of reasoning based on pre-training on a vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and a combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows RATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets.', 'score': 34, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '7443e92db63fd869', 'data': {'categories': ['#science', '#reasoning', '#dataset', '#multilingual', '#training', '#math', '#data', '#transfer_learning', '#benchmark', '#architecture', '#synthetic'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RATIONALYST: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ RATIONALYST - Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 79 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. RATIONALYST Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 3.9% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 7 ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4.'}, 'en': {'title': 'RATIONALYST: Enhancing Reasoning with Explicit Rationale Learning', 'desc': 'RATIONALYST is a new model designed to improve reasoning in language models by addressing the issue of incomplete reasoning steps. It achieves this by pre-training on a large dataset of rationale annotations, which helps the model learn to make its reasoning more explicit. By extracting 79,000 rationales from a vast collection of unlabeled data, RATIONALYST can generalize effectively across various reasoning tasks, such as mathematical and commonsense reasoning. The model shows a notable improvement in accuracy, outperforming larger models like GPT-4 on several reasoning benchmarks.'}, 'zh': {'title': 'RATIONALYSTï¼šæå‡æ¨ç†å‡†ç¡®æ€§çš„åˆ›æ–°æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRATIONALYSTçš„æ¨¡å‹ï¼Œæ—¨åœ¨æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸è¶³ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨å¤§é‡æœªæ ‡æ³¨æ•°æ®ä¸­æå–çš„79,000ä¸ªæ¨ç†ç†ç”±è¿›è¡Œé¢„è®­ç»ƒï¼Œä»è€Œå®ç°äº†å¯¹æ¨ç†è¿‡ç¨‹çš„ç›‘ç£ã€‚RATIONALYSTåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬æ•°å­¦ã€å¸¸è¯†ã€ç§‘å­¦å’Œé€»è¾‘æ¨ç†ï¼Œå¹³å‡æé«˜äº†3.9%çš„å‡†ç¡®ç‡ã€‚ä¸æ›´å¤§è§„æ¨¡çš„éªŒè¯æ¨¡å‹å¦‚GPT-4ç›¸æ¯”ï¼ŒRATIONALYSTåœ¨æ¨ç†å‡†ç¡®æ€§ä¸Šä¹Ÿè¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01680', 'title': 'PHI-S: Distribution Balancing for Label-Free Multi-Teacher Distillation', 'url': 'https://huggingface.co/papers/2410.01680', 'abstract': 'Various visual foundation models have distinct strengths and weaknesses, both of which can be improved through heterogeneous multi-teacher knowledge distillation without labels, termed "agglomerative models." We build upon this body of work by studying the effect of the teachers\' activation statistics, particularly the impact of the loss function on the resulting student model quality. We explore a standard toolkit of statistical normalization techniques to better align the different distributions and assess their effects. Further, we examine the impact on downstream teacher-matching metrics, which motivates the use of Hadamard matrices. With these matrices, we demonstrate useful properties, showing how they can be used for isotropic standardization, where each dimension of a multivariate distribution is standardized using the same scale. We call this technique "PHI Standardization" (PHI-S) and empirically demonstrate that it produces the best student model across the suite of methods studied.', 'score': 32, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'dc301080cf253805', 'data': {'categories': ['#cv', '#training', '#optimization', '#transfer_learning', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'PHI-ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ³Ğ»Ğ¾Ğ¼ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ³Ğ»Ğ¾Ğ¼ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'PHI-ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸' Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† ĞĞ´Ğ°Ğ¼Ğ°Ñ€Ğ°, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ€ĞµĞ´Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ²."}, 'en': {'title': 'Enhancing Student Models with PHI Standardization', 'desc': "This paper explores how to improve visual foundation models using a technique called heterogeneous multi-teacher knowledge distillation without labels, known as agglomerative models. It focuses on the importance of the teachers' activation statistics and how different loss functions affect the quality of the student model. The authors investigate various statistical normalization techniques to align the distributions of the teachers' outputs and their impact on matching metrics. They introduce a new method called PHI Standardization (PHI-S), which standardizes multivariate distributions effectively, leading to superior student model performance."}, 'zh': {'title': 'èšåˆæ¨¡å‹ï¼šæå‡è§†è§‰æ¨¡å‹çš„æœ€ä½³å®è·µ', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†ä¸åŒè§†è§‰åŸºç¡€æ¨¡å‹åœ¨æ— æ ‡ç­¾æƒ…å†µä¸‹é€šè¿‡å¼‚æ„å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦çš„æ”¹è¿›æ–¹æ³•ï¼Œç§°ä¸ºâ€œèšåˆæ¨¡å‹â€ã€‚æˆ‘ä»¬é‡ç‚¹åˆ†æäº†æ•™å¸ˆæ¨¡å‹çš„æ¿€æ´»ç»Ÿè®¡ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯æŸå¤±å‡½æ•°å¯¹å­¦ç”Ÿæ¨¡å‹è´¨é‡çš„å½±å“ã€‚é€šè¿‡ä½¿ç”¨ç»Ÿè®¡å½’ä¸€åŒ–æŠ€æœ¯ï¼Œæˆ‘ä»¬æ›´å¥½åœ°å¯¹é½ä¸åŒåˆ†å¸ƒï¼Œå¹¶è¯„ä¼°å…¶æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†HadamardçŸ©é˜µï¼Œå±•ç¤ºäº†å…¶åœ¨å„ç»´åº¦æ ‡å‡†åŒ–ä¸­çš„æœ‰ç”¨ç‰¹æ€§ï¼Œæå‡ºäº†â€œPHIæ ‡å‡†åŒ–â€ï¼ˆPHI-Sï¼‰æŠ€æœ¯ï¼Œå¹¶å®éªŒè¯æ˜å…¶åœ¨å¤šç§æ–¹æ³•ä¸­äº§ç”Ÿäº†æœ€ä½³çš„å­¦ç”Ÿæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01215', 'title': 'From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging', 'url': 'https://huggingface.co/papers/2410.01215', 'abstract': 'While large language models have made significant strides in code generation, the pass rate of the generated code is bottlenecked on subtle errors, often requiring human intervention to pass tests, especially for complex problems. Existing LLM-based debugging systems treat generated programs as monolithic units, failing to address bugs at multiple levels of granularity, from low-level syntax errors to high-level algorithmic flaws. In this paper, we introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger by isolating, identifying, and resolving bugs at various levels of granularity. MGDebugger decomposes problematic code into a hierarchical tree structure of subfunctions, with each level representing a particular granularity of error. During debugging, it analyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately. Extensive experiments demonstrate that MGDebugger outperforms existing debugging systems, achieving an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6% repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes bugs across different categories and difficulty levels, demonstrating its robustness and effectiveness.', 'score': 30, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'c2e7c70ae8f76f9b', 'data': {'categories': ['#reasoning', '#inference', '#interpretability', '#plp', '#optimization', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ»Ğ°Ğ´Ñ‡Ğ¸Ğº ĞºĞ¾Ğ´Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞµ ĞºĞ¾Ğ´Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Multi-Granularity Debugger (MGDebugger), Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ² Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ´Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. MGDebugger Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM-ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ Python Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Hierarchical Debugging for Enhanced Code Generation Accuracy', 'desc': 'This paper presents the Multi-Granularity Debugger (MGDebugger), a novel hierarchical debugging system designed to improve the accuracy of code generated by large language models (LLMs). Unlike traditional debugging systems that treat code as a single unit, MGDebugger breaks down code into a tree structure of subfunctions, allowing for the identification and resolution of errors at various levels of granularity. The system employs an LLM-simulated Python executor to trace code execution and monitor variable states, enabling precise error detection. Experimental results show that MGDebugger significantly enhances debugging performance, achieving higher accuracy and repair success rates compared to existing methods.'}, 'zh': {'title': 'å¤šç²’åº¦è°ƒè¯•ï¼Œæå‡ä»£ç ä¿®å¤æ•ˆç‡', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ä»£ç è°ƒè¯•ç³»ç»Ÿï¼Œç§°ä¸ºå¤šç²’åº¦è°ƒè¯•å™¨ï¼ˆMGDebuggerï¼‰ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†ä»£ç åˆ†è§£ä¸ºå±‚æ¬¡æ ‘ç»“æ„ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒç²’åº¦ä¸Šè¯†åˆ«å’Œè§£å†³é”™è¯¯ï¼Œä»ä½çº§è¯­æ³•é”™è¯¯åˆ°é«˜çº§ç®—æ³•ç¼ºé™·ã€‚MGDebuggeré‡‡ç”¨è‡ªåº•å‘ä¸Šçš„æ–¹æ³•ï¼Œé€ä¸ªåˆ†æå­å‡½æ•°å¹¶è§£å†³é—®é¢˜ï¼Œç¡®ä¿è°ƒè¯•è¿‡ç¨‹çš„é«˜æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMGDebuggeråœ¨ä»£ç ä¿®å¤çš„å‡†ç¡®æ€§å’ŒæˆåŠŸç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„è°ƒè¯•ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01647', 'title': '3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection', 'url': 'https://huggingface.co/papers/2410.01647', 'abstract': 'Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and have been adapted for 3D Object Detection (3DOD), offering a promising approach to 3DOD through view-synthesis representation. However, NeRF faces inherent limitations: (i) limited representational capacity for 3DOD due to its implicit nature, and (ii) slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS) has emerged as an explicit 3D representation that addresses these limitations. Inspired by these advantages, this paper introduces 3DGS into 3DOD for the first time, identifying two main challenges: (i) Ambiguous spatial distribution of Gaussian blobs: 3DGS primarily relies on 2D pixel-level supervision, resulting in unclear 3D spatial distribution of Gaussian blobs and poor differentiation between objects and background, which hinders 3DOD; (ii) Excessive background blobs: 2D images often include numerous background pixels, leading to densely reconstructed 3DGS with many noisy Gaussian blobs representing the background, negatively affecting detection. To tackle the challenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D images, and propose an elegant and efficient solution by incorporating 2D Boundary Guidance to significantly enhance the spatial distribution of Gaussian blobs, resulting in clearer differentiation between objects and their background. To address the challenge (ii), we propose a Box-Focused Sampling strategy using 2D boxes to generate object probability distribution in 3D spaces, allowing effective probabilistic sampling in 3D to retain more object blobs and reduce noisy background blobs. Benefiting from our designs, our 3DGS-DET significantly outperforms the SOTA NeRF-based method, NeRF-Det, achieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet dataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset.', 'score': 28, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '951512e0e25bc7da', 'data': {'categories': ['#dataset', '#cv', '#graphs', '#optimization', '#benchmark', '#3d'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': '3DGS-DET: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ 3D-Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ 3D Gaussian Splatting (3DGS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼: Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ±Ğ»Ğ¾Ğ±Ğ¾Ğ² Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±Ğ»Ğ¾Ğ±Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ 2D Boundary Guidance, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ»Ğ¾Ğ±Ğ¾Ğ². Ğ’Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Box-Focused Sampling, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ±Ğ»Ğ¾Ğ±Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… Ñ„Ğ¾Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±Ğ»Ğ¾Ğ±Ğ¾Ğ².'}, 'en': {'title': 'Enhancing 3D Object Detection with 3D Gaussian Splatting', 'desc': 'This paper presents a novel approach to 3D Object Detection (3DOD) by integrating 3D Gaussian Splatting (3DGS) with traditional methods. The authors identify two main challenges with 3DGS: unclear spatial distribution of Gaussian blobs and excessive background noise from 2D images. To improve the clarity of object differentiation, they introduce 2D Boundary Guidance, which enhances the spatial arrangement of Gaussian blobs. Additionally, they propose a Box-Focused Sampling strategy to effectively reduce background noise while retaining important object information, leading to significant performance improvements over existing methods.'}, 'zh': {'title': '3DGSï¼šæå‡ä¸‰ç»´ç‰©ä½“æ£€æµ‹çš„æ–°æ–¹æ³•', 'desc': 'ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æ–°è§†è§’åˆæˆå’Œä¸‰ç»´ç‰©ä½“æ£€æµ‹ï¼ˆ3DODï¼‰ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨ä¸€äº›å›ºæœ‰çš„å±€é™æ€§ï¼Œå¦‚éšå¼è¡¨ç¤ºå¯¼è‡´çš„è¡¨ç¤ºèƒ½åŠ›æœ‰é™å’Œæ¸²æŸ“é€Ÿåº¦æ…¢ã€‚æœ€è¿‘ï¼Œä¸‰ç»´é«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰ä½œä¸ºä¸€ç§æ˜¾å¼ä¸‰ç»´è¡¨ç¤ºæ–¹æ³•ï¼Œè§£å†³äº†è¿™äº›é—®é¢˜ã€‚æœ¬æ–‡é¦–æ¬¡å°†3DGSå¼•å…¥3DODï¼Œæå‡ºäº†ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šé«˜æ–¯æ–‘ç‚¹çš„ç©ºé—´åˆ†å¸ƒæ¨¡ç³Šå’ŒèƒŒæ™¯æ–‘ç‚¹è¿‡å¤šã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†2Dè¾¹ç•Œå¼•å¯¼å’ŒåŸºäºç›’å­çš„é‡‡æ ·ç­–ç•¥ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç‰©ä½“ä¸èƒŒæ™¯çš„åŒºåˆ†åº¦ï¼Œå¹¶å‡å°‘äº†å™ªå£°èƒŒæ™¯æ–‘ç‚¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01748', 'title': 'Not All LLM Reasoners Are Created Equal', 'url': 'https://huggingface.co/papers/2410.01748', 'abstract': 'We study the depth of grade-school math (GSM) problem-solving capabilities of LLMs. To this end, we evaluate their performance on pairs of existing math word problems together so that the answer to the second problem depends on correctly answering the first problem. Our findings reveal a significant reasoning gap in most LLMs, that is performance difference between solving the compositional pairs and solving each question independently. This gap is more pronounced in smaller, more cost-efficient, and math-specialized models. Moreover, instruction-tuning recipes and code generation have varying effects across LLM sizes, while finetuning on GSM can lead to task overfitting. Our analysis indicates that large reasoning gaps are not because of test-set leakage, but due to distraction from additional context and poor second-hop reasoning. Overall, LLMs exhibit systematic differences in their reasoning abilities, despite what their performance on standard benchmarks indicates.', 'score': 27, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '4179f096b2f31dbd', 'data': {'categories': ['#reasoning', '#leakage', '#training', '#math', '#optimization', '#benchmark', '#small_models'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾Ñ‚Ñ‹ĞºĞ°ÑÑ‚ÑÑ Ğ½Ğ° ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑˆĞºĞ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ½Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ… ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ³Ğ´Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° LLM, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ² Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ…, Ğ±Ğ¾Ğ»ĞµĞµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ñ‹ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ Ğ¾Ñ‚Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ, Ğ° Ğ½Ğµ Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¾Ğ¹ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Uncovering Reasoning Gaps in LLMs for Math Problem Solving', 'desc': 'This paper investigates how well large language models (LLMs) can solve grade-school math problems, especially when the answer to one problem relies on the answer to another. The study finds that there is a notable reasoning gap, meaning LLMs perform worse when problems are connected compared to when they are solved separately. Smaller and more specialized models show an even larger gap in their reasoning abilities. The research suggests that this gap is not due to issues like test-set leakage, but rather because of distractions from extra information and difficulties in multi-step reasoning.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å·®è·', 'desc': 'æˆ‘ä»¬ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å°å­¦æ•°å­¦é—®é¢˜ï¼ˆGSMï¼‰æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡è¯„ä¼°æ¨¡å‹åœ¨ä¸€å¯¹æ•°å­¦åº”ç”¨é¢˜ä¸Šçš„è¡¨ç°ï¼Œæˆ‘ä»¬å‘ç°å¤§å¤šæ•°LLMsåœ¨è§£å†³ç»„åˆé—®é¢˜æ—¶å­˜åœ¨æ˜¾è‘—çš„æ¨ç†å·®è·ã€‚è¿™ä¸ªå·®è·åœ¨è¾ƒå°ã€æˆæœ¬æ•ˆç›Šé«˜ä¸”ä¸“æ³¨äºæ•°å­¦çš„æ¨¡å‹ä¸­æ›´ä¸ºæ˜æ˜¾ã€‚æ­¤å¤–ï¼ŒæŒ‡ä»¤è°ƒä¼˜å’Œä»£ç ç”Ÿæˆå¯¹ä¸åŒè§„æ¨¡çš„LLMsæœ‰ä¸åŒçš„å½±å“ï¼Œè€Œåœ¨GSMä¸Šè¿›è¡Œå¾®è°ƒå¯èƒ½å¯¼è‡´ä»»åŠ¡è¿‡æ‹Ÿåˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01744', 'title': 'LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks', 'url': 'https://huggingface.co/papers/2410.01744', 'abstract': "Text-rich images, where text serves as the central visual element guiding the overall understanding, are prevalent in real-world applications, such as presentation slides, scanned documents, and webpage snapshots. Tasks involving multiple text-rich images are especially challenging, as they require not only understanding the content of individual images but reasoning about inter-relationships and logical flows across multiple visual inputs. Despite the importance of these scenarios, current multimodal large language models (MLLMs) struggle to handle such tasks due to two key challenges: (1) the scarcity of high-quality instruction tuning datasets for text-rich multi-image scenarios, and (2) the difficulty in balancing image resolution with visual feature sequence length. To address these challenges, we propose \\OurMethod, a MLLM designed specifically for handling vision-language tasks involving multiple text-rich images. First, we curated about one million high-quality multimodal instruction-tuning data, tailored to text-rich, multi-image scenarios. Second, we developed an adaptive high-resolution multi-image encoding module to dynamically optimize the allocation of visual sequence length based on the original aspect ratios and resolutions of the input images. Experiments across a wide range of benchmarks demonstrate our model's superior capabilities in text-rich, multi-image evaluations and competitive performance in general domain evaluations.", 'score': 24, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'f787d537107fa831', 'data': {'categories': ['#reasoning', '#dataset', '#cv', '#training', '#data', '#optimization', '#transfer_learning', '#benchmark', '#architecture', '#synthetic', '#multimodal'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ğ°Ñ MLLM Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM) Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Empowering Multimodal Understanding with OurMethod', 'desc': 'This paper introduces a new multimodal large language model (MLLM) called \textit{OurMethod}, which is designed to effectively process and understand multiple text-rich images. The authors address two main challenges: the lack of quality datasets for training and the need to balance image resolution with the length of visual features. They created a dataset of about one million high-quality instruction-tuning examples specifically for text-rich, multi-image tasks. Additionally, they developed a novel encoding module that adapts to the resolutions of input images, leading to improved performance in evaluating text-rich, multi-image scenarios.'}, 'zh': {'title': 'ä¸“ä¸ºæ–‡æœ¬ä¸°å¯Œå›¾åƒè®¾è®¡çš„å¤šæ¨¡æ€æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä¸“é—¨ç”¨äºå¤„ç†åŒ…å«å¤šä¸ªæ–‡æœ¬ä¸°å¯Œå›¾åƒçš„è§†è§‰-è¯­è¨€ä»»åŠ¡ã€‚æˆ‘ä»¬é¦–å…ˆæ”¶é›†äº†çº¦ä¸€ç™¾ä¸‡ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ•°æ®ï¼Œä¸“é—¨é’ˆå¯¹æ–‡æœ¬ä¸°å¯Œçš„å¤šå›¾åƒåœºæ™¯ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è‡ªé€‚åº”é«˜åˆ†è¾¨ç‡å¤šå›¾åƒç¼–ç æ¨¡å—ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥å›¾åƒçš„åŸå§‹çºµæ¨ªæ¯”å’Œåˆ†è¾¨ç‡åŠ¨æ€ä¼˜åŒ–è§†è§‰åºåˆ—é•¿åº¦çš„åˆ†é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–‡æœ¬ä¸°å¯Œçš„å¤šå›¾åƒè¯„ä¼°ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå¹¶åœ¨ä¸€èˆ¬é¢†åŸŸè¯„ä¼°ä¸­ä¹Ÿå…·æœ‰ç«äº‰åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01463', 'title': 'Selective Aggregation for Low-Rank Adaptation in Federated Learning', 'url': 'https://huggingface.co/papers/2410.01463', 'abstract': 'We investigate LoRA in federated learning through the lens of the asymmetry analysis of the learned A and B matrices. In doing so, we uncover that A matrices are responsible for learning general knowledge, while B matrices focus on capturing client-specific knowledge. Based on this finding, we introduce Federated Share-A Low-Rank Adaptation (FedSA-LoRA), which employs two low-rank trainable matrices A and B to model the weight update, but only A matrices are shared with the server for aggregation. Moreover, we delve into the relationship between the learned A and B matrices in other LoRA variants, such as rsLoRA and VeRA, revealing a consistent pattern. Consequently, we extend our FedSA-LoRA method to these LoRA variants, resulting in FedSA-rsLoRA and FedSA-VeRA. In this way, we establish a general paradigm for integrating LoRA with FL, offering guidance for future work on subsequent LoRA variants combined with FL. Extensive experimental results on natural language understanding and generation tasks demonstrate the effectiveness of the proposed method.', 'score': 18, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '44e5fb0db92df32b', 'data': {'categories': ['#training', '#rl', '#optimization', '#transfer_learning', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LoRA Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸: Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° LoRA Ğ² Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ A Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‚ Ğ·Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ B - Ğ·Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ FedSA-LoRA, Ğ³Ğ´Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ A Ğ¿ĞµÑ€ĞµĞ´Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑĞµÑ€Ğ²ĞµÑ€ Ğ´Ğ»Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ¸ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ… LoRA, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ FedSA-rsLoRA Ğ¸ FedSA-VeRA. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing Federated Learning with Low-Rank Adaptation', 'desc': 'This paper explores the use of Low-Rank Adaptation (LoRA) in federated learning by analyzing the asymmetry of the learned matrices A and B. It finds that matrix A captures general knowledge applicable across clients, while matrix B focuses on client-specific information. The authors propose a new method called Federated Share-A Low-Rank Adaptation (FedSA-LoRA), which shares only the A matrices with the server for aggregation, enhancing privacy and efficiency. They also extend this approach to other LoRA variants, establishing a comprehensive framework for integrating LoRA with federated learning, supported by strong experimental results in natural language tasks.'}, 'zh': {'title': 'è”é‚¦å­¦ä¹ ä¸­çš„ä½ç§©é€‚åº”æ–°èŒƒå¼', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†åœ¨è”é‚¦å­¦ä¹ ä¸­ä½¿ç”¨LoRAçš„æ–¹å¼ï¼Œåˆ†æäº†å­¦ä¹ åˆ°çš„Aå’ŒBçŸ©é˜µçš„ä¸å¯¹ç§°æ€§ã€‚ç ”ç©¶å‘ç°ï¼ŒAçŸ©é˜µè´Ÿè´£å­¦ä¹ é€šç”¨çŸ¥è¯†ï¼Œè€ŒBçŸ©é˜µåˆ™ä¸“æ³¨äºæ•æ‰å®¢æˆ·ç«¯ç‰¹å®šçš„çŸ¥è¯†ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæå‡ºäº†è”é‚¦å…±äº«ä½ç§©é€‚åº”ï¼ˆFedSA-LoRAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ä¸¤ä¸ªä½ç§©å¯è®­ç»ƒçŸ©é˜µAå’ŒBæ¥å»ºæ¨¡æƒé‡æ›´æ–°ï¼Œä½†ä»…å…±äº«AçŸ©é˜µä¸æœåŠ¡å™¨è¿›è¡Œèšåˆã€‚é€šè¿‡å¯¹å…¶ä»–LoRAå˜ä½“ï¼ˆå¦‚rsLoRAå’ŒVeRAï¼‰ä¸­å­¦ä¹ åˆ°çš„Aå’ŒBçŸ©é˜µçš„å…³ç³»è¿›è¡Œæ·±å…¥æ¢è®¨ï¼Œå»ºç«‹äº†å°†LoRAä¸è”é‚¦å­¦ä¹ ç»“åˆçš„ä¸€èˆ¬èŒƒå¼ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01257', 'title': 'HelpSteer2-Preference: Complementing Ratings with Preferences', 'url': 'https://huggingface.co/papers/2410.01257', 'abstract': 'Reward models are critical for aligning models to follow instructions, and are typically trained following one of two popular paradigms: Bradley-Terry style or Regression style. However, there is a lack of evidence that either approach is better than the other, when adequately matched for data. This is primarily because these approaches require data collected in different (but incompatible) formats, meaning that adequately matched data is not available in existing public datasets. To tackle this problem, we release preference annotations (designed for Bradley-Terry training) to complement existing ratings (designed for Regression style training) in the HelpSteer2 dataset. To improve data interpretability, preference annotations are accompanied with human-written justifications. Using this data, we conduct the first head-to-head comparison of Bradley-Terry and Regression models when adequately matched for data. Based on insights derived from such a comparison, we propose a novel approach to combine Bradley-Terry and Regression reward modeling. A Llama-3.1-70B-Instruct model tuned with this approach scores 94.1 on RewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. We also demonstrate the effectiveness of this reward model at aligning models to follow instructions in RLHF. We open-source this dataset (CC-BY-4.0 license) at https://huggingface.co/datasets/nvidia/HelpSteer2 and openly release the trained Reward Model at https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward', 'score': 18, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'b46023070ee2b9c9', 'data': {'categories': ['#dataset', '#training', '#interpretability', '#alignment', '#benchmark', '#open_source', '#rlhf'], 'emoji': 'ğŸ†', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ: ÑÑ‚Ğ¸Ğ»ÑŒ Ğ‘Ñ€ÑĞ´Ğ»Ğ¸-Ğ¢ĞµÑ€Ñ€Ğ¸ Ğ¸ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ¸Ğ»ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… HelpSteer2, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ğ° ÑÑ‚Ğ¸Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Llama-3.1-70B-Instruct, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ½Ğ°Ğ¸Ğ²Ñ‹ÑÑˆĞµĞ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ² 94.1 Ğ±Ğ°Ğ»Ğ»Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ RewardBench.'}, 'en': {'title': 'Bridging Reward Models: A New Approach for Better Alignment', 'desc': 'This paper discusses the importance of reward models in aligning machine learning models to follow instructions. It compares two popular training paradigms for these models: Bradley-Terry and Regression styles, highlighting the lack of compatible data for a fair comparison. To address this, the authors introduce preference annotations to the HelpSteer2 dataset, which allows for a direct comparison of the two approaches. They also propose a new method that combines both paradigms, resulting in a highly effective reward model that outperforms others in the field.'}, 'zh': {'title': 'å¥–åŠ±æ¨¡å‹çš„åˆ›æ–°å¯¹æ¯”ä¸ç»“åˆ', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¥–åŠ±æ¨¡å‹åœ¨æŒ‡ä»¤å¯¹é½ä¸­çš„é‡è¦æ€§ï¼Œä¸»è¦æ¯”è¾ƒäº†Bradley-Terryé£æ ¼å’Œå›å½’é£æ ¼çš„è®­ç»ƒæ–¹æ³•ã€‚ç”±äºè¿™ä¸¤ç§æ–¹æ³•éœ€è¦ä¸åŒæ ¼å¼çš„æ•°æ®ï¼Œå¯¼è‡´ç°æœ‰å…¬å…±æ•°æ®é›†ä¸­ç¼ºä¹é€‚å½“åŒ¹é…çš„æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨HelpSteer2æ•°æ®é›†ä¸­å‘å¸ƒäº†ç”¨äºBradley-Terryè®­ç»ƒçš„åå¥½æ³¨é‡Šï¼Œå¹¶é™„ä¸Šäº†äººç±»æ’°å†™çš„ç†ç”±ï¼Œä»¥æé«˜æ•°æ®çš„å¯è§£é‡Šæ€§ã€‚é€šè¿‡å¯¹æ¯”å®éªŒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†Bradley-Terryå’Œå›å½’å¥–åŠ±å»ºæ¨¡ç›¸ç»“åˆï¼Œæœ€ç»ˆåœ¨RewardBenchä¸Šå–å¾—äº†94.1çš„é«˜åˆ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2409.20059', 'title': 'Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis', 'url': 'https://huggingface.co/papers/2409.20059', 'abstract': 'Neural metrics for machine translation (MT) evaluation have become increasingly prominent due to their superior correlation with human judgments compared to traditional lexical metrics. Researchers have therefore utilized neural metrics through quality-informed decoding strategies, achieving better results than likelihood-based methods. With the rise of Large Language Models (LLMs), preference-based alignment techniques have gained attention for their potential to enhance translation quality by optimizing model weights directly on preferences induced by quality estimators. This study focuses on Contrastive Preference Optimization (CPO) and conducts extensive experiments to evaluate the impact of preference-based alignment on translation quality. Our findings indicate that while CPO consistently outperforms Supervised Fine-Tuning (SFT) on high-quality data with regard to the alignment metric, it may lead to instability across downstream evaluation metrics, particularly between neural and lexical ones. Additionally, we demonstrate that relying solely on the base model for generating candidate translations achieves performance comparable to using multiple external systems, while ensuring better consistency across downstream metrics.', 'score': 15, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '700774cfa4699b68', 'data': {'categories': ['#multilingual', '#training', '#machine_translation', '#optimization', '#alignment', '#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Contrastive Preference Optimization (CPO) Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ ĞµĞ³Ğ¾ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğº Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ´Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Enhancing Translation Quality with Contrastive Preference Optimization', 'desc': 'This paper discusses the use of neural metrics for evaluating machine translation (MT), which are more aligned with human judgment than traditional methods. It highlights the effectiveness of quality-informed decoding strategies that leverage these neural metrics, particularly in the context of Large Language Models (LLMs). The study introduces Contrastive Preference Optimization (CPO) as a technique to improve translation quality by directly optimizing model weights based on preferences from quality estimators. The results show that while CPO outperforms Supervised Fine-Tuning (SFT) on high-quality data, it can cause instability in evaluation metrics, and using the base model for generating translations can yield results similar to those from multiple external systems.'}, 'zh': {'title': 'æå‡æœºå™¨ç¿»è¯‘è´¨é‡çš„å¯¹æ¯”åå¥½ä¼˜åŒ–', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†ç¥ç»åº¦é‡åœ¨æœºå™¨ç¿»è¯‘è¯„ä¼°ä¸­çš„åº”ç”¨ï¼Œæ˜¾ç¤ºå…¶ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§ä¼˜äºä¼ ç»Ÿçš„è¯æ±‡åº¦é‡ã€‚ç ”ç©¶è€…ä»¬é€šè¿‡è´¨é‡ä¿¡æ¯è§£ç ç­–ç•¥åˆ©ç”¨ç¥ç»åº¦é‡ï¼Œå–å¾—äº†æ¯”åŸºäºä¼¼ç„¶çš„æ–¹æ³•æ›´å¥½çš„ç»“æœã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…´èµ·ï¼ŒåŸºäºåå¥½çš„å¯¹é½æŠ€æœ¯å—åˆ°å…³æ³¨ï¼Œèƒ½å¤Ÿé€šè¿‡ä¼˜åŒ–æ¨¡å‹æƒé‡æ¥æå‡ç¿»è¯‘è´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡å¯¹æ¯”åå¥½ä¼˜åŒ–ï¼ˆCPOï¼‰åœ¨é«˜è´¨é‡æ•°æ®ä¸Šä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½†åœ¨ä¸‹æ¸¸è¯„ä¼°æŒ‡æ ‡ä¸Šå¯èƒ½å¯¼è‡´ä¸ç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01731', 'title': 'ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2410.01731', 'abstract': 'The practical use of text-to-image generation has evolved from simple, monolithic models to complex workflows that combine multiple specialized components. While workflow-based approaches can lead to improved image quality, crafting effective workflows requires significant expertise, owing to the large number of available components, their complex inter-dependence, and their dependence on the generation prompt. Here, we introduce the novel task of prompt-adaptive workflow generation, where the goal is to automatically tailor a workflow to each user prompt. We propose two LLM-based approaches to tackle this task: a tuning-based method that learns from user-preference data, and a training-free method that uses the LLM to select existing flows. Both approaches lead to improved image quality when compared to monolithic models or generic, prompt-independent workflows. Our work shows that prompt-dependent flow prediction offers a new pathway to improving text-to-image generation quality, complementing existing research directions in the field.', 'score': 15, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'e1174d1b695405ab', 'data': {'categories': ['#cv', '#training', '#alignment', '#diffusion', '#architecture'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚, Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². ĞĞ±Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ.'}, 'en': {'title': 'Tailoring Workflows for Enhanced Image Generation', 'desc': 'This paper discusses advancements in text-to-image generation, moving from simple models to more complex workflows that utilize specialized components. It introduces the task of prompt-adaptive workflow generation, which aims to automatically customize workflows based on user prompts. The authors propose two methods using large language models (LLMs): one that learns from user preferences and another that selects existing workflows without additional training. Both methods enhance image quality compared to traditional models, highlighting the importance of adapting workflows to specific prompts for better results.'}, 'zh': {'title': 'åŸºäºæç¤ºçš„å·¥ä½œæµç”Ÿæˆæå‡å›¾åƒè´¨é‡', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ä»»åŠ¡â€”â€”åŸºäºæç¤ºçš„å·¥ä½œæµç”Ÿæˆï¼Œæ—¨åœ¨è‡ªåŠ¨æ ¹æ®ç”¨æˆ·æç¤ºå®šåˆ¶å·¥ä½œæµã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼šä¸€ç§æ˜¯åŸºäºè°ƒä¼˜çš„æ–¹æ³•ï¼Œé€šè¿‡ç”¨æˆ·åå¥½æ•°æ®è¿›è¡Œå­¦ä¹ ï¼›å¦ä¸€ç§æ˜¯æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨LLMé€‰æ‹©ç°æœ‰çš„å·¥ä½œæµã€‚è¿™ä¸¤ç§æ–¹æ³•åœ¨å›¾åƒè´¨é‡ä¸Šä¼˜äºå•ä¸€æ¨¡å‹æˆ–é€šç”¨çš„ã€ä¸æç¤ºæ— å…³çš„å·¥ä½œæµã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºæç¤ºçš„å·¥ä½œæµé¢„æµ‹ä¸ºæé«˜æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è´¨é‡æä¾›äº†ä¸€æ¡æ–°è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01036', 'title': 'MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages', 'url': 'https://huggingface.co/papers/2410.01036', 'abstract': 'The rise of foundation models (FMs), coupled with regulatory efforts addressing their risks and impacts, has sparked significant interest in open-source models. However, existing speech FMs (SFMs) fall short of full compliance with the open-source principles, even if claimed otherwise, as no existing SFM has model weights, code, and training data publicly available under open-source terms. In this work, we take the first step toward filling this gap by focusing on the 24 official languages of the European Union (EU). We collect suitable training data by surveying automatic speech recognition datasets and unlabeled speech corpora under open-source compliant licenses, for a total of 950k hours. Additionally, we release automatic transcripts for 441k hours of unlabeled data under the permissive CC-BY license, thereby facilitating the creation of open-source SFMs for the EU languages.', 'score': 14, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '9714f6cb6169fec1', 'data': {'categories': ['#audio', '#dataset', '#multilingual', '#data', '#open_source', '#low_resource'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ•Ğ¡: Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ ÑĞ´ĞµĞ»Ğ°Ğ½', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SFM) Ğ´Ğ»Ñ 24 Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ•Ğ²Ñ€Ğ¾Ğ¿ĞµĞ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¡Ğ¾ÑĞ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ 950 Ñ‚Ñ‹ÑÑÑ‡ Ñ‡Ğ°ÑĞ¾Ğ² Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ 441 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ñ‡Ğ°ÑĞ¾Ğ² Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹ CC-BY. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… SFM, Ñ‚Ğ°Ğº ĞºĞ°Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ²ÑĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Building Open-Source Speech Models for EU Languages', 'desc': 'This paper addresses the limitations of existing speech foundation models (SFMs) in terms of open-source compliance. It highlights that no current SFM provides model weights, code, and training data that are fully accessible under open-source terms. The authors collect a substantial dataset of 950,000 hours of training data from various automatic speech recognition datasets and unlabeled speech corpora that comply with open-source licenses. They also release automatic transcripts for 441,000 hours of unlabeled data, promoting the development of open-source SFMs for the 24 official languages of the European Union.'}, 'zh': {'title': 'æ¨åŠ¨æ¬§ç›Ÿè¯­è¨€çš„å¼€æºè¯­éŸ³æ¨¡å‹å‘å±•', 'desc': 'æœ¬è®ºæ–‡å…³æ³¨åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨å¼€æºæ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰ã€‚ç›®å‰ï¼Œç°æœ‰çš„è¯­éŸ³åŸºç¡€æ¨¡å‹æœªèƒ½å®Œå…¨éµå¾ªå¼€æºåŸåˆ™ï¼Œå› ä¸ºæ²¡æœ‰å…¬å¼€çš„æ¨¡å‹æƒé‡ã€ä»£ç å’Œè®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬é’ˆå¯¹æ¬§ç›Ÿçš„24ç§å®˜æ–¹è¯­è¨€ï¼Œæ”¶é›†äº†ç¬¦åˆå¼€æºè®¸å¯çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ•°æ®é›†å’Œæœªæ ‡è®°è¯­éŸ³è¯­æ–™ï¼Œæ€»è®¡è¾¾åˆ°950kå°æ—¶ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†441kå°æ—¶æœªæ ‡è®°æ•°æ®çš„è‡ªåŠ¨è½¬å½•ï¼Œä¿ƒè¿›äº†æ¬§ç›Ÿè¯­è¨€çš„å¼€æºè¯­éŸ³åŸºç¡€æ¨¡å‹çš„åˆ›å»ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01769', 'title': 'Quantifying Generalization Complexity for Large Language Models', 'url': 'https://huggingface.co/papers/2410.01769', 'abstract': "While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.", 'score': 13, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'fc737f630759a34b', 'data': {'categories': ['#reasoning', '#training', '#interpretability', '#benchmark', '#open_source', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Scylla: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Scylla - Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Scylla Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ½Ğµ ĞµĞ³Ğ¾ Ñ‡ĞµÑ€ĞµĞ· 20 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ¾Ğ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ 'Ğ´Ğ¾Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ'. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¼ĞµÑ‰Ğ°ĞµÑ‚ÑÑ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'Scylla: Unraveling Generalization in Large Language Models', 'desc': "This paper presents Scylla, a new evaluation framework designed to measure the generalization abilities of large language models (LLMs) while separating it from memorization. Scylla evaluates model performance on both in-distribution (ID) and out-of-distribution (OOD) data across various tasks of increasing complexity. The study identifies a phenomenon called the 'generalization valley,' which highlights a critical complexity threshold where models tend to rely more on memorization rather than generalization. Additionally, it shows that as LLMs grow in size, they can tackle more complex tasks before this reliance on memorization becomes problematic."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºScyllaçš„åŠ¨æ€è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå®šé‡æµ‹é‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ³›åŒ–èƒ½åŠ›ã€‚Scyllaé€šè¿‡è¯„ä¼°æ¨¡å‹åœ¨åˆ†å¸ƒå†…ï¼ˆIDï¼‰å’Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®ä¸Šçš„è¡¨ç°ï¼Œæ¥åŒºåˆ†æ³›åŒ–ä¸è®°å¿†åŒ–ã€‚ç ”ç©¶å‘ç°ä»»åŠ¡å¤æ‚æ€§ä¸IDå’ŒOODæ•°æ®ä¹‹é—´çš„æ€§èƒ½å·®è·å‘ˆç°éå•è°ƒå…³ç³»ï¼Œç§°ä¸ºæ³›åŒ–è°·ã€‚éšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œå…³é”®å¤æ‚æ€§å‘æ›´é«˜çš„ä»»åŠ¡å¤æ‚æ€§ç§»åŠ¨ï¼Œè¡¨æ˜æ›´å¤§çš„æ¨¡å‹åœ¨è¿‡åº¦ä¾èµ–è®°å¿†åŒ–ä¹‹å‰èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01691', 'title': 'FactAlign: Long-form Factuality Alignment of Large Language Models', 'url': 'https://huggingface.co/papers/2410.01691', 'abstract': "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FactAlign, a novel alignment framework designed to enhance the factuality of LLMs' long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FactAlign utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FactAlign significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FactAlign is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score. Our source code, datasets, and trained models are publicly available at https://github.com/MiuLab/FactAlign", 'score': 8, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '3b5151bf24c4fc15', 'data': {'categories': ['#dataset', '#hallucinations', '#long_context', '#training', '#alignment', '#open_source', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'FactAlign: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FactAlign - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ fKTO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FactAlign Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ñ… Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Factual Accuracy in Language Models with FactAlign', 'desc': 'This paper introduces FactAlign, a new framework aimed at improving the factual accuracy of long-form responses generated by large language models (LLMs). The authors highlight the problem of hallucination, where LLMs produce incorrect or misleading information, especially in extended outputs. FactAlign employs a fine-grained alignment algorithm called fKTO, which enhances the alignment process by using detailed factuality evaluations. Experimental results show that FactAlign not only boosts the factual accuracy of LLM responses but also maintains their overall helpfulness, leading to better performance in information retrieval tasks.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§ä¸æœ‰ç”¨æ€§', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯¹é½æ¡†æ¶FactAlignï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿æ–‡æœ¬å“åº”ä¸­çš„äº‹å®å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»†ç²’åº¦çš„å¥å­çº§å¯¹é½ç®—æ³•fKTOï¼Œæ‰©å±•äº†Kahneman-Tverskyä¼˜åŒ–æ–¹æ³•ã€‚FactAlignåˆ©ç”¨è‡ªåŠ¨äº‹å®è¯„ä¼°çš„æœ€æ–°è¿›å±•ï¼Œé€šè¿‡ç»†ç²’åº¦çš„äº‹å®è¯„ä¼°æ¥æŒ‡å¯¼å¯¹é½è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFactAlignæ˜¾è‘—æé«˜äº†LLMå“åº”çš„äº‹å®å‡†ç¡®æ€§å’Œæœ‰ç”¨æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ä¿¡æ¯çš„ä¸°å¯Œæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.02197', 'title': 'General Preference Modeling with Preference Representations for Aligning Language Models', 'url': 'https://huggingface.co/papers/2410.02197', 'abstract': 'Modeling human preferences is crucial for aligning foundation models with human values. Traditional reward modeling methods, such as the Bradley-Terry (BT) reward model, fall short in expressiveness, particularly in addressing intransitive preferences. Although supervised pair preference models (PairPM) can express general preferences, their implementation is highly ad-hoc and cannot guarantee a consistent preference probability of compared pairs. Additionally, they impose high computational costs due to their quadratic query complexity when comparing multiple responses. In this paper, we introduce preference representation learning, an approach that embeds responses into a latent space to capture intricate preference structures efficiently, achieving linear query complexity. Additionally, we propose preference score-based General Preference Optimization (GPO), which generalizes reward-based reinforcement learning from human feedback. Experimental results show that our General Preference representation model (GPM) outperforms the BT reward model on the RewardBench benchmark with a margin of up to 5.6% and effectively models cyclic preferences where any BT reward model behaves like a random guess. Furthermore, evaluations on downstream tasks such as AlpacaEval2.0 and MT-Bench, following the language model post-training with GPO and our general preference model, reveal substantial performance improvements with margins up to 9.3%. These findings indicate that our method may enhance the alignment of foundation models with nuanced human values. The code is available at https://github.com/general-preference/general-preference-model.', 'score': 7, 'issue_id': 1, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}, 'hash': 'eb2fb462dcfba826', 'data': {'categories': ['#training', '#optimization', '#alignment', '#benchmark', '#open_source', '#rlhf', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ alignment Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ alignment ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ preference representation learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ General Preference Optimization (GPO), Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ General Preference Model (GPM) Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼.'}, 'en': {'title': 'Enhancing Model Alignment with Human Preferences through Efficient Learning', 'desc': 'This paper addresses the challenge of aligning foundation models with human preferences by introducing preference representation learning. Unlike traditional reward modeling methods, which struggle with intransitive preferences and high computational costs, this new approach efficiently captures complex preference structures in a latent space. The authors also present General Preference Optimization (GPO), which extends reinforcement learning from human feedback to improve preference modeling. Experimental results demonstrate that their General Preference representation model (GPM) significantly outperforms existing methods, particularly in handling cyclic preferences and enhancing model performance on various tasks.'}, 'zh': {'title': 'æå‡æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åå¥½è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æ›´å¥½åœ°æ•æ‰äººç±»çš„å¤æ‚åå¥½ç»“æ„ã€‚ä¸ä¼ ç»Ÿçš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†å“åº”åµŒå…¥åˆ°æ½œåœ¨ç©ºé—´ä¸­ï¼Œæ˜¾è‘—æé«˜äº†è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºåå¥½åˆ†æ•°çš„é€šç”¨åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»äººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é€šç”¨åå¥½æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¯¹é½åŸºç¡€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01171', 'title': 'BordIRlines: A Dataset for Evaluating Cross-lingual Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2410.01171', 'abstract': "Large language models excel at creative generation but continue to struggle with the issues of hallucination and bias. While retrieval-augmented generation (RAG) provides a framework for grounding LLMs' responses in accurate and up-to-date information, it still raises the question of bias: which sources should be selected for inclusion in the context? And how should their importance be weighted? In this paper, we study the challenge of cross-lingual RAG and present a dataset to investigate the robustness of existing systems at answering queries about geopolitical disputes, which exist at the intersection of linguistic, cultural, and political boundaries. Our dataset is sourced from Wikipedia pages containing information relevant to the given queries and we investigate the impact of including additional context, as well as the composition of this context in terms of language and source, on an LLM's response. Our results show that existing RAG systems continue to be challenged by cross-lingual use cases and suffer from a lack of consistency when they are provided with competing information in multiple languages. We present case studies to illustrate these issues and outline steps for future research to address these challenges. We make our dataset and code publicly available at https://github.com/manestay/bordIRlines.", 'score': 5, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '6a353ff492b330af', 'data': {'categories': ['#dataset', '#hallucinations', '#multilingual', '#rag', '#ethics', '#transfer_learning', '#open_source'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² retrieval-augmented generation', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ retrieval-augmented generation (RAG) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ³ĞµĞ¾Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸ĞµÑÑ Ğ½Ğ° Ğ¿ĞµÑ€ĞµÑĞµÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ…, ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾-Ğ¿Ñ€ĞµĞ¶Ğ½ĞµĞ¼Ñƒ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼.'}, 'en': {'title': 'Enhancing LLMs: Tackling Bias and Hallucination in Cross-Lingual Contexts', 'desc': 'This paper addresses the limitations of large language models (LLMs) in generating accurate responses, particularly focusing on the issues of hallucination and bias. It explores retrieval-augmented generation (RAG) as a method to enhance LLMs by grounding their outputs in reliable information, while also questioning how to select and weigh sources effectively. The authors introduce a new dataset aimed at evaluating RAG systems in the context of geopolitical disputes, highlighting the challenges posed by cross-lingual queries. Their findings reveal that current RAG systems struggle with consistency and accuracy when faced with conflicting information across different languages, suggesting a need for further research in this area.'}, 'zh': {'title': 'è·¨è¯­è¨€æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æŒ‘æˆ˜ä¸æœºé‡', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆåˆ›æ„å†…å®¹æ—¶é¢ä¸´çš„å¹»è§‰å’Œåè§é—®é¢˜ã€‚å°½ç®¡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å“åº”æä¾›äº†å‡†ç¡®å’Œæœ€æ–°ä¿¡æ¯çš„æ¡†æ¶ï¼Œä½†åœ¨é€‰æ‹©ä¿¡æ¯æ¥æºæ—¶ä»ç„¶å­˜åœ¨åè§é—®é¢˜ã€‚æˆ‘ä»¬ç ”ç©¶äº†è·¨è¯­è¨€RAGçš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œä»¥è°ƒæŸ¥ç°æœ‰ç³»ç»Ÿåœ¨å›ç­”åœ°ç¼˜æ”¿æ²»äº‰ç«¯æŸ¥è¯¢æ—¶çš„é²æ£’æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„RAGç³»ç»Ÿåœ¨å¤„ç†å¤šè¯­è¨€ç«äº‰ä¿¡æ¯æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œç¼ºä¹ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2409.18111', 'title': 'E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding', 'url': 'https://huggingface.co/papers/2409.18111', 'abstract': 'Recent advances in Video Large Language Models (Video-LLMs) have demonstrated their great potential in general-purpose video understanding. To verify the significance of these models, a number of benchmarks have been proposed to diagnose their capabilities in different scenarios. However, existing benchmarks merely evaluate models through video-level question-answering, lacking fine-grained event-level assessment and task diversity. To fill this gap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding Benchmark), a large-scale and high-quality benchmark for open-ended event-level video understanding. Categorized within a 3-level task taxonomy, E.T. Bench encompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length) under 8 domains, providing comprehensive evaluations. We extensively evaluated 8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that state-of-the-art models for coarse-level (video-level) understanding struggle to solve our fine-grained tasks, e.g., grounding event-of-interests within videos, largely due to the short video context length, improper time representations, and lack of multi-event training data. Focusing on these issues, we further propose a strong baseline model, E.T. Chat, together with an instruction-tuning dataset E.T. Instruct 164K tailored for fine-grained event-level understanding. Our simple but effective solution demonstrates superior performance in multiple scenarios.', 'score': 5, 'issue_id': 1, 'pub_date': '2024-09-26', 'pub_date_card': {'ru': '26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 26', 'zh': '9æœˆ26æ—¥'}, 'hash': '5135f0df381cd4c0', 'data': {'categories': ['#video', '#long_context', '#training', '#interpretability', '#benchmark', '#games', '#synthetic'], 'emoji': 'ğŸ¥', 'ru': {'title': 'E.T. Bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº E.T. Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Video-LLM Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 7.3 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¿Ğ¾ 12 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 7 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· 8 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 8 Image-LLM Ğ¸ 12 Video-LLM Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ E.T. Chat Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… E.T. Instruct 164K Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸.'}, 'en': {'title': 'E.T. Bench: Elevating Video Understanding to Event-Level Precision', 'desc': 'This paper discusses the development of E.T. Bench, a new benchmark designed for evaluating Video Large Language Models (Video-LLMs) in understanding events within videos. Unlike previous benchmarks that only assess video-level question-answering, E.T. Bench focuses on fine-grained event-level tasks across various scenarios. The benchmark includes a diverse set of tasks and a large dataset, allowing for comprehensive evaluation of model capabilities. Additionally, the authors introduce E.T. Chat, a baseline model that improves performance on these fine-grained tasks by addressing issues like short video context and inadequate training data.'}, 'zh': {'title': 'æå‡è§†é¢‘ç†è§£çš„ç»†ç²’åº¦è¯„ä¼°', 'desc': 'æœ€è¿‘ï¼Œè§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å±•ç°äº†å·¨å¤§çš„æ½œåŠ›ã€‚ä¸ºäº†éªŒè¯è¿™äº›æ¨¡å‹çš„èƒ½åŠ›ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼Œä½†ç°æœ‰çš„åŸºå‡†ä»…é€šè¿‡è§†é¢‘çº§é—®ç­”è¿›è¡Œè¯„ä¼°ï¼Œç¼ºä¹å¯¹äº‹ä»¶çº§çš„ç»†è‡´è¯„ä¼°å’Œä»»åŠ¡å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†E.T. Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å¼€æ”¾å¼äº‹ä»¶çº§è§†é¢‘ç†è§£åŸºå‡†ï¼Œæ¶µç›–äº†12ä¸ªä»»åŠ¡å’Œ7.3Kæ ·æœ¬ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†E.T. Chatæ¨¡å‹å’Œé’ˆå¯¹ç»†ç²’åº¦äº‹ä»¶ç†è§£çš„æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†E.T. Instruct 164Kï¼Œå±•ç¤ºäº†åœ¨å¤šä¸ªåœºæ™¯ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01804', 'title': 'EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis', 'url': 'https://huggingface.co/papers/2410.01804', 'abstract': 'We present Exact Volumetric Ellipsoid Rendering (EVER), a method for real-time differentiable emission-only volume rendering. Unlike recent rasterization based approach by 3D Gaussian Splatting (3DGS), our primitive based representation allows for exact volume rendering, rather than alpha compositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does not suffer from popping artifacts and view dependent density, but still achieves frame rates of sim!30 FPS at 720p on an NVIDIA RTX4090. Since our approach is built upon ray tracing it enables effects such as defocus blur and camera distortion (e.g. such as from fisheye cameras), which are difficult to achieve by rasterization. We show that our method is more accurate with fewer blending issues than 3DGS and follow-up work on view-consistent rendering, especially on the challenging large-scale scenes from the Zip-NeRF dataset where it achieves sharpest results among real-time techniques.', 'score': 5, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '268be8bd1eb6316f', 'data': {'categories': ['#dataset', '#cv', '#3d'], 'emoji': 'ğŸ”®', 'ru': {'title': 'EVER: Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'EVER (Exact Volumetric Ellipsoid Rendering) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ 3D Gaussian Splatting, EVER Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ² Ğ±ĞµĞ· Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 30 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ 720p Ğ½Ğ° NVIDIA RTX4090. EVER Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ğ¸Ğ· Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Zip-NeRF.'}, 'en': {'title': 'Achieving Real-Time Precision in Volume Rendering with EVER', 'desc': 'The paper introduces Exact Volumetric Ellipsoid Rendering (EVER), a novel technique for real-time volume rendering that focuses on emission-only scenarios. Unlike the 3D Gaussian Splatting (3DGS) method, which uses alpha compositing, EVER employs a primitive-based representation that ensures precise volume rendering without artifacts. This method achieves high frame rates of around 30 FPS at 720p resolution on advanced hardware, while also supporting complex effects like defocus blur and camera distortion. The results demonstrate that EVER outperforms 3DGS in accuracy and blending quality, particularly in large-scale scenes from the Zip-NeRF dataset.'}, 'zh': {'title': 'å®æ—¶ç²¾ç¡®ä½“ç§¯æ¸²æŸ“çš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºç²¾ç¡®ä½“ç§¯æ¤­çƒæ¸²æŸ“ï¼ˆEVERï¼‰çš„æ–¹æ³•ï¼Œç”¨äºå®æ—¶å¯å¾®åˆ†çš„ä»…å‘å°„ä½“ç§¯æ¸²æŸ“ã€‚ä¸åŸºäºå…‰æ …åŒ–çš„3Dé«˜æ–¯ç‚¹äº‘æ–¹æ³•ï¼ˆ3DGSï¼‰ä¸åŒï¼Œæˆ‘ä»¬çš„åŸå§‹è¡¨ç¤ºå…è®¸è¿›è¡Œç²¾ç¡®çš„ä½“ç§¯æ¸²æŸ“ï¼Œè€Œä¸æ˜¯å¯¹3Dé«˜æ–¯å¹¿å‘Šç‰Œè¿›è¡Œé€æ˜åˆæˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ²¡æœ‰å‡ºç°3DGSä¸­çš„å¼¹è·³ä¼ªå½±å’Œè§†è§’ä¾èµ–å¯†åº¦é—®é¢˜ï¼ŒåŒæ—¶åœ¨NVIDIA RTX4090ä¸Šä»èƒ½ä»¥720pçš„åˆ†è¾¨ç‡è¾¾åˆ°æ¯ç§’30å¸§çš„å¸§ç‡ã€‚ç”±äºæˆ‘ä»¬çš„æ–¹æ³•åŸºäºå…‰çº¿è¿½è¸ªï¼Œå®ƒèƒ½å¤Ÿå®ç°æ¨¡ç³Šå’Œç›¸æœºç•¸å˜ç­‰æ•ˆæœï¼Œè¿™äº›æ•ˆæœåœ¨å…‰æ …åŒ–ä¸­éš¾ä»¥å®ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.00296', 'title': 'VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data', 'url': 'https://huggingface.co/papers/2410.00296', 'abstract': 'Vision-language models (VLMs) are essential for contextual understanding of both visual and textual information. However, their vulnerability to adversarially manipulated inputs presents significant risks, leading to compromised outputs and raising concerns about the reliability in VLM-integrated applications. Detecting these malicious prompts is thus crucial for maintaining trust in VLM generations. A major challenge in developing a safeguarding prompt classifier is the lack of a large amount of labeled benign and malicious data. To address the issue, we introduce VLMGuard, a novel learning framework that leverages the unlabeled user prompts in the wild for malicious prompt detection. These unlabeled prompts, which naturally arise when VLMs are deployed in the open world, consist of both benign and malicious information. To harness the unlabeled data, we present an automated maliciousness estimation score for distinguishing between benign and malicious samples within this unlabeled mixture, thereby enabling the training of a binary prompt classifier on top. Notably, our framework does not require extra human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiment shows VLMGuard achieves superior detection results, significantly outperforming state-of-the-art methods. Disclaimer: This paper may contain offensive examples; reader discretion is advised.', 'score': 4, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'eb03a7f1b6890abe', 'data': {'categories': ['#cv', '#security', '#training', '#data', '#synthetic', '#multimodal'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'VLMGuard: Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ VLMGuard - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° - Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VLMGuard Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ VLM.'}, 'en': {'title': 'VLMGuard: Safeguarding Vision-Language Models from Malicious Prompts', 'desc': 'This paper introduces VLMGuard, a framework designed to detect malicious prompts in vision-language models (VLMs) without needing extensive labeled data. It utilizes unlabeled user prompts, which are common in real-world applications, to differentiate between benign and malicious inputs. By implementing an automated maliciousness estimation score, VLMGuard can effectively train a binary classifier to enhance the reliability of VLM outputs. The results demonstrate that VLMGuard significantly outperforms existing methods in detecting adversarial prompts, ensuring safer use of VLMs in various applications.'}, 'zh': {'title': 'VLMGuardï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„ä¸Šä¸‹æ–‡ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹æ¶æ„è¾“å…¥çš„è„†å¼±æ€§å¸¦æ¥äº†é‡å¤§é£é™©ï¼Œå½±å“äº†è¾“å‡ºçš„å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VLMGuardï¼Œä¸€ä¸ªæ–°é¢–çš„å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨æœªæ ‡è®°çš„ç”¨æˆ·æç¤ºæ¥æ£€æµ‹æ¶æ„æç¤ºã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡è‡ªåŠ¨åŒ–çš„æ¶æ„æ€§ä¼°è®¡è¯„åˆ†ï¼Œèƒ½å¤Ÿåœ¨æœªæ ‡è®°çš„æ•°æ®ä¸­åŒºåˆ†è‰¯æ€§å’Œæ¶æ„æ ·æœ¬ï¼Œä»è€Œè®­ç»ƒå‡ºä¸€ä¸ªäºŒå…ƒæç¤ºåˆ†ç±»å™¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.00316', 'title': 'EmoKnob: Enhance Voice Cloning with Fine-Grained Emotion Control', 'url': 'https://huggingface.co/papers/2410.00316', 'abstract': 'While recent advances in Text-to-Speech (TTS) technology produce natural and expressive speech, they lack the option for users to select emotion and control intensity. We propose EmoKnob, a framework that allows fine-grained emotion control in speech synthesis with few-shot demonstrative samples of arbitrary emotion. Our framework leverages the expressive speaker representation space made possible by recent advances in foundation voice cloning models. Based on the few-shot capability of our emotion control framework, we propose two methods to apply emotion control on emotions described by open-ended text, enabling an intuitive interface for controlling a diverse array of nuanced emotions. To facilitate a more systematic emotional speech synthesis field, we introduce a set of evaluation metrics designed to rigorously assess the faithfulness and recognizability of emotion control frameworks. Through objective and subjective evaluations, we show that our emotion control framework effectively embeds emotions into speech and surpasses emotion expressiveness of commercial TTS services.', 'score': 4, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '3b1f67773dd59956', 'data': {'categories': ['#audio', '#dataset', '#benchmark', '#open_source', '#synthetic'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ EmoKnob - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾ÑĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²ĞµĞ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ·Ğ½Ğ°Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ€ĞµÑ‡Ğ¸.'}, 'en': {'title': 'EmoKnob: Fine-Grained Emotion Control in Speech Synthesis', 'desc': 'This paper introduces EmoKnob, a novel framework for enhancing Text-to-Speech (TTS) systems by allowing users to control emotions and their intensity in synthesized speech. It utilizes few-shot learning techniques to enable emotion control based on limited examples, leveraging advanced voice cloning models for expressive speaker representation. The framework includes methods for interpreting open-ended text descriptions of emotions, providing a user-friendly interface for nuanced emotional expression. Additionally, the authors propose new evaluation metrics to assess the effectiveness of emotion control in TTS, demonstrating that EmoKnob outperforms existing commercial TTS services in emotional expressiveness.'}, 'zh': {'title': 'æƒ…æ„Ÿæ§åˆ¶ï¼Œè¯­éŸ³åˆæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºEmoKnobçš„æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯ä¸­çš„æƒ…æ„Ÿæ§åˆ¶ã€‚è¯¥æ¡†æ¶å…è®¸ç”¨æˆ·é€šè¿‡å°‘é‡ç¤ºä¾‹æ ·æœ¬æ¥ç²¾ç»†è°ƒèŠ‚åˆæˆè¯­éŸ³çš„æƒ…æ„Ÿå’Œå¼ºåº¦ã€‚æˆ‘ä»¬åˆ©ç”¨äº†åŸºç¡€è¯­éŸ³å…‹éš†æ¨¡å‹çš„è¿›å±•ï¼Œæ„å»ºäº†ä¸€ä¸ªå¯Œæœ‰è¡¨ç°åŠ›çš„è¯´è¯è€…è¡¨ç¤ºç©ºé—´ã€‚é€šè¿‡å¼•å…¥ä¸€å¥—è¯„ä¼°æŒ‡æ ‡ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†æƒ…æ„Ÿæ§åˆ¶çš„æœ‰æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜è¯¥æ¡†æ¶åœ¨æƒ…æ„Ÿè¡¨è¾¾ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å•†ä¸šTTSæœåŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01723', 'title': 'HarmoniCa: Harmonizing Training and Inference for Better Feature Cache in Diffusion Transformer Acceleration', 'url': 'https://huggingface.co/papers/2410.01723', 'abstract': 'Diffusion Transformers (DiTs) have gained prominence for outstanding scalability and extraordinary performance in generative tasks. However, their considerable inference costs impede practical deployment. The feature cache mechanism, which involves storing and retrieving redundant computations across timesteps, holds promise for reducing per-step inference time in diffusion models. Most existing caching methods for DiT are manually designed. Although the learning-based approach attempts to optimize strategies adaptively, it suffers from discrepancies between training and inference, which hampers both the performance and acceleration ratio. Upon detailed analysis, we pinpoint that these discrepancies primarily stem from two aspects: (1) Prior Timestep Disregard, where training ignores the effect of cache usage at earlier timesteps, and (2) Objective Mismatch, where the training target (align predicted noise in each timestep) deviates from the goal of inference (generate the high-quality image). To alleviate these discrepancies, we propose HarmoniCa, a novel method that Harmonizes training and inference with a novel learning-based Caching framework built upon Step-Wise Denoising Training (SDT) and Image Error Proxy-Guided Objective (IEPO). Compared to the traditional training paradigm, the newly proposed SDT maintains the continuity of the denoising process, enabling the model to leverage information from prior timesteps during training, similar to the way it operates during inference. Furthermore, we design IEPO, which integrates an efficient proxy mechanism to approximate the final image error caused by reusing the cached feature. Therefore, IEPO helps balance final image quality and cache utilization, resolving the issue of training that only considers the impact of cache usage on the predicted output at each timestep.', 'score': 4, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '1347f88bc3c0da94', 'data': {'categories': ['#training', '#inference', '#optimization', '#diffusion', '#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ HarmoniCa Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ… Ğº ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ (SDT) Ğ¸ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ (IEPO). HarmoniCa Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºÑÑˆĞ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'HarmoniCa: Bridging Training and Inference for Efficient Diffusion Transformers', 'desc': 'This paper introduces HarmoniCa, a new method designed to improve the efficiency of Diffusion Transformers (DiTs) in generative tasks by addressing the discrepancies between training and inference. The authors identify two main issues: the neglect of prior timestep effects during training and the mismatch between training objectives and inference goals. HarmoniCa employs a Step-Wise Denoising Training (SDT) approach to ensure that the model learns to utilize cached features effectively, mirroring its inference behavior. Additionally, the Image Error Proxy-Guided Objective (IEPO) is introduced to optimize the balance between image quality and cache usage, enhancing the overall performance of DiTs during deployment.'}, 'zh': {'title': 'HarmoniCaï¼šæå‡æ‰©æ•£æ¨¡å‹æ¨ç†æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰åœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ¨ç†æˆæœ¬è¾ƒé«˜ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•HarmoniCaï¼Œé€šè¿‡ä¸€ç§åŸºäºå­¦ä¹ çš„ç¼“å­˜æ¡†æ¶æ¥åè°ƒè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼Œè§£å†³äº†è®­ç»ƒå’Œæ¨ç†ä¹‹é—´çš„å·®å¼‚ã€‚è¯¥æ–¹æ³•é‡‡ç”¨é€æ­¥å»å™ªè®­ç»ƒï¼ˆSDTï¼‰å’Œå›¾åƒè¯¯å·®ä»£ç†å¼•å¯¼ç›®æ ‡ï¼ˆIEPOï¼‰ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒæ—¶èƒ½å¤Ÿåˆ©ç”¨ä¹‹å‰æ—¶é—´æ­¥çš„ä¿¡æ¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒHarmoniCaæé«˜äº†æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆå›¾åƒçš„é«˜è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2409.20325', 'title': 'Old Optimizer, New Norm: An Anthology', 'url': 'https://huggingface.co/papers/2409.20325', 'abstract': 'Deep learning optimizers are often motivated through a mix of convex and approximate second-order theory. We select three such methods -- Adam, Shampoo and Prodigy -- and argue that each method can instead be understood as a squarely first-order method without convexity assumptions. In fact, after switching off exponential moving averages, each method is equivalent to steepest descent under a particular norm. By generalizing this observation, we chart a new design space for training algorithms. Different operator norms should be assigned to different tensors based on the role that the tensor plays within the network. For example, while linear and embedding layers may have the same weight space of R^{mtimes n}, these layers play different roles and should be assigned different norms. We hope that this idea of carefully metrizing the neural architecture might lead to more stable, scalable and indeed faster training.', 'score': 3, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'cf41fa7b2190f430', 'data': {'categories': ['#math', '#training', '#optimization', '#architecture'], 'emoji': 'ğŸ§­', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹: Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ²Ğ°Ğ¶Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ Ğ¼Ñ‹ Ğ´ÑƒĞ¼Ğ°Ğ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Adam, Shampoo Ğ¸ Prodigy, Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ²Ñ‹Ğ¿ÑƒĞºĞ»Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚ĞµĞ½ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¿ÑƒÑĞºÑƒ Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ¾Ñ€Ğ¼Ğ¾Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ°Ğ¼ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ½Ğ¾Ñ€Ğ¼Ñ‹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¸Ñ… Ñ€Ğ¾Ğ»Ğ¸ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸.'}, 'en': {'title': 'Reimagining Optimizers: Tailoring Norms for Neural Network Efficiency', 'desc': 'This paper explores deep learning optimizers, specifically Adam, Shampoo, and Prodigy, and reinterprets them as first-order methods without relying on convexity. The authors demonstrate that by disabling exponential moving averages, these optimizers can be viewed as steepest descent methods under specific norms. They propose a novel approach to designing training algorithms by assigning different operator norms to tensors based on their roles in the neural network. This careful metrization of the architecture aims to enhance the stability, scalability, and speed of the training process.'}, 'zh': {'title': 'ä¼˜åŒ–å™¨çš„æ–°è§†è§’ï¼šæ ¹æ®è§’è‰²åˆ†é…èŒƒæ•°', 'desc': 'æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨é€šå¸¸åŸºäºå‡¸æ€§å’Œè¿‘ä¼¼äºŒé˜¶ç†è®ºè¿›è¡Œè®¾è®¡ã€‚æœ¬æ–‡é€‰æ‹©äº†ä¸‰ç§ä¼˜åŒ–æ–¹æ³•â€”â€”Adamã€Shampooå’ŒProdigyï¼Œå¹¶æå‡ºå®ƒä»¬å¯ä»¥è¢«ç†è§£ä¸ºä¸ä¾èµ–äºå‡¸æ€§å‡è®¾çš„ä¸€é˜¶æ–¹æ³•ã€‚é€šè¿‡å…³é—­æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼Œè¿™äº›æ–¹æ³•å®é™…ä¸Šç­‰åŒäºåœ¨ç‰¹å®šèŒƒæ•°ä¸‹çš„æœ€é™¡ä¸‹é™æ³•ã€‚æˆ‘ä»¬å»ºè®®æ ¹æ®å¼ é‡åœ¨ç½‘ç»œä¸­çš„è§’è‰²ï¼Œä¸ºä¸åŒçš„å¼ é‡åˆ†é…ä¸åŒçš„ç®—å­èŒƒæ•°ï¼Œä»¥æœŸå®ç°æ›´ç¨³å®šã€æ›´å¯æ‰©å±•å’Œæ›´å¿«é€Ÿçš„è®­ç»ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01440', 'title': 'Closed-loop Long-horizon Robotic Planning via Equilibrium Sequence Modeling', 'url': 'https://huggingface.co/papers/2410.01440', 'abstract': 'In the endeavor to make autonomous robots take actions, task planning is a major challenge that requires translating high-level task descriptions into long-horizon action sequences. Despite recent advances in language model agents, they remain prone to planning errors and limited in their ability to plan ahead. To address these limitations in robotic planning, we advocate a self-refining scheme that iteratively refines a draft plan until an equilibrium is reached. Remarkably, this process can be optimized end-to-end from an analytical perspective without the need to curate additional verifiers or reward models, allowing us to train self-refining planners in a simple supervised learning fashion. Meanwhile, a nested equilibrium sequence modeling procedure is devised for efficient closed-loop planning that incorporates useful feedback from the environment (or an internal world model). Our method is evaluated on the VirtualHome-Env benchmark, showing advanced performance with better scaling for inference computation. Code is available at https://github.com/Singularity0104/equilibrium-planner.', 'score': 3, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '903dc847118efa24', 'data': {'categories': ['#reasoning', '#inference', '#rl', '#optimization', '#benchmark', '#open_source', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰ĞµĞµÑÑ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰ĞµĞ¹ÑÑ ÑÑ…ĞµĞ¼Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ğ´Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VirtualHome-Env Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ.'}, 'en': {'title': 'Self-Refining Planning for Autonomous Robots', 'desc': 'This paper addresses the challenge of task planning in autonomous robots, which involves converting high-level tasks into detailed action sequences. The authors propose a self-refining planning approach that iteratively improves an initial draft plan until it stabilizes at an optimal solution. This method can be trained using supervised learning without needing extra verification systems or reward models, making it simpler to implement. Additionally, they introduce a nested equilibrium sequence modeling technique that enhances planning efficiency by utilizing feedback from the environment or an internal model.'}, 'zh': {'title': 'è‡ªæˆ‘ç²¾ç‚¼ï¼šæå‡æœºå™¨äººä»»åŠ¡è§„åˆ’çš„æ™ºèƒ½', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è‡ªä¸»æœºå™¨äººä»»åŠ¡è§„åˆ’çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å°†é«˜å±‚ä»»åŠ¡æè¿°è½¬åŒ–ä¸ºé•¿æ—¶é—´çš„è¡ŒåŠ¨åºåˆ—ã€‚å°½ç®¡è¯­è¨€æ¨¡å‹ä»£ç†æœ‰äº†è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨è§„åˆ’æ—¶ä»å®¹æ˜“å‡ºé”™ï¼Œä¸”å‰ç»æ€§æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªæˆ‘ç²¾ç‚¼çš„æ–¹æ¡ˆï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–è‰æ‹Ÿè®¡åˆ’ï¼Œç›´åˆ°è¾¾åˆ°å¹³è¡¡çŠ¶æ€ã€‚è¯¥æ–¹æ³•å¯ä»¥ä»åˆ†æçš„è§’åº¦è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œæ— éœ€é¢å¤–çš„éªŒè¯å™¨æˆ–å¥–åŠ±æ¨¡å‹ï¼Œä¸”åœ¨VirtualHome-EnvåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01481', 'title': 'SonicSim: A customizable simulation platform for speech processing in moving sound source scenarios', 'url': 'https://huggingface.co/papers/2410.01481', 'abstract': 'The systematic evaluation of speech separation and enhancement models under moving sound source conditions typically requires extensive data comprising diverse scenarios. However, real-world datasets often contain insufficient data to meet the training and evaluation requirements of models. Although synthetic datasets offer a larger volume of data, their acoustic simulations lack realism. Consequently, neither real-world nor synthetic datasets effectively fulfill practical needs. To address these issues, we introduce SonicSim, a synthetic toolkit de-designed to generate highly customizable data for moving sound sources. SonicSim is developed based on the embodied AI simulation platform, Habitat-sim, supporting multi-level adjustments, including scene-level, microphone-level, and source-level, thereby generating more diverse synthetic data. Leveraging SonicSim, we constructed a moving sound source benchmark dataset, SonicSet, using the Librispeech, the Freesound Dataset 50k (FSD50K) and Free Music Archive (FMA), and 90 scenes from the Matterport3D to evaluate speech separation and enhancement models. Additionally, to validate the differences between synthetic data and real-world data, we randomly selected 5 hours of raw data without reverberation from the SonicSet validation set to record a real-world speech separation dataset, which was then compared with the corresponding synthetic datasets. Similarly, we utilized the real-world speech enhancement dataset RealMAN to validate the acoustic gap between other synthetic datasets and the SonicSet dataset for speech enhancement. The results indicate that the synthetic data generated by SonicSim can effectively generalize to real-world scenarios. Demo and code are publicly available at https://cslikai.cn/SonicSim/.', 'score': 2, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'd284601ce4d1a07d', 'data': {'categories': ['#audio', '#dataset', '#synthetic', '#data', '#benchmark', '#open_source', '#robotics', '#3d'], 'emoji': 'ğŸ”Š', 'ru': {'title': 'SonicSim: Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ·Ğ²ÑƒĞºĞ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµÑ‡Ğ¸', 'desc': 'SonicSim - ÑÑ‚Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ·Ğ²ÑƒĞºĞ°. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ Habitat-sim Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ¼Ğ¸ĞºÑ€Ğ¾Ñ„Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ·Ğ²ÑƒĞºĞ°. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SonicSim Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SonicSet Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµÑ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ SonicSim, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸.'}, 'en': {'title': 'SonicSim: Bridging the Gap in Speech Separation Datasets', 'desc': 'This paper presents SonicSim, a synthetic toolkit designed to create customizable datasets for evaluating speech separation and enhancement models in dynamic environments. Traditional datasets often lack the necessary diversity and realism, which SonicSim addresses by allowing multi-level adjustments for various sound source conditions. The authors constructed a benchmark dataset called SonicSet, which combines data from multiple sources to facilitate comprehensive model evaluation. Results show that the synthetic data generated by SonicSim can effectively generalize to real-world applications, bridging the gap between synthetic and real-world datasets.'}, 'zh': {'title': 'SonicSimï¼šä¸ºç§»åŠ¨å£°æºç”ŸæˆçœŸå®æ„Ÿåˆæˆæ•°æ®çš„å·¥å…·', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSonicSimçš„åˆæˆå·¥å…·åŒ…ï¼Œç”¨äºç”Ÿæˆå¯å®šåˆ¶çš„ç§»åŠ¨å£°æºæ•°æ®ã€‚è¯¥å·¥å…·åŒ…åŸºäºHabitat-simå¹³å°ï¼Œæ”¯æŒå¤šå±‚æ¬¡çš„è°ƒæ•´ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´ä¸°å¯Œçš„åˆæˆæ•°æ®ã€‚é€šè¿‡SonicSimï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç§»åŠ¨å£°æºåŸºå‡†æ•°æ®é›†SonicSetï¼Œç”¨äºè¯„ä¼°è¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºæ¨¡å‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒSonicSimç”Ÿæˆçš„åˆæˆæ•°æ®èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°çœŸå®åœºæ™¯ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01518', 'title': 'InfiniPot: Infinite Context Processing on Memory-Constrained LLMs', 'url': 'https://huggingface.co/papers/2410.01518', 'abstract': 'Handling long input contexts remains a significant challenge for Large Language Models (LLMs), particularly in resource-constrained environments such as mobile devices. Our work aims to address this limitation by introducing InfiniPot, a novel KV cache control framework designed to enable pre-trained LLMs to manage extensive sequences within fixed memory constraints efficiently, without requiring additional training. InfiniPot leverages Continual Context Distillation (CCD), an iterative process that compresses and retains essential information through novel importance metrics, effectively maintaining critical data even without access to future context. Our comprehensive evaluations indicate that InfiniPot significantly outperforms models trained for long contexts in various NLP tasks, establishing its efficacy and versatility. This work represents a substantial advancement toward making LLMs applicable to a broader range of real-world scenarios.', 'score': 2, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'a13831078cca1658', 'data': {'categories': ['#long_context', '#training', '#optimization', '#inference'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² LLM Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'InfiniPot - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑÑˆĞµĞ¼ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. InfiniPot Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° (CCD) Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'InfiniPot: Efficient Long Context Management for LLMs', 'desc': 'This paper presents InfiniPot, a new framework that helps Large Language Models (LLMs) handle long input sequences efficiently, especially on devices with limited resources. It uses a technique called Continual Context Distillation (CCD) to compress important information and keep it accessible without needing extra training. By focusing on key data, InfiniPot allows LLMs to perform better in natural language processing tasks compared to models specifically trained for long contexts. This advancement makes it easier to use LLMs in various real-world applications where memory is a constraint.'}, 'zh': {'title': 'InfiniPotï¼šé«˜æ•ˆç®¡ç†é•¿ä¸Šä¸‹æ–‡çš„åˆ›æ–°æ¡†æ¶', 'desc': 'å¤„ç†é•¿è¾“å…¥ä¸Šä¸‹æ–‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶æå‡ºäº†InfiniPotï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„KVç¼“å­˜æ§åˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿é¢„è®­ç»ƒçš„LLMsèƒ½å¤Ÿåœ¨å›ºå®šå†…å­˜é™åˆ¶å†…é«˜æ•ˆç®¡ç†å¤§é‡åºåˆ—ï¼Œè€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚InfiniPotåˆ©ç”¨æŒç»­ä¸Šä¸‹æ–‡è’¸é¦ï¼ˆCCDï¼‰ï¼Œé€šè¿‡æ–°é¢–çš„é‡è¦æ€§åº¦é‡è¿­ä»£å‹ç¼©å’Œä¿ç•™å…³é”®ä¿¡æ¯ï¼Œå³ä½¿åœ¨æ²¡æœ‰æœªæ¥ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æœ‰æ•ˆç»´æŠ¤é‡è¦æ•°æ®ã€‚æˆ‘ä»¬çš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒInfiniPotåœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¸ºé•¿ä¸Šä¸‹æ–‡è®­ç»ƒçš„æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå¤šåŠŸèƒ½æ€§ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents', '#agi', '#alignment (5)', '#architecture (12)', '#audio (3)', '#benchmark (11)', '#cv (6)', '#data (5)', '#dataset (10)', '#diffusion (2)', '#ethics (1)', '#games (1)', '#graphs (1)', '#hallucinations (2)', '#healthcare', '#inference (4)', '#interpretability (4)', '#leakage (1)', '#long_context (3)', '#low_resource (1)', '#machine_translation (1)', '#math (3)', '#multilingual (4)', '#multimodal (2)', '#open_source (9)', '#optimization (12)', '#plp (1)', '#rag (1)', '#reasoning (6)', '#rl (2)', '#rlhf (2)', '#robotics (2)', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (6)', '#training (16)', '#transfer_learning (5)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">ğŸ“ ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-10-03 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-03 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-03 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    