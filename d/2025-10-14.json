{
    "date": {
        "ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 14",
        "zh": "10æœˆ14æ—¥"
    },
    "time_utc": "2025-10-14 02:19",
    "weekday": 1,
    "issue_id": 6398,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.08886",
            "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark\n  for Evaluating LLMs",
            "url": "https://huggingface.co/papers/2510.08886",
            "abstract": "FinAuditing is a benchmark for evaluating LLMs on structured financial auditing tasks, revealing their limitations in handling taxonomy-driven, hierarchical financial documents.  \t\t\t\t\tAI-generated summary \t\t\t\t The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical structure of eXtensible Business Reporting Language (XBRL) filings make financial auditing increasingly difficult to automate and verify. While large language models (LLMs) have demonstrated strong capabilities in unstructured text understanding, their ability to reason over structured, interdependent, and taxonomy-driven financial documents remains largely unexplored. To fill this gap, we introduce FinAuditing, the first taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings, FinAuditing defines three complementary subtasks, FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each targeting a distinct aspect of structured auditing reasoning. We further propose a unified evaluation framework integrating retrieval, classification, and reasoning metrics across these subtasks. Extensive zero-shot experiments on 13 state-of-the-art LLMs reveal that current models perform inconsistently across semantic, relational, and mathematical dimensions, with accuracy drops of up to 60-90% when reasoning over hierarchical multi-document structures. Our findings expose the systematic limitations of modern LLMs in taxonomy-grounded financial reasoning and establish FinAuditing as a foundation for developing trustworthy, structure-aware, and regulation-aligned financial intelligence systems. The benchmark dataset is available at Hugging Face.",
            "score": 7,
            "issue_id": 6398,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            },
            "hash": "af8af45f11c7cfc5",
            "authors": [
                "Yan Wang",
                "Keyi Wang",
                "Shanshan Yang",
                "Jaisal Patel",
                "Jeff Zhao",
                "Fengran Mo",
                "Xueqing Peng",
                "Lingfei Qian",
                "Jimin Huang",
                "Guojun Xiong",
                "Xiao-Yang Liu",
                "Jian-Yun Nie"
            ],
            "affiliations": [
                "Columbia University USA",
                "The Fin AI USA",
                "University of Montreal Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08886.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "LLM Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ»Ğ¸ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½ Ğ¿Ğ¾ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ°ÑƒĞ´Ğ¸Ñ‚Ñƒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº FinAuditing Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ÑĞ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ XBRL. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹, Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…, ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°Ğ¼ US-GAAP. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 13 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ â€” Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ğ´Ğ°Ğ»Ğ° Ğ½Ğ° 60-90% Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ AI Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ°."
                },
                "en": {
                    "title": "FinAuditing: Bridging the Gap in Financial Reasoning for LLMs",
                    "desc": "FinAuditing is a new benchmark designed to assess large language models (LLMs) on structured financial auditing tasks, particularly focusing on the challenges posed by hierarchical financial documents. It highlights the difficulties LLMs face in reasoning over complex, taxonomy-driven structures like those found in Generally Accepted Accounting Principles (GAAP) and eXtensible Business Reporting Language (XBRL) filings. The benchmark includes three specific subtasks: FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each addressing different aspects of financial auditing. Results from testing 13 advanced LLMs show significant performance drops, revealing their limitations in handling structured financial reasoning, thus paving the way for improved financial intelligence systems."
                },
                "zh": {
                    "title": "FinAuditingï¼šæ­ç¤ºLLMsåœ¨è´¢åŠ¡å®¡è®¡ä¸­çš„å±€é™æ€§",
                    "desc": "FinAuditingæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»“æ„åŒ–è´¢åŠ¡å®¡è®¡ä»»åŠ¡ä¸­çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æ­ç¤ºäº†LLMsåœ¨å¤„ç†åŸºäºåˆ†ç±»æ³•çš„å±‚æ¬¡è´¢åŠ¡æ–‡æ¡£æ—¶çš„å±€é™æ€§ã€‚é€šè¿‡å®šä¹‰ä¸‰ä¸ªäº’è¡¥çš„å­ä»»åŠ¡ï¼ŒFinSMã€FinREå’ŒFinMRï¼ŒFinAuditingä¸“æ³¨äºè¯­ä¹‰ä¸€è‡´æ€§ã€å…³ç³»ä¸€è‡´æ€§å’Œæ•°å€¼ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å¤„ç†å±‚æ¬¡å¤šæ–‡æ¡£ç»“æ„æ—¶ï¼Œå‡†ç¡®ç‡ä¸‹é™é«˜è¾¾60-90%ï¼Œæ˜¾ç¤ºå‡ºç°ä»£LLMsåœ¨è´¢åŠ¡æ¨ç†æ–¹é¢çš„ç³»ç»Ÿæ€§å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.10670",
            "title": "AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning\n  in 4D Scenes",
            "url": "https://huggingface.co/papers/2510.10670",
            "abstract": "A two-stage paradigm adapts pre-trained Text-to-Video models for viewpoint prediction in 4D scenes by integrating an adaptive learning branch and a camera extrinsic diffusion branch.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Text-to-Video (T2V) models have demonstrated powerful capability in visual simulation of real-world geometry and physical laws, indicating its potential as implicit world models. Inspired by this, we explore the feasibility of leveraging the video generation prior for viewpoint planning from given 4D scenes, since videos internally accompany dynamic scenes with natural viewpoints. To this end, we propose a two-stage paradigm to adapt pre-trained T2V models for viewpoint prediction, in a compatible manner. First, we inject the 4D scene representation into the pre-trained T2V model via an adaptive learning branch, where the 4D scene is viewpoint-agnostic and the conditional generated video embeds the viewpoints visually. Then, we formulate viewpoint extraction as a hybrid-condition guided camera extrinsic denoising process. Specifically, a camera extrinsic diffusion branch is further introduced onto the pre-trained T2V model, by taking the generated video and 4D scene as input. Experimental results show the superiority of our proposed method over existing competitors, and ablation studies validate the effectiveness of our key technical designs. To some extent, this work proves the potential of video generation models toward 4D interaction in real world.",
            "score": 6,
            "issue_id": 6398,
            "pub_date": "2025-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "e9218d263c01e3bc",
            "authors": [
                "Yu Li",
                "Menghan Xia",
                "Gongye Liu",
                "Jianhong Bai",
                "Xintao Wang",
                "Conglang Zhang",
                "Yuxuan Lin",
                "Ruihang Chu",
                "Pengfei Wan",
                "Yujiu Yang"
            ],
            "affiliations": [
                "HKUST",
                "HUST",
                "Kling Team, Kuaishou Technology",
                "Tsinghua University",
                "Wuhan University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.10670.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² 4D ÑÑ†ĞµĞ½Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Text-to-Video Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ² 4D ÑÑ†ĞµĞ½Ğ°Ñ…. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ 4D ÑÑ†ĞµĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² T2V Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²ĞµÑ‚ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ĞµĞ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… world models Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ¼ Ğ² 4D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "Harnessing Video Generation for 4D Viewpoint Prediction",
                    "desc": "This paper presents a two-stage approach to adapt pre-trained Text-to-Video (T2V) models for predicting viewpoints in 4D scenes. The first stage involves integrating a 4D scene representation into the T2V model using an adaptive learning branch, allowing the model to generate videos that visually represent different viewpoints. The second stage formulates viewpoint extraction as a denoising process, utilizing a camera extrinsic diffusion branch that processes both the generated video and the 4D scene. The results demonstrate that this method outperforms existing techniques, highlighting the potential of T2V models for real-world 4D interactions."
                },
                "zh": {
                    "title": "åˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿›è¡Œ4Dè§†è§’é¢„æµ‹çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡é€‚åº”æ€§å­¦ä¹ åˆ†æ”¯å’Œç›¸æœºå¤–éƒ¨æ‰©æ•£åˆ†æ”¯ï¼Œå°†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ï¼ˆT2Vï¼‰åº”ç”¨äº4Dåœºæ™¯çš„è§†è§’é¢„æµ‹ã€‚é¦–å…ˆï¼Œé€šè¿‡é€‚åº”æ€§å­¦ä¹ åˆ†æ”¯å°†4Dåœºæ™¯è¡¨ç¤ºæ³¨å…¥åˆ°é¢„è®­ç»ƒçš„T2Væ¨¡å‹ä¸­ï¼Œä½¿å¾—ç”Ÿæˆçš„è§†é¢‘èƒ½å¤Ÿè‡ªç„¶åœ°åµŒå…¥è§†è§’ä¿¡æ¯ã€‚æ¥ç€ï¼Œå°†è§†è§’æå–è¿‡ç¨‹è§†ä¸ºä¸€ç§æ··åˆæ¡ä»¶å¼•å¯¼çš„ç›¸æœºå¤–éƒ¨å»å™ªè¿‡ç¨‹ï¼Œè¿›ä¸€æ­¥å¼•å…¥ç›¸æœºå¤–éƒ¨æ‰©æ•£åˆ†æ”¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰ç«äº‰è€…ï¼ŒéªŒè¯äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç°å®ä¸–ç•Œ4Däº¤äº’ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07841",
            "title": "Self-Improving LLM Agents at Test-Time",
            "url": "https://huggingface.co/papers/2510.07841",
            "abstract": "A test-time self-improvement method enhances language models by generating additional training examples from uncertain cases, leading to better performance with fewer samples.  \t\t\t\t\tAI-generated summary \t\t\t\t One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution.",
            "score": 4,
            "issue_id": 6398,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "60bb755e99449195",
            "authors": [
                "Emre Can Acikgoz",
                "Cheng Qian",
                "Heng Ji",
                "Dilek Hakkani-TÃ¼r",
                "Gokhan Tur"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07841.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#agents",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ: ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ½Ğ° ÑĞ²Ğ¾Ğ¸Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (TT-SI). ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ Ğ¾Ğ½Ğ° ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ğ»Ğ¾Ñ…Ğ¾, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ½ĞµÑ† Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 5.48% Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² 68 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğº ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering Language Models Through Self-Improvement at Test-Time",
                    "desc": "This paper introduces a novel method called Test-Time Self-Improvement (TT-SI) for enhancing language models by generating additional training examples from uncertain cases. The approach involves three key steps: identifying challenging samples, creating similar examples from these samples, and fine-tuning the model using the newly generated data. By focusing on self-awareness and self-data augmentation, TT-SI allows models to improve their performance significantly while using far fewer training samples. Empirical results show that this method leads to an average accuracy gain of 5.48% across various benchmarks, demonstrating its effectiveness compared to traditional learning techniques."
                },
                "zh": {
                    "title": "æµ‹è¯•æ—¶è‡ªæˆ‘æ”¹è¿›ï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶è‡ªæˆ‘æ”¹è¿›çš„æ–¹æ³•ï¼Œé€šè¿‡ä»ä¸ç¡®å®šçš„æ¡ˆä¾‹ä¸­ç”Ÿæˆé¢å¤–çš„è®­ç»ƒæ ·æœ¬ï¼Œå¢å¼ºè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼šé¦–å…ˆè¯†åˆ«æ¨¡å‹éš¾ä»¥å¤„ç†çš„æ ·æœ¬ï¼Œå…¶æ¬¡ä»è¿™äº›ä¸ç¡®å®šæ ·æœ¬ä¸­ç”Ÿæˆç›¸ä¼¼çš„ä¾‹å­ï¼Œæœ€ååœ¨æµ‹è¯•æ—¶è¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†5.48%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬å‡å°‘äº†68å€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæµ‹è¯•æ—¶è‡ªæˆ‘æ”¹è¿›ç®—æ³•ä¸ºæ„å»ºæ›´å¼ºå¤§çš„æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08026",
            "title": "PEAR: Phase Entropy Aware Reward for Efficient Reasoning",
            "url": "https://huggingface.co/papers/2510.08026",
            "abstract": "A reward mechanism called Phase Entropy Aware Reward (PEAR) controls the length of reasoning in large models by adjusting entropy at different stages, balancing conciseness and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have achieved impressive performance on complex reasoning tasks by generating detailed chain-of-thought (CoT) explanations. However, these responses are often excessively long, containing redundant reasoning steps that inflate inference cost and reduce usability. Controlling the length of generated reasoning without sacrificing accuracy remains an open challenge. Through a systematic empirical analysis, we reveal a consistent positive correlation between model entropy and response length at different reasoning stages across diverse LRMs: the thinking phase exhibits higher entropy, reflecting exploratory behavior of longer responses, while the final answer phase shows lower entropy, indicating a more deterministic solution. This observation suggests that entropy at different reasoning stages can serve as a control knob for balancing conciseness and performance. Based on this insight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism that incorporating phase-dependent entropy into the reward design. Instead of treating all tokens uniformly, PEAR penalize excessive entropy during the thinking phase and allowing moderate exploration at the final answer phase, which encourages models to generate concise reasoning traces that retain sufficient flexibility to solve the task correctly. This enables adaptive control of response length without relying on explicit length targets or rigid truncation rules. Extensive experiments across four benchmarks demonstrate that PEAR consistently reduces response length while sustaining competitive accuracy across model scales. In addition, PEAR demonstrates strong out-of-distribution (OOD) robustness beyond the training distribution. Our code is available at: https://github.com/iNLP-Lab/PEAR.",
            "score": 2,
            "issue_id": 6398,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "f4c7a863b396ac9a",
            "authors": [
                "Chen Huang",
                "Wei Lu",
                "Wenxuan Zhang"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08026.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ„Ğ°Ğ·Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ² Ñ„Ğ°Ğ·Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼, Ğ½Ğ¸Ğ·ĞºĞ°Ñ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„Ğ°Ğ·Ğµ â€” Ğº Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ PEAR, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑˆÑ‚Ñ€Ğ°Ñ„ÑƒĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ² Ñ„Ğ°Ğ·Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½ÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„Ğ°Ğ·Ğµ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Large Reasoning Models Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PEAR ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Balancing Conciseness and Accuracy with PEAR",
                    "desc": "The paper introduces a new reward mechanism called Phase Entropy Aware Reward (PEAR) that helps large reasoning models (LRMs) generate concise yet accurate responses. It identifies a relationship between model entropy and response length, where higher entropy during the thinking phase leads to longer, more exploratory responses, while lower entropy in the final answer phase results in more deterministic outputs. PEAR adjusts the reward based on the entropy at different reasoning stages, penalizing excessive exploration in the thinking phase while allowing some flexibility in the final answer phase. This approach effectively reduces response length without compromising accuracy, demonstrating improved performance across various benchmarks and robustness to out-of-distribution scenarios."
                },
                "zh": {
                    "title": "æ§åˆ¶æ¨ç†é•¿åº¦ï¼Œæå‡æ¨¡å‹æ€§èƒ½çš„PEARæœºåˆ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé˜¶æ®µç†µæ„ŸçŸ¥å¥–åŠ±ï¼ˆPEARï¼‰çš„å¥–åŠ±æœºåˆ¶ï¼Œç”¨äºæ§åˆ¶å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†é•¿åº¦ã€‚é€šè¿‡è°ƒæ•´ä¸åŒé˜¶æ®µçš„ç†µï¼ŒPEARåœ¨ç®€æ´æ€§å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹çš„ç†µä¸å“åº”é•¿åº¦ä¹‹é—´å­˜åœ¨æ­£ç›¸å…³å…³ç³»ï¼Œæ€è€ƒé˜¶æ®µçš„ç†µè¾ƒé«˜ï¼Œè€Œæœ€ç»ˆç­”æ¡ˆé˜¶æ®µçš„ç†µè¾ƒä½ã€‚PEARé€šè¿‡åœ¨æ€è€ƒé˜¶æ®µæƒ©ç½šè¿‡é«˜çš„ç†µï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆç®€æ´çš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒè¶³å¤Ÿçš„çµæ´»æ€§ä»¥æ­£ç¡®è§£å†³ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04617",
            "title": "Making Mathematical Reasoning Adaptive",
            "url": "https://huggingface.co/papers/2510.04617",
            "abstract": "AdaR framework enhances LLMs' robustness and generalization in mathematical reasoning by synthesizing logically equivalent queries and using RLVR to penalize spurious logic.  \t\t\t\t\tAI-generated summary \t\t\t\t Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. However, existing LLMs exhibit failures of robustness and generalization. This paper attributes these deficiencies to spurious reasoning, i.e., producing answers from superficial features. To address this challenge, we propose the AdaR framework to enable adaptive reasoning, wherein models rely on problem-solving logic to produce answers. AdaR synthesizes logically equivalent queries by varying variable values, and trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic. To improve data quality, we extract the problem-solving logic from the original query and generate the corresponding answer by code execution, then apply a sanity check. Experimental results demonstrate that AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis indicates that data synthesis and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs. Subsequent analyses derive key design insights into the effect of critical factors and the applicability to instruct LLMs. Our project is available at https://github.com/LaiZhejian/AdaR",
            "score": 1,
            "issue_id": 6398,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "82cf47c00d882ce9",
            "authors": [
                "Zhejian Lai",
                "Xiang Geng",
                "Zhijun Wang",
                "Yang Bai",
                "Jiahuan Li",
                "Rongxiang Weng",
                "Jingang Wang",
                "Xuezhi Cao",
                "Xunliang Cai",
                "Shujian Huang"
            ],
            "affiliations": [
                "Meituan Inc., China",
                "Nanjing University, Nanjing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04617.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#data",
                    "#math"
                ],
                "emoji": "ğŸ”¢",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ reasoning Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº AdaR Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ reasoning Ğ² LLM Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ (spurious reasoning), ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ RLVR (reinforcement learning) Ğ´Ğ»Ñ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ reasoning. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AdaR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing LLMs' Reasoning with AdaR Framework",
                    "desc": "The AdaR framework aims to improve the robustness and generalization of large language models (LLMs) in mathematical reasoning tasks. It addresses the issue of spurious reasoning by synthesizing logically equivalent queries and employing Reinforcement Learning with Value Regularization (RLVR) to penalize incorrect logic. By extracting problem-solving logic and generating answers through code execution, AdaR enhances data quality and encourages models to rely on sound reasoning. Experimental results show that AdaR significantly boosts performance in mathematical reasoning while ensuring efficient use of data."
                },
                "zh": {
                    "title": "AdaRæ¡†æ¶ï¼šæå‡LLMsæ•°å­¦æ¨ç†çš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†AdaRæ¡†æ¶ï¼Œä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰çš„LLMså¸¸å¸¸å› è¡¨é¢ç‰¹å¾å¯¼è‡´é”™è¯¯æ¨ç†ï¼Œç¼ºä¹æ·±å±‚æ¬¡çš„é€»è¾‘æ€è€ƒã€‚AdaRé€šè¿‡åˆæˆé€»è¾‘ç­‰ä»·çš„æŸ¥è¯¢å¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å˜ä½“ï¼ˆRLVRï¼‰æ¥æƒ©ç½šè™šå‡é€»è¾‘ï¼Œä»è€Œä¿ƒè¿›æ¨¡å‹çš„è‡ªé€‚åº”æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaRæ˜¾è‘—æé«˜äº†æ•°å­¦æ¨ç†çš„è¡¨ç°ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•°æ®æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04587",
            "title": "Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole\n  Slide Image Diagnosis Behavior",
            "url": "https://huggingface.co/papers/2510.04587",
            "abstract": "A framework records and utilizes expert navigation behavior in whole-slide imaging to build an agentic system for pathology diagnosis, achieving high precision and recall in metastasis detection.  \t\t\t\t\tAI-generated summary \t\t\t\t Diagnosing a whole-slide image is an interactive, multi-stage process involving changes in magnification and movement between fields. Although recent pathology foundation models are strong, practical agentic systems that decide what field to examine next, adjust magnification, and deliver explainable diagnoses are still lacking. The blocker is data: scalable, clinically aligned supervision of expert viewing behavior that is tacit and experience-based, not written in textbooks or online, and therefore absent from large language model training. We introduce the AI Session Recorder, which works with standard WSI viewers to unobtrusively record routine navigation and convert the viewer logs into standardized behavioral commands (inspect or peek at discrete magnifications) and bounding boxes. A lightweight human-in-the-loop review turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired \"where to look\" and \"why it matters\" supervision produced at roughly six times lower labeling time. Using this behavioral data, we build Pathologist-o3, a two-stage agent that first proposes regions of interest and then performs behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection, it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the state-of-the-art OpenAI o3 model and generalizing across backbones. To our knowledge, this constitutes one of the first behavior-grounded agentic systems in pathology. Turning everyday viewer logs into scalable, expert-validated supervision, our framework makes agentic pathology practical and establishes a path to human-aligned, upgradeable clinical AI.",
            "score": 0,
            "issue_id": 6398,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 6",
                "zh": "10æœˆ6æ—¥"
            },
            "hash": "89586b8e177d522a",
            "authors": [
                "Sheng Wang",
                "Ruiming Wu",
                "Charles Herndon",
                "Yihang Liu",
                "Shunsuke Koga",
                "Jeanne Shen",
                "Zhi Huang"
            ],
            "affiliations": [
                "Department of Biostatistics, Epidemiology & Informatics, University of Pennsylvania",
                "Department of Electrical and System Engineering, University of Pennsylvania",
                "Department of Pathology and Laboratory Medicine, University of Pennsylvania",
                "Department of Pathology, Stanford University",
                "Department of Pathology, University of California at San Francisco"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04587.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#agi",
                    "#reasoning",
                    "#healthcare",
                    "#interpretability",
                    "#science",
                    "#dataset"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ Ğ¸Ñ… Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ AI Session Recorder â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğµ Ğ³Ğ¸ÑÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½ Ğ°Ğ³ĞµĞ½Ñ‚ Pathologist-o3, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ°, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 100% Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ¸ 84.5% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ°ÑÑ‚Ğ°Ğ·Ğ¾Ğ² Ğ² Ğ»Ğ¸Ğ¼Ñ„Ğ¾ÑƒĞ·Ğ»Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ OpenAI o3. Ğ­Ñ‚Ğ¾ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ· Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Transforming Expert Navigation into Smart Pathology Diagnosis",
                    "desc": "This paper presents a new framework that captures expert navigation behavior in whole-slide imaging to enhance pathology diagnosis. It introduces the AI Session Recorder, which records how pathologists interact with images and converts this data into actionable commands for an AI system. The resulting model, Pathologist-o3, uses this behavioral data to identify areas of interest and provide reasoning for its decisions, achieving impressive metrics in detecting metastasis. This approach not only improves diagnostic accuracy but also paves the way for more practical and explainable AI systems in clinical settings."
                },
                "zh": {
                    "title": "æ™ºèƒ½ç—…ç†è¯Šæ–­çš„æ–°è·¯å¾„",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡è®°å½•å’Œåˆ©ç”¨ä¸“å®¶åœ¨å…¨åˆ‡ç‰‡æˆåƒä¸­çš„å¯¼èˆªè¡Œä¸ºï¼Œæ„å»ºäº†ä¸€ç§ç”¨äºç—…ç†è¯Šæ–­çš„æ™ºèƒ½ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåœ¨è½¬ç§»æ€§è‚¿ç˜¤æ£€æµ‹ä¸­å®ç°äº†é«˜ç²¾åº¦å’Œé«˜å¬å›ç‡ã€‚æˆ‘ä»¬å¼•å…¥äº†AIä¼šè¯è®°å½•å™¨ï¼Œèƒ½å¤Ÿæ— ç¼è®°å½•ä¸“å®¶çš„å¸¸è§„å¯¼èˆªï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„è¡Œä¸ºå‘½ä»¤å’Œè¾¹ç•Œæ¡†ã€‚æœ€ç»ˆï¼ŒåŸºäºè¿™äº›è¡Œä¸ºæ•°æ®ï¼Œæˆ‘ä»¬æ„å»ºäº†Pathologist-o3ï¼Œä¸€ä¸ªèƒ½å¤Ÿæå‡ºæ„Ÿå…´è¶£åŒºåŸŸå¹¶è¿›è¡Œè¡Œä¸ºå¼•å¯¼æ¨ç†çš„åŒé˜¶æ®µæ™ºèƒ½ä½“ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-13.html",
    "link_next": "2025-10-15.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "13.10",
        "en": "10/13",
        "zh": "10æœˆ13æ—¥"
    },
    "short_date_next": {
        "ru": "15.10",
        "en": "10/15",
        "zh": "10æœˆ15æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 2,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}