
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 27 papers. May 16.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">16 Ğ¼Ğ°Ñ</span> | <span id="title-articles-count">27 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-15.html">â¬…ï¸ <span id="prev-date">15.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-19.html">â¡ï¸ <span id="next-date">19.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'};
        let feedDateNext = {'ru': '19.05', 'en': '05/19', 'zh': '5æœˆ19æ—¥'};
        let feedDatePrev = {'ru': '15.05', 'en': '05/15', 'zh': '5æœˆ15æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.10554', 'title': "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models", 'url': 'https://huggingface.co/papers/2505.10554', 'abstract': 'Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the model\'s "aha moment". However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMs\' reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental "aha moments". Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10\\% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional 2\\% average gain in the performance ceiling across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment', 'score': 71, 'issue_id': 3792, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': '9b574dafe9f7fb18', 'authors': ['Zhiyuan Hu', 'Yibo Wang', 'Hanze Dong', 'Yuhui Xu', 'Amrita Saha', 'Caiming Xiong', 'Bryan Hooi', 'Junnan Li'], 'affiliations': ['National University of Singapore', 'Salesforce AI Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10554.jpg', 'data': {'categories': ['#alignment', '#science', '#rl', '#optimization', '#training', '#reasoning', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ°-ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‚Ñ€ĞµĞ¼Ñ Ğ¼ĞµÑ‚Ğ°-ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸: Ğ´ĞµĞ´ÑƒĞºÑ†Ğ¸ĞµĞ¹, Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ğ¸ Ğ°Ğ±Ğ´ÑƒĞºÑ†Ğ¸ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 10% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Reasoning in Models through Explicit Meta-Ability Alignment', 'desc': "This paper discusses how large reasoning models (LRMs) can improve their reasoning abilities through a structured approach. It highlights the limitations of relying on spontaneous reasoning behaviors, known as 'aha moments', which can be unpredictable. The authors propose a method to explicitly align models with three key reasoning skills: deduction, induction, and abduction, using self-verifiable tasks. Their approach, which includes a three-stage pipeline and domain-specific reinforcement learning, significantly enhances the performance of LRMs on various benchmarks."}, 'zh': {'title': 'æ˜ç¡®å¯¹é½ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å…·å¤‡é•¿é“¾æ¨ç†çš„æ½œåœ¨èƒ½åŠ›ã€‚ä»¥ç»“æœä¸ºåŸºç¡€çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¶ç„¶å¼•å‘é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘ä¿®æ­£å’Œå›æº¯ï¼Œä½†è¿™äº›è¡Œä¸ºçš„æ—¶æœºå’Œä¸€è‡´æ€§éš¾ä»¥é¢„æµ‹ï¼Œé™åˆ¶äº†LRMsæ¨ç†èƒ½åŠ›çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„è‡ªæˆ‘éªŒè¯ä»»åŠ¡ï¼Œæ˜ç¡®å¯¹æ¨¡å‹è¿›è¡Œä¸‰ç§å…ƒèƒ½åŠ›çš„å¯¹é½ï¼šæ¼”ç»ã€å½’çº³å’Œæº¯å› ã€‚é€šè¿‡ä¸‰é˜¶æ®µçš„ä¸ªä½“å¯¹é½ã€å‚æ•°ç©ºé—´åˆå¹¶å’Œé¢†åŸŸç‰¹å®šçš„å¼ºåŒ–å­¦ä¹ ï¼Œæ€§èƒ½æå‡è¶…è¿‡10%ï¼Œå¹¶åœ¨æ•°å­¦ã€ç¼–ç¨‹å’Œç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°é¢å¤–çš„2%çš„å¹³å‡å¢ç›Šï¼Œè¯æ˜äº†æ˜ç¡®çš„å…ƒèƒ½åŠ›å¯¹é½ä¸ºæ¨ç†æä¾›äº†å¯æ‰©å±•å’Œå¯é çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.09666', 'title': 'System Prompt Optimization with Meta-Learning', 'url': 'https://huggingface.co/papers/2505.09666', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains. Motivated by this, we introduce the novel problem of bilevel system prompt optimization, whose objective is to design system prompts that are robust to diverse user prompts and transferable to unseen tasks. To tackle this problem, we then propose a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them. We conduct experiments on 14 unseen datasets spanning 5 different domains, on which we show that our approach produces system prompts that generalize effectively to diverse user prompts. Also, our findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.', 'score': 47, 'issue_id': 3792, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ', 'en': 'May 14', 'zh': '5æœˆ14æ—¥'}, 'hash': 'e8d5cb78c5949430', 'authors': ['Yumin Choi', 'Jinheon Baek', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2505.09666.jpg', 'data': {'categories': ['#multimodal', '#training', '#optimization', '#transfer_learning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 14 Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ½ĞµĞ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼.'}, 'en': {'title': 'Optimizing System Prompts for Versatile Language Model Performance', 'desc': 'This paper addresses the optimization of system prompts in Large Language Models (LLMs), which are crucial for enhancing model performance across various tasks. It introduces a new approach called bilevel system prompt optimization, focusing on creating system prompts that can adapt to different user prompts and tasks. The authors propose a meta-learning framework that iteratively refines both system and user prompts, ensuring they work well together. Experimental results demonstrate that the optimized system prompts can generalize effectively and adapt quickly to new tasks with fewer optimization steps, leading to better performance.'}, 'zh': {'title': 'ä¼˜åŒ–ç³»ç»Ÿæç¤ºï¼Œæå‡æ¨¡å‹é€‚åº”æ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œè€Œä¼˜åŒ–è¾“å…¥æç¤ºåœ¨æå‡å…¶æ€§èƒ½ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚ç°æœ‰çš„æç¤ºä¼˜åŒ–ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šä»»åŠ¡çš„ç”¨æˆ·æç¤ºä¸Šï¼Œå¿½è§†äº†ç³»ç»Ÿæç¤ºçš„ä¼˜åŒ–ã€‚æœ¬æ–‡æå‡ºäº†åŒå±‚ç³»ç»Ÿæç¤ºä¼˜åŒ–çš„æ–°é—®é¢˜ï¼Œæ—¨åœ¨è®¾è®¡èƒ½å¤Ÿé€‚åº”å¤šç§ç”¨æˆ·æç¤ºå¹¶å¯è¿ç§»åˆ°æœªè§ä»»åŠ¡çš„ç³»ç»Ÿæç¤ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…ƒå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜åŒ–ç³»ç»Ÿæç¤ºï¼ŒåŒæ—¶è¿­ä»£æ›´æ–°ç”¨æˆ·æç¤ºï¼Œä»¥ç¡®ä¿ä¸¤è€…ä¹‹é—´çš„ååŒä½œç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.09723', 'title': 'EnerVerse-AC: Envisioning Embodied Environments with Action Condition', 'url': 'https://huggingface.co/papers/2505.09723', 'abstract': "Robotic imitation learning has advanced from solving static tasks to addressing dynamic interaction scenarios, but testing and evaluation remain costly and challenging due to the need for real-time interaction with dynamic environments. We propose EnerVerse-AC (EVAC), an action-conditional world model that generates future visual observations based on an agent's predicted actions, enabling realistic and controllable robotic inference. Building on prior architectures, EVAC introduces a multi-level action-conditioning mechanism and ray map encoding for dynamic multi-view image generation while expanding training data with diverse failure trajectories to improve generalization. As both a data engine and evaluator, EVAC augments human-collected trajectories into diverse datasets and generates realistic, action-conditioned video observations for policy testing, eliminating the need for physical robots or complex simulations. This approach significantly reduces costs while maintaining high fidelity in robotic manipulation evaluation. Extensive experiments validate the effectiveness of our method. Code, checkpoints, and datasets can be found at <https://annaj2178.github.io/EnerverseAC.github.io>.", 'score': 18, 'issue_id': 3793, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ', 'en': 'May 14', 'zh': '5æœˆ14æ—¥'}, 'hash': '8adef8c283985aee', 'authors': ['Yuxin Jiang', 'Shengcong Chen', 'Siyuan Huang', 'Liliang Chen', 'Pengfei Zhou', 'Yue Liao', 'Xindong He', 'Chiming Liu', 'Hongsheng Li', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['AgiBot', 'MMLab-CUHK', 'SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2505.09723.jpg', 'data': {'categories': ['#agents', '#dataset', '#training', '#optimization', '#open_source', '#robotics', '#video'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'EVAC: Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'EVAC - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. EVAC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»ÑƒÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'Revolutionizing Robotic Learning with EnerVerse-AC', 'desc': 'The paper presents EnerVerse-AC (EVAC), a novel action-conditional world model designed for robotic imitation learning in dynamic environments. EVAC generates future visual observations based on predicted actions, allowing for realistic robotic inference without the need for physical robots. It enhances training data by incorporating diverse failure trajectories and employs a multi-level action-conditioning mechanism for improved generalization. This method significantly reduces evaluation costs while ensuring high fidelity in testing robotic manipulation policies.'}, 'zh': {'title': 'åŠ¨æ€äº¤äº’ä¸­çš„æœºå™¨äººæ¨¡ä»¿å­¦ä¹ æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºEnerVerse-ACï¼ˆEVACï¼‰çš„åŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººæ¨¡ä»¿å­¦ä¹ åœ¨åŠ¨æ€äº¤äº’åœºæ™¯ä¸­çš„è¡¨ç°ã€‚EVACèƒ½å¤Ÿæ ¹æ®ä»£ç†çš„é¢„æµ‹åŠ¨ä½œç”Ÿæˆæœªæ¥çš„è§†è§‰è§‚å¯Ÿï¼Œä»è€Œå®ç°æ›´çœŸå®å’Œå¯æ§çš„æœºå™¨äººæ¨ç†ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†å¤šå±‚æ¬¡çš„åŠ¨ä½œæ¡ä»¶æœºåˆ¶å’Œå…‰çº¿å›¾ç¼–ç ï¼Œå¢å¼ºäº†åŠ¨æ€å¤šè§†å›¾å›¾åƒç”Ÿæˆçš„èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¤šæ ·åŒ–çš„å¤±è´¥è½¨è¿¹æ‰©å±•è®­ç»ƒæ•°æ®ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å°†äººç±»æ”¶é›†çš„è½¨è¿¹è½¬åŒ–ä¸ºå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼ŒEVACæ˜¾è‘—é™ä½äº†æµ‹è¯•æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†é«˜ä¿çœŸåº¦çš„æœºå™¨äººæ“ä½œè¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10475', 'title': 'Parallel Scaling Law for Language Models', 'url': 'https://huggingface.co/papers/2505.10475', 'abstract': "It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply P diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the P outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with P parallel streams is similar to scaling the parameters by O(log P) while showing superior inference efficiency. For example, ParScale can use up to 22times less memory increase and 6times less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.", 'score': 16, 'issue_id': 3800, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': '71bd56e1da07bcba', 'authors': ['Mouxiang Chen', 'Binyuan Hui', 'Zeyu Cui', 'Jiaxi Yang', 'Dayiheng Liu', 'Jianling Sun', 'Junyang Lin', 'Zhongxin Liu'], 'affiliations': ['Qwen Team, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10475.jpg', 'data': {'categories': ['#inference', '#low_resource', '#training', '#architecture', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ (ParScale). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ P Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ÑĞ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ P Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ². ParScale Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğº Ğ»ÑĞ±Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ĞµĞ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Unlocking Efficiency: Parallel Scaling for Language Models', 'desc': "This paper introduces a new method called parallel scaling (ParScale) for improving the efficiency of language models during training and inference. Instead of simply increasing the number of parameters or output tokens, ParScale enhances the model's ability to perform computations in parallel by applying multiple transformations to the input. The authors propose a new scaling law that shows how using P parallel streams can achieve similar performance to increasing parameters by O(log P), while significantly reducing memory and latency costs. This approach allows for the reuse of existing models and can be adapted to various tasks, making it a cost-effective solution for deploying powerful models in resource-constrained environments."}, 'zh': {'title': 'å¹¶è¡Œæ‰©å±•ï¼šé«˜æ•ˆçš„æ¨¡å‹è®¡ç®—æ–°æ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹æ‰©å±•æ–¹æ³•ï¼Œç§°ä¸ºå¹¶è¡Œæ‰©å±•ï¼ˆParScaleï¼‰ï¼Œå®ƒé€šè¿‡å¢åŠ æ¨¡å‹çš„å¹¶è¡Œè®¡ç®—æ¥æé«˜æ¨ç†æ•ˆç‡ï¼Œè€Œä¸æ˜¯å•çº¯å¢åŠ å‚æ•°æˆ–è¾“å‡ºä»¤ç‰Œã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹è¾“å…¥åº”ç”¨å¤šç§å¯å­¦ä¹ çš„å˜æ¢ï¼Œå¹¶åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶å¹¶è¡Œæ‰§è¡Œæ¨¡å‹çš„å‰å‘ä¼ é€’ï¼ŒåŠ¨æ€èšåˆå¤šä¸ªè¾“å‡ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨Pä¸ªå¹¶è¡Œæµçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šç›¸å½“äºå‚æ•°æ‰©å±•çš„O(log P)ï¼Œä½†åœ¨æ¨ç†æ•ˆç‡ä¸Šæ›´ä¼˜ã€‚ParScaleè¿˜å¯ä»¥é€šè¿‡åœ¨å°‘é‡ä»¤ç‰Œä¸Šè¿›è¡Œåè®­ç»ƒï¼Œå°†ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹è½¬åŒ–ä¸ºå¹¶è¡Œæ‰©å±•æ¨¡å‹ï¼Œä»è€Œé™ä½è®­ç»ƒæˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10185', 'title': 'The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think', 'url': 'https://huggingface.co/papers/2505.10185', 'abstract': 'Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such approaches are constrained by human intuition and fail to capture the full diversity of model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up framework for analyzing and steering model reasoning. Our method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior. Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods. Moreover, we demonstrate that this understanding enables performance gains: we can predict which strategy a model is likely to use and guide it toward more effective alternatives. Finally, we provide practical insights, such as that training data format (e.g., free-form vs. multiple-choice) has a far greater impact on reasoning behavior than data domain, underscoring the importance of format-aware model design.', 'score': 16, 'issue_id': 3791, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': 'dc93377d68390280', 'authors': ['Seongyun Lee', 'Seungone Kim', 'Minju Seo', 'Yongrae Jo', 'Dongyoung Go', 'Hyeonbin Hwang', 'Jinho Park', 'Xiang Yue', 'Sean Welleck', 'Graham Neubig', 'Moontae Lee', 'Minjoon Seo'], 'affiliations': ['Carnegie Mellon University', 'Cornell University', 'KAIST AI', 'LG AI Research', 'NAVER Search US'], 'pdf_title_img': 'assets/pdf/title_img/2505.10185.jpg', 'data': {'categories': ['#reasoning', '#data', '#training', '#interpretability', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜: Ğ¾Ñ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğº ÑĞ½Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¿ĞµĞ´Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'CoT Encyclopedia', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡ĞµĞ¼ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ°Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ."}, 'en': {'title': 'Unlocking Model Reasoning with the CoT Encyclopedia', 'desc': 'This paper presents the CoT Encyclopedia, a new framework for analyzing the reasoning strategies of large language models through their chain-of-thought (CoT) outputs. Unlike previous methods that rely on predefined categories, this approach uses a bottom-up technique to automatically extract and cluster diverse reasoning criteria from the models. The framework not only enhances the interpretability of model behaviors but also improves performance by predicting and guiding models towards more effective reasoning strategies. Additionally, the study highlights the significant influence of training data format on reasoning behavior, emphasizing the need for format-aware design in model training.'}, 'zh': {'title': 'ç†è§£æ¨ç†ï¼Œæå‡æ¨¡å‹è¡¨ç°', 'desc': 'é•¿é“¾æ¨ç†ï¼ˆCoTï¼‰æ˜¯ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ•ˆä½¿ç”¨çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†æˆ‘ä»¬å¯¹å…¶æ¨ç†ç­–ç•¥çš„ç†è§£ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†CoTç™¾ç§‘å…¨ä¹¦ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªä¸‹è€Œä¸Šçš„æ¡†æ¶ï¼Œç”¨äºåˆ†æå’Œå¼•å¯¼æ¨¡å‹æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•è‡ªåŠ¨æå–æ¨¡å‹ç”Ÿæˆçš„CoTä¸­çš„å¤šæ ·åŒ–æ¨ç†æ ‡å‡†ï¼Œå°†å…¶åµŒå…¥è¯­ä¹‰ç©ºé—´ï¼Œèšç±»æˆä»£è¡¨æ€§ç±»åˆ«ï¼Œå¹¶æ¨å¯¼å‡ºå¯¹æ¯”æ€§æ ‡å‡†ä»¥è§£é‡Šæ¨ç†è¡Œä¸ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ¡†æ¶æ¯”ç°æœ‰æ–¹æ³•æä¾›äº†æ›´å¯è§£é‡Šå’Œå…¨é¢çš„åˆ†æï¼Œå¹¶ä¸”èƒ½å¤Ÿé¢„æµ‹æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„ç­–ç•¥ï¼Œä»è€Œå¼•å¯¼å…¶æœå‘æ›´æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.09694', 'title': 'EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied\n  World Models', 'url': 'https://huggingface.co/papers/2505.09694', 'abstract': 'Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from language commands, effectively bridging vision and action in embodied AI applications. This work addresses the critical challenge of evaluating EWMs beyond general perceptual metrics to ensure the generation of physically grounded and action-consistent behaviors. We propose the Embodied World Model Benchmark (EWMBench), a dedicated framework designed to evaluate EWMs based on three key aspects: visual scene consistency, motion correctness, and semantic alignment. Our approach leverages a meticulously curated dataset encompassing diverse scenes and motion patterns, alongside a comprehensive multi-dimensional evaluation toolkit, to assess and compare candidate models. The proposed benchmark not only identifies the limitations of existing video generation models in meeting the unique requirements of embodied tasks but also provides valuable insights to guide future advancements in the field. The dataset and evaluation tools are publicly available at https://github.com/AgibotTech/EWMBench.', 'score': 15, 'issue_id': 3792, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ', 'en': 'May 14', 'zh': '5æœˆ14æ—¥'}, 'hash': '8439c79042ee5e23', 'authors': ['Hu Yue', 'Siyuan Huang', 'Yue Liao', 'Shengcong Chen', 'Pengfei Zhou', 'Liliang Chen', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['AgiBot', 'HIT', 'MMLab-CUHK', 'SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2505.09694.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#multimodal', '#games', '#video', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'EWMBench: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº EWMBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Embodied World Models, EWM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ EWM Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼: ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ñ‹, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. EWMBench Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': "Evaluating AI's Ability to Create Realistic Actions in Video", 'desc': 'This paper discusses the development of embodied world models (EWMs) that generate realistic video scenes based on text descriptions. It highlights the need for better evaluation methods for these models, moving beyond simple visual quality assessments to include physical realism and action consistency. The authors introduce the Embodied World Model Benchmark (EWMBench), which evaluates EWMs on visual scene consistency, motion correctness, and semantic alignment. By providing a curated dataset and evaluation tools, this work aims to improve the performance of video generation models in tasks that require understanding and interaction with the environment.'}, 'zh': {'title': 'è¯„ä¼°å…·èº«ä¸–ç•Œæ¨¡å‹çš„æ–°åŸºå‡†', 'desc': 'æœ€è¿‘ï¼Œåˆ›æ„äººå·¥æ™ºèƒ½çš„è¿›æ­¥ä½¿å¾—æ ¹æ®è¯­è¨€æŒ‡ä»¤åˆæˆé«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘æˆä¸ºå¯èƒ½ã€‚åŸºäºè¿™äº›å‘å±•ï¼Œæ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹æ¼”å˜ä¸ºå…·èº«ä¸–ç•Œæ¨¡å‹ï¼ˆEWMï¼‰ï¼Œèƒ½å¤Ÿæ ¹æ®è¯­è¨€å‘½ä»¤ç”Ÿæˆç‰©ç†ä¸Šåˆç†çš„åœºæ™¯ï¼Œæœ‰æ•ˆåœ°å°†è§†è§‰ä¸è¡ŒåŠ¨ç»“åˆåœ¨å…·èº«äººå·¥æ™ºèƒ½åº”ç”¨ä¸­ã€‚æœ¬æ–‡æå‡ºäº†å…·èº«ä¸–ç•Œæ¨¡å‹åŸºå‡†ï¼ˆEWMBenchï¼‰ï¼Œæ—¨åœ¨é€šè¿‡è§†è§‰åœºæ™¯ä¸€è‡´æ€§ã€è¿åŠ¨æ­£ç¡®æ€§å’Œè¯­ä¹‰å¯¹é½ä¸‰ä¸ªå…³é”®æ–¹é¢æ¥è¯„ä¼°EWMã€‚è¯¥åŸºå‡†ä¸ä»…è¯†åˆ«äº†ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æ»¡è¶³å…·èº«ä»»åŠ¡ç‹¬ç‰¹éœ€æ±‚æ–¹é¢çš„å±€é™æ€§ï¼Œè¿˜ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10527', 'title': 'WorldPM: Scaling Human Preference Modeling', 'url': 'https://huggingface.co/papers/2505.10527', 'abstract': "Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodies a unified representation of human preferences. In this paper, we collect preference data from public forums covering diverse user communities, and conduct extensive training using 15M-scale data across models ranging from 1.5B to 72B parameters. We observe distinct patterns across different evaluation metrics: (1) Adversarial metrics (ability to identify deceptive features) consistently scale up with increased training data and base model size; (2) Objective metrics (objective knowledge with well-defined answers) show emergent behavior in larger language models, highlighting WorldPM's scalability potential; (3) Subjective metrics (subjective preferences from a limited number of humans or AI) do not demonstrate scaling trends. Further experiments validate the effectiveness of WorldPM as a foundation for preference fine-tuning. Through evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly improves the generalization performance across human preference datasets of varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5% on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we observe significant improvements on both in-house and public evaluation sets, with notable gains of 4% to 8% in our in-house evaluations.", 'score': 13, 'issue_id': 3791, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': 'd2fe74535293635c', 'authors': ['Binghai Wang', 'Runji Lin', 'Keming Lu', 'Le Yu', 'Zhenru Zhang', 'Fei Huang', 'Chujie Zheng', 'Kai Dang', 'Yang Fan', 'Xingzhang Ren', 'An Yang', 'Binyuan Hui', 'Dayiheng Liu', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang', 'Yu-Gang Jiang', 'Bowen Yu', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Institute of Trustworthy Embodied Artificial Intelligence, Fudan University', 'Qwen Team, Alibaba Group', 'School of Computer Science, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10527.jpg', 'data': {'categories': ['#data', '#training', '#benchmark', '#optimization', '#dataset', '#rlhf', '#alignment'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ÑÑ‚ÑÑ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ World Preference Modeling (WorldPM) Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 15 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 1.5 Ğ´Ğ¾ 72 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ½Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ WorldPM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Scaling Human Preferences with WorldPM', 'desc': 'This paper introduces World Preference Modeling (WorldPM), which explores how human preferences can be effectively modeled and scaled in machine learning. The authors demonstrate that preference modeling follows similar scaling laws as language modeling, where larger models and datasets lead to improved performance. They collect diverse preference data and train models with varying sizes, revealing that adversarial and objective metrics improve with scale, while subjective metrics do not show consistent trends. The findings suggest that WorldPM enhances generalization across different human preference datasets and significantly boosts performance in reinforcement learning from human feedback (RLHF) applications.'}, 'zh': {'title': 'ä¸–ç•Œåå¥½å»ºæ¨¡ï¼šæå‡äººç±»åå¥½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨åå¥½å»ºæ¨¡ä¸­å­˜åœ¨çš„è§„æ¨¡æ³•åˆ™ï¼Œç±»ä¼¼äºè¯­è¨€å»ºæ¨¡ä¸­çš„ç°è±¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸–ç•Œåå¥½å»ºæ¨¡ï¼ˆWorldPMï¼‰ï¼Œå¼ºè°ƒå…¶åœ¨å¤„ç†äººç±»åå¥½æ—¶çš„ç»Ÿä¸€è¡¨ç¤ºèƒ½åŠ›ã€‚é€šè¿‡æ”¶é›†æ¥è‡ªå…¬å…±è®ºå›çš„åå¥½æ•°æ®ï¼Œå¹¶åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬å‘ç°ä¸åŒè¯„ä¼°æŒ‡æ ‡çš„è¡¨ç°å­˜åœ¨æ˜æ˜¾å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWorldPMåœ¨å¤šç§äººç±»åå¥½æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ï¼Œå°¤å…¶åœ¨å…³é”®å­ä»»åŠ¡ä¸Šæå‡è¶…è¿‡5%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10562', 'title': 'End-to-End Vision Tokenizer Tuning', 'url': 'https://huggingface.co/papers/2505.10562', 'abstract': 'Existing vision tokenization isolates the optimization of vision tokenizers from downstream training, implicitly assuming the visual tokens can generalize well across various tasks, e.g., image generation and visual question answering. The vision tokenizer optimized for low-level reconstruction is agnostic to downstream tasks requiring varied representations and semantics. This decoupled paradigm introduces a critical misalignment: The loss of the vision tokenization can be the representation bottleneck for target tasks. For example, errors in tokenizing text in a given image lead to poor results when recognizing or generating them. To address this, we propose ETT, an end-to-end vision tokenizer tuning approach that enables joint optimization between vision tokenization and target autoregressive tasks. Unlike prior autoregressive models that use only discrete indices from a frozen vision tokenizer, ETT leverages the visual embeddings of the tokenizer codebook, and optimizes the vision tokenizers end-to-end with both reconstruction and caption objectives. ETT can be seamlessly integrated into existing training pipelines with minimal architecture modifications. Our ETT is simple to implement and integrate, without the need to adjust the original codebooks or architectures of the employed large language models. Extensive experiments demonstrate that our proposed end-to-end vision tokenizer tuning unlocks significant performance gains, i.e., 2-6% for multimodal understanding and visual generation tasks compared to frozen tokenizer baselines, while preserving the original reconstruction capability. We hope this very simple and strong method can empower multimodal foundation models besides image generation and understanding.', 'score': 12, 'issue_id': 3792, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': '1f57cc7a0953749d', 'authors': ['Wenxuan Wang', 'Fan Zhang', 'Yufeng Cui', 'Haiwen Diao', 'Zhuoyan Luo', 'Huchuan Lu', 'Jing Liu', 'Xinlong Wang'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Dalian University of Technology', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10562.jpg', 'data': {'categories': ['#alignment', '#cv', '#optimization', '#multimodal', '#training', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšĞ¾Ğ½ĞµÑ†-Ğ²-ĞºĞ¾Ğ½ĞµÑ† Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ETT Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ETT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ†ĞµĞ»ĞµĞ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 2-6% Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering Vision Tokenization for Better Multimodal Performance', 'desc': 'This paper introduces ETT, a new method for optimizing vision tokenizers in machine learning. Traditional vision tokenization methods often work separately from the tasks they support, which can lead to poor performance due to misalignment in representation. ETT addresses this issue by allowing joint optimization of vision tokenization and downstream tasks, improving the quality of visual representations. The results show that ETT significantly enhances performance in multimodal tasks while maintaining the original capabilities of the tokenizer.'}, 'zh': {'title': 'ç«¯åˆ°ç«¯è§†è§‰æ ‡è®°å™¨è°ƒä¼˜ï¼Œæå‡å¤šæ¨¡æ€æ€§èƒ½', 'desc': 'ç°æœ‰çš„è§†è§‰æ ‡è®°åŒ–æ–¹æ³•å°†è§†è§‰æ ‡è®°å™¨çš„ä¼˜åŒ–ä¸ä¸‹æ¸¸è®­ç»ƒåˆ†å¼€ï¼Œå‡è®¾è§†è§‰æ ‡è®°åœ¨å„ç§ä»»åŠ¡ä¸­èƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ETTï¼Œä¸€ç§ç«¯åˆ°ç«¯çš„è§†è§‰æ ‡è®°å™¨è°ƒä¼˜æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°è§†è§‰æ ‡è®°åŒ–ä¸ç›®æ ‡è‡ªå›å½’ä»»åŠ¡çš„è”åˆä¼˜åŒ–ã€‚ETTåˆ©ç”¨æ ‡è®°å™¨ä»£ç æœ¬çš„è§†è§‰åµŒå…¥ï¼Œä¼˜åŒ–è§†è§‰æ ‡è®°å™¨ï¼ŒåŒæ—¶å…¼é¡¾é‡å»ºå’Œæè¿°ç›®æ ‡ã€‚å®éªŒè¡¨æ˜ï¼ŒETTåœ¨å¤šæ¨¡æ€ç†è§£å’Œè§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­ç›¸æ¯”äºå›ºå®šæ ‡è®°å™¨åŸºçº¿ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07782', 'title': 'MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering', 'url': 'https://huggingface.co/papers/2505.07782', 'abstract': "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents.", 'score': 12, 'issue_id': 3791, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '6a556cb214d3d71d', 'authors': ['Rushi Qiang', 'Yuchen Zhuang', 'Yinghao Li', 'Dingu Sagar V K', 'Rongzhi Zhang', 'Changhao Li', 'Ian Shu-Hei Wong', 'Sherry Yang', 'Percy Liang', 'Chao Zhang', 'Bo Dai'], 'affiliations': ['Georgia Institute of Technology', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.07782.jpg', 'data': {'categories': ['#data', '#training', '#architecture', '#benchmark', '#open_source', '#games', '#agents', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'MLE-Dojo: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'MLE-Dojo - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², MLE-Dojo Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ, Ğ¾Ñ‚Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 200 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Kaggle Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ¸ÑĞº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ° ĞºĞ¾Ğ´Ğ°. MLE-Dojo Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ñ‚Ğ°Ğº Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'MLE-Dojo: Empowering Iterative Learning for LLMs', 'desc': 'MLE-Dojo is a new framework designed for reinforcement learning and improving large language model (LLM) agents through iterative machine learning engineering (MLE) processes. It offers an interactive environment where agents can experiment and refine their solutions based on structured feedback, unlike traditional benchmarks that use static datasets. The framework is built on over 200 real-world Kaggle challenges, allowing for diverse MLE tasks such as data processing and hyperparameter tuning. Evaluations show that while LLMs can make iterative improvements, they still struggle with generating long-term solutions and solving complex issues, highlighting the need for further advancements in autonomous learning.'}, 'zh': {'title': 'MLE-Dojoï¼šæ¨åŠ¨è‡ªä¸»å­¦ä¹ æ¨¡å‹çš„åˆ›æ–°å¹³å°', 'desc': 'MLE-Dojoæ˜¯ä¸€ä¸ªç±»ä¼¼Gymçš„æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»ŸåŒ–åœ°è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œè¯„ä¼°å’Œæ”¹è¿›è‡ªä¸»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ã€‚ä¸ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒMLE-Dojoæä¾›äº†ä¸€ä¸ªäº’åŠ¨ç¯å¢ƒï¼Œä½¿ä»£ç†èƒ½å¤Ÿé€šè¿‡ç»“æ„åŒ–åé¦ˆå¾ªç¯è¿›è¡Œè¿­ä»£å®éªŒã€è°ƒè¯•å’Œä¼˜åŒ–è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶åŸºäº200å¤šä¸ªçœŸå®çš„KaggleæŒ‘æˆ˜ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„å¼€æ”¾å¼æœºå™¨å­¦ä¹ å·¥ç¨‹ä»»åŠ¡ï¼Œåæ˜ äº†ç°å®çš„å·¥ç¨‹åœºæ™¯ã€‚MLE-Dojoçš„å¯æ‰§è¡Œç¯å¢ƒæ”¯æŒé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œå…¨é¢çš„ä»£ç†è®­ç»ƒï¼Œä¿ƒè¿›äº†è¿­ä»£å®éªŒå’Œå®æ—¶ç»“æœéªŒè¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.06027', 'title': 'Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target\n  Self-Distillation', 'url': 'https://huggingface.co/papers/2505.06027', 'abstract': "This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like GDPR. Unlike prior methods that rely on static hyperparameters or starting model outputs, Unilogit dynamically adjusts target logits to achieve a uniform probability for the target token, leveraging the current model's outputs for more accurate self-distillation targets. This approach not only eliminates the need for additional hyperparameters but also enhances the model's ability to approximate the golden targets. Extensive experiments on public benchmarks and an in-house e-commerce dataset demonstrate Unilogit's superior performance in balancing forget and retain objectives, outperforming state-of-the-art methods such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness across various scenarios, highlighting its practical applicability and effectiveness in achieving efficacious machine unlearning.", 'score': 11, 'issue_id': 3804, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 Ğ¼Ğ°Ñ', 'en': 'May 9', 'zh': '5æœˆ9æ—¥'}, 'hash': '8abae468afb82fcd', 'authors': ['Stefan Vasilev', 'Christian Herold', 'Baohao Liao', 'Seyyed Hadi Hashemi', 'Shahram Khadivi', 'Christof Monz'], 'affiliations': ['University of Amsterdam', 'eBay Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.06027.jpg', 'data': {'categories': ['#training', '#benchmark', '#dataset', '#security', '#ethics'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Unilogit: ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Unilogit - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Unilogit Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Unilogit Ğ² Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unilogit: Smart Forgetting for Safer AI', 'desc': "This paper presents Unilogit, a new self-distillation technique designed for machine unlearning in Large Language Models. It focuses on the ability to forget specific information while still keeping the model useful, which is important for following data privacy laws like GDPR. Unilogit improves upon previous methods by dynamically adjusting target logits based on the model's current outputs, rather than using fixed hyperparameters. The results show that Unilogit performs better than existing methods in both forgetting and retaining information, proving its effectiveness in real-world applications."}, 'zh': {'title': 'Unilogitï¼šæ™ºèƒ½é—å¿˜çš„è‡ªæˆ‘è’¸é¦æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„è‡ªæˆ‘è’¸é¦æ–¹æ³•Unilogitï¼Œç”¨äºå¤§è¯­è¨€æ¨¡å‹ä¸­çš„æœºå™¨é—å¿˜ã€‚Unilogitè§£å†³äº†åœ¨éµå®ˆæ•°æ®éšç§æ³•è§„ï¼ˆå¦‚GDPRï¼‰çš„åŒæ—¶ï¼Œé€‰æ‹©æ€§é—å¿˜ç‰¹å®šä¿¡æ¯çš„æŒ‘æˆ˜ã€‚ä¸ä¾èµ–é™æ€è¶…å‚æ•°æˆ–åˆå§‹æ¨¡å‹è¾“å‡ºçš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒUnilogitåŠ¨æ€è°ƒæ•´ç›®æ ‡logitsï¼Œä»¥å®ç°ç›®æ ‡æ ‡è®°çš„å‡åŒ€æ¦‚ç‡ï¼Œä»è€Œæé«˜è‡ªæˆ‘è’¸é¦ç›®æ ‡çš„å‡†ç¡®æ€§ã€‚é€šè¿‡åœ¨å…¬å…±åŸºå‡†å’Œå†…éƒ¨ç”µå­å•†åŠ¡æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒUnilogitåœ¨é—å¿˜ä¸ä¿ç•™ç›®æ ‡çš„å¹³è¡¡æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šäº†NPOå’ŒUnDIALç­‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10320', 'title': 'J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.10320', 'abstract': 'The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this work we introduce J1, a reinforcement learning approach to training such models. Our method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, our approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. We provide analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. We find that our models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses.', 'score': 10, 'issue_id': 3791, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': '61617ab29ad31cc7', 'authors': ['Chenxi Whitehouse', 'Tianlu Wang', 'Ping Yu', 'Xian Li', 'Jason Weston', 'Ilia Kulikov', 'Swarnadeep Saha'], 'affiliations': ['FAIR at Meta', 'GenAI at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2505.10320.jpg', 'data': {'categories': ['#reasoning', '#training', '#benchmark', '#optimization', '#hallucinations', '#rlhf', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'J1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ÑÑƒĞ´ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ÑÑƒĞ´ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ J1. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. J1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 8B Ğ¸ 70B, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· DeepSeek-R1. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ J1 ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ñ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Empowering AI Judgment with J1: A Reinforcement Learning Breakthrough', 'desc': 'This paper addresses the challenge of evaluating AI models by introducing J1, a reinforcement learning method designed to enhance the judgment capabilities of large language models (LLMs). J1 transforms both verifiable and non-verifiable prompts into judgment tasks that provide clear rewards, promoting better reasoning and reducing bias in evaluations. The results show that J1 outperforms existing models of similar sizes, demonstrating its effectiveness in training models to make more accurate judgments. The study also includes a detailed analysis of various training strategies and their impact on model performance, highlighting the importance of structured evaluation criteria and self-referential comparisons.'}, 'zh': {'title': 'æå‡AIè¯„ä¼°è´¨é‡çš„å…³é”®ï¼šJ1æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½è¯„ä¼°è´¨é‡å¯¹AIè¿›æ­¥çš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºJ1çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œåˆ¤æ–­ã€‚J1é€šè¿‡å°†å¯éªŒè¯å’Œä¸å¯éªŒè¯çš„æç¤ºè½¬æ¢ä¸ºå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„åˆ¤æ–­ä»»åŠ¡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€ç»´èƒ½åŠ›å¹¶å‡å°‘åˆ¤æ–­åå·®ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒJ1åœ¨è®­ç»ƒæ—¶è¡¨ç°ä¼˜äºç°æœ‰çš„8Bå’Œ70Bæ¨¡å‹ï¼Œç”šè‡³åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒçš„è®­ç»ƒç­–ç•¥å’Œæ¨¡å‹å˜ä½“ï¼Œå‘ç°J1æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°è¿›è¡Œåˆ¤æ–­ï¼Œå­¦ä¹ è¯„ä¼°æ ‡å‡†å¹¶è‡ªæˆ‘ç”Ÿæˆå‚è€ƒç­”æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.09990', 'title': 'PointArena: Probing Multimodal Grounding Through Language-Guided\n  Pointing', 'url': 'https://huggingface.co/papers/2505.09990', 'abstract': 'Pointing serves as a fundamental and intuitive mechanism for grounding language within visual contexts, with applications spanning robotics, assistive technologies, and interactive AI systems. While recent multimodal models have started to support pointing capabilities, existing benchmarks typically focus only on referential object localization tasks. We introduce PointArena, a comprehensive platform for evaluating multimodal pointing across diverse reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a curated dataset containing approximately 1,000 pointing tasks across five reasoning categories; (2) Point-Battle, an interactive, web-based arena facilitating blind, pairwise model comparisons, which has already gathered over 4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation system allowing users to directly evaluate multimodal model pointing capabilities in practical settings. We conducted extensive evaluations of both state-of-the-art open-source and proprietary multimodal models. Results indicate that Molmo-72B consistently outperforms other models, though proprietary models increasingly demonstrate comparable performance. Additionally, we find that supervised training specifically targeting pointing tasks significantly enhances model performance. Across our multi-stage evaluation pipeline, we also observe strong correlations, underscoring the critical role of precise pointing capabilities in enabling multimodal models to effectively bridge abstract reasoning with concrete, real-world actions. Project page: https://pointarena.github.io/', 'score': 9, 'issue_id': 3796, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': 'f0065735407ccc64', 'authors': ['Long Cheng', 'Jiafei Duan', 'Yi Ru Wang', 'Haoquan Fang', 'Boyang Li', 'Yushan Huang', 'Elvis Wang', 'Ainaz Eftekhar', 'Jason Lee', 'Wentao Yuan', 'Rose Hendrix', 'Noah A. Smith', 'Fei Xia', 'Dieter Fox', 'Ranjay Krishna'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Anderson Collegiate Vocational Institute', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.09990.jpg', 'data': {'categories': ['#training', '#multimodal', '#reasoning', '#open_source', '#benchmark', '#robotics', '#dataset'], 'emoji': 'ğŸ‘†', 'ru': {'title': 'PointArena: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PointArena - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Point-Bench Ñ 1000 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ñ€ĞµĞ½Ñƒ Point-Battle Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Point-Act Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Molmo-72B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ, Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ.'}, 'en': {'title': 'PointArena: Evaluating Multimodal Pointing for Real-World Applications', 'desc': 'This paper introduces PointArena, a new platform designed to evaluate how well multimodal models can perform pointing tasks in various reasoning scenarios. It includes a dataset called Point-Bench with around 1,000 tasks, an interactive comparison tool named Point-Battle, and a robotic system called Point-Act for real-world testing. The study shows that the Molmo-72B model outperforms others, especially when trained specifically for pointing tasks. The findings highlight the importance of accurate pointing in helping models connect abstract reasoning with real-world actions.'}, 'zh': {'title': 'å¤šæ¨¡æ€æŒ‡å‘èƒ½åŠ›çš„è¯„ä¼°æ–°å¹³å°', 'desc': 'æœ¬æ–‡ä»‹ç»äº†PointArenaï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€æŒ‡å‘èƒ½åŠ›çš„ç»¼åˆå¹³å°ã€‚å®ƒåŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šPoint-Benchï¼Œä¸€ä¸ªåŒ…å«çº¦1000ä¸ªæŒ‡å‘ä»»åŠ¡çš„æ•°æ®é›†ï¼›Point-Battleï¼Œä¸€ä¸ªäº’åŠ¨çš„ç½‘ç»œå¹³å°ï¼Œç”¨äºæ¨¡å‹çš„ç›²æµ‹æ¯”è¾ƒï¼›ä»¥åŠPoint-Actï¼Œä¸€ä¸ªå…è®¸ç”¨æˆ·åœ¨å®é™…ç¯å¢ƒä¸­è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹æŒ‡å‘èƒ½åŠ›çš„æœºå™¨äººæ“ä½œç³»ç»Ÿã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸“é—¨é’ˆå¯¹æŒ‡å‘ä»»åŠ¡çš„ç›‘ç£è®­ç»ƒæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¼ºè°ƒäº†ç²¾ç¡®æŒ‡å‘èƒ½åŠ›åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­è¿æ¥æŠ½è±¡æ¨ç†ä¸å®é™…è¡ŒåŠ¨çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10558', 'title': 'Style Customization of Text-to-Vector Generation with Image Diffusion\n  Priors', 'url': 'https://huggingface.co/papers/2505.10558', 'abstract': 'Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style customization, which is vital for producing a collection of vector graphics with consistent visual appearance and coherent aesthetics. Extending existing T2V methods for style customization poses certain challenges. Optimization-based T2V models can utilize the priors of text-to-image (T2I) models for customization, but struggle with maintaining structural regularity. On the other hand, feed-forward T2V models can ensure structural regularity, yet they encounter difficulties in disentangling content and style due to limited SVG training data.   To address these challenges, we propose a novel two-stage style customization pipeline for SVG generation, making use of the advantages of both feed-forward T2V models and T2I image priors. In the first stage, we train a T2V diffusion model with a path-level representation to ensure the structural regularity of SVGs while preserving diverse expressive capabilities. In the second stage, we customize the T2V diffusion model to different styles by distilling customized T2I models. By integrating these techniques, our pipeline can generate high-quality and diverse SVGs in custom styles based on text prompts in an efficient feed-forward manner. The effectiveness of our method has been validated through extensive experiments. The project page is https://customsvg.github.io.', 'score': 8, 'issue_id': 3794, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': '949341b92df361ab', 'authors': ['Peiying Zhang', 'Nanxuan Zhao', 'Jing Liao'], 'affiliations': ['Adobe Research', 'City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.10558.jpg', 'data': {'categories': ['#cv', '#diffusion', '#multimodal', '#optimization'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ (SVG) Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑÑ‚Ğ¸Ğ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Text-to-Vector (T2V) Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Text-to-Image (T2I). ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ T2V Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ SVG. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ T2V Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ T2I.'}, 'en': {'title': 'Custom SVGs: Merging Structure and Style in Vector Graphics Generation', 'desc': 'This paper presents a new method for generating Scalable Vector Graphics (SVGs) from text prompts while allowing for style customization. The proposed two-stage pipeline combines the strengths of both text-to-vector (T2V) diffusion models and text-to-image (T2I) models to ensure structural regularity and diverse styles. In the first stage, a T2V diffusion model is trained to maintain the SVG structure, while the second stage focuses on customizing the model to different styles using T2I model distillation. The results demonstrate that this approach can efficiently produce high-quality SVGs that meet aesthetic requirements based on user-defined text inputs.'}, 'zh': {'title': 'é«˜æ•ˆç”Ÿæˆå®šåˆ¶åŒ–SVGçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µæ ·å¼å®šåˆ¶ç®¡é“ï¼Œç”¨äºä»æ–‡æœ¬ç”Ÿæˆå¯ç¼©æ”¾çŸ¢é‡å›¾å½¢ï¼ˆSVGï¼‰ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªåŸºäºè·¯å¾„çº§è¡¨ç¤ºçš„æ–‡æœ¬åˆ°çŸ¢é‡ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç¡®ä¿SVGçš„ç»“æ„è§„åˆ™æ€§ï¼ŒåŒæ—¶ä¿ç•™å¤šæ ·çš„è¡¨ç°èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µï¼Œé€šè¿‡æç‚¼å®šåˆ¶çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹æ¥å®ç°ä¸åŒæ ·å¼çš„å®šåˆ¶ã€‚æˆ‘ä»¬çš„ç®¡é“èƒ½å¤Ÿé«˜æ•ˆåœ°ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„SVGï¼Œæ»¡è¶³å®é™…åº”ç”¨ä¸­çš„æ ·å¼éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.09738', 'title': 'Achieving Tokenizer Flexibility in Language Models through Heuristic\n  Adaptation and Supertoken Learning', 'url': 'https://huggingface.co/papers/2505.09738', 'abstract': 'Pretrained language models (LLMs) are often constrained by their fixed tokenization schemes, leading to inefficiencies and performance limitations, particularly for multilingual or specialized applications. This tokenizer lock-in presents significant challenges. standard methods to overcome this often require prohibitive computational resources. Although tokenizer replacement with heuristic initialization aims to reduce this burden, existing methods often require exhaustive residual fine-tuning and still may not fully preserve semantic nuances or adequately address the underlying compression inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a model-agnostic tokenizer transplantation method, and second, novel pre-tokenization learning for multi-word Supertokens to enhance compression and reduce fragmentation. Tokenadapt initializes new unique token embeddings via a hybrid heuristic that combines two methods: a local estimate based on subword decomposition using the old tokenizer, and a global estimate utilizing the top-k semantically similar tokens from the original vocabulary. This methodology aims to preserve semantics while significantly minimizing retraining requirements. Empirical investigations validate both contributions: the transplantation heuristic successfully initializes unique tokens, markedly outperforming conventional baselines and sophisticated methods including Transtokenizer and ReTok, while our Supertokens achieve notable compression gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid initialization consistently yields lower perplexity ratios compared to both ReTok and TransTokenizer baselines across different base models and newly trained target tokenizers. TokenAdapt typically reduced the overall perplexity ratio significantly compared to ReTok, yielding at least a 2-fold improvement in these aggregate scores.', 'score': 8, 'issue_id': 3795, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ', 'en': 'May 14', 'zh': '5æœˆ14æ—¥'}, 'hash': 'ae96fcdfda8fee90', 'authors': ['Shaurya Sharthak', 'Vinayak Pahalwan', 'Adithya Kamath', 'Adarsh Shirawalmath'], 'affiliations': ['proton.me', 'tensoic.com', 'tinycompany.in'], 'pdf_title_img': 'assets/pdf/title_img/2505.09738.jpg', 'data': {'categories': ['#optimization', '#multilingual', '#training', '#transfer_learning', '#data'], 'emoji': 'ğŸ”„', 'ru': {'title': 'TokenAdapt: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ TokenAdapt Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ÑĞ»Ğ¾Ğ² Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğµ-Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… ÑÑƒĞ¿ĞµÑ€Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ñ… ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‰Ğ¸Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TokenAdapt Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Unlocking Language Models: Efficient Tokenization with Tokenadapt', 'desc': 'This paper addresses the limitations of fixed tokenization schemes in pretrained language models, which can hinder performance, especially in multilingual contexts. The authors propose a novel framework called Tokenadapt, which allows for efficient tokenizer transplantation without extensive retraining. By introducing a hybrid heuristic for initializing new token embeddings and utilizing multi-word Supertokens, the method aims to enhance semantic preservation and reduce fragmentation. Empirical results show that Tokenadapt significantly outperforms existing methods in terms of perplexity, demonstrating its effectiveness in improving language model efficiency.'}, 'zh': {'title': 'çªç ´åˆ†è¯é™åˆ¶ï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸å¸¸å—åˆ°å›ºå®šåˆ†è¯æ–¹æ¡ˆçš„é™åˆ¶ï¼Œè¿™å¯¼è‡´åœ¨å¤šè¯­è¨€æˆ–ä¸“ä¸šåº”ç”¨ä¸­æ•ˆç‡ä½ä¸‹å’Œæ€§èƒ½å—é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼ŒåŒ…å«ä¸¤é¡¹åˆ›æ–°ï¼šTokenadaptï¼Œä¸€ç§æ¨¡å‹æ— å…³çš„åˆ†è¯å™¨ç§»æ¤æ–¹æ³•ï¼Œä»¥åŠç”¨äºå¤šè¯è¶…çº§æ ‡è®°çš„æ–°å‹é¢„åˆ†è¯å­¦ä¹ ï¼Œä»¥å¢å¼ºå‹ç¼©æ•ˆæœå¹¶å‡å°‘ç¢ç‰‡åŒ–ã€‚Tokenadapté€šè¿‡ç»“åˆæ—§åˆ†è¯å™¨çš„å­è¯åˆ†è§£å’ŒåŸå§‹è¯æ±‡ä¸­è¯­ä¹‰ç›¸ä¼¼çš„å‰kä¸ªæ ‡è®°çš„å…¨å±€ä¼°è®¡ï¼Œåˆå§‹åŒ–æ–°çš„ç‹¬ç‰¹æ ‡è®°åµŒå…¥ï¼Œä»è€Œåœ¨å‡å°‘é‡è®­ç»ƒéœ€æ±‚çš„åŒæ—¶ä¿æŒè¯­ä¹‰ã€‚å®éªŒè¯æ˜ï¼ŒTokenadaptåœ¨åˆå§‹åŒ–ç‹¬ç‰¹æ ‡è®°æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿å’Œå…¶ä»–å¤æ‚æ–¹æ³•ï¼Œä¸”è¶…çº§æ ‡è®°åœ¨å‹ç¼©æ•ˆæœä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10565', 'title': 'Depth Anything with Any Prior', 'url': 'https://huggingface.co/papers/2505.10565', 'abstract': 'This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene. To this end, we design a coarse-to-fine pipeline to progressively integrate the two complementary depth sources. First, we introduce pixel-level metric alignment and distance-aware weighting to pre-fill diverse metric priors by explicitly using depth prediction. It effectively narrows the domain gap between prior patterns, enhancing generalization across varying scenarios. Second, we develop a conditioned monocular depth estimation (MDE) model to refine the inherent noise of depth priors. By conditioning on the normalized pre-filled prior and prediction, the model further implicitly merges the two complementary depth sources. Our model showcases impressive zero-shot generalization across depth completion, super-resolution, and inpainting over 7 real-world datasets, matching or even surpassing previous task-specific methods. More importantly, it performs well on challenging, unseen mixed priors and enables test-time improvements by switching prediction models, providing a flexible accuracy-efficiency trade-off while evolving with advancements in MDE models.', 'score': 7, 'issue_id': 3794, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': '2291c0b9541ea999', 'authors': ['Zehan Wang', 'Siyu Chen', 'Lihe Yang', 'Jialei Wang', 'Ziang Zhang', 'Hengshuang Zhao', 'Zhou Zhao'], 'affiliations': ['The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10565.jpg', 'data': {'categories': ['#cv', '#dataset', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ»ÑĞ±Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Prior Depth Anything - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ»ÑĞ±Ñ‹Ñ… ÑÑ†ĞµĞ½. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½ÑƒÑ, Ğ½Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹, Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 7 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹.'}, 'en': {'title': 'Merging Depth Insights for Accurate Scene Mapping', 'desc': "This paper introduces Prior Depth Anything, a novel framework that effectively merges incomplete metric depth information with complete geometric structures to create accurate depth maps. It employs a coarse-to-fine approach that integrates these two sources of depth data, enhancing the model's ability to generalize across different scenarios. The framework utilizes pixel-level metric alignment and a conditioned monocular depth estimation model to refine depth predictions and reduce noise. The results demonstrate strong performance in various depth-related tasks, showcasing the model's adaptability and efficiency in real-world applications."}, 'zh': {'title': 'æ·±åº¦æµ‹é‡ä¸é¢„æµ‹çš„å®Œç¾ç»“åˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPrior Depth Anythingçš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†ä¸å®Œæ•´ä½†ç²¾ç¡®çš„æ·±åº¦æµ‹é‡ä¿¡æ¯ä¸ç›¸å¯¹ä½†å®Œæ•´çš„å‡ ä½•ç»“æ„ç»“åˆèµ·æ¥ï¼Œä»è€Œç”Ÿæˆå‡†ç¡®ã€å¯†é›†ä¸”è¯¦ç»†çš„æ·±åº¦å›¾ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç²—åˆ°ç»†çš„æµç¨‹ï¼Œé€æ­¥æ•´åˆè¿™ä¸¤ç§äº’è¡¥çš„æ·±åº¦æ¥æºã€‚é¦–å…ˆï¼Œé€šè¿‡å¼•å…¥åƒç´ çº§çš„åº¦é‡å¯¹é½å’Œè·ç¦»æ„ŸçŸ¥åŠ æƒï¼Œåˆ©ç”¨æ·±åº¦é¢„æµ‹æ¥é¢„å¡«å……å¤šæ ·çš„åº¦é‡å…ˆéªŒï¼Œæœ‰æ•ˆç¼©å°äº†å…ˆéªŒæ¨¡å¼ä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ¡ä»¶å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œä»¥è¿›ä¸€æ­¥ç²¾ç‚¼æ·±åº¦å…ˆéªŒçš„å›ºæœ‰å™ªå£°ï¼Œä»è€Œåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå±•ç¤ºäº†å‡ºè‰²çš„é›¶-shotæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.08617', 'title': 'OpenThinkIMG: Learning to Think with Images via Visual Tool\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.08617', 'abstract': 'While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely "think with images".', 'score': 6, 'issue_id': 3792, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ', 'en': 'May 13', 'zh': '5æœˆ13æ—¥'}, 'hash': '8cef19b2c7c9a459', 'authors': ['Zhaochen Su', 'Linjie Li', 'Mingyang Song', 'Yunzhuo Hao', 'Zhengyuan Yang', 'Jun Zhang', 'Guanjie Chen', 'Jiawei Gu', 'Juntao Li', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Fudan University', 'Huazhong University of Science and Technology', 'Microsoft', 'Shanghai Jiao Tong University', 'Soochow University', 'Sun Yat-sen University', 'The Chinese University of Hong Kong', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.08617.jpg', 'data': {'categories': ['#cv', '#rl', '#dataset', '#agents', '#training', '#open_source', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'OpenThinkIMG: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OpenThinkIMG - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ Ğ³Ğ¸Ğ±ĞºÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ V-ToolRL Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ V-ToolRL, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ¶Ğµ GPT-4.1.'}, 'en': {'title': 'Empowering AI to Think with Images through OpenThinkIMG', 'desc': 'This paper presents OpenThinkIMG, an innovative framework designed to enhance Large Vision-Language Models (LVLMs) by integrating visual tools for improved problem-solving. The framework addresses the challenges of standardization and data generation, allowing for better training of agents that can adaptively use visual tools. A key feature is the introduction of V-ToolRL, a reinforcement learning approach that enables LVLMs to learn optimal strategies for tool usage through direct feedback from their interactions. The results show that agents trained with V-ToolRL significantly outperform traditional supervised fine-tuning methods and even leading closed-source models, demonstrating the potential of dynamic tool-augmented visual reasoning.'}, 'zh': {'title': 'å¼€å¯è§†è§‰å·¥å…·å¢å¼ºçš„æ™ºèƒ½æ¨ç†æ–°æ—¶ä»£', 'desc': 'æœ¬æ–‡ä»‹ç»äº†OpenThinkIMGï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¼€æºçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä½¿ç”¨è§†è§‰å·¥å…·çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æä¾›äº†æ ‡å‡†åŒ–çš„è§†è§‰å·¥å…·æ¥å£å’Œçµæ´»çš„è®­ç»ƒç¯å¢ƒï¼Œä»¥ä¾¿æ›´å¥½åœ°é›†æˆå¤šç§å·¥å…·å¹¶ç”Ÿæˆä¸°å¯Œçš„äº¤äº’æ•°æ®ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿç›‘ç£å¾®è°ƒæ–¹æ³•çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶V-ToolRLï¼Œä½¿LVLMsèƒ½å¤Ÿè‡ªä¸»å­¦ä¹ åŠ¨æ€å·¥å…·è°ƒç”¨çš„é€‚åº”æ€§ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨V-ToolRLè®­ç»ƒçš„ä»£ç†åœ¨å¤æ‚çš„å›¾è¡¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†å…¶ä»–åŸºçº¿æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.08581', 'title': 'ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible\n  Long-term Tracking', 'url': 'https://huggingface.co/papers/2505.08581', 'abstract': 'Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However, existing methods are limited by low efficiency and short-term tracking, hindering their applicability in complex real-world surgical scenarios. In this paper, we introduce ReSurgSAM2, a two-stage surgical referring segmentation framework that leverages Segment Anything Model 2 to perform text-referred target detection, followed by tracking with reliable initial frame identification and diversity-driven long-term memory. For the detection stage, we propose a cross-modal spatial-temporal Mamba to generate precise detection and segmentation results. Based on these results, our credible initial frame selection strategy identifies the reliable frame for the subsequent tracking. Upon selecting the initial frame, our method transitions to the tracking stage, where it incorporates a diversity-driven memory mechanism that maintains a credible and diverse memory bank, ensuring consistent long-term tracking. Extensive experiments demonstrate that ReSurgSAM2 achieves substantial improvements in accuracy and efficiency compared to existing methods, operating in real-time at 61.2 FPS. Our code and datasets will be available at https://github.com/jinlab-imvr/ReSurgSAM2.', 'score': 6, 'issue_id': 3793, 'pub_date': '2025-05-13', 'pub_date_card': {'ru': '13 Ğ¼Ğ°Ñ', 'en': 'May 13', 'zh': '5æœˆ13æ—¥'}, 'hash': '3dab22f0e497d468', 'authors': ['Haofeng Liu', 'Mingqi Gao', 'Xuxiao Luo', 'Ziyue Wang', 'Guanyi Qin', 'Junde Wu', 'Yueming Jin'], 'affiliations': ['National University of Singapore, Singapore, Singapore', 'Southern University of Science and Technology, Shenzhen, China', 'University of Oxford, Oxford, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2505.08581.jpg', 'data': {'categories': ['#cv', '#healthcare', '#multimodal'], 'emoji': 'ğŸ”ª', 'ru': {'title': 'ReSurgSAM2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ReSurgSAM2 - Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SAM2 Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mamba Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ½ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ ÑĞ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ 61.2 ĞºĞ°Ğ´Ñ€Ğ° Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ.'}, 'en': {'title': 'Revolutionizing Surgical Segmentation with ReSurgSAM2', 'desc': 'This paper presents ReSurgSAM2, a novel framework for surgical scene segmentation that enhances the interactive experience for surgeons. It consists of two main stages: the first stage focuses on text-referred target detection using the Segment Anything Model 2, while the second stage emphasizes reliable tracking through an innovative initial frame selection and a diversity-driven memory mechanism. The proposed method addresses the limitations of existing techniques by improving efficiency and enabling long-term tracking in complex surgical environments. Experimental results show that ReSurgSAM2 significantly outperforms previous methods, achieving real-time performance at 61.2 frames per second.'}, 'zh': {'title': 'æå‡æ‰‹æœ¯åˆ†å‰²æ•ˆç‡ä¸å‡†ç¡®æ€§çš„ReSurgSAM2æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºReSurgSAM2çš„æ‰‹æœ¯åœºæ™¯åˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„æ‰‹æœ¯å‚è€ƒåˆ†å‰²æ–¹æ³•ï¼Œåˆ©ç”¨Segment Anything Model 2è¿›è¡Œæ–‡æœ¬å¼•ç”¨ç›®æ ‡æ£€æµ‹ï¼Œå¹¶é€šè¿‡å¯é çš„åˆå§‹å¸§è¯†åˆ«å’Œå¤šæ ·æ€§é©±åŠ¨çš„é•¿æœŸè®°å¿†è¿›è¡Œè·Ÿè¸ªã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æ—¶ç©ºMambaï¼Œç”¨äºç”Ÿæˆç²¾ç¡®çš„æ£€æµ‹å’Œåˆ†å‰²ç»“æœï¼Œå¹¶é€šè¿‡å¯ä¿¡çš„åˆå§‹å¸§é€‰æ‹©ç­–ç•¥æ¥ç¡®ä¿åç»­è·Ÿè¸ªçš„å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReSurgSAM2åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿä»¥61.2 FPSçš„å®æ—¶é€Ÿåº¦è¿è¡Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10167', 'title': 'QuXAI: Explainers for Hybrid Quantum Machine Learning Models', 'url': 'https://huggingface.co/papers/2505.10167', 'abstract': 'The emergence of hybrid quantum-classical machine learning (HQML) models opens new horizons of computational intelligence but their fundamental complexity frequently leads to black box behavior that undermines transparency and reliability in their application. Although XAI for quantum systems still in its infancy, a major research gap is evident in robust global and local explainability approaches that are designed for HQML architectures that employ quantized feature encoding followed by classical learning. The gap is the focus of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an explainer for explaining feature importance in these hybrid systems. Our model entails the creation of HQML models incorporating quantum feature maps, the use of Q-MEDLEY, which combines feature based inferences, preserving the quantum transformation stage and visualizing the resulting attributions. Our result shows that Q-MEDLEY delineates influential classical aspects in HQML models, as well as separates their noise, and competes well against established XAI techniques in classical validation settings. Ablation studies more significantly expose the virtues of the composite structure used in Q-MEDLEY. The implications of this work are critically important, as it provides a route to improve the interpretability and reliability of HQML models, thus promoting greater confidence and being able to engage in safer and more responsible use of quantum-enhanced AI technology.', 'score': 5, 'issue_id': 3801, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': '924a78566cebf728', 'authors': ['Saikat Barua', 'Mostafizur Rahman', 'Shehenaz Khaled', 'Md Jafor Sadek', 'Rafiul Islam', 'Shahnewaz Siddique'], 'affiliations': ['North South University, Dhaka'], 'pdf_title_img': 'assets/pdf/title_img/2505.10167.jpg', 'data': {'categories': ['#data', '#inference', '#interpretability', '#architecture'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'QuXAI: ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾-ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ QuXAI - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾-ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (HQML). Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ QuXAI Ğ»ĞµĞ¶Ğ¸Ñ‚ Q-MEDLEY - Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ñ‚ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ñ‚Ğ°ĞºĞ¸Ñ… Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ HQML Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Q-MEDLEY Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Q-MEDLEY ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ»Ğ¸ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ² HQML Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ XAI.'}, 'en': {'title': 'Unlocking the Black Box of Hybrid Quantum-Classical AI', 'desc': 'This paper addresses the challenge of understanding hybrid quantum-classical machine learning (HQML) models, which often behave like black boxes. It introduces QuXAI, a framework that enhances explainability in HQML by utilizing Q-MEDLEY, an explainer that assesses feature importance while maintaining the quantum transformation process. The study demonstrates that Q-MEDLEY effectively identifies key classical features and reduces noise in HQML models, outperforming traditional explainable AI (XAI) methods in classical validation tests. Ultimately, this work aims to improve the transparency and reliability of HQML systems, fostering safer and more responsible applications of quantum-enhanced AI.'}, 'zh': {'title': 'æå‡æ··åˆé‡å­-ç»å…¸æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸å¯é æ€§', 'desc': 'æ··åˆé‡å­-ç»å…¸æœºå™¨å­¦ä¹ ï¼ˆHQMLï¼‰æ¨¡å‹çš„å‡ºç°ä¸ºè®¡ç®—æ™ºèƒ½å¼€è¾Ÿäº†æ–°è§†é‡ï¼Œä½†å…¶å¤æ‚æ€§å¸¸å¯¼è‡´é»‘ç®±è¡Œä¸ºï¼Œå½±å“é€æ˜æ€§å’Œå¯é æ€§ã€‚æœ¬æ–‡æå‡ºäº†QuXAIæ¡†æ¶ï¼Œæ—¨åœ¨å¡«è¡¥é’ˆå¯¹HQMLæ¶æ„çš„å¯è§£é‡Šæ€§ç ”ç©¶ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯ç»“åˆé‡å­ç‰¹å¾ç¼–ç å’Œç»å…¸å­¦ä¹ çš„æ¨¡å‹ã€‚QuXAIåŸºäºQ-MEDLEYï¼Œèƒ½å¤Ÿè§£é‡Šç‰¹å¾é‡è¦æ€§ï¼Œå¹¶é€šè¿‡å¯è§†åŒ–ç»“æœå½’å› æ¥æ­ç¤ºHQMLæ¨¡å‹ä¸­çš„ç»å…¸å½±å“å› ç´ ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒQ-MEDLEYåœ¨ç»å…¸éªŒè¯ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œæå‡äº†HQMLæ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯é æ€§ï¼Œä¿ƒè¿›äº†é‡å­å¢å¼ºäººå·¥æ™ºèƒ½æŠ€æœ¯çš„å®‰å…¨ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10566', 'title': '3D-Fixup: Advancing Photo Editing with 3D Priors', 'url': 'https://huggingface.co/papers/2505.10566', 'abstract': 'Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. The framework supports difficult editing situations such as object translation and 3D rotation. To achieve this, we leverage a training-based approach that harnesses the generative power of diffusion models. As video data naturally encodes real-world physical dynamics, we turn to video data for generating training data pairs, i.e., a source and a target frame. Rather than relying solely on a single trained model to infer transformations between source and target frames, we incorporate 3D guidance from an Image-to-3D model, which bridges this challenging task by explicitly projecting 2D information into 3D space. We design a data generation pipeline to ensure high-quality 3D guidance throughout training. Results show that by integrating these 3D priors, 3D-Fixup effectively supports complex, identity coherent 3D-aware edits, achieving high-quality results and advancing the application of diffusion models in realistic image manipulation. The code is provided at https://3dfixup.github.io/', 'score': 4, 'issue_id': 3808, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': 'f1dc934cd3d0961a', 'authors': ['Yen-Chi Cheng', 'Krishna Kumar Singh', 'Jae Shin Yoon', 'Alex Schwing', 'Liangyan Gui', 'Matheus Gadelha', 'Paul Guerrero', 'Nanxuan Zhao'], 'affiliations': ['Adobe Research, UK', 'Adobe Research, USA', 'University of Illinois Urbana-Champaign, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.10566.jpg', 'data': {'categories': ['#cv', '#diffusion', '#3d', '#video', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': '3D-Fixup: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ 3D-Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ğ¼Ğ¸', 'desc': '3D-Fixup - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ². ĞĞ½ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ 3D-Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ€ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 3D-Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Image-to-3D. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 3D-Fixup ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing 2D Image Editing with 3D Insights', 'desc': 'The paper introduces 3D-Fixup, a novel framework designed for 2D image editing using learned 3D priors. It addresses the challenges of editing images that require understanding of 3D transformations, such as object translation and rotation. By utilizing video data to create training pairs, the framework enhances the generative capabilities of diffusion models. The integration of an Image-to-3D model allows for effective projection of 2D information into 3D space, resulting in high-quality, coherent edits that push the boundaries of image manipulation.'}, 'zh': {'title': '3D-Fixupï¼šå®ç°é«˜è´¨é‡çš„3Dæ„ŸçŸ¥å›¾åƒç¼–è¾‘', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º3D-Fixupçš„æ–°æ¡†æ¶ï¼Œç”¨äºé€šè¿‡å­¦ä¹ çš„3Då…ˆéªŒæ¥ç¼–è¾‘2Då›¾åƒã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†å¤æ‚çš„ç¼–è¾‘ä»»åŠ¡ï¼Œå¦‚ç‰©ä½“å¹³ç§»å’Œ3Dæ—‹è½¬ã€‚æˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡è§†é¢‘æ•°æ®ç”Ÿæˆè®­ç»ƒæ•°æ®å¯¹ï¼Œä»¥æé«˜ç¼–è¾‘æ•ˆæœã€‚é€šè¿‡ç»“åˆå›¾åƒåˆ°3Dæ¨¡å‹çš„3DæŒ‡å¯¼ï¼Œ3D-Fixupèƒ½å¤Ÿåœ¨2Då’Œ3Dç©ºé—´ä¹‹é—´è¿›è¡Œæœ‰æ•ˆçš„è½¬æ¢ï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„å›¾åƒç¼–è¾‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10046', 'title': 'Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis', 'url': 'https://huggingface.co/papers/2505.10046', 'abstract': 'This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis -- specifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-modal generation. Previous studies mainly focused on overall system performance rather than detailed comparisons with alternative methods, and key design details and training recipes were often left undisclosed. These gaps create uncertainty about the real potential of this approach. To fill these gaps, we conduct an empirical study on text-to-image generation, performing controlled comparisons with established baselines, analyzing important design choices, and providing a clear, reproducible recipe for training at scale. We hope this work offers meaningful data points and practical guidelines for future research in multi-modal generation.', 'score': 4, 'issue_id': 3792, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': 'e297e154b434ef38', 'authors': ['Bingda Tang', 'Boyang Zheng', 'Xichen Pan', 'Sayak Paul', 'Saining Xie'], 'affiliations': ['Hugging Face', 'New York University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10046.jpg', 'data': {'categories': ['#multimodal', '#training', '#diffusion', '#survey'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ LLM Ğ¸ DiT Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT) Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‡ĞµÑ‚ĞºĞ¸Ğ¹ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ.'}, 'en': {'title': 'Unlocking the Potential of Multi-Modal Generation', 'desc': 'This paper investigates the integration of large language models (LLMs) and diffusion transformers (DiTs) in the field of text-to-image synthesis. It highlights the lack of detailed comparisons and transparency in previous studies, which often overlooked critical design choices and training methodologies. By conducting empirical studies and controlled comparisons with established baselines, the authors aim to clarify the potential of this multi-modal generation approach. The paper provides a reproducible training recipe and valuable insights to guide future research in this area.'}, 'zh': {'title': 'æ¢ç´¢æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„æ–°è®¾è®¡ç©ºé—´', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­çš„ä¸€ä¸ªé‡è¦è®¾è®¡ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰æ·±åº¦èåˆçš„å¤šæ¨¡æ€ç”Ÿæˆã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨ç³»ç»Ÿæ•´ä½“æ€§èƒ½ï¼Œè€Œç¼ºä¹å¯¹æ›¿ä»£æ–¹æ³•çš„è¯¦ç»†æ¯”è¾ƒï¼Œå…³é”®è®¾è®¡ç»†èŠ‚å’Œè®­ç»ƒæ–¹æ¡ˆå¾€å¾€æœªè¢«æŠ«éœ²ã€‚è¿™äº›ç©ºç™½å¯¼è‡´äº†å¯¹è¯¥æ–¹æ³•çœŸå®æ½œåŠ›çš„æ€€ç–‘ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å®è¯ç ”ç©¶ï¼Œè¿›è¡Œæ§åˆ¶æ¯”è¾ƒï¼Œåˆ†æé‡è¦è®¾è®¡é€‰æ‹©ï¼Œå¹¶æä¾›æ¸…æ™°ã€å¯é‡å¤çš„è®­ç»ƒæ–¹æ¡ˆï¼Œä»¥æœŸä¸ºæœªæ¥çš„å¤šæ¨¡æ€ç”Ÿæˆç ”ç©¶æä¾›æœ‰æ„ä¹‰çš„æ•°æ®å’Œå®ç”¨æŒ‡å—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.09926', 'title': 'AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection', 'url': 'https://huggingface.co/papers/2505.09926', 'abstract': 'Universal visual anomaly detection aims to identify anomalies from novel or unseen vision domains without additional fine-tuning, which is critical in open scenarios. Recent studies have demonstrated that pre-trained vision-language models like CLIP exhibit strong generalization with just zero or a few normal images. However, existing methods struggle with designing prompt templates, complex token interactions, or requiring additional fine-tuning, resulting in limited flexibility. In this work, we present a simple yet effective method called AdaptCLIP based on two key insights. First, adaptive visual and textual representations should be learned alternately rather than jointly. Second, comparative learning between query and normal image prompt should incorporate both contextual and aligned residual features, rather than relying solely on residual features. AdaptCLIP treats CLIP models as a foundational service, adding only three simple adapters, visual adapter, textual adapter, and prompt-query adapter, at its input or output ends. AdaptCLIP supports zero-/few-shot generalization across domains and possesses a training-free manner on target domains once trained on a base dataset. AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks from industrial and medical domains, significantly outperforming existing competitive methods. We will make the code and model of AdaptCLIP available at https://github.com/gaobb/AdaptCLIP.', 'score': 4, 'issue_id': 3791, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': '25c3eaaddbc9d5ca', 'authors': ['Bin-Bin Gao', 'Yue Zhu', 'Jiangtao Yan', 'Yuezhi Cai', 'Weixi Zhang', 'Meng Wang', 'Jun Liu', 'Yong Liu', 'Lei Wang', 'Chengjie Wang'], 'affiliations': ['Shanghai Jiao Tong University', 'Siemens Corporate Research', 'Technical University of Munich', 'Tencent YouTu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.09926.jpg', 'data': {'categories': ['#cv', '#benchmark', '#open_source', '#transfer_learning', '#healthcare'], 'emoji': 'ğŸ”', 'ru': {'title': 'AdaptCLIP: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ CLIP', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ AdaptCLIP. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞµĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. AdaptCLIP Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 12 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ….'}, 'en': {'title': 'AdaptCLIP: Simplifying Anomaly Detection with Adaptive Learning', 'desc': 'This paper introduces AdaptCLIP, a novel approach for universal visual anomaly detection that operates without the need for additional fine-tuning. It leverages pre-trained vision-language models like CLIP to generalize effectively from just a few normal images. The method emphasizes learning adaptive visual and textual representations alternately and incorporates comparative learning that utilizes both contextual and aligned residual features. AdaptCLIP demonstrates superior performance on various anomaly detection benchmarks, showcasing its flexibility and efficiency in handling unseen vision domains.'}, 'zh': {'title': 'AdaptCLIPï¼šæ— å¾®è°ƒçš„é€šç”¨è§†è§‰å¼‚å¸¸æ£€æµ‹', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAdaptCLIPçš„é€šç”¨è§†è§‰å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨æ— éœ€é¢å¤–å¾®è°ƒå³å¯è¯†åˆ«æ–°é¢–æˆ–æœªè§çš„è§†è§‰é¢†åŸŸä¸­çš„å¼‚å¸¸ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹å¦‚CLIPåœ¨ä»…ä½¿ç”¨é›¶æˆ–å°‘é‡æ­£å¸¸å›¾åƒæ—¶å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚AdaptCLIPé€šè¿‡äº¤æ›¿å­¦ä¹ è‡ªé€‚åº”çš„è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºï¼Œç»“åˆä¸Šä¸‹æ–‡å’Œå¯¹é½çš„æ®‹å·®ç‰¹å¾è¿›è¡Œæ¯”è¾ƒå­¦ä¹ ï¼Œä»è€Œæé«˜äº†çµæ´»æ€§ã€‚è¯¥æ–¹æ³•åœ¨12ä¸ªå·¥ä¸šå’ŒåŒ»ç–—é¢†åŸŸçš„å¼‚å¸¸æ£€æµ‹åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„ç«äº‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.09265', 'title': 'MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning', 'url': 'https://huggingface.co/papers/2505.09265', 'abstract': 'Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation. We present a novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets. We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world. To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation. This is the first work to achieve universal anomaly segmentation using a pure vision model without relying on special anomaly detection datasets and pre-trained visual-language models. Our method effectively and efficiently segments any anomalies with only one normal image prompt and enjoys training-free without guidance from language. Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods. The code and pre-trained models are available at https://github.com/gaobb/MetaUAS.', 'score': 3, 'issue_id': 3791, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ', 'en': 'May 14', 'zh': '5æœˆ14æ—¥'}, 'hash': '62287f3647d6b6a7', 'authors': ['Bin-Bin Gao'], 'affiliations': ['Tencent YouTu Lab, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.09265.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#synthetic', '#dataset'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ±ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº MetaUAS, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ zero-shot, few-shot Ğ¸ full-shot ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Anomaly Segmentation with Pure Vision Models', 'desc': 'This paper introduces a new approach for visual anomaly segmentation that does not depend on language, using a pure visual foundation model instead of traditional vision-language models. The authors propose a unified framework that treats anomaly segmentation as a form of change segmentation, allowing the use of synthetic image pairs to train the model. They develop a Meta-learning framework called MetaUAS, which can effectively identify unseen anomalies with just one normal image prompt. The method includes a soft feature alignment module to address geometric differences, achieving superior performance compared to existing anomaly segmentation techniques.'}, 'zh': {'title': 'çº¯è§†è§‰æ¨¡å‹å®ç°é€šç”¨å¼‚å¸¸åˆ†å‰²', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„è§†è§‰å¼‚å¸¸åˆ†å‰²æ–¹æ³•ï¼Œç§°ä¸ºMetaUASï¼Œæ—¨åœ¨ä½¿ç”¨çº¯è§†è§‰æ¨¡å‹è€Œéä¼ ç»Ÿçš„è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¼‚å¸¸åˆ†å‰²ç»Ÿä¸€ä¸ºå˜åŒ–åˆ†å‰²ï¼Œåˆ©ç”¨å¤§è§„æ¨¡åˆæˆå›¾åƒå¯¹è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå®ç°å¯¹æœªçŸ¥å¼‚å¸¸çš„æœ‰æ•ˆåˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºçš„è½¯ç‰¹å¾å¯¹é½æ¨¡å—èƒ½å¤Ÿå¤„ç†æç¤ºå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„å‡ ä½•å˜åŒ–ï¼Œæå‡äº†åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚MetaUASåœ¨é›¶-shotã€å°‘-shotå’Œå…¨-shotå¼‚å¸¸åˆ†å‰²æ–¹æ³•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.09264', 'title': 'Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt', 'url': 'https://huggingface.co/papers/2505.09264', 'abstract': 'Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect reconstruction for both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Additionally, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space. To enable reconstruction models enjoying high efficiency while enhancing their generalization for unified anomaly detection, we propose a simple yet effective method that reconstructs normal features and restores anomaly features with just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP allows for the first time to reconstruct or restore anomalies with just one normal image prompt, effectively boosting unified anomaly detection performance. Furthermore, we propose a supervised refiner that regresses reconstruction errors by using both real normal and synthesized anomalous images, which significantly improves pixel-level anomaly segmentation. OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA. The code and pre-trained models are available at https://github.com/gaobb/OneNIP.', 'score': 3, 'issue_id': 3791, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ', 'en': 'May 14', 'zh': '5æœˆ14æ—¥'}, 'hash': 'd89f10b01e706c42', 'authors': ['Bin-Bin Gao'], 'affiliations': ['Tencent YouTu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.09264.jpg', 'data': {'categories': ['#cv', '#benchmark', '#optimization', '#survey', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'OneNIP: ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ OneNIP Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², OneNIP ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ñ‹Ğ¹ ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ OneNIP Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Anomaly Detection with One Normal Image Prompt', 'desc': 'This paper introduces a novel method called One Normal Image Prompt (OneNIP) for improving anomaly detection using self-attention transformers. Traditional models struggle with accurately identifying anomalies because they can reconstruct both normal and anomalous features too well, leading to confusion. OneNIP addresses this by allowing the model to reconstruct normal features while effectively restoring anomalies using just one normal image as a reference. Additionally, a supervised refiner is proposed to enhance pixel-level segmentation of anomalies, resulting in superior performance on multiple industry benchmarks.'}, 'zh': {'title': 'ç”¨ä¸€ä¸ªæ­£å¸¸å›¾åƒæç¤ºæå‡å¼‚å¸¸æ£€æµ‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— ç›‘ç£é‡å»ºç½‘ç»œæ–¹æ³•ï¼Œç§°ä¸ºOne Normal Image Promptï¼ˆOneNIPï¼‰ï¼Œç”¨äºå¤šç±»å¼‚å¸¸æ£€æµ‹ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒOneNIPåªéœ€ä¸€ä¸ªæ­£å¸¸å›¾åƒæç¤ºå³å¯é‡å»ºæˆ–æ¢å¤å¼‚å¸¸ç‰¹å¾ï¼Œä»è€Œæé«˜äº†å¼‚å¸¸æ£€æµ‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ç›‘ç£ç²¾ç‚¼å™¨ï¼Œåˆ©ç”¨çœŸå®æ­£å¸¸å›¾åƒå’Œåˆæˆå¼‚å¸¸å›¾åƒæ¥å›å½’é‡å»ºè¯¯å·®ï¼Œæ˜¾è‘—æ”¹å–„äº†åƒç´ çº§å¼‚å¸¸åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOneNIPåœ¨å¤šä¸ªè¡Œä¸šå¼‚å¸¸æ£€æµ‹åŸºå‡†ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.09263', 'title': 'Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation', 'url': 'https://huggingface.co/papers/2505.09263', 'abstract': 'Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.', 'score': 3, 'issue_id': 3791, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ', 'en': 'May 14', 'zh': '5æœˆ14æ—¥'}, 'hash': '1cde18cecb1ca918', 'authors': ['Guan Gui', 'Bin-Bin Gao', 'Jun Liu', 'Chengjie Wang', 'Yunsheng Wu'], 'affiliations': ['Shanghai Jiao Tong University', 'Tencent YouTu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.09263.jpg', 'data': {'categories': ['#data', '#cv', '#training', '#benchmark', '#dataset', '#synthetic', '#diffusion'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ AnoGen Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½ÑĞ¿ĞµĞºÑ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². AnoGen Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ»Ğ°Ğ±Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MVTec Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ DRAEM Ğ¸ DesTSeg Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹.'}, 'en': {'title': 'Generating Realistic Anomalies for Better Detection', 'desc': 'This paper presents a novel method called Few-shot Anomaly-driven Generation (AnoGen) for improving anomaly detection in industrial settings. The approach addresses the challenge of limited real anomaly samples by using a diffusion model to generate realistic anomalies based on a few existing examples. It consists of three stages: learning the anomaly distribution, guiding the generation of diverse anomalies, and training a weakly-supervised detection model. The results show significant improvements in both anomaly classification and segmentation tasks, demonstrating the effectiveness of the generated anomalies in enhancing model performance.'}, 'zh': {'title': 'å°‘æ ·æœ¬é©±åŠ¨çš„å¼‚å¸¸ç”Ÿæˆï¼Œæå‡æ£€æµ‹æ€§èƒ½ï¼', 'desc': 'å¼‚å¸¸æ£€æµ‹æ˜¯ä¸€é¡¹å®é™…ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå·¥ä¸šæ£€æµ‹ä¸­å¼‚å¸¸æ ·æœ¬ç¨€ç¼ºã€‚ç°æœ‰çš„ä¸€äº›å¼‚å¸¸æ£€æµ‹æ–¹æ³•é€šè¿‡åˆæˆå™ªå£°æˆ–å¤–éƒ¨æ•°æ®æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†åˆæˆå¼‚å¸¸ä¸çœŸå®å¼‚å¸¸ä¹‹é—´å­˜åœ¨è¾ƒå¤§çš„è¯­ä¹‰å·®è·ï¼Œå¯¼è‡´æ£€æµ‹æ€§èƒ½è¾ƒå¼±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°‘é‡æ ·æœ¬é©±åŠ¨çš„å¼‚å¸¸ç”Ÿæˆæ–¹æ³•ï¼ˆAnoGenï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å°‘é‡çœŸå®å¼‚å¸¸æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”ŸæˆçœŸå®ä¸”å¤šæ ·çš„å¼‚å¸¸ï¼Œä»è€Œæœ‰åˆ©äºè®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç”Ÿæˆçš„å¼‚å¸¸æœ‰æ•ˆæé«˜äº†æ¨¡å‹åœ¨å¼‚å¸¸åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10468', 'title': 'AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and\n  Challenge', 'url': 'https://huggingface.co/papers/2505.10468', 'abstract': 'This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven by Large Language Models (LLMs) and Large Image Models (LIMs) for narrow, task-specific automation. Generative AI is positioned as a precursor, with AI Agents advancing through tool integration, prompt engineering, and reasoning enhancements. In contrast, Agentic AI systems represent a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Through a sequential evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both paradigms. Application domains such as customer support, scheduling, and data summarization are contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and propose targeted solutions such as ReAct loops, RAG, orchestration layers, and causal modeling. This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision Support System, Agentic-AI Applications', 'score': 2, 'issue_id': 3802, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': '613808a0b3d41836', 'authors': ['Ranjan Sapkota', 'Konstantinos I. Roumeliotis', 'Manoj Karkee'], 'affiliations': ['Cornell University, Department of Environmental and Biological Engineering, USA', 'Department of Informatics and Telecommunications, University of the Peloponnese, 22131 Tripoli, Greece'], 'pdf_title_img': 'assets/pdf/title_img/2505.10468.jpg', 'data': {'categories': ['#healthcare', '#hallucinations', '#agents', '#architecture', '#multimodal', '#reasoning', '#optimization', '#agi'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜: ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ˜Ğ˜, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, ĞºĞ°Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ². Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‚ÑÑ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (LIM) Ğ´Ğ»Ñ ÑƒĞ·ĞºĞ¾Ğ¹, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ´Ğ²Ğ¸Ğ³, Ğ¾Ñ‚Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ¸ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ñ…Ñ€ÑƒĞ¿ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Understanding AI Agents vs. Agentic AI: A New Frontier in AI Development', 'desc': 'This paper differentiates between AI Agents and Agentic AI, providing a clear framework for understanding their unique characteristics and applications. AI Agents are defined as modular systems that utilize Large Language Models (LLMs) and Large Image Models (LIMs) for specific tasks, while Agentic AI represents a more advanced form that incorporates multi-agent collaboration and dynamic task management. The study evaluates the evolution of these systems, highlighting their operational differences and the challenges they face, such as hallucination and coordination failure. It also suggests solutions like ReAct loops and orchestration layers to enhance the effectiveness of both AI paradigms.'}, 'zh': {'title': 'åŒºåˆ†äººå·¥æ™ºèƒ½ä»£ç†ä¸ä»£ç†æ™ºèƒ½çš„æœªæ¥ä¹‹è·¯', 'desc': 'æœ¬ç ”ç©¶æ˜ç¡®åŒºåˆ†äº†äººå·¥æ™ºèƒ½ä»£ç†å’Œä»£ç†æ™ºèƒ½ï¼Œæä¾›äº†ç»“æ„åŒ–çš„æ¦‚å¿µåˆ†ç±»ã€åº”ç”¨æ˜ å°„å’ŒæŒ‘æˆ˜åˆ†æï¼Œä»¥é˜æ˜å®ƒä»¬ä¸åŒçš„è®¾è®¡ç†å¿µå’Œèƒ½åŠ›ã€‚æˆ‘ä»¬å°†äººå·¥æ™ºèƒ½ä»£ç†å®šä¹‰ä¸ºç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹å›¾åƒæ¨¡å‹ï¼ˆLIMsï¼‰é©±åŠ¨çš„æ¨¡å—åŒ–ç³»ç»Ÿï¼Œä¸»è¦ç”¨äºç‰¹å®šä»»åŠ¡çš„è‡ªåŠ¨åŒ–ã€‚ä»£ç†æ™ºèƒ½ç³»ç»Ÿåˆ™ä»£è¡¨äº†ä¸€ç§èŒƒå¼è½¬å˜ï¼Œå¼ºè°ƒå¤šä»£ç†åä½œã€åŠ¨æ€ä»»åŠ¡åˆ†è§£ã€æŒä¹…è®°å¿†å’Œè‡ªä¸»åè°ƒã€‚é€šè¿‡å¯¹ä¸¤ç§èŒƒå¼çš„æ¶æ„æ¼”å˜ã€æ“ä½œæœºåˆ¶ã€äº¤äº’é£æ ¼å’Œè‡ªä¸»æ°´å¹³çš„æ¯”è¾ƒåˆ†æï¼Œæˆ‘ä»¬ä¸ºå¼€å‘å¼ºå¤§ã€å¯æ‰©å±•å’Œå¯è§£é‡Šçš„äººå·¥æ™ºèƒ½ä»£ç†å’Œä»£ç†æ™ºèƒ½é©±åŠ¨ç³»ç»Ÿæä¾›äº†æ˜ç¡®çš„è·¯çº¿å›¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.09601', 'title': 'Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or\n  Robot Hardware', 'url': 'https://huggingface.co/papers/2505.09601', 'abstract': 'Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com', 'score': 1, 'issue_id': 3811, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ', 'en': 'May 14', 'zh': '5æœˆ14æ—¥'}, 'hash': '65500a5b9d477d08', 'authors': ['Justin Yu', 'Letian Fu', 'Huang Huang', 'Karim El-Refai', 'Rares Andrei Ambrus', 'Richard Cheng', 'Muhammad Zubair Irshad', 'Ken Goldberg'], 'affiliations': ['Toyota Research Institute', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.09601.jpg', 'data': {'categories': ['#optimization', '#dataset', '#synthetic', '#3d', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹', 'desc': 'R2R2R - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞ»ĞµÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½ Ğ´Ğ»Ñ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹, Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. R2R2R Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ 3D Gaussian Splatting Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑÑĞµÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¶ĞµÑÑ‚ĞºĞ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ÑˆĞ°Ñ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… R2R2R Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° 150 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ… Ñ Ñ‚ĞµĞ»ĞµÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Revolutionizing Robot Training with R2R2R', 'desc': 'The paper presents Real2Render2Real (R2R2R), a new method for generating training data for robots without the need for expensive human teleoperation or complex simulations. It utilizes smartphone scans and a single video of human actions to create thousands of high-quality, robot-agnostic demonstrations. By employing 3D Gaussian Splatting, R2R2R can flexibly generate 3D models and trajectories for various objects, which are then converted into meshes for use in rendering engines. Experiments show that models trained on data from R2R2R can achieve performance comparable to those trained on a much larger dataset of human demonstrations.'}, 'zh': {'title': 'æœºå™¨äººå­¦ä¹ çš„æ–°æ–¹æ³•ï¼šæ— éœ€é¥æ§å³å¯ç”Ÿæˆè®­ç»ƒæ•°æ®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºReal2Render2Realï¼ˆR2R2Rï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆæœºå™¨äººè®­ç»ƒæ•°æ®ï¼Œè€Œæ— éœ€ä¾èµ–ç‰©ä½“åŠ¨åŠ›å­¦æ¨¡æ‹Ÿæˆ–äººå·¥é¥æ§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ™ºèƒ½æ‰‹æœºæ•æ‰çš„ç‰©ä½“æ‰«æå’Œäººç±»æ¼”ç¤ºè§†é¢‘ï¼Œé‡å»ºè¯¦ç»†çš„3Dç‰©ä½“å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚ï¼Œç”Ÿæˆé«˜è§†è§‰ä¿çœŸåº¦çš„æœºå™¨äººæ¼”ç¤ºã€‚R2R2Råˆ©ç”¨3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰æŠ€æœ¯ï¼Œçµæ´»ç”Ÿæˆèµ„äº§å’Œè½¨è¿¹ï¼Œé€‚ç”¨äºåˆšæ€§å’Œå…³èŠ‚ç‰©ä½“ï¼Œå¹¶å°†è¿™äº›è¡¨ç¤ºè½¬æ¢ä¸ºç½‘æ ¼ï¼Œä»¥ä¾¿ä¸å¯æ‰©å±•çš„æ¸²æŸ“å¼•æ“å…¼å®¹ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºR2R2Ræ•°æ®è®­ç»ƒçš„æ¨¡å‹å¯ä»¥ä¸åŸºäº150ä¸ªäººå·¥é¥æ§æ¼”ç¤ºè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ç›¸åŒ¹é…ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07096', 'title': 'X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real', 'url': 'https://huggingface.co/papers/2505.07096', 'abstract': 'Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/.', 'score': 1, 'issue_id': 3806, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 Ğ¼Ğ°Ñ', 'en': 'May 11', 'zh': '5æœˆ11æ—¥'}, 'hash': 'cb9029d3f1641bd2', 'authors': ['Prithwish Dan', 'Kushal Kedia', 'Angela Chao', 'Edward Weiyi Duan', 'Maximus Adrian Pace', 'Wei-Chiu Ma', 'Sanjiban Choudhury'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07096.jpg', 'data': {'categories': ['#synthetic', '#rl', '#training', '#robotics', '#optimization', '#transfer_learning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ñƒ Ğ»ÑĞ´ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'X-Sim - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑÑ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°. X-Sim Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 30% Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞ»ĞµĞ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°.'}, 'en': {'title': 'X-Sim: Bridging Human Videos to Robot Actions Efficiently', 'desc': 'The paper introduces X-Sim, a framework designed to train robot manipulation policies using human videos without needing action labels. It reconstructs a realistic simulation from RGBD videos and tracks object movements to create rewards for reinforcement learning. The learned policy is then refined into a diffusion policy that adapts to real-world conditions through online domain adaptation. X-Sim demonstrates significant improvements in task performance and efficiency compared to existing methods, showcasing its ability to generalize across different environments and viewpoints.'}, 'zh': {'title': 'X-Simï¼šä»äººç±»è§†é¢‘åˆ°æœºå™¨äººæ“ä½œçš„æ™ºèƒ½è½¬ç§»', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºX-Simçš„æ¡†æ¶ï¼Œç”¨äºé€šè¿‡äººç±»è§†é¢‘è®­ç»ƒæœºå™¨äººæ“ä½œç­–ç•¥ã€‚X-Simåˆ©ç”¨ç‰©ä½“è¿åŠ¨ä½œä¸ºå¯†é›†ä¸”å¯è½¬ç§»çš„ä¿¡å·ï¼Œé¦–å…ˆä»RGBDäººç±»è§†é¢‘é‡å»ºå‡ºé€¼çœŸçš„ä»¿çœŸç¯å¢ƒï¼Œå¹¶è·Ÿè¸ªç‰©ä½“è½¨è¿¹ä»¥å®šä¹‰ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„å¥–åŠ±ã€‚ç„¶åï¼Œè¿™äº›å¥–åŠ±ç”¨äºåœ¨ä»¿çœŸä¸­è®­ç»ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­–ç•¥ï¼Œå¹¶é€šè¿‡åˆæˆçš„è§†è§’å’Œå…‰ç…§å˜åŒ–æ¸²æŸ“çš„å›æ”¾å°†å­¦ä¹ åˆ°çš„ç­–ç•¥æç‚¼ä¸ºå›¾åƒæ¡ä»¶æ‰©æ•£ç­–ç•¥ã€‚æœ€åï¼ŒX-Simå¼•å…¥åœ¨çº¿é¢†åŸŸé€‚åº”æŠ€æœ¯ï¼Œåœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­å¯¹çœŸå®å’Œä»¿çœŸè§‚å¯Ÿè¿›è¡Œå¯¹é½ï¼Œä»è€Œå®ç°æ— æœºå™¨äººé¥æ§æ•°æ®çš„æœ‰æ•ˆè½¬ç§»ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (4)', '#agi (1)', '#alignment (3)', '#architecture (5)', '#audio', '#benchmark (10)', '#cv (10)', '#data (6)', '#dataset (11)', '#diffusion (5)', '#ethics (1)', '#games (2)', '#graphs', '#hallucinations (2)', '#healthcare (3)', '#inference (2)', '#interpretability (2)', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (9)', '#open_source (5)', '#optimization (14)', '#plp', '#rag', '#reasoning (6)', '#rl (5)', '#rlhf (2)', '#robotics (4)', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey (2)', '#synthetic (4)', '#training (19)', '#transfer_learning (4)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-05-16 22:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-16 22:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-16 22:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    