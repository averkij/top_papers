{
    "date": {
        "ru": "4 августа",
        "en": "August 4",
        "zh": "8月4日"
    },
    "time_utc": "2025-08-04 08:18",
    "weekday": 0,
    "issue_id": 5159,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.00819",
            "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models",
            "url": "https://huggingface.co/papers/2508.00819",
            "abstract": "DAEDAL, a novel training-free denoising strategy, enables dynamic length adaptation in Diffusion Large Language Models, improving performance and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.",
            "score": 21,
            "issue_id": 5154,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 августа",
                "en": "August 1",
                "zh": "8月1日"
            },
            "hash": "2241beba3b69f1fd",
            "authors": [
                "Jinsong Li",
                "Xiaoyi Dong",
                "Yuhang Zang",
                "Yuhang Cao",
                "Jiaqi Wang",
                "Dahua Lin"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00819.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#long_context"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Динамическая адаптация длины раскрывает потенциал диффузионных языковых моделей",
                    "desc": "DAEDAL - это новая стратегия динамической адаптации длины для диффузионных больших языковых моделей (DLLM). Она позволяет преодолеть ограничение статически заданной длины генерации, которое снижает эффективность DLLM. DAEDAL работает в два этапа: сначала расширяет начальную короткую длину до подходящей для задачи, а затем во время денойзинга динамически расширяет недостаточные области генерации. Эксперименты показывают, что DAEDAL достигает сравнимой или превосходящей производительности по сравнению с тщательно настроенными базовыми моделями фиксированной длины, одновременно повышая вычислительную эффективность."
                },
                "en": {
                    "title": "Dynamic Length Adaptation for Enhanced DLLM Performance",
                    "desc": "DAEDAL is a new method that improves Diffusion Large Language Models (DLLMs) by allowing them to adapt their output length dynamically without needing additional training. Traditional DLLMs require a fixed length for generation, which can limit their performance on complex tasks or waste computational resources. DAEDAL addresses this issue by using internal signals from the model to determine the optimal length for responses, expanding the generation length as needed. This approach not only enhances the quality of the generated text but also increases efficiency, making DLLMs more competitive with Autoregressive models."
                },
                "zh": {
                    "title": "DAEDAL：动态适应长度的去噪新策略",
                    "desc": "DAEDAL是一种新颖的无训练去噪策略，能够在扩散大型语言模型中实现动态长度适应，从而提高性能和计算效率。扩散大型语言模型（DLLMs）在生成效率和全局上下文建模方面表现出色，但其静态生成长度限制了实际应用。DAEDAL通过利用模型内部信号，动态调整生成长度，解决了静态长度带来的性能和计算开销问题。实验表明，DAEDAL在性能上与固定长度基线相当，甚至在某些情况下更优，同时提高了计算效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23268",
            "title": "PixNerd: Pixel Neural Field Diffusion",
            "url": "https://huggingface.co/papers/2507.23268",
            "abstract": "Pixel Neural Field Diffusion (PixNerd) achieves high-quality image generation in a single-scale, single-stage process without VAEs or complex pipelines, and extends to text-to-image applications with competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity. In contrast to their efforts, we propose to model the patch-wise decoding with neural field and present a single-scale, single-stage, efficient, end-to-end solution, coined as pixel neural field diffusion~(PixelNerd). Thanks to the efficient neural field representation in PixNerd, we directly achieved 2.15 FID on ImageNet 256times256 and 2.84 FID on ImageNet 512times512 without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.",
            "score": 16,
            "issue_id": 5154,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "f035699955568725",
            "authors": [
                "Shuai Wang",
                "Ziteng Gao",
                "Chenhui Zhu",
                "Weilin Huang",
                "Limin Wang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Nanjing University",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23268.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Эффективная генерация изображений без сложных архитектур",
                    "desc": "PixNerd (Pixel Neural Field Diffusion) - это новый метод генерации изображений, работающий в пиксельном пространстве без использования вариационных автоэнкодеров. Он предлагает одноэтапный процесс генерации высококачественных изображений без сложных каскадных архитектур. PixNerd достигает впечатляющих результатов на наборе данных ImageNet, превосходя существующие методы по метрике FID. Кроме того, модель успешно применяется для задачи генерации изображений по текстовому описанию, показывая конкурентоспособные результаты на бенчмарках GenEval и DPG."
                },
                "en": {
                    "title": "Efficient Image Generation with PixNerd: No VAEs, No Hassle!",
                    "desc": "Pixel Neural Field Diffusion (PixNerd) introduces a novel approach to image generation that operates in a single-scale and single-stage manner, eliminating the need for variational autoencoders (VAEs) and complex pipelines. This method addresses the issues of accumulated errors and artifacts that arise from traditional two-stage training processes. By utilizing a patch-wise decoding strategy with neural fields, PixNerd achieves impressive performance metrics, such as a 2.15 FID score on ImageNet 256x256. Additionally, it extends its capabilities to text-to-image generation, demonstrating competitive results on various benchmarks."
                },
                "zh": {
                    "title": "高效图像生成的新方法：PixNerd",
                    "desc": "Pixel Neural Field Diffusion（PixNerd）是一种高效的图像生成方法，采用单尺度、单阶段的流程，无需变分自编码器（VAE）或复杂的管道。该方法通过神经场模型实现了补丁级解码，避免了传统方法中常见的累积误差和解码伪影。PixNerd在ImageNet数据集上取得了2.15的FID分数，显示出其优越的性能。我们还将PixNerd扩展到文本生成图像的应用中，取得了在GenEval和DPG基准测试中的竞争性成绩。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23361",
            "title": "SWE-Exp: Experience-Driven Software Issue Resolution",
            "url": "https://huggingface.co/papers/2507.23361",
            "abstract": "SWE-Exp enhances software issue resolution by systematically accumulating and leveraging repair expertise from past agent experiences, improving resolution rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.",
            "score": 5,
            "issue_id": 5154,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "e16fe4dad5f61553",
            "authors": [
                "Silin Chen",
                "Shaoxin Lin",
                "Xiaodong Gu",
                "Yuling Shi",
                "Heng Lian",
                "Longfei Yun",
                "Dong Chen",
                "Weiguo Sun",
                "Lin Cao",
                "Qianxiang Wang"
            ],
            "affiliations": [
                "Huawei, China",
                "Shanghai Jiao Tong University, China",
                "UC San Diego, United States",
                "Xidian University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23361.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Опыт - ключ к эффективному решению проблем в ПО",
                    "desc": "SWE-Exp - это новый подход к решению проблем в программном обеспечении, использующий накопленный опыт предыдущих попыток исправления ошибок. Метод создает многоуровневый банк опыта, который включает как успешные, так и неудачные попытки решения проблем. SWE-Exp извлекает многоуровневые знания о решении проблем - от общего понимания до конкретных изменений в коде. Эксперименты показывают, что SWE-Exp достигает наилучших результатов в решении проблем на тестовом наборе SWE-bench-Verified среди агентов с открытым исходным кодом."
                },
                "en": {
                    "title": "Transforming Software Issue Resolution with Experience-Driven Learning",
                    "desc": "SWE-Exp is a novel approach that enhances software issue resolution by utilizing past experiences of agents to improve their performance. Unlike traditional agents that do not retain knowledge, SWE-Exp builds an experience bank that captures both successful and failed attempts at resolving issues. This allows the system to learn from previous repairs and apply that knowledge to new problems, leading to more efficient and effective resolutions. The method has demonstrated a significant improvement in resolution rates, showcasing a shift from random exploration to a more strategic, experience-driven process."
                },
                "zh": {
                    "title": "经验驱动的软件问题解决新方法",
                    "desc": "SWE-Exp是一种增强软件问题解决能力的方法，通过系统地积累和利用过去代理的修复经验，提高了解决率。当前的代理在处理问题时缺乏记忆，无法重用之前的知识，导致重复探索失败的路径。SWE-Exp通过建立一个多层次的经验库，提取成功和失败的修复尝试中的可重用知识，从而实现跨问题的持续学习。实验表明，SWE-Exp在开源代理框架下的SWE-bench-Verified上达到了41.6%的最佳解决率，标志着自动化软件工程代理的一个新范式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23348",
            "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution",
            "url": "https://huggingface.co/papers/2507.23348",
            "abstract": "SWE-Debate, a competitive multi-agent framework, enhances issue resolution in software engineering by promoting diverse reasoning and achieving better issue localization and fix planning.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.",
            "score": 4,
            "issue_id": 5154,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "28b58ecd36ac995b",
            "authors": [
                "Han Li",
                "Yuling Shi",
                "Shaoxin Lin",
                "Xiaodong Gu",
                "Heng Lian",
                "Xin Wang",
                "Yantao Jia",
                "Tao Huang",
                "Qianxiang Wang"
            ],
            "affiliations": [
                "Huawei China",
                "Shanghai Jiao Tong University China",
                "Xidian University China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23348.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#open_source",
                    "#reasoning",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Дебаты ИИ-агентов для улучшения разработки ПО",
                    "desc": "SWE-Debate - это новая система для решения проблем в разработке программного обеспечения, использующая несколько ИИ-агентов. Система организует структурированные дебаты между агентами, каждый из которых предлагает свой подход к локализации и исправлению ошибок. Этот метод позволяет находить более комплексные решения, охватывающие различные части кодовой базы. В результате SWE-Debate превосходит существующие подходы в локализации проблем и планировании исправлений."
                },
                "en": {
                    "title": "Empowering Software Issue Resolution through Competitive Multi-Agent Debate",
                    "desc": "SWE-Debate is a multi-agent framework designed to improve issue resolution in software engineering by fostering diverse reasoning among agents. It utilizes large language models to enhance the reasoning capabilities of autonomous agents, allowing them to tackle complex tasks more effectively. By organizing a structured debate among agents with different perspectives, SWE-Debate helps identify broader issue patterns and achieve better localization of software faults. The framework has demonstrated significant improvements in performance on the SWE-bench benchmark, setting new standards in open-source agent frameworks."
                },
                "zh": {
                    "title": "SWE-Debate：多样化推理促进软件问题解决",
                    "desc": "SWE-Debate是一个竞争性的多智能体框架，旨在通过促进多样化的推理来增强软件工程中的问题解决能力。该框架利用大型语言模型的推理能力，帮助智能体在复杂的软件工程任务中进行自主探索。与以往的独立探索方法不同，SWE-Debate通过组织智能体之间的辩论，鼓励不同的推理路径，从而更好地定位问题并制定修复计划。实验结果表明，SWE-Debate在开源智能体框架中达到了新的最先进水平，显著优于基线方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00265",
            "title": "Multimodal Referring Segmentation: A Survey",
            "url": "https://huggingface.co/papers/2508.00265",
            "abstract": "A survey of multimodal referring segmentation techniques, covering advancements in convolutional neural networks, transformers, and large language models for segmenting objects in images, videos, and 3D scenes based on text or audio instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format. This task plays a crucial role in practical applications requiring accurate object perception based on user instructions. Over the past decade, it has gained significant attention in the multimodal community, driven by advances in convolutional neural networks, transformers, and large language models, all of which have substantially improved multimodal perception capabilities. This paper provides a comprehensive survey of multimodal referring segmentation. We begin by introducing this field's background, including problem definitions and commonly used datasets. Next, we summarize a unified meta architecture for referring segmentation and review representative methods across three primary visual scenes, including images, videos, and 3D scenes. We further discuss Generalized Referring Expression (GREx) methods to address the challenges of real-world complexity, along with related tasks and practical applications. Extensive performance comparisons on standard benchmarks are also provided. We continually track related works at https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.",
            "score": 3,
            "issue_id": 5154,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 августа",
                "en": "August 1",
                "zh": "8月1日"
            },
            "hash": "1604e587f6dc8177",
            "authors": [
                "Henghui Ding",
                "Song Tang",
                "Shuting He",
                "Chang Liu",
                "Zuxuan Wu",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "ByteDance Inc.",
                "Fudan University",
                "Shanghai University of Finance and Economics"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00265.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#3d",
                    "#survey",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Мультимодальная сегментация: от пикселей к пониманию",
                    "desc": "Эта статья представляет собой обзор методов мультимодальной сегментации по ссылкам, охватывающий достижения в области сверточных нейронных сетей, трансформеров и больших языковых моделей. Авторы рассматривают задачу сегментации объектов на изображениях, видео и в 3D-сценах на основе текстовых или аудио инструкций. В работе представлена унифицированная мета-архитектура для сегментации по ссылкам и обзор репрезентативных методов для различных визуальных сцен. Также обсуждаются обобщенные методы выражения ссылок (GREx) для решения проблем сложности реального мира."
                },
                "en": {
                    "title": "Enhancing Object Segmentation with Multimodal Instructions",
                    "desc": "This paper surveys the field of multimodal referring segmentation, which focuses on identifying and segmenting objects in visual content based on textual or audio instructions. It highlights the advancements made through convolutional neural networks, transformers, and large language models that enhance the ability to understand and process multimodal data. The authors present a unified meta architecture for referring segmentation and review various methods applicable to images, videos, and 3D scenes. Additionally, they discuss challenges in real-world applications and provide performance comparisons on standard benchmarks to evaluate the effectiveness of different approaches."
                },
                "zh": {
                    "title": "多模态指向分割的全面调查",
                    "desc": "多模态指向分割旨在根据文本或音频指令在视觉场景中分割目标物体，如图像、视频和3D场景。该任务在需要根据用户指令进行准确物体感知的实际应用中至关重要。近年来，卷积神经网络、变换器和大型语言模型的进步显著提升了多模态感知能力。本文提供了多模态指向分割的全面调查，涵盖了背景介绍、统一的元架构、代表性方法及其在不同视觉场景中的应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.23478",
            "title": "3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding",
            "url": "https://huggingface.co/papers/2507.23478",
            "abstract": "3D-R1 enhances 3D scene understanding through a high-quality synthetic dataset, reinforcement learning with GRPO, and dynamic view selection, achieving significant improvements in reasoning and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large vision-language models (VLMs) have made significant strides in 2D visual understanding tasks, sparking interest in extending these capabilities to 3D scene understanding. However, current 3D VLMs often struggle with robust reasoning and generalization due to limitations in high-quality spatial data and the static nature of viewpoint assumptions. To address these challenges, we propose 3D-R1, a foundation model that enhances the reasoning capabilities of 3D VLMs. Specifically, we first construct a high-quality synthetic dataset with CoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine based on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1. Moreover, we leverage RLHF policy such as GRPO in the reinforcement learning training process to enhance reasoning capabilities and introduce three reward functions: a perception reward, a semantic similarity reward and a format reward to maintain detection accuracy and answer semantic precision. Furthermore, we introduce a dynamic view selection strategy that adaptively chooses the most informative perspectives for 3D scene understanding. Extensive experiments demonstrate that 3D-R1 delivers an average improvement of 10% across various 3D scene benchmarks, highlighting its effectiveness in enhancing reasoning and generalization in 3D scene understanding. Code: https://github.com/AIGeeksGroup/3D-R1. Website: https://aigeeksgroup.github.io/3D-R1.",
            "score": 3,
            "issue_id": 5155,
            "pub_date": "2025-07-31",
            "pub_date_card": {
                "ru": "31 июля",
                "en": "July 31",
                "zh": "7月31日"
            },
            "hash": "f5e99fc10e8b9ad5",
            "authors": [
                "Ting Huang",
                "Zeyu Zhang",
                "Hao Tang"
            ],
            "affiliations": [
                "School of Computer Science, Peking University",
                "Shanghai University of Engineering Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.23478.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#3d",
                    "#dataset",
                    "#reasoning",
                    "#rlhf",
                    "#synthetic"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "3D-R1: Революция в понимании трехмерных сцен с помощью ИИ",
                    "desc": "Модель 3D-R1 улучшает понимание трехмерных сцен с помощью высококачественного синтетического датасета и обучения с подкреплением. Она использует динамический выбор ракурсов для более информативного анализа 3D-сцен. 3D-R1 применяет функции вознаграждения для улучшения точности восприятия и семантической точности ответов. Эксперименты показывают значительное улучшение результатов на различных бенчмарках трехмерных сцен."
                },
                "en": {
                    "title": "Enhancing 3D Scene Understanding with 3D-R1",
                    "desc": "The paper presents 3D-R1, a model designed to improve 3D scene understanding by addressing the limitations of existing vision-language models (VLMs). It introduces a high-quality synthetic dataset called Scene-30K, which is used to enhance the model's reasoning capabilities. The training process incorporates reinforcement learning with a GRPO policy and utilizes multiple reward functions to ensure accuracy and semantic precision. Additionally, a dynamic view selection strategy is implemented to optimize the perspectives used for analyzing 3D scenes, resulting in a notable average improvement of 10% in performance across various benchmarks."
                },
                "zh": {
                    "title": "3D-R1：提升3D场景理解的智能模型",
                    "desc": "3D-R1 是一个增强 3D 场景理解的基础模型，利用高质量的合成数据集和强化学习方法来提升推理能力。我们构建了一个名为 Scene-30K 的合成数据集，作为 3D-R1 的冷启动初始化数据。通过引入动态视角选择策略，3D-R1 能够自适应选择最具信息量的视角进行 3D 场景理解。实验结果表明，3D-R1 在多个 3D 场景基准测试中平均提升了 10%，有效增强了推理和泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.22720",
            "title": "Investigating Hallucination in Conversations for Low Resource Languages",
            "url": "https://huggingface.co/papers/2507.22720",
            "abstract": "LLMs generate fewer hallucinations in Mandarin compared to Hindi and Farsi across multiple models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.",
            "score": 2,
            "issue_id": 5155,
            "pub_date": "2025-07-30",
            "pub_date_card": {
                "ru": "30 июля",
                "en": "July 30",
                "zh": "7月30日"
            },
            "hash": "c7f5db5f58895f4f",
            "authors": [
                "Amit Das",
                "Md. Najib Hasan",
                "Souvika Sarkar",
                "Zheng Zhang",
                "Fatemeh Jamshidi",
                "Tathagata Bhattacharya",
                "Nilanjana Raychawdhury",
                "Dongji Feng",
                "Vinija Jain",
                "Aman Chadha"
            ],
            "affiliations": [
                "Amazon GenAI",
                "Auburn University",
                "Auburn University at Montgomery",
                "California State Polytechnic University Pomona",
                "Gustavus Adolphus College",
                "Meta",
                "Murray State University",
                "Stanford University",
                "University of North Alabama",
                "Wichita State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.22720.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multilingual",
                    "#hallucinations"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Языковые модели меньше галлюцинируют по-китайски",
                    "desc": "Исследование посвящено проблеме галлюцинаций в больших языковых моделях (LLM) на примере трех языков: хинди, фарси и мандаринского китайского. Авторы провели комплексный анализ фактических и лингвистических ошибок в этих языках для нескольких популярных моделей, включая GPT-3.5, GPT-4, Llama-3.1 и другие. Результаты показали, что LLM генерируют значительно меньше галлюцинаций на мандаринском китайском по сравнению с хинди и фарси. Это исследование расширяет понимание проблемы галлюцинаций за пределы английского языка, что важно для повышения надежности и эффективности LLM."
                },
                "en": {
                    "title": "Mandarin LLMs: Fewer Hallucinations, More Accuracy!",
                    "desc": "This paper investigates the phenomenon of hallucinations in Large Language Models (LLMs) across three languages: Mandarin, Hindi, and Farsi. Hallucinations refer to instances where the models generate incorrect or misleading information. The study analyzes conversational data from various LLMs, including GPT-3.5 and GPT-4o, to compare the frequency of these errors. The findings reveal that LLMs exhibit fewer hallucinations in Mandarin compared to the higher rates observed in Hindi and Farsi, highlighting the need for language-specific improvements in model training."
                },
                "zh": {
                    "title": "普通话中的幻觉现象较少",
                    "desc": "大型语言模型（LLMs）在生成文本方面表现出色，但它们有时会产生不准确的信息，这被称为“幻觉”。本研究探讨了在普通话、印地语和法尔西语中，LLMs的幻觉现象。我们分析了多个模型（如GPT-3.5、GPT-4o等）在这三种语言中的事实和语言错误。结果显示，LLMs在普通话中产生的幻觉响应较少，而在印地语和法尔西语中则显著更多。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00823",
            "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
            "url": "https://huggingface.co/papers/2508.00823",
            "abstract": "IGL-Nav uses an incremental 3D Gaussian representation for efficient and accurate image-goal navigation in 3D space, outperforming existing methods and applicable in real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/.",
            "score": 1,
            "issue_id": 5154,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 августа",
                "en": "August 1",
                "zh": "8月1日"
            },
            "hash": "a4651adceaac80f7",
            "authors": [
                "Wenxuan Guo",
                "Xiuwei Xu",
                "Hang Yin",
                "Ziwei Wang",
                "Jianjiang Feng",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00823.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#3d",
                    "#optimization",
                    "#agents",
                    "#games"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "Навигация в 3D с помощью инкрементальных гауссианов",
                    "desc": "IGL-Nav - это новый метод навигации по изображению-цели в трехмерном пространстве. Он использует инкрементальное представление 3D гауссианов для эффективной и точной локализации целевого изображения. Метод превосходит существующие подходы, сочетая дискретное сопоставление пространства и оптимизацию через дифференцируемый рендеринг. IGL-Nav применим в реальных условиях и может работать с произвольными ракурсами целевых изображений."
                },
                "en": {
                    "title": "Efficient 3D Navigation with Incremental Gaussian Localization",
                    "desc": "IGL-Nav introduces an innovative approach to image-goal navigation in 3D environments using an incremental 3D Gaussian representation. This method enhances localization accuracy by updating the scene representation as new images are processed, allowing for efficient navigation. Unlike traditional methods that struggle with geometric relationships, IGL-Nav utilizes geometric information for effective discrete space matching and fine target pose optimization. The framework demonstrates significant improvements over existing techniques and is suitable for real-world applications, including robotic platforms."
                },
                "zh": {
                    "title": "增量式3D高斯导航：高效准确的图像目标定位",
                    "desc": "IGL-Nav是一种增量式3D高斯定位框架，旨在提高图像目标导航的效率和准确性。该方法通过可渲染的3D高斯表示来建模3D环境与目标图像之间的几何关系，克服了传统方法的局限性。IGL-Nav通过前馈单目预测逐步更新场景表示，并利用几何信息进行粗略定位，最终通过可微渲染优化精确确定目标位置。实验结果表明，IGL-Nav在多种配置下显著超越了现有的最先进方法，并能够在真实世界的机器人平台上应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.00454",
            "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
            "url": "https://huggingface.co/papers/2508.00454",
            "abstract": "An efficient multi-turn dialogue evaluator aggregates multiple LLM judgments into a single model to assess dialogue quality with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.",
            "score": 1,
            "issue_id": 5154,
            "pub_date": "2025-08-01",
            "pub_date_card": {
                "ru": "1 августа",
                "en": "August 1",
                "zh": "8月1日"
            },
            "hash": "7d97f0b64c1261dd",
            "authors": [
                "Yuqi Tang",
                "Kehua Feng",
                "Yunfeng Wang",
                "Zhiwen Chen",
                "Chengfei Lv",
                "Gang Yu",
                "Qiang Zhang",
                "Keyan Ding"
            ],
            "affiliations": [
                "Alibaba Group",
                "College of Computer Science and Technology, Zhejiang University",
                "ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University",
                "ZJU-UIUC Institute, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.00454.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#inference",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Эффективная оценка диалогов: мудрость многих в одной модели",
                    "desc": "Эта статья представляет эффективный метод оценки качества многоэтапных диалогов с использованием больших языковых моделей (LLM). Авторы предлагают агрегировать суждения нескольких LLM в единую модель, что позволяет сохранить преимущества разнообразных оценок, но значительно снизить вычислительные затраты. Метод показал превосходные результаты на семи эталонных наборах данных для оценки диалогов. Предложенный подход обеспечивает быструю и гибкую оценку качества диалогов, сохраняя при этом надежность и согласованность результатов."
                },
                "en": {
                    "title": "Efficient Dialogue Evaluation: Harnessing Collective Wisdom of LLMs",
                    "desc": "This paper introduces an efficient multi-turn dialogue evaluator that combines the judgments of multiple large language models (LLMs) to assess dialogue quality. Traditional methods using a single LLM as a judge often face biases that affect evaluation reliability. The proposed method aggregates the preferences of several LLMs into one model, maintaining the benefits of diverse feedback while significantly lowering computational costs. Experiments show that this new approach outperforms existing methods in various evaluation scenarios, proving its effectiveness and efficiency."
                },
                "zh": {
                    "title": "高效的多轮对话评估器：聚合智慧，降低成本",
                    "desc": "本文提出了一种高效的多轮对话评估器，通过将多个大型语言模型（LLM）的判断汇聚成一个单一模型来评估对话质量，从而降低计算成本。当前的评估方法主要依赖于“LLM作为评审”的模式，但这种方法常常受到偏见的影响，导致评估结果的不可靠性。为了解决这个问题，本文的方法利用多个LLM作为评审，并将它们的偏好知识汇聚到一个模型中，从而保留多评审反馈的优势，同时显著减少评估成本。实验结果表明，该方法在多种对话评估基准上优于现有的基线，展示了其高效性和鲁棒性。"
                }
            }
        }
    ],
    "link_prev": "2025-08-01.html",
    "link_next": "2025-08-05.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "01.08",
        "en": "08/01",
        "zh": "8月1日"
    },
    "short_date_next": {
        "ru": "05.08",
        "en": "08/05",
        "zh": "8月5日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 3,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}