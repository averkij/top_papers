{
    "date": {
        "ru": "2 апреля",
        "en": "April 2",
        "zh": "4月2日"
    },
    "time_utc": "2025-04-02 02:22",
    "weekday": 2,
    "issue_id": 3017,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.00906",
            "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents",
            "url": "https://huggingface.co/papers/2504.00906",
            "abstract": "Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.",
            "score": 5,
            "issue_id": 3017,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "e51174b579a417e9",
            "authors": [
                "Saaket Agashe",
                "Kyle Wong",
                "Vincent Tu",
                "Jiachen Yang",
                "Ang Li",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "Simular Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00906.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Agent S2: Новый уровень автоматизации компьютерных задач с помощью ИИ",
                    "desc": "Статья представляет Agent S2 - новую композиционную систему для автоматизации компьютерных задач через графический интерфейс. Система использует множество специализированных и обобщенных моделей, а также новые методы точной локализации элементов интерфейса и иерархического планирования действий. Agent S2 достигает значительных улучшений производительности по сравнению с существующими решениями на нескольких бенчмарках для разных операционных систем. Авторы утверждают, что их подход открывает новые возможности для повышения продуктивности человека при работе с компьютером."
                },
                "en": {
                    "title": "Agent S2: Revolutionizing Task Automation with Smart Planning and Grounding",
                    "desc": "This paper presents Agent S2, a new framework designed to improve the performance of computer use agents that automate tasks by interacting with graphical user interfaces (GUIs). The framework addresses key challenges such as accurately identifying GUI elements and planning complex tasks over time. It introduces a Mixture-of-Grounding technique for better GUI localization and Proactive Hierarchical Planning to adapt action plans based on real-time observations. Evaluations show that Agent S2 outperforms existing agents on multiple benchmarks, demonstrating significant improvements in task execution across different operating systems."
                },
                "zh": {
                    "title": "Agent S2：智能代理的新纪元",
                    "desc": "本文介绍了一种名为Agent S2的新型智能代理框架，旨在通过将认知任务分配给不同的通用模型和专业模型来提高数字任务的自动化效率。我们提出了一种新的混合定位技术，以实现精确的图形用户界面（GUI）元素定位，并引入了主动层次规划，能够根据不断变化的观察动态调整行动计划。评估结果表明，Agent S2在三个主要的计算机使用基准测试中达到了新的最先进性能，显著超越了现有的领先代理。特别是在OSWorld评估中，Agent S2相较于其他代理实现了18.9%和32.7%的相对提升，展现了其在不同操作系统和应用中的良好泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01016",
            "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors",
            "url": "https://huggingface.co/papers/2504.01016",
            "abstract": "Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.",
            "score": 2,
            "issue_id": 3017,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "9430b45c3324fb61",
            "authors": [
                "Tian-Xing Xu",
                "Xiangjun Gao",
                "Wenbo Hu",
                "Xiaoyu Li",
                "Song-Hai Zhang",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "HKUST",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01016.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#architecture",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "GeometryCrafter: Высокоточная оценка глубины видео для 3D реконструкции",
                    "desc": "GeometryCrafter - это новая система для высокоточной оценки глубины видео. Она использует вариационный автоэнкодер для кодирования карт точек и диффузионную модель для моделирования последовательностей карт точек. Система позволяет выполнять точную 3D/4D реконструкцию и оценку параметров камеры. Эксперименты показали, что GeometryCrafter превосходит существующие методы по точности 3D, временной согласованности и способности к обобщению."
                },
                "en": {
                    "title": "GeometryCrafter: Elevating Video Depth Estimation with High-Fidelity Point Maps",
                    "desc": "This paper introduces GeometryCrafter, a new framework designed to improve video depth estimation by producing high-fidelity point map sequences that maintain temporal coherence. It addresses the limitations of existing methods in achieving accurate geometric representations, which are crucial for tasks like 3D reconstruction and camera parameter estimation. The framework utilizes a point map Variational Autoencoder (VAE) to effectively encode and decode point maps, independent of the video latent distributions. By training a video diffusion model on these point map sequences, GeometryCrafter demonstrates superior performance in 3D accuracy and generalization across various datasets."
                },
                "zh": {
                    "title": "GeometryCrafter：高保真视频深度估计的新框架",
                    "desc": "尽管视频深度估计取得了显著进展，但现有方法在几何保真度方面存在固有局限，限制了其在重建和其他度量基础下游任务中的应用。我们提出了GeometryCrafter，这是一种新颖的框架，可以从开放世界视频中恢复具有时间一致性的高保真点图序列，从而实现准确的3D/4D重建和相机参数估计。我们的方法核心是一个点图变分自编码器（VAE），它学习一个与视频潜在分布无关的潜在空间，以有效地进行点图编码和解码。通过利用VAE，我们训练了一个视频扩散模型，以建模基于输入视频的点图序列的分布。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23361",
            "title": "Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base",
            "url": "https://huggingface.co/papers/2503.23361",
            "abstract": "Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599x and 9x, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.",
            "score": 2,
            "issue_id": 3017,
            "pub_date": "2025-03-30",
            "pub_date_card": {
                "ru": "30 марта",
                "en": "March 30",
                "zh": "3月30日"
            },
            "hash": "9b957c49c958aea3",
            "authors": [
                "Linxin Song",
                "Xuwei Ding",
                "Jieyu Zhang",
                "Taiwei Shi",
                "Ryotaro Shimizu",
                "Rahul Gupta",
                "Yang Liu",
                "Jian Kang",
                "Jieyu Zhao"
            ],
            "affiliations": [
                "AGI",
                "Amazon",
                "University of Rochester",
                "University of Southern California",
                "University of Washington",
                "University of Wisconsin-Madison",
                "ZOZO Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23361.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#hallucinations",
                    "#optimization",
                    "#graphs",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "SEA: Эффективный поиск пробелов в знаниях языковых моделей",
                    "desc": "Эта статья представляет новый метод под названием SEA (стохастический подъем ошибок) для эффективного обнаружения пробелов в знаниях крупных языковых моделей (LLM). SEA использует итеративный процесс оптимизации, чтобы находить новые кандидаты на ошибки, основываясь на семантическом сходстве с ранее обнаруженными ошибками. Метод применяет иерархический поиск и построение графа отношений для выявления систематических ошибок. Эмпирические результаты показывают, что SEA значительно превосходит существующие методы по эффективности обнаружения ошибок в LLM."
                },
                "en": {
                    "title": "Uncovering Knowledge Deficiencies in LLMs Efficiently with SEA",
                    "desc": "This paper introduces a new method called Stochastic Error Ascent (SEA) to identify knowledge deficiencies in large language models (LLMs) that often produce unreliable outputs. SEA efficiently discovers errors by using a stochastic optimization approach, focusing on high-error candidates based on their similarity to previously identified failures. The framework enhances its search capabilities through hierarchical retrieval and a directed acyclic graph to track error propagation. The results show that SEA significantly outperforms existing methods in uncovering knowledge errors while drastically reducing the cost of error discovery."
                },
                "zh": {
                    "title": "发现LLM知识缺陷的新方法",
                    "desc": "大型语言模型（LLMs）在语言能力上表现出色，但常常无法准确保留事实知识，导致幻觉和不可靠的输出。我们提出了一种名为随机错误上升（SEA）的框架，用于在严格的查询预算下发现闭合权重LLMs中的知识缺陷。SEA通过利用与先前观察到的失败的语义相似性，迭代检索新的高错误候选项，从而将错误发现过程形式化为随机优化过程。实验证明，SEA发现的知识错误数量显著高于现有方法，同时大幅降低了每个错误的成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01005",
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning",
            "url": "https://huggingface.co/papers/2504.01005",
            "abstract": "Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.",
            "score": 1,
            "issue_id": 3017,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "ee8e4951bf6c7a18",
            "authors": [
                "Nishad Singhi",
                "Hritik Bansal",
                "Arian Hosseini",
                "Aditya Grover",
                "Kai-Wei Chang",
                "Marcus Rohrbach",
                "Anna Rohrbach"
            ],
            "affiliations": [
                "Google DeepMind",
                "Mila",
                "TU Darmstadt & hessian.AI",
                "University of California Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01005.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#optimization",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Баланс между генерацией решений и их верификацией в языковых моделях",
                    "desc": "Статья исследует стратегии улучшения способностей больших языковых моделей (LLM) к рассуждениям, особенно в задачах математического характера. Авторы сравнивают метод Self-Consistency (SC), генерирующий множество решений и выбирающий наиболее частое, с подходом Generative Reward Models (GenRM), который оценивает решения путем генерации цепочек рассуждений. Исследование показывает, что SC более эффективен по вычислительным ресурсам для большинства практических задач. Авторы также выводят законы масштабирования для парадигмы GenRM, предоставляя практические рекомендации по оптимизации вычислений во время тестирования."
                },
                "en": {
                    "title": "Balancing Solution Generation and Verification for Efficient Reasoning in LLMs",
                    "desc": "This paper explores how to improve the reasoning abilities of large language models (LLMs) during problem-solving by adjusting the amount of computation used at test time. It compares two methods: Self-Consistency (SC), which generates multiple answers and picks the most common, and Generative Reward Models (GenRM), which scores answers based on a next-token prediction approach. The study finds that SC is generally more efficient in terms of compute resources compared to GenRM, especially under limited budgets. The authors provide insights on how to effectively balance the generation of solutions and their verification to optimize performance."
                },
                "zh": {
                    "title": "优化推理能力：解生成与验证的平衡",
                    "desc": "本文探讨了在大语言模型（LLMs）中，如何通过扩展测试时计算来提升推理能力，尤其是在数学问题解决任务中。传统的自一致性（Self-Consistency, SC）方法通过生成多个解并采用多数投票选择最常见的答案。最近的生成奖励模型（Generative Reward Models, GenRM）则将验证重构为下一个标记预测任务，从而在推理时引入新的扩展方式。研究表明，在固定的推理预算下，SC在大多数实际情况下比GenRM更具计算效率，提供了在测试时扩展中优化解生成与验证的实用指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.23733",
            "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization",
            "url": "https://huggingface.co/papers/2503.23733",
            "abstract": "Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.",
            "score": 0,
            "issue_id": 3017,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 марта",
                "en": "March 31",
                "zh": "3月31日"
            },
            "hash": "ed45063868071c13",
            "authors": [
                "Yiyang Du",
                "Xiaochen Wang",
                "Chi Chen",
                "Jiabo Ye",
                "Yiru Wang",
                "Peng Li",
                "Ming Yan",
                "Ji Zhang",
                "Fei Huang",
                "Zhifang Sui",
                "Maosong Sun",
                "Yang Liu"
            ],
            "affiliations": [
                "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China",
                "Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China",
                "Institute of Intelligent Computing, Alibaba Group",
                "Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China",
                "ModelTC Open Source Organization, Beijing, China",
                "School of Software Microelectronics, Peking University, Beijing, China",
                "Shanghai Artificial Intelligence Laboratory, Shanghai, China",
                "State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.23733.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#transfer_learning",
                    "#optimization",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "AdaMMS: Эффективное слияние разнородных мультимодальных языковых моделей",
                    "desc": "Статья представляет AdaMMS - новый метод объединения мультимодальных языковых моделей (MLLM) с разнородной архитектурой. Метод включает три этапа: отображение параметров между моделями, их слияние с помощью линейной интерполяции и поиск оптимальных гиперпараметров. AdaMMS решает проблемы, связанные с различиями в архитектуре и асимметрией в пространстве параметров разнородных MLLM. Эксперименты показали превосходство AdaMMS над существующими методами объединения моделей на различных мультимодальных задачах."
                },
                "en": {
                    "title": "Merging Diverse Models with AdaMMS",
                    "desc": "This paper introduces AdaMMS, a new method for merging Multimodal Large Language Models (MLLMs) that have different architectures. Traditional merging techniques struggle with these heterogeneous models due to their varying structures and parameter spaces. AdaMMS addresses this by first mapping the models, then merging their weights through linear interpolation, and finally optimizing hyper-parameters using an unsupervised approach. The results show that AdaMMS significantly improves performance on vision-language tasks compared to earlier methods."
                },
                "zh": {
                    "title": "异质模型合并的新突破",
                    "desc": "最近，模型合并方法在结合多个大型语言模型（LLMs）在不同任务上的能力方面表现出强大的优势。以往的模型合并方法主要集中在合并具有相同架构的同质模型，但在处理具有固有异质性的多模态大型语言模型（MLLMs）时面临挑战。我们提出了AdaMMS，这是一种专为异质MLLMs设计的新型模型合并方法，采用映射、合并和搜索三个步骤来解决这些挑战。通过设计模型之间的映射函数、对模型权重进行线性插值以及提出无监督的超参数选择方法，AdaMMS在各种视觉-语言基准测试中超越了以往的模型合并方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21860",
            "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning",
            "url": "https://huggingface.co/papers/2503.21860",
            "abstract": "Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.",
            "score": 0,
            "issue_id": 3017,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 марта",
                "en": "March 27",
                "zh": "3月27日"
            },
            "hash": "0d9ea55946287027",
            "authors": [
                "Kailin Li",
                "Puhao Li",
                "Tengyu Liu",
                "Yuyang Li",
                "Siyuan Huang"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University",
                "Institute for Artificial Intelligence, Peking University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21860.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#robotics",
                    "#transfer_learning",
                    "#optimization",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ManipTrans: эффективная передача человеческих навыков манипуляции роботам",
                    "desc": "ManipTrans - это новый метод передачи навыков бимануальной манипуляции от человека к роботизированным рукам в симуляции. Он включает предварительное обучение имитатора траектории движения рук и дообучение специфического остаточного модуля с учетом ограничений взаимодействия. Метод превосходит существующие подходы по точности и эффективности выполнения сложных задач манипуляции. На его основе создан крупномасштабный датасет DexManipNet с 3.3 тыс. эпизодов роботизированной манипуляции для обучения политик управления ловкими руками."
                },
                "en": {
                    "title": "Efficiently Teaching Robots to Manipulate Like Humans",
                    "desc": "This paper presents ManipTrans, a two-stage method designed to transfer human bimanual manipulation skills to robotic hands in a simulated environment. The first stage involves pre-training a trajectory imitator that learns to replicate human hand movements, while the second stage fine-tunes a residual module to enhance performance under specific interaction constraints. This approach allows for efficient learning and execution of complex tasks, outperforming existing methods in terms of success rate and efficiency. Additionally, the authors introduce DexManipNet, a comprehensive dataset that includes diverse manipulation tasks, paving the way for improved policy training and real-world applications of dexterous robotic hands."
                },
                "zh": {
                    "title": "高效转移人类双手技能的机器人手",
                    "desc": "本论文介绍了一种名为ManipTrans的新方法，用于将人类双手的技能高效地转移到灵巧的机器人手上。该方法分为两个阶段：首先训练一个通用的轨迹模仿器来模拟手部动作，然后在交互约束下微调特定的残差模块，从而实现复杂双手任务的高效学习和准确执行。实验结果表明，ManipTrans在成功率、保真度和效率上超越了现有的最先进方法。此外，利用ManipTrans，我们创建了一个名为DexManipNet的大规模数据集，包含了3.3K个机器人操作的实例，支持进一步的策略训练和实际应用。"
                }
            }
        }
    ],
    "link_prev": "2025-04-01.html",
    "link_next": "2025-04-03.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "01.04",
        "en": "04/01",
        "zh": "4月1日"
    },
    "short_date_next": {
        "ru": "03.04",
        "en": "04/03",
        "zh": "4月3日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的视频生成任务，称为 Talking Characters。它能通过语音和文本直接生成说话的动画角色。与之前的 talking head 不同，Talking Characters 生成的是一个或多个角色的全身像，而不仅仅是面部。研究提出了 MoCha 模型，并使用语音-视频窗口注意力机制来精确同步语音和视频。为了解决大规模语音标注视频数据集稀缺的问题，研究还提出了一种联合训练策略，提升了模型的泛化能力。",
        "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
        "pinyin": "这篇文章介绍了一种新的视频生成任务，称为 Talking Characters。它能通过语音和文本直接生成说话的动画角色。与之前的 talking head 不同，Talking Characters 生成的是一个或多个角色的全身像，而不仅仅是面部。研究提出了 MoCha 模型，并使用语音-视频窗口注意力机制来精确同步语音和视频。为了解决大规模语音标注视频数据集稀缺的问题，研究还提出了一种联合训练策略，提升了模型的泛化能力。\n\nzhè piān wén zhāng jiè shào le yī zhǒng xīn de shì pǐn shēng chéng rèn wù, chēng wéi Talking Characters. tā néng tōng guò yǔ yīn hé wén běn zhí jiē shēng chéng shuō huà de dòng huà jué sè. yǔ zhī qián de talking head bù tóng, Talking Characters shēng chéng de shì yī gè huò duō gè jué sè de quán shēn xiàng, ér bù jǐn jǐn shì miàn bù. yán jiū tí chū le MoCha mó xíng, bìng shǐ yòng yǔ yīn-shì pǐn chuāng kǒu zhù yì lì jī zhī lái jīng xiào tóng bù yǔ yīn hé shì pǐn. wèi le jiě jué dà guī mó yǔ yīn biāo zhù shì pǐn shù jù jí xī quē de wèn tí, yán jiū hái tí chū le yī zhǒng lián hé xùn liàn cè lüè, tí shēng le mó xíng de fàn huà néng lì.",
        "vocab": "[\n    {\"word\": \"视频生成\", \"pinyin\": \"shìpín shēngchéng\", \"trans\": \"video generation\"},\n    {\"word\": \"称为\", \"pinyin\": \"chēngwéi\", \"trans\": \"called\"},\n    {\"word\": \"Talking Characters\", \"pinyin\": \"Talking Characters\", \"trans\": \"Talking Characters\"},\n    {\"word\": \"语音\", \"pinyin\": \"yǔyīn\", \"trans\": \"audio\"},\n    {\"word\": \"文本\", \"pinyin\": \"wénběn\", \"trans\": \"text\"},\n    {\"word\": \"动画角色\", \"pinyin\": \"dònghuà juésè\", \"trans\": \"animated character\"},\n    {\"word\": \"talking head\", \"pinyin\": \"talking head\", \"trans\": \"talking head\"},\n    {\"word\": \"全身像\", \"pinyin\": \"quánshēn xiàng\", \"trans\": \"full-body image\"},\n    {\"word\": \"面部\", \"pinyin\": \"miànbù\", \"trans\": \"face\"},\n    {\"word\": \"MoCha\", \"pinyin\": \"MoCha\", \"trans\": \"MoCha\"},\n    {\"word\": \"模型\", \"pinyin\": \"móxíng\", \"trans\": \"model\"},\n    {\"word\": \"语音-视频窗口注意力机制\", \"pinyin\": \"yǔyīn shìpín chuāngkǒu zhùyìlì jīzhì\", \"trans\": \"audio-video window attention mechanism\"},\n    {\"word\": \"精确同步\", \"pinyin\": \"jīngquè tóngbù\", \"trans\": \"precise synchronization\"},\n    {\"word\": \"大规模\", \"pinyin\": \"dàguīmó\", \"trans\": \"large-scale\"},\n    {\"word\": \"语音标注\", \"pinyin\": \"yǔyīn biāozhù\", \"trans\": \"audio annotation\"},\n    {\"word\": \"视频数据集\", \"pinyin\": \"shìpín shùjùjí\", \"trans\": \"video dataset\"},\n    {\"word\": \"稀缺\", \"pinyin\": \"xīquē\", \"trans\": \"scarce\"},\n    {\"word\": \"联合训练策略\", \"pinyin\": \"liánhé xùnliàn cèlüè\", \"trans\": \"joint training strategy\"},\n    {\"word\": \"提升\", \"pinyin\": \"tíshēng\", \"trans\": \"improve\"},\n    {\"word\": \"泛化能力\", \"pinyin\": \"fànhuà nénglì\", \"trans\": \"generalization capability\"}\n]",
        "trans": "This article introduces a new video generation task called Talking Characters, which can directly generate speaking animated characters from speech and text. Unlike previous talking head approaches, Talking Characters generates full-body images of one or more characters, not just the face. The research proposes the MoCha model and employs a speech-video window attention mechanism to precisely synchronize speech and video. To address the scarcity of large-scale speech-annotated video datasets, the research also proposes a joint training strategy that enhances the model's generalization capability.",
        "update_ts": "2025-04-01 09:12"
    }
}