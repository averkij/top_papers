{
    "date": {
        "ru": "29 ноября",
        "en": "November 29",
        "zh": "11月29日"
    },
    "time_utc": "2024-11-29 09:11",
    "weekday": 4,
    "issue_id": 857,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.18203",
            "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2411.18203",
            "abstract": "Vision-language models~(VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.",
            "score": 10,
            "issue_id": 853,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "13bce174f2b29a74",
            "authors": [
                "Di Zhang",
                "Jingdi Lei",
                "Junxian Li",
                "Xunzhi Wang",
                "Yujie Liu",
                "Zonglin Yang",
                "Jiatong Li",
                "Weida Wang",
                "Suorong Yang",
                "Jianbo Wu",
                "Peng Ye",
                "Wanli Ouyang",
                "Dongzhan Zhou"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Fudan University",
                "Hong Kong Polytechnic University",
                "Nanjing University",
                "Nankai University",
                "Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiaotong University",
                "Shanghai University",
                "Tongji University",
                "University of California, Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18203.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rlhf",
                    "#reasoning",
                    "#hallucinations",
                    "#rl",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Critic-V: улучшение мультимодальных рассуждений с помощью обратной связи",
                    "desc": "В статье представлен новый фреймворк Critic-V для улучшения способностей рассуждения мультимодальных языковых моделей (VLM). Фреймворк состоит из двух компонентов: Reasoner, генерирующего цепочки рассуждений, и Critic, предоставляющего конструктивную критику для их улучшения. Процесс взаимодействия основан на обучении с подкреплением, где Critic дает обратную связь на естественном языке. Оценка показала, что Critic-V превосходит существующие методы, включая GPT-4V, на большинстве тестовых наборов данных."
                },
                "en": {
                    "title": "Enhancing VLMs with Critic-V: A New Era in Multimodal Reasoning",
                    "desc": "This paper presents Critic-V, a new framework designed to improve the reasoning abilities of vision-language models (VLMs) in multimodal tasks. It separates the reasoning and critique processes by using two components: the Reasoner, which creates reasoning paths from visual and textual data, and the Critic, which evaluates and refines these paths. The Critic provides feedback in the form of natural language critiques, rather than simple rewards, allowing for more detailed guidance in the reasoning process. The results demonstrate that Critic-V significantly enhances reasoning accuracy and efficiency compared to existing models, making it a valuable advancement for applications requiring complex reasoning."
                },
                "zh": {
                    "title": "Critic-V：提升视觉语言模型推理能力的新框架",
                    "desc": "本文介绍了一种新的框架Critic-V，旨在提升视觉语言模型（VLMs）的推理能力。该框架借鉴了演员-评论家（Actor-Critic）范式，将推理过程与评论过程解耦，分别由推理器和评论器两个独立组件完成。推理器根据视觉和文本输入生成推理路径，而评论器则提供建设性的反馈以优化这些路径。通过这种互动过程，Critic-V在复杂推理任务中显著提高了VLMs的准确性和效率，展示了在多模态推理应用中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17176",
            "title": "ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting",
            "url": "https://huggingface.co/papers/2411.17176",
            "abstract": "Despite the significant advancements in text-to-image (T2I) generative models, users often face a trial-and-error challenge in practical scenarios. This challenge arises from the complexity and uncertainty of tedious steps such as crafting suitable prompts, selecting appropriate models, and configuring specific arguments, making users resort to labor-intensive attempts for desired images. This paper proposes Automatic T2I generation, which aims to automate these tedious steps, allowing users to simply describe their needs in a freestyle chatting way. To systematically study this problem, we first introduce ChatGenBench, a novel benchmark designed for Automatic T2I. It features high-quality paired data with diverse freestyle inputs, enabling comprehensive evaluation of automatic T2I models across all steps. Additionally, recognizing Automatic T2I as a complex multi-step reasoning task, we propose ChatGen-Evo, a multi-stage evolution strategy that progressively equips models with essential automation skills. Through extensive evaluation across step-wise accuracy and image quality, ChatGen-Evo significantly enhances performance over various baselines. Our evaluation also uncovers valuable insights for advancing automatic T2I. All our data, code, and models will be available in https://chengyou-jia.github.io/ChatGen-Home",
            "score": 4,
            "issue_id": 856,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "1abb1fa2701cf0cc",
            "authors": [
                "Chengyou Jia",
                "Changliang Xia",
                "Zhuohang Dang",
                "Weijia Wu",
                "Hangwei Qian",
                "Minnan Luo"
            ],
            "affiliations": [
                "CFAR, A*STAR",
                "National University of Singapore",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17176.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Автоматизация генерации изображений по тексту: от сложных промптов к простому чату",
                    "desc": "Эта статья представляет новый подход к автоматической генерации изображений по текстовому описанию (Automatic T2I). Авторы предлагают ChatGenBench - набор данных для оценки моделей Automatic T2I, содержащий разнообразные входные данные в свободной форме. Они также разрабатывают ChatGen-Evo - многоэтапную стратегию эволюции, которая последовательно обучает модели необходимым навыкам автоматизации. Эксперименты показывают, что ChatGen-Evo значительно превосходит базовые модели по точности и качеству генерируемых изображений."
                },
                "en": {
                    "title": "Streamlining Text-to-Image Generation with Automation",
                    "desc": "This paper addresses the challenges users face when generating images from text descriptions using text-to-image (T2I) models. It introduces Automatic T2I generation, which simplifies the process by allowing users to communicate their needs in a conversational manner. The authors present ChatGenBench, a new benchmark that provides high-quality paired data for evaluating T2I models. Additionally, they propose ChatGen-Evo, a multi-stage evolution strategy that enhances model performance through systematic automation of the T2I generation steps."
                },
                "zh": {
                    "title": "自动化文本到图像生成，简化创作过程！",
                    "desc": "尽管文本到图像生成模型取得了显著进展，但用户在实际应用中常常面临反复试验的挑战。这个挑战源于创建合适提示、选择合适模型和配置特定参数等繁琐步骤的复杂性和不确定性。本文提出了自动化文本到图像生成，旨在简化这些繁琐步骤，用户只需以自由聊天的方式描述需求。我们还引入了ChatGenBench，一个新颖的基准，旨在系统性地评估自动化文本到图像生成模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14951",
            "title": "Morph: A Motion-free Physics Optimization Framework for Human Motion Generation",
            "url": "https://huggingface.co/papers/2411.14951",
            "abstract": "Human motion generation plays a vital role in applications such as digital humans and humanoid robot control. However, most existing approaches disregard physics constraints, leading to the frequent production of physically implausible motions with pronounced artifacts such as floating and foot sliding. In this paper, we propose Morph, a Motion-free physics optimization framework, comprising a Motion Generator and a Motion Physics Refinement module, for enhancing physical plausibility without relying on costly real-world motion data. Specifically, the Motion Generator is responsible for providing large-scale synthetic motion data, while the Motion Physics Refinement Module utilizes these synthetic data to train a motion imitator within a physics simulator, enforcing physical constraints to project the noisy motions into a physically-plausible space. These physically refined motions, in turn, are used to fine-tune the Motion Generator, further enhancing its capability. Experiments on both text-to-motion and music-to-dance generation tasks demonstrate that our framework achieves state-of-the-art motion generation quality while improving physical plausibility drastically.",
            "score": 1,
            "issue_id": 853,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "c2f59b10a563bc5d",
            "authors": [
                "Zhuo Li",
                "Mingshuang Luo",
                "Ruibing Hou",
                "Xin Zhao",
                "Hao Liu",
                "Hong Chang",
                "Zimo Liu",
                "Chen Li"
            ],
            "affiliations": [
                "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, China",
                "MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
                "Peng Cheng Laboratory, China",
                "University of Chinese Academy of Sciences, China",
                "WeChat, Tencent Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14951.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#training",
                    "#optimization",
                    "#agents",
                    "#3d"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Физически достоверные движения без реальных данных",
                    "desc": "Статья представляет Morph - фреймворк для генерации движений человека с учетом физических ограничений. Он состоит из Генератора Движений, создающего синтетические данные, и Модуля Физического Уточнения, который оптимизирует движения в физическом симуляторе. Этот подход позволяет создавать реалистичные движения без использования дорогостоящих реальных данных. Эксперименты показали, что Morph достигает высокого качества генерации движений и значительно улучшает их физическую правдоподобность."
                },
                "en": {
                    "title": "Enhancing Human Motion with Physics-Driven Optimization",
                    "desc": "This paper introduces Morph, a framework designed to improve the physical realism of human motion generation for applications like digital humans and robots. It consists of two main components: a Motion Generator that creates large-scale synthetic motion data, and a Motion Physics Refinement module that uses this data to train a motion imitator within a physics simulator. By enforcing physical constraints, the framework reduces artifacts such as floating and foot sliding, resulting in more believable motions. The approach is validated through experiments in text-to-motion and music-to-dance tasks, showing significant improvements in both motion quality and physical plausibility."
                },
                "zh": {
                    "title": "提升运动生成的物理合理性",
                    "desc": "人类运动生成在数字人类和类人机器人控制等应用中至关重要。现有方法大多忽视物理约束，导致生成的运动常常不符合物理规律，出现漂浮和滑动等明显伪影。本文提出了Morph，一个无运动的物理优化框架，包括运动生成器和运动物理精炼模块，旨在在不依赖昂贵的真实运动数据的情况下增强物理合理性。通过合成运动数据训练运动模仿器，强制物理约束，将噪声运动投影到物理合理的空间，从而实现高质量的运动生成。"
                }
            }
        }
    ],
    "link_prev": "2024-11-28.html",
    "link_next": "2024-12-02.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "28.11",
        "en": "11/28",
        "zh": "11月28日"
    },
    "short_date_next": {
        "ru": "02.12",
        "en": "12/02",
        "zh": "12月2日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "视觉-语言模型（VLMs）在多模态推理任务中取得了显著进展。然而，它们仍然常常生成不准确或无关的响应，原因是对图像理解的幻觉或未精炼的推理路径。为解决这些挑战，我们引入了Critic-V，一种受Actor-Critic范式启发的新框架，以提升VLMs的推理能力。该框架通过整合两个独立组件来解耦推理过程和评论过程：Reasoner生成基于视觉和文本输入的推理路径，Critic提供建设性的评论来完善这些路径。评估结果显示，Critic-V框架在5个基准测试中显著优于现有方法，包括GPT-4V，特别是在推理准确性和效率方面。",
        "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
        "pinyin": "视觉-语言模型（VLMs）在多模态推理任务中取得了显著进展。然而，它们仍然常常生成不准确或无关的响应，原因是对图像理解的幻觉或未精炼的推理路径。为解决这些挑战，我们引入了Critic-V，一种受Actor-Critic范式启发的新框架，以提升VLMs的推理能力。该框架通过整合两个独立组件来解耦推理过程和评论过程：Reasoner生成基于视觉和文本输入的推理路径，Critic提供建设性的评论来完善这些路径。评估结果显示，Critic-V框架在5个基准测试中显著优于现有方法，包括GPT-4V，特别是在推理准确性和效率方面。\n\nShìjué-yǔyán móxíng (VLMs) zài duō móshì tuīlǐ rènwù zhōng qǔdéle xiǎnzhù jìnbù. Rán'ér, tāmen réngrán chángcháng shēngchéng bù zhǔnquè huò wúguān de xiǎngyìng, yuányīn shì duì túxiàng lǐjiě de huànjué huò wèi jīngliàn de tuīlǐ lùjìng. Wèi jiějué zhèxiē tiǎozhàn, wǒmen yǐn rùle Critic-V, yīzhǒng shòu Actor-Critic fànshì qǐfā de xīn kuàngjià, yǐ tíshēng VLMs de tuīlǐ nénglì. Gāi kuàngjià tōngguò zhěnghé liǎng gè dúlì zǔjiàn lái jiěchán tuīlǐ guòchéng hé pínglùn guòchéng: Reasoner shēngchéng jīyú shìjué hé wénběn shūrù de tuīlǐ lùjìng, Critic tígōng jiànshèxìng de pínglùn lái wánshàn zhèxiē lùjìng. Píngjià jiéguǒ xiǎnshì, Critic-V kuàngjià zài 5 gè jīzhǔn cèshì zhōng xiǎnzhù yōu xiànzài fāngfǎ, bāokuò GPT-4V, tèbié shì zài tuīlǐ zhǔnquèxìng hé xiàolǜ fāngmiàn.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"视觉-语言模型\", \"pinyin\": \"shìjué-yǔyán móxíng\", \"trans\": \"vision-language model\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō móshì\", \"trans\": \"multimodal\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"取得\", \"pinyin\": \"qǔdé\", \"trans\": \"achieve\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎnzhù\", \"trans\": \"significant\"},\n    {\"word\": \"进展\", \"pinyin\": \"jìnzhǎn\", \"trans\": \"progress\"},\n    {\"word\": \"然而\", \"pinyin\": \"rán'ér\", \"trans\": \"however\"},\n    {\"word\": \"常常\", \"pinyin\": \"chángcháng\", \"trans\": \"often\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēngchéng\", \"trans\": \"generate\"},\n    {\"word\": \"不准确\", \"pinyin\": \"bù zhǔnquè\", \"trans\": \"inaccurate\"},\n    {\"word\": \"无关\", \"pinyin\": \"wúguān\", \"trans\": \"irrelevant\"},\n    {\"word\": \"响应\", \"pinyin\": \"xiǎngyìng\", \"trans\": \"response\"},\n    {\"word\": \"原因\", \"pinyin\": \"yuányīn\", \"trans\": \"reason\"},\n    {\"word\": \"对\", \"pinyin\": \"duì\", \"trans\": \"towards\"},\n    {\"word\": \"图像\", \"pinyin\": \"túxiàng\", \"trans\": \"image\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐjiě\", \"trans\": \"understanding\"},\n    {\"word\": \"幻觉\", \"pinyin\": \"huànjué\", \"trans\": \"illusion\"},\n    {\"word\": \"未\", \"pinyin\": \"wèi\", \"trans\": \"not yet\"},\n    {\"word\": \"精炼\", \"pinyin\": \"jīngliàn\", \"trans\": \"refined\"},\n    {\"word\": \"路径\", \"pinyin\": \"lùjìng\", \"trans\": \"path\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiějué\", \"trans\": \"solve\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎozhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐnrù\", \"trans\": \"introduce\"},\n    {\"word\": \"Critic-V\", \"pinyin\": \"Critic-V\", \"trans\": \"Critic-V\"},\n    {\"word\": \"受\", \"pinyin\": \"shòu\", \"trans\": \"inspired by\"},\n    {\"word\": \"Actor-Critic\", \"pinyin\": \"Actor-Critic\", \"trans\": \"Actor-Critic\"},\n    {\"word\": \"范式\", \"pinyin\": \"fànshì\", \"trans\": \"paradigm\"},\n    {\"word\": \"启发\", \"pinyin\": \"qǐfā\", \"trans\": \"inspiration\"},\n    {\"word\": \"新\", \"pinyin\": \"xīn\", \"trans\": \"new\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàngjià\", \"trans\": \"framework\"},\n    {\"word\": \"以\", \"pinyin\": \"yǐ\", \"trans\": \"in order to\"},\n    {\"word\": \"提升\", \"pinyin\": \"tíshēng\", \"trans\": \"enhance\"},\n    {\"word\": \"能力\", \"pinyin\": \"nénglì\", \"trans\": \"ability\"},\n    {\"word\": \"该\", \"pinyin\": \"gǎi\", \"trans\": \"this\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōngguò\", \"trans\": \"through\"},\n    {\"word\": \"整合\", \"pinyin\": \"zhěnghé\", \"trans\": \"integrate\"},\n    {\"word\": \"两个\", \"pinyin\": \"liǎng gè\", \"trans\": \"two\"},\n    {\"word\": \"独立\", \"pinyin\": \"dúlì\", \"trans\": \"independent\"},\n    {\"word\": \"组件\", \"pinyin\": \"zǔjiàn\", \"trans\": \"component\"},\n    {\"word\": \"解耦\", \"pinyin\": \"jiě'ǒu\", \"trans\": \"decouple\"},\n    {\"word\": \"过程\", \"pinyin\": \"guòchéng\", \"trans\": \"process\"},\n    {\"word\": \"评论\", \"pinyin\": \"pínglùn\", \"trans\": \"comment\"},\n    {\"word\": \"Reasoner\", \"pinyin\": \"Reasoner\", \"trans\": \"Reasoner\"},\n    {\"word\": \"基于\", \"pinyin\": \"jīyú\", \"trans\": \"based on\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shìjué\", \"trans\": \"visual\"},\n    {\"word\": \"文本\", \"pinyin\": \"wénběn\", \"trans\": \"text\"},\n    {\"word\": \"输入\", \"pinyin\": \"shūrù\", \"trans\": \"input\"},\n    {\"word\": \"Critic\", \"pinyin\": \"Critic\", \"trans\": \"Critic\"},\n    {\"word\": \"提供\", \"pinyin\": \"tígōng\", \"trans\": \"provide\"},\n    {\"word\": \"建设性\", \"pinyin\": \"jiànshèxìng\", \"trans\": \"constructive\"},\n    {\"word\": \"完善\", \"pinyin\": \"wánshàn\", \"trans\": \"improve\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluation\"},\n    {\"word\": \"结果\", \"pinyin\": \"jiéguǒ\", \"trans\": \"result\"},\n    {\"word\": \"显示\", \"pinyin\": \"xiǎnshì\", \"trans\": \"show\"},\n    {\"word\": \"在\", \"pinyin\": \"zài\", \"trans\": \"in\"},\n    {\"word\": \"5个\", \"pinyin\": \"wǔ gè\", \"trans\": \"5\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"测试\", \"pinyin\": \"cèshì\", \"trans\": \"test\"},\n    {\"word\": \"中\", \"pinyin\": \"zhōng\", \"trans\": \"among\"},\n    {\"word\": \"显著优于\", \"pinyin\": \"xiǎnzhù yōuyú\", \"trans\": \"significantly superior to\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiànyǒu\", \"trans\": \"existing\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāngfǎ\", \"trans\": \"method\"},\n    {\"word\": \"包括\", \"pinyin\": \"bāokuò\", \"trans\": \"include\"},\n    {\"word\": \"GPT-4V\", \"pinyin\": \"GPT-4V\", \"trans\": \"GPT-4V\"},\n    {\"word\": \"特别\", \"pinyin\": \"tèbié\", \"trans\": \"especially\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiàolǜ\", \"trans\": \"efficiency\"}\n]",
        "trans": "Vision-language models (VLMs) have made significant progress in multimodal reasoning tasks. However, they often generate inaccurate or irrelevant responses due to illusions in image understanding or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a new framework inspired by the Actor-Critic paradigm to enhance the reasoning capabilities of VLMs. This framework decouples the reasoning process and the critique process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critiques to refine these paths. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, particularly in terms of reasoning accuracy and efficiency, across five benchmark tests.",
        "update_ts": "2024-11-29 09:08"
    }
}