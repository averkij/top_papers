{
    "date": {
        "ru": "29 ноября",
        "en": "November 29",
        "zh": "11月29日"
    },
    "time_utc": "2024-11-30 12:40",
    "weekday": 4,
    "issue_id": 876,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.18203",
            "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2411.18203",
            "abstract": "Vision-language models~(VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.",
            "score": 19,
            "issue_id": 863,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "13bce174f2b29a74",
            "authors": [
                "Di Zhang",
                "Jingdi Lei",
                "Junxian Li",
                "Xunzhi Wang",
                "Yujie Liu",
                "Zonglin Yang",
                "Jiatong Li",
                "Weida Wang",
                "Suorong Yang",
                "Jianbo Wu",
                "Peng Ye",
                "Wanli Ouyang",
                "Dongzhan Zhou"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Fudan University",
                "Hong Kong Polytechnic University",
                "Nanjing University",
                "Nankai University",
                "Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiaotong University",
                "Shanghai University",
                "Tongji University",
                "University of California, Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18203.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#reasoning",
                    "#multimodal",
                    "#rlhf",
                    "#hallucinations",
                    "#architecture"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Critic-V: улучшение мультимодальных рассуждений с помощью обучения с подкреплением",
                    "desc": "Статья представляет Critic-V - новый фреймворк для улучшения способностей мультимодальных моделей к рассуждению. Фреймворк разделяет процессы рассуждения и критики, используя два независимых компонента: Reasoner и Critic. Critic обучается с помощью Direct Preference Optimization на основе ранжированных критических отзывов. Результаты показывают, что Critic-V превосходит существующие методы, включая GPT-4V, на большинстве тестовых наборов данных."
                },
                "en": {
                    "title": "Enhancing VLMs with Critic-V: A New Era in Multimodal Reasoning",
                    "desc": "This paper presents Critic-V, a new framework designed to improve the reasoning abilities of vision-language models (VLMs) in multimodal tasks. It separates the reasoning and critique processes by using two components: the Reasoner, which creates reasoning paths from visual and textual data, and the Critic, which evaluates and refines these paths. The Critic provides feedback in the form of natural language critiques, rather than simple rewards, allowing for more detailed guidance in the reasoning process. The results demonstrate that Critic-V significantly enhances reasoning accuracy and efficiency compared to existing models, making it a valuable advancement for applications requiring complex reasoning."
                },
                "zh": {
                    "title": "Critic-V：提升视觉语言模型推理能力的新框架",
                    "desc": "本文介绍了一种新的框架Critic-V，旨在提升视觉语言模型（VLMs）的推理能力。该框架借鉴了演员-评论家（Actor-Critic）范式，将推理过程与评论过程解耦，分别由推理器和评论器两个独立组件完成。推理器根据视觉和文本输入生成推理路径，而评论器则提供建设性的反馈以优化这些路径。通过这种互动过程，Critic-V在复杂推理任务中显著提高了VLMs的准确性和效率，展示了在多模态推理应用中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17176",
            "title": "ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting",
            "url": "https://huggingface.co/papers/2411.17176",
            "abstract": "Despite the significant advancements in text-to-image (T2I) generative models, users often face a trial-and-error challenge in practical scenarios. This challenge arises from the complexity and uncertainty of tedious steps such as crafting suitable prompts, selecting appropriate models, and configuring specific arguments, making users resort to labor-intensive attempts for desired images. This paper proposes Automatic T2I generation, which aims to automate these tedious steps, allowing users to simply describe their needs in a freestyle chatting way. To systematically study this problem, we first introduce ChatGenBench, a novel benchmark designed for Automatic T2I. It features high-quality paired data with diverse freestyle inputs, enabling comprehensive evaluation of automatic T2I models across all steps. Additionally, recognizing Automatic T2I as a complex multi-step reasoning task, we propose ChatGen-Evo, a multi-stage evolution strategy that progressively equips models with essential automation skills. Through extensive evaluation across step-wise accuracy and image quality, ChatGen-Evo significantly enhances performance over various baselines. Our evaluation also uncovers valuable insights for advancing automatic T2I. All our data, code, and models will be available in https://chengyou-jia.github.io/ChatGen-Home",
            "score": 13,
            "issue_id": 863,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "1abb1fa2701cf0cc",
            "authors": [
                "Chengyou Jia",
                "Changliang Xia",
                "Zhuohang Dang",
                "Weijia Wu",
                "Hangwei Qian",
                "Minnan Luo"
            ],
            "affiliations": [
                "CFAR, A*STAR",
                "National University of Singapore",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17176.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#reasoning",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Автоматизация генерации изображений по тексту: от описания к результату",
                    "desc": "Статья представляет новый подход к автоматической генерации изображений по тексту (Automatic T2I). Авторы предлагают ChatGenBench - набор данных для оценки моделей Automatic T2I, содержащий разнообразные входные данные. Они также разрабатывают ChatGen-Evo - стратегию многоэтапной эволюции для обучения моделей необходимым навыкам автоматизации. Результаты показывают значительное улучшение производительности по сравнению с базовыми моделями в точности генерации и качестве изображений."
                },
                "en": {
                    "title": "Streamlining Text-to-Image Generation with Automation",
                    "desc": "This paper addresses the challenges users face when generating images from text descriptions using text-to-image (T2I) models. It introduces Automatic T2I generation, which simplifies the process by allowing users to communicate their needs in a conversational manner. The authors present ChatGenBench, a new benchmark that provides high-quality paired data for evaluating T2I models. Additionally, they propose ChatGen-Evo, a multi-stage evolution strategy that enhances model performance through systematic automation of the T2I generation steps."
                },
                "zh": {
                    "title": "自动化文本到图像生成，简化创作过程",
                    "desc": "尽管文本到图像生成模型取得了显著进展，但用户在实际应用中常常面临反复试验的挑战。这个挑战源于创建合适提示、选择合适模型和配置特定参数等繁琐步骤的复杂性和不确定性。本文提出了自动化文本到图像生成，旨在简化这些繁琐步骤，用户只需以自由聊天的方式描述需求。我们还引入了ChatGenBench，一个新颖的基准，旨在系统性地评估自动化文本到图像生成模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18350",
            "title": "TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models",
            "url": "https://huggingface.co/papers/2411.18350",
            "abstract": "This paper introduces Virtual Try-Off (VTOFF), a novel task focused on generating standardized garment images from single photos of clothed individuals. Unlike traditional Virtual Try-On (VTON), which digitally dresses models, VTOFF aims to extract a canonical garment image, posing unique challenges in capturing garment shape, texture, and intricate patterns. This well-defined target makes VTOFF particularly effective for evaluating reconstruction fidelity in generative models. We present TryOffDiff, a model that adapts Stable Diffusion with SigLIP-based visual conditioning to ensure high fidelity and detail retention. Experiments on a modified VITON-HD dataset show that our approach outperforms baseline methods based on pose transfer and virtual try-on with fewer pre- and post-processing steps. Our analysis reveals that traditional image generation metrics inadequately assess reconstruction quality, prompting us to rely on DISTS for more accurate evaluation. Our results highlight the potential of VTOFF to enhance product imagery in e-commerce applications, advance generative model evaluation, and inspire future work on high-fidelity reconstruction. Demo, code, and models are available at: https://rizavelioglu.github.io/tryoffdiff/",
            "score": 12,
            "issue_id": 863,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "d5e7e012abbc35e9",
            "authors": [
                "Riza Velioglu",
                "Petra Bevandic",
                "Robin Chan",
                "Barbara Hammer"
            ],
            "affiliations": [
                "Machine Learning Group, CITEC, Bielefeld University, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18350.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#cv",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "👚",
                "ru": {
                    "title": "Виртуальная примерка наизнанку: от фото к стандартизированному изображению одежды",
                    "desc": "Эта статья представляет новую задачу под названием Virtual Try-Off (VTOFF), которая фокусируется на генерации стандартизированных изображений одежды из одиночных фотографий одетых людей. Авторы разработали модель TryOffDiff, адаптирующую Stable Diffusion с визуальным кондиционированием на основе SigLIP для обеспечения высокой точности и сохранения деталей. Эксперименты на модифицированном наборе данных VITON-HD показали, что предложенный подход превосходит базовые методы. Исследование выявило, что традиционные метрики генерации изображений недостаточно хорошо оценивают качество реконструкции, что побудило авторов использовать DISTS для более точной оценки."
                },
                "en": {
                    "title": "Revolutionizing Garment Imaging with VTOFF!",
                    "desc": "This paper presents Virtual Try-Off (VTOFF), a new task that generates standardized images of garments from photos of people wearing them. Unlike traditional Virtual Try-On (VTON), which focuses on dressing models, VTOFF extracts a clear image of the garment, addressing challenges like capturing its shape and texture. The authors introduce TryOffDiff, a model that enhances Stable Diffusion with visual conditioning to improve detail and fidelity in garment representation. Their experiments demonstrate that VTOFF outperforms existing methods in generating high-quality garment images with fewer processing steps, while also suggesting better evaluation metrics for generative models."
                },
                "zh": {
                    "title": "虚拟试衣：提升服装图像生成的创新方法",
                    "desc": "本文介绍了一种新任务，称为虚拟试衣（VTOFF），旨在从穿着者的单张照片生成标准化的服装图像。与传统的虚拟试穿（VTON）不同，VTOFF专注于提取服装的典型图像，这在捕捉服装形状、纹理和复杂图案方面面临独特挑战。我们提出的TryOffDiff模型通过结合稳定扩散和基于SigLIP的视觉条件，确保生成图像的高保真度和细节保留。实验结果表明，我们的方法在重建质量评估上优于基于姿态转移和虚拟试穿的基线方法，展示了VTOFF在电子商务产品图像增强中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17041",
            "title": "Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models",
            "url": "https://huggingface.co/papers/2411.17041",
            "abstract": "Diffusion models have achieved impressive results in generative tasks like text-to-image (T2I) and text-to-video (T2V) synthesis. However, achieving accurate text alignment in T2V generation remains challenging due to the complex temporal dependency across frames. Existing reinforcement learning (RL)-based approaches to enhance text alignment often require differentiable reward functions or are constrained to limited prompts, hindering their scalability and applicability. In this paper, we propose Free^2Guide, a novel gradient-free framework for aligning generated videos with text prompts without requiring additional model training. Leveraging principles from path integral control, Free^2Guide approximates guidance for diffusion models using non-differentiable reward functions, thereby enabling the integration of powerful black-box Large Vision-Language Models (LVLMs) as reward model. Additionally, our framework supports the flexible ensembling of multiple reward models, including large-scale image-based models, to synergistically enhance alignment without incurring substantial computational overhead. We demonstrate that Free^2Guide significantly improves text alignment across various dimensions and enhances the overall quality of generated videos.",
            "score": 6,
            "issue_id": 863,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "adf6ad0f69ef7df2",
            "authors": [
                "Jaemin Kim",
                "Bryan S Kim",
                "Jong Chul Ye"
            ],
            "affiliations": [
                "Graduate School of AI, KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17041.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#rl",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Свободное руководство для точной генерации видео по тексту",
                    "desc": "Статья представляет Free^2Guide - новый подход к улучшению соответствия генерируемых видео текстовым запросам без дополнительного обучения моделей. Метод основан на принципах интегрального управления и позволяет использовать недифференцируемые функции вознаграждения, включая крупные мультимодальные модели. Free^2Guide поддерживает ансамблирование нескольких моделей вознаграждения для синергетического улучшения соответствия. Эксперименты показывают, что подход значительно повышает качество генерируемых видео и их соответствие текстовым запросам."
                },
                "en": {
                    "title": "Free^2Guide: Enhancing Text Alignment in Video Generation Without Training",
                    "desc": "This paper introduces Free^2Guide, a new method for improving text alignment in text-to-video (T2V) generation using diffusion models. Unlike traditional reinforcement learning approaches that require differentiable rewards, Free^2Guide operates without additional model training, making it more scalable. It utilizes non-differentiable reward functions and integrates large vision-language models to guide the video generation process. The results show that Free^2Guide enhances text alignment and overall video quality significantly."
                },
                "zh": {
                    "title": "无梯度框架，提升视频生成文本对齐",
                    "desc": "扩散模型在生成任务中取得了显著成果，如文本到图像（T2I）和文本到视频（T2V）合成。然而，在T2V生成中实现准确的文本对齐仍然具有挑战性，因为帧之间存在复杂的时间依赖关系。现有的基于强化学习（RL）的方法通常需要可微分的奖励函数或受到有限提示的限制，影响了它们的可扩展性和适用性。我们提出了Free^2Guide，这是一种新颖的无梯度框架，可以在不需要额外模型训练的情况下，将生成的视频与文本提示对齐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17863",
            "title": "LongKey: Keyphrase Extraction for Long Documents",
            "url": "https://huggingface.co/papers/2411.17863",
            "abstract": "In an era of information overload, manually annotating the vast and growing corpus of documents and scholarly papers is increasingly impractical. Automated keyphrase extraction addresses this challenge by identifying representative terms within texts. However, most existing methods focus on short documents (up to 512 tokens), leaving a gap in processing long-context documents. In this paper, we introduce LongKey, a novel framework for extracting keyphrases from lengthy documents, which uses an encoder-based language model to capture extended text intricacies. LongKey uses a max-pooling embedder to enhance keyphrase candidate representation. Validated on the comprehensive LDKP datasets and six diverse, unseen datasets, LongKey consistently outperforms existing unsupervised and language model-based keyphrase extraction methods. Our findings demonstrate LongKey's versatility and superior performance, marking an advancement in keyphrase extraction for varied text lengths and domains.",
            "score": 5,
            "issue_id": 863,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "3d8fc08dd3b125a4",
            "authors": [
                "Jeovane Honorio Alves",
                "Radu State",
                "Cinthia Obladen de Almendra Freitas",
                "Jean Paul Barddal"
            ],
            "affiliations": [
                "Graduate Program in Informatics (PPGIa) Pontifícia Universidade Católica do Paraná",
                "Graduate Program in Law (PPGD) Pontifícia Universidade Católica do Paraná",
                "SEDAN - SnT University of Luxembourg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17863.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#data",
                    "#long_context"
                ],
                "emoji": "🔑",
                "ru": {
                    "title": "LongKey: Прорыв в извлечении ключевых фраз из длинных документов",
                    "desc": "Статья представляет LongKey - новую систему для извлечения ключевых фраз из длинных документов. LongKey использует языковую модель на основе энкодера для обработки сложных текстов и применяет max-pooling для улучшения представления кандидатов в ключевые фразы. Система была протестирована на наборе данных LDKP и шести других разнородных датасетах, превзойдя существующие методы извлечения ключевых фраз. LongKey демонстрирует универсальность и превосходную производительность для текстов различной длины и тематики."
                },
                "en": {
                    "title": "Unlocking Keyphrases in Long Texts with LongKey",
                    "desc": "This paper presents LongKey, a new framework designed for extracting keyphrases from long documents, addressing the limitations of existing methods that only work with shorter texts. LongKey utilizes an encoder-based language model to effectively understand and process the complexities of extended text. It incorporates a max-pooling embedder to improve the representation of keyphrase candidates, enhancing the extraction process. The framework has been tested on various datasets, showing that it outperforms current unsupervised and language model-based approaches in keyphrase extraction."
                },
                "zh": {
                    "title": "LongKey：长文档关键短语提取的新突破",
                    "desc": "在信息过载的时代，手动标注大量文档和学术论文变得越来越不切实际。自动关键短语提取通过识别文本中的代表性术语来应对这一挑战。现有方法大多集中在短文档上，处理长文本的能力不足。本文介绍了LongKey，一个新颖的框架，利用编码器基础的语言模型提取长文档中的关键短语，展示了其在多种文本长度和领域中的优越性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15640",
            "title": "AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering Benchmark Dataset",
            "url": "https://huggingface.co/papers/2411.15640",
            "abstract": "Recent advancements in large language model(LLM) performance on medical multiple choice question (MCQ) benchmarks have stimulated interest from healthcare providers and patients globally. Particularly in low-and middle-income countries (LMICs) facing acute physician shortages and lack of specialists, LLMs offer a potentially scalable pathway to enhance healthcare access and reduce costs. However, their effectiveness in the Global South, especially across the African continent, remains to be established. In this work, we introduce AfriMed-QA, the first large scale Pan-African English multi-specialty medical Question-Answering (QA) dataset, 15,000 questions (open and closed-ended) sourced from over 60 medical schools across 16 countries, covering 32 medical specialties. We further evaluate 30 LLMs across multiple axes including correctness and demographic bias. Our findings show significant performance variation across specialties and geographies, MCQ performance clearly lags USMLE (MedQA). We find that biomedical LLMs underperform general models and smaller edge-friendly LLMs struggle to achieve a passing score. Interestingly, human evaluations show a consistent consumer preference for LLM answers and explanations when compared with clinician answers.",
            "score": 2,
            "issue_id": 872,
            "pub_date": "2024-11-23",
            "pub_date_card": {
                "ru": "23 ноября",
                "en": "November 23",
                "zh": "11月23日"
            },
            "hash": "759b4f0b1436b56c",
            "authors": [
                "Tobi Olatunji",
                "Charles Nimo",
                "Abraham Owodunni",
                "Tassallah Abdullahi",
                "Emmanuel Ayodele",
                "Mardhiyah Sanni",
                "Chinemelu Aka",
                "Folafunmi Omofoye",
                "Foutse Yuehgoh",
                "Timothy Faniran",
                "Bonaventure F. P. Dossou",
                "Moshood Yekini",
                "Jonas Kemp",
                "Katherine Heller",
                "Jude Chidubem Omeke",
                "Chidi Asuzu MD",
                "Naome A. Etori",
                "Aimérou Ndiaye",
                "Ifeoma Okoh",
                "Evans Doe Ocansey",
                "Wendy Kinara",
                "Michael Best",
                "Irfan Essa",
                "Stephen Edward Moore",
                "Chris Fourie",
                "Mercy Nyamewaa Asiedu"
            ],
            "affiliations": [
                "BioRAMP",
                "Brown University",
                "Georgia Institute of Technology",
                "Google Research",
                "Intron",
                "Kenyatta University",
                "MILA Quebec",
                "Masakhane",
                "SisonkeBiotik",
                "The Ohio State University",
                "University of Cape Coast",
                "University of Minnesota"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15640.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#benchmark",
                    "#ethics",
                    "#healthcare",
                    "#dataset",
                    "#science"
                ],
                "emoji": "🌍",
                "ru": {
                    "title": "AfriMed-QA: Оценка языковых моделей в африканской медицине",
                    "desc": "Статья представляет AfriMed-QA - первый крупномасштабный панафриканский набор данных для медицинских вопросов и ответов на английском языке. Исследователи оценили производительность 30 различных языковых моделей (LLM) на этом наборе данных, охватывающем 32 медицинские специальности. Результаты показывают значительные различия в эффективности моделей в зависимости от специальности и географии, при этом биомедицинские LLM уступают общим моделям. Интересно, что пользователи предпочитают ответы и объяснения LLM по сравнению с ответами врачей."
                },
                "en": {
                    "title": "Empowering Healthcare in Africa with AI: AfriMed-QA",
                    "desc": "This paper presents AfriMed-QA, a comprehensive medical Question-Answering dataset specifically designed for the African context, containing 15,000 questions from various medical specialties. The study evaluates the performance of 30 large language models (LLMs) on this dataset, highlighting significant variations in their effectiveness based on medical specialty and geographic region. The results indicate that while LLMs show promise, their performance is generally lower than that of established medical exams like the USMLE. Notably, human evaluations reveal that users prefer the answers and explanations provided by LLMs over those from clinicians, suggesting a potential role for LLMs in enhancing medical education and access in low-resource settings."
                },
                "zh": {
                    "title": "提升医疗服务的智能问答新路径",
                    "desc": "这篇论文介绍了AfriMed-QA，这是第一个大规模的泛非英语多专业医学问答数据集，包含来自16个国家的60多所医学院的15,000个问题。研究评估了30种大型语言模型（LLM）的表现，包括正确性和人口偏见等多个方面。结果显示，不同专业和地区的表现差异显著，医学多项选择题的表现明显低于美国医学执照考试（USMLE）。尽管生物医学LLM的表现不如通用模型，但人类评估显示消费者更倾向于LLM的答案和解释。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17190",
            "title": "SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian Splatting",
            "url": "https://huggingface.co/papers/2411.17190",
            "abstract": "We propose SelfSplat, a novel 3D Gaussian Splatting model designed to perform pose-free and 3D prior-free generalizable 3D reconstruction from unposed multi-view images. These settings are inherently ill-posed due to the lack of ground-truth data, learned geometric information, and the need to achieve accurate 3D reconstruction without finetuning, making it difficult for conventional methods to achieve high-quality results. Our model addresses these challenges by effectively integrating explicit 3D representations with self-supervised depth and pose estimation techniques, resulting in reciprocal improvements in both pose accuracy and 3D reconstruction quality. Furthermore, we incorporate a matching-aware pose estimation network and a depth refinement module to enhance geometry consistency across views, ensuring more accurate and stable 3D reconstructions. To present the performance of our method, we evaluated it on large-scale real-world datasets, including RealEstate10K, ACID, and DL3DV. SelfSplat achieves superior results over previous state-of-the-art methods in both appearance and geometry quality, also demonstrates strong cross-dataset generalization capabilities. Extensive ablation studies and analysis also validate the effectiveness of our proposed methods. Code and pretrained models are available at https://gynjn.github.io/selfsplat/",
            "score": 1,
            "issue_id": 875,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "10aa20eb9a3ab0c1",
            "authors": [
                "Gyeongjin Kang",
                "Jisang Yoo",
                "Jihyeon Park",
                "Seungtae Nam",
                "Hyeonsoo Im",
                "Sangheon Shin",
                "Sangpil Kim",
                "Eunbyung Park"
            ],
            "affiliations": [
                "Hanhwa Systems",
                "Korea University",
                "Sungkyunkwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17190.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SelfSplat: Прорыв в 3D-реконструкции без предварительных знаний",
                    "desc": "SelfSplat - это новая модель 3D Gaussian Splatting для обобщаемой 3D-реконструкции из непозиционированных многоракурсных изображений. Модель объединяет явные 3D-представления с самоконтролируемыми методами оценки глубины и позы. Включает сеть оценки позы с учетом соответствий и модуль уточнения глубины для улучшения геометрической согласованности между ракурсами. SelfSplat превосходит современные методы по качеству внешнего вида и геометрии, демонстрируя сильные возможности обобщения на разные наборы данных."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction with SelfSplat",
                    "desc": "SelfSplat is a new model for 3D reconstruction that works without needing specific poses or prior 3D information from images. It tackles the challenges of reconstructing 3D shapes from unposed multi-view images, which is usually difficult due to the lack of ground-truth data. The model combines 3D representations with self-supervised techniques for estimating depth and pose, leading to better accuracy in both pose estimation and 3D reconstruction. Additionally, it includes a network for pose estimation and a module for refining depth, which improves the consistency of the geometry across different views, resulting in high-quality 3D outputs."
                },
                "zh": {
                    "title": "SelfSplat：无姿态3D重建的新突破",
                    "desc": "我们提出了一种新颖的3D高斯点云模型SelfSplat，旨在从未标定的多视角图像中进行无姿态和无3D先验的可泛化3D重建。这种设置由于缺乏真实数据和学习的几何信息，导致传统方法难以实现高质量的3D重建。我们的模型通过有效整合显式3D表示与自监督深度和姿态估计技术，显著提高了姿态准确性和3D重建质量。此外，我们还引入了匹配感知的姿态估计网络和深度精细化模块，以增强视图间的几何一致性，确保更准确和稳定的3D重建。"
                }
            }
        }
    ],
    "link_prev": "2024-11-28.html",
    "link_next": "2024-12-02.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "28.11",
        "en": "11/28",
        "zh": "11月28日"
    },
    "short_date_next": {
        "ru": "02.12",
        "en": "12/02",
        "zh": "12月2日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 0,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "视觉-语言模型（VLMs）在多模态推理任务中取得了显著进展。然而，它们常常生成不准确或无关的响应，原因是对图像理解的幻觉或未精炼的推理路径。为解决这些挑战，我们引入了Critic-V，一个受Actor-Critic范式启发的新框架，以提升VLMs的推理能力。该框架将推理过程和评论过程分离，通过整合两个独立组件：根据视觉和文本输入生成推理路径的Reasoner，以及提供建设性批评以精炼这些路径的Critic。评估结果显示，Critic-V框架在5个8个基准测试中显著优于现有方法，特别是在推理准确性和效率方面。",
        "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
        "pinyin": "Shìjué-yǔyán móxíng (VLMs) zài duō móshī tuīlǐ rènwù zhōng qǔdéle xiǎnzhù jìnbù. Rán'ér, tāmen chángcháng shēngchéng bù zhǔnquè huò wúguān de xiǎngyìng, yuányīn shì duì túxiàng lǐjiě de huànjué huò wèi jīngliàn de tuīlǐ lùjìng. Wèi jiějué zhèxiē tiǎozhàn, wǒmen yǐn rùle Critic-V, yīgè shòu Actor-Critic fànshì qǐfà de xīn kuàngjià, yǐ tíshēng VLMs de tuīlǐ nénglì. Gāi kuàngjià jiāng tuīlǐ guòchéng hé pínglùn guòchéng fēnlí, tōngguò zhěnghé liǎng gè dúlì zǔjiàn: gēnjù shìjué hé wénběn shūrù shēngchéng tuīlǐ lùjìng de Reasoner, yǐjià tígōng jiànshèxìng pīpíng yǐ jīngliàn zhèxiē lùjìng de Critic. Píngjià jiéguǒ xiǎnshì, Critic-V kuàngjià zài 5 gè 8 gè jīzhǔn cèshì zhōng xiǎnzhù yōu xiànzhài yǐcún méifǎ, tèbié shì zài tuīlǐ zhǔnquèxìng hé xiàolǜ fāngmiàn.",
        "vocab": "[{'word': '视觉', 'pinyin': 'shìjué', 'trans': 'vision'},\n{'word': '语言', 'pinyin': 'yǔyán', 'trans': 'language'},\n{'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'},\n{'word': '多模态', 'pinyin': 'duōmómó', 'trans': 'multimodal'},\n{'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'reasoning'},\n{'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'},\n{'word': '取得', 'pinyin': 'qǔdé', 'trans': 'achieve'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '进展', 'pinyin': 'jìnzhǎn', 'trans': 'progress'},\n{'word': '然而', 'pinyin': 'rán'ér', 'trans': 'however'},\n{'word': '常常', 'pinyin': 'chángcháng', 'trans': 'often'},\n{'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'},\n{'word': '不准确', 'pinyin': 'bùzhǔnquè', 'trans': 'inaccurate'},\n{'word': '无关', 'pinyin': 'wúguān', 'trans': 'irrelevant'},\n{'word': '响应', 'pinyin': 'xiǎngyìng', 'trans': 'response'},\n{'word': '原因', 'pinyin': 'yuányīn', 'trans': 'reason'},\n{'word': '幻觉', 'pinyin': 'huànjué', 'trans': 'illusion'},\n{'word': '未精炼', 'pinyin': 'wèijīngliàn', 'trans': 'unrefined'},\n{'word': '路径', 'pinyin': 'lùjìng', 'trans': 'path'},\n{'word': '解决', 'pinyin': 'jiějué', 'trans': 'solve'},\n{'word': '挑战', 'pinyin': 'tiǎozhàn', 'trans': 'challenge'},\n{'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'},\n{'word': 'Critic-V', 'pinyin': 'Kèlìtík-V', 'trans': 'Critic-V'},\n{'word': '受', 'pinyin': 'shòu', 'trans': 'inspired by'},\n{'word': 'Actor-Critic', 'pinyin': 'Éikètà-Kèlìtík', 'trans': 'Actor-Critic'},\n{'word': '范式', 'pinyin': 'fànshì', 'trans': 'paradigm'},\n{'word': '新', 'pinyin': 'xīn', 'trans': 'new'},\n{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '提升', 'pinyin': 'tíshēng', 'trans': 'enhance'},\n{'word': '能力', 'pinyin': 'nénglì', 'trans': 'capability'},\n{'word': '分离', 'pinyin': 'fēnlí', 'trans': 'separate'},\n{'word': '过程', 'pinyin': 'guòchéng', 'trans': 'process'},\n{'word': '评论', 'pinyin': 'pínglùn', 'trans': 'comment'},\n{'word': '整合', 'pinyin': 'zhěnghé', 'trans': 'integrate'},\n{'word': '独立', 'pinyin': 'dúlì', 'trans': 'independent'},\n{'word': '组件', 'pinyin': 'zǔjiàn', 'trans': 'component'},\n{'word': '根据', 'pinyin': 'gēnjù', 'trans': 'based on'},\n{'word': '输入', 'pinyin': 'shūrù', 'trans': 'input'},\n{'word': 'Reasoner', 'pinyin': 'Rízǒnà', 'trans': 'Reasoner'},\n{'word': '提供', 'pinyin': 'tígōng', 'trans': 'provide'},\n{'word': '建设性', 'pinyin': 'jiànshèxìng', 'trans': 'constructive'},\n{'word': '批评', 'pinyin': 'pīpíng', 'trans': 'criticism'},\n{'word': '精炼', 'pinyin': 'jīngliàn', 'trans': 'refine'},\n{'word': '评估', 'pinyin': 'pínggū', 'trans': 'evaluation'},\n{'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'},\n{'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'},\n{'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'},\n{'word': '测试', 'pinyin': 'cèshì', 'trans': 'test'},\n{'word': '优于', 'pinyin': 'yōuyú', 'trans': 'superior to'},\n{'word': '现有', 'pinyin': 'xiànyǒu', 'trans': 'existing'},\n{'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'},\n{'word': '特别', 'pinyin': 'tèbié', 'trans': 'especially'},\n{'word': '效率', 'pinyin': 'xiàolǜ', 'trans': 'efficiency'}]",
        "trans": "Visual-language models (VLMs) have made significant progress in multimodal reasoning tasks. However, they often generate inaccurate or irrelevant responses due to illusions in image understanding or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a new framework inspired by the Actor-Critic paradigm to enhance the reasoning capabilities of VLMs. This framework separates the reasoning process from the critique process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive criticism to refine these paths. Evaluation results show that the Critic-V framework significantly outperforms existing methods on 5 out of 8 benchmark tests, particularly in terms of reasoning accuracy and efficiency.",
        "update_ts": "2024-11-30 12:40"
    }
}