{
    "date": {
        "ru": "29 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 29",
        "zh": "11æœˆ29æ—¥"
    },
    "time_utc": "2024-11-29 09:11",
    "weekday": 4,
    "issue_id": 857,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.18203",
            "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2411.18203",
            "abstract": "Vision-language models~(VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.",
            "score": 10,
            "issue_id": 853,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "13bce174f2b29a74",
            "authors": [
                "Di Zhang",
                "Jingdi Lei",
                "Junxian Li",
                "Xunzhi Wang",
                "Yujie Liu",
                "Zonglin Yang",
                "Jiatong Li",
                "Weida Wang",
                "Suorong Yang",
                "Jianbo Wu",
                "Peng Ye",
                "Wanli Ouyang",
                "Dongzhan Zhou"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Fudan University",
                "Hong Kong Polytechnic University",
                "Nanjing University",
                "Nankai University",
                "Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiaotong University",
                "Shanghai University",
                "Tongji University",
                "University of California, Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18203.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rlhf",
                    "#reasoning",
                    "#hallucinations",
                    "#rl",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Critic-V: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Critic-V Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Reasoner, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ Critic, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Critic Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Critic-V Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4V, Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing VLMs with Critic-V: A New Era in Multimodal Reasoning",
                    "desc": "This paper presents Critic-V, a new framework designed to improve the reasoning abilities of vision-language models (VLMs) in multimodal tasks. It separates the reasoning and critique processes by using two components: the Reasoner, which creates reasoning paths from visual and textual data, and the Critic, which evaluates and refines these paths. The Critic provides feedback in the form of natural language critiques, rather than simple rewards, allowing for more detailed guidance in the reasoning process. The results demonstrate that Critic-V significantly enhances reasoning accuracy and efficiency compared to existing models, making it a valuable advancement for applications requiring complex reasoning."
                },
                "zh": {
                    "title": "Critic-Vï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶Critic-Vï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å€Ÿé‰´äº†æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆActor-Criticï¼‰èŒƒå¼ï¼Œå°†æ¨ç†è¿‡ç¨‹ä¸è¯„è®ºè¿‡ç¨‹è§£è€¦ï¼Œåˆ†åˆ«ç”±æ¨ç†å™¨å’Œè¯„è®ºå™¨ä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶å®Œæˆã€‚æ¨ç†å™¨æ ¹æ®è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆæ¨ç†è·¯å¾„ï¼Œè€Œè¯„è®ºå™¨åˆ™æä¾›å»ºè®¾æ€§çš„åé¦ˆä»¥ä¼˜åŒ–è¿™äº›è·¯å¾„ã€‚é€šè¿‡è¿™ç§äº’åŠ¨è¿‡ç¨‹ï¼ŒCritic-Våœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†VLMsçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå±•ç¤ºäº†åœ¨å¤šæ¨¡æ€æ¨ç†åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17176",
            "title": "ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting",
            "url": "https://huggingface.co/papers/2411.17176",
            "abstract": "Despite the significant advancements in text-to-image (T2I) generative models, users often face a trial-and-error challenge in practical scenarios. This challenge arises from the complexity and uncertainty of tedious steps such as crafting suitable prompts, selecting appropriate models, and configuring specific arguments, making users resort to labor-intensive attempts for desired images. This paper proposes Automatic T2I generation, which aims to automate these tedious steps, allowing users to simply describe their needs in a freestyle chatting way. To systematically study this problem, we first introduce ChatGenBench, a novel benchmark designed for Automatic T2I. It features high-quality paired data with diverse freestyle inputs, enabling comprehensive evaluation of automatic T2I models across all steps. Additionally, recognizing Automatic T2I as a complex multi-step reasoning task, we propose ChatGen-Evo, a multi-stage evolution strategy that progressively equips models with essential automation skills. Through extensive evaluation across step-wise accuracy and image quality, ChatGen-Evo significantly enhances performance over various baselines. Our evaluation also uncovers valuable insights for advancing automatic T2I. All our data, code, and models will be available in https://chengyou-jia.github.io/ChatGen-Home",
            "score": 4,
            "issue_id": 856,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 26",
                "zh": "11æœˆ26æ—¥"
            },
            "hash": "1abb1fa2701cf0cc",
            "authors": [
                "Chengyou Jia",
                "Changliang Xia",
                "Zhuohang Dang",
                "Weijia Wu",
                "Hangwei Qian",
                "Minnan Luo"
            ],
            "affiliations": [
                "CFAR, A*STAR",
                "National University of Singapore",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17176.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ: Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ñ‡Ğ°Ñ‚Ñƒ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ (Automatic T2I). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ChatGenBench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Automatic T2I, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğµ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ChatGen-Evo - Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ChatGen-Evo Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Streamlining Text-to-Image Generation with Automation",
                    "desc": "This paper addresses the challenges users face when generating images from text descriptions using text-to-image (T2I) models. It introduces Automatic T2I generation, which simplifies the process by allowing users to communicate their needs in a conversational manner. The authors present ChatGenBench, a new benchmark that provides high-quality paired data for evaluating T2I models. Additionally, they propose ChatGen-Evo, a multi-stage evolution strategy that enhances model performance through systematic automation of the T2I generation steps."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œç®€åŒ–åˆ›ä½œè¿‡ç¨‹ï¼",
                    "desc": "å°½ç®¡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”¨æˆ·åœ¨å®é™…åº”ç”¨ä¸­å¸¸å¸¸é¢ä¸´åå¤è¯•éªŒçš„æŒ‘æˆ˜ã€‚è¿™ä¸ªæŒ‘æˆ˜æºäºåˆ›å»ºåˆé€‚æç¤ºã€é€‰æ‹©åˆé€‚æ¨¡å‹å’Œé…ç½®ç‰¹å®šå‚æ•°ç­‰ç¹çæ­¥éª¤çš„å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§ã€‚æœ¬æ–‡æå‡ºäº†è‡ªåŠ¨åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œæ—¨åœ¨ç®€åŒ–è¿™äº›ç¹çæ­¥éª¤ï¼Œç”¨æˆ·åªéœ€ä»¥è‡ªç”±èŠå¤©çš„æ–¹å¼æè¿°éœ€æ±‚ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ChatGenBenchï¼Œä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°è¯„ä¼°è‡ªåŠ¨åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14951",
            "title": "Morph: A Motion-free Physics Optimization Framework for Human Motion Generation",
            "url": "https://huggingface.co/papers/2411.14951",
            "abstract": "Human motion generation plays a vital role in applications such as digital humans and humanoid robot control. However, most existing approaches disregard physics constraints, leading to the frequent production of physically implausible motions with pronounced artifacts such as floating and foot sliding. In this paper, we propose Morph, a Motion-free physics optimization framework, comprising a Motion Generator and a Motion Physics Refinement module, for enhancing physical plausibility without relying on costly real-world motion data. Specifically, the Motion Generator is responsible for providing large-scale synthetic motion data, while the Motion Physics Refinement Module utilizes these synthetic data to train a motion imitator within a physics simulator, enforcing physical constraints to project the noisy motions into a physically-plausible space. These physically refined motions, in turn, are used to fine-tune the Motion Generator, further enhancing its capability. Experiments on both text-to-motion and music-to-dance generation tasks demonstrate that our framework achieves state-of-the-art motion generation quality while improving physical plausibility drastically.",
            "score": 1,
            "issue_id": 853,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 22",
                "zh": "11æœˆ22æ—¥"
            },
            "hash": "c2f59b10a563bc5d",
            "authors": [
                "Zhuo Li",
                "Mingshuang Luo",
                "Ruibing Hou",
                "Xin Zhao",
                "Hao Liu",
                "Hong Chang",
                "Zimo Liu",
                "Chen Li"
            ],
            "affiliations": [
                "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, China",
                "MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
                "Peng Cheng Laboratory, China",
                "University of Chinese Academy of Sciences, China",
                "WeChat, Tencent Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14951.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#training",
                    "#optimization",
                    "#agents",
                    "#3d"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Morph - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ”Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰ĞµĞ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¸ ĞœĞ¾Ğ´ÑƒĞ»Ñ Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ£Ñ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Morph Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing Human Motion with Physics-Driven Optimization",
                    "desc": "This paper introduces Morph, a framework designed to improve the physical realism of human motion generation for applications like digital humans and robots. It consists of two main components: a Motion Generator that creates large-scale synthetic motion data, and a Motion Physics Refinement module that uses this data to train a motion imitator within a physics simulator. By enforcing physical constraints, the framework reduces artifacts such as floating and foot sliding, resulting in more believable motions. The approach is validated through experiments in text-to-motion and music-to-dance tasks, showing significant improvements in both motion quality and physical plausibility."
                },
                "zh": {
                    "title": "æå‡è¿åŠ¨ç”Ÿæˆçš„ç‰©ç†åˆç†æ€§",
                    "desc": "äººç±»è¿åŠ¨ç”Ÿæˆåœ¨æ•°å­—äººç±»å’Œç±»äººæœºå™¨äººæ§åˆ¶ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•å¤§å¤šå¿½è§†ç‰©ç†çº¦æŸï¼Œå¯¼è‡´ç”Ÿæˆçš„è¿åŠ¨å¸¸å¸¸ä¸ç¬¦åˆç‰©ç†è§„å¾‹ï¼Œå‡ºç°æ¼‚æµ®å’Œæ»‘åŠ¨ç­‰æ˜æ˜¾ä¼ªå½±ã€‚æœ¬æ–‡æå‡ºäº†Morphï¼Œä¸€ä¸ªæ— è¿åŠ¨çš„ç‰©ç†ä¼˜åŒ–æ¡†æ¶ï¼ŒåŒ…æ‹¬è¿åŠ¨ç”Ÿæˆå™¨å’Œè¿åŠ¨ç‰©ç†ç²¾ç‚¼æ¨¡å—ï¼Œæ—¨åœ¨åœ¨ä¸ä¾èµ–æ˜‚è´µçš„çœŸå®è¿åŠ¨æ•°æ®çš„æƒ…å†µä¸‹å¢å¼ºç‰©ç†åˆç†æ€§ã€‚é€šè¿‡åˆæˆè¿åŠ¨æ•°æ®è®­ç»ƒè¿åŠ¨æ¨¡ä»¿å™¨ï¼Œå¼ºåˆ¶ç‰©ç†çº¦æŸï¼Œå°†å™ªå£°è¿åŠ¨æŠ•å½±åˆ°ç‰©ç†åˆç†çš„ç©ºé—´ï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„è¿åŠ¨ç”Ÿæˆã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-28.html",
    "link_next": "2024-12-02.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "28.11",
        "en": "11/28",
        "zh": "11æœˆ28æ—¥"
    },
    "short_date_next": {
        "ru": "02.12",
        "en": "12/02",
        "zh": "12æœˆ2æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶å¸¸å¸¸ç”Ÿæˆä¸å‡†ç¡®æˆ–æ— å…³çš„å“åº”ï¼ŒåŸå› æ˜¯å¯¹å›¾åƒç†è§£çš„å¹»è§‰æˆ–æœªç²¾ç‚¼çš„æ¨ç†è·¯å¾„ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Critic-Vï¼Œä¸€ç§å—Actor-CriticèŒƒå¼å¯å‘çš„æ–°æ¡†æ¶ï¼Œä»¥æå‡VLMsçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶æ¥è§£è€¦æ¨ç†è¿‡ç¨‹å’Œè¯„è®ºè¿‡ç¨‹ï¼šReasonerç”ŸæˆåŸºäºè§†è§‰å’Œæ–‡æœ¬è¾“å…¥çš„æ¨ç†è·¯å¾„ï¼ŒCriticæä¾›å»ºè®¾æ€§çš„è¯„è®ºæ¥å®Œå–„è¿™äº›è·¯å¾„ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒCritic-Væ¡†æ¶åœ¨5ä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬GPT-4Vï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚",
        "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
        "pinyin": "è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶å¸¸å¸¸ç”Ÿæˆä¸å‡†ç¡®æˆ–æ— å…³çš„å“åº”ï¼ŒåŸå› æ˜¯å¯¹å›¾åƒç†è§£çš„å¹»è§‰æˆ–æœªç²¾ç‚¼çš„æ¨ç†è·¯å¾„ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Critic-Vï¼Œä¸€ç§å—Actor-CriticèŒƒå¼å¯å‘çš„æ–°æ¡†æ¶ï¼Œä»¥æå‡VLMsçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶æ¥è§£è€¦æ¨ç†è¿‡ç¨‹å’Œè¯„è®ºè¿‡ç¨‹ï¼šReasonerç”ŸæˆåŸºäºè§†è§‰å’Œæ–‡æœ¬è¾“å…¥çš„æ¨ç†è·¯å¾„ï¼ŒCriticæä¾›å»ºè®¾æ€§çš„è¯„è®ºæ¥å®Œå–„è¿™äº›è·¯å¾„ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒCritic-Væ¡†æ¶åœ¨5ä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬GPT-4Vï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚\n\nShÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng (VLMs) zÃ i duÅ mÃ³shÃ¬ tuÄ«lÇ rÃ¨nwÃ¹ zhÅng qÇ”dÃ©le xiÇnzhÃ¹ jÃ¬nbÃ¹. RÃ¡n'Ã©r, tÄmen rÃ©ngrÃ¡n chÃ¡ngchÃ¡ng shÄ“ngchÃ©ng bÃ¹ zhÇ”nquÃ¨ huÃ² wÃºguÄn de xiÇngyÃ¬ng, yuÃ¡nyÄ«n shÃ¬ duÃ¬ tÃºxiÃ ng lÇjiÄ› de huÃ njuÃ© huÃ² wÃ¨i jÄ«ngliÃ n de tuÄ«lÇ lÃ¹jÃ¬ng. WÃ¨i jiÄ›juÃ© zhÃ¨xiÄ“ tiÇozhÃ n, wÇ’men yÇn rÃ¹le Critic-V, yÄ«zhÇ’ng shÃ²u Actor-Critic fÃ nshÃ¬ qÇfÄ de xÄ«n kuÃ ngjiÃ , yÇ tÃ­shÄ“ng VLMs de tuÄ«lÇ nÃ©nglÃ¬. GÄi kuÃ ngjiÃ  tÅngguÃ² zhÄ›nghÃ© liÇng gÃ¨ dÃºlÃ¬ zÇ”jiÃ n lÃ¡i jiÄ›chÃ¡n tuÄ«lÇ guÃ²chÃ©ng hÃ© pÃ­nglÃ¹n guÃ²chÃ©ng: Reasoner shÄ“ngchÃ©ng jÄ«yÃº shÃ¬juÃ© hÃ© wÃ©nbÄ›n shÅ«rÃ¹ de tuÄ«lÇ lÃ¹jÃ¬ng, Critic tÃ­gÅng jiÃ nshÃ¨xÃ¬ng de pÃ­nglÃ¹n lÃ¡i wÃ¡nshÃ n zhÃ¨xiÄ“ lÃ¹jÃ¬ng. PÃ­ngjiÃ  jiÃ©guÇ’ xiÇnshÃ¬, Critic-V kuÃ ngjiÃ  zÃ i 5 gÃ¨ jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng xiÇnzhÃ¹ yÅu xiÃ nzÃ i fÄngfÇ, bÄokuÃ² GPT-4V, tÃ¨biÃ© shÃ¬ zÃ i tuÄ«lÇ zhÇ”nquÃ¨xÃ¬ng hÃ© xiÃ olÇœ fÄngmiÃ n.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"è§†è§‰-è¯­è¨€æ¨¡å‹\", \"pinyin\": \"shÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"vision-language model\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³shÃ¬\", \"trans\": \"multimodal\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ«lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨nwÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"å–å¾—\", \"pinyin\": \"qÇ”dÃ©\", \"trans\": \"achieve\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇnzhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"è¿›å±•\", \"pinyin\": \"jÃ¬nzhÇn\", \"trans\": \"progress\"},\n    {\"word\": \"ç„¶è€Œ\", \"pinyin\": \"rÃ¡n'Ã©r\", \"trans\": \"however\"},\n    {\"word\": \"å¸¸å¸¸\", \"pinyin\": \"chÃ¡ngchÃ¡ng\", \"trans\": \"often\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"ä¸å‡†ç¡®\", \"pinyin\": \"bÃ¹ zhÇ”nquÃ¨\", \"trans\": \"inaccurate\"},\n    {\"word\": \"æ— å…³\", \"pinyin\": \"wÃºguÄn\", \"trans\": \"irrelevant\"},\n    {\"word\": \"å“åº”\", \"pinyin\": \"xiÇngyÃ¬ng\", \"trans\": \"response\"},\n    {\"word\": \"åŸå› \", \"pinyin\": \"yuÃ¡nyÄ«n\", \"trans\": \"reason\"},\n    {\"word\": \"å¯¹\", \"pinyin\": \"duÃ¬\", \"trans\": \"towards\"},\n    {\"word\": \"å›¾åƒ\", \"pinyin\": \"tÃºxiÃ ng\", \"trans\": \"image\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇjiÄ›\", \"trans\": \"understanding\"},\n    {\"word\": \"å¹»è§‰\", \"pinyin\": \"huÃ njuÃ©\", \"trans\": \"illusion\"},\n    {\"word\": \"æœª\", \"pinyin\": \"wÃ¨i\", \"trans\": \"not yet\"},\n    {\"word\": \"ç²¾ç‚¼\", \"pinyin\": \"jÄ«ngliÃ n\", \"trans\": \"refined\"},\n    {\"word\": \"è·¯å¾„\", \"pinyin\": \"lÃ¹jÃ¬ng\", \"trans\": \"path\"},\n    {\"word\": \"è§£å†³\", \"pinyin\": \"jiÄ›juÃ©\", \"trans\": \"solve\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇozhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇnrÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"Critic-V\", \"pinyin\": \"Critic-V\", \"trans\": \"Critic-V\"},\n    {\"word\": \"å—\", \"pinyin\": \"shÃ²u\", \"trans\": \"inspired by\"},\n    {\"word\": \"Actor-Critic\", \"pinyin\": \"Actor-Critic\", \"trans\": \"Actor-Critic\"},\n    {\"word\": \"èŒƒå¼\", \"pinyin\": \"fÃ nshÃ¬\", \"trans\": \"paradigm\"},\n    {\"word\": \"å¯å‘\", \"pinyin\": \"qÇfÄ\", \"trans\": \"inspiration\"},\n    {\"word\": \"æ–°\", \"pinyin\": \"xÄ«n\", \"trans\": \"new\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ngjiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"ä»¥\", \"pinyin\": \"yÇ\", \"trans\": \"in order to\"},\n    {\"word\": \"æå‡\", \"pinyin\": \"tÃ­shÄ“ng\", \"trans\": \"enhance\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©nglÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"è¯¥\", \"pinyin\": \"gÇi\", \"trans\": \"this\"},\n    {\"word\": \"é€šè¿‡\", \"pinyin\": \"tÅngguÃ²\", \"trans\": \"through\"},\n    {\"word\": \"æ•´åˆ\", \"pinyin\": \"zhÄ›nghÃ©\", \"trans\": \"integrate\"},\n    {\"word\": \"ä¸¤ä¸ª\", \"pinyin\": \"liÇng gÃ¨\", \"trans\": \"two\"},\n    {\"word\": \"ç‹¬ç«‹\", \"pinyin\": \"dÃºlÃ¬\", \"trans\": \"independent\"},\n    {\"word\": \"ç»„ä»¶\", \"pinyin\": \"zÇ”jiÃ n\", \"trans\": \"component\"},\n    {\"word\": \"è§£è€¦\", \"pinyin\": \"jiÄ›'Ç’u\", \"trans\": \"decouple\"},\n    {\"word\": \"è¿‡ç¨‹\", \"pinyin\": \"guÃ²chÃ©ng\", \"trans\": \"process\"},\n    {\"word\": \"è¯„è®º\", \"pinyin\": \"pÃ­nglÃ¹n\", \"trans\": \"comment\"},\n    {\"word\": \"Reasoner\", \"pinyin\": \"Reasoner\", \"trans\": \"Reasoner\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ«yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"æ–‡æœ¬\", \"pinyin\": \"wÃ©nbÄ›n\", \"trans\": \"text\"},\n    {\"word\": \"è¾“å…¥\", \"pinyin\": \"shÅ«rÃ¹\", \"trans\": \"input\"},\n    {\"word\": \"Critic\", \"pinyin\": \"Critic\", \"trans\": \"Critic\"},\n    {\"word\": \"æä¾›\", \"pinyin\": \"tÃ­gÅng\", \"trans\": \"provide\"},\n    {\"word\": \"å»ºè®¾æ€§\", \"pinyin\": \"jiÃ nshÃ¨xÃ¬ng\", \"trans\": \"constructive\"},\n    {\"word\": \"å®Œå–„\", \"pinyin\": \"wÃ¡nshÃ n\", \"trans\": \"improve\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­nggÅ«\", \"trans\": \"evaluation\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ©guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"æ˜¾ç¤º\", \"pinyin\": \"xiÇnshÃ¬\", \"trans\": \"show\"},\n    {\"word\": \"åœ¨\", \"pinyin\": \"zÃ i\", \"trans\": \"in\"},\n    {\"word\": \"5ä¸ª\", \"pinyin\": \"wÇ” gÃ¨\", \"trans\": \"5\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ«zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"æµ‹è¯•\", \"pinyin\": \"cÃ¨shÃ¬\", \"trans\": \"test\"},\n    {\"word\": \"ä¸­\", \"pinyin\": \"zhÅng\", \"trans\": \"among\"},\n    {\"word\": \"æ˜¾è‘—ä¼˜äº\", \"pinyin\": \"xiÇnzhÃ¹ yÅuyÃº\", \"trans\": \"significantly superior to\"},\n    {\"word\": \"ç°æœ‰\", \"pinyin\": \"xiÃ nyÇ’u\", \"trans\": \"existing\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄngfÇ\", \"trans\": \"method\"},\n    {\"word\": \"åŒ…æ‹¬\", \"pinyin\": \"bÄokuÃ²\", \"trans\": \"include\"},\n    {\"word\": \"GPT-4V\", \"pinyin\": \"GPT-4V\", \"trans\": \"GPT-4V\"},\n    {\"word\": \"ç‰¹åˆ«\", \"pinyin\": \"tÃ¨biÃ©\", \"trans\": \"especially\"},\n    {\"word\": \"æ•ˆç‡\", \"pinyin\": \"xiÃ olÇœ\", \"trans\": \"efficiency\"}\n]",
        "trans": "Vision-language models (VLMs) have made significant progress in multimodal reasoning tasks. However, they often generate inaccurate or irrelevant responses due to illusions in image understanding or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a new framework inspired by the Actor-Critic paradigm to enhance the reasoning capabilities of VLMs. This framework decouples the reasoning process and the critique process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critiques to refine these paths. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, particularly in terms of reasoning accuracy and efficiency, across five benchmark tests.",
        "update_ts": "2024-11-29 09:08"
    }
}