{
    "date": {
        "ru": "29 ноября",
        "en": "November 29",
        "zh": "11月29日"
    },
    "time_utc": "2024-11-29 16:12",
    "weekday": 4,
    "issue_id": 866,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.18203",
            "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2411.18203",
            "abstract": "Vision-language models~(VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.",
            "score": 13,
            "issue_id": 863,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "13bce174f2b29a74",
            "authors": [
                "Di Zhang",
                "Jingdi Lei",
                "Junxian Li",
                "Xunzhi Wang",
                "Yujie Liu",
                "Zonglin Yang",
                "Jiatong Li",
                "Weida Wang",
                "Suorong Yang",
                "Jianbo Wu",
                "Peng Ye",
                "Wanli Ouyang",
                "Dongzhan Zhou"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Fudan University",
                "Hong Kong Polytechnic University",
                "Nanjing University",
                "Nankai University",
                "Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiaotong University",
                "Shanghai University",
                "Tongji University",
                "University of California, Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18203.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#reasoning",
                    "#multimodal",
                    "#rlhf",
                    "#hallucinations",
                    "#architecture"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Critic-V: улучшение мультимодальных рассуждений с помощью обучения с подкреплением",
                    "desc": "Статья представляет Critic-V - новый фреймворк для улучшения способностей мультимодальных моделей к рассуждению. Фреймворк разделяет процессы рассуждения и критики, используя два независимых компонента: Reasoner и Critic. Critic обучается с помощью Direct Preference Optimization на основе ранжированных критических отзывов. Результаты показывают, что Critic-V превосходит существующие методы, включая GPT-4V, на большинстве тестовых наборов данных."
                },
                "en": {
                    "title": "Enhancing VLMs with Critic-V: A New Era in Multimodal Reasoning",
                    "desc": "This paper presents Critic-V, a new framework designed to improve the reasoning abilities of vision-language models (VLMs) in multimodal tasks. It separates the reasoning and critique processes by using two components: the Reasoner, which creates reasoning paths from visual and textual data, and the Critic, which evaluates and refines these paths. The Critic provides feedback in the form of natural language critiques, rather than simple rewards, allowing for more detailed guidance in the reasoning process. The results demonstrate that Critic-V significantly enhances reasoning accuracy and efficiency compared to existing models, making it a valuable advancement for applications requiring complex reasoning."
                },
                "zh": {
                    "title": "Critic-V：提升视觉语言模型推理能力的新框架",
                    "desc": "本文介绍了一种新的框架Critic-V，旨在提升视觉语言模型（VLMs）的推理能力。该框架借鉴了演员-评论家（Actor-Critic）范式，将推理过程与评论过程解耦，分别由推理器和评论器两个独立组件完成。推理器根据视觉和文本输入生成推理路径，而评论器则提供建设性的反馈以优化这些路径。通过这种互动过程，Critic-V在复杂推理任务中显著提高了VLMs的准确性和效率，展示了在多模态推理应用中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17176",
            "title": "ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting",
            "url": "https://huggingface.co/papers/2411.17176",
            "abstract": "Despite the significant advancements in text-to-image (T2I) generative models, users often face a trial-and-error challenge in practical scenarios. This challenge arises from the complexity and uncertainty of tedious steps such as crafting suitable prompts, selecting appropriate models, and configuring specific arguments, making users resort to labor-intensive attempts for desired images. This paper proposes Automatic T2I generation, which aims to automate these tedious steps, allowing users to simply describe their needs in a freestyle chatting way. To systematically study this problem, we first introduce ChatGenBench, a novel benchmark designed for Automatic T2I. It features high-quality paired data with diverse freestyle inputs, enabling comprehensive evaluation of automatic T2I models across all steps. Additionally, recognizing Automatic T2I as a complex multi-step reasoning task, we propose ChatGen-Evo, a multi-stage evolution strategy that progressively equips models with essential automation skills. Through extensive evaluation across step-wise accuracy and image quality, ChatGen-Evo significantly enhances performance over various baselines. Our evaluation also uncovers valuable insights for advancing automatic T2I. All our data, code, and models will be available in https://chengyou-jia.github.io/ChatGen-Home",
            "score": 11,
            "issue_id": 863,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "1abb1fa2701cf0cc",
            "authors": [
                "Chengyou Jia",
                "Changliang Xia",
                "Zhuohang Dang",
                "Weijia Wu",
                "Hangwei Qian",
                "Minnan Luo"
            ],
            "affiliations": [
                "CFAR, A*STAR",
                "National University of Singapore",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17176.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#reasoning",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Автоматизация генерации изображений по тексту: от описания к результату",
                    "desc": "Статья представляет новый подход к автоматической генерации изображений по тексту (Automatic T2I). Авторы предлагают ChatGenBench - набор данных для оценки моделей Automatic T2I, содержащий разнообразные входные данные. Они также разрабатывают ChatGen-Evo - стратегию многоэтапной эволюции для обучения моделей необходимым навыкам автоматизации. Результаты показывают значительное улучшение производительности по сравнению с базовыми моделями в точности генерации и качестве изображений."
                },
                "en": {
                    "title": "Streamlining Text-to-Image Generation with Automation",
                    "desc": "This paper addresses the challenges users face when generating images from text descriptions using text-to-image (T2I) models. It introduces Automatic T2I generation, which simplifies the process by allowing users to communicate their needs in a conversational manner. The authors present ChatGenBench, a new benchmark that provides high-quality paired data for evaluating T2I models. Additionally, they propose ChatGen-Evo, a multi-stage evolution strategy that enhances model performance through systematic automation of the T2I generation steps."
                },
                "zh": {
                    "title": "自动化文本到图像生成，简化创作过程",
                    "desc": "尽管文本到图像生成模型取得了显著进展，但用户在实际应用中常常面临反复试验的挑战。这个挑战源于创建合适提示、选择合适模型和配置特定参数等繁琐步骤的复杂性和不确定性。本文提出了自动化文本到图像生成，旨在简化这些繁琐步骤，用户只需以自由聊天的方式描述需求。我们还引入了ChatGenBench，一个新颖的基准，旨在系统性地评估自动化文本到图像生成模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17041",
            "title": "Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models",
            "url": "https://huggingface.co/papers/2411.17041",
            "abstract": "Diffusion models have achieved impressive results in generative tasks like text-to-image (T2I) and text-to-video (T2V) synthesis. However, achieving accurate text alignment in T2V generation remains challenging due to the complex temporal dependency across frames. Existing reinforcement learning (RL)-based approaches to enhance text alignment often require differentiable reward functions or are constrained to limited prompts, hindering their scalability and applicability. In this paper, we propose Free^2Guide, a novel gradient-free framework for aligning generated videos with text prompts without requiring additional model training. Leveraging principles from path integral control, Free^2Guide approximates guidance for diffusion models using non-differentiable reward functions, thereby enabling the integration of powerful black-box Large Vision-Language Models (LVLMs) as reward model. Additionally, our framework supports the flexible ensembling of multiple reward models, including large-scale image-based models, to synergistically enhance alignment without incurring substantial computational overhead. We demonstrate that Free^2Guide significantly improves text alignment across various dimensions and enhances the overall quality of generated videos.",
            "score": 3,
            "issue_id": 863,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "adf6ad0f69ef7df2",
            "authors": [
                "Jaemin Kim",
                "Bryan S Kim",
                "Jong Chul Ye"
            ],
            "affiliations": [
                "Graduate School of AI, KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17041.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#rl",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Свободное руководство для точной генерации видео по тексту",
                    "desc": "Статья представляет Free^2Guide - новый подход к улучшению соответствия генерируемых видео текстовым запросам без дополнительного обучения моделей. Метод основан на принципах интегрального управления и позволяет использовать недифференцируемые функции вознаграждения, включая крупные мультимодальные модели. Free^2Guide поддерживает ансамблирование нескольких моделей вознаграждения для синергетического улучшения соответствия. Эксперименты показывают, что подход значительно повышает качество генерируемых видео и их соответствие текстовым запросам."
                },
                "en": {
                    "title": "Free^2Guide: Enhancing Text Alignment in Video Generation Without Training",
                    "desc": "This paper introduces Free^2Guide, a new method for improving text alignment in text-to-video (T2V) generation using diffusion models. Unlike traditional reinforcement learning approaches that require differentiable rewards, Free^2Guide operates without additional model training, making it more scalable. It utilizes non-differentiable reward functions and integrates large vision-language models to guide the video generation process. The results show that Free^2Guide enhances text alignment and overall video quality significantly."
                },
                "zh": {
                    "title": "无梯度框架，提升视频生成文本对齐",
                    "desc": "扩散模型在生成任务中取得了显著成果，如文本到图像（T2I）和文本到视频（T2V）合成。然而，在T2V生成中实现准确的文本对齐仍然具有挑战性，因为帧之间存在复杂的时间依赖关系。现有的基于强化学习（RL）的方法通常需要可微分的奖励函数或受到有限提示的限制，影响了它们的可扩展性和适用性。我们提出了Free^2Guide，这是一种新颖的无梯度框架，可以在不需要额外模型训练的情况下，将生成的视频与文本提示对齐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18350",
            "title": "TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models",
            "url": "https://huggingface.co/papers/2411.18350",
            "abstract": "This paper introduces Virtual Try-Off (VTOFF), a novel task focused on generating standardized garment images from single photos of clothed individuals. Unlike traditional Virtual Try-On (VTON), which digitally dresses models, VTOFF aims to extract a canonical garment image, posing unique challenges in capturing garment shape, texture, and intricate patterns. This well-defined target makes VTOFF particularly effective for evaluating reconstruction fidelity in generative models. We present TryOffDiff, a model that adapts Stable Diffusion with SigLIP-based visual conditioning to ensure high fidelity and detail retention. Experiments on a modified VITON-HD dataset show that our approach outperforms baseline methods based on pose transfer and virtual try-on with fewer pre- and post-processing steps. Our analysis reveals that traditional image generation metrics inadequately assess reconstruction quality, prompting us to rely on DISTS for more accurate evaluation. Our results highlight the potential of VTOFF to enhance product imagery in e-commerce applications, advance generative model evaluation, and inspire future work on high-fidelity reconstruction. Demo, code, and models are available at: https://rizavelioglu.github.io/tryoffdiff/",
            "score": 3,
            "issue_id": 863,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "d5e7e012abbc35e9",
            "authors": [
                "Riza Velioglu",
                "Petra Bevandic",
                "Robin Chan",
                "Barbara Hammer"
            ],
            "affiliations": [
                "Machine Learning Group, CITEC, Bielefeld University, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18350.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#cv",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "👚",
                "ru": {
                    "title": "Виртуальная примерка наизнанку: от фото к стандартизированному изображению одежды",
                    "desc": "Эта статья представляет новую задачу под названием Virtual Try-Off (VTOFF), которая фокусируется на генерации стандартизированных изображений одежды из одиночных фотографий одетых людей. Авторы разработали модель TryOffDiff, адаптирующую Stable Diffusion с визуальным кондиционированием на основе SigLIP для обеспечения высокой точности и сохранения деталей. Эксперименты на модифицированном наборе данных VITON-HD показали, что предложенный подход превосходит базовые методы. Исследование выявило, что традиционные метрики генерации изображений недостаточно хорошо оценивают качество реконструкции, что побудило авторов использовать DISTS для более точной оценки."
                },
                "en": {
                    "title": "Revolutionizing Garment Imaging with VTOFF!",
                    "desc": "This paper presents Virtual Try-Off (VTOFF), a new task that generates standardized images of garments from photos of people wearing them. Unlike traditional Virtual Try-On (VTON), which focuses on dressing models, VTOFF extracts a clear image of the garment, addressing challenges like capturing its shape and texture. The authors introduce TryOffDiff, a model that enhances Stable Diffusion with visual conditioning to improve detail and fidelity in garment representation. Their experiments demonstrate that VTOFF outperforms existing methods in generating high-quality garment images with fewer processing steps, while also suggesting better evaluation metrics for generative models."
                },
                "zh": {
                    "title": "虚拟试衣：提升服装图像生成的创新方法",
                    "desc": "本文介绍了一种新任务，称为虚拟试衣（VTOFF），旨在从穿着者的单张照片生成标准化的服装图像。与传统的虚拟试穿（VTON）不同，VTOFF专注于提取服装的典型图像，这在捕捉服装形状、纹理和复杂图案方面面临独特挑战。我们提出的TryOffDiff模型通过结合稳定扩散和基于SigLIP的视觉条件，确保生成图像的高保真度和细节保留。实验结果表明，我们的方法在重建质量评估上优于基于姿态转移和虚拟试穿的基线方法，展示了VTOFF在电子商务产品图像增强中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17863",
            "title": "LongKey: Keyphrase Extraction for Long Documents",
            "url": "https://huggingface.co/papers/2411.17863",
            "abstract": "In an era of information overload, manually annotating the vast and growing corpus of documents and scholarly papers is increasingly impractical. Automated keyphrase extraction addresses this challenge by identifying representative terms within texts. However, most existing methods focus on short documents (up to 512 tokens), leaving a gap in processing long-context documents. In this paper, we introduce LongKey, a novel framework for extracting keyphrases from lengthy documents, which uses an encoder-based language model to capture extended text intricacies. LongKey uses a max-pooling embedder to enhance keyphrase candidate representation. Validated on the comprehensive LDKP datasets and six diverse, unseen datasets, LongKey consistently outperforms existing unsupervised and language model-based keyphrase extraction methods. Our findings demonstrate LongKey's versatility and superior performance, marking an advancement in keyphrase extraction for varied text lengths and domains.",
            "score": 1,
            "issue_id": 863,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "3d8fc08dd3b125a4",
            "authors": [
                "Jeovane Honorio Alves",
                "Radu State",
                "Cinthia Obladen de Almendra Freitas",
                "Jean Paul Barddal"
            ],
            "affiliations": [
                "Graduate Program in Informatics (PPGIa) Pontifícia Universidade Católica do Paraná",
                "Graduate Program in Law (PPGD) Pontifícia Universidade Católica do Paraná",
                "SEDAN - SnT University of Luxembourg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17863.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#data",
                    "#long_context"
                ],
                "emoji": "🔑",
                "ru": {
                    "title": "LongKey: Прорыв в извлечении ключевых фраз из длинных документов",
                    "desc": "Статья представляет LongKey - новую систему для извлечения ключевых фраз из длинных документов. LongKey использует языковую модель на основе энкодера для обработки сложных текстов и применяет max-pooling для улучшения представления кандидатов в ключевые фразы. Система была протестирована на наборе данных LDKP и шести других разнородных датасетах, превзойдя существующие методы извлечения ключевых фраз. LongKey демонстрирует универсальность и превосходную производительность для текстов различной длины и тематики."
                },
                "en": {
                    "title": "Unlocking Keyphrases in Long Texts with LongKey",
                    "desc": "This paper presents LongKey, a new framework designed for extracting keyphrases from long documents, addressing the limitations of existing methods that only work with shorter texts. LongKey utilizes an encoder-based language model to effectively understand and process the complexities of extended text. It incorporates a max-pooling embedder to improve the representation of keyphrase candidates, enhancing the extraction process. The framework has been tested on various datasets, showing that it outperforms current unsupervised and language model-based approaches in keyphrase extraction."
                },
                "zh": {
                    "title": "LongKey：长文档关键短语提取的新突破",
                    "desc": "在信息过载的时代，手动标注大量文档和学术论文变得越来越不切实际。自动关键短语提取通过识别文本中的代表性术语来应对这一挑战。现有方法大多集中在短文档上，处理长文本的能力不足。本文介绍了LongKey，一个新颖的框架，利用编码器基础的语言模型提取长文档中的关键短语，展示了其在多种文本长度和领域中的优越性能。"
                }
            }
        }
    ],
    "link_prev": "2024-11-28.html",
    "link_next": "2024-12-02.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "28.11",
        "en": "11/28",
        "zh": "11月28日"
    },
    "short_date_next": {
        "ru": "02.12",
        "en": "12/02",
        "zh": "12月2日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "视觉-语言模型（VLMs）在多模态推理任务中取得了显著进展。然而，它们仍然常常生成不准确或无关的响应，原因是对图像理解的幻觉或未精炼的推理路径。为解决这些挑战，我们引入了Critic-V，一种受Actor-Critic范式启发的新框架，以提升VLMs的推理能力。该框架通过整合两个独立组件来解耦推理过程和评论过程：Reasoner生成基于视觉和文本输入的推理路径，Critic提供建设性的评论来完善这些路径。评估结果显示，Critic-V框架在5个基准测试中显著优于现有方法，包括GPT-4V，特别是在推理准确性和效率方面。",
        "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
        "pinyin": "视觉-语言模型（VLMs）在多模态推理任务中取得了显著进展。然而，它们仍然常常生成不准确或无关的响应，原因是对图像理解的幻觉或未精炼的推理路径。为解决这些挑战，我们引入了Critic-V，一种受Actor-Critic范式启发的新框架，以提升VLMs的推理能力。该框架通过整合两个独立组件来解耦推理过程和评论过程：Reasoner生成基于视觉和文本输入的推理路径，Critic提供建设性的评论来完善这些路径。评估结果显示，Critic-V框架在5个基准测试中显著优于现有方法，包括GPT-4V，特别是在推理准确性和效率方面。\n\nShìjué-yǔyán móxíng (VLMs) zài duō móshì tuīlǐ rènwù zhōng qǔdéle xiǎnzhù jìnbù. Rán'ér, tāmen réngrán chángcháng shēngchéng bù zhǔnquè huò wúguān de xiǎngyìng, yuányīn shì duì túxiàng lǐjiě de huànjué huò wèi jīngliàn de tuīlǐ lùjìng. Wèi jiějué zhèxiē tiǎozhàn, wǒmen yǐn rùle Critic-V, yīzhǒng shòu Actor-Critic fànshì qǐfā de xīn kuàngjià, yǐ tíshēng VLMs de tuīlǐ nénglì. Gāi kuàngjià tōngguò zhěnghé liǎng gè dúlì zǔjiàn lái jiěchán tuīlǐ guòchéng hé pínglùn guòchéng: Reasoner shēngchéng jīyú shìjué hé wénběn shūrù de tuīlǐ lùjìng, Critic tígōng jiànshèxìng de pínglùn lái wánshàn zhèxiē lùjìng. Píngjià jiéguǒ xiǎnshì, Critic-V kuàngjià zài 5 gè jīzhǔn cèshì zhōng xiǎnzhù yōu xiànzài fāngfǎ, bāokuò GPT-4V, tèbié shì zài tuīlǐ zhǔnquèxìng hé xiàolǜ fāngmiàn.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"视觉-语言模型\", \"pinyin\": \"shìjué-yǔyán móxíng\", \"trans\": \"vision-language model\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō móshì\", \"trans\": \"multimodal\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"取得\", \"pinyin\": \"qǔdé\", \"trans\": \"achieve\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎnzhù\", \"trans\": \"significant\"},\n    {\"word\": \"进展\", \"pinyin\": \"jìnzhǎn\", \"trans\": \"progress\"},\n    {\"word\": \"然而\", \"pinyin\": \"rán'ér\", \"trans\": \"however\"},\n    {\"word\": \"常常\", \"pinyin\": \"chángcháng\", \"trans\": \"often\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēngchéng\", \"trans\": \"generate\"},\n    {\"word\": \"不准确\", \"pinyin\": \"bù zhǔnquè\", \"trans\": \"inaccurate\"},\n    {\"word\": \"无关\", \"pinyin\": \"wúguān\", \"trans\": \"irrelevant\"},\n    {\"word\": \"响应\", \"pinyin\": \"xiǎngyìng\", \"trans\": \"response\"},\n    {\"word\": \"原因\", \"pinyin\": \"yuányīn\", \"trans\": \"reason\"},\n    {\"word\": \"对\", \"pinyin\": \"duì\", \"trans\": \"towards\"},\n    {\"word\": \"图像\", \"pinyin\": \"túxiàng\", \"trans\": \"image\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐjiě\", \"trans\": \"understanding\"},\n    {\"word\": \"幻觉\", \"pinyin\": \"huànjué\", \"trans\": \"illusion\"},\n    {\"word\": \"未\", \"pinyin\": \"wèi\", \"trans\": \"not yet\"},\n    {\"word\": \"精炼\", \"pinyin\": \"jīngliàn\", \"trans\": \"refined\"},\n    {\"word\": \"路径\", \"pinyin\": \"lùjìng\", \"trans\": \"path\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiějué\", \"trans\": \"solve\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎozhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐnrù\", \"trans\": \"introduce\"},\n    {\"word\": \"Critic-V\", \"pinyin\": \"Critic-V\", \"trans\": \"Critic-V\"},\n    {\"word\": \"受\", \"pinyin\": \"shòu\", \"trans\": \"inspired by\"},\n    {\"word\": \"Actor-Critic\", \"pinyin\": \"Actor-Critic\", \"trans\": \"Actor-Critic\"},\n    {\"word\": \"范式\", \"pinyin\": \"fànshì\", \"trans\": \"paradigm\"},\n    {\"word\": \"启发\", \"pinyin\": \"qǐfā\", \"trans\": \"inspiration\"},\n    {\"word\": \"新\", \"pinyin\": \"xīn\", \"trans\": \"new\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàngjià\", \"trans\": \"framework\"},\n    {\"word\": \"以\", \"pinyin\": \"yǐ\", \"trans\": \"in order to\"},\n    {\"word\": \"提升\", \"pinyin\": \"tíshēng\", \"trans\": \"enhance\"},\n    {\"word\": \"能力\", \"pinyin\": \"nénglì\", \"trans\": \"ability\"},\n    {\"word\": \"该\", \"pinyin\": \"gǎi\", \"trans\": \"this\"},\n    {\"word\": \"通过\", \"pinyin\": \"tōngguò\", \"trans\": \"through\"},\n    {\"word\": \"整合\", \"pinyin\": \"zhěnghé\", \"trans\": \"integrate\"},\n    {\"word\": \"两个\", \"pinyin\": \"liǎng gè\", \"trans\": \"two\"},\n    {\"word\": \"独立\", \"pinyin\": \"dúlì\", \"trans\": \"independent\"},\n    {\"word\": \"组件\", \"pinyin\": \"zǔjiàn\", \"trans\": \"component\"},\n    {\"word\": \"解耦\", \"pinyin\": \"jiě'ǒu\", \"trans\": \"decouple\"},\n    {\"word\": \"过程\", \"pinyin\": \"guòchéng\", \"trans\": \"process\"},\n    {\"word\": \"评论\", \"pinyin\": \"pínglùn\", \"trans\": \"comment\"},\n    {\"word\": \"Reasoner\", \"pinyin\": \"Reasoner\", \"trans\": \"Reasoner\"},\n    {\"word\": \"基于\", \"pinyin\": \"jīyú\", \"trans\": \"based on\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shìjué\", \"trans\": \"visual\"},\n    {\"word\": \"文本\", \"pinyin\": \"wénběn\", \"trans\": \"text\"},\n    {\"word\": \"输入\", \"pinyin\": \"shūrù\", \"trans\": \"input\"},\n    {\"word\": \"Critic\", \"pinyin\": \"Critic\", \"trans\": \"Critic\"},\n    {\"word\": \"提供\", \"pinyin\": \"tígōng\", \"trans\": \"provide\"},\n    {\"word\": \"建设性\", \"pinyin\": \"jiànshèxìng\", \"trans\": \"constructive\"},\n    {\"word\": \"完善\", \"pinyin\": \"wánshàn\", \"trans\": \"improve\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluation\"},\n    {\"word\": \"结果\", \"pinyin\": \"jiéguǒ\", \"trans\": \"result\"},\n    {\"word\": \"显示\", \"pinyin\": \"xiǎnshì\", \"trans\": \"show\"},\n    {\"word\": \"在\", \"pinyin\": \"zài\", \"trans\": \"in\"},\n    {\"word\": \"5个\", \"pinyin\": \"wǔ gè\", \"trans\": \"5\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"测试\", \"pinyin\": \"cèshì\", \"trans\": \"test\"},\n    {\"word\": \"中\", \"pinyin\": \"zhōng\", \"trans\": \"among\"},\n    {\"word\": \"显著优于\", \"pinyin\": \"xiǎnzhù yōuyú\", \"trans\": \"significantly superior to\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiànyǒu\", \"trans\": \"existing\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāngfǎ\", \"trans\": \"method\"},\n    {\"word\": \"包括\", \"pinyin\": \"bāokuò\", \"trans\": \"include\"},\n    {\"word\": \"GPT-4V\", \"pinyin\": \"GPT-4V\", \"trans\": \"GPT-4V\"},\n    {\"word\": \"特别\", \"pinyin\": \"tèbié\", \"trans\": \"especially\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiàolǜ\", \"trans\": \"efficiency\"}\n]",
        "trans": "Vision-language models (VLMs) have made significant progress in multimodal reasoning tasks. However, they often generate inaccurate or irrelevant responses due to illusions in image understanding or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a new framework inspired by the Actor-Critic paradigm to enhance the reasoning capabilities of VLMs. This framework decouples the reasoning process and the critique process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critiques to refine these paths. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, particularly in terms of reasoning accuracy and efficiency, across five benchmark tests.",
        "update_ts": "2024-11-29 09:08"
    }
}