{
    "date": {
        "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 23",
        "zh": "1æœˆ23æ—¥"
    },
    "time_utc": "2025-01-23 04:12",
    "weekday": 3,
    "issue_id": 1819,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.12909",
            "title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces",
            "url": "https://huggingface.co/papers/2501.12909",
            "abstract": "Virtual film production requires intricate decision-making processes, including scriptwriting, virtual cinematography, and precise actor positioning and actions. Motivated by recent advances in automated decision-making with language agent-based societies, this paper introduces FilmAgent, a novel LLM-based multi-agent collaborative framework for end-to-end film automation in our constructed 3D virtual spaces. FilmAgent simulates various crew roles, including directors, screenwriters, actors, and cinematographers, and covers key stages of a film production workflow: (1) idea development transforms brainstormed ideas into structured story outlines; (2) scriptwriting elaborates on dialogue and character actions for each scene; (3) cinematography determines the camera setups for each shot. A team of agents collaborates through iterative feedback and revisions, thereby verifying intermediate scripts and reducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key aspects. Human evaluation shows that FilmAgent outperforms all baselines across all aspects and scores 3.98 out of 5 on average, showing the feasibility of multi-agent collaboration in filmmaking. Further analysis reveals that FilmAgent, despite using the less advanced GPT-4o model, surpasses the single-agent o1, showing the advantage of a well-coordinated multi-agent system. Lastly, we discuss the complementary strengths and weaknesses of OpenAI's text-to-video model Sora and our FilmAgent in filmmaking.",
            "score": 13,
            "issue_id": 1819,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "0b73908eee2c2db3",
            "authors": [
                "Zhenran Xu",
                "Longyue Wang",
                "Jifang Wang",
                "Zhouyi Li",
                "Senbao Shi",
                "Xue Yang",
                "Yiyu Wang",
                "Baotian Hu",
                "Jun Yu",
                "Min Zhang"
            ],
            "affiliations": [
                "Harbin Institute of Technology (Shenzhen)",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12909.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#story_generation",
                    "#3d",
                    "#open_source",
                    "#agents",
                    "#hallucinations"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¸Ğ½Ğ¾ÑÑ‚ÑƒĞ´Ğ¸Ñ: Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ñ„Ğ¸Ğ»ÑŒĞ¼Ñ‹ Ğ¾Ñ‚ Ğ¸Ğ´ĞµĞ¸ Ğ´Ğ¾ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°",
                    "desc": "FilmAgent - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑÑŠĞµĞ¼Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ¶Ğ¸ÑÑĞµÑ€Ğ¾Ğ², ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ‚Ğ¾Ğ², Ğ°ĞºÑ‚ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ°: Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸Ğ´ĞµĞ¸, Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ² ÑÑŠĞµĞ¼ĞºĞ¸. FilmAgent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Film Production with Multi-Agent Collaboration",
                    "desc": "This paper presents FilmAgent, a collaborative framework that utilizes large language models (LLMs) to automate the film production process in 3D virtual environments. FilmAgent employs multiple agents that simulate various roles in filmmaking, such as directors and screenwriters, to collaboratively develop ideas, write scripts, and plan cinematography. The framework enhances decision-making through iterative feedback, which helps to refine scripts and minimize errors. Evaluation results indicate that FilmAgent significantly outperforms traditional methods, demonstrating the effectiveness of multi-agent systems in creative tasks like filmmaking."
                },
                "zh": {
                    "title": "å¤šæ™ºèƒ½ä½“åä½œï¼Œé©æ–°è™šæ‹Ÿç”µå½±åˆ¶ä½œ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFilmAgentçš„æ–°å‹å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è™šæ‹Ÿç”µå½±åˆ¶ä½œçš„è‡ªåŠ¨åŒ–ã€‚FilmAgentåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡æ‹Ÿå¯¼æ¼”ã€ç¼–å‰§ã€æ¼”å‘˜å’Œæ‘„å½±å¸ˆç­‰ä¸åŒè§’è‰²ï¼Œæ¶µç›–ç”µå½±åˆ¶ä½œçš„å…³é”®é˜¶æ®µï¼ŒåŒ…æ‹¬åˆ›æ„å¼€å‘ã€å‰§æœ¬å†™ä½œå’Œæ‘„å½±ã€‚é€šè¿‡æ™ºèƒ½ä½“ä¹‹é—´çš„è¿­ä»£åé¦ˆå’Œä¿®è®¢ï¼ŒFilmAgentèƒ½å¤ŸéªŒè¯ä¸­é—´å‰§æœ¬å¹¶å‡å°‘é”™è¯¯ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒFilmAgentåœ¨å¤šä¸ªæ–¹é¢çš„è¡¨ç°ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†å¤šæ™ºèƒ½ä½“åä½œåœ¨ç”µå½±åˆ¶ä½œä¸­çš„å¯è¡Œæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12948",
            "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2501.12948",
            "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
            "score": 9,
            "issue_id": 1819,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "cae642107ec57790",
            "authors": [
                "DeepSeek-AI",
                "Daya Guo",
                "Dejian Yang",
                "Haowei Zhang",
                "Junxiao Song",
                "Ruoyu Zhang",
                "Runxin Xu",
                "Qihao Zhu",
                "Shirong Ma",
                "Peiyi Wang",
                "Xiao Bi",
                "Xiaokang Zhang",
                "Xingkai Yu",
                "Yu Wu",
                "Z. F. Wu",
                "Zhibin Gou",
                "Zhihong Shao",
                "Zhuoshu Li",
                "Ziyi Gao",
                "Aixin Liu",
                "Bing Xue",
                "Bingxuan Wang",
                "Bochao Wu",
                "Bei Feng",
                "Chengda Lu",
                "Chenggang Zhao",
                "Chengqi Deng",
                "Chenyu Zhang",
                "Chong Ruan",
                "Damai Dai",
                "Deli Chen",
                "Dongjie Ji",
                "Erhang Li",
                "Fangyun Lin",
                "Fucong Dai",
                "Fuli Luo",
                "Guangbo Hao",
                "Guanting Chen",
                "Guowei Li",
                "H. Zhang",
                "Han Bao",
                "Hanwei Xu",
                "Haocheng Wang",
                "Honghui Ding",
                "Huajian Xin",
                "Huazuo Gao",
                "Hui Qu",
                "Hui Li",
                "Jianzhong Guo",
                "Jiashi Li",
                "Jiawei Wang",
                "Jingchang Chen",
                "Jingyang Yuan",
                "Junjie Qiu",
                "Junlong Li",
                "J. L. Cai",
                "Jiaqi Ni",
                "Jian Liang",
                "Jin Chen",
                "Kai Dong",
                "Kai Hu",
                "Kaige Gao",
                "Kang Guan",
                "Kexin Huang",
                "Kuai Yu",
                "Lean Wang",
                "Lecong Zhang",
                "Liang Zhao",
                "Litong Wang",
                "Liyue Zhang",
                "Lei Xu",
                "Leyi Xia",
                "Mingchuan Zhang",
                "Minghua Zhang",
                "Minghui Tang",
                "Meng Li",
                "Miaojun Wang",
                "Mingming Li",
                "Ning Tian",
                "Panpan Huang",
                "Peng Zhang",
                "Qiancheng Wang",
                "Qinyu Chen",
                "Qiushi Du",
                "Ruiqi Ge",
                "Ruisong Zhang",
                "Ruizhe Pan",
                "Runji Wang",
                "R. J. Chen",
                "R. L. Jin",
                "Ruyi Chen",
                "Shanghao Lu",
                "Shangyan Zhou",
                "Shanhuang Chen",
                "Shengfeng Ye",
                "Shiyu Wang",
                "Shuiping Yu",
                "Shunfeng Zhou",
                "Shuting Pan",
                "S. S. Li",
                "Shuang Zhou",
                "Shaoqing Wu",
                "Shengfeng Ye",
                "Tao Yun",
                "Tian Pei",
                "Tianyu Sun",
                "T. Wang",
                "Wangding Zeng",
                "Wanjia Zhao",
                "Wen Liu",
                "Wenfeng Liang",
                "Wenjun Gao",
                "Wenqin Yu",
                "Wentao Zhang",
                "W. L. Xiao",
                "Wei An",
                "Xiaodong Liu",
                "Xiaohan Wang",
                "Xiaokang Chen",
                "Xiaotao Nie",
                "Xin Cheng",
                "Xin Liu",
                "Xin Xie",
                "Xingchao Liu",
                "Xinyu Yang",
                "Xinyuan Li",
                "Xuecheng Su",
                "Xuheng Lin",
                "X. Q. Li",
                "Xiangyue Jin",
                "Xiaojin Shen",
                "Xiaosha Chen",
                "Xiaowen Sun",
                "Xiaoxiang Wang",
                "Xinnan Song",
                "Xinyi Zhou",
                "Xianzu Wang",
                "Xinxia Shan",
                "Y. K. Li",
                "Y. Q. Wang",
                "Y. X. Wei",
                "Yang Zhang",
                "Yanhong Xu",
                "Yao Li",
                "Yao Zhao",
                "Yaofeng Sun",
                "Yaohui Wang",
                "Yi Yu",
                "Yichao Zhang",
                "Yifan Shi",
                "Yiliang Xiong",
                "Ying He",
                "Yishi Piao",
                "Yisong Wang",
                "Yixuan Tan",
                "Yiyang Ma",
                "Yiyuan Liu",
                "Yongqiang Guo",
                "Yuan Ou",
                "Yuduan Wang",
                "Yue Gong",
                "Yuheng Zou",
                "Yujia He",
                "Yunfan Xiong",
                "Yuxiang Luo",
                "Yuxiang You",
                "Yuxuan Liu",
                "Yuyang Zhou",
                "Y. X. Zhu",
                "Yanhong Xu",
                "Yanping Huang",
                "Yaohui Li",
                "Yi Zheng",
                "Yuchen Zhu",
                "Yunxian Ma",
                "Ying Tang",
                "Yukun Zha",
                "Yuting Yan",
                "Z. Z. Ren",
                "Zehui Ren",
                "Zhangli Sha",
                "Zhe Fu",
                "Zhean Xu",
                "Zhenda Xie",
                "Zhengyan Zhang",
                "Zhewen Hao",
                "Zhicheng Ma",
                "Zhigang Yan",
                "Zhiyu Wu",
                "Zihui Gu",
                "Zijia Zhu",
                "Zijun Liu",
                "Zilin Li",
                "Ziwei Xie",
                "Ziyang Song",
                "Zizheng Pan",
                "Zhen Huang",
                "Zhipeng Xu",
                "Zhongyu Zhang",
                "Zhen Zhang"
            ],
            "affiliations": [
                "DeepSeek-AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12948.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ DeepSeek-R1-Zero Ğ¸ DeepSeek-R1. DeepSeek-R1-Zero Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. DeepSeek-R1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ OpenAI-o1-1217 Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Reasoning with DeepSeek Models",
                    "desc": "This paper presents two reasoning models, DeepSeek-R1-Zero and DeepSeek-R1, developed for enhanced reasoning capabilities. DeepSeek-R1-Zero is trained using large-scale reinforcement learning without any supervised fine-tuning, showcasing impressive reasoning behaviors but facing issues like readability and language mixing. To improve these aspects, DeepSeek-R1 employs a multi-stage training approach and utilizes cold-start data prior to reinforcement learning. The performance of DeepSeek-R1 is on par with existing models like OpenAI-o1-1217, and both models, along with several distilled versions, are made available to the research community."
                },
                "zh": {
                    "title": "æ·±åº¦æ¨ç†æ¨¡å‹çš„åˆ›æ–°ä¸æŒ‘æˆ˜",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†ç¬¬ä¸€ä»£æ¨ç†æ¨¡å‹DeepSeek-R1-Zeroå’ŒDeepSeek-R1ã€‚DeepSeek-R1-Zeroæ˜¯é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„æ¨¡å‹ï¼Œæ²¡æœ‰ç»è¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå®ƒåœ¨å¯è¯»æ€§å’Œè¯­è¨€æ··åˆæ–¹é¢å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜å¹¶è¿›ä¸€æ­¥æå‡æ¨ç†æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†DeepSeek-R1ï¼Œè¯¥æ¨¡å‹åœ¨è¿›è¡ŒRLä¹‹å‰é‡‡ç”¨äº†å¤šé˜¶æ®µè®­ç»ƒå’Œå†·å¯åŠ¨æ•°æ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13074",
            "title": "Autonomy-of-Experts Models",
            "url": "https://huggingface.co/papers/2501.13074",
            "abstract": "Mixture-of-Experts (MoE) models mostly use a router to assign tokens to specific expert modules, activating only partial parameters and often outperforming dense models. We argue that the separation between the router's decision-making and the experts' execution is a critical yet overlooked issue, leading to suboptimal expert selection and ineffective learning. To address this, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which experts autonomously select themselves to process inputs. AoE is based on the insight that an expert is aware of its own capacity to effectively process a token, an awareness reflected in the scale of its internal activations. In AoE, routers are removed; instead, experts pre-compute internal activations for inputs and are ranked based on their activation norms. Only the top-ranking experts proceed with the forward pass, while the others abort. The overhead of pre-computing activations is reduced through a low-rank weight factorization. This self-evaluating-then-partner-comparing approach ensures improved expert selection and effective learning. We pre-train language models having 700M up to 4B parameters, demonstrating that AoE outperforms traditional MoE models with comparable efficiency.",
            "score": 6,
            "issue_id": 1819,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "5cf511144ad54091",
            "authors": [
                "Ang Lv",
                "Ruobing Xie",
                "Yining Qian",
                "Songhao Wu",
                "Xingwu Sun",
                "Zhanhui Kang",
                "Di Wang",
                "Rui Yan"
            ],
            "affiliations": [
                "Machine Learning Platform Department, Tencent",
                "Renmin University of China",
                "Southeast University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13074.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ñ‚Ğ±Ğ¾Ñ€ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Mixture-of-Experts (MoE) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Autonomy-of-Experts (AoE). Ğ’ AoE ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ ÑĞµĞ±Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑÑ…, Ñ‡Ñ‚Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¾Ñ‚ 700 Ğ¼Ğ»Ğ½ Ğ´Ğ¾ 4 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AoE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MoE Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Empowering Experts: Self-Selection for Enhanced Learning in MoE Models",
                    "desc": "This paper introduces a new approach called Autonomy-of-Experts (AoE) for Mixture-of-Experts (MoE) models, which traditionally rely on a router to assign tasks to expert modules. The authors argue that the separation of decision-making and execution in MoE leads to poor expert selection and learning inefficiencies. In AoE, experts autonomously evaluate their ability to process inputs based on their internal activations, eliminating the need for a router. By allowing only the most capable experts to participate in processing, AoE enhances expert selection and improves overall model performance while maintaining efficiency."
                },
                "zh": {
                    "title": "è‡ªä¸»é€‰æ‹©ï¼Œæå‡ä¸“å®¶å­¦ä¹ æ•ˆç‡",
                    "desc": "æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰é€šå¸¸ä½¿ç”¨è·¯ç”±å™¨å°†è¾“å…¥åˆ†é…ç»™ç‰¹å®šçš„ä¸“å®¶æ¨¡å—ï¼Œä»…æ¿€æ´»éƒ¨åˆ†å‚æ•°ï¼Œé€šå¸¸æ¯”å¯†é›†æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè·¯ç”±å™¨çš„å†³ç­–ä¸ä¸“å®¶çš„æ‰§è¡Œä¹‹é—´çš„åˆ†ç¦»æ˜¯ä¸€ä¸ªå…³é”®ä½†è¢«å¿½è§†çš„é—®é¢˜ï¼Œå¯¼è‡´ä¸“å®¶é€‰æ‹©ä¸ä½³å’Œå­¦ä¹ æ•ˆæœä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªä¸»ä¸“å®¶ï¼ˆAoEï¼‰ï¼Œä¸€ç§æ–°é¢–çš„MoEèŒƒå¼ï¼Œå…¶ä¸­ä¸“å®¶è‡ªä¸»é€‰æ‹©è‡ªå·±å¤„ç†è¾“å…¥ã€‚AoEåŸºäºä¸“å®¶èƒ½å¤Ÿæ„è¯†åˆ°è‡ªèº«å¤„ç†èƒ½åŠ›çš„æ´å¯Ÿï¼Œé€šè¿‡å†…éƒ¨æ¿€æ´»çš„è§„æ¨¡åæ˜ å‡ºæ¥ï¼Œä»è€Œç¡®ä¿äº†æ›´å¥½çš„ä¸“å®¶é€‰æ‹©å’Œæœ‰æ•ˆå­¦ä¹ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12599",
            "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
            "url": "https://huggingface.co/papers/2501.12599",
            "abstract": "Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).",
            "score": 3,
            "issue_id": 1819,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "427fb9e286a6e3a8",
            "authors": [
                "Kimi Team",
                "Angang Du",
                "Bofei Gao",
                "Bowei Xing",
                "Changjiu Jiang",
                "Cheng Chen",
                "Cheng Li",
                "Chenjun Xiao",
                "Chenzhuang Du",
                "Chonghua Liao",
                "Chuning Tang",
                "Congcong Wang",
                "Dehao Zhang",
                "Enming Yuan",
                "Enzhe Lu",
                "Fengxiang Tang",
                "Flood Sung",
                "Guangda Wei",
                "Guokun Lai",
                "Haiqing Guo",
                "Han Zhu",
                "Hao Ding",
                "Hao Hu",
                "Hao Yang",
                "Hao Zhang",
                "Haotian Yao",
                "Haotian Zhao",
                "Haoyu Lu",
                "Haoze Li",
                "Haozhen Yu",
                "Hongcheng Gao",
                "Huabin Zheng",
                "Huan Yuan",
                "Jia Chen",
                "Jianhang Guo",
                "Jianlin Su",
                "Jianzhou Wang",
                "Jie Zhao",
                "Jin Zhang",
                "Jingyuan Liu",
                "Junjie Yan",
                "Junyan Wu",
                "Lidong Shi",
                "Ling Ye",
                "Longhui Yu",
                "Mengnan Dong",
                "Neo Zhang",
                "Ningchen Ma",
                "Qiwei Pan",
                "Qucheng Gong",
                "Shaowei Liu",
                "Shengling Ma",
                "Shupeng Wei",
                "Sihan Cao",
                "Siying Huang",
                "Tao Jiang",
                "Weihao Gao",
                "Weimin Xiong",
                "Weiran He",
                "Weixiao Huang",
                "Wenhao Wu",
                "Wenyang He",
                "Xianghui Wei",
                "Xianqing Jia",
                "Xingzhe Wu",
                "Xinran Xu",
                "Xinxing Zu",
                "Xinyu Zhou",
                "Xuehai Pan",
                "Y. Charles",
                "Yang Li",
                "Yangyang Hu",
                "Yangyang Liu",
                "Yanru Chen",
                "Yejie Wang",
                "Yibo Liu",
                "Yidao Qin",
                "Yifeng Liu",
                "Ying Yang",
                "Yiping Bao",
                "Yulun Du",
                "Yuxin Wu",
                "Yuzhi Wang",
                "Zaida Zhou",
                "Zhaoji Wang",
                "Zhaowei Li",
                "Zhen Zhu",
                "Zheng Zhang",
                "Zhexu Wang",
                "Zhilin Yang",
                "Zhiqi Huang",
                "Zihao Huang",
                "Ziyao Xu",
                "Zonghan Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.12599.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#rl",
                    "#reasoning",
                    "#long_context",
                    "#math"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Kimi k1.5 Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº RL Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ OpenAI's o1."
                },
                "en": {
                    "title": "Unlocking AI Potential with Reinforcement Learning in LLMs",
                    "desc": "This paper discusses the development of Kimi k1.5, a multi-modal large language model (LLM) that utilizes reinforcement learning (RL) to enhance its training data exploration through reward mechanisms. The authors highlight their innovative RL training techniques and infrastructure optimizations that allow for effective long context scaling and policy optimization without complex methods like Monte Carlo tree search. Kimi k1.5 achieves state-of-the-art performance on various reasoning benchmarks, demonstrating its competitive edge over existing models. Additionally, the paper introduces long2short methods that leverage long-context techniques to significantly improve short-context reasoning results, outperforming other models by a substantial margin."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¤§è¯­è¨€æ¨¡å‹çš„çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Kimi k1.5çš„è®­ç»ƒå®è·µï¼Œè¿™æ˜¯ä¸€ç§æœ€æ–°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é•¿ä¸Šä¸‹æ–‡æ‰©å±•å’Œæ”¹è¿›çš„ç­–ç•¥ä¼˜åŒ–ï¼Œå»ºç«‹äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„RLæ¡†æ¶ï¼Œè€Œä¸ä¾èµ–äºå¤æ‚çš„æŠ€æœ¯ï¼Œå¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢å’Œä»·å€¼å‡½æ•°ã€‚Kimi k1.5åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ¨ç†æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„çŸ­é“¾æ¨ç†æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨é•¿é“¾æŠ€æœ¯å¯ä»¥æ˜¾è‘—æå‡çŸ­é“¾æ¨¡å‹çš„è¡¨ç°ï¼Œå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.12570",
            "title": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning",
            "url": "https://huggingface.co/papers/2501.12570",
            "abstract": "Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning process leads to a substantial increase in inference time. A pressing challenge is reducing the inference overhead of long-thought LLMs while ensuring accuracy. In this paper, we experimentally demonstrate that long-thought reasoning models struggle to effectively allocate token budgets based on problem difficulty and reasoning redundancies. To address this, we propose Length-Harmonizing Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while maintaining accuracy. This effective fine-tuning method first estimates the LLM's baseline performance through pre-sampling and then uses RL-style fine-tuning to encourage the model to generate shorter reasoning processes under accuracy constraints. This allows the model to achieve efficient reasoning with lower redundancy while maintaining accuracy. Experiments on various mathematical reasoning benchmarks show that O1-Pruner not only significantly reduces inference overhead but also achieves higher accuracy, providing a novel and promising solution to this challenge. Our code is coming soon at https://github.com/StarDewXXX/O1-Pruner",
            "score": 3,
            "issue_id": 1818,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "2cb7e92315bbf3e4",
            "authors": [
                "Haotian Luo",
                "Li Shen",
                "Haiying He",
                "Yibo Wang",
                "Shiwei Liu",
                "Wei Li",
                "Naiqiang Tan",
                "Xiaochun Cao",
                "Dacheng Tao"
            ],
            "affiliations": [
                "China Agriculture University",
                "Didichuxing Co. Ltd",
                "Nanyang Technological University",
                "Shenzhen Campus of Sun Yat-sen University",
                "Tsinghua University",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12570.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº OpenAI's O1. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Length-Harmonizing Fine-Tuning (O1-Pruner), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ O1-Pruner Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Optimizing Long-Thought Reasoning for Efficient Problem Solving",
                    "desc": "This paper discusses a new approach to improve long-thought reasoning in large language models (LLMs) like OpenAI's O1. The authors identify that while these models enhance problem-solving, they also increase inference time due to inefficient token usage. To tackle this, they introduce Length-Harmonizing Fine-Tuning (O1-Pruner), which optimizes the reasoning process by balancing accuracy and efficiency. Their experiments show that O1-Pruner reduces inference overhead and improves accuracy on mathematical reasoning tasks, making it a valuable advancement in LLM performance."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ¨ç†æ•ˆç‡ï¼Œæå‡å‡†ç¡®æ€§ï¼",
                    "desc": "æœ€è¿‘ï¼Œé•¿æ€è€ƒæ¨ç†çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚OpenAIçš„O1ï¼Œé‡‡ç”¨äº†ç±»ä¼¼äººç±»æ€è€ƒå¤æ‚é—®é¢˜çš„æ‰©å±•æ¨ç†è¿‡ç¨‹ã€‚è¿™ç§æ¨ç†èŒƒå¼æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„è§£å†³é—®é¢˜èƒ½åŠ›ï¼Œå¹¶å–å¾—äº†è‰¯å¥½çš„æ•ˆæœã€‚ç„¶è€Œï¼Œé•¿æ€è€ƒæ¨ç†è¿‡ç¨‹å¯¼è‡´æ¨ç†æ—¶é—´å¤§å¹…å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é•¿åº¦åè°ƒå¾®è°ƒï¼ˆO1-Prunerï¼‰ï¼Œæ—¨åœ¨åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘é•¿æ€è€ƒLLMçš„æ¨ç†å¼€é”€ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-22.html",
    "link_next": "2025-01-24.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "22.01",
        "en": "01/22",
        "zh": "1æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "24.01",
        "en": "01/24",
        "zh": "1æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡æ¨¡ä»¿æ›´å¼ºçš„ä¸“å®¶æ¥æé«˜æ€§èƒ½ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å¸¸å¸¸å¤±è´¥ï¼Œå› ä¸ºæ— æ³•ä»é”™è¯¯ä¸­æ¢å¤ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§è¿­ä»£è‡ªè®­ç»ƒæ¡†æ¶ï¼ŒAgent-Rï¼Œä½¿è¯­è¨€ä»£ç†èƒ½å¤Ÿåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­è¿›è¡Œåæ€ã€‚é€šè¿‡ä½¿ç”¨è’™ç‰¹å¡ç½—æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼ŒAgent-R èƒ½å¤Ÿä»é”™è¯¯è·¯å¾„ä¸­æ¢å¤æ­£ç¡®è·¯å¾„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAgent-R èƒ½å¤Ÿæœ‰æ•ˆåœ°çº æ­£é”™è¯¯è¡Œä¸ºï¼Œå¹¶åœ¨ä¸‰ä¸ªäº’åŠ¨ç¯å¢ƒä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚",
        "title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zÃ i chÇ”lÇ fÃ¹zÃ¡ rÃ¨nwÃ¹ zhÅng de zhÃ²ngyÃ oxÃ¬ng.\n\nç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡æ¨¡ä»¿æ›´å¼ºçš„ä¸“å®¶æ¥æé«˜æ€§èƒ½ï¼Œ\nXiÃ nyÇ’u fÄngfÇ zhÇ”yÃ o tÅngguÃ² mÃ³fÇng gÃ¨ng qiÃ¡ng de zhuÄnjiÄ lÃ¡i tÃ­gÄo xÃ¬ngnÃ©ng,\n\nä½†è¿™äº›æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å¸¸å¸¸å¤±è´¥ï¼Œå› ä¸ºæ— æ³•ä»é”™è¯¯ä¸­æ¢å¤ã€‚\ndÃ n zhÃ¨xiÄ“ fÄngfÇ zÃ i shÃ­jÃ¬ yÃ¬ngyÃ²ng zhÅng chÃ¡ngchÃ¡ng shÄ«bÃ i, yÄ«nwÃ¨i wÃºfÇ cÃ³ng cuÃ²wÃ¹ zhÅng huÄ«fÃ¹.\n\næ–‡ç« æå‡ºäº†ä¸€ç§è¿­ä»£è‡ªè®­ç»ƒæ¡†æ¶ï¼ŒAgent-Rï¼Œä½¿è¯­è¨€ä»£ç†èƒ½å¤Ÿåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­è¿›è¡Œåæ€ã€‚\nWÃ©nzhÄng tÃ­chÅ« le yÄ« zhÇ’ng diÃ©dÃ i zÃ¬ xÃ¹nliÃ n kuÃ ngjiÃ , Agent-R, shÇ yÇ”yÃ¡n dÃ ilÇ nÃ©nggÃ²u zÃ i zhÃ­xÃ­ng guÃ²chÃ©ng zhÅng jÃ¬nxÃ­ng fÇnsÄ«.\n\né€šè¿‡ä½¿ç”¨è’™ç‰¹å¡ç½—æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼ŒAgent-R èƒ½å¤Ÿä»é”™è¯¯è·¯å¾„ä¸­æ¢å¤æ­£ç¡®è·¯å¾„ã€‚\nTÅngguÃ² shÇyÃ²ng MÃ©ngtÃ¨kÇluÃ³ shÃ¹ sÅusuÇ’ (MCTS), Agent-R nÃ©nggÃ²u cÃ³ng cuÃ²wÃ¹ lÃ¹jÃ¬ng zhÅng huÄ«fÃ¹ zhÃ¨ngquÃ¨ lÃ¹jÃ¬ng.\n\nå®éªŒç»“æœæ˜¾ç¤ºï¼ŒAgent-R èƒ½å¤Ÿæœ‰æ•ˆåœ°çº æ­£é”™è¯¯è¡Œä¸ºï¼Œå¹¶åœ¨ä¸‰ä¸ªäº’åŠ¨ç¯å¢ƒä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚\nShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, Agent-R nÃ©nggÃ²u yÇ’uxiÃ o de jiÅ«zhÃ¨ng cuÃ²wÃ¹ xÃ­ngwÃ©i, bÃ¬ng zÃ i sÄn gÃ¨ hÃ¹dÃ²ng huÃ¡njÃ¬ng zhÅng biÇoxiÃ n yÅu yÃº jÄ«xiÃ n fÄngfÇ.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},\n{'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'},\n{'word': 'å¤„ç†', 'pinyin': 'chÇ” lÇ', 'trans': 'handle'},\n{'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'},\n{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wu', 'trans': 'task'},\n{'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'},\n{'word': 'ç°æœ‰', 'pinyin': 'xiÃ n yÇ’u', 'trans': 'existing'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'},\n{'word': 'æ¨¡ä»¿', 'pinyin': 'mÃ³ fÇng', 'trans': 'imitate'},\n{'word': 'ä¸“å®¶', 'pinyin': 'zhuÄn jiÄ', 'trans': 'expert'},\n{'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'},\n{'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'},\n{'word': 'å¤±è´¥', 'pinyin': 'shÄ« bÃ i', 'trans': 'fail'},\n{'word': 'æ¢å¤', 'pinyin': 'huÄ« fÃ¹', 'trans': 'recover'},\n{'word': 'è¿­ä»£', 'pinyin': 'diÃ© dÃ i', 'trans': 'iterate'},\n{'word': 'è‡ªè®­ç»ƒ', 'pinyin': 'zÃ¬ xÃ¹n liÃ n', 'trans': 'self-training'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},\n{'word': 'ä»£ç†', 'pinyin': 'dÃ i lÇ', 'trans': 'agent'},\n{'word': 'åæ€', 'pinyin': 'fÇn sÄ«', 'trans': 'reflect'},\n{'word': 'è’™ç‰¹å¡ç½—æ ‘æœç´¢', 'pinyin': 'mÃ©ng tÃ¨ kÇ luÃ³ shÃ¹ sÅu suÇ’', 'trans': 'Monte Carlo Tree Search'},\n{'word': 'è·¯å¾„', 'pinyin': 'lÃ¹ jÃ¬ng', 'trans': 'path'},\n{'word': 'çº æ­£', 'pinyin': 'jiÅ« zhÃ¨ng', 'trans': 'correct'},\n{'word': 'è¡Œä¸º', 'pinyin': 'xÃ­ng wÃ©i', 'trans': 'behavior'},\n{'word': 'äº’åŠ¨', 'pinyin': 'hÃ¹ dÃ²ng', 'trans': 'interactive'},\n{'word': 'ç¯å¢ƒ', 'pinyin': 'huÃ¡n jÃ¬ng', 'trans': 'environment'},\n{'word': 'åŸºçº¿', 'pinyin': 'jÄ« xiÃ n', 'trans': 'baseline'}]",
        "trans": "This article discusses the importance of large language models (LLMs) in handling complex tasks. Existing methods primarily enhance performance by mimicking stronger experts, but these methods often fail in practical applications because they cannot recover from errors. The article proposes an iterative self-training framework, Agent-R, which enables language agents to reflect during the execution process. By utilizing Monte Carlo Tree Search (MCTS), Agent-R can recover from incorrect paths to the correct ones. Experimental results demonstrate that Agent-R can effectively correct erroneous behaviors and outperforms baseline methods in three interactive environments.",
        "update_ts": "2025-01-22 09:10"
    }
}