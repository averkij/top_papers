{
    "date": {
        "ru": "23 января",
        "en": "January 23",
        "zh": "1月23日"
    },
    "time_utc": "2025-01-23 03:12",
    "weekday": 3,
    "issue_id": 1818,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.12570",
            "title": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning",
            "url": "https://huggingface.co/papers/2501.12570",
            "abstract": "Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning process leads to a substantial increase in inference time. A pressing challenge is reducing the inference overhead of long-thought LLMs while ensuring accuracy. In this paper, we experimentally demonstrate that long-thought reasoning models struggle to effectively allocate token budgets based on problem difficulty and reasoning redundancies. To address this, we propose Length-Harmonizing Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while maintaining accuracy. This effective fine-tuning method first estimates the LLM's baseline performance through pre-sampling and then uses RL-style fine-tuning to encourage the model to generate shorter reasoning processes under accuracy constraints. This allows the model to achieve efficient reasoning with lower redundancy while maintaining accuracy. Experiments on various mathematical reasoning benchmarks show that O1-Pruner not only significantly reduces inference overhead but also achieves higher accuracy, providing a novel and promising solution to this challenge. Our code is coming soon at https://github.com/StarDewXXX/O1-Pruner",
            "score": 0,
            "issue_id": 1818,
            "pub_date": "2025-01-22",
            "pub_date_card": {
                "ru": "22 января",
                "en": "January 22",
                "zh": "1月22日"
            },
            "hash": "2cb7e92315bbf3e4",
            "authors": [
                "Haotian Luo",
                "Li Shen",
                "Haiying He",
                "Yibo Wang",
                "Shiwei Liu",
                "Wei Li",
                "Naiqiang Tan",
                "Xiaochun Cao",
                "Dacheng Tao"
            ],
            "affiliations": [
                "China Agriculture University",
                "Didichuxing Co. Ltd",
                "Nanyang Technological University",
                "Shenzhen Campus of Sun Yat-sen University",
                "Tsinghua University",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.12570.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Ускорение мышления ИИ без потери качества",
                    "desc": "Статья описывает метод оптимизации работы языковых моделей с длительным рассуждением, таких как OpenAI's O1. Авторы предлагают технику под названием Length-Harmonizing Fine-Tuning (O1-Pruner), которая сокращает время вывода, сохраняя точность модели. Метод использует предварительную выборку для оценки базовой производительности модели, а затем применяет обучение с подкреплением для генерации более коротких процессов рассуждения. Эксперименты на различных бенчмарках математического рассуждения показали, что O1-Pruner значительно снижает вычислительные затраты при сохранении или даже повышении точности."
                },
                "en": {
                    "title": "Optimizing Long-Thought Reasoning for Efficient Problem Solving",
                    "desc": "This paper discusses a new approach to improve long-thought reasoning in large language models (LLMs) like OpenAI's O1. The authors identify that while these models enhance problem-solving, they also increase inference time due to inefficient token usage. To tackle this, they introduce Length-Harmonizing Fine-Tuning (O1-Pruner), which optimizes the reasoning process by balancing accuracy and efficiency. Their experiments show that O1-Pruner reduces inference overhead and improves accuracy on mathematical reasoning tasks, making it a valuable advancement in LLM performance."
                },
                "zh": {
                    "title": "优化推理效率，提升准确性！",
                    "desc": "最近，长思考推理的语言模型（LLM）如OpenAI的O1，采用了类似人类思考复杂问题的扩展推理过程。这种推理范式显著增强了模型的解决问题能力，并取得了良好的效果。然而，长思考推理过程导致推理时间大幅增加。为了解决这个问题，我们提出了长度协调微调（O1-Pruner），旨在在保持准确性的同时，减少长思考LLM的推理开销。"
                }
            }
        }
    ],
    "link_prev": "2025-01-22.html",
    "link_next": "2025-01-24.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "22.01",
        "en": "01/22",
        "zh": "1月22日"
    },
    "short_date_next": {
        "ru": "24.01",
        "en": "01/24",
        "zh": "1月24日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了大语言模型（LLMs）在处理复杂任务中的重要性。现有方法主要通过模仿更强的专家来提高性能，但这些方法在实际应用中常常失败，因为无法从错误中恢复。文章提出了一种迭代自训练框架，Agent-R，使语言代理能够在执行过程中进行反思。通过使用蒙特卡罗树搜索（MCTS），Agent-R 能够从错误路径中恢复正确路径。实验结果显示，Agent-R 能够有效地纠正错误行为，并在三个互动环境中表现优于基线方法。",
        "title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training",
        "pinyin": "这篇文章讨论了大语言模型（LLMs）在处理复杂任务中的重要性。\nZhè piān wénzhāng tǎolùn le dà yǔyán móxíng (LLMs) zài chǔlǐ fùzá rènwù zhōng de zhòngyàoxìng.\n\n现有方法主要通过模仿更强的专家来提高性能，\nXiànyǒu fāngfǎ zhǔyào tōngguò mófǎng gèng qiáng de zhuānjiā lái tígāo xìngnéng,\n\n但这些方法在实际应用中常常失败，因为无法从错误中恢复。\ndàn zhèxiē fāngfǎ zài shíjì yìngyòng zhōng chángcháng shībài, yīnwèi wúfǎ cóng cuòwù zhōng huīfù.\n\n文章提出了一种迭代自训练框架，Agent-R，使语言代理能够在执行过程中进行反思。\nWénzhāng tíchū le yī zhǒng diédài zì xùnliàn kuàngjià, Agent-R, shǐ yǔyán dàilǐ nénggòu zài zhíxíng guòchéng zhōng jìnxíng fǎnsī.\n\n通过使用蒙特卡罗树搜索（MCTS），Agent-R 能够从错误路径中恢复正确路径。\nTōngguò shǐyòng Méngtèkǎluó shù sōusuǒ (MCTS), Agent-R nénggòu cóng cuòwù lùjìng zhōng huīfù zhèngquè lùjìng.\n\n实验结果显示，Agent-R 能够有效地纠正错误行为，并在三个互动环境中表现优于基线方法。\nShíyàn jiéguǒ xiǎnshì, Agent-R nénggòu yǒuxiào de jiūzhèng cuòwù xíngwéi, bìng zài sān gè hùdòng huánjìng zhōng biǎoxiàn yōu yú jīxiàn fāngfǎ.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'},\n{'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'handle'},\n{'word': '复杂', 'pinyin': 'fù zá', 'trans': 'complex'},\n{'word': '任务', 'pinyin': 'rèn wu', 'trans': 'task'},\n{'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'},\n{'word': '现有', 'pinyin': 'xiàn yǒu', 'trans': 'existing'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'},\n{'word': '模仿', 'pinyin': 'mó fǎng', 'trans': 'imitate'},\n{'word': '专家', 'pinyin': 'zhuān jiā', 'trans': 'expert'},\n{'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '失败', 'pinyin': 'shī bài', 'trans': 'fail'},\n{'word': '恢复', 'pinyin': 'huī fù', 'trans': 'recover'},\n{'word': '迭代', 'pinyin': 'dié dài', 'trans': 'iterate'},\n{'word': '自训练', 'pinyin': 'zì xùn liàn', 'trans': 'self-training'},\n{'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'},\n{'word': '代理', 'pinyin': 'dài lǐ', 'trans': 'agent'},\n{'word': '反思', 'pinyin': 'fǎn sī', 'trans': 'reflect'},\n{'word': '蒙特卡罗树搜索', 'pinyin': 'méng tè kǎ luó shù sōu suǒ', 'trans': 'Monte Carlo Tree Search'},\n{'word': '路径', 'pinyin': 'lù jìng', 'trans': 'path'},\n{'word': '纠正', 'pinyin': 'jiū zhèng', 'trans': 'correct'},\n{'word': '行为', 'pinyin': 'xíng wéi', 'trans': 'behavior'},\n{'word': '互动', 'pinyin': 'hù dòng', 'trans': 'interactive'},\n{'word': '环境', 'pinyin': 'huán jìng', 'trans': 'environment'},\n{'word': '基线', 'pinyin': 'jī xiàn', 'trans': 'baseline'}]",
        "trans": "This article discusses the importance of large language models (LLMs) in handling complex tasks. Existing methods primarily enhance performance by mimicking stronger experts, but these methods often fail in practical applications because they cannot recover from errors. The article proposes an iterative self-training framework, Agent-R, which enables language agents to reflect during the execution process. By utilizing Monte Carlo Tree Search (MCTS), Agent-R can recover from incorrect paths to the correct ones. Experimental results demonstrate that Agent-R can effectively correct erroneous behaviors and outperforms baseline methods in three interactive environments.",
        "update_ts": "2025-01-22 09:10"
    }
}