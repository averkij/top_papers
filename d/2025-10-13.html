
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 29 papers. October 13.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">29 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-10.html">â¬…ï¸ <span id="prev-date">10.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-14.html">â¡ï¸ <span id="next-date">14.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'};
        let feedDateNext = {'ru': '14.10', 'en': '10/14', 'zh': '10æœˆ14æ—¥'};
        let feedDatePrev = {'ru': '10.10', 'en': '10/10', 'zh': '10æœˆ10æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.08673', 'title': 'Thinking with Camera: A Unified Multimodal Model for Camera-Centric\n  Understanding and Generation', 'url': 'https://huggingface.co/papers/2510.08673', 'abstract': 'Puffin, a unified multimodal model, integrates language regression and diffusion-based generation to enhance camera-centric spatial understanding and generation by treating camera parameters as language.  \t\t\t\t\tAI-generated summary \t\t\t\t Camera-centric understanding and generation are two cornerstones of spatial intelligence, yet they are typically studied in isolation. We present Puffin, a unified camera-centric multimodal model that extends spatial awareness along the camera dimension. Puffin integrates language regression and diffusion-based generation to interpret and create scenes from arbitrary viewpoints. To bridge the modality gap between cameras and vision-language, we introduce a novel paradigm that treats camera as language, enabling thinking with camera. This guides the model to align spatially grounded visual cues with photographic terminology while reasoning across geometric context. Puffin is trained on Puffin-4M, a large-scale dataset of 4 million vision-language-camera triplets. We incorporate both global camera parameters and pixel-wise camera maps, yielding flexible and reliable spatial generation. Experiments demonstrate Puffin superior performance over specialized models for camera-centric generation and understanding. With instruction tuning, Puffin generalizes to diverse cross-view tasks such as spatial imagination, world exploration, and photography guidance. We will release the code, models, dataset pipeline, and benchmark to advance multimodal spatial intelligence research.', 'score': 46, 'issue_id': 6375, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': '2b4c322ac948b29e', 'authors': ['Kang Liao', 'Size Wu', 'Zhonghua Wu', 'Linyi Jin', 'Chao Wang', 'Yikai Wang', 'Fei Wang', 'Wei Li', 'Chen Change Loy'], 'affiliations': ['Max-Planck Institute for Informatics', 'S-Lab, Nanyang Technological University', 'SenseTime Research', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2510.08673.jpg', 'data': {'categories': ['#cv', '#diffusion', '#dataset', '#multimodal', '#benchmark', '#alignment', '#open_source'], 'emoji': 'ğŸ“¸', 'ru': {'title': 'ĞšĞ°Ğ¼ĞµÑ€Ğ° ĞºĞ°Ğº ÑĞ·Ñ‹Ğº: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½', 'desc': 'Puffin â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Â«ĞºĞ°Ğ¼ĞµÑ€Ğ° ĞºĞ°Ğº ÑĞ·Ñ‹ĞºÂ», Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Puffin-4M Ğ¸Ğ· 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚-ĞºĞ°Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹. Puffin Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº ÑÑ†ĞµĞ½Ñ‹.'}, 'en': {'title': 'Puffin: Bridging Language and Camera for Enhanced Spatial Intelligence', 'desc': 'Puffin is a new multimodal model designed to improve how machines understand and generate images based on camera perspectives. It combines language regression and diffusion-based generation techniques to interpret scenes from different viewpoints. By treating camera parameters as a form of language, Puffin aligns visual information with photographic terms, enhancing its spatial reasoning capabilities. Trained on a large dataset of vision-language-camera triplets, Puffin outperforms existing models in tasks related to camera-centric understanding and generation.'}, 'zh': {'title': 'Puffinï¼šç›¸æœºè§†è§’ä¸‹çš„ç©ºé—´æ™ºèƒ½æ–°çªç ´', 'desc': 'Puffinæ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºåŸºäºç›¸æœºçš„ç©ºé—´ç†è§£å’Œç”Ÿæˆã€‚å®ƒé€šè¿‡å°†ç›¸æœºå‚æ•°è§†ä¸ºè¯­è¨€ï¼Œç»“åˆè¯­è¨€å›å½’å’ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ–¹æ³•ï¼Œæ¥å¤„ç†å’Œåˆ›å»ºä¸åŒè§†è§’çš„åœºæ™¯ã€‚Puffinåœ¨ä¸€ä¸ªåŒ…å«400ä¸‡å¯¹è§†è§‰-è¯­è¨€-ç›¸æœºä¸‰å…ƒç»„çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿçµæ´»å¯é åœ°è¿›è¡Œç©ºé—´ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒPuffinåœ¨ç›¸æœºä¸­å¿ƒç”Ÿæˆå’Œç†è§£æ–¹é¢çš„è¡¨ç°ä¼˜äºä¸“é—¨æ¨¡å‹ï¼Œå¹¶ä¸”é€šè¿‡æŒ‡ä»¤è°ƒä¼˜ï¼Œèƒ½å¤Ÿé€‚åº”å¤šæ ·çš„è·¨è§†è§’ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.04533', 'title': 'TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion\n  Sampling', 'url': 'https://huggingface.co/papers/2510.04533', 'abstract': 'Tangential Amplifying Guidance (TAG) improves diffusion model sample quality by directly amplifying tangential components of estimated scores without modifying the model architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent diffusion models achieve the state-of-the-art performance in image generation, but often suffer from semantic inconsistencies or hallucinations. While various inference-time guidance methods can enhance generation, they often operate indirectly by relying on external signals or architectural modifications, which introduces additional computational overhead. In this paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and direct guidance method that operates solely on trajectory signals without modifying the underlying diffusion model. TAG leverages an intermediate sample as a projection basis and amplifies the tangential components of the estimated scores with respect to this basis to correct the sampling trajectory. We formalize this guidance process by leveraging a first-order Taylor expansion, which demonstrates that amplifying the tangential component steers the state toward higher-probability regions, thereby reducing inconsistencies and enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module that improves diffusion sampling fidelity with minimal computational addition, offering a new perspective on diffusion guidance.', 'score': 30, 'issue_id': 6375, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': '0b9aca186679e05a', 'authors': ['Hyunmin Cho', 'Donghoon Ahn', 'Susung Hong', 'Jee Eun Kim', 'Seungryong Kim', 'Kyong Hwan Jin'], 'affiliations': ['KAIST AI', 'Korea University', 'University of California, Berkeley', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2510.04533.jpg', 'data': {'categories': ['#cv', '#diffusion', '#inference', '#optimization', '#hallucinations'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Tangential Amplifying Guidance (TAG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, TAG Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ ĞºĞ°ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞºĞ¾Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. TAG ÑĞ²Ğ»ÑĞµÑ‚ÑÑ plug-and-play Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ»ÑĞ±Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Diffusion Models with Direct Tangential Guidance', 'desc': 'Tangential Amplifying Guidance (TAG) is a novel method designed to enhance the quality of samples generated by diffusion models. Unlike traditional guidance methods that modify the model architecture or rely on external signals, TAG directly amplifies the tangential components of estimated scores during the sampling process. This approach uses an intermediate sample as a basis for projection, allowing for a more efficient correction of the sampling trajectory. By applying a first-order Taylor expansion, TAG effectively steers the sampling towards higher-probability regions, thereby reducing semantic inconsistencies and improving overall sample fidelity.'}, 'zh': {'title': 'åˆ‡å‘æ”¾å¤§å¼•å¯¼ï¼šæå‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼•å¯¼æ–¹æ³•ï¼Œç§°ä¸ºåˆ‡å‘æ”¾å¤§å¼•å¯¼ï¼ˆTAGï¼‰ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚TAG é€šè¿‡ç›´æ¥æ”¾å¤§ä¼°è®¡åˆ†æ•°çš„åˆ‡å‘åˆ†é‡æ¥ä¿®æ­£é‡‡æ ·è½¨è¿¹ï¼Œè€Œä¸éœ€è¦ä¿®æ”¹æ¨¡å‹æ¶æ„ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸­é—´æ ·æœ¬ä½œä¸ºæŠ•å½±åŸºç¡€ï¼Œé‡‡ç”¨ä¸€é˜¶æ³°å‹’å±•å¼€å½¢å¼åŒ–å¼•å¯¼è¿‡ç¨‹ï¼Œä»è€Œå°†çŠ¶æ€å¼•å¯¼è‡³æ›´é«˜æ¦‚ç‡åŒºåŸŸï¼Œå‡å°‘è¯­ä¹‰ä¸ä¸€è‡´æ€§ã€‚TAG æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹æå‡æ‰©æ•£é‡‡æ ·çš„ä¿çœŸåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09201', 'title': 'Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for\n  MLLMs', 'url': 'https://huggingface.co/papers/2510.09201', 'abstract': 'Multimodal Prompt Optimizer (MPO) extends prompt optimization to handle multiple data types, improving performance over text-only methods in various applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-based selection strategy. Through extensive experiments across diverse modalities that go beyond text, such as images, videos, and even molecules, we demonstrate that MPO outperforms leading text-only optimization methods, establishing multimodal prompt optimization as a crucial step to realizing the potential of MLLMs.', 'score': 29, 'issue_id': 6376, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': '349a39e2011c9064', 'authors': ['Yumin Choi', 'Dongki Kim', 'Jinheon Baek', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2510.09201.jpg', 'data': {'categories': ['#optimization', '#training', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Multimodal Prompt Optimizer (MPO) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, MPO Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ½ĞµÑ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² (Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»Ñ‹) Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM.'}, 'en': {'title': 'Unlocking Multimodal Potential with MPO', 'desc': 'The Multimodal Prompt Optimizer (MPO) introduces a new approach to prompt optimization that accommodates various data types, such as text, images, and videos. This method enhances the performance of large language models (LLMs) by allowing for the joint optimization of multimodal prompts, rather than being limited to text-only prompts. MPO employs alignment-preserving updates and a Bayesian-based selection strategy to effectively choose candidate prompts based on prior evaluations. Experimental results show that MPO significantly outperforms traditional text-only optimization techniques, highlighting its importance in maximizing the capabilities of multimodal large language models (MLLMs).'}, 'zh': {'title': 'å¤šæ¨¡æ€æç¤ºä¼˜åŒ–ï¼Œé‡Šæ”¾AIæ½œåŠ›ï¼', 'desc': 'å¤šæ¨¡æ€æç¤ºä¼˜åŒ–å™¨ï¼ˆMPOï¼‰æ‰©å±•äº†æç¤ºä¼˜åŒ–çš„æ¦‚å¿µï¼Œä»¥å¤„ç†å¤šç§æ•°æ®ç±»å‹ï¼Œä»è€Œåœ¨å„ç§åº”ç”¨ä¸­æé«˜æ€§èƒ½ã€‚å°½ç®¡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç°æœ‰çš„æç¤ºä¼˜åŒ–æ–¹æ³•ä»ç„¶å±€é™äºæ–‡æœ¬ï¼Œé™åˆ¶äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ½œåŠ›ã€‚MPOé€šè¿‡è”åˆä¼˜åŒ–å¤šæ¨¡æ€æç¤ºï¼Œåˆ©ç”¨è´å¶æ–¯é€‰æ‹©ç­–ç•¥æŒ‡å¯¼å€™é€‰æç¤ºçš„é€‰æ‹©ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMPOåœ¨å›¾åƒã€è§†é¢‘ç­‰å¤šç§æ¨¡æ€ä¸Šä¼˜äºä¼ ç»Ÿçš„æ–‡æœ¬ä¼˜åŒ–æ–¹æ³•ï¼Œæ ‡å¿—ç€å¤šæ¨¡æ€æç¤ºä¼˜åŒ–æ˜¯å®ç°MLLMsæ½œåŠ›çš„é‡è¦ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06499', 'title': 'Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining\n  Levels', 'url': 'https://huggingface.co/papers/2510.06499', 'abstract': 'A scalable data engine converts large-scale pre-training documents into diverse question-answer pairs for reinforcement learning, significantly improving model performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success through imitation learning on vast text corpora, but this paradigm creates a training-generation gap and limits robust reasoning. Reinforcement learning (RL) offers a more data-efficient solution capable of bridging this gap, yet its application has been constrained by a critical data bottleneck: existing RL datasets are orders of magnitude smaller and less diverse than web-scale pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a scalable data engine that systematically converts large-scale pre-training documents into millions of diverse, verifiable question-answer pairs for RL. Using this pipeline, we construct the Webscale-RL dataset, containing 1.2 million examples across more than 9 domains. Our experiments show that the model trained on this dataset significantly outperforms continual pretraining and strong data refinement baselines across a suite of benchmarks. Notably, RL training with our dataset proves substantially more efficient, achieving the performance of continual pre-training with up to 100times fewer tokens. Our work presents a viable path toward scaling RL to pre-training levels, enabling more capable and efficient language models.', 'score': 17, 'issue_id': 6376, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '8a5d9b213feffe3c', 'authors': ['Zhepeng Cen', 'Haolin Chen', 'Shiyu Wang', 'Zuxin Liu', 'Zhiwei Liu', 'Ding Zhao', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang', 'Weiran Yao'], 'affiliations': ['Carnegie Mellon University', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.06499.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#rl', '#data', '#reasoning', '#optimization'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğº Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RL Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Webscale-RL pipeline â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ´Ğ»Ñ reinforcement learning. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 9 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ LLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ continual pretraining. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RL Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ² 100 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ reinforcement learning Ğ´Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ´ĞµĞ»Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Reinforcement Learning with Scalable Data Generation', 'desc': 'This paper introduces a new method called the Webscale-RL pipeline, which transforms large amounts of pre-training documents into a wide variety of question-answer pairs for reinforcement learning (RL). This approach addresses the issue of limited and less diverse RL datasets, which have hindered the effectiveness of RL in training language models. By creating a dataset with 1.2 million examples from over 9 domains, the authors demonstrate that models trained on this data can significantly outperform traditional methods. The results show that using this dataset allows for more efficient training, achieving high performance with far fewer training tokens compared to continual pre-training methods.'}, 'zh': {'title': 'å¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ•°æ®å¼•æ“', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¯æ‰©å±•çš„æ•°æ®å¼•æ“ï¼Œèƒ½å¤Ÿå°†å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ–‡æ¡£è½¬æ¢ä¸ºå¤šæ ·åŒ–çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œä»¥ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œä»è€Œæ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æ¨¡ä»¿å­¦ä¹ å–å¾—äº†æˆåŠŸï¼Œä½†å­˜åœ¨è®­ç»ƒä¸ç”Ÿæˆä¹‹é—´çš„å·®è·ï¼Œé™åˆ¶äº†æ¨ç†èƒ½åŠ›ã€‚å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆçš„æ•°æ®è§£å†³æ–¹æ¡ˆï¼Œä½†å—é™äºç°æœ‰æ•°æ®é›†çš„è§„æ¨¡å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬æå‡ºçš„Webscale-RLç®¡é“èƒ½å¤Ÿç³»ç»Ÿæ€§åœ°ç”Ÿæˆå¤§é‡é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œæ„å»ºäº†åŒ…å«120ä¸‡ä¸ªç¤ºä¾‹çš„Webscale-RLæ•°æ®é›†ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09606', 'title': 'SpaceVista: All-Scale Visual Spatial Reasoning from mm to km', 'url': 'https://huggingface.co/papers/2510.09606', 'abstract': 'A spatial reasoning model using scale-aware experts and progressive rewards demonstrates competitive performance across diverse tasks and scales using a large, curated dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t With the current surge in spatial reasoning explorations, researchers have made significant progress in understanding indoor scenes, but still struggle with diverse applications such as robotics and autonomous driving. This paper aims to advance all-scale spatial reasoning across diverse scenarios by tackling two key challenges: 1) the heavy reliance on indoor 3D scans and labor-intensive manual annotations for dataset curation; 2) the absence of effective all-scale scene modeling, which often leads to overfitting to individual scenes. In this paper, we introduce a holistic solution that integrates a structured spatial reasoning knowledge system, scale-aware modeling, and a progressive training paradigm, as the first attempt to broaden the all-scale spatial intelligence of MLLMs to the best of our knowledge. Using a task-specific, specialist-driven automated pipeline, we curate over 38K video scenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising approximately 1M spatial QA pairs spanning 19 diverse task types. While specialist models can inject useful domain knowledge, they are not reliable for evaluation. We then build an all-scale benchmark with precise annotations by manually recording, retrieving, and assembling video-based data. However, naive training with SpaceVista-1M often yields suboptimal results due to the potential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a spatial reasoning model that accepts dense inputs beyond semantics and uses scale as an anchor for scale-aware experts and progressive rewards. Finally, extensive evaluations across 5 benchmarks, including our SpaceVista-Bench, demonstrate competitive performance, showcasing strong generalization across all scales and scenarios. Our dataset, model, and benchmark will be released on https://peiwensun2000.github.io/mm2km .', 'score': 12, 'issue_id': 6375, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': '892c43f9f20768e2', 'authors': ['Peiwen Sun', 'Shiqiang Lang', 'Dongming Wu', 'Yi Ding', 'Kaituo Feng', 'Huadai Liu', 'Zhen Ye', 'Rui Liu', 'Yun-Hui Liu', 'Jianan Wang', 'Xiangyu Yue'], 'affiliations': ['Astribot, Beijing University of Posts and Telecommunications', 'Hong Kong University of Science and Technology', 'Multimedia Lab, Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.09606.jpg', 'data': {'categories': ['#data', '#cv', '#training', '#dataset', '#reasoning', '#survey', '#benchmark', '#optimization'], 'emoji': 'ğŸ”­', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…: Ğ¾Ñ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ¾ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SpaceVista-7B â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¾Ñ‚ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑÑ†ĞµĞ½. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SpaceVista-1M Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ pipeline Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ 38 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑÑ†ĞµĞ½. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ scale-aware ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 5 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Advancing Spatial Reasoning with Scale-Aware Models', 'desc': 'This paper presents a novel spatial reasoning model that effectively handles various tasks and scales by utilizing scale-aware experts and a progressive reward system. It addresses challenges in spatial reasoning, particularly the dependence on indoor 3D scans and the need for effective all-scale scene modeling. The authors introduce SpaceVista-1M, a large dataset with over 1 million spatial question-answer pairs, curated from diverse video scenes across multiple scales. The proposed SpaceVista-7B model demonstrates strong generalization capabilities, achieving competitive performance on several benchmarks, thus advancing the field of spatial reasoning in machine learning.'}, 'zh': {'title': 'å…¨å°ºåº¦ç©ºé—´æ¨ç†çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ç©ºé—´æ¨ç†æ¨¡å‹ï¼Œåˆ©ç”¨è§„æ¨¡æ„ŸçŸ¥ä¸“å®¶å’Œæ¸è¿›å¥–åŠ±ï¼Œåœ¨å¤šç§ä»»åŠ¡å’Œå°ºåº¦ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç ”ç©¶è€…ä»¬é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬å¯¹å®¤å†…3Dæ‰«æå’Œäººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œä»¥åŠç¼ºä¹æœ‰æ•ˆçš„å…¨å°ºåº¦åœºæ™¯å»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡ä»‹ç»äº†ä¸€ç§æ•´åˆç»“æ„åŒ–ç©ºé—´æ¨ç†çŸ¥è¯†ç³»ç»Ÿã€è§„æ¨¡æ„ŸçŸ¥å»ºæ¨¡å’Œæ¸è¿›è®­ç»ƒèŒƒå¼çš„æ•´ä½“è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡åˆ›å»ºä¸€ä¸ªåŒ…å«38,000ä¸ªè§†é¢‘åœºæ™¯çš„SpaceVista-1Mæ•°æ®é›†ï¼Œæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09558', 'title': "AutoPR: Let's Automate Your Academic Promotion!", 'url': 'https://huggingface.co/papers/2510.09558', 'abstract': 'AutoPR, a multi-agent framework, automates the promotion of research papers by transforming them into engaging public content, significantly improving engagement metrics compared to direct LLM pipelines.  \t\t\t\t\tAI-generated summary \t\t\t\t As the volume of peer-reviewed research surges, scholars increasingly rely on social platforms for discovery, while authors invest considerable effort in promoting their work to ensure visibility and citations. To streamline this process and reduce the reliance on human effort, we introduce Automatic Promotion (AutoPR), a novel task that transforms research papers into accurate, engaging, and timely public content. To enable rigorous evaluation, we release PRBench, a multimodal benchmark that links 512 peer-reviewed articles to high-quality promotional posts, assessing systems along three axes: Fidelity (accuracy and tone), Engagement (audience targeting and appeal), and Alignment (timing and channel optimization). We also introduce PRAgent, a multi-agent framework that automates AutoPR in three stages: content extraction with multimodal preparation, collaborative synthesis for polished outputs, and platform-specific adaptation to optimize norms, tone, and tagging for maximum reach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates substantial improvements, including a 604% increase in total watch time, a 438% rise in likes, and at least a 2.9x boost in overall engagement. Ablation studies show that platform modeling and targeted promotion contribute the most to these gains. Our results position AutoPR as a tractable, measurable research problem and provide a roadmap for scalable, impactful automated scholarly communication.', 'score': 12, 'issue_id': 6377, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': 'f23989f659120bb4', 'authors': ['Qiguang Chen', 'Zheng Yan', 'Mingda Yang', 'Libo Qin', 'Yixin Yuan', 'Hanjing Li', 'Jinhao Liu', 'Yiyan Ji', 'Dengyun Peng', 'Jiannan Guan', 'Mengkang Hu', 'Yantao Du', 'Wanxiang Che'], 'affiliations': ['ByteDance China (Seed)', 'LARG, Research Center for Social Computing and Interactive Robotics, Harbin Institute of Technology', 'School of Computer Science and Engineering, Central South University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.09558.jpg', 'data': {'categories': ['#agents', '#benchmark', '#optimization', '#multimodal', '#alignment'], 'emoji': 'ğŸ“¢', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ AutoPR â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ PRBench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 512 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¾-Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ğ¾Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ. Ğ˜Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº PRAgent Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹: ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ½Ğ° 604%, Ğ»Ğ°Ğ¹ĞºĞ¾Ğ² Ğ½Ğ° 438% Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ²Ğ¾Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² 2.9 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM.'}, 'en': {'title': 'Transforming Research Promotion with AutoPR', 'desc': 'AutoPR is a multi-agent framework designed to automate the promotion of research papers by converting them into engaging public content. It addresses the challenge of increasing visibility and citations for scholarly work in a crowded digital landscape. The framework includes a benchmark called PRBench, which evaluates promotional effectiveness based on fidelity, engagement, and alignment. Results show that AutoPR significantly outperforms traditional LLM pipelines, enhancing audience engagement metrics like watch time and likes.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–æ¨å¹¿ç ”ç©¶è®ºæ–‡çš„æœªæ¥', 'desc': 'AutoPRæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†ç ”ç©¶è®ºæ–‡è½¬åŒ–ä¸ºå¼•äººå…¥èƒœçš„å…¬å…±å†…å®¹ï¼Œè‡ªåŠ¨åŒ–ç ”ç©¶è®ºæ–‡çš„æ¨å¹¿ã€‚éšç€åŒè¡Œè¯„å®¡ç ”ç©¶æ•°é‡çš„æ¿€å¢ï¼Œå­¦è€…ä»¬è¶Šæ¥è¶Šä¾èµ–ç¤¾äº¤å¹³å°æ¥å‘ç°ç ”ç©¶ï¼Œè€Œä½œè€…åˆ™æŠ•å…¥å¤§é‡ç²¾åŠ›æ¥æå‡å…¶å·¥ä½œçš„å¯è§æ€§å’Œå¼•ç”¨ç‡ã€‚æˆ‘ä»¬æå‡ºçš„PRBenchåŸºå‡†æµ‹è¯•ï¼Œè¿æ¥äº†512ç¯‡åŒè¡Œè¯„å®¡çš„æ–‡ç« ä¸é«˜è´¨é‡çš„æ¨å¹¿å¸–å­ï¼Œä»å‡†ç¡®æ€§ã€å—ä¼—å¸å¼•åŠ›å’Œæ—¶æ•ˆæ€§ç­‰æ–¹é¢è¯„ä¼°ç³»ç»Ÿçš„è¡¨ç°ã€‚ä¸ç›´æ¥çš„LLMç®¡é“ç›¸æ¯”ï¼ŒPRAgentåœ¨æå‡è§‚çœ‹æ—¶é—´ã€ç‚¹èµæ•°å’Œæ•´ä½“å‚ä¸åº¦æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09608', 'title': 'StreamingVLM: Real-Time Understanding for Infinite Video Streams', 'url': 'https://huggingface.co/papers/2510.09608', 'abstract': 'StreamingVLM is a real-time vision-language model that efficiently processes infinite video streams using a compact KV cache and supervised fine-tuning, achieving high performance on long videos and diverse benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.', 'score': 8, 'issue_id': 6375, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': 'ef99bf0338c5c2c5', 'authors': ['Ruyi Xu', 'Guangxuan Xiao', 'Yukang Chen', 'Liuning He', 'Kelly Peng', 'Yao Lu', 'Song Han'], 'affiliations': ['First Intelligence', 'MIT', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2510.09608.jpg', 'data': {'categories': ['#cv', '#video', '#long_context', '#training', '#multimodal', '#benchmark', '#optimization', '#agents'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ‘ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'StreamingVLM - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°ÑÑ‚ÑƒÑ‰Ğ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ KV-ĞºĞµÑˆ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ (attention sinks, Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²ÑĞµĞ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ€ĞµĞ· supervised fine-tuning Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ. ĞĞ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ñ Ğ´Ğ²ÑƒÑ…Ñ‡Ğ°ÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 66.18% win rate Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² GPT-4O mini Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ H100.'}, 'en': {'title': 'Real-Time Understanding of Infinite Video Streams', 'desc': 'StreamingVLM is a vision-language model designed to process continuous video streams in real-time while minimizing latency and memory usage. It utilizes a compact key-value (KV) cache and a supervised fine-tuning (SFT) strategy to maintain coherence and efficiency during inference. By applying full attention on short, overlapping video segments, it effectively simulates the attention patterns needed for long videos without the computational burden of processing entire videos at once. The model demonstrates superior performance on the new Inf-Streams-Eval benchmark, achieving high win rates against existing models and improving visual question answering capabilities without specific fine-tuning.'}, 'zh': {'title': 'å®æ—¶å¤„ç†æ— é™è§†é¢‘æµçš„æ™ºèƒ½æ¨¡å‹', 'desc': 'StreamingVLMæ˜¯ä¸€ç§å®æ—¶è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†æ— é™çš„è§†é¢‘æµã€‚å®ƒé€šè¿‡ç´§å‡‘çš„KVç¼“å­˜å’Œç›‘ç£å¾®è°ƒï¼Œè§£å†³äº†é•¿è§†é¢‘å¤„ç†ä¸­çš„å»¶è¿Ÿå’Œå†…å­˜ä½¿ç”¨é—®é¢˜ã€‚è¯¥æ¨¡å‹åœ¨æ¨ç†æ—¶é‡ç”¨æ³¨æ„åŠ›çŠ¶æ€ï¼Œç»“åˆçŸ­çª—å£å’Œé•¿çª—å£çš„è§†è§‰å’Œæ–‡æœ¬æ ‡è®°ï¼Œç¡®ä¿äº†ç¨³å®šçš„ç†è§£èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒStreamingVLMåœ¨æ–°çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å®æ—¶æ€§èƒ½å’Œå¢å¼ºçš„è§†è§‰é—®ç­”èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.08696', 'title': "Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence\n  Reweighting", 'url': 'https://huggingface.co/papers/2510.08696', 'abstract': 'LENS modifies GRPO by assigning confidence-dependent rewards to incorrect responses, improving efficiency and performance in reinforcement learning with verifiable rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improving large language models (LLMs) on reasoning tasks, with Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO wastes substantial compute on negative groups: groups in which no sampled response is correct yield zero advantage and thus no gradient. We ask whether negative groups can be leveraged without extra supervision. Starting from a maximum-likelihood (MLE) objective in reward modeling, we show that the MLE gradient is equivalent to a policy gradient for a modified value function. This value function adds a confidence-weighted penalty on incorrect responses, imposing larger penalties on more confident mistakes. We refer to this as Likelihood Estimation with Negative Samples (LENS). LENS modifies GRPO to assign non-zero, confidence-dependent rewards to incorrect generations, making negative groups informative and converting previously wasted samples into useful gradient updates. On the MATH benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently outperforms GRPO baseline, with significant gains on harder items. These results demonstrate a principled and practical way to "rescue" negative groups, improving efficiency and performance in RLVR.', 'score': 8, 'issue_id': 6375, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': '8ebc9455daa7299d', 'authors': ['Yunzhen Feng', 'Parag Jain', 'Anthony Hartshorn', 'Yaqi Duan', 'Julia Kempe'], 'affiliations': ['Meta Superintelligence Labs', 'New York University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08696.jpg', 'data': {'categories': ['#training', '#reasoning', '#benchmark', '#rl', '#optimization', '#rlhf'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² ÑƒÑ€Ğ¾ĞºĞ¸: ĞºĞ°Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ Ğ¸Ğ· Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ LENS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ GRPO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° GRPO Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½ Ñ‚Ñ€Ğ°Ñ‚Ğ¸Ñ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ²Ğ¿ÑƒÑÑ‚ÑƒÑ Ğ½Ğ° Â«Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹Â» â€” ÑĞ»ÑƒÑ‡Ğ°Ğ¸, Ğ³Ğ´Ğµ Ğ²ÑĞµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğµ Ğ¸ Ğ½Ğµ Ğ´Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°. LENS Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ¾, Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ ÑˆÑ‚Ñ€Ğ°Ñ„Ñ‹, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ñ‡ĞµĞ¼ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½ĞµĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ, Ñ‚ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑˆÑ‚Ñ€Ğ°Ñ„. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MATH Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LENS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ GRPO, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ±ĞµÑĞ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Transforming Mistakes into Learning: LENS in Reinforcement Learning', 'desc': 'This paper introduces LENS, a method that enhances Group Relative Policy Optimization (GRPO) by incorporating confidence-dependent rewards for incorrect responses in reinforcement learning. By leveraging negative groups, which traditionally provide no gradient information, LENS assigns non-zero penalties based on the confidence of mistakes, thus transforming wasted computational resources into valuable learning signals. The approach is grounded in a maximum-likelihood objective, showing that the modified value function can effectively guide policy updates. Experimental results on the MATH benchmark demonstrate that LENS significantly outperforms the GRPO baseline, particularly on challenging tasks, highlighting its efficiency and effectiveness in reinforcement learning with verifiable rewards.'}, 'zh': {'title': 'åˆ©ç”¨ç½®ä¿¡åº¦æå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡', 'desc': 'LENSæ˜¯ä¸€ç§æ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¯¹é”™è¯¯å“åº”åˆ†é…ä¸ç½®ä¿¡åº¦ç›¸å…³çš„å¥–åŠ±ï¼Œæå‡äº†æ•ˆç‡å’Œæ€§èƒ½ã€‚å®ƒåœ¨å¥–åŠ±å»ºæ¨¡ä¸­é‡‡ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰ç›®æ ‡ï¼Œå±•ç¤ºäº†MLEæ¢¯åº¦ä¸ä¿®æ”¹åçš„ä»·å€¼å‡½æ•°çš„ç­–ç•¥æ¢¯åº¦ç­‰ä»·ã€‚è¯¥ä»·å€¼å‡½æ•°å¯¹é”™è¯¯å“åº”æ–½åŠ ç½®ä¿¡åº¦åŠ æƒçš„æƒ©ç½šï¼Œå¯¹æ›´è‡ªä¿¡çš„é”™è¯¯æ–½åŠ æ›´å¤§çš„æƒ©ç½šã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒLENSä½¿å¾—è´Ÿæ ·æœ¬ç»„å˜å¾—æœ‰ç”¨ï¼Œä»è€Œæé«˜äº†å¼ºåŒ–å­¦ä¹ çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.08189', 'title': 'R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth\n  and Depth?', 'url': 'https://huggingface.co/papers/2510.08189', 'abstract': "R-HORIZON, a method using query composition, improves long-horizon reasoning in Large Reasoning Models through a benchmark of complex multi-step tasks, enhancing performance and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought (CoT). However, existing benchmarks mainly focus on immediate, single-horizon tasks, failing to adequately evaluate models' ability to understand and respond to complex, long-horizon scenarios. To address this incomplete evaluation of Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to stimulate long-horizon reasoning behaviors in LRMs through query composition. Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising complex multi-step reasoning tasks with interdependent problems that span long reasoning horizons. Through comprehensive evaluation of LRMs using the R-HORIZON benchmark, we find that even the most advanced LRMs suffer significant performance degradation. Our analysis reveals that LRMs exhibit limited effective reasoning length and struggle to allocate thinking budget across multiple problems appropriately. Recognizing these limitations, we use R-HORIZON to construct long-horizon reasoning data for reinforcement learning with verified rewards (RLVR). Compared to training with single-horizon data, RLVR with R-HORIZON not only substantially improves performance on the multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as a scalable, controllable, and low-cost paradigm for enhancing and evaluating the long-horizon reasoning capabilities of LRMs.", 'score': 8, 'issue_id': 6375, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': '4d2818b42a388048', 'authors': ['Yi Lu', 'Jianing Wang', 'Linsen Guo', 'Wei He', 'Hongyin Tang', 'Tao Gui', 'Xuanjing Huang', 'Xuezhi Cao', 'Wei Wang', 'Xunliang Cai'], 'affiliations': ['Fudan University', 'Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2510.08189.jpg', 'data': {'categories': ['#long_context', '#training', '#reasoning', '#benchmark', '#rl'], 'emoji': 'ğŸ”­', 'ru': {'title': 'R-HORIZON: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ AI Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ´Ğ°Ğ»ÑŒĞ½Ğ¸Ğµ Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ R-HORIZON - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ R-HORIZON Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° 7.5 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'R-HORIZON: Elevating Long-Horizon Reasoning in AI Models', 'desc': "The paper introduces R-HORIZON, a novel method that enhances long-horizon reasoning in Large Reasoning Models (LRMs) through query composition. It identifies a gap in existing benchmarks that primarily assess immediate tasks, which do not challenge models' abilities to handle complex, multi-step reasoning scenarios. By creating a benchmark that includes interdependent problems requiring extended reasoning, R-HORIZON reveals significant performance limitations in current LRMs. The study demonstrates that using R-HORIZON for reinforcement learning with verified rewards leads to substantial improvements in both multi-horizon and standard reasoning tasks."}, 'zh': {'title': 'R-HORIZONï¼šæå‡é•¿æ—¶é—´æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'R-HORIZONæ˜¯ä¸€ç§é€šè¿‡æŸ¥è¯¢ç»„åˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹æ¨ç†æ¨¡å‹åœ¨é•¿æ—¶é—´æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤æ‚å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡çš„åŸºå‡†ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°æ¨¡å‹åœ¨é•¿æ—¶é—´æ¨ç†åœºæ™¯ä¸­çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„æ¨ç†æ¨¡å‹åœ¨å¤„ç†é•¿æ—¶é—´æ¨ç†æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å¤šä¸ªé—®é¢˜ä¹‹é—´åˆ†é…æ€è€ƒèµ„æºæ–¹é¢å­˜åœ¨å›°éš¾ã€‚é€šè¿‡ä½¿ç”¨R-HORIZONè¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ¨¡å‹åœ¨å¤šæ—¶é—´æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œå‡†ç¡®ç‡ä¹Ÿæœ‰æ‰€æé«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.08457', 'title': 'ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level\n  Entropy Shaping', 'url': 'https://huggingface.co/papers/2510.08457', 'abstract': 'ARES, a unified framework for adaptive reasoning, dynamically adjusts exploration effort based on task difficulty using high window-entropy tokens and hierarchical entropy rewards, improving performance and efficiency across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large reasoning models (MLRMs) have substantially improved their ability to solve complex textual and visual tasks. However, these models tend to overthink on simple problems, producing unnecessarily lengthy reasoning traces, while under-exploring on challenging ones, leading to missed solutions. To address this imbalance, we propose ARES, a unified open-source framework for adaptive reasoning that dynamically allocates exploration effort based on task difficulty. Our approach is motivated by two key empirical findings: (i) while single-token entropy is noisy, high window-entropy (HWE) tokens (token-level entropies averaged under a sliding window) can reliably capture reasoning-critical moments; and (ii) reducing HWE usage benefits easy problems, while increasing it is essential for solving hard ones. Building on these insights, ARES introduces a two-stage training pipeline. In the Adaptive Cold-Start stage, we curate multimodal and textual data paired with reasoning traces of length proportional to problem difficulty, equipping the model with initial difficulty awareness. In the second stage, we develop Adaptive Entropy Policy Optimization (AEPO), which uses HWE tokens as exploration triggers to decide when to explore, and a hierarchical entropy reward with dynamic KL control to decide how much to explore. Extensive experiments demonstrate that ARES achieves superior performance and reasoning efficiency across diverse mathematical, logical, and multimodal benchmarks, while closing the gap to leading commercial systems under significantly lower inference costs.', 'score': 7, 'issue_id': 6377, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': 'a820ad18887c9b93', 'authors': ['Shuang Chen', 'Yue Guo', 'Yimeng Ye', 'Shijue Huang', 'Wenbo Hu', 'Haoxi Li', 'Manyuan Zhang', 'Jiayu Chen', 'Song Guo', 'Nanyun Peng'], 'affiliations': ['Columbia University', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2510.08457.jpg', 'data': {'categories': ['#training', '#reasoning', '#benchmark', '#optimization', '#open_source', '#multimodal', '#dataset'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ARES: ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ AI Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ARES, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ¸Ğ»Ğ¸Ğ¹ Ğ² multimodal LLM: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Â«Ğ´ÑƒĞ¼Ğ°ÑÑ‚Â» Ğ½Ğ°Ğ´ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ´ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸. ARES Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ high window-entropy Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Adaptive Entropy Policy Optimization Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° inference Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Adaptive Reasoning Made Efficient with ARES', 'desc': 'ARES is a novel framework designed to enhance adaptive reasoning in multimodal large reasoning models (MLRMs) by adjusting exploration efforts based on the difficulty of tasks. It utilizes high window-entropy tokens to identify critical reasoning moments and employs hierarchical entropy rewards to optimize exploration strategies. The framework consists of a two-stage training process that first establishes difficulty awareness through curated data and then applies Adaptive Entropy Policy Optimization to fine-tune exploration. As a result, ARES improves both performance and efficiency in solving complex problems while reducing unnecessary reasoning for simpler tasks.'}, 'zh': {'title': 'ARESï¼šè‡ªé€‚åº”æ¨ç†çš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'ARESæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éš¾åº¦åŠ¨æ€è°ƒæ•´æ¢ç´¢åŠ›åº¦ã€‚å®ƒåˆ©ç”¨é«˜çª—å£ç†µä»¤ç‰Œå’Œå±‚æ¬¡ç†µå¥–åŠ±æ¥æé«˜åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼ŒARESé¦–å…ˆé€šè¿‡ä¸æ¨ç†è½¨è¿¹ç›¸åŒ¹é…çš„æ•°æ®æ¥å¢å¼ºæ¨¡å‹çš„éš¾åº¦æ„è¯†ï¼Œç„¶åé€šè¿‡è‡ªé€‚åº”ç†µç­–ç•¥ä¼˜åŒ–æ¥å†³å®šä½•æ—¶å’Œå¦‚ä½•è¿›è¡Œæ¢ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARESåœ¨æ•°å­¦ã€é€»è¾‘å’Œå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶åœ¨æ¨ç†æˆæœ¬ä¸Šæ˜¾è‘—ä½äºé¢†å…ˆçš„å•†ä¸šç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09517', 'title': 'StatEval: A Comprehensive Benchmark for Large Language Models in\n  Statistics', 'url': 'https://huggingface.co/papers/2510.09517', 'abstract': 'StatEval is a comprehensive benchmark for statistical reasoning, covering foundational and research-level problems, and highlights the limitations of current LLMs in this domain.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable advances in mathematical and logical reasoning, yet statistics, as a distinct and integrative discipline, remains underexplored in benchmarking efforts. To address this gap, we introduce StatEval, the first comprehensive benchmark dedicated to statistics, spanning both breadth and depth across difficulty levels. StatEval consists of 13,817 foundational problems covering undergraduate and graduate curricula, together with 2374 research-level proof tasks extracted from leading journals. To construct the benchmark, we design a scalable multi-agent pipeline with human-in-the-loop validation that automates large-scale problem extraction, rewriting, and quality control, while ensuring academic rigor. We further propose a robust evaluation framework tailored to both computational and proof-based tasks, enabling fine-grained assessment of reasoning ability. Experimental results reveal that while closed-source models such as GPT5-mini achieve below 57\\% on research-level problems, with open-source models performing significantly lower. These findings highlight the unique challenges of statistical reasoning and the limitations of current LLMs. We expect StatEval to serve as a rigorous benchmark for advancing statistical intelligence in large language models. All data and code are available on our web platform: https://stateval.github.io/.', 'score': 6, 'issue_id': 6376, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': 'e02b9230dd95512d', 'authors': ['Yuchen Lu', 'Run Yang', 'Yichen Zhang', 'Shuguang Yu', 'Runpeng Dai', 'Ziwei Wang', 'Jiayi Xiang', 'Wenxin E', 'Siran Gao', 'Xinyao Ruan', 'Yirui Huang', 'Chenjing Xi', 'Haibo Hu', 'Yueming Fu', 'Qinglan Yu', 'Xiaobing Wei', 'Jiani Gu', 'Rui Sun', 'Jiaxuan Jia', 'Fan Zhou'], 'affiliations': ['Shanghai University of Finance and Economics', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2510.09517.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#survey', '#reasoning'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'StatEval: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ÑŒ LLM Ğ² ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ StatEval â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 13,817 Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ±Ğ°ĞºĞ°Ğ»Ğ°Ğ²Ñ€Ğ¸Ğ°Ñ‚Ğ° Ğ¸ Ğ¼Ğ°Ğ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚ÑƒÑ€Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ 2,374 Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ¸Ğ· Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¶ÑƒÑ€Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ”Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ multi-agent pipeline Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ closed-source Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4-mini Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 57% Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ° open-source Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ ĞµÑ‰Ñ‘ Ñ…ÑƒĞ¶Ğµ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'StatEval: Advancing Statistical Reasoning in LLMs', 'desc': 'StatEval is a new benchmark designed to evaluate statistical reasoning in large language models (LLMs). It includes a wide range of problems, from foundational undergraduate tasks to advanced research-level proofs, totaling over 16,000 questions. The benchmark uses a multi-agent system with human validation to ensure high-quality problem extraction and assessment. Results show that current LLMs struggle with statistical reasoning, indicating a need for improvement in this area.'}, 'zh': {'title': 'StatEvalï¼šç»Ÿè®¡æ¨ç†çš„æ–°åŸºå‡†', 'desc': 'StatEvalæ˜¯ä¸€ä¸ªå…¨é¢çš„ç»Ÿè®¡æ¨ç†åŸºå‡†ï¼Œæ¶µç›–åŸºç¡€å’Œç ”ç©¶çº§åˆ«çš„é—®é¢˜ï¼Œçªæ˜¾äº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿™ä¸€é¢†åŸŸçš„å±€é™æ€§ã€‚è¯¥åŸºå‡†åŒ…å«13,817ä¸ªåŸºç¡€é—®é¢˜ï¼Œæ¶‰åŠæœ¬ç§‘å’Œç ”ç©¶ç”Ÿè¯¾ç¨‹ï¼Œä»¥åŠ2,374ä¸ªä»é¡¶çº§æœŸåˆŠæå–çš„ç ”ç©¶çº§è¯æ˜ä»»åŠ¡ã€‚ä¸ºäº†æ„å»ºè¿™ä¸ªåŸºå‡†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯æ‰©å±•çš„å¤šä»£ç†ç®¡é“ï¼Œç»“åˆäººå·¥éªŒè¯ï¼Œè‡ªåŠ¨åŒ–å¤§è§„æ¨¡é—®é¢˜æå–ã€é‡å†™å’Œè´¨é‡æ§åˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ä¸€äº›å°é—­æºæ¨¡å‹åœ¨ç ”ç©¶çº§é—®é¢˜ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œä½†StatEvalä¸ºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»Ÿè®¡æ™ºèƒ½æä¾›äº†ä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.04759', 'title': 'Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open\n  Vocabulary Occupancy Prediction', 'url': 'https://huggingface.co/papers/2510.04759', 'abstract': 'PG-Occ, a Progressive Gaussian Transformer Framework, enhances 3D occupancy prediction with progressive densification and anisotropy-aware sampling, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The 3D occupancy prediction task has witnessed remarkable progress in recent years, playing a crucial role in vision-based autonomous driving systems. While traditional methods are limited to fixed semantic categories, recent approaches have moved towards predicting text-aligned features to enable open-vocabulary text queries in real-world scenes. However, there exists a trade-off in text-aligned scene modeling: sparse Gaussian representation struggles to capture small objects in the scene, while dense representation incurs significant computational overhead. To address these limitations, we present PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables open-vocabulary 3D occupancy prediction. Our framework employs progressive online densification, a feed-forward strategy that gradually enhances the 3D Gaussian representation to capture fine-grained scene details. By iteratively enhancing the representation, the framework achieves increasingly precise and detailed scene understanding. Another key contribution is the introduction of an anisotropy-aware sampling strategy with spatio-temporal fusion, which adaptively assigns receptive fields to Gaussians at different scales and stages, enabling more effective feature aggregation and richer scene information capture. Through extensive evaluations, we demonstrate that PG-Occ achieves state-of-the-art performance with a relative 14.3% mIoU improvement over the previous best performing method. Code and pretrained models will be released upon publication on our project page: https://yanchi-3dv.github.io/PG-Occ', 'score': 6, 'issue_id': 6375, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': '681d2ff717aff51e', 'authors': ['Chi Yan', 'Dan Xu'], 'affiliations': ['The Hong Kong University of Science and Technology (HKUST)', 'ZEEKR Automobile R&D Co., Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2510.04759.jpg', 'data': {'categories': ['#3d', '#cv', '#open_source', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ğ»Ğ¾Ñ‚Ğ½ĞµĞ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PG-Occ â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Gaussian Transformer Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ 3D occupancy Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ open-vocabulary Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ â€” Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ğ»Ğ¾Ñ‚Ğ½ĞµĞ½Ğ¸Ğµ 3D-Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ½Ğ¸Ğ·Ğ¾Ñ‚Ñ€Ğ¾Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ°Ğ¼ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ mIoU Ğ½Ğ° 14.3% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing 3D Occupancy Prediction with PG-Occ', 'desc': 'The paper introduces PG-Occ, a Progressive Gaussian Transformer Framework designed to improve 3D occupancy prediction, which is essential for autonomous driving systems. It addresses the limitations of traditional methods by using progressive online densification to enhance Gaussian representations, allowing for better detail capture in complex scenes. Additionally, the framework incorporates an anisotropy-aware sampling strategy that optimizes feature aggregation across different scales, leading to richer scene understanding. The results show that PG-Occ outperforms previous methods, achieving a 14.3% improvement in mean Intersection over Union (mIoU).'}, 'zh': {'title': 'PG-Occï¼šæå‡3Då ç”¨é¢„æµ‹çš„åˆ›æ–°æ¡†æ¶', 'desc': 'PG-Occæ˜¯ä¸€ç§è¿›æ­¥çš„é«˜æ–¯å˜æ¢æ¡†æ¶ï¼Œæ—¨åœ¨æå‡3Då ç”¨é¢„æµ‹çš„ç²¾åº¦ã€‚è¯¥æ¡†æ¶é€šè¿‡é€æ­¥åœ¨çº¿ç¨ å¯†åŒ–ï¼Œé€æ¸å¢å¼º3Dé«˜æ–¯è¡¨ç¤ºï¼Œä»¥æ•æ‰åœºæ™¯ä¸­çš„ç»†èŠ‚ã€‚å®ƒè¿˜å¼•å…¥äº†ä¸€ä¸ªè€ƒè™‘å„å‘å¼‚æ€§çš„é‡‡æ ·ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒå°ºåº¦å’Œé˜¶æ®µè‡ªé€‚åº”åœ°åˆ†é…æ„Ÿå—é‡ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°èšåˆç‰¹å¾ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼ŒPG-Occåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ–¹æ³•ï¼Œå–å¾—äº†14.3%çš„mIoUæå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09510', 'title': 'MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for\n  Reasoning-Intensive Multimodal Retrieval', 'url': 'https://huggingface.co/papers/2510.09510', 'abstract': 'MRMR is a benchmark for expert-level multidisciplinary multimodal retrieval that includes reasoning-intensive tasks, contradiction retrieval, and image-text interleaved sequences, highlighting the need for improved multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MRMR, the first expert-level multidisciplinary multimodal retrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries spanning 23 domains, with positive documents carefully verified by human experts. Compared to prior benchmarks, MRMR introduces three key advancements. First, it challenges retrieval systems across diverse areas of expertise, enabling fine-grained model comparison across domains. Second, queries are reasoning-intensive, with images requiring deeper interpretation such as diagnosing microscopic slides. We further introduce Contradiction Retrieval, a novel task requiring models to identify conflicting concepts. Finally, queries and documents are constructed as image-text interleaved sequences. Unlike earlier benchmarks restricted to single images or unimodal documents, MRMR offers a realistic setting with multi-image queries and mixed-modality corpus documents. We conduct an extensive evaluation of 4 categories of multimodal retrieval systems and 14 frontier models on MRMR. The text embedding model Qwen3-Embedding with LLM-generated image captions achieves the highest performance, highlighting substantial room for improving multimodal retrieval models. Although latest multimodal models such as Ops-MM-Embedding perform competitively on expert-domain queries, they fall short on reasoning-intensive tasks. We believe that MRMR paves the way for advancing multimodal retrieval in more realistic and challenging scenarios.', 'score': 5, 'issue_id': 6376, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': '7df9f5952a859ef9', 'authors': ['Siyue Zhang', 'Yuan Gao', 'Xiao Zhou', 'Yilun Zhao', 'Tingyu Song', 'Arman Cohan', 'Anh Tuan Luu', 'Chen Zhao'], 'affiliations': ['Center for Data Science, New York University', 'NYU Shanghai', 'Nanyang Technological University', 'Shanghai Jiao Tong University', 'University of the Chinese Academy of Sciences', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2510.09510.jpg', 'data': {'categories': ['#benchmark', '#games', '#reasoning', '#multimodal'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ reasoning', 'desc': 'MRMR â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ retrieval ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ reasoning. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1502 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸Ğ· 23 Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ image-text interleaved sequences (Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Contradiction Retrieval â€” Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ°Ñ‰Ğ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 14 frontier Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ°Ñ embedding Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen3-Embedding Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ½Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑÑ‘ ĞµÑ‰Ñ‘ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ reasoning-intensive Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'MRMR: Advancing Multimodal Retrieval with Expert-Level Challenges', 'desc': 'The MRMR benchmark is designed to test advanced multimodal retrieval systems by incorporating complex reasoning tasks and contradiction retrieval. It includes 1,502 queries across 23 different domains, with documents verified by experts to ensure quality. This benchmark emphasizes the need for models that can interpret images deeply and handle mixed modalities effectively. The evaluation shows that while some models perform well, there is significant potential for improvement in reasoning capabilities within multimodal retrieval.'}, 'zh': {'title': 'MRMRï¼šæ¨åŠ¨å¤šæ¨¡æ€æ£€ç´¢çš„æ–°åŸºå‡†', 'desc': 'MRMRæ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸“å®¶çº§å¤šå­¦ç§‘å¤šæ¨¡æ€æ£€ç´¢çš„åŸºå‡†ï¼Œå¼ºè°ƒäº†å¯¹å¤šæ¨¡æ€æ¨¡å‹çš„æ”¹è¿›éœ€æ±‚ã€‚è¯¥åŸºå‡†åŒ…å«1502ä¸ªæŸ¥è¯¢ï¼Œè¦†ç›–23ä¸ªé¢†åŸŸï¼Œæ‰€æœ‰æ­£é¢æ–‡æ¡£å‡ç”±äººç±»ä¸“å®¶ä»”ç»†éªŒè¯ã€‚MRMRå¼•å…¥äº†ä¸‰ä¸ªå…³é”®è¿›å±•ï¼ŒåŒ…æ‹¬è·¨é¢†åŸŸçš„æ£€ç´¢æŒ‘æˆ˜ã€éœ€è¦æ·±å…¥æ¨ç†çš„æŸ¥è¯¢ä»¥åŠçŸ›ç›¾æ£€ç´¢ä»»åŠ¡ã€‚é€šè¿‡å¯¹å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿçš„å¹¿æ³›è¯„ä¼°ï¼ŒMRMRä¸ºæ›´çœŸå®å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­çš„å¤šæ¨¡æ€æ£€ç´¢æä¾›äº†æ–°çš„å‘å±•æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09577', 'title': 'Dyna-Mind: Learning to Simulate from Experience for Better AI Agents', 'url': 'https://huggingface.co/papers/2510.09577', 'abstract': "Introducing Dyna-Mind, a two-stage training framework that enhances AI agents' reasoning and planning abilities through simulation, leading to improved performance in complex interactive environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models have recently shown remarkable progress in domains such as math and coding. However, their expert-level abilities in math and coding contrast sharply with their performance in long-horizon, interactive tasks such as web navigation and computer/phone-use. Inspired by literature on human cognition, we argue that current AI agents need ''vicarious trial and error'' - the capacity to mentally simulate alternative futures before acting - in order to enhance their understanding and performance in complex interactive environments. We introduce Dyna-Mind, a two-stage training framework that explicitly teaches (V)LM agents to integrate such simulation into their reasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which trains the agent to generate structured reasoning traces from expanded search trees built from real experience gathered through environment interactions. ReSim thus grounds the agent's reasoning in faithful world dynamics and equips it with the ability to anticipate future states in its reasoning. In stage 2, we propose Dyna-GRPO, an online reinforcement learning method to further strengthen the agent's simulation and decision-making ability by using both outcome rewards and intermediate states as feedback from real rollouts. Experiments on two synthetic benchmarks (Sokoban and ALFWorld) and one realistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively infuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome and interaction-level signals to learn better policies for long-horizon, planning-intensive tasks. Together, these results highlight the central role of simulation in enabling AI agents to reason, plan, and act more effectively in the ever more challenging environments.", 'score': 4, 'issue_id': 6375, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': 'c25b935d58908751', 'authors': ['Xiao Yu', 'Baolin Peng', 'Michel Galley', 'Hao Cheng', 'Qianhui Wu', 'Janardhan Kulkarni', 'Suman Nath', 'Zhou Yu', 'Jianfeng Gao'], 'affiliations': ['Columbia University, NY', 'Microsoft Research, Redmond'], 'pdf_title_img': 'assets/pdf/title_img/2510.09577.jpg', 'data': {'categories': ['#synthetic', '#training', '#reasoning', '#rl', '#optimization', '#agents'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ£Ñ‡Ğ¸Ğ¼ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ', 'desc': 'Dyna-Mind â€” ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´ ReSim ÑƒÑ‡Ğ¸Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑĞ¾ ÑÑ€ĞµĞ´Ğ¾Ğ¹. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Dyna-GRPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ reinforcement learning Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ĞºĞ°Ğº Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… (Sokoban, ALFWorld) Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ AndroidWorld Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Empowering AI with Simulation for Smarter Decision-Making', 'desc': "Dyna-Mind is a two-stage training framework designed to improve AI agents' reasoning and planning skills through simulation. The first stage, Reasoning with Simulations (ReSim), helps agents create structured reasoning paths by simulating potential future scenarios based on real experiences. The second stage, Dyna-GRPO, employs online reinforcement learning to enhance decision-making by utilizing both rewards and intermediate feedback from real interactions. This approach demonstrates that incorporating simulation significantly boosts AI agents' performance in complex tasks that require long-term planning and reasoning."}, 'zh': {'title': 'Dyna-Mindï¼šé€šè¿‡æ¨¡æ‹Ÿæå‡ AI ä»£ç†çš„æ¨ç†ä¸è§„åˆ’èƒ½åŠ›', 'desc': 'Dyna-Mind æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿå¢å¼º AI ä»£ç†çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œä»è€Œæé«˜å…¶åœ¨å¤æ‚äº¤äº’ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯æ¨¡æ‹Ÿæ¨ç†ï¼ˆReSimï¼‰ï¼Œå®ƒè®­ç»ƒä»£ç†ä»çœŸå®ç¯å¢ƒäº¤äº’ä¸­ç”Ÿæˆç»“æ„åŒ–çš„æ¨ç†è½¨è¿¹ï¼Œå¸®åŠ©ä»£ç†ç†è§£ä¸–ç•ŒåŠ¨æ€å¹¶é¢„æµ‹æœªæ¥çŠ¶æ€ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯ Dyna-GRPOï¼Œè¿™æ˜¯ä¸€ç§åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨ç»“æœå¥–åŠ±å’Œä¸­é—´çŠ¶æ€åé¦ˆï¼Œè¿›ä¸€æ­¥å¢å¼ºä»£ç†çš„æ¨¡æ‹Ÿå’Œå†³ç­–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDyna-Mind æœ‰æ•ˆåœ°æå‡äº† AI ä»£ç†åœ¨é•¿æ—¶é—´è§„åˆ’ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¼ºè°ƒäº†æ¨¡æ‹Ÿåœ¨æ¨ç†å’Œå†³ç­–ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.08697', 'title': 'BigCodeArena: Unveiling More Reliable Human Preferences in Code\n  Generation via Execution', 'url': 'https://huggingface.co/papers/2510.08697', 'abstract': 'BigCodeArena is an open human evaluation platform for code generation that enables real-time execution and interaction, revealing preferences and capabilities of LLMs in coding tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time evaluation from human perspectives to assess the quality of model responses. In the coding domain, manually examining the quality of LLM-generated content is extremely challenging, as it requires understanding long chunks of raw code and deliberately simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation backed by a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena enables the execution of LLM-generated code and allows humans to interact with the execution process and outcomes. We collected over 14,000 raw code-centric conversation sessions across 10 widely used LLMs, spanning 10 languages and 8 types of execution environments. Among these conversations, we identified more than 4,700 multi-turn samples with pairwise human preferences. Further analysis uncovers underexplored preferences of LLMs in fine-grained domains characterized by tasks, languages, and frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curated two benchmarks based on the collected data, namely BigCodeReward and AutoCodeArena. For BigCodeReward, we post-processed the 4,700 conversations and evaluated the consistency between reward models and human preferences. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are available. Inspired by these findings, we propose AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding quality of LLMs without human involvement. We find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation performance among recent emerging models.', 'score': 4, 'issue_id': 6375, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': 'ad4cd46a5b83fe05', 'authors': ['Terry Yue Zhuo', 'Xiaolong Jin', 'Hange Liu', 'Juyong Jiang', 'Tianyang Liu', 'Chen Gong', 'Bhupesh Bishnoi', 'Vaisakhi Mishra', 'Marek Suppa', 'Noah Ziems', 'Saiteja Utpala', 'Ming Xu', 'Guangyu Song', 'Kaixin Li', 'Yuhan Cao', 'Bo Liu', 'Zheng Liu', 'Sabina Abdurakhmanova', 'Wenhao Yu', 'Mengzhao Jia', 'Jihan Yao', 'Kenneth Hamilton', 'Kumar Shridhar', 'Minh Chien Vu', 'Dingmin Wang', 'Jiawei Liu', 'Zijian Wang', 'Qian Liu', 'Binyuan Hui', 'Meg Risdal', 'Ahsen Khaliq', 'Atin Sood', 'Zhenchang Xing', 'Wasi Uddin Ahmad', 'John Grundy', 'David Lo', 'Banghua Zhu', 'Xiaoning Du', 'Torsten Scholak', 'Leandro von Werra'], 'affiliations': ['CNRS, France', 'CSIROs Data61', 'Cisco', 'Comenius University in Bratislava', 'Detomo Inc', 'ETH Zurich', 'Google', 'HKUST (Guangzhou)', 'Hugging Face', 'IBM', 'Independent', 'Institute of Automation, CAS', 'Monash University', 'NUS', 'NVIDIA', 'Nevsky Collective', 'Purdue University', 'ServiceNow Research', 'Singapore Management University', 'Tano Labs', 'Tencent AI Lab', 'UCSD', 'UIUC', 'UVA', 'Uber', 'University of Notre Dame', 'University of Oxford', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2510.08697.jpg', 'data': {'categories': ['#games', '#dataset', '#multilingual', '#benchmark', '#open_source'], 'emoji': 'âš”ï¸', 'ru': {'title': 'ĞÑ€ĞµĞ½Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ°: ĞºÑ€Ğ°ÑƒĞ´ÑĞ¾Ñ€ÑĞ¸Ğ½Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'BigCodeArena â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ÑĞ´ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºĞ¾Ğ´ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 14,000 Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ 10 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… LLM Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸Ğ»Ğ¸ 4,700 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ´Ğ²Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°: BigCodeReward Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ reward-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ AutoCodeArena Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-5, Claude-Sonnet-4 Ğ¸ Claude-Opus-4 Ğ¿Ğ¾-Ğ¿Ñ€ĞµĞ¶Ğ½ĞµĞ¼Ñƒ Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° ÑÑ€ĞµĞ´Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing Code Evaluation with BigCodeArena', 'desc': 'BigCodeArena is a platform designed for evaluating code generation by large language models (LLMs) through real-time human interaction and code execution. It allows users to execute LLM-generated code and observe the outcomes, making it easier to assess the quality of the generated content. The platform has gathered extensive data from over 14,000 coding sessions across various LLMs and programming languages, revealing insights into human preferences and model capabilities. Additionally, it introduces benchmarks like BigCodeReward and AutoCodeArena to systematically evaluate LLM performance in coding tasks, highlighting the strengths of leading models in this domain.'}, 'zh': {'title': 'BigCodeArenaï¼šå®æ—¶è¯„ä¼°ä»£ç ç”Ÿæˆçš„å¼€æ”¾å¹³å°', 'desc': 'BigCodeArenaæ˜¯ä¸€ä¸ªå¼€æ”¾çš„äººç±»è¯„ä¼°å¹³å°ï¼Œä¸“æ³¨äºä»£ç ç”Ÿæˆï¼Œèƒ½å¤Ÿå®æ—¶æ‰§è¡Œå’Œäº’åŠ¨ï¼Œæ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¼–ç ä»»åŠ¡ä¸­çš„åå¥½å’Œèƒ½åŠ›ã€‚è¯¥å¹³å°åŸºäºChatbot Arenaï¼Œæ”¯æŒå¯¹LLMç”Ÿæˆçš„ä»£ç è¿›è¡Œæ‰§è¡Œï¼Œå¹¶å…è®¸äººç±»ä¸æ‰§è¡Œè¿‡ç¨‹å’Œç»“æœè¿›è¡Œäº’åŠ¨ã€‚æˆ‘ä»¬æ”¶é›†äº†è¶…è¿‡14,000ä¸ªä¸ä»£ç ç›¸å…³çš„å¯¹è¯ä¼šè¯ï¼Œæ¶µç›–10ç§æµè¡Œçš„LLMå’Œ8ç§æ‰§è¡Œç¯å¢ƒï¼Œè¯†åˆ«å‡ºè¶…è¿‡4,700ä¸ªå¤šè½®æ ·æœ¬åŠå…¶äººç±»åå¥½ã€‚é€šè¿‡åˆ†æï¼Œæˆ‘ä»¬å‘ç°LLMsåœ¨ç»†åˆ†é¢†åŸŸçš„åå¥½å°šæœªè¢«å……åˆ†æ¢ç´¢ï¼Œå¹¶æå‡ºäº†BigCodeRewardå’ŒAutoCodeArenaä¸¤ä¸ªåŸºå‡†ï¼Œä»¥ç³»ç»Ÿæ€§åœ°è¯„ä¼°å‰æ²¿LLMsçš„ä»£ç ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.08047', 'title': 'Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic\n  Speech Recognition', 'url': 'https://huggingface.co/papers/2510.08047', 'abstract': 'A parameter-space correction method reduces Word Error Rate in ASR systems by addressing pseudo-label biases without target ground truth.  \t\t\t\t\tAI-generated summary \t\t\t\t Robust ASR under domain shift is crucial because real-world systems encounter unseen accents and domains with limited labeled data. Although pseudo-labeling offers a practical workaround, it often introduces systematic, accent-specific errors that filtering fails to fix. We ask: How can we correct these recurring biases without target ground truth? We propose a simple parameter-space correction: in a source domain containing both real and pseudo-labeled data, two ASR models are fine-tuned from the same initialization, one on ground-truth labels and the other on pseudo-labels, and their weight difference forms a correction vector that captures pseudo-label biases. When applied to a pseudo-labeled target model, this vector enhances recognition, achieving up to a 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten African accents with the Whisper tiny model.', 'score': 4, 'issue_id': 6375, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': 'abba5710f84f98b1', 'authors': ['Yi-Cheng Lin', 'Yu-Hsuan Li Liang', 'Hsuan Su', 'Tzu-Quan Lin', 'Shang-Tse Chen', 'Yun-Nung Chen', 'Hung-yi Lee'], 'affiliations': ['National Taiwan University, Taipei, Taiwan'], 'pdf_title_img': 'assets/pdf/title_img/2510.08047.jpg', 'data': {'categories': ['#data', '#training', '#low_resource', '#optimization', '#audio'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞšĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR) Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ²Ğ½Ğ¾ÑÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ Ğ² Ğ²ĞµÑĞ°Ñ… Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚ĞºĞ°Ñ…, Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ¾ 35% Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ„Ñ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¸Ñ… Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Correcting Pseudo-Label Biases for Better ASR Performance', 'desc': 'This paper presents a method to improve Automatic Speech Recognition (ASR) systems by correcting biases introduced by pseudo-labels, which are labels generated without ground truth. The authors highlight that ASR systems often struggle with unseen accents and limited labeled data, leading to increased errors. They propose a parameter-space correction technique that involves fine-tuning two ASR models: one on real labels and the other on pseudo-labels, to create a correction vector. This vector is then used to adjust a target model, resulting in a significant reduction in Word Error Rate (WER), demonstrating the effectiveness of the approach across various African accents.'}, 'zh': {'title': 'ä¿®æ­£ä¼ªæ ‡ç­¾åå·®ï¼Œæå‡è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å‚æ•°ç©ºé—´ä¿®æ­£æ–¹æ³•ï¼Œç”¨äºå‡å°‘è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¸­çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼ªæ ‡ç­¾åå·®çš„é—®é¢˜ï¼Œè€Œæ— éœ€ç›®æ ‡çœŸå®æ ‡ç­¾ã€‚é€šè¿‡åœ¨æºåŸŸä¸­å¯¹ä¸¤ä¸ªASRæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä¸€ä¸ªä½¿ç”¨çœŸå®æ ‡ç­¾ï¼Œå¦ä¸€ä¸ªä½¿ç”¨ä¼ªæ ‡ç­¾ï¼Œå½¢æˆçš„æƒé‡å·®å¼‚æ„æˆäº†æ•æ‰ä¼ªæ ‡ç­¾åå·®çš„ä¿®æ­£å‘é‡ã€‚åº”ç”¨è¯¥å‘é‡åï¼Œåœ¨AfriSpeech-200æ•°æ®é›†ä¸Šï¼Œè¯†åˆ«ç‡æé«˜ï¼Œè¯é”™è¯¯ç‡ç›¸å¯¹é™ä½äº†35%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.07745', 'title': 'Parallel Test-Time Scaling for Latent Reasoning Models', 'url': 'https://huggingface.co/papers/2510.07745', 'abstract': 'Parallel test-time scaling is enabled for latent reasoning models using uncertainty-inspired sampling strategies and a Latent Reward Model for effective trajectory selection.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \\ This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.', 'score': 4, 'issue_id': 6376, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': 'ee812aca3f8a40eb', 'authors': ['Runyang You', 'Yongqi Li', 'Meng Liu', 'Wenjie Wang', 'Liqiang Nie', 'Wenjie Li'], 'affiliations': ['Harbin Institute of Technology (Shenzhen)', 'Shandong Jianzhu University', 'The Hong Kong Polytechnic University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.07745.jpg', 'data': {'categories': ['#inference', '#architecture', '#reasoning', '#optimization', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğµ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ: Monte Carlo Dropout Ğ¸ Ğ°Ğ´Ğ´Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ ÑˆÑƒĞ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸. Ğ”Ğ»Ñ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ….'}, 'en': {'title': 'Enhancing Latent Reasoning with Parallel Test-Time Scaling', 'desc': 'This paper presents a method to improve the performance of latent reasoning models during test-time by using parallel test-time scaling (TTS). It introduces two innovative sampling strategies, Monte Carlo Dropout and Additive Gaussian Noise, which help in selecting effective reasoning trajectories in continuous vector spaces. Additionally, a Latent Reward Model (LatentRM) is developed to score these trajectories, enhancing the aggregation process. The results demonstrate that these techniques not only scale well with computational resources but also provide unique exploration dynamics for better inference.'}, 'zh': {'title': 'æå‡æ½œåœ¨æ¨ç†æ¨¡å‹çš„å¹¶è¡Œæµ‹è¯•æ‰©å±•èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿æ½œåœ¨æ¨ç†æ¨¡å‹èƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶è¿›è¡Œå¹¶è¡Œæ‰©å±•ã€‚é€šè¿‡å¼•å…¥ä¸ç¡®å®šæ€§å¯å‘çš„é‡‡æ ·ç­–ç•¥ï¼Œå¦‚è’™ç‰¹å¡æ´›ä¸¢å¼ƒæ³•å’ŒåŠ æ€§é«˜æ–¯å™ªå£°ï¼Œè§£å†³äº†åœ¨è¿ç»­ç©ºé—´ä¸­ç¼ºä¹é‡‡æ ·æœºåˆ¶çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§æ½œåœ¨å¥–åŠ±æ¨¡å‹ï¼ˆLatentRMï¼‰ï¼Œç”¨äºæœ‰æ•ˆåœ°é€‰æ‹©æ¨ç†è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨è®¡ç®—èµ„æºä¸Šå…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼æ½œåœ¨æ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.08867', 'title': 'ReviewerToo: Should AI Join The Program Committee? A Look At The Future\n  of Peer Review', 'url': 'https://huggingface.co/papers/2510.08867', 'abstract': 'ReviewerToo, a modular AI-assisted peer review framework, complements human judgment with systematic assessments, achieving high accuracy and quality in specific domains while highlighting areas where human expertise remains essential.  \t\t\t\t\tAI-generated summary \t\t\t\t Peer review is the cornerstone of scientific publishing, yet it suffers from inconsistencies, reviewer subjectivity, and scalability challenges. We introduce ReviewerToo, a modular framework for studying and deploying AI-assisted peer review to complement human judgment with systematic and consistent assessments. ReviewerToo supports systematic experiments with specialized reviewer personas and structured evaluation criteria, and can be partially or fully integrated into real conference workflows. We validate ReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR 2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy for the task of categorizing a paper as accept/reject compared to 83.9% for the average human reviewer. Additionally, ReviewerToo-generated reviews are rated as higher quality than the human average by an LLM judge, though still trailing the strongest expert contributions. Our analysis highlights domains where AI reviewers excel (e.g., fact-checking, literature coverage) and where they struggle (e.g., assessing methodological novelty and theoretical contributions), underscoring the continued need for human expertise. Based on these findings, we propose guidelines for integrating AI into peer-review pipelines, showing how AI can enhance consistency, coverage, and fairness while leaving complex evaluative judgments to domain experts. Our work provides a foundation for systematic, hybrid peer-review systems that scale with the growth of scientific publishing.', 'score': 3, 'issue_id': 6375, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': '2d81b8196c727d2e', 'authors': ['Gaurav Sahu', 'Hugo Larochelle', 'Laurent Charlin', 'Christopher Pal'], 'affiliations': ['Canada CIFAR Chair', 'HEC Montreal', 'Mila Quebec AI Institute', 'Polytechnique Montreal', 'ServiceNow Research', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2510.08867.jpg', 'data': {'categories': ['#data', '#dataset', '#multimodal', '#ethics', '#benchmark', '#science'], 'emoji': 'ğŸ”', 'ru': {'title': 'AI-Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚: ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ñ‹ Ğ¿Ğ»ÑÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ReviewerToo â€” Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ AI-Ğ°ÑÑĞ¸ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸Ğ· 1963 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ ICLR 2025 ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 81.8% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ accept/reject (Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 83.9% Ñƒ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ°), Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ AI-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑˆĞµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ AI Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ¾Ğ¼ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ñ‹ Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²ĞºĞ»Ğ°Ğ´Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³Ğ´Ğµ AI Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ·Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Peer Review with AI: A Hybrid Approach', 'desc': 'ReviewerToo is a modular framework designed to enhance the peer review process in scientific publishing by integrating AI with human judgment. It systematically evaluates paper submissions, achieving high accuracy in categorizing them as accept or reject, while also providing consistent assessments. The framework has been validated using a dataset from ICLR 2025, showing that AI-generated reviews can match human accuracy and are often rated higher in quality. However, it also identifies areas where human expertise is crucial, particularly in evaluating methodological novelty and theoretical contributions, suggesting a hybrid approach for future peer review systems.'}, 'zh': {'title': 'AIè¾…åŠ©åŒè¡Œè¯„å®¡ï¼Œæå‡ç§‘å­¦å‡ºç‰ˆè´¨é‡', 'desc': 'ReviewerTooæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–çš„AIè¾…åŠ©åŒè¡Œè¯„å®¡æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç³»ç»ŸåŒ–è¯„ä¼°æ¥è¡¥å……äººç±»åˆ¤æ–­ï¼Œä»è€Œåœ¨ç‰¹å®šé¢†åŸŸå®ç°é«˜å‡†ç¡®æ€§å’Œè´¨é‡ã€‚è¯¥æ¡†æ¶æ”¯æŒä½¿ç”¨ä¸“ä¸šè¯„å®¡è§’è‰²å’Œç»“æ„åŒ–è¯„ä¼°æ ‡å‡†è¿›è¡Œç³»ç»Ÿå®éªŒï¼Œå¯ä»¥éƒ¨åˆ†æˆ–å®Œå…¨èå…¥å®é™…ä¼šè®®å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReviewerTooåœ¨å¯¹è®ºæ–‡è¿›è¡Œæ¥å—/æ‹’ç»åˆ†ç±»æ—¶ï¼Œå‡†ç¡®ç‡è¾¾åˆ°81.8%ï¼Œæ¥è¿‘äººç±»è¯„å®¡è€…çš„83.9%ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼ŒAIè¯„å®¡åœ¨äº‹å®æ ¸æŸ¥å’Œæ–‡çŒ®è¦†ç›–ç­‰é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨è¯„ä¼°æ–¹æ³•æ–°é¢–æ€§å’Œç†è®ºè´¡çŒ®æ–¹é¢ä»éœ€ä¾èµ–äººç±»ä¸“å®¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05608', 'title': 'A Goal Without a Plan Is Just a Wish: Efficient and Effective Global\n  Planner Training for Long-Horizon Agent Tasks', 'url': 'https://huggingface.co/papers/2510.05608', 'abstract': "A plan-and-execute framework with EAGLET enhances LLM-based agents' planning abilities, achieving state-of-the-art performance in long-horizon tasks with reduced training costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Agents based on large language models (LLMs) struggle with brainless trial-and-error and generating hallucinatory actions due to a lack of global planning in long-horizon tasks. In this paper, we introduce a plan-and-execute framework and propose EAGLET, an efficient and effective planner training method to enhance the executor agent's planning abilities without human effort. Specifically, we train a plug-and-play global planner through a two-step process: we first synthesize high-quality plans from an advanced LLM using our proposed homologous consensus filtering strategy, and apply fine-tuning as a cold start. Moreover, we further improve the planner with a rule-based reinforcement learning stage using a novel executor capability gain reward, ensuring it can handle task instructions of varying difficulty. Experiments on three long-horizon agent tasks show that executor agents equipped with our planner outperform existing methods, achieving new state-of-the-art performance. Meanwhile, EAGLET reduces training costs by 8x compared to RL-based baselines, and it does not require manual effort or extra training data, offering an efficient and effective solution.", 'score': 3, 'issue_id': 6376, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '626ab2660f2e0d4b', 'authors': ['Shuzheng Si', 'Haozhe Zhao', 'Kangyang Luo', 'Gang Chen', 'Fanchao Qi', 'Minjia Zhang', 'Baobao Chang', 'Maosong Sun'], 'affiliations': ['DeepLang AI', 'Peking University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2510.05608.jpg', 'data': {'categories': ['#rl', '#agents', '#hallucinations', '#reasoning', '#optimization', '#training', '#long_context'], 'emoji': 'ğŸ¦…', 'ru': {'title': 'EAGLET: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº plan-and-execute Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ EAGLET Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ¼. EAGLET Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ¹ LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ³Ğ¾Ğ¼Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑĞ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ¾Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ñ€Ğ¾ÑÑ‚Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ¼, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² 8 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ RL-Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'EAGLET: Revolutionizing Planning for LLM Agents', 'desc': 'This paper presents a novel plan-and-execute framework called EAGLET, designed to improve the planning capabilities of large language model (LLM)-based agents. Traditional LLM agents often struggle with ineffective trial-and-error methods and generating incorrect actions due to insufficient global planning for long-horizon tasks. EAGLET enhances planning by synthesizing high-quality plans through a two-step process, which includes using a consensus filtering strategy and fine-tuning the planner. The results demonstrate that agents using EAGLET achieve superior performance on long-horizon tasks while significantly reducing training costs and eliminating the need for manual intervention.'}, 'zh': {'title': 'æå‡LLMä»£ç†è§„åˆ’èƒ½åŠ›çš„é«˜æ•ˆæ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è®¡åˆ’ä¸æ‰§è¡Œæ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†EAGLETï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„è§„åˆ’è€…è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†çš„è§„åˆ’èƒ½åŠ›ã€‚é€šè¿‡ä¸¤æ­¥è¿‡ç¨‹ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨å…ˆè¿›çš„LLMåˆæˆé«˜è´¨é‡çš„è®¡åˆ’ï¼Œå¹¶åº”ç”¨å†·å¯åŠ¨çš„å¾®è°ƒã€‚æ¥ç€ï¼Œæˆ‘ä»¬é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›ä¸€æ­¥æå‡è§„åˆ’è€…çš„èƒ½åŠ›ï¼Œç¡®ä¿å…¶èƒ½å¤Ÿå¤„ç†ä¸åŒéš¾åº¦çš„ä»»åŠ¡æŒ‡ä»¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé…å¤‡æˆ‘ä»¬è§„åˆ’è€…çš„æ‰§è¡Œä»£ç†åœ¨ä¸‰é¡¹é•¿æ—¶é—´ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”è®­ç»ƒæˆæœ¬é™ä½äº†8å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09592', 'title': 'Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in\n  Spoken Language Models', 'url': 'https://huggingface.co/papers/2510.09592', 'abstract': 'Mind-Paced Speaking (MPS) is a brain-inspired framework that enables real-time reasoning and fluent speech generation by dividing the process into a "Formulation Brain" for reasoning and an "Articulation Brain" for speech, achieving high accuracy with low latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought (CoT) reasoning due to the prohibitive latency of generating the entire thought process sequentially. Enabling SLMs to think while speaking, similar to humans, is attracting increasing attention. We present, for the first time, Mind-Paced Speaking (MPS), a brain-inspired framework that enables high-fidelity, real-time reasoning. Similar to how humans utilize distinct brain regions for thinking and responding, we propose a novel dual-brain approach, employing a "Formulation Brain" for high-level reasoning to pace and guide a separate "Articulation Brain" for fluent speech generation. This division of labor eliminates mode-switching, preserving the integrity of the reasoning process. Experiments show that MPS significantly outperforms existing think-while-speaking methods and achieves reasoning performance comparable to models that pre-compute the full CoT before speaking, while drastically reducing latency. Under a zero-latency configuration, the proposed method achieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and attains a score of 82.5 on the speech conversation task URO-Bench. Our work effectively bridges the gap between high-quality reasoning and real-time interaction.', 'score': 2, 'issue_id': 6375, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': 'd8062cbce503ccc1', 'authors': ['Donghang Wu', 'Haoyang Zhang', 'Jun Chen', 'Xiangyu', 'Zhang', 'Hexin Liu', 'Eng Siong Chng', 'Fei Tian', 'Xuerui Yang', 'Xiangyu Zhang', 'Daxin Jiang', 'Gang Yu'], 'affiliations': ['Nanyang Technological University', 'StepFun', 'University of New South Wales'], 'pdf_title_img': 'assets/pdf/title_img/2510.09592.jpg', 'data': {'categories': ['#audio', '#reasoning', '#training', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”ÑƒĞ¼Ğ°Ğ¹ Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾: Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ AI', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Mind-Paced Speaking (MPS) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ·Ğ³Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ AI-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑ‡ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Â«Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ²Â»: Â«ĞœĞ¾Ğ·Ğ³ Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÂ» Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ Chain-of-Thought, Ğ° Â«ĞœĞ¾Ğ·Ğ³ ĞÑ€Ñ‚Ğ¸ĞºÑƒĞ»ÑÑ†Ğ¸Ğ¸Â» Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ°Ğ²Ğ½ÑƒÑ Ñ€ĞµÑ‡ÑŒ. Ğ¢Ğ°ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. MPS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 92.8% Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ 82.5 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Think and Speak Like a Human with MPS!', 'desc': "Mind-Paced Speaking (MPS) is a novel framework designed to enhance real-time reasoning and speech generation by mimicking human brain functions. It separates the reasoning process into two components: a 'Formulation Brain' for high-level reasoning and an 'Articulation Brain' for fluent speech output. This dual-brain approach allows for simultaneous thinking and speaking, reducing latency and improving accuracy. Experimental results demonstrate that MPS outperforms existing methods, achieving high accuracy in reasoning tasks while maintaining real-time interaction capabilities."}, 'zh': {'title': 'å®æ—¶æ¨ç†ä¸æµç•…è¡¨è¾¾çš„å®Œç¾ç»“åˆ', 'desc': 'Mind-Paced Speaking (MPS) æ˜¯ä¸€ç§å—å¤§è„‘å¯å‘çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶æ¨ç†å’Œæµç•…çš„è¯­è¨€ç”Ÿæˆã€‚å®ƒå°†è¿‡ç¨‹åˆ†ä¸ºâ€œæ¨ç†å¤§è„‘â€å’Œâ€œè¡¨è¾¾å¤§è„‘â€ï¼Œå‰è€…è´Ÿè´£é«˜å±‚æ¬¡çš„æ¨ç†ï¼Œåè€…è´Ÿè´£æµç•…çš„è¯­è¨€è¡¨è¾¾ã€‚é€šè¿‡è¿™ç§åˆ†å·¥ï¼ŒMPS æ¶ˆé™¤äº†æ¨¡å¼åˆ‡æ¢ï¼Œä¿æŒäº†æ¨ç†è¿‡ç¨‹çš„å®Œæ•´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMPS åœ¨æ¨ç†æ€§èƒ½å’Œå®æ—¶äº¤äº’æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09507', 'title': 'PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs', 'url': 'https://huggingface.co/papers/2510.09507', 'abstract': "PhysToolBench evaluates MLLMs' comprehension of physical tools through a VQA dataset, revealing significant deficiencies in tool understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to use, understand, and create tools is a hallmark of human intelligence, enabling sophisticated interaction with the physical world. For any general-purpose intelligent agent to achieve true versatility, it must also master these fundamental skills. While modern Multimodal Large Language Models (MLLMs) leverage their extensive common knowledge for high-level planning in embodied AI and in downstream Vision-Language-Action (VLA) models, the extent of their true understanding of physical tools remains unquantified. To bridge this gap, we present PhysToolBench, the first benchmark dedicated to evaluating the comprehension of physical tools by MLLMs. Our benchmark is structured as a Visual Question Answering (VQA) dataset comprising over 1,000 image-text pairs. It assesses capabilities across three distinct difficulty levels: (1) Tool Recognition: Requiring the recognition of a tool's primary function. (2) Tool Understanding: Testing the ability to grasp the underlying principles of a tool's operation. (3) Tool Creation: Challenging the model to fashion a new tool from surrounding objects when conventional options are unavailable. Our comprehensive evaluation of 32 MLLMs-spanning proprietary, open-source, specialized embodied, and backbones in VLAs-reveals a significant deficiency in tool understanding. Furthermore, we provide an in-depth analysis and propose preliminary solutions. Code and dataset are publicly available.", 'score': 2, 'issue_id': 6376, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': '7373cdd0bfe24dc0', 'authors': ['Zixin Zhang', 'Kanghao Chen', 'Xingwang Lin', 'Lutao Jiang', 'Xu Zheng', 'Yuanhuiyi Lyu', 'Litao Guo', 'Yinchuan Li', 'Ying-Cong Chen'], 'affiliations': ['Beihang University', 'HKUST', 'HKUST(GZ)', 'Knowin'], 'pdf_title_img': 'assets/pdf/title_img/2510.09507.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#agi', '#multimodal', '#interpretability', '#open_source'], 'emoji': 'ğŸ”§', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM', 'desc': 'PhysToolBench â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ğ³Ğ¾, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 1000 Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Visual Question Answering Ñ Ñ‚Ñ€ĞµĞ¼Ñ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² ĞµĞ³Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ¿Ğ¾Ğ´Ñ€ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 32 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… MLLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ñƒ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ embodied AI ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñƒ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Bridging the Gap in Tool Comprehension for MLLMs', 'desc': "The paper introduces PhysToolBench, a benchmark designed to evaluate how well Multimodal Large Language Models (MLLMs) understand physical tools. It uses a Visual Question Answering (VQA) dataset with over 1,000 image-text pairs to assess MLLMs on three levels: recognizing tools, understanding their functions, and creating new tools. The evaluation of 32 different MLLMs shows that they struggle significantly with tool comprehension. The authors also offer insights and initial solutions to improve MLLMs' understanding of physical tools, with the dataset and code made publicly accessible."}, 'zh': {'title': 'è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„å·¥å…·ç†è§£èƒ½åŠ›', 'desc': 'PhysToolBenchæ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç†è§£ç‰©ç†å·¥å…·èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒé€šè¿‡ä¸€ä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡1000ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹ï¼Œæ¥è¯„ä¼°å·¥å…·è¯†åˆ«ã€ç†è§£å’Œåˆ›é€ çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„MLLMsåœ¨å·¥å…·ç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæ— æ³•æœ‰æ•ˆæŒæ¡å·¥å…·çš„åŸºæœ¬åŠŸèƒ½å’Œæ“ä½œåŸç†ã€‚æœ¬æ–‡è¿˜æä¾›äº†æ·±å…¥åˆ†æå’Œåˆæ­¥è§£å†³æ–¹æ¡ˆï¼Œå¹¶å…¬å¼€äº†ä»£ç å’Œæ•°æ®é›†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.07861', 'title': 'Understanding DeepResearch via Reports', 'url': 'https://huggingface.co/papers/2510.07861', 'abstract': 'A framework evaluates DeepResearch systems by assessing the quality, redundancy, and factuality of their research reports using an LLM-as-a-Judge methodology.  \t\t\t\t\tAI-generated summary \t\t\t\t DeepResearch agents represent a transformative AI paradigm, conducting expert-level research through sophisticated reasoning and multi-tool integration. However, evaluating these systems remains critically challenging due to open-ended research scenarios and existing benchmarks that focus on isolated capabilities rather than holistic performance. Unlike traditional LLM tasks, DeepResearch systems must synthesize diverse sources, generate insights, and present coherent findings, which are capabilities that resist simple verification. To address this gap, we introduce DeepResearch-ReportEval, a comprehensive framework designed to assess DeepResearch systems through their most representative outputs: research reports. Our approach systematically measures three dimensions: quality, redundancy, and factuality, using an innovative LLM-as-a-Judge methodology achieving strong expert concordance. We contribute a standardized benchmark of 100 curated queries spanning 12 real-world categories, enabling systematic capability comparison. Our evaluation of four leading commercial systems reveals distinct design philosophies and performance trade-offs, establishing foundational insights as DeepResearch evolves from information assistants toward intelligent research partners. Source code and data are available at: https://github.com/HKUDS/DeepResearch-Eval.', 'score': 2, 'issue_id': 6376, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': '1e9e0bbf3fa6c570', 'authors': ['Tianyu Fan', 'Xinyao Niu', 'Yuxiang Zheng', 'Fengji Zhang', 'Chengen Huang', 'Bei Chen', 'Junyang Lin', 'Chao Huang'], 'affiliations': ['Alibaba Group', 'City University of Hong Kong', 'Shanghai Jiao Tong University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.07861.jpg', 'data': {'categories': ['#benchmark', '#agents', '#reasoning', '#optimization', '#survey'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° AI-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹: ĞºĞ°Ğº Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DeepResearch-ReportEval â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-ÑĞ¸ÑÑ‚ĞµĞ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ LLM, ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ° â€” ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ LLM-as-a-Judge Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 100 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 12 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ñƒ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Evaluating AI Research: Quality, Redundancy, and Factuality in DeepResearch Systems', 'desc': 'This paper presents a framework called DeepResearch-ReportEval, which evaluates DeepResearch systems by analyzing the quality, redundancy, and factuality of their research reports. The framework utilizes an LLM-as-a-Judge methodology to provide a comprehensive assessment of these systems, which are designed to conduct expert-level research through advanced reasoning and tool integration. The authors highlight the challenges in evaluating these systems due to their need to synthesize diverse information and generate coherent insights, which traditional benchmarks do not adequately address. By introducing a standardized benchmark of 100 curated queries across various categories, the study facilitates systematic comparisons of different DeepResearch systems and their performance characteristics.'}, 'zh': {'title': 'è¯„ä¼°DeepResearchç³»ç»Ÿçš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°DeepResearchç³»ç»Ÿçš„ç ”ç©¶æŠ¥å‘Šè´¨é‡ã€å†—ä½™æ€§å’Œäº‹å®å‡†ç¡®æ€§ï¼Œé‡‡ç”¨äº†LLMä½œä¸ºè¯„åˆ¤è€…çš„æ–¹æ³•ã€‚DeepResearchä»£ç†é€šè¿‡å¤æ‚çš„æ¨ç†å’Œå¤šå·¥å…·é›†æˆï¼Œè¿›è¡Œä¸“å®¶çº§ç ”ç©¶ï¼Œä½†è¯„ä¼°è¿™äº›ç³»ç»Ÿçš„æŒ‘æˆ˜æ€§å¾ˆå¤§ã€‚æˆ‘ä»¬å¼•å…¥äº†DeepResearch-ReportEvalæ¡†æ¶ï¼Œç³»ç»Ÿåœ°æµ‹é‡ä¸‰ä¸ªç»´åº¦ï¼Œå¹¶æä¾›äº†100ä¸ªç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„æŸ¥è¯¢ä½œä¸ºæ ‡å‡†åŸºå‡†ã€‚é€šè¿‡å¯¹å››ä¸ªé¢†å…ˆå•†ä¸šç³»ç»Ÿçš„è¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸åŒçš„è®¾è®¡ç†å¿µå’Œæ€§èƒ½æƒè¡¡ï¼Œä¸ºDeepResearchä»ä¿¡æ¯åŠ©æ‰‹å‘æ™ºèƒ½ç ”ç©¶ä¼™ä¼´çš„æ¼”å˜å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09561', 'title': 'TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion\n  Control', 'url': 'https://huggingface.co/papers/2510.09561', 'abstract': "TC-LoRA enhances generative fidelity and adherence to spatial conditions by dynamically conditioning model weights through a hypernetwork, improving upon static activation-based methods in diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Current controllable diffusion models typically rely on fixed architectures that modify intermediate activations to inject guidance conditioned on a new modality. This approach uses a static conditioning strategy for a dynamic, multi-stage denoising process, limiting the model's ability to adapt its response as the generation evolves from coarse structure to fine detail. We introduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that enables dynamic, context-aware control by conditioning the model's weights directly. Our framework uses a hypernetwork to generate LoRA adapters on-the-fly, tailoring weight modifications for the frozen backbone at each diffusion step based on time and the user's condition. This mechanism enables the model to learn and execute an explicit, adaptive strategy for applying conditional guidance throughout the entire generation process. Through experiments on various data domains, we demonstrate that this dynamic, parametric control significantly enhances generative fidelity and adherence to spatial conditions compared to static, activation-based methods. TC-LoRA establishes an alternative approach in which the model's conditioning strategy is modified through a deeper functional adaptation of its weights, allowing control to align with the dynamic demands of the task and generative stage.", 'score': 1, 'issue_id': 6375, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': '88da4f7bc3556972', 'authors': ['Minkyoung Cho', 'Ruben Ohana', 'Christian Jacobsen', 'Adityan Jothi', 'Min-Hung Chen', 'Z. Morley Mao', 'Ethem Can'], 'affiliations': ['NVIDIA', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2510.09561.jpg', 'data': {'categories': ['#cv', '#training', '#diffusion', '#optimization', '#architecture'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'TC-LoRA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ½Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹. Ğ“Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ - Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Dynamic Control for Enhanced Generative Fidelity', 'desc': 'TC-LoRA introduces a novel method for enhancing generative models by dynamically adjusting model weights using a hypernetwork. Unlike traditional methods that rely on fixed architectures and static activations, TC-LoRA allows for context-aware control throughout the denoising process. This approach enables the model to adapt its responses as it transitions from coarse to fine details, improving the overall quality of generated outputs. Experiments show that TC-LoRA significantly outperforms static methods in terms of generative fidelity and adherence to spatial conditions.'}, 'zh': {'title': 'åŠ¨æ€æ§åˆ¶ï¼Œæå‡ç”Ÿæˆè´¨é‡', 'desc': 'TC-LoRAæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è¶…ç½‘ç»œåŠ¨æ€è°ƒæ•´æ¨¡å‹æƒé‡ï¼Œä»è€Œå¢å¼ºç”Ÿæˆçš„çœŸå®æ€§å’Œç©ºé—´æ¡ä»¶çš„éµå¾ªã€‚ä¸ä¼ ç»Ÿçš„é™æ€æ¿€æ´»æ–¹æ³•ä¸åŒï¼ŒTC-LoRAå…è®¸æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ ¹æ®æ—¶é—´å’Œç”¨æˆ·æ¡ä»¶å®æ—¶è°ƒæ•´æƒé‡ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”ŸæˆLoRAé€‚é…å™¨ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­çµæ´»åº”å¯¹å˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒTC-LoRAåœ¨å¤šä¸ªæ•°æ®é¢†åŸŸä¸­æ˜¾è‘—æé«˜äº†ç”Ÿæˆçš„è´¨é‡å’Œå¯¹ç©ºé—´æ¡ä»¶çš„éµå¾ªèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.08872', 'title': 'GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare', 'url': 'https://huggingface.co/papers/2510.08872', 'abstract': "Game-Theoretic Alignment (GTAlign) improves Large Language Model (LLM) performance by integrating game-theoretic decision making into reasoning and training, enhancing efficiency, answer quality, and mutual welfare.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable progress in reasoning, yet sometimes produce responses that are suboptimal for users in tasks such as writing, information seeking, or providing practical guidance. Conventional alignment practices typically assume that maximizing model reward also maximizes user welfare, but this assumption frequently fails in practice: models may over-clarify or generate overly verbose reasoning when users prefer concise answers. Such behaviors resemble the prisoner's dilemma, where individually rational choices lead to socially suboptimal outcomes. The fundamental challenge is the lack of a principled decision making mechanism that mutually benefits both the LLM and the user. We propose Game-Theoretic Alignment (GTAlign), an alignment framework that integrates game-theoretic decision making into both reasoning and training. During reasoning, the model explicitly treats user-LLM interaction as a strategic game: it constructs payoff matrices within its reasoning chain to estimate welfare for both itself and the user, and then selects actions that are mutually beneficial. During training, we introduce a mutual welfare reward that reinforces cooperative responses, aligning model behavior with socially efficient outcomes. In addition, we introduce an inference technique that leverages game-theoretic reasoning to dynamically adapt LLM's response when pricing policies of LLM service change. Extensive experiments demonstrate that GTAlign substantially improves reasoning efficiency, answer quality, and mutual welfare compared to baselines across diverse tasks. The code is available at https://github.com/ulab-uiuc/GTAlign .", 'score': 1, 'issue_id': 6377, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': '0a412d94a9cf2484', 'authors': ['Siqi Zhu', 'David Zhang', 'Pedro Cisneros-Velarde', 'Jiaxuan You'], 'affiliations': ['University of Illinois Urbana-Champaign', 'VMware Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.08872.jpg', 'data': {'categories': ['#rlhf', '#alignment', '#training', '#reasoning'], 'emoji': 'ğŸ®', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° LLM Ğ¸Ğ³Ñ€Ğ°ĞµÑ‚ Ğ² Ğ¸Ğ³Ñ€Ñ‹ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼: Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸ĞºĞ¾-Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ alignment', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GTAlign â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº alignment LLM, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ³Ñ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ñ…Ğ¾Ñ‚Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚ÑŒ â€” ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ñ, Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ğ»ĞµĞ¼Ğ¼Ñƒ Ğ·Ğ°ĞºĞ»ÑÑ‡Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ inference Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹ÑˆĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ĞºĞ°Ğº ÑĞµĞ±Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ²Ñ‹Ğ³Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞŸÑ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ reward Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑĞµÑ‚ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Aligning AI for Mutual Benefit with Game Theory', 'desc': "Game-Theoretic Alignment (GTAlign) enhances the performance of Large Language Models (LLMs) by applying game-theoretic principles to their reasoning and training processes. This approach addresses the common issue where LLMs produce responses that do not align with user preferences, such as being overly verbose. By treating the interaction between the user and the LLM as a strategic game, GTAlign allows the model to evaluate the benefits of its responses for both parties, leading to more efficient and relevant answers. The framework also introduces a mutual welfare reward during training, promoting cooperative behavior that aligns the model's outputs with user needs, resulting in improved overall performance."}, 'zh': {'title': 'åšå¼ˆè®ºå¯¹é½ï¼šæå‡LLMçš„äº’æƒ ç¦åˆ©ä¸æ¨ç†æ•ˆç‡', 'desc': 'æ¸¸æˆç†è®ºå¯¹é½ï¼ˆGTAlignï¼‰é€šè¿‡å°†åšå¼ˆè®ºå†³ç­–èå…¥æ¨ç†å’Œè®­ç»ƒä¸­ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„å¯¹é½æ–¹æ³•å‡è®¾æœ€å¤§åŒ–æ¨¡å‹å¥–åŠ±ä¹Ÿèƒ½æœ€å¤§åŒ–ç”¨æˆ·ç¦åˆ©ï¼Œä½†è¿™ç§å‡è®¾åœ¨å®é™…ä¸­å¸¸å¸¸å¤±æ•ˆã€‚GTAlignæ¡†æ¶é€šè¿‡å°†ç”¨æˆ·ä¸LLMçš„äº’åŠ¨è§†ä¸ºæˆ˜ç•¥æ¸¸æˆï¼Œæ„å»ºæ”¶ç›ŠçŸ©é˜µæ¥ä¼°è®¡åŒæ–¹çš„ç¦åˆ©ï¼Œä»è€Œé€‰æ‹©äº’åˆ©çš„è¡ŒåŠ¨ã€‚å®éªŒè¡¨æ˜ï¼ŒGTAlignåœ¨æ¨ç†æ•ˆç‡ã€ç­”æ¡ˆè´¨é‡å’Œäº’æƒ ç¦åˆ©æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.07896', 'title': 'ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual\n  Recall', 'url': 'https://huggingface.co/papers/2510.07896', 'abstract': 'ACE, a framework using neuron-level attribution, enhances multi-hop factual recall in LLMs by editing critical query-value pathways, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) require efficient knowledge editing (KE) to update factual information, yet existing methods exhibit significant performance decay in multi-hop factual recall. This failure is particularly acute when edits involve intermediate implicit subjects within reasoning chains. Through causal analysis, we reveal that this limitation stems from an oversight of how chained knowledge is dynamically represented and utilized at the neuron level. We discover that during multi hop reasoning, implicit subjects function as query neurons, which sequentially activate corresponding value neurons across transformer layers to accumulate information toward the final answer, a dynamic prior KE work has overlooked. Guided by this insight, we propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall, a framework that leverages neuron-level attribution to identify and edit these critical query-value (Q-V) pathways. ACE provides a mechanistically grounded solution for multi-hop KE, empirically outperforming state-of-the-art methods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals more fine-grained activation patterns in Qwen3 and demonstrates that the semantic interpretability of value neurons is orchestrated by query-driven accumulation. These findings establish a new pathway for advancing KE capabilities based on the principled understanding of internal reasoning mechanisms.', 'score': 1, 'issue_id': 6378, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': '0e8853762186b621', 'authors': ['Jiayu Yang', 'Yuxuan Fan', 'Songning Lai', 'Shengen Wu', 'Jiaqi Tang', 'Chun Kang', 'Zhijiang Guo', 'Yutao Yue'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'HKUST(GZ) Deep Interdisciplinary Intelligence Lab', 'Institute of Deep Perception Technology, JITRI'], 'pdf_title_img': 'assets/pdf/title_img/2510.07896.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#architecture', '#data', '#interpretability'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ÑƒÑ‚ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ACE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² LLM, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ ĞºĞ°Ğº query-Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹, Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ value-Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ² ÑĞ»Ğ¾ÑÑ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ACE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ query-value, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° 9.44% Ğ´Ğ»Ñ GPT-J Ğ¸ 37.46% Ğ´Ğ»Ñ Qwen3-8B Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Multi-hop Recall with Neuron-Level Knowledge Editing', 'desc': 'The paper introduces ACE, a novel framework that improves multi-hop factual recall in Large Language Models (LLMs) by focusing on neuron-level attribution. It identifies and edits critical query-value pathways that are essential for effective knowledge editing, addressing the limitations of existing methods that struggle with implicit subjects in reasoning chains. Through causal analysis, the authors demonstrate how these implicit subjects act as query neurons, activating value neurons to gather information for answers. ACE outperforms current state-of-the-art techniques, showcasing a significant enhancement in knowledge editing capabilities by leveraging a deeper understanding of the internal mechanisms of LLMs.'}, 'zh': {'title': 'ACEï¼šæå‡å¤šè·³äº‹å®å›å¿†çš„çŸ¥è¯†ç¼–è¾‘æ–°æ¡†æ¶', 'desc': 'ACEæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡ç¥ç»å…ƒçº§åˆ«çš„å½’å› æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè·³äº‹å®å›å¿†ä¸­çš„è¡¨ç°ã€‚ç°æœ‰çš„çŸ¥è¯†ç¼–è¾‘æ–¹æ³•åœ¨å¤„ç†å¤šè·³æ¨ç†æ—¶æ•ˆæœä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠä¸­é—´éšå«ä¸»é¢˜æ—¶ã€‚é€šè¿‡å› æœåˆ†æï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸€é™åˆ¶æºäºå¯¹é“¾å¼çŸ¥è¯†åœ¨ç¥ç»å…ƒå±‚é¢åŠ¨æ€è¡¨ç¤ºå’Œåˆ©ç”¨çš„å¿½è§†ã€‚ACEé€šè¿‡è¯†åˆ«å’Œç¼–è¾‘å…³é”®çš„æŸ¥è¯¢-å€¼è·¯å¾„ï¼Œæä¾›äº†ä¸€ç§åŸºäºæœºåˆ¶çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æé«˜äº†å¤šè·³çŸ¥è¯†ç¼–è¾‘çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.07319', 'title': 'Temporal Prompting Matters: Rethinking Referring Video Object\n  Segmentation', 'url': 'https://huggingface.co/papers/2510.07319', 'abstract': 'The Tenet framework decomposes the RVOS task into referring, video, and segmentation factors, using temporal prompts and prompt preference learning to adapt image-based foundation segmentation models for efficient RVOS.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring Video Object Segmentation (RVOS) aims to segment the object referred to by the query sentence in the video. Most existing methods require end-to-end training with dense mask annotations, which could be computation-consuming and less scalable. In this work, we rethink the RVOS problem and aim to investigate the key to this task. Based on existing foundation segmentation models, we decompose the RVOS task into referring, video, and segmentation factors, and propose a Temporal Prompt Generation and Selection (Tenet) framework to address the referring and video factors while leaving the segmentation problem to foundation models. To efficiently adapt image-based foundation segmentation models to referring video object segmentation, we leverage off-the-shelf object detectors and trackers to produce temporal prompts associated with the referring sentence. While high-quality temporal prompts could be produced, they can not be easily identified from confidence scores. To tackle this issue, we propose Prompt Preference Learning to evaluate the quality of the produced temporal prompts. By taking such prompts to instruct image-based foundation segmentation models, we would be able to produce high-quality masks for the referred object, enabling efficient model adaptation to referring video object segmentation. Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet framework.', 'score': 1, 'issue_id': 6375, 'pub_date': '2025-10-08', 'pub_date_card': {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'}, 'hash': '574ecd29563b6aa8', 'authors': ['Ci-Siang Lin', 'Min-Hung Chen', 'I-Jieh Liu', 'Chien-Yi Wang', 'Sifei Liu', 'Yu-Chiang Frank Wang'], 'affiliations': ['Graduate Institute of Communication Engineering, National Taiwan University, Taiwan', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2510.07319.jpg', 'data': {'categories': ['#cv', '#benchmark', '#video', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ”ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Tenet Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ (RVOS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ°Ğ¼Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ foundation Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ â€” Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ñ‚Ñ€ĞµĞºĞµÑ€Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Prompt Preference Learning. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ end-to-end Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑĞºĞ°Ñ….'}, 'en': {'title': 'Efficient RVOS through Temporal Prompting and Preference Learning', 'desc': 'The paper introduces the Tenet framework, which breaks down the Referring Video Object Segmentation (RVOS) task into three main components: referring, video, and segmentation factors. It utilizes temporal prompts generated from object detectors and trackers to enhance the performance of existing image-based segmentation models without requiring extensive training on dense mask annotations. To ensure the quality of these prompts, the authors implement Prompt Preference Learning, which assesses the effectiveness of the generated prompts. The results show that the Tenet framework significantly improves the efficiency and accuracy of RVOS tasks by leveraging pre-trained segmentation models.'}, 'zh': {'title': 'é«˜æ•ˆçš„å‚è€ƒè§†é¢‘ç‰©ä½“åˆ†å‰²æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†Tenetæ¡†æ¶ï¼Œå°†å‚è€ƒè§†é¢‘ç‰©ä½“åˆ†å‰²ï¼ˆRVOSï¼‰ä»»åŠ¡åˆ†è§£ä¸ºå‚è€ƒã€è§†é¢‘å’Œåˆ†å‰²ä¸‰ä¸ªå› ç´ ã€‚é€šè¿‡ä½¿ç”¨æ—¶é—´æç¤ºå’Œæç¤ºåå¥½å­¦ä¹ ï¼ŒTenetæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†åŸºäºå›¾åƒçš„åŸºç¡€åˆ†å‰²æ¨¡å‹é€‚åº”äºRVOSä»»åŠ¡ã€‚æˆ‘ä»¬åˆ©ç”¨ç°æˆçš„ç‰©ä½“æ£€æµ‹å™¨å’Œè·Ÿè¸ªå™¨ç”Ÿæˆä¸æŸ¥è¯¢å¥å­ç›¸å…³çš„æ—¶é—´æç¤ºï¼Œå¹¶é€šè¿‡æç¤ºåå¥½å­¦ä¹ è¯„ä¼°è¿™äº›æç¤ºçš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTenetæ¡†æ¶åœ¨RVOSåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åˆ†å‰²æ©ç ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09535', 'title': 'Mitigating Overthinking through Reasoning Shaping', 'url': 'https://huggingface.co/papers/2510.09535', 'abstract': 'Group Relative Segment Penalization (GRSP) improves token efficiency in large reasoning models without significantly reducing accuracy, especially for complex problems, by regularizing reasoning at the step level.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier Reward (RLVR) have shown great power in problem solving, yet they often cause overthinking: excessive, meandering reasoning that inflates computational cost. Prior designs of penalization in RLVR manage to reduce token consumption while often harming model performance, which arises from the oversimplicity of token-level supervision. In this paper, we argue that the granularity of supervision plays a crucial role in balancing efficiency and accuracy, and propose Group Relative Segment Penalization (GRSP), a step-level method to regularize reasoning. Since preliminary analyses show that reasoning segments are strongly correlated with token consumption and model performance, we design a length-aware weighting mechanism across segment clusters. Extensive experiments demonstrate that GRSP achieves superior token efficiency without heavily compromising accuracy, especially the advantages with harder problems. Moreover, GRSP stabilizes RL training and scales effectively across model sizes.', 'score': 0, 'issue_id': 6376, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': '160cfe01bcfb9a51', 'authors': ['Feifan Song', 'Shaohang Wei', 'Bofei Gao', 'Yejie Wang', 'Wen Luo', 'Wei Li', 'Linli Yao', 'Weimin Xiong', 'Liang Chen', 'Tianyu Liu', 'Houfeng Wang'], 'affiliations': ['Moonshot AI', 'State Key Laboratory of Multimedia Information Processing School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2510.09535.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‡Ğµ, Ğ½Ğ¾ Ğ½Ğµ Ğ³Ğ»ÑƒĞ¿ĞµĞµ', 'desc': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ñ‡Ğ°ÑÑ‚Ğ¾ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Â«overthinkingÂ» â€” Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Group Relative Segment Penalization (GRSP), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ², Ğ° Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. GRSP Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Efficiency in Reasoning Models with GRSP', 'desc': 'This paper introduces Group Relative Segment Penalization (GRSP), a method designed to enhance token efficiency in large reasoning models (LRMs) while maintaining accuracy. GRSP focuses on regularizing reasoning at the step level, addressing the issue of overthinking that can lead to increased computational costs. By implementing a length-aware weighting mechanism for reasoning segments, GRSP effectively reduces token consumption without sacrificing performance. The results show that GRSP not only improves efficiency, particularly for complex problems, but also stabilizes reinforcement learning training across various model sizes.'}, 'zh': {'title': 'ç¾¤ä½“ç›¸å¯¹æ®µæƒ©ç½šï¼šæå‡æ¨ç†æ¨¡å‹çš„æ•ˆç‡ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºç¾¤ä½“ç›¸å¯¹æ®µæƒ©ç½šï¼ˆGRSPï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„ä»¤ç‰Œæ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚GRSPé€šè¿‡åœ¨æ¨ç†æ­¥éª¤çº§åˆ«è¿›è¡Œæ­£åˆ™åŒ–ï¼Œè§£å†³äº†ä»¥å¾€æ–¹æ³•åœ¨å‡å°‘ä»¤ç‰Œæ¶ˆè€—æ—¶å¸¸å¸¸å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨ç†æ®µä¸ä»¤ç‰Œæ¶ˆè€—å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ï¼Œå› æ­¤æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæ®µé›†ç¾¤çš„é•¿åº¦æ„ŸçŸ¥åŠ æƒæœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGRSPåœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜ä»¤ç‰Œæ•ˆç‡ï¼Œè€Œä¸ä¼šä¸¥é‡å½±å“æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.08994', 'title': 'Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive\n  Text-to-image Generation', 'url': 'https://huggingface.co/papers/2510.08994', 'abstract': 'Speculative Jacobi-Denoising Decoding accelerates autoregressive text-to-image generation by enabling parallel token prediction and reducing model forward passes.  \t\t\t\t\tAI-generated summary \t\t\t\t As a new paradigm of visual content generation, autoregressive text-to-image models suffer from slow inference due to their sequential token-by-token decoding process, often requiring thousands of model forward passes to generate a single image. To address this inefficiency, we propose Speculative Jacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising process into Jacobi iterations to enable parallel token generation in autoregressive models. Our method introduces a next-clean-token prediction paradigm that enables the pre-trained autoregressive models to accept noise-perturbed token embeddings and predict the next clean tokens through low-cost fine-tuning. This denoising paradigm guides the model towards more stable Jacobi trajectories. During inference, our method initializes token sequences with Gaussian noise and performs iterative next-clean-token-prediction in the embedding space. We employ a probabilistic criterion to verify and accept multiple tokens in parallel, and refine the unaccepted tokens for the next iteration with the denoising trajectory. Experiments show that our method can accelerate generation by reducing model forward passes while maintaining the visual quality of generated images.', 'score': 0, 'issue_id': 6377, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': '54fbfde4c298e267', 'authors': ['Yao Teng', 'Fuyun Wang', 'Xian Liu', 'Zhekai Chen', 'Han Shi', 'Yu Wang', 'Zhenguo Li', 'Weiyang Liu', 'Difan Zou', 'Xihui Liu'], 'affiliations': ['CUHK', 'Huawei Noahs Ark Lab', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08994.jpg', 'data': {'categories': ['#inference', '#optimization', '#cv', '#video'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾, Ñ‚Ğ°Ğº ĞºĞ°Ğº ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SJD2 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸ÑÑ… Ğ¯ĞºĞ¾Ğ±Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾.'}, 'en': {'title': 'Accelerating Image Generation with Parallel Token Prediction', 'desc': 'This paper introduces Speculative Jacobi-Denoising Decoding (SJD2), a novel approach to enhance the efficiency of autoregressive text-to-image generation. Traditional models generate images sequentially, which is slow and requires many forward passes through the model. SJD2 allows for parallel token prediction by integrating a denoising process into Jacobi iterations, enabling the model to predict clean tokens from noise-perturbed embeddings. The results demonstrate that this method significantly speeds up image generation while preserving high visual quality.'}, 'zh': {'title': 'åŠ é€Ÿå›¾åƒç”Ÿæˆçš„æŠ•æœºæ€§å»å™ªè§£ç æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºæŠ•æœºæ€§é›…å¯æ¯”å»å™ªè§£ç ï¼ˆSJD2ï¼‰ï¼Œæ—¨åœ¨åŠ é€Ÿè‡ªå›å½’æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶éœ€è¦é€ä¸ªä»¤ç‰Œè§£ç ï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦ç¼“æ…¢ã€‚SJD2é€šè¿‡å°†å»å™ªè¿‡ç¨‹ä¸é›…å¯æ¯”è¿­ä»£ç»“åˆï¼Œå…è®¸å¹¶è¡Œç”Ÿæˆä»¤ç‰Œï¼Œä»è€Œå‡å°‘æ¨¡å‹çš„å‰å‘ä¼ é€’æ¬¡æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç”Ÿæˆå›¾åƒè§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01119', 'title': 'Instant4D: 4D Gaussian Splatting in Minutes', 'url': 'https://huggingface.co/papers/2510.01119', 'abstract': 'Instant4D uses deep visual SLAM and a 4D Gaussian representation to efficiently reconstruct scenes from uncalibrated video sequences in minutes.  \t\t\t\t\tAI-generated summary \t\t\t\t Dynamic view synthesis has seen significant advances, yet reconstructing scenes from uncalibrated, casual video remains challenging due to slow optimization and complex parameter estimation. In this work, we present Instant4D, a monocular reconstruction system that leverages native 4D representation to efficiently process casual video sequences within minutes, without calibrated cameras or depth sensors. Our method begins with geometric recovery through deep visual SLAM, followed by grid pruning to optimize scene representation. Our design significantly reduces redundancy while maintaining geometric integrity, cutting model size to under 10% of its original footprint. To handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian representation, achieving a 30x speed-up and reducing training time to within two minutes, while maintaining competitive performance across several benchmarks. Our method reconstruct a single video within 10 minutes on the Dycheck dataset or for a typical 200-frame video. We further apply our model to in-the-wild videos, showcasing its generalizability. Our project website is published at https://instant4d.github.io/.', 'score': 0, 'issue_id': 6375, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '9c375f4bf8782c80', 'authors': ['Zhanpeng Luo', 'Haoxi Ran', 'Li Lu'], 'affiliations': ['Carnegie Mellon University', 'Sichuan University', 'University of Pittsburgh'], 'pdf_title_img': 'assets/pdf/title_img/2510.01119.jpg', 'data': {'categories': ['#games', '#cv', '#video', '#dataset', '#inference', '#benchmark', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ 4D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ° Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹', 'desc': 'Instant4D â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ½ĞµĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ·Ğ° ÑÑ‡Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ deep visual SLAM Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ 4D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ 10% Ğ¾Ñ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑƒÑĞºĞ¾Ñ€Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² 30 Ñ€Ğ°Ğ· â€” Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ¾Ğ½Ğ¾ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ Ğ´Ğ²Ğµ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ğ¾Ğµ 200-ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ° 10 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ»Ğ¸ĞºĞ°Ñ… Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°.'}, 'en': {'title': 'Revolutionizing Scene Reconstruction in Minutes with Instant4D', 'desc': 'Instant4D is a novel system that utilizes deep visual SLAM and a 4D Gaussian representation to reconstruct scenes from uncalibrated video sequences quickly. It addresses the challenges of slow optimization and complex parameter estimation by processing casual videos in minutes without the need for calibrated cameras or depth sensors. The method employs geometric recovery and grid pruning to optimize the scene representation, significantly reducing redundancy while preserving geometric integrity. With a 30x speed-up in processing time, Instant4D can reconstruct a typical 200-frame video in just 10 minutes, demonstrating its efficiency and effectiveness across various benchmarks.'}, 'zh': {'title': 'å¿«é€Ÿé‡å»ºï¼Œç¬é—´å‘ˆç°', 'desc': 'Instant4D æ˜¯ä¸€ä¸ªå•ç›®é‡å»ºç³»ç»Ÿï¼Œåˆ©ç”¨æ·±åº¦è§†è§‰ SLAM å’Œ 4D é«˜æ–¯è¡¨ç¤ºæ³•ï¼Œèƒ½å¤Ÿåœ¨å‡ åˆ†é’Ÿå†…é«˜æ•ˆåœ°ä»æœªæ ¡å‡†çš„è§†é¢‘åºåˆ—ä¸­é‡å»ºåœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡å‡ ä½•æ¢å¤å’Œç½‘æ ¼ä¿®å‰ªæ¥ä¼˜åŒ–åœºæ™¯è¡¨ç¤ºï¼Œæ˜¾è‘—å‡å°‘å†—ä½™ï¼ŒåŒæ—¶ä¿æŒå‡ ä½•å®Œæ•´æ€§ã€‚Instant4D çš„è®¾è®¡ä½¿å¾—æ¨¡å‹å¤§å°å‡å°‘åˆ°åŸå§‹çš„ 10% ä»¥ä¸‹ï¼Œå¹¶ä¸”åœ¨å¤„ç†æ—¶é—´åŠ¨æ€æ–¹é¢å®ç°äº† 30 å€çš„åŠ é€Ÿã€‚è¯¥ç³»ç»Ÿåœ¨ Dycheck æ•°æ®é›†ä¸Šèƒ½å¤Ÿåœ¨ 10 åˆ†é’Ÿå†…é‡å»ºå•ä¸ªè§†é¢‘ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…è§†é¢‘ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (5)', '#agi (1)', '#alignment (3)', '#architecture (3)', '#audio (2)', '#benchmark (16)', '#cv (9)', '#data (5)', '#dataset (9)', '#diffusion (3)', '#ethics (1)', '#games (3)', '#graphs', '#hallucinations (2)', '#healthcare', '#inference (4)', '#interpretability (2)', '#leakage', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (9)', '#open_source (5)', '#optimization (20)', '#plp', '#rag', '#reasoning (15)', '#rl (6)', '#rlhf (2)', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey (3)', '#synthetic (1)', '#training (15)', '#transfer_learning', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-10-13 06:19',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-13 06:19')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-13 06:19')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    