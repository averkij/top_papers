{
    "date": {
        "ru": "24 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 24",
        "zh": "1æœˆ24æ—¥"
    },
    "time_utc": "2025-01-24 04:12",
    "weekday": 4,
    "issue_id": 1842,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.13629",
            "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models",
            "url": "https://huggingface.co/papers/2501.13629",
            "abstract": "We introduce Sigma, an efficient large language model specialized for the system domain, empowered by a novel architecture including DiffQKV attention, and pre-trained on our meticulously collected system domain data. DiffQKV attention significantly enhances the inference efficiency of Sigma by optimizing the Query (Q), Key (K), and Value (V) components in the attention mechanism differentially, based on their varying impacts on the model performance and efficiency indicators. Specifically, we (1) conduct extensive experiments that demonstrate the model's varying sensitivity to the compression of K and V components, leading to the development of differentially compressed KV, and (2) propose augmented Q to expand the Q head dimension, which enhances the model's representation capacity with minimal impacts on the inference speed. Rigorous theoretical and empirical analyses reveal that DiffQKV attention significantly enhances efficiency, achieving up to a 33.36% improvement in inference speed over the conventional grouped-query attention (GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various sources, including 19.5B system domain data that we carefully collect and 1T tokens of synthesized and rewritten data. In general domains, Sigma achieves comparable performance to other state-of-arts models. In the system domain, we introduce the first comprehensive benchmark AIMicius, where Sigma demonstrates remarkable performance across all tasks, significantly outperforming GPT-4 with an absolute improvement up to 52.5%.",
            "score": 6,
            "issue_id": 1842,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "d036f75a81877ded",
            "authors": [
                "Zhenghao Lin",
                "Zihao Tang",
                "Xiao Liu",
                "Yeyun Gong",
                "Yi Cheng",
                "Qi Chen",
                "Hang Li",
                "Ying Xin",
                "Ziyue Yang",
                "Kailai Yang",
                "Yu Yan",
                "Xiao Liang",
                "Shuai Lu",
                "Yiming Huang",
                "Zheheng Luo",
                "Lei Qu",
                "Xuan Feng",
                "Yaoxiang Wang",
                "Yuqing Xia",
                "Feiyang Chen",
                "Yuting Jiang",
                "Yasen Hu",
                "Hao Ni",
                "Binyang Li",
                "Guoshuai Zhao",
                "Jui-Hao Chiang",
                "Zhongxin Guo",
                "Chen Lin",
                "Kun Kuang",
                "Wenjie Li",
                "Yelong Shen",
                "Jian Jiao",
                "Peng Cheng",
                "Mao Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2501.13629.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#dataset",
                    "#benchmark",
                    "#long_context",
                    "#training",
                    "#synthetic",
                    "#data",
                    "#inference"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "Sigma: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¯Ğœ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Sigma - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ DiffQKV-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Q, K Ğ¸ V Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Sigma Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 6T Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², Ğ° Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4 Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ AIMicius."
                },
                "en": {
                    "title": "Sigma: Revolutionizing System Domain Language Models with DiffQKV Attention",
                    "desc": "The paper presents Sigma, a specialized large language model designed for the system domain, utilizing a new architecture called DiffQKV attention. This innovative attention mechanism optimizes the Query, Key, and Value components to improve inference efficiency, particularly in long-context scenarios. Through extensive experiments, the authors show that Sigma achieves significant speed improvements, outperforming traditional models like GPT-4 in various tasks. The model is pre-trained on a vast dataset, including 19.5 billion tokens from the system domain, establishing a new benchmark for performance in this area."
                },
                "zh": {
                    "title": "Sigmaï¼šç³»ç»Ÿé¢†åŸŸçš„é«˜æ•ˆè¯­è¨€æ¨¡å‹",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†Sigmaï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹ç³»ç»Ÿé¢†åŸŸã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„æ¶æ„ï¼ŒåŒ…æ‹¬DiffQKVæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶åœ¨æˆ‘ä»¬ç²¾å¿ƒæ”¶é›†çš„ç³»ç»Ÿé¢†åŸŸæ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚DiffQKVæ³¨æ„åŠ›é€šè¿‡ä¼˜åŒ–æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰å’Œå€¼ï¼ˆVï¼‰ç»„ä»¶ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSigmaåœ¨ç³»ç»Ÿé¢†åŸŸçš„è¡¨ç°ä¼˜äºGPT-4ï¼Œç»å¯¹æå‡å¹…åº¦å¯è¾¾52.5%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.13926",
            "title": "Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step",
            "url": "https://huggingface.co/papers/2501.13926",
            "abstract": "Chain-of-Thought (CoT) reasoning has been extensively explored in large models to tackle complex understanding tasks. However, it still remains an open question whether such strategies can be applied to verifying and reinforcing image generation scenarios. In this paper, we provide the first comprehensive investigation of the potential of CoT reasoning to enhance autoregressive image generation. We focus on three techniques: scaling test-time computation for verification, aligning model preferences with Direct Preference Optimization (DPO), and integrating these techniques for complementary effects. Our results demonstrate that these approaches can be effectively adapted and combined to significantly improve image generation performance. Furthermore, given the pivotal role of reward models in our findings, we propose the Potential Assessment Reward Model (PARM) and PARM++, specialized for autoregressive image generation. PARM adaptively assesses each generation step through a potential assessment approach, merging the strengths of existing reward models, and PARM++ further introduces a reflection mechanism to self-correct the generated unsatisfactory image. Using our investigated reasoning strategies, we enhance a baseline model, Show-o, to achieve superior results, with a significant +24% improvement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We hope our study provides unique insights and paves a new path for integrating CoT reasoning with autoregressive image generation. Code and models are released at https://github.com/ZiyuGuo99/Image-Generation-CoT",
            "score": 2,
            "issue_id": 1841,
            "pub_date": "2025-01-23",
            "pub_date_card": {
                "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 23",
                "zh": "1æœˆ23æ—¥"
            },
            "hash": "61611cbe661736ff",
            "authors": [
                "Ziyu Guo",
                "Renrui Zhang",
                "Chengzhuo Tong",
                "Zhizheng Zhao",
                "Peng Gao",
                "Hongsheng Li",
                "Pheng-Ann Heng"
            ],
            "affiliations": [
                "CUHK",
                "MMLab",
                "MiuLar Lab",
                "Peking University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.13926.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#games",
                    "#dataset",
                    "#cv",
                    "#reasoning",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (Chain-of-Thought) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ PARM Ğ¸ PARM++, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Show-o Ğ½Ğ° 24% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ¼ GenEval."
                },
                "en": {
                    "title": "Enhancing Image Generation with Chain-of-Thought Reasoning",
                    "desc": "This paper explores the use of Chain-of-Thought (CoT) reasoning to improve autoregressive image generation models. It investigates three main techniques: enhancing verification through increased computation, aligning model preferences using Direct Preference Optimization (DPO), and combining these methods for better outcomes. The authors introduce the Potential Assessment Reward Model (PARM) and its enhanced version PARM++, which help assess and correct image generation steps. The results show a significant performance boost, achieving a 24% improvement on the GenEval benchmark compared to previous models."
                },
                "zh": {
                    "title": "é“¾å¼æ€ç»´æå‡å›¾åƒç”Ÿæˆæ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†åœ¨è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§æŠ€æœ¯ï¼šæµ‹è¯•æ—¶è®¡ç®—çš„æ‰©å±•ã€ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹é½æ¨¡å‹åå¥½ï¼Œä»¥åŠè¿™äº›æŠ€æœ¯çš„æ•´åˆã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥æœ‰æ•ˆç»“åˆï¼Œæ˜¾è‘—æå‡å›¾åƒç”Ÿæˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåŠ›è¯„ä¼°å¥–åŠ±æ¨¡å‹ï¼ˆPARMï¼‰å’ŒPARM++ï¼Œä¸“é—¨ç”¨äºè‡ªå›å½’å›¾åƒç”Ÿæˆï¼Œè¿›ä¸€æ­¥æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.10799",
            "title": "Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback",
            "url": "https://huggingface.co/papers/2501.10799",
            "abstract": "Large language models (LLMs) have recently demonstrated remarkable success in mathematical reasoning. Despite progress in methods like chain-of-thought prompting and self-consistency sampling, these advances often focus on final correctness without ensuring that the underlying reasoning process is coherent and reliable. This paper introduces Step-KTO, a training framework that combines process-level and outcome-level binary feedback to guide LLMs toward more trustworthy reasoning trajectories. By providing binary evaluations for both the intermediate reasoning steps and the final answer, Step-KTO encourages the model to adhere to logical progressions rather than relying on superficial shortcuts. Our experiments on challenging mathematical benchmarks show that Step-KTO significantly improves both final answer accuracy and the quality of intermediate reasoning steps. For example, on the MATH-500 dataset, Step-KTO achieves a notable improvement in Pass@1 accuracy over strong baselines. These results highlight the promise of integrating stepwise process feedback into LLM training, paving the way toward more interpretable and dependable reasoning capabilities.",
            "score": 1,
            "issue_id": 1842,
            "pub_date": "2025-01-18",
            "pub_date_card": {
                "ru": "18 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 18",
                "zh": "1æœˆ18æ—¥"
            },
            "hash": "d43b005a69156930",
            "authors": [
                "Yen-Ting Lin",
                "Di Jin",
                "Tengyu Xu",
                "Tianhao Wu",
                "Sainbayar Sukhbaatar",
                "Chen Zhu",
                "Yun He",
                "Yun-Nung Chen",
                "Jason Weston",
                "Yuandong Tian",
                "Arash Rahnama",
                "Sinong Wang",
                "Hao Ma",
                "Han Fang"
            ],
            "affiliations": [
                "Meta FAIR",
                "Meta GenAI",
                "National Taiwan University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.10799.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#training",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¨Ğ°Ğ³ Ğ·Ğ° ÑˆĞ°Ğ³Ğ¾Ğ¼ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Step-KTO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ñ…Ğ¾Ğ´Ñƒ Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ğ° Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Trustworthy Reasoning in LLMs with Step-KTO",
                    "desc": "This paper presents Step-KTO, a new training framework for large language models (LLMs) that enhances their mathematical reasoning abilities. Unlike previous methods that focus solely on the final answer, Step-KTO provides feedback on both the reasoning process and the outcome, promoting logical coherence. By evaluating intermediate reasoning steps alongside the final result, the framework helps LLMs avoid shortcuts and develop more reliable reasoning paths. Experiments show that Step-KTO significantly boosts accuracy and improves the quality of reasoning in challenging mathematical tasks, indicating its potential for creating more interpretable AI systems."
                },
                "zh": {
                    "title": "æå‡æ¨ç†å¯ä¿¡åº¦çš„Step-KTOæ¡†æ¶",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚å°½ç®¡é“¾å¼æ€ç»´æç¤ºå’Œè‡ªä¸€è‡´æ€§é‡‡æ ·ç­‰æ–¹æ³•æœ‰æ‰€è¿›å±•ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€åªå…³æ³¨æœ€ç»ˆç»“æœçš„æ­£ç¡®æ€§ï¼Œè€Œæœªèƒ½ç¡®ä¿æ¨ç†è¿‡ç¨‹çš„è¿è´¯æ€§å’Œå¯é æ€§ã€‚æœ¬æ–‡æå‡ºäº†Step-KTOï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè¿‡ç¨‹çº§å’Œç»“æœçº§äºŒå…ƒåé¦ˆçš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨å¼•å¯¼LLMsæœç€æ›´å¯ä¿¡çš„æ¨ç†è½¨è¿¹å‘å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStep-KTOæ˜¾è‘—æé«˜äº†æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œä¸­é—´æ¨ç†æ­¥éª¤çš„è´¨é‡ï¼Œå±•ç¤ºäº†é€æ­¥è¿‡ç¨‹åé¦ˆåœ¨LLMè®­ç»ƒä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-23.html",
    "link_next": "2025-01-27.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "23.01",
        "en": "01/23",
        "zh": "1æœˆ23æ—¥"
    },
    "short_date_next": {
        "ru": "27.01",
        "en": "01/27",
        "zh": "1æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§æ¨ç†æ¨¡å‹ï¼šDeepSeek-R1-Zero å’Œ DeepSeek-R1ã€‚DeepSeek-R1-Zero é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå±•ç¤ºäº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨å¯è¯»æ€§å·®å’Œè¯­è¨€æ··åˆçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº† DeepSeek-R1ï¼Œå®ƒåœ¨å¼ºåŒ–å­¦ä¹ ä¹‹å‰è¿›è¡Œå¤šé˜¶æ®µè®­ç»ƒå’Œå†·å¯åŠ¨æ•°æ®å¤„ç†ã€‚DeepSeek-R1 åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ OpenAI-o1-1217 ç›¸å½“ã€‚æˆ‘ä»¬å¼€æºäº†è¿™äº›æ¨¡å‹å’Œå…­ä¸ªåŸºäº Qwen å’Œ Llama çš„å‹ç¼©æ¨¡å‹ã€‚",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "pinyin": "æˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§æ¨ç†æ¨¡å‹ï¼šDeepSeek-R1-Zero å’Œ DeepSeek-R1ã€‚DeepSeek-R1-Zero é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå±•ç¤ºäº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨å¯è¯»æ€§å·®å’Œè¯­è¨€æ··åˆçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº† DeepSeek-R1ï¼Œå®ƒåœ¨å¼ºåŒ–å­¦ä¹ ä¹‹å‰è¿›è¡Œå¤šé˜¶æ®µè®­ç»ƒå’Œå†·å¯åŠ¨æ•°æ®å¤„ç†ã€‚DeepSeek-R1 åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ OpenAI-o1-1217 ç›¸å½“ã€‚æˆ‘ä»¬å¼€æºäº†è¿™äº›æ¨¡å‹å’Œå…­ä¸ªåŸºäº Qwen å’Œ Llama çš„å‹ç¼©æ¨¡å‹ã€‚\n\nWÇ’men jiÃ¨shÃ o le liÇng zhÇ’ng tuÄ«lÇ mÃ³xÃ­ng: DeepSeek-R1-Zero hÃ© DeepSeek-R1. DeepSeek-R1-Zero tÅngguÃ² dÃ  guÄ«mÃ³ qiÃ¡ngzhÃ¹ xuÃ©xÃ­ xÃ¹nliÃ n, zhÇnshÃ¬ le chÅ«sÃ¨ de tuÄ«lÇ nÃ©nglÃ¬, dÃ n cÃºnzÃ i kÄ›dÃºxÃ¬ng chÃ  hÃ© yÇ”yÃ¡n hÃ¹nhÃ© de wÃ¨ntÃ­. WÃ¨ile jiÄ›juÃ© zhÃ¨xiÄ“ wÃ¨ntÃ­, wÇ’men kÄifÄ le DeepSeek-R1, tÄ zÃ i qiÃ¡ngzhÃ¹ xuÃ©xÃ­ zhÄ«qiÃ¡n jÃ¬nxÃ­ng duÅ jiÄ“duÃ n xÃ¹nliÃ n hÃ© lÄ›ng qÇdÃ²ng shÃ¹jÃ¹ chÇ”lÇ. DeepSeek-R1 zÃ i tuÄ«lÇ rÃ¨nwÃ¹ shÃ ng de biÇoxiÃ n yÇ” OpenAI-o1-1217 xiÄngdÄng. WÇ’men kÄiyuÃ¡n le zhÃ¨xiÄ“ mÃ³xÃ­ng hÃ© liÃ¹ gÃ¨ jÄ«yÃº Qwen hÃ© Llama de yÄsuÅ mÃ³xÃ­ng.",
        "vocab": "[{'word': 'ä»‹ç»', 'pinyin': 'jiÃ¨shÃ o', 'trans': 'introduce'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ«lÇ', 'trans': 'reasoning'}, {'word': 'æ¨¡å‹', 'pinyin': 'mÃ³xÃ­ng', 'trans': 'model'}, {'word': 'å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'qiÃ¡ngâ€‹huÃ â€‹xuÃ©â€‹xÃ­', 'trans': 'reinforcement learning'}, {'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹nliÃ n', 'trans': 'training'}, {'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'demonstrate'}, {'word': 'å¯è¯»æ€§', 'pinyin': 'kÄ›â€‹dÃºâ€‹xÃ¬ng', 'trans': 'readability'}, {'word': 'è¯­è¨€æ··åˆ', 'pinyin': 'yÇ”â€‹yÃ¡nâ€‹hÃ¹nâ€‹hÃ©', 'trans': 'language mixing'}, {'word': 'å¼€å‘', 'pinyin': 'kÄifÄ', 'trans': 'develop'}, {'word': 'å¤šé˜¶æ®µ', 'pinyin': 'duÅâ€‹jiÄ“â€‹duÃ n', 'trans': 'multi-stage'}, {'word': 'å†·å¯åŠ¨', 'pinyin': 'lÄ›ngâ€‹qÇâ€‹dÃ²ng', 'trans': 'cold start'}, {'word': 'æ•°æ®å¤„ç†', 'pinyin': 'shÃ¹â€‹jÃ¹â€‹chÇ”â€‹lÇ', 'trans': 'data processing'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇoâ€‹xiÃ n', 'trans': 'performance'}, {'word': 'ç›¸å½“', 'pinyin': 'xiÄngâ€‹dÄng', 'trans': 'equivalent'}, {'word': 'å¼€æº', 'pinyin': 'kÄiâ€‹yuÃ¡n', 'trans': 'open source'}, {'word': 'åŸºäº', 'pinyin': 'jÄ«â€‹yÃº', 'trans': 'based on'}, {'word': 'å‹ç¼©', 'pinyin': 'yÄâ€‹suÅ', 'trans': 'compression'}]",
        "trans": "We introduced two reasoning models: DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, trained through large-scale reinforcement learning, demonstrated excellent reasoning capabilities but suffered from poor readability and language mixing issues. To address these problems, we developed DeepSeek-R1, which undergoes multi-stage training and cold start data processing before reinforcement learning. DeepSeek-R1 performs comparably to OpenAI-o1-1217 in reasoning tasks. We have open-sourced these models along with six compressed models based on Qwen and Llama.",
        "update_ts": "2025-01-23 09:10"
    }
}