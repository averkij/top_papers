
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. November 4.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">4 ноября</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-11-01.html">⬅️ <span id="prev-date">01.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-11-05.html">➡️ <span id="next-date">05.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-11.html">📈 <span id='top-month-label'>Топ за месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'};
        let feedDateNext = {'ru': '05.11', 'en': '11/05', 'zh': '11月5日'};
        let feedDatePrev = {'ru': '01.11', 'en': '11/01', 'zh': '11月1日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Топ за месяц', 'en': 'Top by Month', 'zh': '月度热门论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.23218', 'title': 'OS-ATLAS: A Foundation Action Model for Generalist GUI Agents', 'url': 'https://huggingface.co/papers/2410.23218', 'abstract': 'Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas - a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs.', 'score': 41, 'issue_id': 410, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'd7a3f0fd08f934d5', 'data': {'categories': ['#dataset', '#data', '#agents', '#training', '#benchmark'], 'emoji': '🖥️', 'ru': {'title': 'OS-Atlas: Открытая модель для универсального взаимодействия с GUI', 'desc': 'Исследователи разработали OS-Atlas - основополагающую модель для взаимодействия с графическим интерфейсом пользователя (GUI). Модель использует инновационный подход к данным и моделированию, что позволяет ей эффективно работать с GUI и решать задачи вне распределения (OOD). Авторы создали открытый набор инструментов для синтеза данных о GUI на различных платформах и выпустили крупнейший открытый кросс-платформенный корпус, содержащий более 13 миллионов элементов GUI. OS-Atlas демонстрирует значительное улучшение производительности по сравнению с предыдущими моделями на шести тестовых наборах, охватывающих мобильные, настольные и веб-платформы.'}, 'en': {'title': 'Empowering Open-Source GUI Agents with OS-Atlas', 'desc': "This paper introduces OS-Atlas, an open-source foundational model designed for GUI grounding and Out-Of-Distribution (OOD) tasks. It addresses the performance gap between commercial Vision-Language Models (VLMs) and open-source alternatives by providing a comprehensive toolkit for synthesizing GUI grounding data across various platforms. The authors present the largest open-source cross-platform GUI grounding dataset, featuring over 13 million GUI elements, which enhances the model's ability to understand and generalize from GUI screenshots. Extensive evaluations show that OS-Atlas outperforms previous models, offering insights for further advancements in open-source VLM capabilities."}, 'zh': {'title': '开源GUI模型OS-Atlas：提升界面理解能力的创新之路', 'desc': '本论文介绍了OS-Atlas，一个开源的GUI动作模型，专注于GUI定位和超出分布（OOD）任务。我们开发了一个工具包，可以在多个平台上合成GUI定位数据，包括Windows、Linux、MacOS、Android和网页。OS-Atlas利用超过1300万个GUI元素的数据集，结合创新的模型训练方法，显著提高了对GUI截图的理解能力。通过在六个基准测试中进行广泛评估，OS-Atlas在移动、桌面和网页平台上表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2411.00027', 'title': 'Personalization of Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2411.00027', 'abstract': 'Personalization of Large Language Models (LLMs) has recently become increasingly important with a wide range of applications. Despite the importance and recent progress, most existing works on personalized LLMs have focused either entirely on (a) personalized text generation or (b) leveraging LLMs for personalization-related downstream applications, such as recommendation systems. In this work, we bridge the gap between these two separate main directions for the first time by introducing a taxonomy for personalized LLM usage and summarizing the key differences and challenges. We provide a formalization of the foundations of personalized LLMs that consolidates and expands notions of personalization of LLMs, defining and discussing novel facets of personalization, usage, and desiderata of personalized LLMs. We then unify the literature across these diverse fields and usage scenarios by proposing systematic taxonomies for the granularity of personalization, personalization techniques, datasets, evaluation methods, and applications of personalized LLMs. Finally, we highlight challenges and important open problems that remain to be addressed. By unifying and surveying recent research using the proposed taxonomies, we aim to provide a clear guide to the existing literature and different facets of personalization in LLMs, empowering both researchers and practitioners.', 'score': 23, 'issue_id': 409, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'a190b2e727d2d0ad', 'data': {'categories': ['#survey', '#multimodal', '#dataset'], 'emoji': '🎭', 'ru': {'title': 'Объединяя подходы: комплексный взгляд на персонализацию больших языковых моделей', 'desc': 'Статья посвящена персонализации больших языковых моделей (LLM) и объединяет два основных направления исследований в этой области. Авторы предлагают таксономию использования персонализированных LLM, формализуют основы и расширяют понятия персонализации. Они систематизируют литературу по различным аспектам, включая методы персонализации, наборы данных и способы оценки. В работе также выделяются нерешенные проблемы и открытые вопросы в данной области исследований.'}, 'en': {'title': 'Bridging Personalization Gaps in Large Language Models', 'desc': 'This paper addresses the growing need for personalization in Large Language Models (LLMs) by creating a comprehensive framework that connects personalized text generation with applications like recommendation systems. It introduces a taxonomy that categorizes various aspects of personalized LLMs, including techniques, datasets, and evaluation methods. The authors formalize the concept of personalization in LLMs, discussing its different dimensions and the challenges faced in this area. By synthesizing existing research and identifying open problems, the paper serves as a guide for researchers and practitioners interested in the personalization of LLMs.'}, 'zh': {'title': '统一个性化大型语言模型的研究', 'desc': '本文探讨了大型语言模型（LLMs）个性化的重要性及其应用。我们首次将个性化文本生成与个性化相关的下游应用（如推荐系统）结合起来，提出了个性化LLMs的分类法。文章对个性化LLMs的基础进行了形式化定义，并讨论了个性化的不同方面、使用场景和需求。最后，我们总结了现有文献，并指出了个性化LLMs面临的挑战和未解决的问题，以帮助研究人员和从业者更好地理解这一领域。'}}}, {'id': 'https://huggingface.co/papers/2411.00322', 'title': 'Constant Acceleration Flow', 'url': 'https://huggingface.co/papers/2411.00322', 'abstract': 'Rectified flow and reflow procedures have significantly advanced fast generation by progressively straightening ordinary differential equation (ODE) flows. They operate under the assumption that image and noise pairs, known as couplings, can be approximated by straight trajectories with constant velocity. However, we observe that modeling with constant velocity and using reflow procedures have limitations in accurately learning straight trajectories between pairs, resulting in suboptimal performance in few-step generation. To address these limitations, we introduce Constant Acceleration Flow (CAF), a novel framework based on a simple constant acceleration equation. CAF introduces acceleration as an additional learnable variable, allowing for more expressive and accurate estimation of the ODE flow. Moreover, we propose two techniques to further improve estimation accuracy: initial velocity conditioning for the acceleration model and a reflow process for the initial velocity. Our comprehensive studies on toy datasets, CIFAR-10, and ImageNet 64x64 demonstrate that CAF outperforms state-of-the-art baselines for one-step generation. We also show that CAF dramatically improves few-step coupling preservation and inversion over Rectified flow. Code is available at https://github.com/mlvlab/CAF{https://github.com/mlvlab/CAF}.', 'score': 20, 'issue_id': 412, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'edca1b3005d37bab', 'data': {'categories': ['#cv', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'CAF: Ускоряем генерацию изображений с помощью постоянного ускорения', 'desc': 'Статья представляет новый подход к ускорению генерации изображений в машинном обучении, называемый Constant Acceleration Flow (CAF). В отличие от существующих методов, CAF использует модель постоянного ускорения вместо постоянной скорости для более точного моделирования траекторий между парами изображений и шума. Авторы вводят ускорение как дополнительную обучаемую переменную и предлагают техники улучшения точности оценки, включая обусловливание начальной скорости и процесс рефлоу. Эксперименты на различных наборах данных показывают превосходство CAF над современными методами в одношаговой генерации и сохранении связей при генерации за несколько шагов.'}, 'en': {'title': 'Accelerating Image Generation with Constant Acceleration Flow', 'desc': 'This paper presents a new method called Constant Acceleration Flow (CAF) to improve the generation of images using ordinary differential equations (ODEs). Traditional methods assume that the flow between images and noise can be modeled as straight lines moving at a constant speed, which can lead to inaccuracies. CAF enhances this by introducing acceleration as a learnable variable, allowing for more flexible and precise modeling of the flow. The authors demonstrate that CAF outperforms existing methods in generating images with fewer steps while maintaining better quality and accuracy.'}, 'zh': {'title': '常加速度流：提升图像生成的新方法', 'desc': '本文提出了一种新的框架，称为常加速度流（CAF），用于改进图像生成过程。CAF通过引入加速度作为可学习变量，能够更准确地估计常微分方程（ODE）流。与传统的直线轨迹假设不同，CAF允许在生成过程中考虑加速度，从而提高生成质量。实验结果表明，CAF在少步生成和耦合保持方面优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2410.23266', 'title': 'TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models', 'url': 'https://huggingface.co/papers/2410.23266', 'abstract': "Existing benchmarks often highlight the remarkable performance achieved by state-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal context for video understanding. However, how well do the models truly perform visual temporal reasoning? Our study of existing benchmarks shows that this capability of MFMs is likely overestimated as many questions can be solved by using a single, few, or out-of-order frames. To systematically examine current visual temporal reasoning tasks, we propose three principles with corresponding metrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame Information Disparity. Following these principles, we introduce TOMATO, Temporal Reasoning Multimodal Evaluation, a novel benchmark crafted to rigorously assess MFMs' temporal reasoning capabilities in video understanding. TOMATO comprises 1,484 carefully curated, human-annotated questions spanning six tasks (i.e., action count, direction, rotation, shape & trend, velocity & frequency, and visual cues), applied to 1,417 videos, including 805 self-recorded and -generated videos, that encompass human-centric, real-world, and simulated scenarios. Our comprehensive evaluation reveals a human-model performance gap of 57.3% with the best-performing model. Moreover, our in-depth analysis uncovers more fundamental limitations beyond this gap in current MFMs. While they can accurately recognize events in isolated frames, they fail to interpret these frames as a continuous sequence. We believe TOMATO will serve as a crucial testbed for evaluating the next-generation MFMs and as a call to the community to develop AI systems capable of comprehending human world dynamics through the video modality.", 'score': 19, 'issue_id': 416, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '2743c77af808246f', 'data': {'categories': ['#benchmark', '#video', '#multimodal'], 'emoji': '🍅', 'ru': {'title': 'TOMATO: Новый стандарт для оценки временного рассуждения в видеоанализе', 'desc': 'Исследователи разработали новый бенчмарк TOMATO для оценки способностей мультимодальных фундаментальных моделей (МФМ) к временному рассуждению при анализе видео. Бенчмарк включает 1484 тщательно отобранных вопроса по шести задачам, применяемых к 1417 видео различных сценариев. Авторы выявили значительный разрыв в производительности между человеком и лучшей моделью, а также фундаментальные ограничения существующих МФМ в интерпретации последовательности кадров. TOMATO призван стать важным инструментом для оценки следующего поколения МФМ и стимулировать развитие систем ИИ, способных понимать динамику человеческого мира через видеомодальность.'}, 'en': {'title': 'TOMATO: A New Benchmark for Evaluating Temporal Reasoning in Video Understanding', 'desc': 'This paper investigates the true capabilities of Multimodal Foundation Models (MFMs) in visual temporal reasoning for video understanding. The authors argue that existing benchmarks may overstate the performance of these models, as many tasks can be solved using only a few frames or even out-of-order frames. To address this, they introduce TOMATO, a new benchmark designed to rigorously evaluate MFMs based on three principles: Multi-Frame Gain, Frame Order Sensitivity, and Frame Information Disparity. Their findings reveal a significant performance gap between human understanding and model capabilities, highlighting the need for improved temporal reasoning in future AI systems.'}, 'zh': {'title': '评估多模态模型的时间推理能力', 'desc': '本研究探讨了多模态基础模型（MFM）在视频理解中的视觉时间推理能力。我们发现，现有基准测试可能高估了这些模型的表现，因为许多问题可以通过少量或无序的帧来解决。为此，我们提出了三个原则和相应的评估指标，并引入了TOMATO基准，以系统评估MFM在视频理解中的时间推理能力。我们的评估显示，当前模型与人类表现之间存在57.3%的差距，揭示了MFM在处理连续帧时的基本局限性。'}}}, {'id': 'https://huggingface.co/papers/2411.00776', 'title': 'Randomized Autoregressive Visual Generation', 'url': 'https://huggingface.co/papers/2411.00776', 'abstract': "This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence-typically ordered in raster form-is randomly permuted into different factorization orders with a probability r, where r starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model's capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models will be made available at https://github.com/bytedance/1d-tokenizer", 'score': 16, 'issue_id': 408, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '0cc2c0f19f735f79', 'data': {'categories': ['#cv', '#architecture', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Случайная перестановка для улучшения генерации изображений', 'desc': 'Статья представляет метод Randomized AutoRegressive modeling (RAR) для генерации изображений. RAR использует случайную перестановку входной последовательности во время обучения авторегрессионной модели, что позволяет учитывать двунаправленный контекст. Метод сохраняет совместимость с языковыми моделями и достигает нового state-of-the-art результата на бенчмарке ImageNet-256 с показателем FID 1.48. RAR превосходит как авторегрессионные, так и диффузионные методы генерации изображений.'}, 'en': {'title': 'Revolutionizing Image Generation with Randomized AutoRegressive Modeling', 'desc': 'This paper introduces Randomized AutoRegressive modeling (RAR), a novel approach for generating images that enhances performance while remaining compatible with existing language modeling techniques. RAR employs a unique training method where the input sequence is randomly shuffled during the autoregressive training process, allowing the model to learn from various factorization orders. This strategy helps the model to better understand and utilize bidirectional contexts, leading to improved image generation capabilities. The results show that RAR achieves a remarkable FID score of 1.48 on the ImageNet-256 benchmark, outperforming previous state-of-the-art methods in both autoregressive and diffusion-based image generation.'}, 'zh': {'title': '随机自回归建模：图像生成的新突破', 'desc': '本文提出了一种随机自回归建模（RAR）方法用于视觉生成，在图像生成任务上设定了新的最先进性能，同时与语言建模框架完全兼容。RAR方法简单：在标准的自回归训练过程中，输入序列通常按光栅形式排列，但以概率r随机打乱为不同的因子化顺序，r从1开始，随着训练线性衰减到0。这种退火训练策略使模型能够学习最大化所有因子化顺序的期望似然，从而有效提高模型建模双向上下文的能力。RAR保持了自回归建模框架的完整性，确保与语言建模的完全兼容，同时在图像生成中显著提高了性能。'}}}, {'id': 'https://huggingface.co/papers/2411.00660', 'title': 'Physics in Next-token Prediction', 'url': 'https://huggingface.co/papers/2411.00660', 'abstract': "We discovered the underlying physics in Next-token Prediction (NTP). We identified the law of information conservation within NTP and proposed the First Law of Information Capacity (IC-1), demonstrating that the essence of intelligence emergence in auto-regressive models is fundamentally a process of information transfer. We also introduced Landauer's Principle into NTP, formulating the Second Law of Information Capacity (IC-2), which establishes the relationship between auto-regressive model training and energy consumption. Additionally, we presented several corollaries, which hold practical significance for production practices. Finally, we validated the compatibility and complementarity of our findings with existing theories.", 'score': 11, 'issue_id': 416, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '9114a8de1a432c2d', 'data': {'categories': ['#math', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Физика информации раскрывает тайны искусственного интеллекта', 'desc': 'Исследователи обнаружили фундаментальные физические принципы в задаче предсказания следующего токена (NTP). Они сформулировали Первый закон информационной емкости, показывающий, что возникновение интеллекта в авторегрессионных моделях по сути является процессом передачи информации. Введя принцип Ландауэра в NTP, ученые также сформулировали Второй закон информационной емкости, связывающий обучение авторегрессионных моделей с энергопотреблением. Исследование предлагает ряд практических следствий и согласуется с существующими теориями в области машинного обучения.'}, 'en': {'title': 'Unveiling the Physics of Next-Token Prediction', 'desc': "This paper explores the physics behind Next-token Prediction (NTP) in machine learning. It introduces the First Law of Information Capacity (IC-1), which highlights how intelligence in auto-regressive models arises from information transfer. The authors also apply Landauer's Principle to NTP, leading to the Second Law of Information Capacity (IC-2), linking model training to energy consumption. The findings are supported by practical corollaries and align with existing theories in the field."}, 'zh': {'title': '揭示自回归模型中的信息与能量关系', 'desc': '我们发现了下一步预测（NTP）中的基本物理原理。我们识别了NTP中的信息守恒定律，并提出了信息容量第一定律（IC-1），证明了自回归模型中智能出现的本质是信息传递的过程。我们还将朗道原理引入NTP，制定了信息容量第二定律（IC-2），建立了自回归模型训练与能量消耗之间的关系。此外，我们提出了几个具有实际意义的推论，验证了我们的发现与现有理论的兼容性和互补性。'}}}, {'id': 'https://huggingface.co/papers/2410.24159', 'title': 'GPT or BERT: why not both?', 'url': 'https://huggingface.co/papers/2410.24159', 'abstract': 'We present a simple way to merge masked language modeling with causal language modeling. This hybrid training objective results in a model that combines the strengths of both modeling paradigms within a single transformer stack: GPT-BERT can be transparently used like any standard causal or masked language model. We test the pretraining process that enables this flexible behavior on the BabyLM Challenge 2024. The results show that the hybrid pretraining outperforms masked-only or causal-only models. We openly release the models, training corpora and code.', 'score': 10, 'issue_id': 415, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'f46bbe7c538f7a87', 'data': {'categories': ['#architecture', '#training'], 'emoji': '🤖', 'ru': {'title': 'Гибридное языковое моделирование: лучшее из двух миров', 'desc': 'Исследователи представили новый метод объединения маскированного и причинно-следственного языкового моделирования. Результатом стала модель GPT-BERT, сочетающая преимущества обоих подходов в единой трансформерной архитектуре. Эксперименты в рамках BabyLM Challenge 2024 показали, что гибридное предобучение превосходит модели, использующие только один из методов. Авторы открыто публикуют модели, обучающие корпуса и код.'}, 'en': {'title': 'Merging Masked and Causal Language Models for Enhanced Performance', 'desc': 'This paper introduces a novel approach that merges masked language modeling (MLM) with causal language modeling (CLM) into a single transformer architecture. The resulting model, named GPT-BERT, leverages the advantages of both paradigms, allowing it to function effectively as either a standard masked or causal language model. The authors evaluate this hybrid training method on the BabyLM Challenge 2024, demonstrating that it outperforms models trained exclusively with either MLM or CLM. Additionally, they provide open access to the models, training data, and code to facilitate further research.'}, 'zh': {'title': '混合预训练，模型性能双赢！', 'desc': '本文提出了一种将掩码语言建模与因果语言建模相结合的简单方法。这种混合训练目标使得模型能够在单个变换器架构中结合两种建模范式的优点。我们在2024年BabyLM挑战赛中测试了这种灵活行为的预训练过程。结果表明，混合预训练的性能优于仅使用掩码或仅使用因果的模型。'}}}, {'id': 'https://huggingface.co/papers/2410.22370', 'title': 'Survey of User Interface Design and Interaction Techniques in Generative AI Applications', 'url': 'https://huggingface.co/papers/2410.22370', 'abstract': 'The applications of generative AI have become extremely impressive, and the interplay between users and AI is even more so. Current human-AI interaction literature has taken a broad look at how humans interact with generative AI, but it lacks specificity regarding the user interface designs and patterns used to create these applications. Therefore, we present a survey that comprehensively presents taxonomies of how a human interacts with AI and the user interaction patterns designed to meet the needs of a variety of relevant use cases. We focus primarily on user-guided interactions, surveying interactions that are initiated by the user and do not include any implicit signals given by the user. With this survey, we aim to create a compendium of different user-interaction patterns that can be used as a reference for designers and developers alike. In doing so, we also strive to lower the entry barrier for those attempting to learn more about the design of generative AI applications.', 'score': 10, 'issue_id': 409, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '9701ceb4e85eeeba', 'data': {'categories': ['#survey', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Путеводитель по взаимодействию человека и ИИ', 'desc': 'Статья представляет обзор таксономий взаимодействия человека с генеративным ИИ и паттернов пользовательского интерфейса для различных сценариев использования. Авторы фокусируются на взаимодействиях, инициируемых пользователем, без учета неявных сигналов. Цель работы - создать справочник паттернов взаимодействия для дизайнеров и разработчиков приложений генеративного ИИ. Исследование направлено на снижение входного барьера для тех, кто хочет изучить дизайн приложений генеративного ИИ.'}, 'en': {'title': 'Enhancing User Interaction with Generative AI', 'desc': 'This paper surveys the ways users interact with generative AI, focusing specifically on user-guided interactions. It identifies and categorizes various user interface designs and interaction patterns that cater to different use cases. By providing a comprehensive taxonomy, the authors aim to serve as a reference for designers and developers in creating more effective generative AI applications. The goal is to make it easier for newcomers to understand and engage with the design aspects of these technologies.'}, 'zh': {'title': '提升人机交互，设计更智能的生成式AI应用', 'desc': '本论文探讨了生成式人工智能（AI）与用户之间的互动，强调了用户界面设计的重要性。我们提供了一份全面的调查，分类了人类与AI的互动方式，特别关注用户主导的交互模式。通过这项调查，我们希望为设计师和开发者提供不同的用户交互模式参考，降低学习生成式AI应用设计的门槛。最终目标是提升人机交互的效率和用户体验。'}}}, {'id': 'https://huggingface.co/papers/2410.23775', 'title': 'In-Context LoRA for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2410.23775', 'abstract': 'Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20sim 100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems. We release our code, data, and models at https://github.com/ali-vilab/In-Context-LoRA', 'score': 9, 'issue_id': 407, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '748dab03a37a21a4', 'data': {'categories': ['#cv', '#diffusion', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Раскрытие скрытого потенциала DiT для многозадачной генерации изображений', 'desc': 'Исследование предлагает новый подход к использованию диффузионных трансформеров (DiT) для генерации изображений в различных задачах. Авторы обнаружили, что существующие модели DiT для преобразования текста в изображение уже обладают способностью к контекстной генерации без дополнительной настройки. На основе этого наблюдения они разработали простой pipeline, включающий конкатенацию изображений, совместное создание подписей и применение LoRA-тюнинга на небольших датасетах. Предложенный метод, названный IC-LoRA, позволяет генерировать высококачественные наборы изображений, лучше соответствующие заданным промптам.'}, 'en': {'title': 'Unlocking In-Context Generation with IC-LoRA', 'desc': 'This paper investigates the use of diffusion transformers (DiTs) for generating images without being tied to specific tasks. The authors propose that DiTs can generate images effectively with minimal adjustments, leveraging their inherent in-context generation capabilities. They introduce a new method called In-Context LoRA (IC-LoRA), which simplifies the process by concatenating images and using joint captioning, along with small dataset tuning. This approach enhances the quality of generated images while maintaining a flexible architecture that can adapt to various tasks without extensive retraining.'}, 'zh': {'title': '激活上下文生成能力，提升图像生成质量', 'desc': '本研究探讨了扩散变换器（DiTs）在无任务特定的图像生成中的应用。我们提出，文本到图像的DiTs本身具备上下文生成能力，只需少量调整即可激活。通过实验，我们展示了现有的文本到图像DiTs能够在不调整的情况下有效进行上下文生成。我们提出了一种简单的流程，利用DiTs的上下文能力，生成高保真度的图像集，且不需要对原始模型进行修改。'}}}, {'id': 'https://huggingface.co/papers/2411.00412', 'title': 'Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation', 'url': 'https://huggingface.co/papers/2411.00412', 'abstract': "Large Language Models (LLMs) demonstrate promising capabilities in solving simple scientific problems but often produce hallucinations for complex ones. While integrating LLMs with tools can increase reliability, this approach typically results in over-reliance on tools, diminishing the model's ability to solve simple problems through basic reasoning. In contrast, human experts first assess problem complexity using domain knowledge before choosing an appropriate solution approach. Inspired by this human problem-solving process, we propose a novel two-component fine-tuning method. In the first component World Knowledge Distillation (WKD), LLMs learn directly from solutions generated using tool's information to internalize domain knowledge. In the second component Tool Usage Adaptation (TUA), we partition problems into easy and hard categories based on the model's direct answering accuracy. While maintaining the same alignment target for easy problems as in WKD, we train the model to intelligently switch to tool usage for more challenging problems. We validate our method on six scientific benchmark datasets, spanning mathematics, climate science and epidemiology. On average, our models demonstrate a 28.18% improvement in answer accuracy and a 13.89% increase in tool usage precision across all datasets, surpassing state-of-the-art models including GPT-4o and Claude-3.5.", 'score': 9, 'issue_id': 407, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '27e4deefc7d09df0', 'data': {'categories': ['#rlhf', '#alignment', '#training', '#benchmark', '#math'], 'emoji': '🧠', 'ru': {'title': 'Умное переключение: как научить ИИ эффективно решать задачи разной сложности', 'desc': 'Исследование посвящено улучшению способности больших языковых моделей (LLM) решать научные задачи. Авторы предлагают двухкомпонентный метод дообучения: дистилляция мировых знаний и адаптация использования инструментов. Этот подход позволяет LLM эффективно решать простые задачи с помощью базовых рассуждений, а для сложных - прибегать к инструментам. Метод показал значительное улучшение точности ответов и точности использования инструментов на шести научных наборах данных.'}, 'en': {'title': 'Enhancing LLMs: Smart Tool Use for Complex Problems', 'desc': "This paper addresses the limitations of Large Language Models (LLMs) in solving complex scientific problems, which often lead to inaccuracies or 'hallucinations'. The authors propose a two-component fine-tuning method that mimics human problem-solving strategies by first assessing problem complexity. The first component, World Knowledge Distillation (WKD), allows LLMs to learn from solutions that utilize external tools, while the second component, Tool Usage Adaptation (TUA), helps the model categorize problems as easy or hard and decide when to use tools. The proposed method shows significant improvements in accuracy and tool usage precision across various scientific datasets, outperforming existing models."}, 'zh': {'title': '智能切换，提升模型解决问题的能力', 'desc': '大型语言模型（LLMs）在解决简单科学问题方面表现出色，但在复杂问题上常常出现幻觉。我们提出了一种新颖的双组件微调方法，模仿人类专家的解决问题过程。第一个组件是世界知识蒸馏（WKD），使LLMs从工具生成的解决方案中学习领域知识。第二个组件是工具使用适应（TUA），根据模型的直接回答准确性将问题分为简单和困难两类，从而提高模型在复杂问题上的工具使用能力。'}}}, {'id': 'https://huggingface.co/papers/2411.00771', 'title': 'CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes', 'url': 'https://huggingface.co/papers/2411.00771', 'abstract': 'Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately representing surfaces, especially in large and complex scenarios, remains a significant challenge due to the unstructured nature of 3DGS. In this paper, we present CityGaussianV2, a novel approach for large-scale scene reconstruction that addresses critical challenges related to geometric accuracy and efficiency. Building on the favorable generalization capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and scalability issues. Specifically, we implement a decomposed-gradient-based densification and depth regression technique to eliminate blurry artifacts and accelerate convergence. To scale up, we introduce an elongation filter that mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we optimize the CityGaussian pipeline for parallel training, achieving up to 10times compression, at least 25% savings in training time, and a 50% decrease in memory usage. We also established standard geometry benchmarks under large-scale scenes. Experimental results demonstrate that our method strikes a promising balance between visual quality, geometric accuracy, as well as storage and training costs. The project page is available at https://dekuliutesla.github.io/CityGaussianV2/.', 'score': 8, 'issue_id': 415, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '00c1f9c65cf89cfe', 'data': {'categories': ['#3d', '#benchmark', '#training', '#optimization'], 'emoji': '🏙️', 'ru': {'title': 'CityGaussianV2: Эффективная реконструкция крупномасштабных сцен с помощью улучшенного Gaussian Splatting', 'desc': 'Статья представляет CityGaussianV2 - новый подход к реконструкции крупномасштабных сцен, решающий проблемы геометрической точности и эффективности в 3D Gaussian Splatting. Авторы внедряют технику уплотнения на основе декомпозированного градиента и регрессии глубины для устранения размытых артефактов и ускорения сходимости. Для масштабирования вводится фильтр удлинения, снижающий взрывной рост количества гауссианов. Оптимизированный конвейер CityGaussian позволяет достичь 10-кратного сжатия, экономии времени обучения на 25% и снижения использования памяти на 50%.'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with CityGaussianV2', 'desc': 'This paper introduces CityGaussianV2, a new method for reconstructing large-scale 3D scenes using Gaussian splatting techniques. It addresses challenges in geometric accuracy and efficiency that arise from the unstructured nature of traditional 3D Gaussian Splatting. The authors implement a novel densification and depth regression technique to improve image clarity and speed up the training process. Additionally, they optimize the training pipeline to reduce memory usage and training time significantly while maintaining high visual quality and accuracy.'}, 'zh': {'title': '高效精准的大规模场景重建新方法', 'desc': '3D高斯点云（3DGS）在辐射场重建中取得了显著进展，但在复杂场景中准确表示表面仍然是一个挑战。本文提出了CityGaussianV2，一种针对大规模场景重建的新方法，解决了几何精度和效率的问题。我们采用分解梯度的密集化和深度回归技术，消除模糊伪影并加速收敛。通过引入延伸滤波器，我们有效地减少了高斯数量的爆炸，同时优化了CityGaussian管道，实现了训练时间和内存使用的显著节省。'}}}, {'id': 'https://huggingface.co/papers/2411.00233', 'title': 'SambaMixer: State of Health Prediction of Li-ion Batteries using Mamba State Space Models', 'url': 'https://huggingface.co/papers/2411.00233', 'abstract': 'The state of health (SOH) of a Li-ion battery is a critical parameter that determines the remaining capacity and the remaining lifetime of the battery. In this paper, we propose SambaMixer a novel structured state space model (SSM) for predicting the state of health of Li-ion batteries. The proposed SSM is based on the MambaMixer architecture, which is designed to handle multi-variate time signals. We evaluate our model on the NASA battery discharge dataset and show that our model outperforms the state-of-the-art on this dataset. We further introduce a novel anchor-based resampling method which ensures time signals are of the expected length while also serving as augmentation technique. Finally, we condition prediction on the sample time and the cycle time difference using positional encodings to improve the performance of our model and to learn recuperation effects. Our results proof that our model is able to predict the SOH of Li-ion batteries with high accuracy and robustness.', 'score': 7, 'issue_id': 410, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '6361ca66f5ca137f', 'data': {'categories': ['#dataset', '#data', '#benchmark', '#architecture', '#training', '#medicine'], 'emoji': '🔋', 'ru': {'title': 'Точное прогнозирование срока службы аккумуляторов с помощью глубокого обучения', 'desc': 'Статья представляет SambaMixer - новую модель структурированного пространства состояний для прогнозирования состояния здоровья литий-ионных аккумуляторов. Модель основана на архитектуре MambaMixer и способна обрабатывать многомерные временные сигналы. Авторы предлагают метод ресэмплинга на основе якорей для нормализации длины сигналов и аугментации данных. Использование позиционного кодирования времени выборки и разницы циклов позволяет модели учитывать эффекты восстановления аккумуляторов.'}, 'en': {'title': 'SambaMixer: Revolutionizing Li-ion Battery Health Prediction', 'desc': 'This paper introduces SambaMixer, a new structured state space model (SSM) designed to predict the state of health (SOH) of Li-ion batteries. The model utilizes the MambaMixer architecture to effectively process multi-variate time signals, enhancing prediction accuracy. It is evaluated against the NASA battery discharge dataset, demonstrating superior performance compared to existing methods. Additionally, the paper presents an innovative anchor-based resampling technique and employs positional encodings to improve predictions by accounting for time-related factors.'}, 'zh': {'title': '高效预测锂离子电池健康状态的新方法', 'desc': '本文提出了一种新颖的结构化状态空间模型（SSM），用于预测锂离子电池的健康状态（SOH）。该模型基于MambaMixer架构，能够处理多变量时间信号。我们在NASA电池放电数据集上评估了该模型，结果显示其性能优于现有的最先进方法。此外，我们引入了一种新颖的基于锚点的重采样方法，以确保时间信号的预期长度，并作为数据增强技术。'}}}, {'id': 'https://huggingface.co/papers/2410.22901', 'title': 'HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models', 'url': 'https://huggingface.co/papers/2410.22901', 'abstract': 'We propose an effective method for inserting adapters into text-to-image foundation models, which enables the execution of complex downstream tasks while preserving the generalization ability of the base model. The core idea of this method is to optimize the attention mechanism related to 2D feature maps, which enhances the performance of the adapter. This approach was validated on the task of meme video generation and achieved significant results. We hope this work can provide insights for post-training tasks of large text-to-image models. Additionally, as this method demonstrates good compatibility with SD1.5 derivative models, it holds certain value for the open-source community. Therefore, we will release the related code (https://songkey.github.io/hellomeme).', 'score': 7, 'issue_id': 408, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '801963cbdcf75d7b', 'data': {'categories': ['#cv', '#video', '#training', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'Адаптеры для текст-в-изображение моделей: новый подход к генерации мемов', 'desc': 'Статья представляет эффективный метод внедрения адаптеров в базовые модели преобразования текста в изображение. Этот подход оптимизирует механизм внимания, связанный с двумерными картами признаков, что улучшает производительность адаптера. Метод был успешно применен для генерации мемов в видеоформате. Авторы отмечают совместимость метода с производными моделями SD1.5, что делает его ценным для сообщества открытого исходного кода.'}, 'en': {'title': 'Enhancing Text-to-Image Models with Adapter Integration', 'desc': 'This paper presents a novel method for integrating adapters into text-to-image foundation models, allowing them to perform complex tasks while maintaining their ability to generalize. The method focuses on optimizing the attention mechanism associated with 2D feature maps, which significantly boosts the performance of the adapters. The effectiveness of this approach was demonstrated through the task of meme video generation, yielding impressive results. The authors aim to contribute to the open-source community by sharing their code and providing insights for post-training tasks in large text-to-image models.'}, 'zh': {'title': '适配器插入：提升文本到图像模型的能力', 'desc': '本文提出了一种有效的方法，将适配器插入文本到图像的基础模型中，从而在执行复杂的下游任务时保持基础模型的泛化能力。该方法的核心思想是优化与二维特征图相关的注意力机制，从而增强适配器的性能。我们在生成表情包视频的任务上验证了该方法，并取得了显著的结果。希望这项工作能为大型文本到图像模型的后训练任务提供一些见解，并为开源社区带来价值。'}}}, {'id': 'https://huggingface.co/papers/2411.00225', 'title': 'Fashion-VDM: Video Diffusion Model for Virtual Try-On', 'url': 'https://huggingface.co/papers/2411.00225', 'abstract': "We present Fashion-VDM, a video diffusion model (VDM) for generating virtual try-on videos. Given an input garment image and person video, our method aims to generate a high-quality try-on video of the person wearing the given garment, while preserving the person's identity and motion. Image-based virtual try-on has shown impressive results; however, existing video virtual try-on (VVT) methods are still lacking garment details and temporal consistency. To address these issues, we propose a diffusion-based architecture for video virtual try-on, split classifier-free guidance for increased control over the conditioning inputs, and a progressive temporal training strategy for single-pass 64-frame, 512px video generation. We also demonstrate the effectiveness of joint image-video training for video try-on, especially when video data is limited. Our qualitative and quantitative experiments show that our approach sets the new state-of-the-art for video virtual try-on. For additional results, visit our project page: https://johannakarras.github.io/Fashion-VDM.", 'score': 6, 'issue_id': 418, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'a412815c3df113c6', 'data': {'categories': ['#video', '#diffusion', '#cv'], 'emoji': '👚', 'ru': {'title': 'Виртуальная примерка одежды на видео с помощью диффузионных моделей', 'desc': 'Fashion-VDM - это модель видеодиффузии для создания виртуальных примерочных видео. Модель генерирует высококачественное видео человека, одетого в заданный предмет одежды, сохраняя при этом личность и движения человека. Авторы предлагают архитектуру на основе диффузии, разделенное беcклассификаторное управление и прогрессивную стратегию временного обучения для генерации 64-кадровых видео разрешением 512 пикселей за один проход. Эксперименты показывают, что подход устанавливает новый уровень качества в задаче виртуальной примерки одежды на видео.'}, 'en': {'title': 'Revolutionizing Virtual Try-Ons with Fashion-VDM!', 'desc': 'Fashion-VDM is a novel video diffusion model designed to create realistic virtual try-on videos by combining garment images with person videos. The model focuses on maintaining the identity and motion of the person while accurately displaying the garment. It addresses challenges in existing video virtual try-on methods, such as lack of detail and temporal consistency, by utilizing a diffusion-based architecture and a progressive training strategy. Our experiments demonstrate that Fashion-VDM achieves state-of-the-art results in video virtual try-on, even with limited video data.'}, 'zh': {'title': 'Fashion-VDM：视频虚拟试穿的新突破', 'desc': '我们提出了Fashion-VDM，这是一种用于生成虚拟试穿视频的视频扩散模型。该方法可以根据输入的服装图像和人物视频，生成高质量的试穿视频，同时保持人物的身份和动作。与现有的视频虚拟试穿方法相比，我们的方法在服装细节和时间一致性方面有显著提升。我们的实验结果表明，Fashion-VDM在视频虚拟试穿领域达到了新的最先进水平。'}}}, {'id': 'https://huggingface.co/papers/2411.00680', 'title': 'Zipfian Whitening', 'url': 'https://huggingface.co/papers/2411.00680', 'abstract': "The word embedding space in neural models is skewed, and correcting this can improve task performance. We point out that most approaches for modeling, correcting, and measuring the symmetry of an embedding space implicitly assume that the word frequencies are uniform; in reality, word frequencies follow a highly non-uniform distribution, known as Zipf's law. Surprisingly, simply performing PCA whitening weighted by the empirical word frequency that follows Zipf's law significantly improves task performance, surpassing established baselines. From a theoretical perspective, both our approach and existing methods can be clearly categorized: word representations are distributed according to an exponential family with either uniform or Zipfian base measures. By adopting the latter approach, we can naturally emphasize informative low-frequency words in terms of their vector norm, which becomes evident from the information-geometric perspective, and in terms of the loss functions for imbalanced classification. Additionally, our theory corroborates that popular natural language processing methods, such as skip-gram negative sampling, WhiteningBERT, and headless language models, work well just because their word embeddings encode the empirical word frequency into the underlying probabilistic model.", 'score': 6, 'issue_id': 413, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '26e33c177a131240', 'data': {'categories': ['#data', '#math', '#interpretability', '#transfer_learning'], 'emoji': '📊', 'ru': {'title': 'Улучшение векторных представлений слов с учетом закона Ципфа', 'desc': 'Статья посвящена проблеме асимметрии пространства векторных представлений слов в нейронных моделях. Авторы предлагают метод коррекции этой асимметрии с учетом закона Ципфа о распределении частот слов. Простое применение PCA отбеливания, взвешенного по эмпирической частоте слов, значительно улучшает производительность модели на различных задачах. Теоретический анализ показывает, что этот подход естественным образом подчеркивает информативные низкочастотные слова и объясняет эффективность популярных методов обработки естественного языка.'}, 'en': {'title': 'Correcting Skewness in Word Embeddings for Better Performance', 'desc': "This paper discusses how the word embedding space in neural models can be improved by addressing its skewness. It highlights that many existing methods assume uniform word frequencies, while in reality, word frequencies follow Zipf's law, which is highly non-uniform. The authors propose using PCA whitening that is weighted by empirical word frequencies, leading to significant improvements in task performance. Their theoretical framework categorizes word representations based on exponential families, emphasizing the importance of low-frequency words and providing insights into popular NLP methods."}, 'zh': {'title': '校正词嵌入空间，提升任务性能！', 'desc': '这篇论文探讨了神经模型中的词嵌入空间偏斜问题，并提出了通过校正这一偏斜来提高任务性能的方法。研究表明，现有的许多模型假设词频是均匀分布的，但实际上，词频遵循一种称为齐夫定律的高度非均匀分布。通过对词频进行加权的主成分分析（PCA）白化，显著提升了任务性能，超越了已有的基准。理论上，我们的方法与现有方法可以清晰地分类，强调了低频词的重要性，并解释了流行的自然语言处理方法为何有效。'}}}, {'id': 'https://huggingface.co/papers/2411.00369', 'title': 'GRS-QA -- Graph Reasoning-Structured Question Answering Dataset', 'url': 'https://huggingface.co/papers/2411.00369', 'abstract': 'Large Language Models (LLMs) have excelled in multi-hop question-answering (M-QA) due to their advanced reasoning abilities. However, the impact of the inherent reasoning structures on LLM M-QA performance remains unclear, largely due to the absence of QA datasets that provide fine-grained reasoning structures. To address this gap, we introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which includes both semantic contexts and reasoning structures for QA pairs. Unlike existing M-QA datasets, where different reasoning structures are entangled together, GRS-QA explicitly captures intricate reasoning pathways by constructing reasoning graphs, where nodes represent textual contexts and edges denote logical flows. These reasoning graphs of different structures enable a fine-grained evaluation of LLM reasoning capabilities across various reasoning structures. Our empirical analysis reveals that LLMs perform differently when handling questions with varying reasoning structures. This finding facilitates the exploration of textual structures as compared with semantics.', 'score': 6, 'issue_id': 409, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'b3e4773e065d1bc1', 'data': {'categories': ['#dataset', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Графы рассуждений раскрывают возможности языковых моделей', 'desc': 'Статья представляет новый набор данных GRS-QA для оценки способностей больших языковых моделей (LLM) в многоходовых вопросно-ответных задачах. GRS-QA включает в себя как семантические контексты, так и структуры рассуждений для пар вопрос-ответ. Набор данных использует графы рассуждений, где узлы представляют текстовые контексты, а ребра обозначают логические связи. Эмпирический анализ показывает, что LLM по-разному справляются с вопросами, имеющими различные структуры рассуждений.'}, 'en': {'title': 'Unlocking LLMs: Understanding Reasoning Structures in Multi-Hop QA', 'desc': 'This paper discusses the performance of Large Language Models (LLMs) in multi-hop question-answering (M-QA) tasks, focusing on their reasoning abilities. The authors identify a gap in existing datasets that do not provide detailed reasoning structures, which are crucial for evaluating LLM performance. To fill this gap, they introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which features reasoning graphs that clearly outline the logical pathways for answering questions. Their analysis shows that LLMs exhibit varying performance based on the complexity of the reasoning structures involved, highlighting the importance of understanding both textual and semantic elements in M-QA.'}, 'zh': {'title': '揭示推理结构对LLM表现的影响', 'desc': '大型语言模型（LLMs）在多跳问答（M-QA）中表现出色，主要得益于其先进的推理能力。然而，LLM在多跳问答中的表现受固有推理结构的影响尚不明确，主要是因为缺乏提供细粒度推理结构的问答数据集。为了解决这个问题，我们引入了图推理结构问答数据集（GRS-QA），该数据集为问答对提供了语义上下文和推理结构。与现有的M-QA数据集不同，GRS-QA通过构建推理图来明确捕捉复杂的推理路径，节点表示文本上下文，边表示逻辑流，从而实现对LLM推理能力的细粒度评估。'}}}, {'id': 'https://huggingface.co/papers/2411.00762', 'title': 'Face Anonymization Made Simple', 'url': 'https://huggingface.co/papers/2411.00762', 'abstract': 'Current face anonymization techniques often depend on identity loss calculated by face recognition models, which can be inaccurate and unreliable. Additionally, many methods require supplementary data such as facial landmarks and masks to guide the synthesis process. In contrast, our approach uses diffusion models with only a reconstruction loss, eliminating the need for facial landmarks or masks while still producing images with intricate, fine-grained details. We validated our results on two public benchmarks through both quantitative and qualitative evaluations. Our model achieves state-of-the-art performance in three key areas: identity anonymization, facial attribute preservation, and image quality. Beyond its primary function of anonymization, our model can also perform face swapping tasks by incorporating an additional facial image as input, demonstrating its versatility and potential for diverse applications. Our code and models are available at https://github.com/hanweikung/face_anon_simple .', 'score': 5, 'issue_id': 415, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '0948ed1ff23fd58b', 'data': {'categories': ['#cv', '#benchmark', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'Простая и эффективная анонимизация лиц с помощью диффузионных моделей', 'desc': 'Эта статья представляет новый метод анонимизации лиц с использованием диффузионных моделей. В отличие от существующих подходов, метод не требует дополнительных данных вроде лицевых ориентиров или масок. Модель достигает наилучших результатов в анонимизации личности, сохранении атрибутов лица и качестве изображения. Кроме того, она может выполнять задачи по замене лиц, демонстрируя свою универсальность.'}, 'en': {'title': 'Revolutionizing Face Anonymization with Diffusion Models', 'desc': "This paper presents a novel face anonymization technique that utilizes diffusion models, focusing solely on reconstruction loss without the need for additional data like facial landmarks or masks. The method achieves high-quality image generation while effectively anonymizing identities and preserving facial attributes. The authors demonstrate the model's performance through rigorous evaluations on public benchmarks, showcasing its state-of-the-art results in identity anonymization and image quality. Additionally, the model's versatility is highlighted by its capability to perform face swapping tasks, making it suitable for various applications."}, 'zh': {'title': '创新的面部匿名化与交换技术', 'desc': '当前的面部匿名化技术通常依赖于面部识别模型计算的身份损失，这可能不够准确和可靠。我们的方法使用扩散模型，仅依赖重建损失，省去了面部特征点或面具的需求，同时仍能生成细致的图像。我们的模型在身份匿名化、面部属性保留和图像质量三个关键领域达到了最先进的性能。除了匿名化功能外，我们的模型还可以通过添加额外的面部图像作为输入来执行面部交换任务，展示了其多样性和潜在应用。'}}}, {'id': 'https://huggingface.co/papers/2410.21157', 'title': 'M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation', 'url': 'https://huggingface.co/papers/2410.21157', 'abstract': 'Repository-level code completion has drawn great attention in software engineering, and several benchmark datasets have been introduced. However, existing repository-level code completion benchmarks usually focus on a limited number of languages (<5), which cannot evaluate the general code intelligence abilities across different languages for existing code Large Language Models (LLMs). Besides, the existing benchmarks usually report overall average scores of different languages, where the fine-grained abilities in different completion scenarios are ignored. Therefore, to facilitate the research of code LLMs in multilingual scenarios, we propose a massively multilingual repository-level code completion benchmark covering 18 programming languages (called M2RC-EVAL), and two types of fine-grained annotations (i.e., bucket-level and semantic-level) on different completion scenarios are provided, where we obtain these annotations based on the parsed abstract syntax tree. Moreover, we also curate a massively multilingual instruction corpora M2RC- INSTRUCT dataset to improve the repository-level code completion abilities of existing code LLMs. Comprehensive experimental results demonstrate the effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.', 'score': 5, 'issue_id': 408, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'd6a0779456870cae', 'data': {'categories': ['#dataset', '#benchmark', '#plp', '#multilingual'], 'emoji': '🖥️', 'ru': {'title': 'Многоязычный бенчмарк для оценки LLM в автодополнении кода', 'desc': 'Статья представляет новый бенчмарк M2RC-EVAL для оценки способностей больших языковых моделей (LLM) в задаче автодополнения кода на уровне репозитория. Бенчмарк охватывает 18 языков программирования и включает детальные аннотации для различных сценариев дополнения. Авторы также создали набор данных M2RC-INSTRUCT для улучшения возможностей существующих LLM в этой задаче. Эксперименты подтверждают эффективность предложенных инструментов.'}, 'en': {'title': 'Empowering Multilingual Code Completion with M2RC-EVAL and M2RC-INSTRUCT', 'desc': 'This paper introduces a new benchmark called M2RC-EVAL for repository-level code completion that supports 18 programming languages, addressing the limitations of existing benchmarks that only cover a few languages. It provides fine-grained annotations based on abstract syntax trees, allowing for a more detailed evaluation of code completion scenarios. Additionally, the authors present the M2RC-INSTRUCT dataset to enhance the performance of code Large Language Models (LLMs) in multilingual contexts. Experimental results show that both M2RC-EVAL and M2RC-INSTRUCT significantly improve the capabilities of existing code LLMs.'}, 'zh': {'title': '多语言代码补全的新基准测试', 'desc': '本论文提出了一种新的多语言代码补全基准测试，称为M2RC-EVAL，涵盖了18种编程语言。现有的基准测试通常只关注少数几种语言，无法全面评估大型语言模型在不同语言中的代码智能能力。此外，M2RC-EVAL提供了细粒度的注释，帮助研究人员更好地理解模型在不同补全场景下的表现。为了进一步提升代码补全能力，我们还构建了一个多语言指令数据集M2RC-INSTRUCT。'}}}, {'id': 'https://huggingface.co/papers/2411.00030', 'title': 'WikiNER-fr-gold: A Gold-Standard NER Corpus', 'url': 'https://huggingface.co/papers/2411.00030', 'abstract': 'We address in this article the the quality of the WikiNER corpus, a multilingual Named Entity Recognition corpus, and provide a consolidated version of it. The annotation of WikiNER was produced in a semi-supervised manner i.e. no manual verification has been carried out a posteriori. Such corpus is called silver-standard. In this paper we propose WikiNER-fr-gold which is a revised version of the French proportion of WikiNER. Our corpus consists of randomly sampled 20% of the original French sub-corpus (26,818 sentences with 700k tokens). We start by summarizing the entity types included in each category in order to define an annotation guideline, and then we proceed to revise the corpus. Finally we present an analysis of errors and inconsistency observed in the WikiNER-fr corpus, and we discuss potential future work directions.', 'score': 4, 'issue_id': 411, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '5f3e739256c5e800', 'data': {'categories': ['#dataset', '#data', '#multilingual'], 'emoji': '🏷️', 'ru': {'title': 'Золотой стандарт для французского NER: улучшение WikiNER', 'desc': 'Статья посвящена улучшению качества корпуса WikiNER для распознавания именованных сущностей на нескольких языках. Авторы создали WikiNER-fr-gold - вручную проверенную версию французской части корпуса, состоящую из 20% оригинальных данных. Они разработали руководство по аннотации, пересмотрели корпус и проанализировали ошибки в исходном WikiNER-fr. Исследование направлено на повышение точности обучения моделей NER на французском языке.'}, 'en': {'title': 'Enhancing Named Entity Recognition with WikiNER-fr-gold', 'desc': 'This paper focuses on improving the quality of the WikiNER corpus, which is used for Named Entity Recognition (NER) in multiple languages. The original WikiNER corpus was created using a semi-supervised approach, resulting in a silver-standard dataset without manual verification. The authors introduce WikiNER-fr-gold, a refined version of the French subset of WikiNER, based on a carefully sampled 20% of the original data. They outline the types of entities included, establish annotation guidelines, and analyze errors in the original corpus to suggest future improvements.'}, 'zh': {'title': '提升WikiNER语料库质量的探索', 'desc': '本文讨论了WikiNER语料库的质量，这是一个多语言命名实体识别语料库，并提供了一个整合版本。WikiNER的标注是以半监督的方式生成的，即没有进行后期的人工验证，因此该语料库被称为银标准。我们提出了WikiNER-fr-gold，这是WikiNER法语部分的修订版本，包含了原法语子语料库的20%随机抽样（26,818个句子，700k个标记）。最后，我们分析了WikiNER-fr语料库中观察到的错误和不一致，并讨论了未来的研究方向。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi', '#alignment (1)', '#architecture (4)', '#audio', '#benchmark (9)', '#cv (6)', '#data (4)', '#dataset (6)', '#diffusion (3)', '#edge_computing', '#ethics', '#games', '#graphs', '#hallucinations', '#inference', '#interpretability (1)', '#long_context', '#math (3)', '#medicine (1)', '#multilingual (2)', '#multimodal (3)', '#optimization (3)', '#plp (1)', '#rag', '#reasoning (1)', '#rl', '#rlhf (1)', '#robotics', '#security', '#story_generation', '#survey (2)', '#synthetic', '#training (9)', '#transfer_learning (1)', '#translation', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-11-04 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-11-04 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-11-04 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    