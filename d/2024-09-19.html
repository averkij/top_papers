
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 17 papers. September 19.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            margin-top: 10px;
            margin-bottom: 10px;
            display: block;
            border-radius: 5px;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">19 сентября</span> | <span id="title-articles-count">17 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-09-18.html">⬅️ <span id="prev-date">18.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-09-20.html">➡️ <span id="next-date">20.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '19 сентября', 'en': 'September 19', 'zh': '9月19日'};
        let feedDateNext = {'ru': '20.09', 'en': '09/20', 'zh': '9月20日'};
        let feedDatePrev = {'ru': '18.09', 'en': '09/18', 'zh': '9月18日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2409.12186', 'title': 'Qwen2.5-Coder Technical Report', 'url': 'https://huggingface.co/papers/2409.12186', 'abstract': 'In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes two models: Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general versatility. The model has been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will not only push the boundaries of research in code intelligence but also, through its permissive licensing, encourage broader adoption by developers in real-world applications.', 'score': 125, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '3a409a257f1d480b', 'data': {'categories': ['#reasoning', '#training', '#data', '#plp', '#benchmark', '#open_source', '#small_models', '#architecture', '#synthetic'], 'emoji': '🖥️', 'ru': {'title': 'Qwen2.5-Coder: прорыв в области искусственного интеллекта для программирования', 'desc': 'В статье представлена серия моделей Qwen2.5-Coder, улучшенная версия CodeQwen1.5. Модели основаны на архитектуре Qwen2.5 и дообучены на корпусе из более чем 5,5 триллионов токенов. Благодаря тщательной очистке данных, масштабируемой генерации синтетических данных и сбалансированному смешиванию данных, Qwen2.5-Coder демонстрирует впечатляющие возможности генерации кода. Модели достигли наилучших результатов в более чем 10 бенчмарках, включая генерацию, завершение, рассуждение и исправление кода.'}, 'en': {'title': 'Empowering Code Generation with Qwen2.5-Coder!', 'desc': 'The Qwen2.5-Coder series represents a major advancement in code generation models, succeeding the CodeQwen1.5. It consists of two versions, Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B, which are built on the Qwen2.5 architecture and trained on an extensive dataset of over 5.5 trillion tokens. This model excels in various code-related tasks, achieving state-of-the-art performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair. The enhancements in data processing and model training are designed to foster greater adoption among developers for practical applications.'}, 'zh': {'title': 'Qwen2.5-Coder：代码生成的新标杆', 'desc': '本文介绍了Qwen2.5-Coder系列，这是对其前身CodeQwen1.5的重要升级。该系列包括两个模型：Qwen2.5-Coder-1.5B和Qwen2.5-Coder-7B，专注于代码生成。Qwen2.5-Coder基于Qwen2.5架构，经过超过5.5万亿个标记的预训练，展现出卓越的代码生成能力。该模型在多项代码相关任务上表现出色，超越了同等规模的更大模型，推动了代码智能研究的前沿。'}}}, {'id': 'https://huggingface.co/papers/2409.12191', 'title': "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution", 'url': 'https://huggingface.co/papers/2409.12191', 'abstract': "We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at https://github.com/QwenLM/Qwen2-VL.", 'score': 73, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '298712ce7466399d', 'data': {'categories': ['#video', '#cv', '#training', '#optimization', '#alignment', '#open_source', '#small_models', '#architecture', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Динамическое разрешение для улучшенного мультимодального восприятия', 'desc': 'Статья представляет серию моделей Qwen2-VL, которые улучшают обработку изображений с помощью механизма Naive Dynamic Resolution. Это позволяет динамически обрабатывать изображения разного разрешения, создавая более эффективные визуальные представления. Модели используют мультимодальное позиционное кодирование M-RoPE для объединения информации из текста, изображений и видео. Исследование также изучает масштабирование больших мультимодальных моделей, демонстрируя конкурентоспособные результаты на различных бенчмарках.'}, 'en': {'title': 'Dynamic Resolution for Enhanced Visual Understanding', 'desc': 'The Qwen2-VL Series is an upgraded version of the Qwen-VL models that changes how visual data is processed by using a Naive Dynamic Resolution mechanism. This allows the model to handle images of different resolutions more flexibly, resulting in better and more accurate visual representations. It also incorporates Multimodal Rotary Position Embedding (M-RoPE) to effectively combine positional information from text, images, and videos. By scaling the model size and training data, the Qwen2-VL-72B model achieves performance on par with top models like GPT-4o and Claude3.5-Sonnet in multimodal tasks.'}, 'zh': {'title': '动态分辨率，提升视觉感知能力！', 'desc': 'Qwen2-VL系列是对之前Qwen-VL模型的高级升级，重新定义了视觉处理中的传统预设分辨率方法。它引入了简单动态分辨率机制，使模型能够动态处理不同分辨率的图像，并生成不同数量的视觉标记。该模型还集成了多模态旋转位置嵌入（M-RoPE），有效融合文本、图像和视频的位置信息。通过统一的图像和视频处理范式，Qwen2-VL增强了模型的视觉感知能力，并在大规模多模态模型的研究中取得了显著的性能。'}}}, {'id': 'https://huggingface.co/papers/2409.12181', 'title': 'A Controlled Study on Long Context Extension and Generalization in LLMs', 'url': 'https://huggingface.co/papers/2409.12181', 'abstract': 'Broad textual understanding and in-context learning require language models that utilize full document contexts. Due to the implementation challenges associated with directly training long-context models, many methods have been proposed for extending models to handle long contexts. However, owing to differences in data and model classes, it has been challenging to compare these approaches, leading to uncertainty as to how to evaluate long-context performance and whether it differs from standard evaluation. We implement a controlled protocol for extension methods with a standardized evaluation, utilizing consistent base models and extension data. Our study yields several insights into long-context behavior. First, we reaffirm the critical role of perplexity as a general-purpose performance indicator even in longer-context tasks. Second, we find that current approximate attention methods systematically underperform across long-context tasks. Finally, we confirm that exact fine-tuning based methods are generally effective within the range of their extension, whereas extrapolation remains challenging. All codebases, models, and checkpoints will be made available open-source, promoting transparency and facilitating further research in this critical area of AI development.', 'score': 43, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '40d004b4e127be2d', 'data': {'categories': ['#long_context', '#training', '#benchmark', '#open_source', '#architecture'], 'emoji': '📏', 'ru': {'title': 'Стандартизация оценки языковых моделей с длинным контекстом', 'desc': 'Статья посвящена исследованию методов расширения языковых моделей для работы с длинными контекстами. Авторы проводят стандартизированную оценку различных подходов, используя одинаковые базовые модели и данные для расширения. Исследование подтверждает важность перплексии как универсального показателя производительности даже для задач с длинным контекстом. Результаты показывают, что методы точной донастройки эффективны в пределах диапазона расширения, но экстраполяция остается сложной задачей.'}, 'en': {'title': 'Unlocking Long-Context Understanding in Language Models', 'desc': 'This paper discusses the challenges of training language models to understand long documents. It highlights the difficulty in comparing different methods for extending models to handle longer contexts due to varying data and model types. The authors propose a standardized evaluation protocol to assess the performance of these long-context models. Their findings indicate that perplexity remains a key performance metric, while approximate attention methods tend to underperform, and fine-tuning methods are effective but struggle with extrapolation.'}, 'zh': {'title': '提升长文本理解的关键在于困惑度', 'desc': '这篇论文探讨了语言模型在处理长文本上下文时的表现。研究表明，困惑度是评估长上下文任务的重要指标。当前的近似注意力方法在长上下文任务中表现不佳，而精确微调的方法在其扩展范围内通常有效。作者还提供了开源代码和模型，以促进该领域的进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2409.12183', 'title': 'To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning', 'url': 'https://huggingface.co/papers/2409.12183', 'abstract': "Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra ``thinking'' really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and ran our own evaluations of 20 datasets across 14 models. Our results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, we analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. Our results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications.", 'score': 36, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '062e35c77608607b', 'data': {'categories': ['#reasoning', '#survey', '#dataset', '#math', '#inference', '#interpretability', '#benchmark'], 'emoji': '🧠', 'ru': {'title': "Цепочка рассуждений: когда действительно нужно 'думать'?", 'desc': 'Статья анализирует эффективность метода цепочки рассуждений (Chain-of-Thought, CoT) для языковых моделей. Исследователи провели мета-анализ более 100 работ и собственные эксперименты на 20 наборах данных. Результаты показывают, что CoT наиболее эффективен для задач, связанных с математикой и логикой. Авторы предлагают селективное применение CoT и разработку новых подходов для промежуточных вычислений в различных приложениях языковых моделей.'}, 'en': {'title': "Unlocking the Power of Thought: CoT's Role in Symbolic Reasoning", 'desc': 'This paper investigates the effectiveness of Chain-of-Thought (CoT) prompting in large language models (LLMs) for various tasks. Through a meta-analysis of over 100 studies and evaluations on 20 datasets, the authors find that CoT significantly enhances performance mainly in math and logic tasks, while showing limited benefits for other tasks. They also discover that generating answers directly without CoT yields similar accuracy, except for questions involving symbolic reasoning. The study suggests that CoT should be used selectively to optimize performance and reduce computational costs, and advocates for exploring new methods beyond traditional CoT prompting.'}, 'zh': {'title': '思维链：提升逻辑与数学任务的关键', 'desc': '本文探讨了通过提示引导的思维链（CoT）在大型语言模型（LLMs）中的应用效果。我们对100多篇使用CoT的论文进行了定量元分析，并在14个模型上评估了20个数据集。结果表明，CoT在数学或逻辑任务中表现出显著的性能提升，而在其他类型任务中的提升则较小。研究还发现，CoT在符号执行方面的改进是其性能提升的主要原因，但相较于使用符号求解器，其表现仍然不足。'}}}, {'id': 'https://huggingface.co/papers/2409.11901', 'title': 'LLMs + Persona-Plug = Personalized LLMs', 'url': 'https://huggingface.co/papers/2409.11901', 'abstract': "Personalization plays a critical role in numerous language tasks and applications, since users with the same requirements may prefer diverse outputs based on their individual interests. This has led to the development of various personalized approaches aimed at adapting large language models (LLMs) to generate customized outputs aligned with user preferences. Some of them involve fine-tuning a unique personalized LLM for each user, which is too expensive for widespread application. Alternative approaches introduce personalization information in a plug-and-play manner by retrieving the user's relevant historical texts as demonstrations. However, this retrieval-based strategy may break the continuity of the user history and fail to capture the user's overall styles and patterns, hence leading to sub-optimal performance. To address these challenges, we propose a novel personalized LLM model, . It constructs a user-specific embedding for each individual by modeling all her historical contexts through a lightweight plug-in user embedder module. By attaching this embedding to the task input, LLMs can better understand and capture user habits and preferences, thereby producing more personalized outputs without tuning their own parameters. Extensive experiments on various tasks in the language model personalization (LaMP) benchmark demonstrate that the proposed model significantly outperforms existing personalized LLM approaches.", 'score': 30, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': 'd24ec09831b2ffd9', 'data': {'categories': ['#personalization', '#training', '#alignment', '#benchmark', '#architecture'], 'emoji': '👤', 'ru': {'title': 'PersoNULL: Персонализация языковых моделей без тонкой настройки', 'desc': 'Эта статья представляет новую модель персонализированного большого языкового моделирования (LLM) под названием PersoNULL. Модель создает уникальное пользовательское встраивание (embedding) для каждого пользователя, анализируя все его исторические контексты через легкий подключаемый модуль. Это встраивание добавляется к входным данным задачи, позволяя LLM лучше понимать и учитывать привычки и предпочтения пользователя без настройки собственных параметров. Эксперименты показывают, что PersoNULL значительно превосходит существующие подходы к персонализации LLM.'}, 'en': {'title': 'Personalized Language Models Made Easy!', 'desc': 'This paper discusses the importance of personalization in language tasks, highlighting that users with similar needs may still desire different outputs based on their unique preferences. It critiques existing methods that either require expensive fine-tuning of large language models (LLMs) or rely on retrieval-based strategies that can disrupt user history. The authors propose a new model that creates a user-specific embedding by analyzing all historical contexts through a lightweight module, allowing LLMs to better capture individual user styles. Experimental results show that this approach significantly improves performance in generating personalized outputs compared to previous methods.'}, 'zh': {'title': '个性化语言模型的新突破', 'desc': '个性化在许多语言任务和应用中起着关键作用，因为具有相同需求的用户可能会根据个人兴趣偏好不同的输出。本文提出了一种新颖的个性化大型语言模型（LLM），通过轻量级的用户嵌入模块为每个用户构建特定的嵌入，能够更好地理解和捕捉用户的习惯和偏好。与传统的个性化方法不同，该模型无需调整自身参数即可生成更个性化的输出。实验结果表明，该模型在语言模型个性化基准测试中显著优于现有的个性化LLM方法。'}}}, {'id': 'https://huggingface.co/papers/2409.11564', 'title': 'Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey', 'url': 'https://huggingface.co/papers/2409.11564', 'abstract': 'Preference tuning is a crucial process for aligning deep generative models with human preferences. This survey offers a thorough overview of recent advancements in preference tuning and the integration of human feedback. The paper is organized into three main sections: 1) introduction and preliminaries: an introduction to reinforcement learning frameworks, preference tuning tasks, models, and datasets across various modalities: language, speech, and vision, as well as different policy approaches, 2) in-depth examination of each preference tuning approach: a detailed analysis of the methods used in preference tuning, and 3) applications, discussion, and future directions: an exploration of the applications of preference tuning in downstream tasks, including evaluation methods for different modalities, and an outlook on future research directions. Our objective is to present the latest methodologies in preference tuning and model alignment, enhancing the understanding of this field for researchers and practitioners. We hope to encourage further engagement and innovation in this area.', 'score': 19, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': '6b85a58b33baead9', 'data': {'categories': ['#survey', '#training', '#rl', '#alignment', '#rlhf', '#multimodal'], 'emoji': '🎛️', 'ru': {'title': 'Настройка предпочтений: ключ к человекоориентированным ИИ-моделям', 'desc': 'Это обзор последних достижений в настройке предпочтений (preference tuning) для генеративных моделей. Статья охватывает основы обучения с подкреплением, задачи настройки предпочтений и различные модальности данных. Авторы подробно анализируют методы настройки предпочтений и их применение в различных задачах. Цель работы - улучшить понимание этой области и стимулировать дальнейшие исследования.'}, 'en': {'title': 'Aligning Models with Human Preferences: A Survey on Preference Tuning', 'desc': 'This paper surveys the advancements in preference tuning, which is essential for aligning deep generative models with human preferences. It covers reinforcement learning frameworks, various preference tuning tasks, and the models and datasets used across language, speech, and vision. The paper provides a detailed analysis of different preference tuning methods and discusses their applications in real-world tasks. Finally, it outlines future research directions to foster innovation in the field of preference tuning and model alignment.'}, 'zh': {'title': '偏好调优：对齐模型与人类需求的关键', 'desc': '偏好调优是将深度生成模型与人类偏好对齐的重要过程。本文综述了偏好调优和人类反馈整合的最新进展，分为三个主要部分：首先介绍强化学习框架、偏好调优任务、模型和数据集；其次深入分析各种偏好调优方法；最后探讨偏好调优在下游任务中的应用及未来研究方向。我们的目标是展示偏好调优和模型对齐的最新方法，促进研究者和从业者对该领域的理解和创新。'}}}, {'id': 'https://huggingface.co/papers/2409.12136', 'title': 'GRIN: GRadient-INformed MoE', 'url': 'https://huggingface.co/papers/2409.12136', 'abstract': 'Mixture-of-Experts (MoE) models scale more effectively than dense models due to sparse computation through expert routing, selectively activating only a small subset of expert modules. However, sparse computation challenges traditional training practices, as discrete expert routing hinders standard backpropagation and thus gradient-based optimization, which are the cornerstone of deep learning. To better pursue the scaling power of MoE, we introduce GRIN (GRadient-INformed MoE training), which incorporates sparse gradient estimation for expert routing and configures model parallelism to avoid token dropping. Applying GRIN to autoregressive language modeling, we develop a top-2 16times3.8B MoE model. Our model, with only 6.6B activated parameters, outperforms a 7B dense model and matches the performance of a 14B dense model trained on the same data. Extensive evaluations across diverse tasks demonstrate the potential of GRIN to significantly enhance MoE efficacy, achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.', 'score': 14, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': 'bb28b0c9d4b617d5', 'data': {'categories': ['#reasoning', '#training', '#math', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'GRIN: Эффективное обучение разреженных MoE моделей с помощью градиентной оценки', 'desc': 'Статья представляет новый метод обучения моделей Mixture-of-Experts (MoE) под названием GRIN. GRIN использует оценку разреженных градиентов для маршрутизации экспертов и оптимизирует параллелизм модели для избежания отбрасывания токенов. Авторы применили GRIN к авторегрессионной языковой модели и разработали MoE модель с 6.6 миллиардами активированных параметров, превосходящую плотную модель с 7 миллиардами параметров. Результаты оценки на различных задачах демонстрируют потенциал GRIN для значительного повышения эффективности MoE моделей.'}, 'en': {'title': 'Unlocking the Power of Mixture-of-Experts with GRIN', 'desc': 'This paper presents a new approach called GRIN for training Mixture-of-Experts (MoE) models, which are designed to scale better than traditional dense models by using sparse computation. The challenge with MoE is that the discrete routing of experts complicates the standard backpropagation process, which is essential for optimizing deep learning models. GRIN addresses this issue by using sparse gradient estimation to improve expert routing and implementing model parallelism to prevent token loss. The results show that the proposed MoE model, with fewer activated parameters, outperforms a larger dense model and achieves competitive performance on various language tasks.'}, 'zh': {'title': 'GRIN：提升混合专家模型效率的创新训练方法', 'desc': '混合专家模型（MoE）通过稀疏计算和专家路由实现了比密集模型更有效的扩展。由于离散的专家路由会阻碍标准的反向传播，传统的训练方法面临挑战。为了解决这个问题，我们提出了GRIN（基于梯度的信息的MoE训练），它通过稀疏梯度估计来优化专家路由，并配置模型并行以避免丢失标记。通过在自回归语言建模中应用GRIN，我们开发了一个性能优于7B密集模型的MoE模型，展示了GRIN在提升MoE效率方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2409.12139', 'title': 'Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models', 'url': 'https://huggingface.co/papers/2409.12139', 'abstract': 'With the advent of the big data and large language model era, zero-shot personalized rapid customization has emerged as a significant trend. In this report, we introduce Takin AudioLLM, a series of techniques and models, mainly including Takin TTS, Takin VC, and Takin Morphing, specifically designed for audiobook production. These models are capable of zero-shot speech production, generating high-quality speech that is nearly indistinguishable from real human speech and facilitating individuals to customize the speech content according to their own needs. Specifically, we first introduce Takin TTS, a neural codec language model that builds upon an enhanced neural speech codec and a multi-task training framework, capable of generating high-fidelity natural speech in a zero-shot way. For Takin VC, we advocate an effective content and timbre joint modeling approach to improve the speaker similarity, while advocating for a conditional flow matching based decoder to further enhance its naturalness and expressiveness. Last, we propose the Takin Morphing system with highly decoupled and advanced timbre and prosody modeling approaches, which enables individuals to customize speech production with their preferred timbre and prosody in a precise and controllable manner. Extensive experiments validate the effectiveness and robustness of our Takin AudioLLM series models. For detailed demos, please refer to https://takinaudiollm.github.io.', 'score': 11, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '11d685ad0e258a9d', 'data': {'categories': ['#audio', '#training', '#optimization', '#transfer_learning', '#architecture', '#synthetic'], 'emoji': '🎙️', 'ru': {'title': 'Революция в создании аудиокниг: персонализированный синтез речи с помощью ИИ', 'desc': 'В статье представлена серия моделей Takin AudioLLM для создания аудиокниг с использованием технологий обработки естественного языка и синтеза речи. Модели включают Takin TTS для генерации высококачественной речи, Takin VC для улучшения сходства голоса с оригинальным диктором, и Takin Morphing для настройки тембра и просодии. Эти модели позволяют создавать речь, практически неотличимую от человеческой, в режиме zero-shot. Эксперименты подтверждают эффективность и надежность предложенных моделей в серии Takin AudioLLM.'}, 'en': {'title': 'Revolutionizing Audiobook Production with Zero-Shot Customization', 'desc': 'The paper presents Takin AudioLLM, a set of advanced models for audiobook production that enable zero-shot personalized speech generation. It includes Takin TTS, which uses a neural codec language model to produce high-quality, natural-sounding speech without prior training on specific data. Takin VC enhances speaker similarity through a joint modeling approach, while Takin Morphing allows users to customize speech characteristics like timbre and prosody. The effectiveness of these models is demonstrated through extensive experiments, showcasing their ability to generate human-like speech tailored to individual preferences.'}, 'zh': {'title': '零-shot个性化语音定制的未来', 'desc': '随着大数据和大型语言模型时代的到来，零-shot个性化快速定制成为一个重要趋势。本文介绍了Takin AudioLLM系列技术和模型，主要包括Takin TTS、Takin VC和Takin Morphing，专为有声书制作而设计。这些模型能够实现零-shot语音生成，生成的高质量语音几乎与真实人声无异，方便用户根据自身需求定制语音内容。通过大量实验验证了Takin AudioLLM系列模型的有效性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2409.08425', 'title': 'SoloAudio: Target Sound Extraction with Language-oriented Audio Diffusion Transformer', 'url': 'https://huggingface.co/papers/2409.08425', 'abstract': 'In this paper, we introduce SoloAudio, a novel diffusion-based generative model for target sound extraction (TSE). Our approach trains latent diffusion models on audio, replacing the previous U-Net backbone with a skip-connected Transformer that operates on latent features. SoloAudio supports both audio-oriented and language-oriented TSE by utilizing a CLAP model as the feature extractor for target sounds. Furthermore, SoloAudio leverages synthetic audio generated by state-of-the-art text-to-audio models for training, demonstrating strong generalization to out-of-domain data and unseen sound events. We evaluate this approach on the FSD Kaggle 2018 mixture dataset and real data from AudioSet, where SoloAudio achieves the state-of-the-art results on both in-domain and out-of-domain data, and exhibits impressive zero-shot and few-shot capabilities. Source code and demos are released.', 'score': 9, 'issue_id': 1, 'pub_date': '2024-09-12', 'pub_date_card': {'ru': '12 сентября', 'en': 'September 12', 'zh': '9月12日'}, 'hash': '85f96fee2e333d85', 'data': {'categories': ['#audio', '#dataset', '#multilingual', '#training', '#open_source', '#diffusion', '#architecture', '#synthetic'], 'emoji': '🎵', 'ru': {'title': 'Извлечение целевых звуков с помощью диффузионной модели нового поколения', 'desc': 'SoloAudio - это новая генеративная модель на основе диффузии для извлечения целевых звуков. Она использует латентную диффузионную модель с трансформером вместо U-Net и модель CLAP для извлечения признаков. SoloAudio обучается на синтетических аудиоданных и демонстрирует сильную обобщающую способность. Модель достигает лучших результатов на наборах данных FSD Kaggle 2018 и AudioSet, показывая впечатляющие возможности в режимах zero-shot и few-shot.'}, 'en': {'title': 'SoloAudio: Revolutionizing Target Sound Extraction with Diffusion Models', 'desc': 'This paper presents SoloAudio, a new generative model that uses diffusion techniques for extracting specific sounds from audio. It innovatively replaces the traditional U-Net architecture with a Transformer that processes latent audio features, enhancing performance. SoloAudio is versatile, supporting both audio and language-based sound extraction by employing a CLAP model for feature extraction. The model is trained on synthetic audio from advanced text-to-audio systems, achieving top results on various datasets and demonstrating strong abilities in zero-shot and few-shot scenarios.'}, 'zh': {'title': 'SoloAudio：目标声音提取的新突破', 'desc': '本文介绍了一种新颖的基于扩散的生成模型SoloAudio，用于目标声音提取（TSE）。我们的方法在音频上训练潜在扩散模型，使用跳跃连接的Transformer替代了之前的U-Net骨干网络。SoloAudio通过利用CLAP模型作为目标声音的特征提取器，支持音频导向和语言导向的TSE。此外，SoloAudio利用最先进的文本到音频模型生成的合成音频进行训练，在未见声音事件和领域外数据上表现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2409.12193', 'title': 'Vista3D: Unravel the 3D Darkside of a Single Image', 'url': 'https://huggingface.co/papers/2409.12193', 'abstract': 'We embark on the age-old quest: unveiling the hidden dimensions of objects from mere glimpses of their visible parts. To address this, we present Vista3D, a framework that realizes swift and consistent 3D generation within a mere 5 minutes. At the heart of Vista3D lies a two-phase approach: the coarse phase and the fine phase. In the coarse phase, we rapidly generate initial geometry with Gaussian Splatting from a single image. In the fine phase, we extract a Signed Distance Function (SDF) directly from learned Gaussian Splatting, optimizing it with a differentiable isosurface representation. Furthermore, it elevates the quality of generation by using a disentangled representation with two independent implicit functions to capture both visible and obscured aspects of objects. Additionally, it harmonizes gradients from 2D diffusion prior with 3D-aware diffusion priors by angular diffusion prior composition. Through extensive evaluation, we demonstrate that Vista3D effectively sustains a balance between the consistency and diversity of the generated 3D objects. Demos and code will be available at https://github.com/florinshen/Vista3D.', 'score': 9, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': 'e21a2ff70200771f', 'data': {'categories': ['#cv', '#graphs', '#open_source', '#diffusion', '#architecture', '#3d'], 'emoji': '🔍', 'ru': {'title': 'От 2D к 3D: быстрая и точная реконструкция объектов', 'desc': 'Vista3D - это фреймворк для быстрой и согласованной генерации 3D-объектов по одному изображению. Он использует двухфазный подход: грубую фазу с Gaussian Splatting и точную фазу с оптимизацией функции расстояния со знаком (SDF). Vista3D применяет разделенное представление для видимых и скрытых частей объектов, а также комбинирует 2D и 3D-адаптированные диффузионные приоры. Фреймворк эффективно балансирует между согласованностью и разнообразием генерируемых 3D-объектов.'}, 'en': {'title': 'Swift and Consistent 3D Generation with Vista3D', 'desc': 'Vista3D is a novel framework designed for rapid 3D object generation from limited visual input. It employs a two-phase approach, starting with a coarse phase that uses Gaussian Splatting to create initial geometry from a single image. The fine phase enhances this geometry by extracting a Signed Distance Function (SDF) and optimizing it through a differentiable isosurface representation. By utilizing a disentangled representation and harmonizing gradients from 2D and 3D diffusion priors, Vista3D achieves a remarkable balance between consistency and diversity in the generated 3D models.'}, 'zh': {'title': 'Vista3D：快速生成三维物体的创新框架', 'desc': '本文介绍了一种名为Vista3D的框架，旨在从物体的可见部分快速生成其隐藏的三维维度。该框架采用两阶段的方法：粗略阶段和精细阶段。在粗略阶段，Vista3D通过高斯点云从单张图像中快速生成初始几何形状；在精细阶段，则直接从学习到的高斯点云中提取带符号距离函数（SDF），并通过可微分的等值面表示进行优化。此外，Vista3D通过使用解耦表示和独立的隐式函数，提升了生成质量，能够捕捉物体的可见和被遮挡的部分。'}}}, {'id': 'https://huggingface.co/papers/2409.09401', 'title': 'Towards Diverse and Efficient Audio Captioning via Diffusion Models', 'url': 'https://huggingface.co/papers/2409.09401', 'abstract': 'We introduce Diffusion-based Audio Captioning (DAC), a non-autoregressive diffusion model tailored for diverse and efficient audio captioning. Although existing captioning models relying on language backbones have achieved remarkable success in various captioning tasks, their insufficient performance in terms of generation speed and diversity impede progress in audio understanding and multimedia applications. Our diffusion-based framework offers unique advantages stemming from its inherent stochasticity and holistic context modeling in captioning. Through rigorous evaluation, we demonstrate that DAC not only achieves SOTA performance levels compared to existing benchmarks in the caption quality, but also significantly outperforms them in terms of generation speed and diversity. The success of DAC illustrates that text generation can also be seamlessly integrated with audio and visual generation tasks using a diffusion backbone, paving the way for a unified, audio-related generative model across different modalities.', 'score': 6, 'issue_id': 1, 'pub_date': '2024-09-14', 'pub_date_card': {'ru': '14 сентября', 'en': 'September 14', 'zh': '9月14日'}, 'hash': 'a78b001ecd3e1a38', 'data': {'categories': ['#audio', '#benchmark', '#games', '#diffusion', '#architecture', '#multimodal'], 'emoji': '🎵', 'ru': {'title': 'DAC: Революция в генерации аудиоподписей с помощью диффузионных моделей', 'desc': 'Представлен новый метод генерации текстовых описаний аудио под названием DAC (Diffusion-based Audio Captioning). Это неавторегрессивная диффузионная модель, которая обеспечивает разнообразие и эффективность при создании аудиоподписей. DAC превосходит существующие методы по качеству генерации, скорости и разнообразию выходных данных. Успех DAC показывает возможность интеграции генерации текста с аудио и визуальными задачами с использованием диффузионной основы.'}, 'en': {'title': 'Revolutionizing Audio Captioning with Diffusion Models', 'desc': 'The paper presents Diffusion-based Audio Captioning (DAC), a new model designed for creating captions for audio content. Unlike traditional models that rely heavily on language processing, DAC uses a diffusion approach that enhances both the speed and variety of generated captions. This model excels in generating high-quality captions while also being faster and more diverse than existing methods. The findings suggest that DAC can effectively combine text generation with audio and visual tasks, promoting a more integrated approach to multimedia understanding.'}, 'zh': {'title': '基于扩散的音频描述：速度与多样性的突破', 'desc': '我们介绍了一种基于扩散的音频描述模型（DAC），它是一种非自回归的扩散模型，专门用于高效多样的音频描述。现有的描述模型虽然在各种任务中取得了显著成功，但在生成速度和多样性方面的不足限制了音频理解和多媒体应用的进展。我们的扩散框架通过固有的随机性和整体上下文建模，提供了独特的优势。通过严格的评估，我们证明DAC在描述质量上达到了当前最先进的性能，并在生成速度和多样性方面显著优于现有基准。'}}}, {'id': 'https://huggingface.co/papers/2409.11074', 'title': 'RoMath: A Mathematical Reasoning Benchmark in Romanian', 'url': 'https://huggingface.co/papers/2409.11074', 'abstract': 'Mathematics has long been conveyed through natural language, primarily for human understanding. With the rise of mechanized mathematics and proof assistants, there is a growing need to understand informal mathematical text, yet most existing benchmarks focus solely on English, overlooking other languages. This paper introduces RoMath, a Romanian mathematical reasoning benchmark suite comprising three datasets: RoMath-Baccalaureate, RoMath-Competitions and RoMath-Synthetic, which cover a range of mathematical domains and difficulty levels, aiming to improve non-English language models and promote multilingual AI development. By focusing on Romanian, a low-resource language with unique linguistic features, RoMath addresses the limitations of Anglo-centric models and emphasizes the need for dedicated resources beyond simple automatic translation. We benchmark several open-weight language models, highlighting the importance of creating resources for underrepresented languages. We make the code and dataset available.', 'score': 3, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': '4dd29be6c679fb86', 'data': {'categories': ['#reasoning', '#dataset', '#multilingual', '#math', '#benchmark', '#open_source', '#low_resource'], 'emoji': '🇷🇴', 'ru': {'title': 'RoMath: преодоление языкового барьера в математическом ИИ', 'desc': 'Статья представляет RoMath - набор данных для оценки математического мышления на румынском языке. Он включает три датасета разной сложности и охватывает различные области математики. RoMath направлен на улучшение языковых моделей для неанглийских языков и развитие многоязычного ИИ. Авторы провели бенчмаркинг нескольких языковых моделей с открытыми весами, подчеркивая важность создания ресурсов для малоресурсных языков.'}, 'en': {'title': 'Empowering Romanian Mathematics: RoMath for Multilingual AI', 'desc': 'This paper presents RoMath, a benchmark suite designed to enhance mathematical reasoning in Romanian, a low-resource language. It includes three datasets that cover various mathematical topics and difficulty levels, aiming to support the development of multilingual AI models. The study highlights the limitations of existing benchmarks that primarily focus on English, advocating for resources that cater to underrepresented languages. By evaluating open-weight language models on these datasets, the paper underscores the importance of creating dedicated tools for non-English mathematical understanding.'}, 'zh': {'title': '推动多语言数学推理的革命', 'desc': '本文介绍了RoMath，这是一个罗马尼亚数学推理基准套件，包含三个数据集：RoMath-Baccalaureate、RoMath-Competitions和RoMath-Synthetic。这些数据集涵盖了多种数学领域和难度级别，旨在提升非英语语言模型的性能，促进多语言人工智能的发展。通过关注罗马尼亚语这一低资源语言，RoMath解决了以英语为中心模型的局限性，并强调了超越简单自动翻译的专用资源的必要性。我们对多个开放权重语言模型进行了基准测试，突出了为代表性不足语言创建资源的重要性。'}}}, {'id': 'https://huggingface.co/papers/2409.12001', 'title': 'Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning', 'url': 'https://huggingface.co/papers/2409.12001', 'abstract': 'Offline multi-agent reinforcement learning (MARL) is an exciting direction of research that uses static datasets to find optimal control policies for multi-agent systems. Though the field is by definition data-driven, efforts have thus far neglected data in their drive to achieve state-of-the-art results. We first substantiate this claim by surveying the literature, showing how the majority of works generate their own datasets without consistent methodology and provide sparse information about the characteristics of these datasets. We then show why neglecting the nature of the data is problematic, through salient examples of how tightly algorithmic performance is coupled to the dataset used, necessitating a common foundation for experiments in the field. In response, we take a big step towards improving data usage and data awareness in offline MARL, with three key contributions: (1) a clear guideline for generating novel datasets; (2) a standardisation of over 80 existing datasets, hosted in a publicly available repository, using a consistent storage format and easy-to-use API; and (3) a suite of analysis tools that allow us to understand these datasets better, aiding further development.', 'score': 3, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '2ff547dffde5361c', 'data': {'categories': ['#survey', '#dataset', '#rl', '#data', '#agents', '#benchmark', '#games', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Данные - ключ к прогрессу в оффлайн мультиагентном обучении с подкреплением', 'desc': 'Статья посвящена оффлайн-обучению с подкреплением для мультиагентных систем (MARL) на основе статических датасетов. Авторы отмечают недостаточное внимание к данным в существующих исследованиях и показывают, как характеристики датасетов влияют на производительность алгоритмов. Они предлагают руководство по созданию новых датасетов, стандартизируют более 80 существующих наборов данных и разрабатывают инструменты для их анализа. Эта работа направлена на улучшение использования данных и повышение осведомленности о них в области оффлайн MARL.'}, 'en': {'title': 'Enhancing Data Awareness in Offline Multi-Agent Reinforcement Learning', 'desc': 'This paper addresses the challenges in offline multi-agent reinforcement learning (MARL) by highlighting the importance of data quality and consistency. It critiques the current practices where many studies create their own datasets without a standardized approach, leading to unclear results. The authors propose a framework that includes guidelines for dataset generation, a standardized repository for existing datasets, and tools for dataset analysis. These contributions aim to enhance data awareness and improve the overall performance of algorithms in offline MARL.'}, 'zh': {'title': '提升离线MARL的数据使用与意识', 'desc': '离线多智能体强化学习（MARL）是一个利用静态数据集寻找多智能体系统最优控制策略的研究方向。尽管该领域以数据驱动为特征，但目前的研究往往忽视了数据的重要性，导致算法性能与数据集之间的关系不明确。我们通过文献调查证明了这一点，并指出大多数研究生成的数据集缺乏一致的方法论。为此，我们提出了三项关键贡献，以改善离线MARL中的数据使用和数据意识，包括生成新数据集的明确指南、对80多个现有数据集的标准化以及一套分析工具。'}}}, {'id': 'https://huggingface.co/papers/2409.11363', 'title': 'CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark', 'url': 'https://huggingface.co/papers/2409.11363', 'abstract': 'AI agents have the potential to aid users on a variety of consequential tasks, including conducting scientific research. To spur the development of useful agents, we need benchmarks that are challenging, but more crucially, directly correspond to real-world tasks of interest. This paper introduces such a benchmark, designed to measure the accuracy of AI agents in tackling a crucial yet surprisingly challenging aspect of scientific research: computational reproducibility. This task, fundamental to the scientific process, involves reproducing the results of a study using the provided code and data. We introduce CORE-Bench (Computational Reproducibility Agent Benchmark), a benchmark consisting of 270 tasks based on 90 scientific papers across three disciplines (computer science, social science, and medicine). Tasks in CORE-Bench consist of three difficulty levels and include both language-only and vision-language tasks. We provide an evaluation system to measure the accuracy of agents in a fast and parallelizable way, saving days of evaluation time for each run compared to a sequential implementation. We evaluated two baseline agents: the general-purpose AutoGPT and a task-specific agent called CORE-Agent. We tested both variants using two underlying language models: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21% on the hardest task, showing the vast scope for improvement in automating routine scientific tasks. Having agents that can reproduce existing work is a necessary step towards building agents that can conduct novel research and could verify and improve the performance of other research agents. We hope that CORE-Bench can improve the state of reproducibility and spur the development of future research agents.', 'score': 2, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': '83eb0c2355b12707', 'data': {'categories': ['#science', '#cv', '#healthcare', '#agents', '#benchmark', '#small_models', '#multimodal'], 'emoji': '🧪', 'ru': {'title': 'CORE-Bench: новый стандарт для оценки ИИ-агентов в воспроизведении научных результатов', 'desc': 'Статья представляет новый бенчмарк CORE-Bench для оценки точности ИИ-агентов в задаче вычислительной воспроизводимости научных исследований. Бенчмарк включает 270 заданий на основе 90 научных статей из трех дисциплин с тремя уровнями сложности. Авторы оценили два базовых агента: AutoGPT и специализированный CORE-Agent, используя языковые модели GPT-4o и GPT-4o-mini. Лучший агент достиг точности 21% на самом сложном задании, что показывает большой потенциал для улучшения автоматизации рутинных научных задач.'}, 'en': {'title': 'CORE-Bench: Advancing AI in Scientific Reproducibility', 'desc': 'This paper presents CORE-Bench, a benchmark designed to evaluate AI agents on their ability to achieve computational reproducibility in scientific research. It consists of 270 tasks derived from 90 scientific papers across three fields: computer science, social science, and medicine, with varying levels of difficulty. The benchmark allows for efficient evaluation of agents, significantly reducing the time required for testing. Results from baseline agents indicate that while current performance is low, there is substantial potential for improvement in automating scientific tasks.'}, 'zh': {'title': '提升科学研究的可重复性', 'desc': '本文介绍了CORE-Bench（计算可重复性代理基准），这是一个旨在评估人工智能代理在科学研究中可重复性任务表现的基准。该基准包含270个任务，基于90篇科学论文，涵盖计算机科学、社会科学和医学三个领域。任务分为三种难度级别，包括仅语言和视觉-语言任务。通过快速且可并行的评估系统，我们能够显著节省评估时间，并为未来的研究代理的发展提供支持。'}}}, {'id': 'https://huggingface.co/papers/2409.11315', 'title': 'fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction', 'url': 'https://huggingface.co/papers/2409.11315', 'abstract': "Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI) data, introduced as Recon3DMind in our conference work, is of significant interest to both cognitive neuroscience and computer vision. To advance this task, we present the fMRI-3D dataset, which includes data from 15 participants and showcases a total of 4768 3D objects. The dataset comprises two components: fMRI-Shape, previously introduced and accessible at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape, and fMRI-Objaverse, proposed in this paper and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse includes data from 5 subjects, 4 of whom are also part of the Core set in fMRI-Shape, with each subject viewing 3142 3D objects across 117 categories, all accompanied by text captions. This significantly enhances the diversity and potential applications of the dataset. Additionally, we propose MinD-3D, a novel framework designed to decode 3D visual information from fMRI signals. The framework first extracts and aggregates features from fMRI data using a neuro-fusion encoder, then employs a feature-bridge diffusion model to generate visual features, and finally reconstructs the 3D object using a generative transformer decoder. We establish new benchmarks by designing metrics at both semantic and structural levels to evaluate model performance. Furthermore, we assess our model's effectiveness in an Out-of-Distribution setting and analyze the attribution of the extracted features and the visual ROIs in fMRI signals. Our experiments demonstrate that MinD-3D not only reconstructs 3D objects with high semantic and spatial accuracy but also deepens our understanding of how human brain processes 3D visual information. Project page at: https://jianxgao.github.io/MinD-3D.", 'score': 2, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': '75e76c540084b86e', 'data': {'categories': ['#science', '#dataset', '#cv', '#healthcare', '#graphs', '#benchmark', '#diffusion', '#architecture', '#3d'], 'emoji': '🧠', 'ru': {'title': 'Расшифровка 3D-мыслей: от фМРТ к объектам', 'desc': 'Исследователи представили набор данных fMRI-3D, включающий фМРТ-данные 15 участников и 4768 3D-объектов. Они также предложили новую модель MinD-3D для декодирования 3D визуальной информации из сигналов фМРТ. Модель использует нейро-фузионный энкодер, диффузионную модель и генеративный трансформерный декодер для реконструкции 3D-объектов. Эксперименты показали высокую семантическую и пространственную точность реконструкции, а также углубили понимание обработки 3D визуальной информации мозгом.'}, 'en': {'title': 'Reconstructing 3D Visuals from Brain Signals with MinD-3D', 'desc': 'This paper introduces a method called MinD-3D for reconstructing 3D visuals from fMRI data, which is crucial for understanding brain activity related to visual processing. The authors present a new dataset, fMRI-3D, containing data from 15 participants and 4768 3D objects, enhancing the diversity of training data for machine learning models. The MinD-3D framework utilizes a neuro-fusion encoder to extract features from fMRI signals, followed by a diffusion model and a generative transformer decoder to create accurate 3D object reconstructions. The study establishes new evaluation metrics and demonstrates that MinD-3D achieves high accuracy in both semantic and structural aspects, contributing to cognitive neuroscience and computer vision fields.'}, 'zh': {'title': '从fMRI数据重建3D视觉的创新探索', 'desc': '本研究提出了一种名为Recon3DMind的方法，用于从功能性磁共振成像（fMRI）数据中重建3D视觉信息。我们创建了fMRI-3D数据集，包含15名参与者的4768个3D对象，数据集分为fMRI-Shape和fMRI-Objaverse两个部分。我们还提出了MinD-3D框架，通过神经融合编码器提取fMRI特征，并利用扩散模型生成视觉特征，最终重建3D对象。实验结果表明，MinD-3D在语义和空间准确性方面表现优异，增强了我们对人脑处理3D视觉信息的理解。'}}}, {'id': 'https://huggingface.co/papers/2409.12134', 'title': 'BERT-VBD: Vietnamese Multi-Document Summarization Framework', 'url': 'https://huggingface.co/papers/2409.12134', 'abstract': 'In tackling the challenge of Multi-Document Summarization (MDS), numerous methods have been proposed, spanning both extractive and abstractive summarization techniques. However, each approach has its own limitations, making it less effective to rely solely on either one. An emerging and promising strategy involves a synergistic fusion of extractive and abstractive summarization methods. Despite the plethora of studies in this domain, research on the combined methodology remains scarce, particularly in the context of Vietnamese language processing. This paper presents a novel Vietnamese MDS framework leveraging a two-component pipeline architecture that integrates extractive and abstractive techniques. The first component employs an extractive approach to identify key sentences within each document. This is achieved by a modification of the pre-trained BERT network, which derives semantically meaningful phrase embeddings using siamese and triplet network structures. The second component utilizes the VBD-LLaMA2-7B-50b model for abstractive summarization, ultimately generating the final summary document. Our proposed framework demonstrates a positive performance, attaining ROUGE-2 scores of 39.6% on the VN-MDS dataset and outperforming the state-of-the-art baselines.', 'score': 1, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': '596ba994e9995eba', 'data': {'categories': ['#dataset', '#cv', '#multilingual', '#transfer_learning', '#architecture', '#low_resource'], 'emoji': '🇻🇳', 'ru': {'title': 'Гибридный подход к многодокументному реферированию на вьетнамском языке', 'desc': 'Статья представляет новый подход к многодокументному реферированию на вьетнамском языке. Авторы предлагают гибридный метод, сочетающий экстрактивные и абстрактивные техники суммаризации. Первый компонент использует модифицированную BERT для выделения ключевых предложений, а второй применяет VBD-LLaMA2-7B-50b для генерации итогового реферата. Предложенный метод превзошел существующие подходы, достигнув показателя ROUGE-2 в 39.6% на датасете VN-MDS.'}, 'en': {'title': 'Fusing Extractive and Abstractive Techniques for Enhanced Vietnamese Summarization', 'desc': 'This paper addresses the challenge of Multi-Document Summarization (MDS) by proposing a new framework that combines both extractive and abstractive summarization techniques. The first part of the framework uses a modified BERT model to extract key sentences from documents, leveraging siamese and triplet networks for better semantic understanding. The second part employs the VBD-LLaMA2-7B-50b model to generate a coherent summary from the extracted information. The results show that this integrated approach achieves a ROUGE-2 score of 39.6% on the VN-MDS dataset, surpassing existing methods in the field.'}, 'zh': {'title': '融合提取与生成，提升多文档摘要效果', 'desc': '本文探讨了多文档摘要（MDS）的挑战，提出了一种结合提取式和生成式摘要的新框架。该框架采用双组件管道架构，首先通过修改预训练的BERT网络提取每个文档中的关键句子。接着，使用VBD-LLaMA2-7B-50b模型进行生成式摘要，最终生成摘要文档。实验结果表明，该框架在VN-MDS数据集上取得了39.6%的ROUGE-2分数，超越了现有的最先进基线。'}}}, {'id': 'https://huggingface.co/papers/2409.12106', 'title': 'Measuring Human and AI Values based on Generative Psychometrics with Large Language Models', 'url': 'https://huggingface.co/papers/2409.12106', 'abstract': 'Human values and their measurement are long-standing interdisciplinary inquiry. Recent advances in AI have sparked renewed interest in this area, with large language models (LLMs) emerging as both tools and subjects of value measurement. This work introduces Generative Psychometrics for Values (GPV), an LLM-based, data-driven value measurement paradigm, theoretically grounded in text-revealed selective perceptions. We begin by fine-tuning an LLM for accurate perception-level value measurement and verifying the capability of LLMs to parse texts into perceptions, forming the core of the GPV pipeline. Applying GPV to human-authored blogs, we demonstrate its stability, validity, and superiority over prior psychological tools. Then, extending GPV to LLM value measurement, we advance the current art with 1) a psychometric methodology that measures LLM values based on their scalable and free-form outputs, enabling context-specific measurement; 2) a comparative analysis of measurement paradigms, indicating response biases of prior methods; and 3) an attempt to bridge LLM values and their safety, revealing the predictive power of different value systems and the impacts of various values on LLM safety. Through interdisciplinary efforts, we aim to leverage AI for next-generation psychometrics and psychometrics for value-aligned AI.', 'score': 1, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': 'f795400e3345e3d7', 'data': {'categories': ['#multilingual', '#training', '#ethics', '#data', '#interpretability', '#alignment', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Измерение ценностей в эпоху искусственного интеллекта: новый подход с использованием больших языковых моделей', 'desc': 'Статья представляет новый подход под названием Generative Psychometrics for Values (GPV) для измерения ценностей с помощью больших языковых моделей (LLM). GPV основан на анализе текстовых данных и использует fine-tuned LLM для точного измерения ценностей на уровне восприятия. Метод был применен к блогам, написанным людьми, и показал преимущества перед традиционными психологическими инструментами. Авторы также расширили GPV для измерения ценностей самих LLM, что позволило провести сравнительный анализ различных подходов и исследовать связь между ценностями LLM и их безопасностью.'}, 'en': {'title': 'Harnessing AI to Measure Human Values with Precision', 'desc': 'This paper presents Generative Psychometrics for Values (GPV), a new method for measuring human values using large language models (LLMs). The authors fine-tune an LLM to accurately assess perceptions of values from text, establishing a robust pipeline for value measurement. They demonstrate that GPV outperforms traditional psychological tools when applied to human-written blogs, showing its reliability and validity. Additionally, the paper explores how LLM values can be measured in a context-sensitive manner and discusses the implications of these values for LLM safety.'}, 'zh': {'title': '利用AI推动价值测量的新纪元', 'desc': '本研究提出了一种基于大型语言模型（LLM）的价值测量新方法，称为生成心理测量学（GPV）。该方法通过对文本的选择性感知进行理论基础，旨在准确测量人类的价值观。我们通过微调LLM，验证其解析文本为感知的能力，并将其应用于人类撰写的博客中，展示了其稳定性和有效性。最后，我们扩展了GPV以测量LLM的价值，揭示了不同价值体系对LLM安全性的影响。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi', '#alignment (4)', '#architecture (12)', '#audio (3)', '#benchmark (9)', '#cv (5)', '#data (3)', '#dataset (6)', '#diffusion (4)', '#ethics (1)', '#games (2)', '#graphs (2)', '#hallucinations', '#healthcare (2)', '#inference (1)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource (2)', '#machine_translation', '#math (3)', '#multilingual (4)', '#multimodal (4)', '#open_source (7)', '#optimization (3)', '#plp (1)', '#rag', '#reasoning (4)', '#rl (2)', '#rlhf (1)', '#robotics', '#science (2)', '#security', '#small_models (3)', '#story_generation', '#survey (3)', '#synthetic (3)', '#training (9)', '#transfer_learning (2)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <img class="article-pdf-title-img" src="${pdfImg}" />
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-09-19 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-09-19 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-09-19 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    