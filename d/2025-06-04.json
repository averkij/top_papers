{
    "date": {
        "ru": "4 июня",
        "en": "June 4",
        "zh": "6月4日"
    },
    "time_utc": "2025-06-04 02:41",
    "weekday": 2,
    "issue_id": 4110,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.01674",
            "title": "MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs",
            "url": "https://huggingface.co/papers/2506.01674",
            "abstract": "MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advancements in Multimodal Large Language Models (MLLMs), their proficiency in fine-grained video motion understanding remains critically limited. They often lack inter-frame differencing and tend to average or ignore subtle visual cues. Furthermore, while visual prompting has shown potential in static images, its application to video's temporal complexities, particularly for fine-grained motion understanding, remains largely unexplored. We investigate whether inherent capability can be unlocked and boost MLLMs' motion perception and enable distinct visual signatures tailored to decouple object and camera motion cues. In this study, we introduce MotionSight, a novel zero-shot method pioneering object-centric visual spotlight and motion blur as visual prompts to effectively improve fine-grained motion understanding without training. To convert this into valuable data assets, we curated MotionVid-QA, the first large-scale dataset for fine-grained video motion understanding, with hierarchical annotations including SFT and preference data, {\\Theta}(40K) video clips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves state-of-the-art open-source performance and competitiveness with commercial models. In particular, for fine-grained motion understanding we present a novel zero-shot technique and a large-scale, high-quality dataset. All the code and annotations will be publicly available.",
            "score": 10,
            "issue_id": 4110,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 июня",
                "en": "June 2",
                "zh": "6月2日"
            },
            "hash": "baebffe54058ae77",
            "authors": [
                "Yipeng Du",
                "Tiehan Fan",
                "Kepan Nan",
                "Rui Xie",
                "Penghao Zhou",
                "Xiang Li",
                "Jian Yang",
                "Zhenheng Yang",
                "Ying Tai"
            ],
            "affiliations": [
                "ByteDance",
                "Nanjing University",
                "Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01674.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#multimodal",
                    "#open_source",
                    "#games"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Новый взгляд на движение: MotionSight улучшает понимание видео без обучения",
                    "desc": "MotionSight - это новый метод без предварительного обучения для улучшения понимания движения в видео мультимодальными большими языковыми моделями (MLLM). Он использует объектно-ориентированное визуальное выделение и размытие движения в качестве подсказок для моделей. Авторы также создали крупномасштабный датасет MotionVid-QA с иерархическими аннотациями для оценки понимания движения в видео. Эксперименты показывают, что MotionSight достигает лучших результатов среди открытых моделей и конкурентоспособен с коммерческими решениями."
                },
                "en": {
                    "title": "Unlocking Fine-Grained Motion Understanding with MotionSight",
                    "desc": "MotionSight is a novel zero-shot method designed to enhance fine-grained video motion understanding by utilizing object-centric visual prompts and motion blur. This approach addresses the limitations of Multimodal Large Language Models (MLLMs) in capturing subtle motion cues across video frames. By introducing a large-scale dataset called MotionVid-QA, which includes hierarchical annotations, the method allows for effective evaluation and improvement of motion perception without the need for extensive training. The results demonstrate that MotionSight achieves state-of-the-art performance, showcasing its potential to unlock advanced motion understanding capabilities in videos."
                },
                "zh": {
                    "title": "MotionSight：提升视频运动理解的新方法",
                    "desc": "MotionSight是一种零样本方法，利用以物体为中心的视觉聚焦和运动模糊作为提示，显著提升了细粒度视频运动理解的能力。该方法在MotionVid-QA数据集上取得了最先进的性能，该数据集具有层次化的注释。尽管多模态大型语言模型在视频运动理解方面取得了一些进展，但仍然存在显著的局限性，尤其是在处理细微的视觉线索时。通过引入MotionSight，我们展示了如何在不进行训练的情况下，利用视觉提示来解耦物体和相机运动线索，从而提高运动感知能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03136",
            "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.03136",
            "abstract": "CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE",
            "score": 9,
            "issue_id": 4110,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 июня",
                "en": "June 3",
                "zh": "6月3日"
            },
            "hash": "6a5362c4ed28a3b0",
            "authors": [
                "Yinjie Wang",
                "Ling Yang",
                "Ye Tian",
                "Ke Shen",
                "Mengdi Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Peking University",
                "Princeton University",
                "University of Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03136.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#agents",
                    "#rl",
                    "#games"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "CURE: эволюция кодирования и тестирования без эталонов",
                    "desc": "CURE - это новая система обучения с подкреплением для улучшения генерации кода и модульных тестов без использования эталонного кода. Модели ReasonFlux-Coder, созданные с помощью CURE, показывают значительное улучшение точности генерации кода по сравнению с аналогичными моделями. Система позволяет тестировщику учиться непосредственно на ошибках программиста и естественным образом распространяется на смежные задачи. Примечательно, что модель CURE может служить эффективной моделью вознаграждения для обучения с подкреплением базовых моделей."
                },
                "en": {
                    "title": "CURE: Evolving Code and Tests Together for Better Accuracy",
                    "desc": "CURE is a reinforcement learning framework designed to enhance the accuracy of code and unit test generation without relying on ground-truth supervision. It features a unique reward system that allows coding and testing processes to evolve together based on their interactions. This method enables the unit tester to learn from the coder's errors, leading to improved performance in various coding tasks. The framework's models, such as ReasonFlux-Coder, show significant accuracy improvements over existing models, making it a powerful tool for developers."
                },
                "zh": {
                    "title": "CURE：无监督强化学习提升代码生成与测试准确性",
                    "desc": "CURE是一个强化学习框架，旨在提高代码和单元测试生成的准确性，而无需真实标签的监督。该框架通过设计专门的奖励机制，使编码和单元测试生成能力能够相互演化，从而直接从编码者的错误中学习。经过优化后，我们的ReasonFlux-Coder模型在代码生成准确性上提高了5.3%，在最佳准确性上提高了9.0%。此外，该模型在单元测试生成中表现出64.8%的推理效率，展现了其在下游任务中的灵活性和可扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03147",
            "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation",
            "url": "https://huggingface.co/papers/2506.03147",
            "abstract": "A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.  \t\t\t\t\tAI-generated summary \t\t\t\t Although existing unified models deliver strong performance on vision-language understanding and text-to-image generation, their models are limited in exploring image perception and manipulation tasks, which are urgently desired by users for wide applications. Recently, OpenAI released their powerful GPT-4o-Image model for comprehensive image perception and manipulation, achieving expressive capability and attracting community interests. By observing the performance of GPT-4o-Image in our carefully constructed experiments, we infer that GPT-4o-Image leverages features extracted by semantic encoders instead of VAE, while VAEs are considered essential components in many image manipulation models. Motivated by such inspiring observations, we present a unified generative framework named UniWorld based on semantic features provided by powerful visual-language models and contrastive semantic encoders. As a result, we build a strong unified model using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on image editing benchmarks. UniWorld also maintains competitive image understanding and generation capabilities, achieving strong performance across multiple image perception tasks. We fully open-source our models, including model weights, training and evaluation scripts, and datasets.",
            "score": 7,
            "issue_id": 4110,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 июня",
                "en": "June 3",
                "zh": "6月3日"
            },
            "hash": "34f1d96b37be24a1",
            "authors": [
                "Bin Lin",
                "Zongjian Li",
                "Xinhua Cheng",
                "Yuwei Niu",
                "Yang Ye",
                "Xianyi He",
                "Shenghai Yuan",
                "Wangbo Yu",
                "Shaodong Wang",
                "Yunyang Ge",
                "Yatian Pang",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University, Shenzhen Graduate School",
                "Peng Cheng Laboratory",
                "Rabbitpre AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03147.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#multimodal",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "UniWorld: Мощная модель для работы с изображениями на основе семантических признаков",
                    "desc": "UniWorld - это унифицированная генеративная модель для восприятия и редактирования изображений. Она использует семантические признаки из визуально-языковых моделей вместо вариационных автоэнкодеров. UniWorld превосходит модель BAGEL по качеству редактирования изображений, используя всего 1% данных. Модель также демонстрирует высокие результаты в задачах понимания и генерации изображений."
                },
                "en": {
                    "title": "UniWorld: Efficient Image Manipulation with Semantic Power",
                    "desc": "UniWorld is a new generative framework that enhances image perception and manipulation by utilizing semantic features from visual-language models. It outperforms the existing model BAGEL while using only 1% of its data, demonstrating efficiency in data usage. The framework leverages insights from the GPT-4o-Image model, which effectively uses semantic encoders instead of traditional Variational Autoencoders (VAEs). UniWorld not only excels in image editing tasks but also maintains strong capabilities in image understanding and generation across various applications."
                },
                "zh": {
                    "title": "UniWorld：图像感知与操作的新纪元",
                    "desc": "UniWorld是一个统一的生成框架，利用视觉语言模型的语义特征来进行图像感知和操作。与BAGEL相比，UniWorld在数据使用上减少了90%，但在图像编辑基准测试中表现更优。该模型不仅在图像理解和生成方面保持竞争力，还在多个图像感知任务中取得了良好的效果。我们将模型权重、训练和评估脚本以及数据集全部开源，方便研究者使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02096",
            "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis",
            "url": "https://huggingface.co/papers/2506.02096",
            "abstract": "SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) trained via reinforcement learning with verifiable reward (RLVR) have shown notable progress in scaling test-time compute effectively. In this work, we investigate how synthesized RL data can further improve RLVR. To this end, we propose SynthRL-a scalable and guaranteed pipeline for automatic data scaling in reasoning-oriented RL training. SynthRL comprises three key stages: (1) selecting seed questions with appropriate distribution, (2) augmenting them into more challenging variants while preserving the original answers, and (3) a guaranteed verification stage that ensures near-perfect correctness and difficulty enhancement. Our empirical experiments demonstrate SynthRL's scalability and effectiveness. When applied to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable, challenging questions from approximately 8K seed samples. Models trained with our synthesized data achieve consistent gains across five out-of-domain visual math reasoning benchmarks, with a significant improvement over baseline models trained on seed data alone. Notably, detailed analysis reveals that the gains are more pronounced on the most challenging evaluation samples, highlighting SynthRL's effectiveness in eliciting deeper and more complex reasoning patterns.",
            "score": 5,
            "issue_id": 4110,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 июня",
                "en": "June 2",
                "zh": "6月2日"
            },
            "hash": "9ea84a7af081829e",
            "authors": [
                "Zijian Wu",
                "Jinjie Ni",
                "Xiangyan Liu",
                "Zichen Liu",
                "Hang Yan",
                "Michael Qizhe Shieh"
            ],
            "affiliations": [
                "National University of Singapore",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02096.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#training",
                    "#synthetic",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SynthRL: Синтез данных для улучшения математических рассуждений ИИ",
                    "desc": "SynthRL - это масштабируемый конвейер для синтеза данных в обучении с подкреплением с проверяемыми наградами. Он улучшает визуальные языковые модели для математических рассуждений, генерируя сложные, проверяемые вопросы. SynthRL включает три ключевых этапа: выбор исходных вопросов, их усложнение с сохранением ответов и верификацию для обеспечения корректности. Эксперименты показали, что модели, обученные на синтезированных данных, достигают стабильных улучшений на пяти тестовых наборах по визуальным математическим рассуждениям."
                },
                "en": {
                    "title": "SynthRL: Elevating Visual Math Reasoning with Scalable Data Synthesis",
                    "desc": "SynthRL is a novel pipeline designed to enhance reinforcement learning (RL) by synthesizing data with verifiable rewards, specifically for visual math reasoning tasks. It operates in three stages: selecting initial questions, creating more challenging variants while keeping the answers intact, and verifying the correctness of these questions. This approach allows for the generation of over 3,300 additional verifiable questions from a smaller set of seed samples, significantly improving model performance. Empirical results show that models trained with SynthRL's data outperform those trained only on seed data, especially on difficult reasoning tasks, demonstrating its effectiveness in fostering advanced reasoning capabilities."
                },
                "zh": {
                    "title": "SynthRL：提升视觉数学推理的智能合成管道",
                    "desc": "SynthRL是一种可扩展的数据合成管道，旨在增强强化学习中的可验证奖励（RLVR），特别是在视觉数学推理模型（VLMs）中。该方法通过生成具有挑战性和可验证的问题，来提高模型的推理能力。SynthRL包括三个关键阶段：选择合适分布的种子问题、将其增强为更具挑战性的变体，并确保答案的正确性和难度的提升。实验结果表明，使用SynthRL合成的数据在多个视觉数学推理基准测试中显著提高了模型的表现，尤其是在最具挑战性的样本上。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24714",
            "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation",
            "url": "https://huggingface.co/papers/2505.24714",
            "abstract": "FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at https://huggingface.co/datasets/luojunyu/FinMME and https://github.com/luo-junyu/FinMME.",
            "score": 5,
            "issue_id": 4110,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 мая",
                "en": "May 30",
                "zh": "5月30日"
            },
            "hash": "a6dcbb10b5be7f41",
            "authors": [
                "Junyu Luo",
                "Zhizhuo Kou",
                "Liming Yang",
                "Xiao Luo",
                "Jinsheng Huang",
                "Zhiping Xiao",
                "Jingshu Peng",
                "Chengzhong Liu",
                "Jiaming Ji",
                "Xuanzhe Liu",
                "Sirui Han",
                "Ming Zhang",
                "Yike Guo"
            ],
            "affiliations": [
                "HKUST",
                "School of Computer Science, Peking University",
                "State Key Laboratory for Multimedia Information Processing, PKU-Anker LLM Lab",
                "University of California, Los Angeles",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24714.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#science",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "FinMME и FinScore: Новый стандарт для оценки мультимодальных языковых моделей в финансовой сфере",
                    "desc": "FinMME - это обширный мультимодальный датасет для финансовых исследований, включающий более 11 000 высококачественных образцов из 18 финансовых областей. Датасет содержит различные типы графиков и диаграмм, а его качество обеспечивается тщательной аннотацией и валидацией. FinScore - это система оценки, учитывающая штрафы за галлюцинации и многомерную оценку возможностей моделей. Эксперименты показали, что даже передовые модели вроде GPT-4o демонстрируют неудовлетворительные результаты на FinMME, что подчеркивает сложность датасета."
                },
                "en": {
                    "title": "FinMME: Elevating Financial AI with Robust Evaluation",
                    "desc": "The paper introduces FinMME, a new multimodal dataset specifically designed for financial research, which includes over 11,000 samples across various financial domains and asset classes. It addresses the gap in specialized evaluation datasets for Multimodal Large Language Models (MLLMs) in finance. The authors also present FinScore, an evaluation system that assesses model performance while penalizing inaccuracies and measuring multiple capabilities. Experimental results reveal that even advanced models like GPT-4o struggle with the challenges posed by FinMME, underscoring the dataset's robustness and reliability."
                },
                "zh": {
                    "title": "推动金融领域的多模态研究",
                    "desc": "FinMME是一个全面的多模态金融研究数据集，旨在推动金融领域的多模态大语言模型（MLLMs）发展。该数据集包含超过11,000个高质量的金融研究样本，涵盖18个金融领域和6种资产类别，提供10种主要图表类型和21种子类型。为了确保数据质量，研究团队使用了20名注释员和精心设计的验证机制。此外，FinScore评估系统引入了幻觉惩罚和多维能力评估，以提供公正的评估结果，尽管先进模型如GPT-4o在FinMME上表现不佳，显示出其挑战性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02387",
            "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments",
            "url": "https://huggingface.co/papers/2506.02387",
            "abstract": "VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent tasks, yet existing benchmarks remain limited to single-agent or text-only environments. In contrast, real-world scenarios often involve multiple agents interacting within rich visual and linguistic contexts, posing challenges with both multimodal observations and strategic interactions. To bridge this gap, we introduce Visual Strategic Bench (VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning and decision-making in multi-agent environments. VS-Bench comprises eight vision-grounded environments spanning cooperative, competitive, and mixed-motive interactions, designed to assess agents' ability to predict others' future moves and optimize for long-term objectives. We consider two complementary evaluation dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode return. Extensive experiments of fourteen leading VLMs reveal a significant gap between current models and optimal performance, with the best models attaining 47.8% prediction accuracy and 24.3% normalized return. We further conduct in-depth analyses on multimodal observations, test-time scaling, social behaviors, and failure cases of VLM agents. By standardizing the evaluation and highlighting the limitations of existing models, we envision VS-Bench as a foundation for future research on strategic multimodal agents. Code and data are available at https://vs-bench.github.io.",
            "score": 3,
            "issue_id": 4110,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 июня",
                "en": "June 3",
                "zh": "6月3日"
            },
            "hash": "829f3546d3ac9345",
            "authors": [
                "Zelai Xu",
                "Zhexuan Xu",
                "Xiangmin Yi",
                "Huining Yuan",
                "Xinlei Chen",
                "Yi Wu",
                "Chao Yu",
                "Yu Wang"
            ],
            "affiliations": [
                "Beijing Zhongguancun Academy",
                "Shanghai Qi Zhi Institute",
                "Tsinghua University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02387.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#agents",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "VS-Bench: Новый рубеж в оценке стратегического мышления мультимодальных ИИ-агентов",
                    "desc": "VS-Bench - это мультимодальный бенчмарк для оценки стратегического мышления и принятия решений vision-language моделями в сложных мультиагентных средах. Он включает восемь визуальных сред с кооперативными, соревновательными и смешанными взаимодействиями. Бенчмарк оценивает способность моделей предсказывать будущие действия других агентов и оптимизировать долгосрочные цели. Эксперименты с 14 ведущими VLM показали значительный разрыв между текущими моделями и оптимальной производительностью."
                },
                "en": {
                    "title": "Evaluating Strategic Reasoning in Multi-Agent Environments with VS-Bench",
                    "desc": "VS-Bench is a new benchmark created to test Vision Language Models (VLMs) in complex situations where multiple agents interact. Unlike previous benchmarks that focused on single agents or text-only tasks, VS-Bench evaluates how well VLMs can reason and make decisions in environments that include both visual and language elements. It features eight different scenarios that require agents to work together, compete, or navigate mixed motives, assessing their ability to predict actions and achieve long-term goals. The results show that current VLMs still have a long way to go, with significant gaps in their performance, highlighting the need for further research in this area."
                },
                "zh": {
                    "title": "多模态智能体的战略推理新基准",
                    "desc": "VS-Bench是一个多模态基准，旨在评估视觉语言模型（VLM）在复杂多智能体环境中的战略推理和决策能力。与现有的单智能体或仅文本环境的基准不同，VS-Bench考虑了多个智能体在丰富的视觉和语言背景下的互动。该基准包括八个基于视觉的环境，涵盖合作、竞争和混合动机的互动，评估智能体预测他人未来动作和优化长期目标的能力。通过标准化评估，VS-Bench为未来的战略多模态智能体研究奠定了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02510",
            "title": "M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset",
            "url": "https://huggingface.co/papers/2506.02510",
            "abstract": "A new multilingual, multi-sector, and multi-task benchmark, M³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called M^3FinMeeting, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, M^3FinMeeting supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, M^3FinMeeting includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of M^3FinMeeting as a benchmark for assessing LLMs' financial meeting comprehension skills.",
            "score": 2,
            "issue_id": 4110,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 июня",
                "en": "June 3",
                "zh": "6月3日"
            },
            "hash": "903bb57b5cc664ec",
            "authors": [
                "Jie Zhu",
                "Junhui Li",
                "Yalong Wen",
                "Xiandong Li",
                "Lifan Guo",
                "Feng Chen"
            ],
            "affiliations": [
                "Nanjing University",
                "Qwen DianJin Team, Alibaba Cloud Computing",
                "School of Computer Science and Technology, Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02510.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#translation",
                    "#science",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "M³FinMeeting: многоязычный бенчмарк для оценки понимания финансовых встреч языковыми моделями",
                    "desc": "Статья представляет новый бенчмарк M³FinMeeting для оценки больших языковых моделей в понимании финансовых встреч на разных языках и в разных отраслях. Бенчмарк включает тексты на английском, китайском и японском языках, охватывая различные секторы экономики по классификации GICS. M³FinMeeting содержит три задачи: суммаризацию, извлечение пар вопрос-ответ и вопросно-ответную систему. Эксперименты с семью популярными языковыми моделями показали, что даже самые продвинутые модели имеют значительный потенциал для улучшения в этой области."
                },
                "en": {
                    "title": "M³FinMeeting: Bridging Language Gaps in Financial Comprehension",
                    "desc": "The M³FinMeeting benchmark is designed to evaluate large language models (LLMs) in understanding financial meetings across multiple languages and sectors. It addresses the limitations of existing benchmarks that primarily focus on static financial documents like news articles. This new dataset supports English, Chinese, and Japanese, allowing for a broader assessment of multilingual financial discussions. Additionally, it includes tasks such as summarization and question answering, highlighting the need for LLMs to improve their comprehension of dynamic financial interactions."
                },
                "zh": {
                    "title": "M³FinMeeting：金融会议理解的新基准",
                    "desc": "M³FinMeeting是一个新的多语言、多行业和多任务的基准，旨在评估大型语言模型在理解金融会议方面的表现。该基准支持英语、中文和日语，增强了对不同语言环境中金融讨论的理解。它涵盖了全球行业分类标准（GICS）定义的多个行业，确保基准能够覆盖广泛的金融活动。通过对七种流行的大型语言模型进行实验，结果显示即使是最先进的长上下文模型在理解能力上仍有很大提升空间，证明了M³FinMeeting作为评估大型语言模型金融会议理解能力的有效性。"
                }
            }
        }
    ],
    "link_prev": "2025-06-03.html",
    "link_next": "2025-06-05.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "03.06",
        "en": "06/03",
        "zh": "6月3日"
    },
    "short_date_next": {
        "ru": "05.06",
        "en": "06/05",
        "zh": "6月5日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0,
        "#translation": 1
    },
    "zh": {
        "text": "这篇文章探讨了增强大语言模型（LLM）推理能力的新方法，称为可验证奖励的强化学习（RLVR）。研究发现，高熵值的词对模型的推理性能和优化有显著影响。通过分析词的熵值模式，研究人员观察到只有少量词具有高熵值，这些词决定了模型的推理路径。进一步的训练表明，RLVR主要调整高熵值词的熵值。通过仅更新高熵值词的策略梯度，研究人员在不同模型上取得了显著的性能提升。",
        "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning",
        "pinyin": "Zhè piān wénzhāng tàntào le zēngqiáng dà yǔyán móxíng (LLM) tuīlǐ nénglì de xīn fāngfǎ, chēngwéi kě yànzhèng jiǎnglì de qiángzhù xuéxí (RLVR). Yánjiū fāxiàn, gāo shāngzhí de cí duì móxíng de tuīlǐ xìngnéng hé yōuhuà yǒu xiǎnzhù yǐngxiǎng. Tōngguò fēnxī cí de shāngzhí móshì, yánjiū rényuán guānchá dào zhǐyǒu shǎoliàng cí jùyǒu gāo shāngzhí, zhèxiē cí juédìngle móxíng de tuīlǐ lùjìng. Jìn yībù de xùnliàn biǎomíng, RLVR zhǔyào tiáozhěng gāo shāngzhí cí de shāngzhí. Tōngguò jǐn gēngxīn gāo shāngzhí cí de cèlüè tiāndù, yánjiū rényuán zài bùtóng móxíng shàng qǔdéle xiǎnzhù de xìngnéng tíshēng.",
        "vocab": "[\n    {\"word\": \"探讨\", \"pinyin\": \"tàn tǎo\", \"trans\": \"discuss\"},\n    {\"word\": \"增强\", \"pinyin\": \"zēng qiáng\", \"trans\": \"enhance\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"可验证\", \"pinyin\": \"kě yàn zhèng\", \"trans\": \"verifiable\"},\n    {\"word\": \"奖励\", \"pinyin\": \"jiǎng lì\", \"trans\": \"reward\"},\n    {\"word\": \"强化\", \"pinyin\": \"qiáng huà\", \"trans\": \"reinforce\"},\n    {\"word\": \"学习\", \"pinyin\": \"xué xí\", \"trans\": \"learning\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"发现\", \"pinyin\": \"fā xiàn\", \"trans\": \"discover\"},\n    {\"word\": \"熵值\", \"pinyin\": \"shāng zhí\", \"trans\": \"entropy value\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"影响\", \"pinyin\": \"yǐng xiǎng\", \"trans\": \"impact\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimization\"},\n    {\"word\": \"模式\", \"pinyin\": \"mó shì\", \"trans\": \"pattern\"},\n    {\"word\": \"观察\", \"pinyin\": \"guān chá\", \"trans\": \"observe\"},\n    {\"word\": \"少量\", \"pinyin\": \"shǎo liàng\", \"trans\": \"small amount\"},\n    {\"word\": \"决定\", \"pinyin\": \"jué dìng\", \"trans\": \"determine\"},\n    {\"word\": \"路径\", \"pinyin\": \"lù jìng\", \"trans\": \"path\"},\n    {\"word\": \"调整\", \"pinyin\": \"tiáo zhěng\", \"trans\": \"adjust\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"梯度\", \"pinyin\": \"tī dù\", \"trans\": \"gradient\"},\n    {\"word\": \"更新\", \"pinyin\": \"gèng xīn\", \"trans\": \"update\"},\n    {\"word\": \"取得\", \"pinyin\": \"qǔ dé\", \"trans\": \"achieve\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"}\n]",
        "trans": "This article discusses a new method for enhancing the reasoning capabilities of large language models (LLMs), known as Reinforcement Learning with Verifiable Rewards (RLVR). The study found that words with high entropy values have a significant impact on the model's reasoning performance and optimization. By analyzing the entropy patterns of words, researchers observed that only a small number of words have high entropy values, and these words determine the model's reasoning path. Further training indicated that RLVR primarily adjusts the entropy values of high-entropy words. By updating the policy gradients of only high-entropy words, researchers achieved significant performance improvements across different models.",
        "update_ts": "2025-06-03 09:13"
    }
}