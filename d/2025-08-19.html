
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 9 papers. August 19.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">19 августа</span> | <span id="title-articles-count">9 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-18.html">⬅️ <span id="prev-date">18.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-08-20.html">➡️ <span id="next-date">20.08</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-08.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '19 августа', 'en': 'August 19', 'zh': '8月19日'};
        let feedDateNext = {'ru': '20.08', 'en': '08/20', 'zh': '8月20日'};
        let feedDatePrev = {'ru': '18.08', 'en': '08/18', 'zh': '8月18日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.11737', 'title': 'Ovis2.5 Technical Report', 'url': 'https://huggingface.co/papers/2508.11737', 'abstract': 'Ovis2.5, a native-resolution vision transformer with multimodal reasoning, achieves state-of-the-art performance on various benchmarks through advanced training techniques and efficient scaling methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Ovis2.5, a successor to Ovis2 designed for native-resolution visual perception and strong multimodal reasoning. Ovis2.5 integrates a native-resolution vision transformer that processes images at their native, variable resolutions, avoiding the degradation from fixed-resolution tiling and preserving both fine detail and global layout -- crucial for visually dense content like complex charts. To strengthen reasoning, we train the model to move beyond linear chain-of-thought and perform reflection -- including self-checking and revision. This advanced capability is exposed as an optional "thinking mode" at inference time, allowing users to trade latency for enhanced accuracy on difficult inputs. The model is trained via a comprehensive five-phase curriculum that progressively builds its skills. The process begins with foundational visual and multimodal pretraining, advances through large-scale instruction tuning, and culminates in alignment and reasoning enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ multimodal data packing and hybrid parallelism, yielding a significant end-to-end speedup. We release two open-source models: Ovis2.5-9B and Ovis2.5-2B. The latter continues the "small model, big performance" philosophy of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a substantial improvement over its predecessor, Ovis2-8B, and achieving state-of-the-art results among open-source MLLMs in the sub-40B parameter range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong capabilities on grounding and video tasks, and achieves open-source SOTA at its scale for complex chart analysis.', 'score': 50, 'issue_id': 5417, 'pub_date': '2025-08-15', 'pub_date_card': {'ru': '15 августа', 'en': 'August 15', 'zh': '8月15日'}, 'hash': 'ccd30a02aad117a4', 'authors': ['Shiyin Lu', 'Yang Li', 'Yu Xia', 'Yuwei Hu', 'Shanshan Zhao', 'Yanqing Ma', 'Zhichao Wei', 'Yinglun Li', 'Lunhao Duan', 'Jianshan Zhao', 'Yuxuan Han', 'Haijun Li', 'Wanying Chen', 'Junke Tang', 'Chengkun Hou', 'Zhixing Du', 'Tianli Zhou', 'Wenjie Zhang', 'Huping Ding', 'Jiahe Li', 'Wen Li', 'Gui Hu', 'Yiliang Gu', 'Siran Yang', 'Jiamang Wang', 'Hailong Sun', 'Yibo Wang', 'Hui Sun', 'Jinlong Huang', 'Yuping He', 'Shengze Shi', 'Weihong Zhang', 'Guodong Zheng', 'Junpeng Jiang', 'Sensen Gao', 'Yi-Feng Wu', 'Sijia Chen', 'Yuhui Chen', 'Qing-Guo Chen', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.11737.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#low_resource', '#open_source', '#science', '#interpretability', '#benchmark', '#small_models', '#training', '#architecture', '#agi', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Оvis2.5: Мультимодальное зрение и рассуждение нового уровня', 'desc': "Ovis2.5 - это новая модель компьютерного зрения на основе трансформера, обрабатывающая изображения в нативном разрешении. Модель обучена выполнять рефлексию, включая самопроверку и исправление, что доступно как опциональный 'режим мышления' при инференсе. Обучение проводилось по пятифазной программе, включающей предобучение, инструктивную настройку и улучшение рассуждений с помощью DPO и GRPO. Ovis2.5 достигает state-of-the-art результатов на различных бенчмарках, особенно в задачах STEM и анализе сложных диаграмм."}, 'en': {'title': 'Ovis2.5: Revolutionizing Visual Perception with Multimodal Reasoning', 'desc': "Ovis2.5 is a cutting-edge vision transformer designed for high-resolution visual perception and advanced multimodal reasoning. It processes images at their native resolutions, which helps maintain detail and layout, making it effective for complex visual data. The model features a unique 'thinking mode' that allows for self-reflection and revision during inference, enhancing accuracy on challenging tasks. Trained through a structured five-phase curriculum, Ovis2.5 demonstrates significant performance improvements on various benchmarks, particularly in STEM and complex chart analysis, while also being optimized for efficiency and scalability."}, 'zh': {'title': 'Ovis2.5：原生分辨率与多模态推理的结合', 'desc': 'Ovis2.5是一种新型的视觉变换器，专注于原生分辨率的视觉感知和多模态推理。它能够处理不同分辨率的图像，避免了固定分辨率切片带来的细节损失，适合复杂图表等视觉密集内容。该模型通过五个阶段的训练课程逐步提升能力，并引入了反思机制，允许用户在推理时选择更高的准确性。Ovis2.5在多个基准测试中表现出色，尤其在STEM领域和复杂图表分析中取得了领先的结果。'}}}, {'id': 'https://huggingface.co/papers/2508.09834', 'title': 'Speed Always Wins: A Survey on Efficient Architectures for Large\n  Language Models', 'url': 'https://huggingface.co/papers/2508.09834', 'abstract': 'This survey examines innovative architectures for large language models to enhance efficiency, covering linear and sparse sequence modeling, efficient attention mechanisms, sparse mixture-of-experts, hybrid models, and diffusion LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.', 'score': 15, 'issue_id': 5417, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': '97c57497d6821327', 'authors': ['Weigao Sun', 'Jiaxi Hu', 'Yucheng Zhou', 'Jusen Du', 'Disen Lan', 'Kexin Wang', 'Tong Zhu', 'Xiaoye Qu', 'Yu Zhang', 'Xiaoyu Mo', 'Daizong Liu', 'Yuxuan Liang', 'Wenliang Chen', 'Guoqi Li', 'Yu Cheng'], 'affiliations': ['HKUST (GZ)', 'Institute of Automation, Chinese Academy of Sciences', 'KTH Royal Institute of Technology', 'Peking University', 'Shanghai AI Laboratory', 'Soochow University', 'The Chinese University of Hong Kong', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2508.09834.jpg', 'data': {'categories': ['#multimodal', '#survey', '#architecture', '#agi', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Инновации в архитектуре LLM: путь к эффективности и масштабируемости', 'desc': 'Это обзор инновационных архитектур для больших языковых моделей (LLM), направленных на повышение их эффективности. Рассматриваются линейные и разреженные методы моделирования последовательностей, эффективные механизмы внимания и разреженные смеси экспертов. Также обсуждаются гибридные модели и диффузионные LLM. Обзор предоставляет систематическое исследование современных эффективных архитектур LLM, преодолевающих ограничения традиционных трансформеров.'}, 'en': {'title': 'Innovating Efficiency in Large Language Models', 'desc': 'This survey explores new architectures for large language models (LLMs) aimed at improving their efficiency. It discusses various techniques such as linear and sparse sequence modeling, efficient attention mechanisms, and sparse mixture-of-experts. The paper highlights the limitations of traditional transformer models and presents innovative solutions to enhance scalability and reduce computational demands. By categorizing recent advancements, it provides a roadmap for future research in developing more efficient AI systems.'}, 'zh': {'title': '提升大型语言模型效率的创新架构', 'desc': '本调查研究了大型语言模型（LLM）的创新架构，以提高其效率。我们探讨了线性和稀疏序列建模、高效注意力机制、稀疏专家混合模型、混合模型以及扩散LLM等技术。传统的变换器架构虽然表现出色，但在大规模训练和实际部署中面临计算量大的挑战。通过系统性地分析这些新架构，我们希望为未来的高效、灵活的人工智能系统研究提供蓝图。'}}}, {'id': 'https://huggingface.co/papers/2508.10419', 'title': 'ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long\n  Narrative Reasoning', 'url': 'https://huggingface.co/papers/2508.10419', 'abstract': "ComoRAG, an iterative retrieval-based approach, enhances long-context narrative comprehension by dynamically updating memory and generating probing queries, outperforming traditional RAG methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at https://github.com/EternityJune25/ComoRAG", 'score': 13, 'issue_id': 5417, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '5b8e1dc52a233ce3', 'authors': ['Juyuan Wang', 'Rongchen Zhao', 'Wei Wei', 'Yufeng Wang', 'Mo Yu', 'Jie Zhou', 'Jin Xu', 'Liyan Xu'], 'affiliations': ['Independent Researcher', 'Pattern Recognition Center, WeChat AI, Tencent', 'Pazhou Lab, Guangzhou', 'School of Future Technology, South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.10419.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#long_context', '#rag'], 'emoji': '📚', 'ru': {'title': 'ComoRAG: когнитивно-мотивированный подход к пониманию длинных текстов', 'desc': 'ComoRAG - это новый подход к обработке длинных текстов, основанный на итеративном извлечении информации. Он динамически обновляет память и генерирует уточняющие запросы, что позволяет лучше понимать сложные сюжетные линии и отношения между персонажами. В отличие от традиционных методов RAG, ComoRAG использует многошаговый процесс, имитирующий человеческое познание. Эксперименты показали, что ComoRAG превосходит базовые методы RAG на сложных задачах понимания длинных текстов.'}, 'en': {'title': 'ComoRAG: Enhancing Narrative Comprehension through Dynamic Memory and Iterative Retrieval', 'desc': 'ComoRAG is a novel approach that improves understanding of long narratives by using an iterative retrieval process. Unlike traditional Retrieval-Augmented Generation (RAG) methods, which retrieve information in a single step, ComoRAG dynamically updates its memory and generates probing queries to better capture complex relationships in the text. This method mimics human cognitive processes, allowing for a more effective integration of new information with existing knowledge. As a result, ComoRAG shows significant performance improvements on long-context narrative tasks, achieving up to 11% better results than existing RAG models.'}, 'zh': {'title': 'ComoRAG：动态记忆与叙事推理的结合', 'desc': 'ComoRAG是一种基于检索的迭代方法，旨在提高对长篇叙事的理解能力。它通过动态更新记忆和生成探测查询，克服了传统RAG方法的局限性。该方法模拟人类的认知过程，强调叙事推理是一个动态的、不断演变的过程。实验结果表明，ComoRAG在多个长篇叙事基准测试中表现优于传统方法，提升了查询解析的连贯性。'}}}, {'id': 'https://huggingface.co/papers/2508.13154', 'title': '4DNeX: Feed-Forward 4D Generative Modeling Made Easy', 'url': 'https://huggingface.co/papers/2508.13154', 'abstract': '4DNeX generates high-quality dynamic 3D scene representations from a single image using a fine-tuned pretrained video diffusion model, outperforming existing methods in efficiency and generalizability.  \t\t\t\t\tAI-generated summary \t\t\t\t We present 4DNeX, the first feed-forward framework for generating 4D (i.e., dynamic 3D) scene representations from a single image. In contrast to existing methods that rely on computationally intensive optimization or require multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model. Specifically, 1) to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale dataset with high-quality 4D annotations generated using advanced reconstruction approaches. 2) we introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry. 3) we propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for 4D modeling. 4DNeX produces high-quality dynamic point clouds that enable novel-view video synthesis. Extensive experiments demonstrate that 4DNeX outperforms existing 4D generation methods in efficiency and generalizability, offering a scalable solution for image-to-4D modeling and laying the foundation for generative 4D world models that simulate dynamic scene evolution.', 'score': 7, 'issue_id': 5416, 'pub_date': '2025-08-18', 'pub_date_card': {'ru': '18 августа', 'en': 'August 18', 'zh': '8月18日'}, 'hash': 'cd93b5ad64365be7', 'authors': ['Zhaoxi Chen', 'Tianqi Liu', 'Long Zhuo', 'Jiawei Ren', 'Zeng Tao', 'He Zhu', 'Fangzhou Hong', 'Liang Pan', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.13154.jpg', 'data': {'categories': ['#cv', '#synthetic', '#3d', '#dataset', '#diffusion'], 'emoji': '🌟', 'ru': {'title': 'От 2D к 4D: революция в генерации динамических 3D-сцен', 'desc': '4DNeX - это новый фреймворк для создания динамических 3D-представлений сцен из одного изображения. Он использует предобученную модель диффузии видео, адаптированную для 4D-моделирования. 4DNeX превосходит существующие методы по эффективности и обобщаемости, генерируя высококачественные динамические облака точек. Метод включает создание большого набора данных 4DNeX-10M и использование унифицированного 6D-представления видео для совместного моделирования RGB и XYZ последовательностей.'}, 'en': {'title': 'Transforming Images into Dynamic 3D Worlds Efficiently', 'desc': '4DNeX is a novel framework that generates dynamic 3D scene representations from a single image using a fine-tuned video diffusion model. Unlike traditional methods that require multiple frames or extensive computational resources, 4DNeX streamlines the process into an efficient, end-to-end image-to-4D generation. It introduces a large-scale dataset, 4DNeX-10M, with high-quality 4D annotations to address data scarcity and employs a unified 6D video representation for better learning of both visual appearance and spatial geometry. The framework demonstrates superior performance in generating high-quality dynamic point clouds, paving the way for advanced generative models that can simulate evolving scenes.'}, 'zh': {'title': '4DNeX：高效生成动态3D场景的创新框架', 'desc': '4DNeX是一种新型的框架，可以从单张图像生成高质量的动态3D场景表示。与传统方法不同，4DNeX利用经过微调的预训练视频扩散模型，能够高效地实现图像到4D的生成。为了克服4D数据稀缺的问题，研究者们构建了一个包含高质量4D注释的大规模数据集。实验结果表明，4DNeX在效率和通用性方面优于现有的4D生成方法，为图像到4D建模提供了可扩展的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2508.13009', 'title': 'Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive\n  World Model', 'url': 'https://huggingface.co/papers/2508.13009', 'abstract': "Matrix-Game 2.0 generates real-time interactive videos using few-step auto-regressive diffusion, addressing the limitations of lengthy inference in existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in interactive video generations have demonstrated diffusion model's potential as world models by capturing complex physical dynamics and interactive behaviors. However, existing interactive world models depend on bidirectional attention and lengthy inference steps, severely limiting real-time performance. Consequently, they are hard to simulate real-world dynamics, where outcomes must update instantaneously based on historical context and current actions. To address this, we present Matrix-Game 2.0, an interactive world model generates long videos on-the-fly via few-step auto-regressive diffusion. Our framework consists of three key components: (1) A scalable data production pipeline for Unreal Engine and GTA5 environments to effectively produce massive amounts (about 1200 hours) of video data with diverse interaction annotations; (2) An action injection module that enables frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step distillation based on the casual architecture for real-time and streaming video generation. Matrix Game 2.0 can generate high-quality minute-level videos across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our model weights and codebase to advance research in interactive world modeling.", 'score': 2, 'issue_id': 5417, 'pub_date': '2025-08-18', 'pub_date_card': {'ru': '18 августа', 'en': 'August 18', 'zh': '8月18日'}, 'hash': 'ee608d0de07a48cb', 'authors': ['Xianglong He', 'Chunli Peng', 'Zexiang Liu', 'Boyang Wang', 'Yifan Zhang', 'Qi Cui', 'Fei Kang', 'Biao Jiang', 'Mengyin An', 'Yangyang Ren', 'Baixin Xu', 'Hao-Xiang Guo', 'Kaixiong Gong', 'Cyrus Wu', 'Wei Li', 'Xuchen Song', 'Yang Liu', 'Eric Li', 'Yahui Zhou'], 'affiliations': ['Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.13009.jpg', 'data': {'categories': ['#video', '#data', '#open_source', '#diffusion', '#dataset', '#games', '#architecture'], 'emoji': '🎮', 'ru': {'title': 'Революция в интерактивных видеоиграх: генерация в реальном времени с Matrix-Game 2.0', 'desc': 'Статья представляет Matrix-Game 2.0 - модель для генерации интерактивных видео в реальном времени, используя авторегрессивную диффузию с малым количеством шагов. Модель решает проблему длительного вывода, характерную для существующих интерактивных моделей мира. Matrix-Game 2.0 включает масштабируемый конвейер производства данных, модуль внедрения действий и дистилляцию на основе каузальной архитектуры. Система способна генерировать высококачественные минутные видео со скоростью 25 кадров в секунду.'}, 'en': {'title': 'Real-Time Interactive Video Generation with Matrix-Game 2.0', 'desc': 'Matrix-Game 2.0 is a novel framework for generating interactive videos in real-time using a few-step auto-regressive diffusion approach. This model overcomes the limitations of traditional interactive world models that rely on lengthy inference and bidirectional attention, which hinder real-time performance. By incorporating a scalable data production pipeline and an action injection module, it allows for dynamic interactions based on user inputs. The result is the ability to produce high-quality videos at an impressive speed of 25 frames per second, making it suitable for simulating real-world dynamics effectively.'}, 'zh': {'title': '实时互动视频生成的新突破', 'desc': 'Matrix-Game 2.0 是一种实时生成互动视频的模型，采用少步自回归扩散方法，克服了现有模型推理时间过长的限制。该模型能够捕捉复杂的物理动态和互动行为，适用于实时性能要求高的场景。它的框架包括可扩展的数据生产管道、动作注入模块和基于因果架构的少步蒸馏技术。通过这些创新，Matrix-Game 2.0 可以以每秒25帧的速度生成高质量的分钟级视频，推动互动世界建模的研究。'}}}, {'id': 'https://huggingface.co/papers/2508.12466', 'title': 'Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision\n  Mapping', 'url': 'https://huggingface.co/papers/2508.12466', 'abstract': 'Inverse-LLaVA eliminates alignment pre-training by mapping text embeddings into continuous visual representation space, improving reasoning tasks while reducing computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional multimodal learning approaches require expensive alignment pre-training to bridge vision and language modalities, typically projecting visual features into discrete text token spaces. We challenge both fundamental assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel approach that eliminates alignment pre-training entirely while inverting the conventional mapping direction. Rather than projecting visual features to text space, our method maps text embeddings into continuous visual representation space and performs fusion within transformer intermediate layers. Through selective additive components in attention mechanisms, we enable dynamic integration of visual and textual representations without requiring massive image-text alignment datasets. Comprehensive experiments across nine multimodal benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%, VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing expected decreases in perception tasks requiring memorized visual-text associations (celebrity recognition: -49.5%, OCR: -21.3%). These results provide the first empirical evidence that alignment pre-training is not necessary for effective multimodal learning, particularly for complex reasoning tasks. Our work establishes the feasibility of a new paradigm that reduces computational requirements by 45%, challenges conventional wisdom about modality fusion, and opens new research directions for efficient multimodal architectures that preserve modality-specific characteristics. Our project website with code and additional resources is available at https://inverse-llava.github.io.', 'score': 2, 'issue_id': 5416, 'pub_date': '2025-08-17', 'pub_date_card': {'ru': '17 августа', 'en': 'August 17', 'zh': '8月17日'}, 'hash': '18214c417cbd148a', 'authors': ['Xuhui Zhan', 'Tyler Derr'], 'affiliations': ['Computer Science Department Vanderbilt University', 'Data Science Institute Vanderbilt University'], 'pdf_title_img': 'assets/pdf/title_img/2508.12466.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#open_source', '#reasoning', '#architecture', '#alignment'], 'emoji': '🔄', 'ru': {'title': 'Инверсия модальностей для эффективного мультимодального обучения', 'desc': 'Inverse-LLaVA - это новый подход к мультимодальному обучению, который устраняет необходимость в предварительном обучении выравниванию модальностей. Вместо проецирования визуальных признаков в пространство текстовых токенов, метод отображает текстовые эмбеддинги в непрерывное пространство визуальных представлений. Эксперименты показывают улучшение производительности на задачах рассуждений и когнитивных задачах, но ожидаемое снижение в задачах восприятия. Подход сокращает вычислительные требования на 45% и открывает новые направления исследований эффективных мультимодальных архитектур.'}, 'en': {'title': 'Revolutionizing Multimodal Learning: No More Alignment Pre-Training!', 'desc': 'Inverse-LLaVA introduces a new method for multimodal learning that eliminates the need for alignment pre-training, which is typically required to connect visual and textual data. Instead of converting visual features into discrete text tokens, this approach maps text embeddings directly into a continuous visual representation space. By integrating visual and textual information within transformer layers using selective attention mechanisms, Inverse-LLaVA enhances performance on reasoning tasks while reducing the need for large datasets. The results show significant improvements in cognitive reasoning tasks, suggesting that traditional alignment methods may not be necessary for effective multimodal learning.'}, 'zh': {'title': '逆向LLaVA：无需对齐预训练的多模态学习新方法', 'desc': 'Inverse-LLaVA是一种新颖的多模态学习方法，它通过将文本嵌入映射到连续的视觉表示空间，完全消除了对对齐预训练的需求。这种方法在推理任务上表现出显著的改进，同时减少了计算资源的需求。通过在变换器的中间层进行视觉和文本表示的动态融合，Inverse-LLaVA能够在不依赖大量图像-文本对齐数据集的情况下实现有效的多模态学习。实验结果表明，该方法在复杂推理任务上取得了显著的性能提升，同时在需要记忆视觉-文本关联的感知任务上表现有所下降。'}}}, {'id': 'https://huggingface.co/papers/2508.12945', 'title': 'Lumen: Consistent Video Relighting and Harmonious Background Replacement\n  with Video Generative Models', 'url': 'https://huggingface.co/papers/2508.12945', 'abstract': 'Lumen is an end-to-end video relighting framework that uses a large-scale dataset of realistic and synthetic videos to achieve consistent lighting and foreground preservation in edited videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Video relighting is a challenging yet valuable task, aiming to replace the background in videos while correspondingly adjusting the lighting in the foreground with harmonious blending. During translation, it is essential to preserve the original properties of the foreground, e.g., albedo, and propagate consistent relighting among temporal frames. In this paper, we propose Lumen, an end-to-end video relighting framework developed on large-scale video generative models, receiving flexible textual description for instructing the control of lighting and background. Considering the scarcity of high-qualified paired videos with the same foreground in various lighting conditions, we construct a large-scale dataset with a mixture of realistic and synthetic videos. For the synthetic domain, benefiting from the abundant 3D assets in the community, we leverage advanced 3D rendering engine to curate video pairs in diverse environments. For the realistic domain, we adapt a HDR-based lighting simulation to complement the lack of paired in-the-wild videos. Powered by the aforementioned dataset, we design a joint training curriculum to effectively unleash the strengths of each domain, i.e., the physical consistency in synthetic videos, and the generalized domain distribution in realistic videos. To implement this, we inject a domain-aware adapter into the model to decouple the learning of relighting and domain appearance distribution. We construct a comprehensive benchmark to evaluate Lumen together with existing methods, from the perspectives of foreground preservation and video consistency assessment. Experimental results demonstrate that Lumen effectively edit the input into cinematic relighted videos with consistent lighting and strict foreground preservation. Our project page: https://lumen-relight.github.io/', 'score': 1, 'issue_id': 5417, 'pub_date': '2025-08-18', 'pub_date_card': {'ru': '18 августа', 'en': 'August 18', 'zh': '8月18日'}, 'hash': 'c5be2dd35a23538c', 'authors': ['Jianshu Zeng', 'Yuxuan Liu', 'Yutong Feng', 'Chenxuan Miao', 'Zixiang Gao', 'Jiwang Qu', 'Jianzhang Zhang', 'Bin Wang', 'Kun Yuan'], 'affiliations': ['Hangzhou Normal University', 'Kunbyte AI', 'Peking University', 'University of Chinese Academy of Sciences', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.12945.jpg', 'data': {'categories': ['#dataset', '#video', '#synthetic', '#benchmark'], 'emoji': '💡', 'ru': {'title': 'Lumen: Интеллектуальная перерисовка освещения в видео с сохранением переднего плана', 'desc': 'Lumen - это комплексная система перерисовки освещения в видео, использующая масштабный набор данных реалистичных и синтетических видео. Она позволяет согласованно изменять освещение и сохранять передний план в отредактированных видео. Система обучается на смешанном наборе данных, включающем синтетические видео, созданные с помощью 3D-рендеринга, и реалистичные видео с симуляцией HDR-освещения. Lumen использует доменно-ориентированный адаптер для разделения обучения перерисовке освещения и распределению внешнего вида домена.'}, 'en': {'title': 'Lumen: Mastering Video Relighting with Consistent Foreground Preservation', 'desc': 'Lumen is a novel framework designed for video relighting, which focuses on changing the background of videos while ensuring that the lighting on the foreground remains consistent. It utilizes a large-scale dataset of both realistic and synthetic videos to train its model, addressing the challenge of preserving foreground properties like albedo during the relighting process. The framework employs a joint training approach that leverages the strengths of synthetic videos for physical consistency and realistic videos for generalization. By incorporating a domain-aware adapter, Lumen effectively separates the learning of relighting from the appearance distribution of different domains, resulting in high-quality, edited videos that maintain both lighting consistency and foreground integrity.'}, 'zh': {'title': 'Lumen：实现一致光照与前景保留的视频重光框架', 'desc': 'Lumen是一个端到端的视频重光框架，旨在通过使用大规模的真实和合成视频数据集，实现编辑视频中的一致光照和前景保留。该框架能够根据灵活的文本描述来控制光照和背景，解决了视频重光中前景属性（如反照率）的保留问题。为了应对高质量配对视频的稀缺性，我们构建了一个包含多种环境的合成视频和真实视频的大规模数据集。实验结果表明，Lumen能够有效地将输入视频编辑为具有一致光照和严格前景保留的电影级重光视频。'}}}, {'id': 'https://huggingface.co/papers/2508.12880', 'title': 'S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2508.12880', 'abstract': "S^2-Guidance, a novel method using stochastic block-dropping, improves sample quality and prompt adherence in diffusion models by refining suboptimal predictions, outperforming Classifier-free Guidance and other advanced strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Classifier-free Guidance (CFG) is a widely used technique in modern diffusion models for enhancing sample quality and prompt adherence. However, through an empirical analysis on Gaussian mixture modeling with a closed-form solution, we observe a discrepancy between the suboptimal results produced by CFG and the ground truth. The model's excessive reliance on these suboptimal predictions often leads to semantic incoherence and low-quality outputs. To address this issue, we first empirically demonstrate that the model's suboptimal predictions can be effectively refined using sub-networks of the model itself. Building on this insight, we propose S^2-Guidance, a novel method that leverages stochastic block-dropping during the forward process to construct stochastic sub-networks, effectively guiding the model away from potential low-quality predictions and toward high-quality outputs. Extensive qualitative and quantitative experiments on text-to-image and text-to-video generation tasks demonstrate that S^2-Guidance delivers superior performance, consistently surpassing CFG and other advanced guidance strategies. Our code will be released.", 'score': 1, 'issue_id': 5417, 'pub_date': '2025-08-18', 'pub_date_card': {'ru': '18 августа', 'en': 'August 18', 'zh': '8月18日'}, 'hash': '8d70a9abdc48a41a', 'authors': ['Chubin Chen', 'Jiashu Zhu', 'Xiaokun Feng', 'Nisha Huang', 'Meiqi Wu', 'Fangyuan Mao', 'Jiahong Wu', 'Xiangxiang Chu', 'Xiu Li'], 'affiliations': ['AMAP, Alibaba Group', 'CASIA', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.12880.jpg', 'data': {'categories': ['#video', '#open_source', '#diffusion', '#training', '#cv', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'S^2-Guidance: новый путь к качественной генерации в диффузионных моделях', 'desc': 'S^2-Guidance - это новый метод, использующий стохастическое отбрасывание блоков для улучшения качества сэмплов и соответствия промпту в диффузионных моделях. Метод превосходит Classifier-free Guidance и другие продвинутые стратегии, уточняя субоптимальные предсказания модели. S^2-Guidance использует стохастические подсети модели во время прямого прохода, эффективно направляя модель к выходам высокого качества. Эксперименты на задачах генерации изображений и видео по тексту показали превосходство S^2-Guidance над существующими методами.'}, 'en': {'title': 'Refining Predictions for Superior Sample Quality in Diffusion Models', 'desc': 'The paper introduces S^2-Guidance, a new method that enhances the performance of diffusion models by refining their predictions. It addresses the limitations of Classifier-free Guidance (CFG), which often leads to low-quality outputs due to reliance on suboptimal predictions. By using stochastic block-dropping, S^2-Guidance creates sub-networks that help the model focus on generating higher quality samples. Experimental results show that this method significantly outperforms CFG and other existing strategies in tasks like text-to-image and text-to-video generation.'}, 'zh': {'title': 'S^2-Guidance：提升扩散模型的样本质量与提示遵循性', 'desc': 'S^2-Guidance是一种新颖的方法，通过随机块丢弃技术来提高扩散模型的样本质量和提示遵循性。该方法通过改进次优预测，克服了无分类器引导（CFG）等传统技术的不足。研究表明，模型的次优预测可以通过自身的子网络有效地进行优化，从而减少语义不一致和低质量输出的问题。通过在文本到图像和文本到视频生成任务上的广泛实验，S^2-Guidance展现了优越的性能，超越了CFG和其他先进的引导策略。'}}}, {'id': 'https://huggingface.co/papers/2508.11598', 'title': 'Representing Speech Through Autoregressive Prediction of Cochlear Tokens', 'url': 'https://huggingface.co/papers/2508.11598', 'abstract': "AuriStream, a biologically inspired two-stage model, encodes speech using cochlear tokens and an autoregressive sequence model, achieving state-of-the-art performance on speech tasks and generating interpretable audio continuations.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce AuriStream, a biologically inspired model for encoding speech via a two-stage framework inspired by the human auditory processing hierarchy. The first stage transforms raw audio into a time-frequency representation based on the human cochlea, from which we extract discrete cochlear tokens. The second stage applies an autoregressive sequence model over the cochlear tokens. AuriStream learns meaningful phoneme and word representations, and state-of-the-art lexical semantics. AuriStream shows competitive performance on diverse downstream SUPERB speech tasks. Complementing AuriStream's strong representational capabilities, it generates continuations of audio which can be visualized in a spectrogram space and decoded back into audio, providing insights into the model's predictions. In summary, we present a two-stage framework for speech representation learning to advance the development of more human-like models that efficiently handle a range of speech-based tasks.", 'score': 1, 'issue_id': 5417, 'pub_date': '2025-08-15', 'pub_date_card': {'ru': '15 августа', 'en': 'August 15', 'zh': '8月15日'}, 'hash': '6421a301f871081b', 'authors': ['Greta Tuckute', 'Klemen Kotar', 'Evelina Fedorenko', 'Daniel L. K. Yamins'], 'affiliations': ['Department of Brain and Cognitive Sciences & McGovern Institute for Brain Research, MIT, USA', 'Department of Computer Science & Wu Tsai Neurosciences Institute, Stanford University, USA', 'Department of Psychology, Stanford University, USA', 'Program in Speech and Hearing Bioscience and Technology, Harvard University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.11598.jpg', 'data': {'categories': ['#science', '#audio', '#interpretability'], 'emoji': '👂', 'ru': {'title': 'Биологически вдохновленная модель для эффективной обработки речи', 'desc': 'AuriStream - это биологически вдохновленная двухэтапная модель для кодирования речи. Первый этап преобразует сырой аудиосигнал в частотно-временное представление, основанное на работе человеческой улитки, из которого извлекаются дискретные кохлеарные токены. На втором этапе применяется авторегрессионная последовательностная модель к этим токенам. AuriStream демонстрирует высокую производительность на различных задачах обработки речи и способна генерировать интерпретируемые продолжения аудио.'}, 'en': {'title': 'AuriStream: Advancing Speech Processing with Human-Inspired Learning', 'desc': 'AuriStream is a two-stage model designed for encoding speech, inspired by how humans process sound. The first stage converts raw audio into a time-frequency representation, extracting discrete cochlear tokens that represent sound features. The second stage uses an autoregressive sequence model to analyze these tokens, enabling the model to learn meaningful phoneme and word representations. AuriStream achieves state-of-the-art results on various speech tasks and can generate audio continuations that are interpretable, enhancing our understanding of its predictions.'}, 'zh': {'title': 'AuriStream：生物启发的语音编码新模型', 'desc': 'AuriStream 是一个受生物启发的两阶段模型，用于编码语音。第一阶段将原始音频转换为基于人类耳蜗的时频表示，并提取离散的耳蜗标记。第二阶段在耳蜗标记上应用自回归序列模型，学习有意义的音素和单词表示。AuriStream 在多种语音任务上表现出色，并能够生成可视化的音频延续，提供对模型预测的深入理解。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents', '#agi (2)', '#alignment (1)', '#architecture (4)', '#audio (1)', '#benchmark (2)', '#cv (3)', '#data (1)', '#dataset (3)', '#diffusion (3)', '#ethics', '#games (1)', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual', '#multimodal (4)', '#open_source (4)', '#optimization (3)', '#plp', '#rag (1)', '#reasoning (3)', '#rl', '#rlhf', '#robotics', '#science (2)', '#security', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (2)', '#transfer_learning', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-08-19 03:37',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-19 03:37')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-19 03:37')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    