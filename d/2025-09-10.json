{
    "date": {
        "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 10",
        "zh": "9æœˆ10æ—¥"
    },
    "time_utc": "2025-09-10 11:09",
    "weekday": 2,
    "issue_id": 5815,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.07980",
            "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.07980",
            "abstract": "Parallel-R1, a reinforcement learning framework, enhances large language models' reasoning capabilities by enabling parallel thinking through a progressive curriculum, leading to significant performance improvements on math benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a mid-training exploration scaffold, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1.",
            "score": 60,
            "issue_id": 5806,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 9",
                "zh": "9æœˆ9æ—¥"
            },
            "hash": "41def489cc53d3e0",
            "authors": [
                "Tong Zheng",
                "Hongming Zhang",
                "Wenhao Yu",
                "Xiaoyang Wang",
                "Xinyu Yang",
                "Runpeng Dai",
                "Rui Liu",
                "Huiwen Bao",
                "Chengsong Huang",
                "Heng Huang",
                "Dong Yu"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "City University of Hong Kong",
                "Tencent AI Lab Seattle",
                "University of Maryland, College Park",
                "University of North Carolina at Chapel Hill",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07980.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#open_source",
                    "#rl",
                    "#optimization",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Parallel-R1 - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑƒÑ‡ĞµĞ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ĞºĞ°Ğº ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°ÑĞ¿ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸."
                },
                "en": {
                    "title": "Unlocking Reasoning Power with Parallel Thinking",
                    "desc": "The paper introduces Parallel-R1, a reinforcement learning framework designed to improve the reasoning abilities of large language models (LLMs) by facilitating parallel thinking. This approach allows the model to explore multiple reasoning paths simultaneously, which enhances its performance on complex tasks. Unlike traditional methods that rely on supervised fine-tuning, Parallel-R1 employs a progressive curriculum that first trains the model on simpler tasks before transitioning to more challenging ones using reinforcement learning. The results demonstrate significant accuracy improvements on various math benchmarks, showcasing the effectiveness of parallel thinking in enhancing model performance."
                },
                "zh": {
                    "title": "å¹¶è¡Œæ€ç»´ï¼šæå‡æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "Parallel-R1æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¹¶è¡Œæ€ç»´æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¸è¿›å¼è¯¾ç¨‹ï¼Œè§£å†³äº†åœ¨è®­ç»ƒå¹¶è¡Œæ€ç»´æ—¶çš„å†·å¯åŠ¨é—®é¢˜ã€‚é€šè¿‡åœ¨ç®€å•ä»»åŠ¡ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œæ¨¡å‹å­¦ä¹ å¹¶è¡Œæ€ç»´èƒ½åŠ›ï¼Œç„¶åè½¬å‘å¼ºåŒ–å­¦ä¹ ä»¥åº”å¯¹æ›´å¤æ‚çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒParallel-R1åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å¹¶è¡Œæ€ç»´åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07969",
            "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search",
            "url": "https://huggingface.co/papers/2509.07969",
            "abstract": "Mini-o3, a system for deep, multi-turn reasoning in visual search tasks, uses an iterative data collection pipeline and over-turn masking strategy to achieve state-of-the-art performance with rich reasoning patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.",
            "score": 40,
            "issue_id": 5806,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 9",
                "zh": "9æœˆ9æ—¥"
            },
            "hash": "4d29daad3dc9ac61",
            "authors": [
                "Xin Lai",
                "Junyi Li",
                "Wei Li",
                "Tao Liu",
                "Tianjian Li",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "ByteDance",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07969.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#cv",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ: Mini-o3 Ñ€Ğ°Ğ·Ğ´Ğ²Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Mini-o3 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑˆĞµÑÑ‚ÑŒ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ´Ğ¾ Ğ´ĞµÑÑÑ‚ĞºĞ¾Ğ² ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mini-o3 ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Deep Reasoning in Visual Search with Mini-o3",
                    "desc": "Mini-o3 is a novel system designed for deep reasoning in visual search tasks, enabling multi-turn interactions that enhance problem-solving capabilities. It utilizes an iterative data collection pipeline and an over-turn masking strategy to improve performance and reasoning diversity. By constructing the Visual Probe Dataset, Mini-o3 addresses the limitations of existing models that struggle with complex tasks requiring extensive exploration. The system achieves state-of-the-art results by allowing for a greater number of reasoning steps during inference, leading to more accurate and nuanced solutions."
                },
                "zh": {
                    "title": "Mini-o3ï¼šæ·±åº¦å¤šè½®æ¨ç†çš„è§†è§‰æœç´¢æ–°çªç ´",
                    "desc": "Mini-o3æ˜¯ä¸€ä¸ªç”¨äºè§†è§‰æœç´¢ä»»åŠ¡çš„æ·±åº¦å¤šè½®æ¨ç†ç³»ç»Ÿï¼Œé‡‡ç”¨è¿­ä»£æ•°æ®æ”¶é›†ç®¡é“å’Œè¶…è½®æ©è”½ç­–ç•¥ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ„å»ºè§†è§‰æ¢æµ‹æ•°æ®é›†ï¼Œè®¾è®¡äº†æ•°åƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰æœç´¢é—®é¢˜ï¼Œä»¥æ”¯æŒæ¢ç´¢æ€§æ¨ç†ã€‚Mini-o3èƒ½å¤Ÿæ‰§è¡Œæ·±åº¦çš„å¤šè½®æ¨ç†ï¼Œå¤„ç†æ•°åä¸ªæ­¥éª¤ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨äº¤äº’è½®æ¬¡å’Œæ¨ç†æ¨¡å¼ä¸Šçš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMini-o3èƒ½å¤Ÿç”Ÿæˆä¸°å¯Œçš„æ¨ç†æ¨¡å¼å’Œæ·±åº¦æ€è€ƒè·¯å¾„ï¼Œæœ‰æ•ˆè§£å†³å¤æ‚çš„è§†è§‰æœç´¢é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07979",
            "title": "Visual Representation Alignment for Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2509.07979",
            "abstract": "VIRAL, a regularization strategy, aligns MLLMs' visual representations with pre-trained VFMs, enhancing performance on vision-centric tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs.",
            "score": 36,
            "issue_id": 5806,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 9",
                "zh": "9æœˆ9æ—¥"
            },
            "hash": "f229168a41923a52",
            "authors": [
                "Heeji Yoon",
                "Jaewoo Jung",
                "Junwan Kim",
                "Hyungyu Choi",
                "Heeseong Shin",
                "Sangbeom Lim",
                "Honggyu An",
                "Chaehyun Kim",
                "Jisang Han",
                "Donghyun Kim",
                "Chanho Eom",
                "Sunghwan Hong",
                "Seungryong Kim"
            ],
            "affiliations": [
                "Chung-Ang University",
                "ETH Zurich",
                "KAIST AI",
                "Korea University",
                "NYU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07979.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#alignment",
                    "#training",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VIRAL - ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). VIRAL Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ MLLM Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (VFM). Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ MLLM ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ· VFM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Aligning Visual Representations for Better Multimodal Learning",
                    "desc": "This paper introduces VIRAL, a regularization technique that improves the performance of multimodal large language models (MLLMs) on vision-related tasks. The authors identify that MLLMs struggle with tasks like object counting due to a lack of direct visual supervision, which leads to the loss of important visual details. VIRAL addresses this issue by aligning the visual representations of MLLMs with those from pre-trained vision foundation models (VFMs), ensuring that the models retain critical visual information. The results show that this alignment significantly enhances the models' reasoning capabilities on complex visual inputs across various benchmarks."
                },
                "zh": {
                    "title": "VIRALï¼šæå‡è§†è§‰ä»»åŠ¡è¡¨ç°çš„å¯¹é½ç­–ç•¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVIRALçš„æ­£åˆ™åŒ–ç­–ç•¥ï¼Œæ—¨åœ¨å°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰è¡¨ç¤ºä¸é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰å¯¹é½ï¼Œä»è€Œæå‡åœ¨è§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¼ ç»Ÿçš„æ–‡æœ¬ç›‘ç£æ–¹æ³•å¯¹è§†è§‰è·¯å¾„çš„æŒ‡å¯¼æœ‰é™ï¼Œå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¿½è§†äº†ç»†è‡´çš„è§†è§‰ä¿¡æ¯ã€‚é€šè¿‡å¼ºåˆ¶å¯¹é½å†…éƒ¨è§†è§‰è¡¨ç¤ºï¼ŒVIRALä¸ä»…å¸®åŠ©æ¨¡å‹ä¿ç•™è¾“å…¥è§†è§‰ç¼–ç å™¨ä¸­çš„é‡è¦ç»†èŠ‚ï¼Œè¿˜èƒ½è¡¥å……æ¥è‡ªVFMsçš„é¢å¤–è§†è§‰çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVIRALåœ¨å¤šé¡¹å¹¿æ³›é‡‡ç”¨çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07295",
            "title": "Reconstruction Alignment Improves Unified Multimodal Models",
            "url": "https://huggingface.co/papers/2509.07295",
            "abstract": "Reconstruction Alignment (RecA) is a post-training method that enhances multimodal models by using visual embeddings as dense prompts, improving image generation and editing fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense \"text prompts,\" providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73rightarrow0.90) and DPGBench (80.93rightarrow88.15), while also boosting editing benchmarks (ImgEdit 3.38rightarrow3.75, GEdit 6.94rightarrow7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs",
            "score": 24,
            "issue_id": 5807,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "b638c4100f242a73",
            "authors": [
                "Ji Xie",
                "Trevor Darrell",
                "Luke Zettlemoyer",
                "XuDong Wang"
            ],
            "affiliations": [
                "UC Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07295.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#open_source",
                    "#multimodal",
                    "#optimization",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "RecA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Reconstruction Alignment (RecA) - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². RecA Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (UMM) Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ñ‚ĞµĞ¼ ÑĞ°Ğ¼Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸ĞµĞ¹ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ UMM, Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ñƒ, RecA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Enhancing Multimodal Models with Reconstruction Alignment",
                    "desc": "Reconstruction Alignment (RecA) is a novel post-training technique designed to enhance unified multimodal models (UMMs) by utilizing visual embeddings as dense prompts. This method addresses the limitations of traditional training, which often relies on sparse image-text pairs that fail to capture detailed visual information. By conditioning UMMs on their own visual understanding embeddings and optimizing for self-supervised reconstruction, RecA effectively realigns the model's understanding and generation capabilities. The results demonstrate significant improvements in image generation and editing fidelity across various UMM architectures, making RecA a resource-efficient and broadly applicable strategy."
                },
                "zh": {
                    "title": "é‡å»ºå¯¹é½ï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„å›¾åƒç”Ÿæˆä¸ç¼–è¾‘ç²¾åº¦",
                    "desc": "é‡å»ºå¯¹é½ï¼ˆRecAï¼‰æ˜¯ä¸€ç§åè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨è§†è§‰åµŒå…¥ä½œä¸ºå¯†é›†æç¤ºï¼Œå¢å¼ºå¤šæ¨¡æ€æ¨¡å‹çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ç²¾åº¦ã€‚ä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•ä¾èµ–äºå›¾åƒ-æ–‡æœ¬å¯¹ï¼Œä½†è¿™äº›æ–‡æœ¬é€šå¸¸ç¼ºä¹ç»†è‡´çš„è§†è§‰ç»†èŠ‚ã€‚RecAåˆ©ç”¨è§†è§‰ç†è§£ç¼–ç å™¨çš„åµŒå…¥ä½œä¸ºä¸°å¯Œçš„â€œæ–‡æœ¬æç¤ºâ€ï¼Œåœ¨æ²¡æœ‰æ–‡æœ¬æè¿°çš„æƒ…å†µä¸‹æä¾›ç›‘ç£ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§å¤šæ¨¡æ€æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒç”Ÿæˆå’Œç¼–è¾‘çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06818",
            "title": "UMO: Scaling Multi-Identity Consistency for Image Customization via\n  Matching Reward",
            "url": "https://huggingface.co/papers/2509.06818",
            "abstract": "UMO, a Unified Multi-identity Optimization framework, enhances identity consistency and reduces confusion in multi-reference image customization using reinforcement learning on diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With \"multi-to-multi matching\" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO",
            "score": 22,
            "issue_id": 5807,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "42f9165098d4a2b8",
            "authors": [
                "Yufeng Cheng",
                "Wenxu Wu",
                "Shaojin Wu",
                "Mengqi Huang",
                "Fei Ding",
                "Qian He"
            ],
            "affiliations": [
                "UXO Team, Intelligent Creation Lab, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06818.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#open_source",
                    "#optimization",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "UMO: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "UMO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ² ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ. UMO Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ 'Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ-ĞºĞ¾-Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğ¼' Ğ´Ğ»Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "UMO: Enhancing Identity Consistency in Image Customization",
                    "desc": "The paper introduces UMO, a framework that improves how images are customized while keeping identities consistent across multiple references. It uses reinforcement learning on diffusion models to tackle the challenge of identity confusion, which is crucial since humans are particularly sensitive to faces. UMO reformulates the problem of generating multiple identities as a global assignment optimization task, enhancing the scalability of identity preservation. The authors also create a new dataset and metric to support their framework and demonstrate that UMO significantly outperforms existing methods in maintaining identity consistency."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¤šèº«ä»½ä¼˜åŒ–ï¼Œæå‡å›¾åƒå®šåˆ¶ä¸€è‡´æ€§",
                    "desc": "UMOï¼ˆç»Ÿä¸€å¤šèº«ä»½ä¼˜åŒ–æ¡†æ¶ï¼‰é€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œå¢å¼ºäº†å¤šå‚è€ƒå›¾åƒå®šåˆ¶ä¸­çš„èº«ä»½ä¸€è‡´æ€§ï¼Œå‡å°‘äº†èº«ä»½æ··æ·†ã€‚è¯¥æ¡†æ¶è§£å†³äº†åœ¨å¤šå‚è€ƒå›¾åƒä¸­ä¿æŒä¸€è‡´èº«ä»½çš„æŒ‘æˆ˜ï¼Œæå‡äº†å®šåˆ¶æ¨¡å‹çš„èº«ä»½å¯æ‰©å±•æ€§ã€‚UMOå°†å¤šèº«ä»½ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºå…¨å±€åˆ†é…ä¼˜åŒ–é—®é¢˜ï¼Œåˆ©ç”¨â€œå¤šå¯¹å¤šåŒ¹é…â€èŒƒå¼å®ç°èº«ä»½ä¸€è‡´æ€§ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«åˆæˆå’ŒçœŸå®éƒ¨åˆ†çš„å¯æ‰©å±•å®šåˆ¶æ•°æ®é›†ï¼ŒUMOåœ¨å¤šä¸ªå›¾åƒå®šåˆ¶æ–¹æ³•ä¸Šæ˜¾è‘—æé«˜äº†èº«ä»½ä¸€è‡´æ€§ï¼Œå¹¶å‡å°‘äº†èº«ä»½æ··æ·†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06830",
            "title": "Curia: A Multi-Modal Foundation Model for Radiology",
            "url": "https://huggingface.co/papers/2509.06830",
            "abstract": "Curia, a foundation model trained on extensive cross-sectional imaging data, demonstrates superior performance across multiple radiological tasks and shows emergent properties in cross-modality and low-data settings.  \t\t\t\t\tAI-generated summary \t\t\t\t AI-assisted radiological interpretation is based on predominantly narrow, single-task models. This approach is impractical for covering the vast spectrum of imaging modalities, diseases, and radiological findings. Foundation models (FMs) hold the promise of broad generalization across modalities and in low-data settings. However, this potential has remained largely unrealized in radiology. We introduce Curia, a foundation model trained on the entire cross-sectional imaging output of a major hospital over several years, which to our knowledge is the largest such corpus of real-world data-encompassing 150,000 exams (130 TB). On a newly curated 19-task external validation benchmark, Curia accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent foundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes. To accelerate progress, we release our base model's weights at https://huggingface.co/raidium/curia.",
            "score": 16,
            "issue_id": 5812,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "5a09abd9043552bb",
            "authors": [
                "Corentin Dancette",
                "Julien Khlaut",
                "Antoine Saporta",
                "Helene Philippe",
                "Elodie Ferreres",
                "Baptiste Callard",
                "ThÃ©o Danielou",
                "LÃ©o Alberge",
                "LÃ©o Machado",
                "Daniel Tordjman",
                "Julie Dupuis",
                "Korentin Le Floch",
                "Jean Du Terrail",
                "Mariam Moshiri",
                "Laurent Dercle",
                "Tom Boeken",
                "Jules Gregory",
                "Maxime Ronot",
                "FranÃ§ois Legou",
                "Pascal Roux",
                "Marc Sapoval",
                "Pierre Manceron",
                "Paul HÃ©rent"
            ],
            "affiliations": [
                ".omics, Paris, France",
                "Centre Cardiologique du Nord, Saint-Denis, 93200, France",
                "Department of Radiology and Radiological Science, Medical University of South Carolina, Charleston, SC, USA",
                "Department of Radiology, Columbia University Irving Medical Center, New York, NY, 10032, USA",
                "Department of Radiology, FHU MOSAIC, Beaujon Hospital, APHP.Nord, Clichy, France",
                "Department of Vascular and Oncological Interventional Radiology, HË†opital Europeen Georges Pompidou, AP-HP, Paris, France",
                "Faculte de Sante, Universite Paris-Cite, Paris, France",
                "HEKA, INRIA, Paris, France",
                "PARCC 970, INSERM, Paris, France",
                "Raidium, 27 rue du faubourg Saint-Jacques, Paris, 75014, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06830.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#healthcare",
                    "#benchmark",
                    "#data",
                    "#low_resource",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Curia: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "ĞœĞ¾Ğ´ĞµĞ»ÑŒ Curia - ÑÑ‚Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ², Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹. Curia Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ² ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 150 000 Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ (130 Ğ¢Ğ‘ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…) Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Curia: Revolutionizing Radiology with a Foundation Model",
                    "desc": "Curia is a foundation model designed for radiology, trained on a vast dataset of cross-sectional imaging from a major hospital. It excels in various radiological tasks, outperforming traditional narrow models by demonstrating strong generalization across different imaging modalities and in scenarios with limited data. The model has been validated on a comprehensive benchmark, showing its ability to accurately identify organs and detect critical conditions. By releasing its weights, Curia aims to foster further advancements in AI-assisted radiological interpretation."
                },
                "zh": {
                    "title": "Curiaï¼šæ”¾å°„å­¦çš„åŸºç¡€æ¨¡å‹æ–°çªç ´",
                    "desc": "Curiaæ˜¯ä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œç»è¿‡å¤§é‡æ¨ªæ–­é¢å½±åƒæ•°æ®çš„è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæ”¾å°„å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒåœ¨è·¨æ¨¡æ€å’Œä½æ•°æ®ç¯å¢ƒä¸‹å±•ç°å‡ºæ–°å…´ç‰¹æ€§ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„å•ä»»åŠ¡æ¨¡å‹ã€‚Curiaä½¿ç”¨äº†æ¥è‡ªä¸€å®¶å¤§å‹åŒ»é™¢çš„150,000ä¸ªæ£€æŸ¥æ•°æ®ï¼Œæˆä¸ºç°å®ä¸–ç•Œæ•°æ®ä¸­æœ€å¤§çš„è®­ç»ƒé›†ä¹‹ä¸€ã€‚é€šè¿‡åœ¨19ä¸ªä»»åŠ¡çš„å¤–éƒ¨éªŒè¯åŸºå‡†ä¸Šæµ‹è¯•ï¼ŒCuriaçš„è¡¨ç°ä¸æ”¾å°„ç§‘åŒ»ç”Ÿç›¸å½“ï¼Œç”šè‡³æ›´ä¼˜ï¼Œæ˜¾ç¤ºå‡ºå…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06951",
            "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation\n  to Actions",
            "url": "https://huggingface.co/papers/2509.06951",
            "abstract": "F1, a pretrained VLA framework with foresight generation, improves task success and generalization in dynamic environments through a Mixture-of-Transformer architecture and next-scale prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability.",
            "score": 15,
            "issue_id": 5807,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "c287d68dc6b0f03d",
            "authors": [
                "Qi Lv",
                "Weijie Kong",
                "Hao Li",
                "Jia Zeng",
                "Zherui Qiu",
                "Delin Qu",
                "Haoming Song",
                "Qizhi Chen",
                "Xiang Deng",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Harbin Institute of Technology (Shenzhen)",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06951.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#cv",
                    "#reasoning",
                    "#benchmark",
                    "#agents",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "F1 - ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture-of-Transformer Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. F1 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†ĞµĞ»ÑŒÑ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğµ Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "F1: Enhancing Decision-Making with Visual Foresight in Dynamic Environments",
                    "desc": "The paper presents F1, a pretrained Vision-Language-Action (VLA) framework designed to enhance performance in dynamic environments. Unlike traditional models that react to immediate states, F1 incorporates visual foresight generation to improve decision-making and robustness. It utilizes a Mixture-of-Transformer architecture that integrates perception, foresight, and control, allowing for better planning and action generation. Through a comprehensive training process on a large dataset, F1 demonstrates superior task success and generalization compared to existing methods."
                },
                "zh": {
                    "title": "F1ï¼šåŠ¨æ€ç¯å¢ƒä¸­çš„å‰ç»æ€§å†³ç­–æ¡†æ¶",
                    "desc": "F1æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è§†è§‰å‰ç»ç”Ÿæˆæ¥æé«˜åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„ä»»åŠ¡æˆåŠŸç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®ƒé‡‡ç”¨æ··åˆå˜æ¢å™¨æ¶æ„ï¼Œç»“åˆæ„ŸçŸ¥ã€å‰ç»ç”Ÿæˆå’Œæ§åˆ¶æ¨¡å—ï¼Œå¢å¼ºäº†ç†è§£ã€ç”Ÿæˆå’Œè¡ŒåŠ¨ä¹‹é—´çš„è”ç³»ã€‚F1çš„æ ¸å¿ƒæ˜¯ä¸‹ä¸€å°ºåº¦é¢„æµ‹æœºåˆ¶ï¼Œé€šè¿‡é¢„æµ‹æœªæ¥çš„è§†è§‰çŠ¶æ€ï¼Œå°†è¡ŒåŠ¨ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªä»¥å‰ç»ä¸ºæŒ‡å¯¼çš„é€†åŠ¨åŠ›å­¦é—®é¢˜ã€‚ç»è¿‡åœ¨è¶…è¿‡33ä¸‡æ¡è½¨è¿¹å’Œ136ä¸ªå¤šæ ·åŒ–ä»»åŠ¡ä¸Šçš„ä¸‰é˜¶æ®µè®­ç»ƒï¼ŒF1åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡å’Œæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†ä»»åŠ¡æˆåŠŸç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06923",
            "title": "Staying in the Sweet Spot: Responsive Reasoning Evolution via\n  Capability-Adaptive Hint Scaffolding",
            "url": "https://huggingface.co/papers/2509.06923",
            "abstract": "SEELE, a novel RLVR framework, dynamically adjusts problem difficulty using adaptive hint lengths to enhance exploration efficiency and improve performance in math reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success in enhancing the reasoning capabilities of large language models (LLMs). However, existing RLVR methods often suffer from exploration inefficiency due to mismatches between the training data's difficulty and the model's capability. LLMs fail to discover viable reasoning paths when problems are overly difficult, while learning little new capability when problems are too simple. In this work, we formalize the impact of problem difficulty by quantifying the relationship between loss descent speed and rollout accuracy. Building on this analysis, we propose SEELE, a novel supervision-aided RLVR framework that dynamically adjusts problem difficulty to stay within the high-efficiency region. SEELE augments each training sample by appending a hint (part of a full solution) after the original problem. Unlike previous hint-based approaches, SEELE deliberately and adaptively adjusts the hint length for each problem to achieve an optimal difficulty. To determine the optimal hint length, SEELE employs a multi-round rollout sampling strategy. In each round, it fits an item response theory model to the accuracy-hint pairs collected in preceding rounds to predict the required hint length for the next round. This instance-level, real-time difficulty adjustment aligns problem difficulty with the evolving model capability, thereby improving exploration efficiency. Experimental results show that SEELE outperforms Group Relative Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5 points, respectively, and surpasses the best previous supervision-aided approach by +3.6 points on average across six math reasoning benchmarks.",
            "score": 15,
            "issue_id": 5806,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "da5bcd38585e0d46",
            "authors": [
                "Ziheng Li",
                "Zexu Sun",
                "Jinman Zhao",
                "Erxue Min",
                "Yongcheng Zeng",
                "Hui Wu",
                "Hengyi Cai",
                "Shuaiqiang Wang",
                "Dawei Yin",
                "Xu Chen",
                "Zhi-Hong Deng"
            ],
            "affiliations": [
                "Aerospace Information Research Institute, Chinese Academy of Sciences",
                "Baidu Inc.",
                "Department of Computer Science, University of Toronto",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Intelligence Science and Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06923.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#rl",
                    "#optimization",
                    "#training",
                    "#rlhf",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "SEELE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. SEELE Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SEELE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RLVR Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Dynamic Difficulty for Enhanced Learning Efficiency",
                    "desc": "SEELE is a new framework in reinforcement learning with verifiable rewards (RLVR) that improves how models learn to solve math problems by adjusting the difficulty of tasks dynamically. It does this by adding hints to problems, where the length of the hint is tailored to match the model's current abilities, ensuring that the challenges are neither too hard nor too easy. This adaptive hinting strategy helps the model explore more effectively and learn better by staying within an optimal difficulty range. Experiments show that SEELE significantly outperforms existing methods, leading to better performance in math reasoning tasks."
                },
                "zh": {
                    "title": "SEELEï¼šåŠ¨æ€è°ƒæ•´éš¾åº¦ï¼Œæå‡æ¨ç†æ•ˆç‡",
                    "desc": "SEELEæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€è°ƒæ•´é—®é¢˜éš¾åº¦æ¥æé«˜æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æ¢ç´¢æ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨åŸå§‹é—®é¢˜åé™„åŠ æç¤ºï¼ˆéƒ¨åˆ†å®Œæ•´è§£å†³æ–¹æ¡ˆï¼‰æ¥å¢å¼ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬ã€‚SEELEä¸ä»¥å¾€çš„æç¤ºæ–¹æ³•ä¸åŒï¼Œå®ƒæ ¹æ®æ¯ä¸ªé—®é¢˜çš„ç‰¹ç‚¹ï¼Œçµæ´»åœ°è°ƒæ•´æç¤ºé•¿åº¦ï¼Œä»¥å®ç°æœ€ä½³éš¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEELEåœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07414",
            "title": "Language Self-Play For Data-Free Training",
            "url": "https://huggingface.co/papers/2509.07414",
            "abstract": "Language Self-Play (LSP) enhances large language models' performance on instruction-following tasks through self-play, surpassing data-driven methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself - a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained models can not only enhance their performance on challenging tasks through self-play alone, but can also do so more effectively than data-driven baselines.",
            "score": 8,
            "issue_id": 5806,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 9",
                "zh": "9æœˆ9æ—¥"
            },
            "hash": "438447513c4c481f",
            "authors": [
                "Jakub Grudzien Kuba",
                "Mengting Gu",
                "Qi Ma",
                "Yuandong Tian",
                "Vijai Mohan"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07414.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#rl",
                    "#optimization",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Language Self-Play (LSP). LSP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ³Ñ€ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑĞ°Ğ¼Ğ¾Ğ¹ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LSP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ LLM."
                },
                "en": {
                    "title": "Empowering Models Through Self-Play: No Data Needed!",
                    "desc": "Language Self-Play (LSP) is a novel approach that improves large language models' (LLMs) ability to follow instructions without needing more training data. By using a game-theoretic framework, LSP allows models to play against themselves, which helps them develop stronger skills over time. This self-play method leads to better performance on instruction-following tasks compared to traditional data-driven methods. Experiments show that pretrained models can significantly enhance their capabilities through this self-play mechanism."
                },
                "zh": {
                    "title": "è¯­è¨€è‡ªæˆ‘å¯¹å¼ˆï¼šæ— æ•°æ®æå‡æ¨¡å‹æ€§èƒ½çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "è¯­è¨€è‡ªæˆ‘å¯¹å¼ˆï¼ˆLSPï¼‰æ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨éµå¾ªæŒ‡ä»¤ä»»åŠ¡ä¸Šè¡¨ç°çš„æ–¹æ³•ã€‚é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰é¢å¤–æ•°æ®çš„æƒ…å†µä¸‹æå‡è‡ªèº«èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åšå¼ˆè®ºæ¡†æ¶ï¼Œå°†æ¨¡å‹çš„è¡¨ç°è§†ä¸ºåœ¨ç«äº‰æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œä»è€Œä¿ƒä½¿æ›´å¼ºçš„ç­–ç•¥äº§ç”Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆå¯ä»¥æœ‰æ•ˆæé«˜åœ¨æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè¶…è¶Šäº†åŸºäºæ•°æ®çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07301",
            "title": "Causal Attention with Lookahead Keys",
            "url": "https://huggingface.co/papers/2509.07301",
            "abstract": "CASTLE, an attention mechanism that updates keys with future context while maintaining autoregressive properties, outperforms standard causal attention in language modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t In standard causal attention, each token's query, key, and value (QKV) are static and encode only preceding context. We introduce CAuSal aTtention with Lookahead kEys (CASTLE), an attention mechanism that continually updates each token's keys as the context unfolds. We term these updated keys lookahead keys because they belong to earlier positions yet integrate information from tokens that appear later relative to those positions, while strictly preserving the autoregressive property. Although the mechanism appears sequential, we derive a mathematical equivalence that avoids explicitly materializing lookahead keys at each position and enables efficient parallel training. On language modeling benchmarks, CASTLE consistently outperforms standard causal attention across model scales, reducing validation perplexity and improving performance on a range of downstream tasks.",
            "score": 4,
            "issue_id": 5807,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 9",
                "zh": "9æœˆ9æ—¥"
            },
            "hash": "2a5370a17853db77",
            "authors": [
                "Zhuoqing Song",
                "Peng Sun",
                "Huizhuo Yuan",
                "Quanquan Gu"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07301.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ°",
                "ru": {
                    "title": "CASTLE: Ğ’Ğ·Ğ³Ğ»ÑĞ´ Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CASTLE Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, CASTLE Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾. CASTLE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "CASTLE: Future Context for Smarter Attention in Language Models",
                    "desc": "CASTLE is a novel attention mechanism designed for language modeling that enhances the standard causal attention by updating keys with future context. Unlike traditional methods where keys are static and only consider past tokens, CASTLE introduces lookahead keys that incorporate information from future tokens while maintaining the autoregressive nature of the model. This allows for a more dynamic representation of context, leading to improved performance on various language tasks. The mechanism is efficient, enabling parallel training without the need to explicitly compute lookahead keys at each position, resulting in lower validation perplexity and better overall results."
                },
                "zh": {
                    "title": "CASTLEï¼šæœªæ¥ä¸Šä¸‹æ–‡çš„è‡ªå›å½’æ³¨æ„åŠ›æœºåˆ¶",
                    "desc": "CASTLEæ˜¯ä¸€ç§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒåœ¨ä¿æŒè‡ªå›å½’ç‰¹æ€§çš„åŒæ—¶ï¼Œä½¿ç”¨æœªæ¥ä¸Šä¸‹æ–‡æ›´æ–°é”®å€¼ã€‚ä¸æ ‡å‡†çš„å› æœæ³¨æ„åŠ›ä¸åŒï¼ŒCASTLEçš„æ¯ä¸ªä»¤ç‰Œçš„é”®ä¼šéšç€ä¸Šä¸‹æ–‡çš„å‘å±•è€Œä¸æ–­æ›´æ–°ã€‚æˆ‘ä»¬ç§°è¿™äº›æ›´æ–°åçš„é”®ä¸ºå‰ç»é”®ï¼Œå› ä¸ºå®ƒä»¬æ¥è‡ªäºè¾ƒæ—©çš„ä½ç½®ï¼Œä½†æ•´åˆäº†ç›¸å¯¹è¿™äº›ä½ç½®åé¢å‡ºç°çš„ä»¤ç‰Œçš„ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCASTLEåœ¨è¯­è¨€å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ ‡å‡†å› æœæ³¨æ„åŠ›ï¼Œé™ä½äº†éªŒè¯å›°æƒ‘åº¦ï¼Œå¹¶åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­æå‡äº†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07968",
            "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge",
            "url": "https://huggingface.co/papers/2509.07968",
            "abstract": "SimpleQA Verified is a refined benchmark for evaluating the factuality of Large Language Models, addressing issues in previous benchmarks and providing a more reliable evaluation tool.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It addresses critical limitations in OpenAI's benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reconciliation to produce a more reliable and challenging evaluation set, alongside improvements in the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. This work provides the research community with a higher-fidelity tool to track genuine progress in parametric model factuality and to mitigate hallucinations. The benchmark dataset, evaluation code, and leaderboard are available at: https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.",
            "score": 3,
            "issue_id": 5806,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 9",
                "zh": "9æœˆ9æ—¥"
            },
            "hash": "7fb6599f8657bb35",
            "authors": [
                "Lukas Haas",
                "Gal Yona",
                "Giovanni D'Antonio",
                "Sasha Goldshtein",
                "Dipanjan Das"
            ],
            "affiliations": [
                "Google DeepMind, Google Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07968.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#interpretability",
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "SimpleQA Verified - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸, Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ‚ĞµĞ¼ Ğ¸ ÑĞ²ĞµÑ€ĞºÑƒ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemini 2.5 Pro Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ F1-Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ Ğ² 55.6, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Elevating Factuality Evaluation for Language Models",
                    "desc": "SimpleQA Verified is a new benchmark designed to assess the factual accuracy of Large Language Models (LLMs) more effectively. It improves upon previous benchmarks by eliminating issues like incorrect labels and biases, ensuring a more reliable evaluation process. The benchmark consists of 1,000 carefully curated prompts that have undergone rigorous filtering to enhance their quality and challenge. With this tool, researchers can better measure the factual performance of models like Gemini 2.5 Pro, which has achieved a leading F1-score, thus helping to reduce the occurrence of hallucinations in AI-generated content."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹äº‹å®æ€§è¯„ä¼°çš„åŸºå‡†å·¥å…·",
                    "desc": "SimpleQA Verified æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çŸ­æ–‡æœ¬äº‹å®æ€§çš„åŸºå‡†ï¼ŒåŒ…å«1000ä¸ªæç¤ºã€‚å®ƒè§£å†³äº†OpenAIåŸºå‡†ä¸­çš„ä¸€äº›å…³é”®é—®é¢˜ï¼Œå¦‚æ ‡ç­¾å™ªå£°ã€ä¸»é¢˜åè§å’Œé—®é¢˜å†—ä½™ã€‚é€šè¿‡ä¸¥æ ¼çš„å¤šé˜¶æ®µè¿‡æ»¤è¿‡ç¨‹ï¼ŒSimpleQA Verified æä¾›äº†ä¸€ä¸ªæ›´å¯é å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„è¯„ä¼°é›†ï¼Œå¹¶æ”¹è¿›äº†è‡ªåŠ¨è¯„åˆ†æç¤ºã€‚è¯¥åŸºå‡†ä½¿ç ”ç©¶ç¤¾åŒºèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è·Ÿè¸ªå‚æ•°æ¨¡å‹çš„äº‹å®æ€§è¿›å±•ï¼Œå¹¶å‡å°‘å¹»è§‰ç°è±¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06942",
            "title": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human\n  Preference",
            "url": "https://huggingface.co/papers/2509.06942",
            "abstract": "Direct-Align and Semantic Relative Preference Optimization improve diffusion models' alignment with human preferences by reducing computational costs and minimizing offline reward adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.",
            "score": 3,
            "issue_id": 5815,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "666a0df353939fc4",
            "authors": [
                "Xiangwei Shen",
                "Zhimin Li",
                "Zhantao Yang",
                "Shiyi Zhang",
                "Yingfang Zhang",
                "Donghao Li",
                "Chunyu Wang",
                "Qinglin Lu",
                "Yansong Tang"
            ],
            "affiliations": [
                "Hunyuan, Tencent",
                "School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen",
                "Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06942.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#alignment",
                    "#rlhf",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Direct-Align Ğ¸ Semantic Relative Preference Optimization (SRPO). Direct-Align Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¾Ñ€ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ…. SRPO Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ FLUX.1.dev ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 3 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ»ÑĞ´ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Diffusion Models with Direct-Align and SRPO",
                    "desc": "This paper presents two innovative methods, Direct-Align and Semantic Relative Preference Optimization (SRPO), to enhance diffusion models' alignment with human preferences. Direct-Align simplifies the process of recovering original images by using a predefined noise prior, which reduces the need for costly multistep denoising. SRPO allows for real-time adjustments of rewards based on text prompts, minimizing the need for extensive offline reward model adaptations. Together, these methods significantly improve the aesthetic quality and realism of generated images while lowering computational costs."
                },
                "zh": {
                    "title": "æå‡æ‰©æ•£æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½",
                    "desc": "æœ¬æ–‡æå‡ºäº†Direct-Alignå’Œè¯­ä¹‰ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆSRPOï¼‰ä¸¤ç§æ–¹æ³•ï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½åº¦ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚Direct-Aligné€šè¿‡é¢„å®šä¹‰å™ªå£°æ¥æœ‰æ•ˆæ¢å¤åŸå§‹å›¾åƒï¼Œé¿å…äº†åœ¨åæœŸæ—¶é—´æ­¥çš„è¿‡åº¦ä¼˜åŒ–ã€‚SRPOåˆ™å°†å¥–åŠ±ä¿¡å·ä¸æ–‡æœ¬æ¡ä»¶ç›¸ç»“åˆï¼Œå®ç°äº†åœ¨çº¿è°ƒæ•´å¥–åŠ±ï¼Œä»è€Œå‡å°‘äº†å¯¹ç¦»çº¿å¥–åŠ±å¾®è°ƒçš„ä¾èµ–ã€‚é€šè¿‡ä¼˜åŒ–å»å™ªå’Œåœ¨çº¿å¥–åŠ±è°ƒæ•´ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†FLUX.1.devæ¨¡å‹åœ¨çœŸå®æ„Ÿå’Œç¾å­¦è´¨é‡ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01624",
            "title": "Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with\n  Quantization-Aware Scheduling",
            "url": "https://huggingface.co/papers/2509.01624",
            "abstract": "Q-Sched, a novel post-training quantization method for diffusion models, reduces model size by 4x while maintaining full-precision accuracy and improving image quality metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models are computationally intensive, often requiring dozens of forward passes through large transformer backbones. For instance, Stable Diffusion XL generates high-quality images with 50 evaluations of a 2.6B-parameter model, an expensive process even for a single batch. Few-step diffusion models reduce this cost to 2-8 denoising steps but still depend on large, uncompressed U-Net or diffusion transformer backbones, which are often too costly for full-precision inference without datacenter GPUs. These requirements also limit existing post-training quantization methods that rely on full-precision calibration. We introduce Q-Sched, a new paradigm for post-training quantization that modifies the diffusion model scheduler rather than model weights. By adjusting the few-step sampling trajectory, Q-Sched achieves full-precision accuracy with a 4x reduction in model size. To learn quantization-aware pre-conditioning coefficients, we propose the JAQ loss, which combines text-image compatibility with an image quality metric for fine-grained optimization. JAQ is reference-free and requires only a handful of calibration prompts, avoiding full-precision inference during calibration. Q-Sched delivers substantial gains: a 15.5% FID improvement over the FP16 4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step Phased Consistency Model, showing that quantization and few-step distillation are complementary for high-fidelity generation. A large-scale user study with more than 80,000 annotations further confirms Q-Sched's effectiveness on both FLUX.1[schnell] and SDXL-Turbo.",
            "score": 3,
            "issue_id": 5809,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "377f1ad33c67cc37",
            "authors": [
                "Natalia Frumkin",
                "Diana Marculescu"
            ],
            "affiliations": [
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01624.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Q-Sched: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Q-Sched - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² 4 Ñ€Ğ°Ğ·Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Q-Sched Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ½Ğµ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ JAQ-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ FID Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Q-Sched: Efficient Quantization for High-Quality Diffusion Models",
                    "desc": "Q-Sched is a new method for post-training quantization specifically designed for diffusion models, which helps to significantly reduce the model size by 4 times while keeping the accuracy intact. This method modifies the diffusion model scheduler instead of changing the model weights, allowing for efficient few-step sampling that maintains full-precision performance. It introduces the JAQ loss, which optimizes quantization-aware pre-conditioning coefficients by focusing on text-image compatibility and image quality without needing full-precision calibration. The results show that Q-Sched not only improves image quality metrics but also demonstrates that quantization and few-step distillation can work together effectively for high-quality image generation."
                },
                "zh": {
                    "title": "Q-Schedï¼šé‡åŒ–ä¸é«˜ä¿çœŸç”Ÿæˆçš„å®Œç¾ç»“åˆ",
                    "desc": "Q-Schedæ˜¯ä¸€ç§æ–°é¢–çš„åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œä¸“ä¸ºæ‰©æ•£æ¨¡å‹è®¾è®¡ã€‚å®ƒé€šè¿‡è°ƒæ•´æ‰©æ•£æ¨¡å‹çš„è°ƒåº¦å™¨ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¿®æ”¹æ¨¡å‹æƒé‡ï¼Œå®ç°äº†æ¨¡å‹å¤§å°å‡å°‘4å€ï¼ŒåŒæ—¶ä¿æŒå…¨ç²¾åº¦çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†JAQæŸå¤±å‡½æ•°ï¼Œç»“åˆæ–‡æœ¬-å›¾åƒå…¼å®¹æ€§å’Œå›¾åƒè´¨é‡æŒ‡æ ‡ï¼Œè¿›è¡Œç²¾ç»†ä¼˜åŒ–ã€‚Q-Schedåœ¨å¤šä¸ªå®éªŒä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†é‡åŒ–å’Œå°‘æ­¥è’¸é¦åœ¨é«˜ä¿çœŸç”Ÿæˆä¸­çš„äº’è¡¥æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07558",
            "title": "Î”L Normalization: Rethink Loss Aggregation in RLVR",
            "url": "https://huggingface.co/papers/2509.07558",
            "abstract": "Î”L Normalization addresses gradient variance in Reinforcement Learning with Verifiable Rewards by providing an unbiased policy loss estimate with minimal variance.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Delta L Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed Delta L Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at https://github.com/zerolllin/Delta-L-Normalization.",
            "score": 2,
            "issue_id": 5812,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 9",
                "zh": "9æœˆ9æ—¥"
            },
            "hash": "d915dc7f29ac41a8",
            "authors": [
                "Zhiyuan He",
                "Xufang Luo",
                "Yike Zhang",
                "Yuqing Yang",
                "Lili Qiu"
            ],
            "affiliations": [
                "Microsoft Research",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07558.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Delta L Normalization",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Delta L Normalization Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Delta L Normalization Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞ¼ĞµÑ‰ĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Minimizing Variance for Unbiased Policy Loss in RL",
                    "desc": "Delta L Normalization is a novel method designed to reduce gradient variance in Reinforcement Learning with Verifiable Rewards (RLVR). It addresses the challenge of high variability in response lengths during training, which can lead to unstable optimization. Unlike previous methods that either introduce bias or fail to minimize variance, Delta L Normalization provides an unbiased estimate of policy loss while effectively reducing gradient variance. Experimental results demonstrate its effectiveness across various model sizes and tasks, showcasing its potential to enhance the performance of large language models."
                },
                "zh": {
                    "title": "Î”Lå½’ä¸€åŒ–ï¼šç¨³å®šå¼ºåŒ–å­¦ä¹ çš„æ— åæŸå¤±ä¼°è®¡",
                    "desc": "Î”Lå½’ä¸€åŒ–æ˜¯ä¸€ç§é’ˆå¯¹å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ä¸­æ¢¯åº¦æ–¹å·®é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚å®ƒé€šè¿‡æä¾›æ— åçš„ç­–ç•¥æŸå¤±ä¼°è®¡ï¼Œæ˜¾è‘—é™ä½äº†æ¢¯åº¦æ–¹å·®ï¼Œä»è€Œå®ç°æ›´ç¨³å®šçš„ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºåŠ¨æ€ç”Ÿæˆé•¿åº¦çš„æƒ…å†µï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¹å–„å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒÎ”Lå½’ä¸€åŒ–åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ã€æœ€å¤§é•¿åº¦å’Œä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-09.html",
    "link_next": "2025-09-11.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "09.09",
        "en": "09/09",
        "zh": "9æœˆ9æ—¥"
    },
    "short_date_next": {
        "ru": "11.09",
        "en": "09/11",
        "zh": "9æœˆ11æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 3,
        "#rl": 6,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 12,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 6,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}