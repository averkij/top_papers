{
    "date": {
        "ru": "10 сентября",
        "en": "September 10",
        "zh": "9月10日"
    },
    "time_utc": "2025-09-10 02:15",
    "weekday": 2,
    "issue_id": 5806,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.07980",
            "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.07980",
            "abstract": "Parallel-R1, a reinforcement learning framework, enhances large language models' reasoning capabilities by enabling parallel thinking through a progressive curriculum, leading to significant performance improvements on math benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a mid-training exploration scaffold, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1.",
            "score": 22,
            "issue_id": 5806,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 сентября",
                "en": "September 9",
                "zh": "9月9日"
            },
            "hash": "41def489cc53d3e0",
            "authors": [
                "Tong Zheng",
                "Hongming Zhang",
                "Wenhao Yu",
                "Xiaoyang Wang",
                "Xinyu Yang",
                "Runpeng Dai",
                "Rui Liu",
                "Huiwen Bao",
                "Chengsong Huang",
                "Heng Huang",
                "Dong Yu"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "City University of Hong Kong",
                "Tencent AI Lab Seattle",
                "University of Maryland, College Park",
                "University of North Carolina at Chapel Hill",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07980.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#open_source",
                    "#rl",
                    "#optimization",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Параллельное мышление для ИИ: новый уровень рассуждений",
                    "desc": "Parallel-R1 - это фреймворк обучения с подкреплением, который улучшает способности больших языковых моделей к рассуждениям путем параллельного мышления. Он использует прогрессивную учебную программу, начиная с обучения на более простых задачах и переходя к более сложным. Эксперименты показали значительное улучшение точности на математических бенчмарках по сравнению с последовательным мышлением. Анализ выявил, что модель использует параллельное мышление сначала как стратегию исследования, а затем для многоаспектной проверки."
                },
                "en": {
                    "title": "Unlocking Reasoning Power with Parallel Thinking",
                    "desc": "The paper introduces Parallel-R1, a reinforcement learning framework designed to improve the reasoning abilities of large language models (LLMs) by facilitating parallel thinking. This approach allows the model to explore multiple reasoning paths simultaneously, which enhances its performance on complex tasks. Unlike traditional methods that rely on supervised fine-tuning, Parallel-R1 employs a progressive curriculum that first trains the model on simpler tasks before transitioning to more challenging ones using reinforcement learning. The results demonstrate significant accuracy improvements on various math benchmarks, showcasing the effectiveness of parallel thinking in enhancing model performance."
                },
                "zh": {
                    "title": "并行思维：提升推理能力的新方法",
                    "desc": "Parallel-R1是一个强化学习框架，旨在通过并行思维来增强大型语言模型的推理能力。该框架采用渐进式课程，解决了在训练并行思维时的冷启动问题。通过在简单任务上进行监督微调，模型学习并行思维能力，然后转向强化学习以应对更复杂的问题。实验结果表明，Parallel-R1在数学基准测试中显著提高了模型的准确性，展示了并行思维在推理任务中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06923",
            "title": "Staying in the Sweet Spot: Responsive Reasoning Evolution via\n  Capability-Adaptive Hint Scaffolding",
            "url": "https://huggingface.co/papers/2509.06923",
            "abstract": "SEELE, a novel RLVR framework, dynamically adjusts problem difficulty using adaptive hint lengths to enhance exploration efficiency and improve performance in math reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success in enhancing the reasoning capabilities of large language models (LLMs). However, existing RLVR methods often suffer from exploration inefficiency due to mismatches between the training data's difficulty and the model's capability. LLMs fail to discover viable reasoning paths when problems are overly difficult, while learning little new capability when problems are too simple. In this work, we formalize the impact of problem difficulty by quantifying the relationship between loss descent speed and rollout accuracy. Building on this analysis, we propose SEELE, a novel supervision-aided RLVR framework that dynamically adjusts problem difficulty to stay within the high-efficiency region. SEELE augments each training sample by appending a hint (part of a full solution) after the original problem. Unlike previous hint-based approaches, SEELE deliberately and adaptively adjusts the hint length for each problem to achieve an optimal difficulty. To determine the optimal hint length, SEELE employs a multi-round rollout sampling strategy. In each round, it fits an item response theory model to the accuracy-hint pairs collected in preceding rounds to predict the required hint length for the next round. This instance-level, real-time difficulty adjustment aligns problem difficulty with the evolving model capability, thereby improving exploration efficiency. Experimental results show that SEELE outperforms Group Relative Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5 points, respectively, and surpasses the best previous supervision-aided approach by +3.6 points on average across six math reasoning benchmarks.",
            "score": 12,
            "issue_id": 5806,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 сентября",
                "en": "September 8",
                "zh": "9月8日"
            },
            "hash": "da5bcd38585e0d46",
            "authors": [
                "Ziheng Li",
                "Zexu Sun",
                "Jinman Zhao",
                "Erxue Min",
                "Yongcheng Zeng",
                "Hui Wu",
                "Hengyi Cai",
                "Shuaiqiang Wang",
                "Dawei Yin",
                "Xu Chen",
                "Zhi-Hong Deng"
            ],
            "affiliations": [
                "Aerospace Information Research Institute, Chinese Academy of Sciences",
                "Baidu Inc.",
                "Department of Computer Science, University of Toronto",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Intelligence Science and Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06923.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#rl",
                    "#optimization",
                    "#training",
                    "#rlhf",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Динамическая настройка сложности для эффективного обучения ИИ математическому мышлению",
                    "desc": "SEELE - это новая система обучения с подкреплением с проверяемыми наградами (RLVR), которая динамически регулирует сложность задач для улучшения исследовательской эффективности в задачах математического рассуждения. Система использует адаптивную длину подсказок, чтобы поддерживать оптимальный уровень сложности для обучающейся языковой модели. SEELE применяет многораундовую стратегию сэмплирования и теорию ответов на вопросы для определения оптимальной длины подсказки. Эксперименты показывают, что SEELE превосходит существующие методы RLVR и обучения с учителем на нескольких эталонных тестах по математическому рассуждению."
                },
                "en": {
                    "title": "Dynamic Difficulty for Enhanced Learning Efficiency",
                    "desc": "SEELE is a new framework in reinforcement learning with verifiable rewards (RLVR) that improves how models learn to solve math problems by adjusting the difficulty of tasks dynamically. It does this by adding hints to problems, where the length of the hint is tailored to match the model's current abilities, ensuring that the challenges are neither too hard nor too easy. This adaptive hinting strategy helps the model explore more effectively and learn better by staying within an optimal difficulty range. Experiments show that SEELE significantly outperforms existing methods, leading to better performance in math reasoning tasks."
                },
                "zh": {
                    "title": "SEELE：动态调整难度，提升推理效率",
                    "desc": "SEELE是一种新颖的强化学习可验证奖励（RLVR）框架，旨在通过动态调整问题难度来提高数学推理任务中的探索效率。该框架通过在原始问题后附加提示（部分完整解决方案）来增强每个训练样本。SEELE与以往的提示方法不同，它根据每个问题的特点，灵活地调整提示长度，以实现最佳难度。实验结果表明，SEELE在六个数学推理基准测试中，表现优于其他方法，显著提高了模型的推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07969",
            "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search",
            "url": "https://huggingface.co/papers/2509.07969",
            "abstract": "Mini-o3, a system for deep, multi-turn reasoning in visual search tasks, uses an iterative data collection pipeline and over-turn masking strategy to achieve state-of-the-art performance with rich reasoning patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.",
            "score": 2,
            "issue_id": 5806,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 сентября",
                "en": "September 9",
                "zh": "9月9日"
            },
            "hash": "4d29daad3dc9ac61",
            "authors": [
                "Xin Lai",
                "Junyi Li",
                "Wei Li",
                "Tao Liu",
                "Tianjian Li",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "ByteDance",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07969.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#cv",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Глубокое мышление в визуальном поиске: Mini-o3 раздвигает границы рассуждений",
                    "desc": "Система Mini-o3 представляет собой подход к глубокому многоступенчатому рассуждению в задачах визуального поиска. Она использует итеративный конвейер сбора данных и стратегию маскирования избыточных шагов для достижения наилучших результатов с богатыми паттернами рассуждений. Несмотря на обучение с ограничением в шесть шагов взаимодействия, модель генерирует траектории, естественно масштабируемые до десятков шагов при выводе. Эксперименты показывают, что Mini-o3 эффективно решает сложные задачи визуального поиска, демонстрируя глубокие пути рассуждений."
                },
                "en": {
                    "title": "Unlocking Deep Reasoning in Visual Search with Mini-o3",
                    "desc": "Mini-o3 is a novel system designed for deep reasoning in visual search tasks, enabling multi-turn interactions that enhance problem-solving capabilities. It utilizes an iterative data collection pipeline and an over-turn masking strategy to improve performance and reasoning diversity. By constructing the Visual Probe Dataset, Mini-o3 addresses the limitations of existing models that struggle with complex tasks requiring extensive exploration. The system achieves state-of-the-art results by allowing for a greater number of reasoning steps during inference, leading to more accurate and nuanced solutions."
                },
                "zh": {
                    "title": "Mini-o3：深度多轮推理的视觉搜索新突破",
                    "desc": "Mini-o3是一个用于视觉搜索任务的深度多轮推理系统，采用迭代数据收集管道和超轮掩蔽策略，达到了最先进的性能。该系统通过构建视觉探测数据集，设计了数千个具有挑战性的视觉搜索问题，以支持探索性推理。Mini-o3能够执行深度的多轮推理，处理数十个步骤，克服了现有方法在交互轮次和推理模式上的局限性。实验结果表明，Mini-o3能够生成丰富的推理模式和深度思考路径，有效解决复杂的视觉搜索问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07979",
            "title": "Visual Representation Alignment for Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2509.07979",
            "abstract": "VIRAL, a regularization strategy, aligns MLLMs' visual representations with pre-trained VFMs, enhancing performance on vision-centric tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs.",
            "score": 1,
            "issue_id": 5806,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 сентября",
                "en": "September 9",
                "zh": "9月9日"
            },
            "hash": "f229168a41923a52",
            "authors": [
                "Heeji Yoon",
                "Jaewoo Jung",
                "Junwan Kim",
                "Hyungyu Choi",
                "Heeseong Shin",
                "Sangbeom Lim",
                "Honggyu An",
                "Chaehyun Kim",
                "Jisang Han",
                "Donghyun Kim",
                "Chanho Eom",
                "Sunghwan Hong",
                "Seungryong Kim"
            ],
            "affiliations": [
                "Chung-Ang University",
                "ETH Zurich",
                "KAIST AI",
                "Korea University",
                "NYU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07979.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#alignment",
                    "#training",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Усиление визуального понимания языковых моделей через выравнивание представлений",
                    "desc": "Статья представляет VIRAL - стратегию регуляризации для мультимодальных больших языковых моделей (MLLM). VIRAL выравнивает внутренние визуальные представления MLLM с предобученными визуальными фундаментальными моделями (VFM). Это позволяет MLLM сохранять важные визуальные детали и дополнять их знаниями из VFM. Эксперименты показывают улучшение результатов на различных мультимодальных задачах."
                },
                "en": {
                    "title": "Aligning Visual Representations for Better Multimodal Learning",
                    "desc": "This paper introduces VIRAL, a regularization technique that improves the performance of multimodal large language models (MLLMs) on vision-related tasks. The authors identify that MLLMs struggle with tasks like object counting due to a lack of direct visual supervision, which leads to the loss of important visual details. VIRAL addresses this issue by aligning the visual representations of MLLMs with those from pre-trained vision foundation models (VFMs), ensuring that the models retain critical visual information. The results show that this alignment significantly enhances the models' reasoning capabilities on complex visual inputs across various benchmarks."
                },
                "zh": {
                    "title": "VIRAL：提升视觉任务表现的对齐策略",
                    "desc": "本文提出了一种名为VIRAL的正则化策略，旨在将多模态大语言模型（MLLMs）的视觉表示与预训练的视觉基础模型（VFMs）对齐，从而提升在视觉任务上的表现。我们发现，传统的文本监督方法对视觉路径的指导有限，导致模型在训练过程中忽视了细致的视觉信息。通过强制对齐内部视觉表示，VIRAL不仅帮助模型保留输入视觉编码器中的重要细节，还能补充来自VFMs的额外视觉知识。实验结果表明，VIRAL在多项广泛采用的多模态基准测试中均取得了一致的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07968",
            "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge",
            "url": "https://huggingface.co/papers/2509.07968",
            "abstract": "SimpleQA Verified is a refined benchmark for evaluating the factuality of Large Language Models, addressing issues in previous benchmarks and providing a more reliable evaluation tool.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It addresses critical limitations in OpenAI's benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reconciliation to produce a more reliable and challenging evaluation set, alongside improvements in the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. This work provides the research community with a higher-fidelity tool to track genuine progress in parametric model factuality and to mitigate hallucinations. The benchmark dataset, evaluation code, and leaderboard are available at: https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.",
            "score": 1,
            "issue_id": 5806,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 сентября",
                "en": "September 9",
                "zh": "9月9日"
            },
            "hash": "7fb6599f8657bb35",
            "authors": [
                "Lukas Haas",
                "Gal Yona",
                "Giovanni D'Antonio",
                "Sasha Goldshtein",
                "Dipanjan Das"
            ],
            "affiliations": [
                "Google DeepMind, Google Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07968.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#interpretability",
                    "#benchmark"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Точный бенчмарк для оценки фактической достоверности языковых моделей",
                    "desc": "SimpleQA Verified - это усовершенствованный бенчмарк для оценки фактической точности больших языковых моделей (LLM). Он устраняет недостатки предыдущих бенчмарков, включая шумные и неправильные метки, тематические смещения и избыточность вопросов. Бенчмарк был создан с помощью многоступенчатого процесса фильтрации, включающего дедупликацию, балансировку тем и сверку источников. На этом новом бенчмарке модель Gemini 2.5 Pro достигла наилучшего F1-показателя в 55.6, превзойдя другие передовые модели."
                },
                "en": {
                    "title": "Elevating Factuality Evaluation for Language Models",
                    "desc": "SimpleQA Verified is a new benchmark designed to assess the factual accuracy of Large Language Models (LLMs) more effectively. It improves upon previous benchmarks by eliminating issues like incorrect labels and biases, ensuring a more reliable evaluation process. The benchmark consists of 1,000 carefully curated prompts that have undergone rigorous filtering to enhance their quality and challenge. With this tool, researchers can better measure the factual performance of models like Gemini 2.5 Pro, which has achieved a leading F1-score, thus helping to reduce the occurrence of hallucinations in AI-generated content."
                },
                "zh": {
                    "title": "提升大型语言模型事实性评估的基准工具",
                    "desc": "SimpleQA Verified 是一个用于评估大型语言模型（LLM）短文本事实性的基准，包含1000个提示。它解决了OpenAI基准中的一些关键问题，如标签噪声、主题偏见和问题冗余。通过严格的多阶段过滤过程，SimpleQA Verified 提供了一个更可靠和具有挑战性的评估集，并改进了自动评分提示。该基准使研究社区能够更准确地跟踪参数模型的事实性进展，并减少幻觉现象。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07414",
            "title": "Language Self-Play For Data-Free Training",
            "url": "https://huggingface.co/papers/2509.07414",
            "abstract": "Language Self-Play (LSP) enhances large language models' performance on instruction-following tasks through self-play, surpassing data-driven methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself - a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained models can not only enhance their performance on challenging tasks through self-play alone, but can also do so more effectively than data-driven baselines.",
            "score": 1,
            "issue_id": 5806,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 сентября",
                "en": "September 9",
                "zh": "9月9日"
            },
            "hash": "438447513c4c481f",
            "authors": [
                "Jakub Grudzien Kuba",
                "Mengting Gu",
                "Qi Ma",
                "Yuandong Tian",
                "Vijai Mohan"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07414.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#rl",
                    "#optimization",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Самоигра языковых моделей: путь к улучшению без новых данных",
                    "desc": "Статья представляет новый метод улучшения больших языковых моделей (LLM) под названием Language Self-Play (LSP). LSP использует принципы теории игр и самообучения, позволяя модели соревноваться с самой собой для улучшения своих навыков. Эксперименты показали, что LSP превосходит традиционные методы, основанные на дополнительных данных. Этот подход может помочь преодолеть ограничения, связанные с необходимостью постоянного увеличения объема обучающих данных для LLM."
                },
                "en": {
                    "title": "Empowering Models Through Self-Play: No Data Needed!",
                    "desc": "Language Self-Play (LSP) is a novel approach that improves large language models' (LLMs) ability to follow instructions without needing more training data. By using a game-theoretic framework, LSP allows models to play against themselves, which helps them develop stronger skills over time. This self-play method leads to better performance on instruction-following tasks compared to traditional data-driven methods. Experiments show that pretrained models can significantly enhance their capabilities through this self-play mechanism."
                },
                "zh": {
                    "title": "语言自我对弈：无数据提升模型性能的创新方法",
                    "desc": "语言自我对弈（LSP）是一种增强大型语言模型在遵循指令任务上表现的方法。通过自我对弈，模型能够在没有额外数据的情况下提升自身能力。该方法利用博弈论框架，将模型的表现视为在竞争游戏中的表现，从而促使更强的策略产生。实验结果表明，预训练模型通过自我对弈可以有效提高在挑战性任务上的表现，超越了基于数据的方法。"
                }
            }
        }
    ],
    "link_prev": "2025-09-09.html",
    "link_next": "2025-09-11.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "09.09",
        "en": "09/09",
        "zh": "9月9日"
    },
    "short_date_next": {
        "ru": "11.09",
        "en": "09/11",
        "zh": "9月11日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 2,
        "#rl": 4,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}