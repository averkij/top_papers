{
    "date": {
        "ru": "4 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 4",
        "zh": "11æœˆ4æ—¥"
    },
    "time_utc": "2024-11-04 08:16",
    "weekday": 0,
    "issue_id": 410,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.23218",
            "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
            "url": "https://huggingface.co/papers/2410.23218",
            "abstract": "Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas - a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs.",
            "score": 18,
            "issue_id": 410,
            "pub_date": "2024-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "d7a3f0fd08f934d5",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#agents",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "OS-Atlas: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ GUI",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ OS-Atlas - Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ (GUI). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ GUI Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ (OOD). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ GUI Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ¸ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ĞºÑ€Ğ¾ÑÑ-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 13 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² GUI. OS-Atlas Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ, Ğ½Ğ°ÑÑ‚Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ²ĞµĞ±-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹."
                },
                "en": {
                    "title": "Empowering Open-Source GUI Agents with OS-Atlas",
                    "desc": "This paper introduces OS-Atlas, an open-source foundational model designed for GUI grounding and Out-Of-Distribution (OOD) tasks. It addresses the performance gap between commercial Vision-Language Models (VLMs) and open-source alternatives by providing a comprehensive toolkit for synthesizing GUI grounding data across various platforms. The authors present the largest open-source cross-platform GUI grounding dataset, featuring over 13 million GUI elements, which enhances the model's ability to understand and generalize from GUI screenshots. Extensive evaluations show that OS-Atlas outperforms previous models, offering insights for further advancements in open-source VLM capabilities."
                },
                "zh": {
                    "title": "å¼€æºGUIæ¨¡å‹OS-Atlasï¼šæå‡ç•Œé¢ç†è§£èƒ½åŠ›çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†OS-Atlasï¼Œä¸€ä¸ªå¼€æºçš„GUIåŠ¨ä½œæ¨¡å‹ï¼Œä¸“æ³¨äºGUIå®šä½å’Œè¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå·¥å…·åŒ…ï¼Œå¯ä»¥åœ¨å¤šä¸ªå¹³å°ä¸ŠåˆæˆGUIå®šä½æ•°æ®ï¼ŒåŒ…æ‹¬Windowsã€Linuxã€MacOSã€Androidå’Œç½‘é¡µã€‚OS-Atlasåˆ©ç”¨è¶…è¿‡1300ä¸‡ä¸ªGUIå…ƒç´ çš„æ•°æ®é›†ï¼Œç»“åˆåˆ›æ–°çš„æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å¯¹GUIæˆªå›¾çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼ŒOS-Atlasåœ¨ç§»åŠ¨ã€æ¡Œé¢å’Œç½‘é¡µå¹³å°ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00412",
            "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation",
            "url": "https://huggingface.co/papers/2411.00412",
            "abstract": "Large Language Models (LLMs) demonstrate promising capabilities in solving simple scientific problems but often produce hallucinations for complex ones. While integrating LLMs with tools can increase reliability, this approach typically results in over-reliance on tools, diminishing the model's ability to solve simple problems through basic reasoning. In contrast, human experts first assess problem complexity using domain knowledge before choosing an appropriate solution approach. Inspired by this human problem-solving process, we propose a novel two-component fine-tuning method. In the first component World Knowledge Distillation (WKD), LLMs learn directly from solutions generated using tool's information to internalize domain knowledge. In the second component Tool Usage Adaptation (TUA), we partition problems into easy and hard categories based on the model's direct answering accuracy. While maintaining the same alignment target for easy problems as in WKD, we train the model to intelligently switch to tool usage for more challenging problems. We validate our method on six scientific benchmark datasets, spanning mathematics, climate science and epidemiology. On average, our models demonstrate a 28.18% improvement in answer accuracy and a 13.89% increase in tool usage precision across all datasets, surpassing state-of-the-art models including GPT-4o and Claude-3.5.",
            "score": 7,
            "issue_id": 407,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 1",
                "zh": "11æœˆ1æ—¥"
            },
            "hash": "27e4deefc7d09df0",
            "data": {
                "categories": [
                    "#rlhf",
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… - Ğ¿Ñ€Ğ¸Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing LLMs: Smart Tool Use for Complex Problems",
                    "desc": "This paper addresses the limitations of Large Language Models (LLMs) in solving complex scientific problems, which often lead to inaccuracies or 'hallucinations'. The authors propose a two-component fine-tuning method that mimics human problem-solving strategies by first assessing problem complexity. The first component, World Knowledge Distillation (WKD), allows LLMs to learn from solutions that utilize external tools, while the second component, Tool Usage Adaptation (TUA), helps the model categorize problems as easy or hard and decide when to use tools. The proposed method shows significant improvements in accuracy and tool usage precision across various scientific datasets, outperforming existing models."
                },
                "zh": {
                    "title": "æ™ºèƒ½åˆ‡æ¢ï¼Œæå‡æ¨¡å‹è§£å†³é—®é¢˜çš„èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³ç®€å•ç§‘å­¦é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚é—®é¢˜ä¸Šå¸¸å¸¸å‡ºç°å¹»è§‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒç»„ä»¶å¾®è°ƒæ–¹æ³•ï¼Œæ¨¡ä»¿äººç±»ä¸“å®¶çš„è§£å†³é—®é¢˜è¿‡ç¨‹ã€‚ç¬¬ä¸€ä¸ªç»„ä»¶æ˜¯ä¸–ç•ŒçŸ¥è¯†è’¸é¦ï¼ˆWKDï¼‰ï¼Œä½¿LLMsä»å·¥å…·ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆä¸­å­¦ä¹ é¢†åŸŸçŸ¥è¯†ã€‚ç¬¬äºŒä¸ªç»„ä»¶æ˜¯å·¥å…·ä½¿ç”¨é€‚åº”ï¼ˆTUAï¼‰ï¼Œæ ¹æ®æ¨¡å‹çš„ç›´æ¥å›ç­”å‡†ç¡®æ€§å°†é—®é¢˜åˆ†ä¸ºç®€å•å’Œå›°éš¾ä¸¤ç±»ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å¤æ‚é—®é¢˜ä¸Šçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00776",
            "title": "Randomized Autoregressive Visual Generation",
            "url": "https://huggingface.co/papers/2411.00776",
            "abstract": "This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence-typically ordered in raster form-is randomly permuted into different factorization orders with a probability r, where r starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model's capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models will be made available at https://github.com/bytedance/1d-tokenizer",
            "score": 6,
            "issue_id": 408,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 1",
                "zh": "11æœˆ1æ—¥"
            },
            "hash": "0cc2c0f19f735f79",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Randomized AutoRegressive modeling (RAR) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. RAR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ImageNet-256 Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¼ FID 1.48. RAR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Randomized AutoRegressive Modeling",
                    "desc": "This paper introduces Randomized AutoRegressive modeling (RAR), a novel approach for generating images that enhances performance while remaining compatible with existing language modeling techniques. RAR employs a unique training method where the input sequence is randomly shuffled during the autoregressive training process, allowing the model to learn from various factorization orders. This strategy helps the model to better understand and utilize bidirectional contexts, leading to improved image generation capabilities. The results show that RAR achieves a remarkable FID score of 1.48 on the ImageNet-256 benchmark, outperforming previous state-of-the-art methods in both autoregressive and diffusion-based image generation."
                },
                "zh": {
                    "title": "éšæœºè‡ªå›å½’å»ºæ¨¡ï¼šå›¾åƒç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§éšæœºè‡ªå›å½’å»ºæ¨¡ï¼ˆRARï¼‰æ–¹æ³•ç”¨äºè§†è§‰ç”Ÿæˆï¼Œåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶ä¸è¯­è¨€å»ºæ¨¡æ¡†æ¶å®Œå…¨å…¼å®¹ã€‚RARæ–¹æ³•ç®€å•ï¼šåœ¨æ ‡å‡†çš„è‡ªå›å½’è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¾“å…¥åºåˆ—é€šå¸¸æŒ‰å…‰æ …å½¢å¼æ’åˆ—ï¼Œä½†ä»¥æ¦‚ç‡réšæœºæ‰“ä¹±ä¸ºä¸åŒçš„å› å­åŒ–é¡ºåºï¼Œrä»1å¼€å§‹ï¼Œéšç€è®­ç»ƒçº¿æ€§è¡°å‡åˆ°0ã€‚è¿™ç§é€€ç«è®­ç»ƒç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æœ€å¤§åŒ–æ‰€æœ‰å› å­åŒ–é¡ºåºçš„æœŸæœ›ä¼¼ç„¶ï¼Œä»è€Œæœ‰æ•ˆæé«˜æ¨¡å‹å»ºæ¨¡åŒå‘ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚RARä¿æŒäº†è‡ªå›å½’å»ºæ¨¡æ¡†æ¶çš„å®Œæ•´æ€§ï¼Œç¡®ä¿ä¸è¯­è¨€å»ºæ¨¡çš„å®Œå…¨å…¼å®¹ï¼ŒåŒæ—¶åœ¨å›¾åƒç”Ÿæˆä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.23775",
            "title": "In-Context LoRA for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2410.23775",
            "abstract": "Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20sim 100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems. We release our code, data, and models at https://github.com/ali-vilab/In-Context-LoRA",
            "score": 4,
            "issue_id": 407,
            "pub_date": "2024-10-31",
            "pub_date_card": {
                "ru": "31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 31",
                "zh": "10æœˆ31æ—¥"
            },
            "hash": "748dab03a37a21a4",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° DiT Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DiT Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑƒĞ¶Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ pipeline, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ LoRA-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ IC-LoRA, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼."
                },
                "en": {
                    "title": "Unlocking In-Context Generation with IC-LoRA",
                    "desc": "This paper investigates the use of diffusion transformers (DiTs) for generating images without being tied to specific tasks. The authors propose that DiTs can generate images effectively with minimal adjustments, leveraging their inherent in-context generation capabilities. They introduce a new method called In-Context LoRA (IC-LoRA), which simplifies the process by concatenating images and using joint captioning, along with small dataset tuning. This approach enhances the quality of generated images while maintaining a flexible architecture that can adapt to various tasks without extensive retraining."
                },
                "zh": {
                    "title": "æ¿€æ´»ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ›ï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰åœ¨æ— ä»»åŠ¡ç‰¹å®šçš„å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„DiTsæœ¬èº«å…·å¤‡ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ›ï¼Œåªéœ€å°‘é‡è°ƒæ•´å³å¯æ¿€æ´»ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒDiTsèƒ½å¤Ÿåœ¨ä¸è°ƒæ•´çš„æƒ…å†µä¸‹æœ‰æ•ˆè¿›è¡Œä¸Šä¸‹æ–‡ç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æµç¨‹ï¼Œåˆ©ç”¨DiTsçš„ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œç”Ÿæˆé«˜ä¿çœŸåº¦çš„å›¾åƒé›†ï¼Œä¸”ä¸éœ€è¦å¯¹åŸå§‹æ¨¡å‹è¿›è¡Œä¿®æ”¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00233",
            "title": "SambaMixer: State of Health Prediction of Li-ion Batteries using Mamba State Space Models",
            "url": "https://huggingface.co/papers/2411.00233",
            "abstract": "The state of health (SOH) of a Li-ion battery is a critical parameter that determines the remaining capacity and the remaining lifetime of the battery. In this paper, we propose SambaMixer a novel structured state space model (SSM) for predicting the state of health of Li-ion batteries. The proposed SSM is based on the MambaMixer architecture, which is designed to handle multi-variate time signals. We evaluate our model on the NASA battery discharge dataset and show that our model outperforms the state-of-the-art on this dataset. We further introduce a novel anchor-based resampling method which ensures time signals are of the expected length while also serving as augmentation technique. Finally, we condition prediction on the sample time and the cycle time difference using positional encodings to improve the performance of our model and to learn recuperation effects. Our results proof that our model is able to predict the SOH of Li-ion batteries with high accuracy and robustness.",
            "score": 3,
            "issue_id": 410,
            "pub_date": "2024-10-31",
            "pub_date_card": {
                "ru": "31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 31",
                "zh": "10æœˆ31æ—¥"
            },
            "hash": "6361ca66f5ca137f",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#architecture",
                    "#training",
                    "#medicine"
                ],
                "emoji": "ğŸ”‹",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ¾ĞºĞ° ÑĞ»ÑƒĞ¶Ğ±Ñ‹ Ğ°ĞºĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SambaMixer - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ»Ğ¸Ñ‚Ğ¸Ğ¹-Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°ĞºĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ MambaMixer Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºĞ¾Ñ€ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñ‹ Ñ†Ğ¸ĞºĞ»Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°ĞºĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "SambaMixer: Revolutionizing Li-ion Battery Health Prediction",
                    "desc": "This paper introduces SambaMixer, a new structured state space model (SSM) designed to predict the state of health (SOH) of Li-ion batteries. The model utilizes the MambaMixer architecture to effectively process multi-variate time signals, enhancing prediction accuracy. It is evaluated against the NASA battery discharge dataset, demonstrating superior performance compared to existing methods. Additionally, the paper presents an innovative anchor-based resampling technique and employs positional encodings to improve predictions by accounting for time-related factors."
                },
                "zh": {
                    "title": "é«˜æ•ˆé¢„æµ‹é”‚ç¦»å­ç”µæ± å¥åº·çŠ¶æ€çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œç”¨äºé¢„æµ‹é”‚ç¦»å­ç”µæ± çš„å¥åº·çŠ¶æ€ï¼ˆSOHï¼‰ã€‚è¯¥æ¨¡å‹åŸºäºMambaMixeræ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†å¤šå˜é‡æ—¶é—´ä¿¡å·ã€‚æˆ‘ä»¬åœ¨NASAç”µæ± æ”¾ç”µæ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºå…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŸºäºé”šç‚¹çš„é‡é‡‡æ ·æ–¹æ³•ï¼Œä»¥ç¡®ä¿æ—¶é—´ä¿¡å·çš„é¢„æœŸé•¿åº¦ï¼Œå¹¶ä½œä¸ºæ•°æ®å¢å¼ºæŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00027",
            "title": "Personalization of Large Language Models: A Survey",
            "url": "https://huggingface.co/papers/2411.00027",
            "abstract": "Personalization of Large Language Models (LLMs) has recently become increasingly important with a wide range of applications. Despite the importance and recent progress, most existing works on personalized LLMs have focused either entirely on (a) personalized text generation or (b) leveraging LLMs for personalization-related downstream applications, such as recommendation systems. In this work, we bridge the gap between these two separate main directions for the first time by introducing a taxonomy for personalized LLM usage and summarizing the key differences and challenges. We provide a formalization of the foundations of personalized LLMs that consolidates and expands notions of personalization of LLMs, defining and discussing novel facets of personalization, usage, and desiderata of personalized LLMs. We then unify the literature across these diverse fields and usage scenarios by proposing systematic taxonomies for the granularity of personalization, personalization techniques, datasets, evaluation methods, and applications of personalized LLMs. Finally, we highlight challenges and important open problems that remain to be addressed. By unifying and surveying recent research using the proposed taxonomies, we aim to provide a clear guide to the existing literature and different facets of personalization in LLMs, empowering both researchers and practitioners.",
            "score": 3,
            "issue_id": 409,
            "pub_date": "2024-10-29",
            "pub_date_card": {
                "ru": "29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 29",
                "zh": "10æœˆ29æ—¥"
            },
            "hash": "a190b2e727d2d0ad",
            "data": {
                "categories": [
                    "#survey",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… LLM, Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ÑÑ Ğ½ĞµÑ€ĞµÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Bridging Personalization Gaps in Large Language Models",
                    "desc": "This paper addresses the growing need for personalization in Large Language Models (LLMs) by creating a comprehensive framework that connects personalized text generation with applications like recommendation systems. It introduces a taxonomy that categorizes various aspects of personalized LLMs, including techniques, datasets, and evaluation methods. The authors formalize the concept of personalization in LLMs, discussing its different dimensions and the challenges faced in this area. By synthesizing existing research and identifying open problems, the paper serves as a guide for researchers and practitioners interested in the personalization of LLMs."
                },
                "zh": {
                    "title": "ç»Ÿä¸€ä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ªæ€§åŒ–çš„é‡è¦æ€§åŠå…¶åº”ç”¨ã€‚æˆ‘ä»¬é¦–æ¬¡å°†ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆä¸ä¸ªæ€§åŒ–ç›¸å…³çš„ä¸‹æ¸¸åº”ç”¨ï¼ˆå¦‚æ¨èç³»ç»Ÿï¼‰ç»“åˆèµ·æ¥ï¼Œæå‡ºäº†ä¸ªæ€§åŒ–LLMsçš„åˆ†ç±»æ³•ã€‚æ–‡ç« å¯¹ä¸ªæ€§åŒ–LLMsçš„åŸºç¡€è¿›è¡Œäº†å½¢å¼åŒ–å®šä¹‰ï¼Œå¹¶è®¨è®ºäº†ä¸ªæ€§åŒ–çš„ä¸åŒæ–¹é¢ã€ä½¿ç”¨åœºæ™¯å’Œéœ€æ±‚ã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“äº†ç°æœ‰æ–‡çŒ®ï¼Œå¹¶æŒ‡å‡ºäº†ä¸ªæ€§åŒ–LLMsé¢ä¸´çš„æŒ‘æˆ˜å’Œæœªè§£å†³çš„é—®é¢˜ï¼Œä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æ›´å¥½åœ°ç†è§£è¿™ä¸€é¢†åŸŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.21157",
            "title": "M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation",
            "url": "https://huggingface.co/papers/2410.21157",
            "abstract": "Repository-level code completion has drawn great attention in software engineering, and several benchmark datasets have been introduced. However, existing repository-level code completion benchmarks usually focus on a limited number of languages (<5), which cannot evaluate the general code intelligence abilities across different languages for existing code Large Language Models (LLMs). Besides, the existing benchmarks usually report overall average scores of different languages, where the fine-grained abilities in different completion scenarios are ignored. Therefore, to facilitate the research of code LLMs in multilingual scenarios, we propose a massively multilingual repository-level code completion benchmark covering 18 programming languages (called M2RC-EVAL), and two types of fine-grained annotations (i.e., bucket-level and semantic-level) on different completion scenarios are provided, where we obtain these annotations based on the parsed abstract syntax tree. Moreover, we also curate a massively multilingual instruction corpora M2RC- INSTRUCT dataset to improve the repository-level code completion abilities of existing code LLMs. Comprehensive experimental results demonstrate the effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.",
            "score": 3,
            "issue_id": 408,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "d6a0779456870cae",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#plp",
                    "#multilingual"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº M2RC-EVAL Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 18 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… M2RC-INSTRUCT Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… LLM Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Multilingual Code Completion with M2RC-EVAL and M2RC-INSTRUCT",
                    "desc": "This paper introduces a new benchmark called M2RC-EVAL for repository-level code completion that supports 18 programming languages, addressing the limitations of existing benchmarks that only cover a few languages. It provides fine-grained annotations based on abstract syntax trees, allowing for a more detailed evaluation of code completion scenarios. Additionally, the authors present the M2RC-INSTRUCT dataset to enhance the performance of code Large Language Models (LLMs) in multilingual contexts. Experimental results show that both M2RC-EVAL and M2RC-INSTRUCT significantly improve the capabilities of existing code LLMs."
                },
                "zh": {
                    "title": "å¤šè¯­è¨€ä»£ç è¡¥å…¨çš„æ–°åŸºå‡†æµ‹è¯•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šè¯­è¨€ä»£ç è¡¥å…¨åŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºM2RC-EVALï¼Œæ¶µç›–äº†18ç§ç¼–ç¨‹è¯­è¨€ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸åªå…³æ³¨å°‘æ•°å‡ ç§è¯­è¨€ï¼Œæ— æ³•å…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ä¸­çš„ä»£ç æ™ºèƒ½èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒM2RC-EVALæä¾›äº†ç»†ç²’åº¦çš„æ³¨é‡Šï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£æ¨¡å‹åœ¨ä¸åŒè¡¥å…¨åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡ä»£ç è¡¥å…¨èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªå¤šè¯­è¨€æŒ‡ä»¤æ•°æ®é›†M2RC-INSTRUCTã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.22370",
            "title": "Survey of User Interface Design and Interaction Techniques in Generative AI Applications",
            "url": "https://huggingface.co/papers/2410.22370",
            "abstract": "The applications of generative AI have become extremely impressive, and the interplay between users and AI is even more so. Current human-AI interaction literature has taken a broad look at how humans interact with generative AI, but it lacks specificity regarding the user interface designs and patterns used to create these applications. Therefore, we present a survey that comprehensively presents taxonomies of how a human interacts with AI and the user interaction patterns designed to meet the needs of a variety of relevant use cases. We focus primarily on user-guided interactions, surveying interactions that are initiated by the user and do not include any implicit signals given by the user. With this survey, we aim to create a compendium of different user-interaction patterns that can be used as a reference for designers and developers alike. In doing so, we also strive to lower the entry barrier for those attempting to learn more about the design of generative AI applications.",
            "score": 1,
            "issue_id": 409,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 28",
                "zh": "10æœˆ28æ—¥"
            },
            "hash": "9701ceb4e85eeeba",
            "data": {
                "categories": [
                    "#survey",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑƒÑ‚ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ¿Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ˜Ğ˜ Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼, Ğ±ĞµĞ· ÑƒÑ‡ĞµÑ‚Ğ° Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ¦ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ - ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ´Ğ»Ñ Ñ‚ĞµÑ…, ĞºÑ‚Ğ¾ Ñ…Ğ¾Ñ‡ĞµÑ‚ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Enhancing User Interaction with Generative AI",
                    "desc": "This paper surveys the ways users interact with generative AI, focusing specifically on user-guided interactions. It identifies and categorizes various user interface designs and interaction patterns that cater to different use cases. By providing a comprehensive taxonomy, the authors aim to serve as a reference for designers and developers in creating more effective generative AI applications. The goal is to make it easier for newcomers to understand and engage with the design aspects of these technologies."
                },
                "zh": {
                    "title": "æå‡äººæœºäº¤äº’ï¼Œè®¾è®¡æ›´æ™ºèƒ½çš„ç”Ÿæˆå¼AIåº”ç”¨",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¸ç”¨æˆ·ä¹‹é—´çš„äº’åŠ¨ï¼Œå¼ºè°ƒäº†ç”¨æˆ·ç•Œé¢è®¾è®¡çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä»½å…¨é¢çš„è°ƒæŸ¥ï¼Œåˆ†ç±»äº†äººç±»ä¸AIçš„äº’åŠ¨æ–¹å¼ï¼Œç‰¹åˆ«å…³æ³¨ç”¨æˆ·ä¸»å¯¼çš„äº¤äº’æ¨¡å¼ã€‚é€šè¿‡è¿™é¡¹è°ƒæŸ¥ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸ºè®¾è®¡å¸ˆå’Œå¼€å‘è€…æä¾›ä¸åŒçš„ç”¨æˆ·äº¤äº’æ¨¡å¼å‚è€ƒï¼Œé™ä½å­¦ä¹ ç”Ÿæˆå¼AIåº”ç”¨è®¾è®¡çš„é—¨æ§›ã€‚æœ€ç»ˆç›®æ ‡æ˜¯æå‡äººæœºäº¤äº’çš„æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00369",
            "title": "GRS-QA -- Graph Reasoning-Structured Question Answering Dataset",
            "url": "https://huggingface.co/papers/2411.00369",
            "abstract": "Large Language Models (LLMs) have excelled in multi-hop question-answering (M-QA) due to their advanced reasoning abilities. However, the impact of the inherent reasoning structures on LLM M-QA performance remains unclear, largely due to the absence of QA datasets that provide fine-grained reasoning structures. To address this gap, we introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which includes both semantic contexts and reasoning structures for QA pairs. Unlike existing M-QA datasets, where different reasoning structures are entangled together, GRS-QA explicitly captures intricate reasoning pathways by constructing reasoning graphs, where nodes represent textual contexts and edges denote logical flows. These reasoning graphs of different structures enable a fine-grained evaluation of LLM reasoning capabilities across various reasoning structures. Our empirical analysis reveals that LLMs perform differently when handling questions with varying reasoning structures. This finding facilitates the exploration of textual structures as compared with semantics.",
            "score": 1,
            "issue_id": 409,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 1",
                "zh": "11æœˆ1æ—¥"
            },
            "hash": "b3e4773e065d1bc1",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ñ„Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GRS-QA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. GRS-QA Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ĞºĞ°Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹, Ñ‚Ğ°Ğº Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ ÑƒĞ·Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹, Ğ° Ñ€ĞµĞ±Ñ€Ğ° Ğ¾Ğ±Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ÑÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ğ¸Ğ¼ĞµÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking LLMs: Understanding Reasoning Structures in Multi-Hop QA",
                    "desc": "This paper discusses the performance of Large Language Models (LLMs) in multi-hop question-answering (M-QA) tasks, focusing on their reasoning abilities. The authors identify a gap in existing datasets that do not provide detailed reasoning structures, which are crucial for evaluating LLM performance. To fill this gap, they introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which features reasoning graphs that clearly outline the logical pathways for answering questions. Their analysis shows that LLMs exhibit varying performance based on the complexity of the reasoning structures involved, highlighting the importance of understanding both textual and semantic elements in M-QA."
                },
                "zh": {
                    "title": "æ­ç¤ºæ¨ç†ç»“æ„å¯¹LLMè¡¨ç°çš„å½±å“",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè·³é—®ç­”ï¼ˆM-QAï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸»è¦å¾—ç›Šäºå…¶å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMåœ¨å¤šè·³é—®ç­”ä¸­çš„è¡¨ç°å—å›ºæœ‰æ¨ç†ç»“æ„çš„å½±å“å°šä¸æ˜ç¡®ï¼Œä¸»è¦æ˜¯å› ä¸ºç¼ºä¹æä¾›ç»†ç²’åº¦æ¨ç†ç»“æ„çš„é—®ç­”æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å›¾æ¨ç†ç»“æ„é—®ç­”æ•°æ®é›†ï¼ˆGRS-QAï¼‰ï¼Œè¯¥æ•°æ®é›†ä¸ºé—®ç­”å¯¹æä¾›äº†è¯­ä¹‰ä¸Šä¸‹æ–‡å’Œæ¨ç†ç»“æ„ã€‚ä¸ç°æœ‰çš„M-QAæ•°æ®é›†ä¸åŒï¼ŒGRS-QAé€šè¿‡æ„å»ºæ¨ç†å›¾æ¥æ˜ç¡®æ•æ‰å¤æ‚çš„æ¨ç†è·¯å¾„ï¼ŒèŠ‚ç‚¹è¡¨ç¤ºæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œè¾¹è¡¨ç¤ºé€»è¾‘æµï¼Œä»è€Œå®ç°å¯¹LLMæ¨ç†èƒ½åŠ›çš„ç»†ç²’åº¦è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.22901",
            "title": "HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models",
            "url": "https://huggingface.co/papers/2410.22901",
            "abstract": "We propose an effective method for inserting adapters into text-to-image foundation models, which enables the execution of complex downstream tasks while preserving the generalization ability of the base model. The core idea of this method is to optimize the attention mechanism related to 2D feature maps, which enhances the performance of the adapter. This approach was validated on the task of meme video generation and achieved significant results. We hope this work can provide insights for post-training tasks of large text-to-image models. Additionally, as this method demonstrates good compatibility with SD1.5 derivative models, it holds certain value for the open-source community. Therefore, we will release the related code (https://songkey.github.io/hellomeme).",
            "score": 1,
            "issue_id": 408,
            "pub_date": "2024-10-30",
            "pub_date_card": {
                "ru": "30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 30",
                "zh": "10æœˆ30æ—¥"
            },
            "hash": "801963cbdcf75d7b",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¼Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ² Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¼Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ SD1.5, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ†ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing Text-to-Image Models with Adapter Integration",
                    "desc": "This paper presents a novel method for integrating adapters into text-to-image foundation models, allowing them to perform complex tasks while maintaining their ability to generalize. The method focuses on optimizing the attention mechanism associated with 2D feature maps, which significantly boosts the performance of the adapters. The effectiveness of this approach was demonstrated through the task of meme video generation, yielding impressive results. The authors aim to contribute to the open-source community by sharing their code and providing insights for post-training tasks in large text-to-image models."
                },
                "zh": {
                    "title": "é€‚é…å™¨æ’å…¥ï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå°†é€‚é…å™¨æ’å…¥æ–‡æœ¬åˆ°å›¾åƒçš„åŸºç¡€æ¨¡å‹ä¸­ï¼Œä»è€Œåœ¨æ‰§è¡Œå¤æ‚çš„ä¸‹æ¸¸ä»»åŠ¡æ—¶ä¿æŒåŸºç¡€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¼˜åŒ–ä¸äºŒç»´ç‰¹å¾å›¾ç›¸å…³çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œå¢å¼ºé€‚é…å™¨çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ç”Ÿæˆè¡¨æƒ…åŒ…è§†é¢‘çš„ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„ç»“æœã€‚å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½ä¸ºå¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åè®­ç»ƒä»»åŠ¡æä¾›ä¸€äº›è§è§£ï¼Œå¹¶ä¸ºå¼€æºç¤¾åŒºå¸¦æ¥ä»·å€¼ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-01.html",
    "link_next": "2024-11-05.html",
    "short_date_prev": {
        "ru": "01.11",
        "en": "11/01",
        "zh": "11æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "05.11",
        "en": "11/05",
        "zh": "11æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#medicine": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨ã€‚SAEså¯ä»¥å°†ä¸æ˜“è§£é‡Šçš„ä¸­é—´è¡¨ç¤ºåˆ†è§£æˆæ˜“è§£é‡Šçš„ç‰¹å¾ï¼Œä»è€Œæ›´å¥½åœ°æ§åˆ¶å’Œåˆ†æã€‚ç„¶è€Œï¼Œç±»ä¼¼çš„åˆ†æå’Œæ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„æ¨¡å‹ä¸­å°šç¼ºä¹ç ”ç©¶ã€‚ä½œè€…ç ”ç©¶äº†åœ¨å°‘æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚SDXL Turboï¼‰ä¸­ä½¿ç”¨SAEså­¦ä¹ å¯è§£é‡Šç‰¹å¾çš„å¯èƒ½æ€§ã€‚ç»“æœå‘ç°ï¼ŒSAEså­¦åˆ°çš„ç‰¹å¾å¯è§£é‡Šï¼Œå¹¶å¯¹ç”Ÿæˆè¿‡ç¨‹äº§ç”Ÿå› æœå½±å“ï¼Œæ­ç¤ºäº†æ¨¡å‹å†…éƒ¨çš„ä¸“ä¸šåŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæœ‰ä¸€ä¸ªæ¨¡å—ä¸»è¦å¤„ç†å›¾åƒç»„åˆï¼Œä¸€ä¸ªè´Ÿè´£æ·»åŠ ç»†èŠ‚ï¼Œå¦ä¸€ä¸ªè´Ÿè´£é¢œè‰²ã€å…‰ç…§å’Œé£æ ¼ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œæ˜¯ç†è§£ç”Ÿæˆæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å†…éƒ¨çš„é‡è¦ä¸€æ­¥ï¼Œå±•ç¤ºäº†SAEsåœ¨è§†è§‰é¢†åŸŸçš„æ½œåŠ›ã€‚",
        "title": "Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨ã€‚SAEså¯ä»¥å°†ä¸æ˜“è§£é‡Šçš„ä¸­é—´è¡¨ç¤ºåˆ†è§£æˆæ˜“è§£é‡Šçš„ç‰¹å¾ï¼Œä»è€Œæ›´å¥½åœ°æ§åˆ¶å’Œåˆ†æã€‚ç„¶è€Œï¼Œç±»ä¼¼çš„åˆ†æå’Œæ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„æ¨¡å‹ä¸­å°šç¼ºä¹ç ”ç©¶ã€‚ä½œè€…ç ”ç©¶äº†åœ¨å°‘æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚SDXL Turboï¼‰ä¸­ä½¿ç”¨SAEså­¦ä¹ å¯è§£é‡Šç‰¹å¾çš„å¯èƒ½æ€§ã€‚ç»“æœå‘ç°ï¼ŒSAEså­¦åˆ°çš„ç‰¹å¾å¯è§£é‡Šï¼Œå¹¶å¯¹ç”Ÿæˆè¿‡ç¨‹äº§ç”Ÿå› æœå½±å“ï¼Œæ­ç¤ºäº†æ¨¡å‹å†…éƒ¨çš„ä¸“ä¸šåŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæœ‰ä¸€ä¸ªæ¨¡å—ä¸»è¦å¤„ç†å›¾åƒç»„åˆï¼Œä¸€ä¸ªè´Ÿè´£æ·»åŠ ç»†èŠ‚ï¼Œå¦ä¸€ä¸ªè´Ÿè´£é¢œè‰²ã€å…‰ç…§å’Œé£æ ¼ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œæ˜¯ç†è§£ç”Ÿæˆæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å†…éƒ¨çš„é‡è¦ä¸€æ­¥ï¼Œå±•ç¤ºäº†SAEsåœ¨è§†è§‰é¢†åŸŸçš„æ½œåŠ›ã€‚\n\nPinyin transcription:\n\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le xÄ«shÅ« zÃ¬biÇnmÇqÃ¬ (SAEs) zÃ i dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) zhÅng de yÃ¬ngyÃ²ng. SAEs kÄ›yÇ jiÄng bÃ¹yÃ¬ jiÄ›shÃ¬ de zhÅngjiÄn biÇoshÃ¬ fÄ“njiÄ› chÃ©ng yÃ¬ jiÄ›shÃ¬ de tÃ¨zhÄ“ng, cÃ³ng'Ã©r gÃ¨ng hÇo de kÃ²ngzhÃ¬ hÃ© fÄ“nxÄ«. RÃ¡n'Ã©r, lÃ¨isÃ¬ de fÄ“nxÄ« hÃ© fÄngfÇ zÃ i wÃ©nbÄ›n dÃ o tÃºxiÃ ng de mÃ³xÃ­ng zhÅng shÃ ng quÄ“fÃ¡ yÃ¡njiÅ«. ZuÃ²zhÄ› yÃ¡njiÅ« le zÃ i shÇo bÃ¹ wÃ©nbÄ›n dÃ o tÃºxiÃ ng kuÃ²sÃ n mÃ³xÃ­ng (rÃº SDXL Turbo) zhÅng shÇyÃ²ng SAEs xuÃ©xÃ­ kÄ› jiÄ›shÃ¬ tÃ¨zhÄ“ng de kÄ›nÃ©ngxÃ¬ng. JiÃ©guÇ’ fÄxiÃ n, SAEs xuÃ© dÃ o de tÃ¨zhÄ“ng kÄ› jiÄ›shÃ¬, bÃ¬ng duÃ¬ shÄ“ngchÃ©ng guÃ²chÃ©ng chÇnshÄ“ng yÄ«nguÇ’ yÇngxiÇng, jiÄ“shÃ¬ le mÃ³xÃ­ng nÃ¨ibÃ¹ de zhuÄnmÃ©nhuÃ . JÃ¹tÇ lÃ¡i shuÅ, yÇ’u yÄ«gÃ¨ mÃ³kuÃ i zhÇ”yÃ o chÇ”lÇ tÃºxiÃ ng zÇ”hÃ©, yÄ«gÃ¨ fÃ¹zÃ© tiÄnjiÇ xÃ¬jiÄ›, lÃ¬ng yÄ«gÃ¨ fÃ¹zÃ© yÃ¡nsÃ¨, guÄngzhÃ o hÃ© fÄ“nggÃ©. YÄ«ncÇ, zhÃ¨ xiÃ ng gÅngzuÃ² shÃ¬ liÇojiÄ› shÄ“ngchÃ©ng wÃ©nbÄ›n dÃ o tÃºxiÃ ng mÃ³xÃ­ng nÃ¨ibÃ¹ de zhÃ²ngyÃ o yÄ« bÃ¹, zhÇnshÃ¬ le SAEs zÃ i shÃ¬juÃ© lÇngyÃ¹ de qiÃ¡nlÃ¬.",
        "vocab": "[\n    {\"word\": \"ç¨€ç–è‡ªç¼–ç å™¨\", \"pinyin\": \"xÄ« shÅ« zÃ¬ biÄn mÇ qÃ¬\", \"trans\": \"Sparse Autoencoder\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"ä¸­é—´è¡¨ç¤º\", \"pinyin\": \"zhÅng jiÄn biÇo shÃ¬\", \"trans\": \"Intermediate Representation\"},\n    {\"word\": \"ç‰¹å¾\", \"pinyin\": \"tÃ¨ zhÄ“ng\", \"trans\": \"Feature\"},\n    {\"word\": \"åˆ†è§£\", \"pinyin\": \"fÄ“n jiÄ›\", \"trans\": \"Decompose\"},\n    {\"word\": \"æ§åˆ¶\", \"pinyin\": \"kÃ²ng zhÃ¬\", \"trans\": \"Control\"},\n    {\"word\": \"åˆ†æ\", \"pinyin\": \"fÄ“n xÄ«\", \"trans\": \"Analyze\"},\n    {\"word\": \"æ–‡æœ¬åˆ°å›¾åƒ\", \"pinyin\": \"wÃ©n bÄ›n dÃ o tÃº xiÃ ng\", \"trans\": \"Text-to-Image\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"Model\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡n jiÅ«\", \"trans\": \"Research\"},\n    {\"word\": \"å°‘æ­¥\", \"pinyin\": \"shÇo bÃ¹\", \"trans\": \"Few-Step\"},\n    {\"word\": \"æ‰©æ•£æ¨¡å‹\", \"pinyin\": \"kuÃ² sÃ n mÃ³ xÃ­ng\", \"trans\": \"Diffusion Model\"},\n    {\"word\": \"å¯è§£é‡Š\", \"pinyin\": \"kÄ› jiÄ› shÃ¬\", \"trans\": \"Interpretable\"},\n    {\"word\": \"å› æœå½±å“\", \"pinyin\": \"yÄ«n guÇ’ yÇng xiÇng\", \"trans\": \"Causal Effect\"},\n    {\"word\": \"æ­ç¤º\", \"pinyin\": \"jiÄ“ shÃ¬\", \"trans\": \"Reveal\"},\n    {\"word\": \"ä¸“ä¸šåŒ–\", \"pinyin\": \"zhuÄn yÃ¨ huÃ \", \"trans\": \"Specialization\"},\n    {\"word\": \"æ¨¡å—\", \"pinyin\": \"mÃ³ kuÃ i\", \"trans\": \"Module\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"Process\"},\n    {\"word\": \"å›¾åƒç»„åˆ\", \"pinyin\": \"tÃº xiÃ ng zÇ” hÃ©\", \"trans\": \"Image Composition\"},\n    {\"word\": \"ç»†èŠ‚\", \"pinyin\": \"xÃ¬ jiÄ›\", \"trans\": \"Detail\"},\n    {\"word\": \"é¢œè‰²\", \"pinyin\": \"yÃ¡n sÃ¨\", \"trans\": \"Color\"},\n    {\"word\": \"å…‰ç…§\", \"pinyin\": \"guÄng zhÃ o\", \"trans\": \"Lighting\"},\n    {\"word\": \"é£æ ¼\", \"pinyin\": \"fÄ“ng gÄ“\", \"trans\": \"Style\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"Generate\"},\n    {\"word\": \"è§†è§‰é¢†åŸŸ\", \"pinyin\": \"shÃ¬ juÃ© lÇng yÃ¹\", \"trans\": \"Visual Domain\"},\n    {\"word\": \"æ½œåŠ›\", \"pinyin\": \"qiÃ¡n lÃ¬\", \"trans\": \"Potential\"}\n]",
        "trans": "This article discusses the application of Sparse Autoencoders (SAEs) in large language models (LLMs). SAEs can decompose hard-to-interpret intermediate representations into interpretable features, thereby enabling better control and analysis. However, similar analyses and methods in text-to-image models are still lacking in research. The authors investigated the possibility of using SAEs to learn interpretable features in few-step text-to-image diffusion models (such as SDXL Turbo). The results showed that the features learned by SAEs are interpretable and have a causal impact on the generation process, revealing specialization within the model. Specifically, one module mainly handles image composition, another is responsible for adding details, and another deals with color, lighting, and style. Therefore, this work is an important step towards understanding the internal workings of text-to-image generation models and demonstrates the potential of SAEs in the visual domain.",
        "update_ts": "2024-11-03 09:12"
    }
}