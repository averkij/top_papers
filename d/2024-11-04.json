{
    "date": {
        "ru": "4 ноября",
        "en": "November 4",
        "zh": "11月4日"
    },
    "time_utc": "2024-11-04 08:16",
    "weekday": 0,
    "issue_id": 410,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.23218",
            "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
            "url": "https://huggingface.co/papers/2410.23218",
            "abstract": "Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas - a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs.",
            "score": 18,
            "issue_id": 410,
            "pub_date": "2024-10-30",
            "pub_date_card": {
                "ru": "30 октября",
                "en": "October 30",
                "zh": "10月30日"
            },
            "hash": "d7a3f0fd08f934d5",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#agents",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "OS-Atlas: Открытая модель для универсального взаимодействия с GUI",
                    "desc": "Исследователи разработали OS-Atlas - основополагающую модель для взаимодействия с графическим интерфейсом пользователя (GUI). Модель использует инновационный подход к данным и моделированию, что позволяет ей эффективно работать с GUI и решать задачи вне распределения (OOD). Авторы создали открытый набор инструментов для синтеза данных о GUI на различных платформах и выпустили крупнейший открытый кросс-платформенный корпус, содержащий более 13 миллионов элементов GUI. OS-Atlas демонстрирует значительное улучшение производительности по сравнению с предыдущими моделями на шести тестовых наборах, охватывающих мобильные, настольные и веб-платформы."
                },
                "en": {
                    "title": "Empowering Open-Source GUI Agents with OS-Atlas",
                    "desc": "This paper introduces OS-Atlas, an open-source foundational model designed for GUI grounding and Out-Of-Distribution (OOD) tasks. It addresses the performance gap between commercial Vision-Language Models (VLMs) and open-source alternatives by providing a comprehensive toolkit for synthesizing GUI grounding data across various platforms. The authors present the largest open-source cross-platform GUI grounding dataset, featuring over 13 million GUI elements, which enhances the model's ability to understand and generalize from GUI screenshots. Extensive evaluations show that OS-Atlas outperforms previous models, offering insights for further advancements in open-source VLM capabilities."
                },
                "zh": {
                    "title": "开源GUI模型OS-Atlas：提升界面理解能力的创新之路",
                    "desc": "本论文介绍了OS-Atlas，一个开源的GUI动作模型，专注于GUI定位和超出分布（OOD）任务。我们开发了一个工具包，可以在多个平台上合成GUI定位数据，包括Windows、Linux、MacOS、Android和网页。OS-Atlas利用超过1300万个GUI元素的数据集，结合创新的模型训练方法，显著提高了对GUI截图的理解能力。通过在六个基准测试中进行广泛评估，OS-Atlas在移动、桌面和网页平台上表现出显著的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00412",
            "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation",
            "url": "https://huggingface.co/papers/2411.00412",
            "abstract": "Large Language Models (LLMs) demonstrate promising capabilities in solving simple scientific problems but often produce hallucinations for complex ones. While integrating LLMs with tools can increase reliability, this approach typically results in over-reliance on tools, diminishing the model's ability to solve simple problems through basic reasoning. In contrast, human experts first assess problem complexity using domain knowledge before choosing an appropriate solution approach. Inspired by this human problem-solving process, we propose a novel two-component fine-tuning method. In the first component World Knowledge Distillation (WKD), LLMs learn directly from solutions generated using tool's information to internalize domain knowledge. In the second component Tool Usage Adaptation (TUA), we partition problems into easy and hard categories based on the model's direct answering accuracy. While maintaining the same alignment target for easy problems as in WKD, we train the model to intelligently switch to tool usage for more challenging problems. We validate our method on six scientific benchmark datasets, spanning mathematics, climate science and epidemiology. On average, our models demonstrate a 28.18% improvement in answer accuracy and a 13.89% increase in tool usage precision across all datasets, surpassing state-of-the-art models including GPT-4o and Claude-3.5.",
            "score": 7,
            "issue_id": 407,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 ноября",
                "en": "November 1",
                "zh": "11月1日"
            },
            "hash": "27e4deefc7d09df0",
            "data": {
                "categories": [
                    "#rlhf",
                    "#alignment",
                    "#training",
                    "#benchmark",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Умное переключение: как научить ИИ эффективно решать задачи разной сложности",
                    "desc": "Исследование посвящено улучшению способности больших языковых моделей (LLM) решать научные задачи. Авторы предлагают двухкомпонентный метод дообучения: дистилляция мировых знаний и адаптация использования инструментов. Этот подход позволяет LLM эффективно решать простые задачи с помощью базовых рассуждений, а для сложных - прибегать к инструментам. Метод показал значительное улучшение точности ответов и точности использования инструментов на шести научных наборах данных."
                },
                "en": {
                    "title": "Enhancing LLMs: Smart Tool Use for Complex Problems",
                    "desc": "This paper addresses the limitations of Large Language Models (LLMs) in solving complex scientific problems, which often lead to inaccuracies or 'hallucinations'. The authors propose a two-component fine-tuning method that mimics human problem-solving strategies by first assessing problem complexity. The first component, World Knowledge Distillation (WKD), allows LLMs to learn from solutions that utilize external tools, while the second component, Tool Usage Adaptation (TUA), helps the model categorize problems as easy or hard and decide when to use tools. The proposed method shows significant improvements in accuracy and tool usage precision across various scientific datasets, outperforming existing models."
                },
                "zh": {
                    "title": "智能切换，提升模型解决问题的能力",
                    "desc": "大型语言模型（LLMs）在解决简单科学问题方面表现出色，但在复杂问题上常常出现幻觉。我们提出了一种新颖的双组件微调方法，模仿人类专家的解决问题过程。第一个组件是世界知识蒸馏（WKD），使LLMs从工具生成的解决方案中学习领域知识。第二个组件是工具使用适应（TUA），根据模型的直接回答准确性将问题分为简单和困难两类，从而提高模型在复杂问题上的工具使用能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00776",
            "title": "Randomized Autoregressive Visual Generation",
            "url": "https://huggingface.co/papers/2411.00776",
            "abstract": "This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence-typically ordered in raster form-is randomly permuted into different factorization orders with a probability r, where r starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model's capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models will be made available at https://github.com/bytedance/1d-tokenizer",
            "score": 6,
            "issue_id": 408,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 ноября",
                "en": "November 1",
                "zh": "11月1日"
            },
            "hash": "0cc2c0f19f735f79",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Случайная перестановка для улучшения генерации изображений",
                    "desc": "Статья представляет метод Randomized AutoRegressive modeling (RAR) для генерации изображений. RAR использует случайную перестановку входной последовательности во время обучения авторегрессионной модели, что позволяет учитывать двунаправленный контекст. Метод сохраняет совместимость с языковыми моделями и достигает нового state-of-the-art результата на бенчмарке ImageNet-256 с показателем FID 1.48. RAR превосходит как авторегрессионные, так и диффузионные методы генерации изображений."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Randomized AutoRegressive Modeling",
                    "desc": "This paper introduces Randomized AutoRegressive modeling (RAR), a novel approach for generating images that enhances performance while remaining compatible with existing language modeling techniques. RAR employs a unique training method where the input sequence is randomly shuffled during the autoregressive training process, allowing the model to learn from various factorization orders. This strategy helps the model to better understand and utilize bidirectional contexts, leading to improved image generation capabilities. The results show that RAR achieves a remarkable FID score of 1.48 on the ImageNet-256 benchmark, outperforming previous state-of-the-art methods in both autoregressive and diffusion-based image generation."
                },
                "zh": {
                    "title": "随机自回归建模：图像生成的新突破",
                    "desc": "本文提出了一种随机自回归建模（RAR）方法用于视觉生成，在图像生成任务上设定了新的最先进性能，同时与语言建模框架完全兼容。RAR方法简单：在标准的自回归训练过程中，输入序列通常按光栅形式排列，但以概率r随机打乱为不同的因子化顺序，r从1开始，随着训练线性衰减到0。这种退火训练策略使模型能够学习最大化所有因子化顺序的期望似然，从而有效提高模型建模双向上下文的能力。RAR保持了自回归建模框架的完整性，确保与语言建模的完全兼容，同时在图像生成中显著提高了性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.23775",
            "title": "In-Context LoRA for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2410.23775",
            "abstract": "Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20sim 100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems. We release our code, data, and models at https://github.com/ali-vilab/In-Context-LoRA",
            "score": 4,
            "issue_id": 407,
            "pub_date": "2024-10-31",
            "pub_date_card": {
                "ru": "31 октября",
                "en": "October 31",
                "zh": "10月31日"
            },
            "hash": "748dab03a37a21a4",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Раскрытие скрытого потенциала DiT для многозадачной генерации изображений",
                    "desc": "Исследование предлагает новый подход к использованию диффузионных трансформеров (DiT) для генерации изображений в различных задачах. Авторы обнаружили, что существующие модели DiT для преобразования текста в изображение уже обладают способностью к контекстной генерации без дополнительной настройки. На основе этого наблюдения они разработали простой pipeline, включающий конкатенацию изображений, совместное создание подписей и применение LoRA-тюнинга на небольших датасетах. Предложенный метод, названный IC-LoRA, позволяет генерировать высококачественные наборы изображений, лучше соответствующие заданным промптам."
                },
                "en": {
                    "title": "Unlocking In-Context Generation with IC-LoRA",
                    "desc": "This paper investigates the use of diffusion transformers (DiTs) for generating images without being tied to specific tasks. The authors propose that DiTs can generate images effectively with minimal adjustments, leveraging their inherent in-context generation capabilities. They introduce a new method called In-Context LoRA (IC-LoRA), which simplifies the process by concatenating images and using joint captioning, along with small dataset tuning. This approach enhances the quality of generated images while maintaining a flexible architecture that can adapt to various tasks without extensive retraining."
                },
                "zh": {
                    "title": "激活上下文生成能力，提升图像生成质量",
                    "desc": "本研究探讨了扩散变换器（DiTs）在无任务特定的图像生成中的应用。我们提出，文本到图像的DiTs本身具备上下文生成能力，只需少量调整即可激活。通过实验，我们展示了现有的文本到图像DiTs能够在不调整的情况下有效进行上下文生成。我们提出了一种简单的流程，利用DiTs的上下文能力，生成高保真度的图像集，且不需要对原始模型进行修改。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00233",
            "title": "SambaMixer: State of Health Prediction of Li-ion Batteries using Mamba State Space Models",
            "url": "https://huggingface.co/papers/2411.00233",
            "abstract": "The state of health (SOH) of a Li-ion battery is a critical parameter that determines the remaining capacity and the remaining lifetime of the battery. In this paper, we propose SambaMixer a novel structured state space model (SSM) for predicting the state of health of Li-ion batteries. The proposed SSM is based on the MambaMixer architecture, which is designed to handle multi-variate time signals. We evaluate our model on the NASA battery discharge dataset and show that our model outperforms the state-of-the-art on this dataset. We further introduce a novel anchor-based resampling method which ensures time signals are of the expected length while also serving as augmentation technique. Finally, we condition prediction on the sample time and the cycle time difference using positional encodings to improve the performance of our model and to learn recuperation effects. Our results proof that our model is able to predict the SOH of Li-ion batteries with high accuracy and robustness.",
            "score": 3,
            "issue_id": 410,
            "pub_date": "2024-10-31",
            "pub_date_card": {
                "ru": "31 октября",
                "en": "October 31",
                "zh": "10月31日"
            },
            "hash": "6361ca66f5ca137f",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#architecture",
                    "#training",
                    "#medicine"
                ],
                "emoji": "🔋",
                "ru": {
                    "title": "Точное прогнозирование срока службы аккумуляторов с помощью глубокого обучения",
                    "desc": "Статья представляет SambaMixer - новую модель структурированного пространства состояний для прогнозирования состояния здоровья литий-ионных аккумуляторов. Модель основана на архитектуре MambaMixer и способна обрабатывать многомерные временные сигналы. Авторы предлагают метод ресэмплинга на основе якорей для нормализации длины сигналов и аугментации данных. Использование позиционного кодирования времени выборки и разницы циклов позволяет модели учитывать эффекты восстановления аккумуляторов."
                },
                "en": {
                    "title": "SambaMixer: Revolutionizing Li-ion Battery Health Prediction",
                    "desc": "This paper introduces SambaMixer, a new structured state space model (SSM) designed to predict the state of health (SOH) of Li-ion batteries. The model utilizes the MambaMixer architecture to effectively process multi-variate time signals, enhancing prediction accuracy. It is evaluated against the NASA battery discharge dataset, demonstrating superior performance compared to existing methods. Additionally, the paper presents an innovative anchor-based resampling technique and employs positional encodings to improve predictions by accounting for time-related factors."
                },
                "zh": {
                    "title": "高效预测锂离子电池健康状态的新方法",
                    "desc": "本文提出了一种新颖的结构化状态空间模型（SSM），用于预测锂离子电池的健康状态（SOH）。该模型基于MambaMixer架构，能够处理多变量时间信号。我们在NASA电池放电数据集上评估了该模型，结果显示其性能优于现有的最先进方法。此外，我们引入了一种新颖的基于锚点的重采样方法，以确保时间信号的预期长度，并作为数据增强技术。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00027",
            "title": "Personalization of Large Language Models: A Survey",
            "url": "https://huggingface.co/papers/2411.00027",
            "abstract": "Personalization of Large Language Models (LLMs) has recently become increasingly important with a wide range of applications. Despite the importance and recent progress, most existing works on personalized LLMs have focused either entirely on (a) personalized text generation or (b) leveraging LLMs for personalization-related downstream applications, such as recommendation systems. In this work, we bridge the gap between these two separate main directions for the first time by introducing a taxonomy for personalized LLM usage and summarizing the key differences and challenges. We provide a formalization of the foundations of personalized LLMs that consolidates and expands notions of personalization of LLMs, defining and discussing novel facets of personalization, usage, and desiderata of personalized LLMs. We then unify the literature across these diverse fields and usage scenarios by proposing systematic taxonomies for the granularity of personalization, personalization techniques, datasets, evaluation methods, and applications of personalized LLMs. Finally, we highlight challenges and important open problems that remain to be addressed. By unifying and surveying recent research using the proposed taxonomies, we aim to provide a clear guide to the existing literature and different facets of personalization in LLMs, empowering both researchers and practitioners.",
            "score": 3,
            "issue_id": 409,
            "pub_date": "2024-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "a190b2e727d2d0ad",
            "data": {
                "categories": [
                    "#survey",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Объединяя подходы: комплексный взгляд на персонализацию больших языковых моделей",
                    "desc": "Статья посвящена персонализации больших языковых моделей (LLM) и объединяет два основных направления исследований в этой области. Авторы предлагают таксономию использования персонализированных LLM, формализуют основы и расширяют понятия персонализации. Они систематизируют литературу по различным аспектам, включая методы персонализации, наборы данных и способы оценки. В работе также выделяются нерешенные проблемы и открытые вопросы в данной области исследований."
                },
                "en": {
                    "title": "Bridging Personalization Gaps in Large Language Models",
                    "desc": "This paper addresses the growing need for personalization in Large Language Models (LLMs) by creating a comprehensive framework that connects personalized text generation with applications like recommendation systems. It introduces a taxonomy that categorizes various aspects of personalized LLMs, including techniques, datasets, and evaluation methods. The authors formalize the concept of personalization in LLMs, discussing its different dimensions and the challenges faced in this area. By synthesizing existing research and identifying open problems, the paper serves as a guide for researchers and practitioners interested in the personalization of LLMs."
                },
                "zh": {
                    "title": "统一个性化大型语言模型的研究",
                    "desc": "本文探讨了大型语言模型（LLMs）个性化的重要性及其应用。我们首次将个性化文本生成与个性化相关的下游应用（如推荐系统）结合起来，提出了个性化LLMs的分类法。文章对个性化LLMs的基础进行了形式化定义，并讨论了个性化的不同方面、使用场景和需求。最后，我们总结了现有文献，并指出了个性化LLMs面临的挑战和未解决的问题，以帮助研究人员和从业者更好地理解这一领域。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.21157",
            "title": "M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation",
            "url": "https://huggingface.co/papers/2410.21157",
            "abstract": "Repository-level code completion has drawn great attention in software engineering, and several benchmark datasets have been introduced. However, existing repository-level code completion benchmarks usually focus on a limited number of languages (<5), which cannot evaluate the general code intelligence abilities across different languages for existing code Large Language Models (LLMs). Besides, the existing benchmarks usually report overall average scores of different languages, where the fine-grained abilities in different completion scenarios are ignored. Therefore, to facilitate the research of code LLMs in multilingual scenarios, we propose a massively multilingual repository-level code completion benchmark covering 18 programming languages (called M2RC-EVAL), and two types of fine-grained annotations (i.e., bucket-level and semantic-level) on different completion scenarios are provided, where we obtain these annotations based on the parsed abstract syntax tree. Moreover, we also curate a massively multilingual instruction corpora M2RC- INSTRUCT dataset to improve the repository-level code completion abilities of existing code LLMs. Comprehensive experimental results demonstrate the effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.",
            "score": 3,
            "issue_id": 408,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "d6a0779456870cae",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#plp",
                    "#multilingual"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Многоязычный бенчмарк для оценки LLM в автодополнении кода",
                    "desc": "Статья представляет новый бенчмарк M2RC-EVAL для оценки способностей больших языковых моделей (LLM) в задаче автодополнения кода на уровне репозитория. Бенчмарк охватывает 18 языков программирования и включает детальные аннотации для различных сценариев дополнения. Авторы также создали набор данных M2RC-INSTRUCT для улучшения возможностей существующих LLM в этой задаче. Эксперименты подтверждают эффективность предложенных инструментов."
                },
                "en": {
                    "title": "Empowering Multilingual Code Completion with M2RC-EVAL and M2RC-INSTRUCT",
                    "desc": "This paper introduces a new benchmark called M2RC-EVAL for repository-level code completion that supports 18 programming languages, addressing the limitations of existing benchmarks that only cover a few languages. It provides fine-grained annotations based on abstract syntax trees, allowing for a more detailed evaluation of code completion scenarios. Additionally, the authors present the M2RC-INSTRUCT dataset to enhance the performance of code Large Language Models (LLMs) in multilingual contexts. Experimental results show that both M2RC-EVAL and M2RC-INSTRUCT significantly improve the capabilities of existing code LLMs."
                },
                "zh": {
                    "title": "多语言代码补全的新基准测试",
                    "desc": "本论文提出了一种新的多语言代码补全基准测试，称为M2RC-EVAL，涵盖了18种编程语言。现有的基准测试通常只关注少数几种语言，无法全面评估大型语言模型在不同语言中的代码智能能力。此外，M2RC-EVAL提供了细粒度的注释，帮助研究人员更好地理解模型在不同补全场景下的表现。为了进一步提升代码补全能力，我们还构建了一个多语言指令数据集M2RC-INSTRUCT。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.22370",
            "title": "Survey of User Interface Design and Interaction Techniques in Generative AI Applications",
            "url": "https://huggingface.co/papers/2410.22370",
            "abstract": "The applications of generative AI have become extremely impressive, and the interplay between users and AI is even more so. Current human-AI interaction literature has taken a broad look at how humans interact with generative AI, but it lacks specificity regarding the user interface designs and patterns used to create these applications. Therefore, we present a survey that comprehensively presents taxonomies of how a human interacts with AI and the user interaction patterns designed to meet the needs of a variety of relevant use cases. We focus primarily on user-guided interactions, surveying interactions that are initiated by the user and do not include any implicit signals given by the user. With this survey, we aim to create a compendium of different user-interaction patterns that can be used as a reference for designers and developers alike. In doing so, we also strive to lower the entry barrier for those attempting to learn more about the design of generative AI applications.",
            "score": 1,
            "issue_id": 409,
            "pub_date": "2024-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "9701ceb4e85eeeba",
            "data": {
                "categories": [
                    "#survey",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Путеводитель по взаимодействию человека и ИИ",
                    "desc": "Статья представляет обзор таксономий взаимодействия человека с генеративным ИИ и паттернов пользовательского интерфейса для различных сценариев использования. Авторы фокусируются на взаимодействиях, инициируемых пользователем, без учета неявных сигналов. Цель работы - создать справочник паттернов взаимодействия для дизайнеров и разработчиков приложений генеративного ИИ. Исследование направлено на снижение входного барьера для тех, кто хочет изучить дизайн приложений генеративного ИИ."
                },
                "en": {
                    "title": "Enhancing User Interaction with Generative AI",
                    "desc": "This paper surveys the ways users interact with generative AI, focusing specifically on user-guided interactions. It identifies and categorizes various user interface designs and interaction patterns that cater to different use cases. By providing a comprehensive taxonomy, the authors aim to serve as a reference for designers and developers in creating more effective generative AI applications. The goal is to make it easier for newcomers to understand and engage with the design aspects of these technologies."
                },
                "zh": {
                    "title": "提升人机交互，设计更智能的生成式AI应用",
                    "desc": "本论文探讨了生成式人工智能（AI）与用户之间的互动，强调了用户界面设计的重要性。我们提供了一份全面的调查，分类了人类与AI的互动方式，特别关注用户主导的交互模式。通过这项调查，我们希望为设计师和开发者提供不同的用户交互模式参考，降低学习生成式AI应用设计的门槛。最终目标是提升人机交互的效率和用户体验。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.00369",
            "title": "GRS-QA -- Graph Reasoning-Structured Question Answering Dataset",
            "url": "https://huggingface.co/papers/2411.00369",
            "abstract": "Large Language Models (LLMs) have excelled in multi-hop question-answering (M-QA) due to their advanced reasoning abilities. However, the impact of the inherent reasoning structures on LLM M-QA performance remains unclear, largely due to the absence of QA datasets that provide fine-grained reasoning structures. To address this gap, we introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which includes both semantic contexts and reasoning structures for QA pairs. Unlike existing M-QA datasets, where different reasoning structures are entangled together, GRS-QA explicitly captures intricate reasoning pathways by constructing reasoning graphs, where nodes represent textual contexts and edges denote logical flows. These reasoning graphs of different structures enable a fine-grained evaluation of LLM reasoning capabilities across various reasoning structures. Our empirical analysis reveals that LLMs perform differently when handling questions with varying reasoning structures. This finding facilitates the exploration of textual structures as compared with semantics.",
            "score": 1,
            "issue_id": 409,
            "pub_date": "2024-11-01",
            "pub_date_card": {
                "ru": "1 ноября",
                "en": "November 1",
                "zh": "11月1日"
            },
            "hash": "b3e4773e065d1bc1",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Графы рассуждений раскрывают возможности языковых моделей",
                    "desc": "Статья представляет новый набор данных GRS-QA для оценки способностей больших языковых моделей (LLM) в многоходовых вопросно-ответных задачах. GRS-QA включает в себя как семантические контексты, так и структуры рассуждений для пар вопрос-ответ. Набор данных использует графы рассуждений, где узлы представляют текстовые контексты, а ребра обозначают логические связи. Эмпирический анализ показывает, что LLM по-разному справляются с вопросами, имеющими различные структуры рассуждений."
                },
                "en": {
                    "title": "Unlocking LLMs: Understanding Reasoning Structures in Multi-Hop QA",
                    "desc": "This paper discusses the performance of Large Language Models (LLMs) in multi-hop question-answering (M-QA) tasks, focusing on their reasoning abilities. The authors identify a gap in existing datasets that do not provide detailed reasoning structures, which are crucial for evaluating LLM performance. To fill this gap, they introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which features reasoning graphs that clearly outline the logical pathways for answering questions. Their analysis shows that LLMs exhibit varying performance based on the complexity of the reasoning structures involved, highlighting the importance of understanding both textual and semantic elements in M-QA."
                },
                "zh": {
                    "title": "揭示推理结构对LLM表现的影响",
                    "desc": "大型语言模型（LLMs）在多跳问答（M-QA）中表现出色，主要得益于其先进的推理能力。然而，LLM在多跳问答中的表现受固有推理结构的影响尚不明确，主要是因为缺乏提供细粒度推理结构的问答数据集。为了解决这个问题，我们引入了图推理结构问答数据集（GRS-QA），该数据集为问答对提供了语义上下文和推理结构。与现有的M-QA数据集不同，GRS-QA通过构建推理图来明确捕捉复杂的推理路径，节点表示文本上下文，边表示逻辑流，从而实现对LLM推理能力的细粒度评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.22901",
            "title": "HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models",
            "url": "https://huggingface.co/papers/2410.22901",
            "abstract": "We propose an effective method for inserting adapters into text-to-image foundation models, which enables the execution of complex downstream tasks while preserving the generalization ability of the base model. The core idea of this method is to optimize the attention mechanism related to 2D feature maps, which enhances the performance of the adapter. This approach was validated on the task of meme video generation and achieved significant results. We hope this work can provide insights for post-training tasks of large text-to-image models. Additionally, as this method demonstrates good compatibility with SD1.5 derivative models, it holds certain value for the open-source community. Therefore, we will release the related code (https://songkey.github.io/hellomeme).",
            "score": 1,
            "issue_id": 408,
            "pub_date": "2024-10-30",
            "pub_date_card": {
                "ru": "30 октября",
                "en": "October 30",
                "zh": "10月30日"
            },
            "hash": "801963cbdcf75d7b",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Адаптеры для текст-в-изображение моделей: новый подход к генерации мемов",
                    "desc": "Статья представляет эффективный метод внедрения адаптеров в базовые модели преобразования текста в изображение. Этот подход оптимизирует механизм внимания, связанный с двумерными картами признаков, что улучшает производительность адаптера. Метод был успешно применен для генерации мемов в видеоформате. Авторы отмечают совместимость метода с производными моделями SD1.5, что делает его ценным для сообщества открытого исходного кода."
                },
                "en": {
                    "title": "Enhancing Text-to-Image Models with Adapter Integration",
                    "desc": "This paper presents a novel method for integrating adapters into text-to-image foundation models, allowing them to perform complex tasks while maintaining their ability to generalize. The method focuses on optimizing the attention mechanism associated with 2D feature maps, which significantly boosts the performance of the adapters. The effectiveness of this approach was demonstrated through the task of meme video generation, yielding impressive results. The authors aim to contribute to the open-source community by sharing their code and providing insights for post-training tasks in large text-to-image models."
                },
                "zh": {
                    "title": "适配器插入：提升文本到图像模型的能力",
                    "desc": "本文提出了一种有效的方法，将适配器插入文本到图像的基础模型中，从而在执行复杂的下游任务时保持基础模型的泛化能力。该方法的核心思想是优化与二维特征图相关的注意力机制，从而增强适配器的性能。我们在生成表情包视频的任务上验证了该方法，并取得了显著的结果。希望这项工作能为大型文本到图像模型的后训练任务提供一些见解，并为开源社区带来价值。"
                }
            }
        }
    ],
    "link_prev": "2024-11-01.html",
    "link_next": "2024-11-05.html",
    "short_date_prev": {
        "ru": "01.11",
        "en": "11/01",
        "zh": "11月1日"
    },
    "short_date_next": {
        "ru": "05.11",
        "en": "11/05",
        "zh": "11月5日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#medicine": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    },
    "zh": {
        "text": "这篇文章讨论了稀疏自编码器（SAEs）在大语言模型（LLMs）中的应用。SAEs可以将不易解释的中间表示分解成易解释的特征，从而更好地控制和分析。然而，类似的分析和方法在文本到图像的模型中尚缺乏研究。作者研究了在少步文本到图像扩散模型（如SDXL Turbo）中使用SAEs学习可解释特征的可能性。结果发现，SAEs学到的特征可解释，并对生成过程产生因果影响，揭示了模型内部的专业化。具体来说，有一个模块主要处理图像组合，一个负责添加细节，另一个负责颜色、光照和风格。因此，这项工作是理解生成文本到图像模型内部的重要一步，展示了SAEs在视觉领域的潜力。",
        "title": "Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders",
        "pinyin": "这篇文章讨论了稀疏自编码器（SAEs）在大语言模型（LLMs）中的应用。SAEs可以将不易解释的中间表示分解成易解释的特征，从而更好地控制和分析。然而，类似的分析和方法在文本到图像的模型中尚缺乏研究。作者研究了在少步文本到图像扩散模型（如SDXL Turbo）中使用SAEs学习可解释特征的可能性。结果发现，SAEs学到的特征可解释，并对生成过程产生因果影响，揭示了模型内部的专业化。具体来说，有一个模块主要处理图像组合，一个负责添加细节，另一个负责颜色、光照和风格。因此，这项工作是理解生成文本到图像模型内部的重要一步，展示了SAEs在视觉领域的潜力。\n\nPinyin transcription:\n\nZhè piān wénzhāng tǎolùn le xīshū zìbiǎnmǎqì (SAEs) zài dà yǔyán móxíng (LLMs) zhōng de yìngyòng. SAEs kěyǐ jiāng bùyì jiěshì de zhōngjiān biǎoshì fēnjiě chéng yì jiěshì de tèzhēng, cóng'ér gèng hǎo de kòngzhì hé fēnxī. Rán'ér, lèisì de fēnxī hé fāngfǎ zài wénběn dào túxiàng de móxíng zhōng shàng quēfá yánjiū. Zuòzhě yánjiū le zài shǎo bù wénběn dào túxiàng kuòsàn móxíng (rú SDXL Turbo) zhōng shǐyòng SAEs xuéxí kě jiěshì tèzhēng de kěnéngxìng. Jiéguǒ fāxiàn, SAEs xué dào de tèzhēng kě jiěshì, bìng duì shēngchéng guòchéng chǎnshēng yīnguǒ yǐngxiǎng, jiēshì le móxíng nèibù de zhuānménhuà. Jùtǐ lái shuō, yǒu yīgè mókuài zhǔyào chǔlǐ túxiàng zǔhé, yīgè fùzé tiānjiǎ xìjiě, lìng yīgè fùzé yánsè, guāngzhào hé fēnggé. Yīncǐ, zhè xiàng gōngzuò shì liǎojiě shēngchéng wénběn dào túxiàng móxíng nèibù de zhòngyào yī bù, zhǎnshì le SAEs zài shìjué lǐngyù de qiánlì.",
        "vocab": "[\n    {\"word\": \"稀疏自编码器\", \"pinyin\": \"xī shū zì biān mǎ qì\", \"trans\": \"Sparse Autoencoder\"},\n    {\"word\": \"大语言模型\", \"pinyin\": \"dà yǔ yán mó xíng\", \"trans\": \"Large Language Model\"},\n    {\"word\": \"中间表示\", \"pinyin\": \"zhōng jiān biǎo shì\", \"trans\": \"Intermediate Representation\"},\n    {\"word\": \"特征\", \"pinyin\": \"tè zhēng\", \"trans\": \"Feature\"},\n    {\"word\": \"分解\", \"pinyin\": \"fēn jiě\", \"trans\": \"Decompose\"},\n    {\"word\": \"控制\", \"pinyin\": \"kòng zhì\", \"trans\": \"Control\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēn xī\", \"trans\": \"Analyze\"},\n    {\"word\": \"文本到图像\", \"pinyin\": \"wén běn dào tú xiàng\", \"trans\": \"Text-to-Image\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"Model\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"Research\"},\n    {\"word\": \"少步\", \"pinyin\": \"shǎo bù\", \"trans\": \"Few-Step\"},\n    {\"word\": \"扩散模型\", \"pinyin\": \"kuò sàn mó xíng\", \"trans\": \"Diffusion Model\"},\n    {\"word\": \"可解释\", \"pinyin\": \"kě jiě shì\", \"trans\": \"Interpretable\"},\n    {\"word\": \"因果影响\", \"pinyin\": \"yīn guǒ yǐng xiǎng\", \"trans\": \"Causal Effect\"},\n    {\"word\": \"揭示\", \"pinyin\": \"jiē shì\", \"trans\": \"Reveal\"},\n    {\"word\": \"专业化\", \"pinyin\": \"zhuān yè huà\", \"trans\": \"Specialization\"},\n    {\"word\": \"模块\", \"pinyin\": \"mó kuài\", \"trans\": \"Module\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"Process\"},\n    {\"word\": \"图像组合\", \"pinyin\": \"tú xiàng zǔ hé\", \"trans\": \"Image Composition\"},\n    {\"word\": \"细节\", \"pinyin\": \"xì jiě\", \"trans\": \"Detail\"},\n    {\"word\": \"颜色\", \"pinyin\": \"yán sè\", \"trans\": \"Color\"},\n    {\"word\": \"光照\", \"pinyin\": \"guāng zhào\", \"trans\": \"Lighting\"},\n    {\"word\": \"风格\", \"pinyin\": \"fēng gē\", \"trans\": \"Style\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"Generate\"},\n    {\"word\": \"视觉领域\", \"pinyin\": \"shì jué lǐng yù\", \"trans\": \"Visual Domain\"},\n    {\"word\": \"潜力\", \"pinyin\": \"qián lì\", \"trans\": \"Potential\"}\n]",
        "trans": "This article discusses the application of Sparse Autoencoders (SAEs) in large language models (LLMs). SAEs can decompose hard-to-interpret intermediate representations into interpretable features, thereby enabling better control and analysis. However, similar analyses and methods in text-to-image models are still lacking in research. The authors investigated the possibility of using SAEs to learn interpretable features in few-step text-to-image diffusion models (such as SDXL Turbo). The results showed that the features learned by SAEs are interpretable and have a causal impact on the generation process, revealing specialization within the model. Specifically, one module mainly handles image composition, another is responsible for adding details, and another deals with color, lighting, and style. Therefore, this work is an important step towards understanding the internal workings of text-to-image generation models and demonstrates the potential of SAEs in the visual domain.",
        "update_ts": "2024-11-03 09:12"
    }
}