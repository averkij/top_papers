{
    "date": {
        "ru": "30 декабря",
        "en": "December 30",
        "zh": "12月30日"
    },
    "time_utc": "2024-12-30 03:17",
    "weekday": 0,
    "issue_id": 1381,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.18605",
            "title": "Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models",
            "url": "https://huggingface.co/papers/2412.18605",
            "abstract": "Orientation is a key attribute of objects, crucial for understanding their spatial pose and arrangement in images. However, practical solutions for accurate orientation estimation from a single image remain underexplored. In this work, we introduce Orient Anything, the first expert and foundational model designed to estimate object orientation in a single- and free-view image. Due to the scarcity of labeled data, we propose extracting knowledge from the 3D world. By developing a pipeline to annotate the front face of 3D objects and render images from random views, we collect 2M images with precise orientation annotations. To fully leverage the dataset, we design a robust training objective that models the 3D orientation as probability distributions of three angles and predicts the object orientation by fitting these distributions. Besides, we employ several strategies to improve synthetic-to-real transfer. Our model achieves state-of-the-art orientation estimation accuracy in both rendered and real images and exhibits impressive zero-shot ability in various scenarios. More importantly, our model enhances many applications, such as comprehension and generation of complex spatial concepts and 3D object pose adjustment.",
            "score": 1,
            "issue_id": 1381,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 декабря",
                "en": "December 24",
                "zh": "12月24日"
            },
            "hash": "6bc56feed4022217",
            "authors": [
                "Zehan Wang",
                "Ziang Zhang",
                "Tianyu Pang",
                "Chao Du",
                "Hengshuang Zhao",
                "Zhou Zhao"
            ],
            "affiliations": [
                "Sea AI Lab",
                "The University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18605.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#training",
                    "#dataset",
                    "#3d",
                    "#transfer_learning"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "Orient Anything: точное определение ориентации объектов по одному изображению",
                    "desc": "Статья представляет новую модель Orient Anything для оценки ориентации объектов на изображениях. Авторы создали датасет из 2 миллионов синтетических изображений с точными аннотациями ориентации. Модель использует вероятностный подход, моделируя ориентацию как распределения трех углов. Orient Anything достигает наилучших результатов в оценке ориентации на синтетических и реальных изображениях, демонстрируя способность к обобщению."
                },
                "en": {
                    "title": "Revolutionizing Object Orientation Estimation from Images",
                    "desc": "This paper presents 'Orient Anything', a novel model for estimating the orientation of objects from single images. It addresses the challenge of limited labeled data by utilizing 3D object knowledge to create a large dataset of 2 million images with accurate orientation labels. The model employs a training objective that treats object orientation as probability distributions of angles, allowing it to predict orientations effectively. Additionally, it demonstrates strong performance in both synthetic and real-world scenarios, enhancing applications related to spatial understanding and 3D object manipulation."
                },
                "zh": {
                    "title": "单图像方向估计的新突破",
                    "desc": "本文介绍了一种名为Orient Anything的模型，旨在从单张图像中准确估计物体的方向。由于标注数据稀缺，我们通过从3D世界提取知识，收集了200万张带有精确方向标注的图像。该模型通过将3D方向建模为三个角度的概率分布，来预测物体的方向，并采用多种策略提高合成图像到真实图像的迁移效果。最终，我们的模型在渲染和真实图像中都达到了最先进的方向估计精度，并在多种场景中展现了出色的零样本能力。"
                }
            }
        }
    ],
    "link_prev": "2024-12-27.html",
    "link_next": "2024-12-31.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "27.12",
        "en": "12/27",
        "zh": "12月27日"
    },
    "short_date_next": {
        "ru": "31.12",
        "en": "12/31",
        "zh": "12月31日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一个高效的大语言模型YuLan-Mini。该模型有2.42B参数，性能卓越。研究团队通过数据清洗、优化方法和退火技术提高了训练效果。YuLan-Mini在1.08T tokens上训练，性能媲美需要更多数据的行业领先模型。详细信息可以在GitHub上找到。",
        "title": "YuLan-Mini: An Open Data-efficient Language Model",
        "pinyin": "这篇文章介绍了一个高效的大语言模型YuLan-Mini。该模型有2.42B参数，性能卓越。研究团队通过数据清洗、优化方法和退火技术提高了训练效果。YuLan-Mini在1.08T tokens上训练，性能媲美需要更多数据的行业领先模型。详细信息可以在GitHub上找到。\n\nzhè piān wén zhāng jiè shào le yī gè gāo xiào de dà yǔ yán mó xíng YuLan-Mini. gǎi mó xíng yǒu 2.42B cān shǔ, xìng néng zhuó yuè. yán jiū tuán duī tōng guò shù jù qīng xǐ, yōu huà fāng fǎ hé tuì huǒ jì shù tí gāo le xùn liàn xiào guǒ. YuLan-Mini zài 1.08T tokens shàng xùn liàn, xìng néng jì mǐ xū yào gèng duō shù jù de háng yè lǐng xiān mó xíng. xiáng xì xìn xī kě yǐ zài GitHub shàng zhǎo dào.",
        "vocab": "[\n    {\"word\": \"卓越\", \"pinyin\": \"zhuó yuè\", \"trans\": \"outstanding\"},\n    {\"word\": \"清洗\", \"pinyin\": \"qīng xǐ\", \"trans\": \"cleaning\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimization\"},\n    {\"word\": \"退火\", \"pinyin\": \"tuì huǒ\", \"trans\": \"annealing\"},\n    {\"word\": \"媲美\", \"pinyin\": \"pì měi\", \"trans\": \"rival\"},\n    {\"word\": \"领先\", \"pinyin\": \"lǐng xiān\", \"trans\": \"leading\"}\n]",
        "trans": "This article introduces an efficient large language model called YuLan-Mini. The model has 2.42 billion parameters and delivers outstanding performance. The research team enhanced the training effectiveness through data cleaning, optimization methods, and annealing techniques. YuLan-Mini was trained on 1.08 trillion tokens and performs comparably to industry-leading models that require more data. Detailed information can be found on GitHub.",
        "update_ts": "2024-12-29 12:38"
    }
}