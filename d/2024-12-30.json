{
    "date": {
        "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 30",
        "zh": "12æœˆ30æ—¥"
    },
    "time_utc": "2024-12-30 21:09",
    "weekday": 0,
    "issue_id": 1399,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.18925",
            "title": "HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs",
            "url": "https://huggingface.co/papers/2412.18925",
            "abstract": "The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains.",
            "score": 50,
            "issue_id": 1382,
            "pub_date": "2024-12-25",
            "pub_date_card": {
                "ru": "25 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 25",
                "zh": "12æœˆ25æ—¥"
            },
            "hash": "218dd2a8c2ae478f",
            "authors": [
                "Junying Chen",
                "Zhenyang Cai",
                "Ke Ji",
                "Xidong Wang",
                "Wanlong Liu",
                "Rongsheng Wang",
                "Jianye Hou",
                "Benyou Wang"
            ],
            "affiliations": [
                "Shenzhen Research Institute of Big Data",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18925.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#healthcare",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ HuatuoGPT-01, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Enhancing Medical Reasoning with HuatuoGPT-o1",
                    "desc": "This paper discusses the development of HuatuoGPT-o1, a medical language model (LLM) that enhances reasoning capabilities specifically for medical tasks. Unlike traditional approaches that focus on mathematical reasoning, this research emphasizes the need for robust reasoning in healthcare, where accuracy is critical. The authors propose a two-stage method that includes a medical verifier to ensure the correctness of model outputs and reinforcement learning (RL) to further improve reasoning skills. The results demonstrate that HuatuoGPT-o1 significantly outperforms existing models by effectively solving complex medical problems using a limited dataset of verifiable problems."
                },
                "zh": {
                    "title": "åŒ»å­¦æ¨ç†çš„æ–°çªç ´ï¼šHuatuoGPT-o1",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†OpenAIçš„o1åœ¨å¢å¼ºæ¨ç†èƒ½åŠ›æ–¹é¢çš„çªç ´ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ã€‚å°½ç®¡å¤§å¤šæ•°æ¨ç†ç ”ç©¶é›†ä¸­åœ¨æ•°å­¦ä»»åŠ¡ä¸Šï¼Œä½†åŒ»å­¦åŒæ ·éœ€è¦å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ä»¥æä¾›å¯é çš„ç­”æ¡ˆã€‚ä¸ºäº†éªŒè¯åŒ»å­¦æ¨ç†çš„æ­£ç¡®æ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¯éªŒè¯çš„åŒ»å­¦é—®é¢˜å’ŒåŒ»å­¦éªŒè¯å™¨ï¼Œå¸®åŠ©æ£€æŸ¥æ¨¡å‹è¾“å‡ºçš„å‡†ç¡®æ€§ã€‚æœ€ç»ˆï¼Œè®ºæ–‡ä»‹ç»äº†HuatuoGPT-o1ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿè¿›è¡Œå¤æ‚æ¨ç†çš„åŒ»å­¦å¤§è¯­è¨€æ¨¡å‹ï¼Œå®éªŒè¡¨æ˜å…¶åœ¨è§£å†³åŒ»å­¦é—®é¢˜æ—¶è¡¨ç°ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18619",
            "title": "Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey",
            "url": "https://huggingface.co/papers/2412.18619",
            "abstract": "Building on the foundations of language modeling in natural language processing, Next Token Prediction (NTP) has evolved into a versatile training objective for machine learning tasks across various modalities, achieving considerable success. As Large Language Models (LLMs) have advanced to unify understanding and generation tasks within the textual modality, recent research has shown that tasks from different modalities can also be effectively encapsulated within the NTP framework, transforming the multimodal information into tokens and predict the next one given the context. This survey introduces a comprehensive taxonomy that unifies both understanding and generation within multimodal learning through the lens of NTP. The proposed taxonomy covers five key aspects: Multimodal tokenization, MMNTP model architectures, unified task representation, datasets \\& evaluation, and open challenges. This new taxonomy aims to aid researchers in their exploration of multimodal intelligence. An associated GitHub repository collecting the latest papers and repos is available at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction",
            "score": 15,
            "issue_id": 1392,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "48fd5f4393f283a7",
            "authors": [
                "Liang Chen",
                "Zekun Wang",
                "Shuhuai Ren",
                "Lei Li",
                "Haozhe Zhao",
                "Yunshui Li",
                "Zefan Cai",
                "Hongcheng Guo",
                "Lei Zhang",
                "Yizhe Xiong",
                "Yichi Zhang",
                "Ruoyu Wu",
                "Qingxiu Dong",
                "Ge Zhang",
                "Jian Yang",
                "Lingwei Meng",
                "Shujie Hu",
                "Yulong Chen",
                "Junyang Lin",
                "Shuai Bai",
                "Andreas Vlachos",
                "Xu Tan",
                "Minjia Zhang",
                "Wen Xiao",
                "Aaron Yee",
                "Tianyu Liu",
                "Baobao Chang"
            ],
            "affiliations": [
                "Beihang University",
                "Peking University",
                "Shenzhen Institute of Advanced Technology, China Academy of Sciences",
                "Tsinghua University",
                "University of Hongkong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18619.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#architecture",
                    "#survey"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€, Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° (NTP) Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ NTP. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ²: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ MMNTP, ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. Ğ¦ĞµĞ»ÑŒ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ - Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ² Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Unifying Multimodal Learning through Next Token Prediction",
                    "desc": "This paper discusses the Next Token Prediction (NTP) approach in machine learning, which has become a key method for training models across different types of data. It highlights how NTP can be applied not only to text but also to other modalities by converting various forms of information into tokens for prediction. The authors present a new taxonomy that organizes the understanding and generation tasks in multimodal learning, focusing on aspects like tokenization and model architectures. This framework is designed to support researchers in advancing multimodal intelligence and addresses current challenges in the field."
                },
                "zh": {
                    "title": "ä¸‹ä¸€æ ‡è®°é¢„æµ‹ï¼šå¤šæ¨¡æ€å­¦ä¹ çš„æ–°è§†è§’",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¸‹ä¸€æ ‡è®°é¢„æµ‹ï¼ˆNTPï¼‰åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒå…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é‡è¦æ€§ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼ŒNTPä¸ä»…é€‚ç”¨äºæ–‡æœ¬ä»»åŠ¡ï¼Œè¿˜èƒ½æœ‰æ•ˆå¤„ç†å…¶ä»–æ¨¡æ€çš„ä¿¡æ¯ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åˆ†ç±»æ³•ï¼Œæ¶µç›–äº†å¤šæ¨¡æ€æ ‡è®°åŒ–ã€MMNTPæ¨¡å‹æ¶æ„ã€ç»Ÿä¸€ä»»åŠ¡è¡¨ç¤ºã€æ•°æ®é›†ä¸è¯„ä¼°ä»¥åŠå¼€æ”¾æŒ‘æˆ˜ç­‰äº”ä¸ªå…³é”®æ–¹é¢ã€‚è¯¥åˆ†ç±»æ³•æ—¨åœ¨å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°æ¢ç´¢å¤šæ¨¡æ€æ™ºèƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.19326",
            "title": "Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment",
            "url": "https://huggingface.co/papers/2412.19326",
            "abstract": "Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals though they give comprehensive perception and reasoning in a spectrum of vision applications. Recent studies either develop tool-using or unify specific visual tasks into the autoregressive framework, often at the expense of overall multimodal performance. To address this issue and enhance MLLMs with visual tasks in a scalable fashion, we propose Task Preference Optimization (TPO), a novel method that utilizes differentiable task preferences derived from typical fine-grained visual tasks. TPO introduces learnable task tokens that establish connections between multiple task-specific heads and the MLLM. By leveraging rich visual labels during training, TPO significantly enhances the MLLM's multimodal capabilities and task-specific performance. Through multi-task co-training within TPO, we observe synergistic benefits that elevate individual task performance beyond what is achievable through single-task training methodologies. Our instantiation of this approach with VideoChat and LLaVA demonstrates an overall 14.6% improvement in multimodal performance compared to baseline models. Additionally, MLLM-TPO demonstrates robust zero-shot capabilities across various tasks, performing comparably to state-of-the-art supervised models. The code will be released at https://github.com/OpenGVLab/TPO",
            "score": 11,
            "issue_id": 1383,
            "pub_date": "2024-12-26",
            "pub_date_card": {
                "ru": "26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 26",
                "zh": "12æœˆ26æ—¥"
            },
            "hash": "84d0f2e573ba96b2",
            "authors": [
                "Ziang Yan",
                "Zhilin Li",
                "Yinan He",
                "Chenting Wang",
                "Kunchang Li",
                "Xinhao Li",
                "Xiangyu Zeng",
                "Zilei Wang",
                "Yali Wang",
                "Yu Qiao",
                "Limin Wang",
                "Yi Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.19326.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Task Preference Optimization (TPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. TPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ MLLM Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 14.6% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multimodal Models with Task Preference Optimization",
                    "desc": "This paper introduces Task Preference Optimization (TPO), a new method designed to improve multimodal large language models (MLLMs) in understanding visual tasks. TPO uses learnable task tokens to connect different task-specific heads with the MLLM, allowing for better integration of fine-grained visual information. By training with rich visual labels and employing multi-task co-training, TPO enhances both the overall multimodal performance and the performance on specific tasks. The results show a significant improvement in performance, with TPO achieving a 14.6% increase compared to baseline models, while also demonstrating strong zero-shot capabilities."
                },
                "zh": {
                    "title": "ä»»åŠ¡åå¥½ä¼˜åŒ–ï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›",
                    "desc": "å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç»†ç²’åº¦çš„è§†è§‰ä»»åŠ¡ä¸Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºä»»åŠ¡åå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰ï¼Œå®ƒé€šè¿‡å¯å¾®åˆ†çš„ä»»åŠ¡åå¥½æ¥å¢å¼ºMLLMsçš„è§†è§‰ä»»åŠ¡èƒ½åŠ›ã€‚TPOå¼•å…¥äº†å¯å­¦ä¹ çš„ä»»åŠ¡æ ‡è®°ï¼Œå»ºç«‹äº†å¤šä¸ªç‰¹å®šä»»åŠ¡å¤´ä¸MLLMä¹‹é—´çš„è¿æ¥ï¼Œä»è€Œæå‡äº†å¤šæ¨¡æ€èƒ½åŠ›å’Œä»»åŠ¡ç‰¹å®šæ€§èƒ½ã€‚é€šè¿‡å¤šä»»åŠ¡å…±åŒè®­ç»ƒï¼ŒTPOæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ•´ä½“è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18653",
            "title": "1.58-bit FLUX",
            "url": "https://huggingface.co/papers/2412.18653",
            "abstract": "We present 1.58-bit FLUX, the first successful approach to quantizing the state-of-the-art text-to-image generation model, FLUX.1-dev, using 1.58-bit weights (i.e., values in {-1, 0, +1}) while maintaining comparable performance for generating 1024 x 1024 images. Notably, our quantization method operates without access to image data, relying solely on self-supervision from the FLUX.1-dev model. Additionally, we develop a custom kernel optimized for 1.58-bit operations, achieving a 7.7x reduction in model storage, a 5.1x reduction in inference memory, and improved inference latency. Extensive evaluations on the GenEval and T2I Compbench benchmarks demonstrate the effectiveness of 1.58-bit FLUX in maintaining generation quality while significantly enhancing computational efficiency.",
            "score": 10,
            "issue_id": 1396,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "6476ec807199e8af",
            "authors": [
                "Chenglin Yang",
                "Celong Liu",
                "Xueqing Deng",
                "Dongwon Kim",
                "Xing Mei",
                "Xiaohui Shen",
                "Liang-Chieh Chen"
            ],
            "affiliations": [
                "ByteDance",
                "POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18653.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#inference",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ FLUX.1-dev Ğ´Ğ¾ 1,58 Ğ±Ğ¸Ñ‚. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ´Ñ€Ğ¾ Ğ´Ğ»Ñ 1,58-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Efficient Image Generation with 1.58-bit FLUX",
                    "desc": "This paper introduces 1.58-bit FLUX, a novel method for quantizing the FLUX.1-dev text-to-image generation model. By using only 1.58-bit weights, which can take values of -1, 0, or +1, the model achieves similar performance in generating high-resolution images while drastically reducing storage and memory requirements. The quantization process is unique as it does not require access to image data, instead utilizing self-supervision from the existing model. The authors also present a custom kernel that optimizes 1.58-bit operations, leading to significant improvements in computational efficiency without compromising image generation quality."
                },
                "zh": {
                    "title": "é«˜æ•ˆé‡åŒ–ï¼Œå›¾åƒç”Ÿæˆæ–°çªç ´",
                    "desc": "æˆ‘ä»¬æå‡ºäº†1.58-bit FLUXï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæˆåŠŸé‡åŒ–æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹FLUX.1-devçš„æ–¹æ³•ï¼Œä½¿ç”¨1.58-bitæƒé‡ï¼ˆå³{-1, 0, +1}çš„å€¼ï¼‰ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆ1024 x 1024å›¾åƒçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„é‡åŒ–æ–¹æ³•ä¸ä¾èµ–äºå›¾åƒæ•°æ®ï¼Œè€Œæ˜¯ä»…ä¾é FLUX.1-devæ¨¡å‹çš„è‡ªæˆ‘ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé’ˆå¯¹1.58-bitæ“ä½œä¼˜åŒ–çš„è‡ªå®šä¹‰å†…æ ¸ï¼Œå®ç°äº†æ¨¡å‹å­˜å‚¨å‡å°‘7.7å€ï¼Œæ¨ç†å†…å­˜å‡å°‘5.1å€ï¼Œå¹¶æ”¹å–„äº†æ¨ç†å»¶è¿Ÿã€‚åœ¨GenEvalå’ŒT2I CompbenchåŸºå‡†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†1.58-bit FLUXåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18605",
            "title": "Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models",
            "url": "https://huggingface.co/papers/2412.18605",
            "abstract": "Orientation is a key attribute of objects, crucial for understanding their spatial pose and arrangement in images. However, practical solutions for accurate orientation estimation from a single image remain underexplored. In this work, we introduce Orient Anything, the first expert and foundational model designed to estimate object orientation in a single- and free-view image. Due to the scarcity of labeled data, we propose extracting knowledge from the 3D world. By developing a pipeline to annotate the front face of 3D objects and render images from random views, we collect 2M images with precise orientation annotations. To fully leverage the dataset, we design a robust training objective that models the 3D orientation as probability distributions of three angles and predicts the object orientation by fitting these distributions. Besides, we employ several strategies to improve synthetic-to-real transfer. Our model achieves state-of-the-art orientation estimation accuracy in both rendered and real images and exhibits impressive zero-shot ability in various scenarios. More importantly, our model enhances many applications, such as comprehension and generation of complex spatial concepts and 3D object pose adjustment.",
            "score": 9,
            "issue_id": 1381,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "6bc56feed4022217",
            "authors": [
                "Zehan Wang",
                "Ziang Zhang",
                "Tianyu Pang",
                "Chao Du",
                "Hengshuang Zhao",
                "Zhou Zhao"
            ],
            "affiliations": [
                "Sea AI Lab",
                "The University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18605.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#training",
                    "#dataset",
                    "#3d",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Orient Anything: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Orient Anything Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ… ÑƒĞ³Ğ»Ğ¾Ğ². Orient Anything Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Object Orientation Estimation from Images",
                    "desc": "This paper presents 'Orient Anything', a novel model for estimating the orientation of objects from single images. It addresses the challenge of limited labeled data by utilizing 3D object knowledge to create a large dataset of 2 million images with accurate orientation labels. The model employs a training objective that treats object orientation as probability distributions of angles, allowing it to predict orientations effectively. Additionally, it demonstrates strong performance in both synthetic and real-world scenarios, enhancing applications related to spatial understanding and 3D object manipulation."
                },
                "zh": {
                    "title": "å•å›¾åƒæ–¹å‘ä¼°è®¡çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOrient Anythingçš„æ¨¡å‹ï¼Œæ—¨åœ¨ä»å•å¼ å›¾åƒä¸­å‡†ç¡®ä¼°è®¡ç‰©ä½“çš„æ–¹å‘ã€‚ç”±äºæ ‡æ³¨æ•°æ®ç¨€ç¼ºï¼Œæˆ‘ä»¬é€šè¿‡ä»3Dä¸–ç•Œæå–çŸ¥è¯†ï¼Œæ”¶é›†äº†200ä¸‡å¼ å¸¦æœ‰ç²¾ç¡®æ–¹å‘æ ‡æ³¨çš„å›¾åƒã€‚è¯¥æ¨¡å‹é€šè¿‡å°†3Dæ–¹å‘å»ºæ¨¡ä¸ºä¸‰ä¸ªè§’åº¦çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ¥é¢„æµ‹ç‰©ä½“çš„æ–¹å‘ï¼Œå¹¶é‡‡ç”¨å¤šç§ç­–ç•¥æé«˜åˆæˆå›¾åƒåˆ°çœŸå®å›¾åƒçš„è¿ç§»æ•ˆæœã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ¸²æŸ“å’ŒçœŸå®å›¾åƒä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ–¹å‘ä¼°è®¡ç²¾åº¦ï¼Œå¹¶åœ¨å¤šç§åœºæ™¯ä¸­å±•ç°äº†å‡ºè‰²çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17762",
            "title": "The Superposition of Diffusion Models Using the ItÃ´ Density Estimator",
            "url": "https://huggingface.co/papers/2412.17762",
            "abstract": "The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. SuperDiff leverages a new scalable It\\^o density estimator for the log likelihood of the diffusion SDE which incurs no additional overhead compared to the well-known Hutchinson's estimator needed for divergence calculations. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performed solely through composition during inference, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiff is efficient during inference time, and mimics traditional composition operators such as the logical OR and the logical AND. We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, and improved unconditional de novo structure design of proteins. https://github.com/necludov/super-diffusion",
            "score": 8,
            "issue_id": 1387,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "8f698bcd1c639eb2",
            "authors": [
                "Marta Skreta",
                "Lazar Atanackovic",
                "Avishek Joey Bose",
                "Alexander Tong",
                "Kirill Neklyudov"
            ],
            "affiliations": [
                "Mila - Quebec AI Institute",
                "University of Oxford",
                "University of Toronto",
                "UniversitÃ© de MontrÃ©al",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17762.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#data",
                    "#inference",
                    "#cv"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "SuperDiff: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SuperDiff Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ 'ÑÑƒĞ¿ĞµÑ€Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸', Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. SuperDiff Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¡Ğ”Ğ£. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ Ğ±ĞµĞ»ĞºĞ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€."
                },
                "en": {
                    "title": "Effortless Fusion of Diffusion Models with SuperDiff",
                    "desc": "This paper introduces a new method called SuperDiff for combining multiple pre-trained diffusion models without the need for extensive re-training. The authors derive a theoretical framework based on the continuity equation and propose two algorithms that efficiently merge these models during the generation phase. SuperDiff utilizes a scalable ItÃ´ density estimator to calculate log likelihoods, ensuring minimal computational overhead. The method is shown to enhance image diversity, improve prompt-based image editing, and aid in protein structure design, demonstrating its practical applications in various domains."
                },
                "zh": {
                    "title": "é«˜æ•ˆç»“åˆé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºè¶…å åŠ ï¼ˆsuperpositionï¼‰ï¼Œç”¨äºåœ¨ç”Ÿæˆé˜¶æ®µç»“åˆå¤šä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ›´å¤§çš„æ¨¡å‹ã€‚æˆ‘ä»¬ä»è¿ç»­æ€§æ–¹ç¨‹å‡ºå‘ï¼Œç†è®ºä¸Šæ¨å¯¼å‡ºè¶…å åŠ ï¼Œå¹¶è®¾è®¡äº†ä¸¤ç§æ–°ç®—æ³•ï¼Œä¸“é—¨ç”¨äºåœ¨SuperDiffä¸­ç»“åˆæ‰©æ•£æ¨¡å‹ã€‚SuperDiffåˆ©ç”¨äº†ä¸€ç§æ–°çš„å¯æ‰©å±•ItÃ´å¯†åº¦ä¼°è®¡å™¨ï¼Œèƒ½å¤Ÿé«˜æ•ˆè®¡ç®—æ‰©æ•£éšæœºå¾®åˆ†æ–¹ç¨‹çš„å¯¹æ•°ä¼¼ç„¶ï¼Œè€Œä¸ä¼šå¢åŠ é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚é€šè¿‡è‡ªåŠ¨é‡åŠ æƒæ–¹æ¡ˆï¼ŒSuperDiffèƒ½å¤Ÿè½»æ¾å®ç°ä¸åŒé¢„è®­ç»ƒå‘é‡åœºçš„ç»„åˆï¼Œå¹¶åœ¨æ¨ç†æ—¶è¡¨ç°å‡ºé«˜æ•ˆæ€§ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å…·å¤šæ ·æ€§çš„å›¾åƒå’Œæ›´å‡†ç¡®çš„å›¾åƒç¼–è¾‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.19712",
            "title": "From Elements to Design: A Layered Approach for Automatic Graphic Design Composition",
            "url": "https://huggingface.co/papers/2412.19712",
            "abstract": "In this work, we investigate automatic design composition from multimodal graphic elements. Although recent studies have developed various generative models for graphic design, they usually face the following limitations: they only focus on certain subtasks and are far from achieving the design composition task; they do not consider the hierarchical information of graphic designs during the generation process. To tackle these issues, we introduce the layered design principle into Large Multimodal Models (LMMs) and propose a novel approach, called LaDeCo, to accomplish this challenging task. Specifically, LaDeCo first performs layer planning for a given element set, dividing the input elements into different semantic layers according to their contents. Based on the planning results, it subsequently predicts element attributes that control the design composition in a layer-wise manner, and includes the rendered image of previously generated layers into the context. With this insightful design, LaDeCo decomposes the difficult task into smaller manageable steps, making the generation process smoother and clearer. The experimental results demonstrate the effectiveness of LaDeCo in design composition. Furthermore, we show that LaDeCo enables some interesting applications in graphic design, such as resolution adjustment, element filling, design variation, etc. In addition, it even outperforms the specialized models in some design subtasks without any task-specific training.",
            "score": 6,
            "issue_id": 1383,
            "pub_date": "2024-12-27",
            "pub_date_card": {
                "ru": "27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 27",
                "zh": "12æœˆ27æ—¥"
            },
            "hash": "389e5dbbfa107ed6",
            "authors": [
                "Jiawei Lin",
                "Shizhao Sun",
                "Danqing Huang",
                "Ting Liu",
                "Ji Li",
                "Jiang Bian"
            ],
            "affiliations": [
                "Microsoft Research",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.19712.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸĞ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½-ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM). ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ LaDeCo Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ±Ğ¸Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹, Ğ´ĞµĞ»Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ LaDeCo Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½-ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ¸ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°."
                },
                "en": {
                    "title": "Layered Design for Smarter Graphic Composition",
                    "desc": "This paper presents LaDeCo, a novel approach for automatic design composition using Large Multimodal Models (LMMs). It addresses limitations in existing generative models by incorporating a layered design principle, which organizes graphic elements into semantic layers. LaDeCo enhances the design generation process by predicting element attributes in a structured, layer-wise manner, allowing for better management of complex design tasks. Experimental results show that LaDeCo not only improves design composition but also outperforms specialized models in certain subtasks without requiring specific training."
                },
                "zh": {
                    "title": "LaDeCoï¼šæ™ºèƒ½å›¾å½¢è®¾è®¡çš„åˆ†å±‚ç»„åˆæ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†ä»å¤šæ¨¡æ€å›¾å½¢å…ƒç´ è‡ªåŠ¨è®¾è®¡ç»„åˆçš„æ–¹æ³•ã€‚å°½ç®¡è¿‘æœŸæœ‰å¤šç§ç”Ÿæˆæ¨¡å‹ç”¨äºå›¾å½¢è®¾è®¡ï¼Œä½†å®ƒä»¬é€šå¸¸åªå…³æ³¨æŸäº›å­ä»»åŠ¡ï¼Œä¸”æœªèƒ½æœ‰æ•ˆå¤„ç†è®¾è®¡ç»„åˆä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å°†åˆ†å±‚è®¾è®¡åŸåˆ™å¼•å…¥å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºLaDeCoã€‚LaDeCoé€šè¿‡å¯¹è¾“å…¥å…ƒç´ è¿›è¡Œå±‚æ¬¡è§„åˆ’ï¼Œåˆ†ç¦»å‡ºä¸åŒè¯­ä¹‰å±‚ï¼Œä»è€Œä½¿ç”Ÿæˆè¿‡ç¨‹æ›´åŠ é¡ºç•…å’Œæ¸…æ™°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.19512",
            "title": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging",
            "url": "https://huggingface.co/papers/2412.19512",
            "abstract": "Fine-tuning large language models (LLMs) for downstream tasks is a widely adopted approach, but it often leads to safety degradation in safety-aligned LLMs. Currently, many solutions address this issue by incorporating additional safety data, which can be impractical in many cases. In this paper, we address the question: How can we improve downstream task performance while preserving safety in LLMs without relying on additional safety data? We propose a simple and effective method that maintains the inherent safety of LLMs while enhancing their downstream task performance: merging the weights of pre- and post-fine-tuned safety-aligned models. Experimental results across various downstream tasks, models, and merging methods demonstrate that this approach effectively mitigates safety degradation while improving downstream task performance, offering a practical solution for adapting safety-aligned LLMs.",
            "score": 5,
            "issue_id": 1389,
            "pub_date": "2024-12-27",
            "pub_date_card": {
                "ru": "27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 27",
                "zh": "12æœˆ27æ—¥"
            },
            "hash": "87b5eeead71b4993",
            "authors": [
                "Hua Farn",
                "Hsuan Su",
                "Shachi H Kumar",
                "Saurav Sahay",
                "Shang-Tse Chen",
                "Hung-yi Lee"
            ],
            "affiliations": [
                "Intel Lab",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.19512.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#alignment"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¸Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Performance While Preserving Safety in LLMs",
                    "desc": "This paper discusses the challenge of fine-tuning large language models (LLMs) for specific tasks without compromising their safety features. It highlights that traditional methods often require extra safety data, which can be difficult to obtain. The authors propose a novel technique that combines the weights of models before and after fine-tuning to enhance performance while preserving safety. Their experiments show that this merging method successfully reduces safety degradation and improves task performance, providing a practical solution for using safety-aligned LLMs."
                },
                "zh": {
                    "title": "æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œä¿éšœå®‰å…¨æ€§ï¼",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨ä¸ä¾èµ–é¢å¤–å®‰å…¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒæ—¶ä¿æŒå…¶å®‰å…¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡åˆå¹¶é¢„è°ƒä¼˜å’Œåè°ƒä¼˜çš„å®‰å…¨å¯¹é½æ¨¡å‹çš„æƒé‡ï¼Œæ¥å¢å¼ºä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡å’Œæ¨¡å‹ä¸­æœ‰æ•ˆå‡è½»äº†å®‰å…¨æ€§ä¸‹é™çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå®‰å…¨å¯¹é½çš„LLMsæä¾›äº†ä¸€ç§å®ç”¨çš„é€‚åº”æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.19645",
            "title": "VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models",
            "url": "https://huggingface.co/papers/2412.19645",
            "abstract": "Zero-shot customized video generation has gained significant attention due to its substantial application potential. Existing methods rely on additional models to extract and inject reference subject features, assuming that the Video Diffusion Model (VDM) alone is insufficient for zero-shot customized video generation. However, these methods often struggle to maintain consistent subject appearance due to suboptimal feature extraction and injection techniques. In this paper, we reveal that VDM inherently possesses the force to extract and inject subject features. Departing from previous heuristic approaches, we introduce a novel framework that leverages VDM's inherent force to enable high-quality zero-shot customized video generation. Specifically, for feature extraction, we directly input reference images into VDM and use its intrinsic feature extraction process, which not only provides fine-grained features but also significantly aligns with VDM's pre-trained knowledge. For feature injection, we devise an innovative bidirectional interaction between subject features and generated content through spatial self-attention within VDM, ensuring that VDM has better subject fidelity while maintaining the diversity of the generated video.Experiments on both customized human and object video generation validate the effectiveness of our framework.",
            "score": 3,
            "issue_id": 1383,
            "pub_date": "2024-12-27",
            "pub_date_card": {
                "ru": "27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 27",
                "zh": "12æœˆ27æ—¥"
            },
            "hash": "1728f5feb9ef107d",
            "authors": [
                "Tao Wu",
                "Yong Zhang",
                "Xiaodong Cun",
                "Zhongang Qi",
                "Junfu Pu",
                "Huanzhang Dou",
                "Guangcong Zheng",
                "Ying Shan",
                "Xi Li"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "College of Computer Science and Technology, Zhejiang University",
                "Huawei Noahs Ark Lab",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.19645.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° VDM Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ’Ğ¸Ğ´ĞµĞ¾ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞœĞ¾Ğ´ĞµĞ»Ğ¸ (VDM) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¿Ğ¾Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² VDM Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Harnessing VDM for High-Quality Zero-Shot Video Generation",
                    "desc": "This paper addresses the challenge of zero-shot customized video generation, which is the ability to create videos featuring specific subjects without prior training on those subjects. The authors argue that existing methods rely too heavily on external models for feature extraction and injection, leading to inconsistencies in subject appearance. They propose a new framework that utilizes the Video Diffusion Model (VDM) itself for both extracting and injecting subject features, enhancing the quality of the generated videos. Their approach includes a novel bidirectional interaction mechanism that improves subject fidelity while preserving the diversity of the output, as demonstrated through experiments with human and object video generation."
                },
                "zh": {
                    "title": "åˆ©ç”¨VDMå®ç°é«˜è´¨é‡é›¶-shotå®šåˆ¶è§†é¢‘ç”Ÿæˆ",
                    "desc": "é›¶-shotå®šåˆ¶è§†é¢‘ç”Ÿæˆå› å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›è€Œå—åˆ°å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–é¢å¤–æ¨¡å‹æå–å’Œæ³¨å…¥å‚è€ƒä¸»ä½“ç‰¹å¾ï¼Œå‡è®¾è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰å•ç‹¬æ— æ³•å®ç°é›¶-shotå®šåˆ¶è§†é¢‘ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨ä¿æŒä¸»ä½“å¤–è§‚ä¸€è‡´æ€§æ–¹é¢å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œåˆ©ç”¨VDMçš„å†…åœ¨ç‰¹æ€§ï¼Œå®ç°é«˜è´¨é‡çš„é›¶-shotå®šåˆ¶è§†é¢‘ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17606",
            "title": "SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images",
            "url": "https://huggingface.co/papers/2412.17606",
            "abstract": "Building a large-scale figure QA dataset requires a considerable amount of work, from gathering and selecting figures to extracting attributes like text, numbers, and colors, and generating QAs. Although recent developments in LLMs have led to efforts to synthesize figures, most of these focus primarily on QA generation. Additionally, creating figures directly using LLMs often encounters issues such as code errors, similar-looking figures, and repetitive content in figures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic Figures), a dataset for pre-training figure QA. Our proposed pipeline enables the creation of chart figures with complete annotations of the visualized data and dense QA annotations without any manual annotation process. Our stage-by-stage pipeline makes it possible to create diverse topic and appearance figures efficiently while minimizing code errors. Our SBSFigures demonstrate a strong pre-training effect, making it possible to achieve efficient training with a limited amount of real-world chart data starting from our pre-trained weights.",
            "score": 2,
            "issue_id": 1385,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "71e72ba060f5d9bd",
            "authors": [
                "Risa Shinoda",
                "Kuniaki Saito",
                "Shohei Tanaka",
                "Tosho Hirasawa",
                "Yoshitaka Ushiku"
            ],
            "affiliations": [
                "Kyoto University, Japan",
                "OMRON SINIC Corp., Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17606.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#data",
                    "#synthetic"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SBSFigures - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞŸĞ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² ĞºĞ¾Ğ´Ğµ. SBSFigures Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Automating Figure QA with SBSFigures: Efficient, Diverse, and Error-Free!",
                    "desc": "This paper introduces SBSFigures, a new dataset designed for pre-training figure question answering (QA) systems. The dataset is generated through a novel stage-by-stage pipeline that automates the creation of chart figures with detailed annotations and dense QA pairs, eliminating the need for manual annotation. By addressing common issues in figure generation, such as code errors and repetitive content, SBSFigures allows for the efficient production of diverse figures. The results show that using this dataset for pre-training significantly enhances the performance of QA models, even when trained on a limited amount of real-world data."
                },
                "zh": {
                    "title": "é˜¶æ®µæ€§åˆæˆå›¾å½¢ï¼šé«˜æ•ˆçš„å›¾å½¢é—®ç­”é¢„è®­ç»ƒæ•°æ®é›†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSBSFiguresï¼ˆé˜¶æ®µæ€§åˆæˆå›¾å½¢ï¼‰çš„æ•°æ®é›†ï¼Œç”¨äºé¢„è®­ç»ƒå›¾å½¢é—®ç­”ï¼ˆQAï¼‰ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¸€ä¸ªé˜¶æ®µæ€§æµç¨‹ï¼Œè‡ªåŠ¨ç”Ÿæˆå¸¦æœ‰å®Œæ•´æ³¨é‡Šçš„å›¾è¡¨ï¼Œé¿å…äº†æ‰‹åŠ¨æ ‡æ³¨çš„ç¹çè¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSBSFiguresèƒ½å¤Ÿé«˜æ•ˆåœ°åˆ›å»ºå¤šæ ·åŒ–çš„ä¸»é¢˜å’Œå¤–è§‚å›¾å½¢ï¼ŒåŒæ—¶å‡å°‘ä»£ç é”™è¯¯ã€‚é€šè¿‡ä½¿ç”¨æˆ‘ä»¬çš„é¢„è®­ç»ƒæƒé‡ï¼ŒSBSFigureså±•ç¤ºäº†å¼ºå¤§çš„é¢„è®­ç»ƒæ•ˆæœï¼Œä½¿å¾—åœ¨æœ‰é™çš„çœŸå®å›¾è¡¨æ•°æ®ä¸Šå®ç°é«˜æ•ˆè®­ç»ƒæˆä¸ºå¯èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18702",
            "title": "CypherBench: Towards Precise Retrieval over Full-scale Modern Knowledge Graphs in the LLM Era",
            "url": "https://huggingface.co/papers/2412.18702",
            "abstract": "Retrieval from graph data is crucial for augmenting large language models (LLM) with both open-domain knowledge and private enterprise data, and it is also a key component in the recent GraphRAG system (edge et al., 2024). Despite decades of research on knowledge graphs and knowledge base question answering, leading LLM frameworks (e.g. Langchain and LlamaIndex) have only minimal support for retrieval from modern encyclopedic knowledge graphs like Wikidata. In this paper, we analyze the root cause and suggest that modern RDF knowledge graphs (e.g. Wikidata, Freebase) are less efficient for LLMs due to overly large schemas that far exceed the typical LLM context window, use of resource identifiers, overlapping relation types and lack of normalization. As a solution, we propose property graph views on top of the underlying RDF graph that can be efficiently queried by LLMs using Cypher. We instantiated this idea on Wikidata and introduced CypherBench, the first benchmark with 11 large-scale, multi-domain property graphs with 7.8 million entities and over 10,000 questions. To achieve this, we tackled several key challenges, including developing an RDF-to-property graph conversion engine, creating a systematic pipeline for text-to-Cypher task generation, and designing new evaluation metrics.",
            "score": 1,
            "issue_id": 1398,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "d404a0a9acade186",
            "authors": [
                "Yanlin Feng",
                "Simone Papicchio",
                "Sajjadur Rahman"
            ],
            "affiliations": [
                "Megagon Labs",
                "Politecnico di Torino"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18702.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#graphs",
                    "#rag"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ñ„Ñ‹ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼ Ğ´Ğ»Ñ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ğ· Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… RDF-Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ´Ğ»Ñ LLM Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¿Ğ¾Ğ²ĞµÑ€Ñ… RDF. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ CypherBench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Wikidata. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€ĞµÑˆĞµĞ½Ñ‹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚ĞµÑ€Ğ° RDF Ğ² Ğ³Ñ€Ğ°Ñ„ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ text-to-Cypher."
                },
                "en": {
                    "title": "Optimizing Knowledge Retrieval for LLMs with Property Graphs",
                    "desc": "This paper addresses the challenges of retrieving information from knowledge graphs to enhance large language models (LLMs). It identifies inefficiencies in modern RDF knowledge graphs, such as complex schemas and overlapping relation types, which hinder effective querying by LLMs. The authors propose using property graph views to simplify and optimize the querying process, allowing LLMs to access knowledge more efficiently. They introduce CypherBench, a benchmark for evaluating this approach with large-scale property graphs and a systematic pipeline for generating queries."
                },
                "zh": {
                    "title": "æå‡LLMæ£€ç´¢æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä»å›¾æ•°æ®ä¸­æ£€ç´¢ä¿¡æ¯ï¼Œä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†è·å–èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°ä»£RDFçŸ¥è¯†å›¾è°±ï¼ˆå¦‚Wikidataï¼‰ç”±äºå…¶åºå¤§çš„æ¨¡å¼å’Œå¤æ‚çš„å…³ç³»ç±»å‹ï¼Œå¯¼è‡´LLMçš„æ£€ç´¢æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨RDFå›¾ä¸Šæ„å»ºå±æ€§å›¾è§†å›¾çš„æ–¹æ³•ï¼Œä½¿å¾—LLMèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ä½¿ç”¨CypheræŸ¥è¯¢ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†CypherBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«å¤šä¸ªé¢†åŸŸçš„å¤§è§„æ¨¡å±æ€§å›¾åŸºå‡†ï¼Œæ—¨åœ¨æ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-27.html",
    "link_next": "2024-12-31.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "27.12",
        "en": "12/27",
        "zh": "12æœˆ27æ—¥"
    },
    "short_date_next": {
        "ru": "31.12",
        "en": "12/31",
        "zh": "12æœˆ31æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 5,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†OpenAI o1çš„çªç ´ï¼Œå¼ºè°ƒäº†å¢å¼ºæ¨ç†ä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨ç†ç ”ç©¶é›†ä¸­åœ¨æ•°å­¦ä»»åŠ¡ä¸Šï¼ŒåŒ»å­¦é¢†åŸŸä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åŒ»å­¦é¢†åŸŸéœ€è¦å¼ºå¤§çš„æ¨ç†èƒ½åŠ›æ¥æä¾›å¯é çš„ç­”æ¡ˆï¼Œä½†éªŒè¯åŒ»å­¦æ¨ç†æ¯”æ•°å­¦æ¨ç†æ›´å…·æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å¯éªŒè¯çš„åŒ»å­¦é—®é¢˜å’Œä¸€ä¸ªåŒ»å­¦éªŒè¯å™¨æ¥æ£€æŸ¥æ¨¡å‹è¾“å‡ºçš„æ­£ç¡®æ€§ã€‚è¿™ç§å¯éªŒè¯æ€§ä½¿å¾—é€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•è¿›ä¸€æ­¥æ¨è¿›åŒ»å­¦æ¨ç†æˆä¸ºå¯èƒ½ã€‚",
        "title": "HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le OpenAI o1 de tÅ«pÃ², qiÃ¡ngdiÃ o le zÄ“ngqiÃ¡ng tuÃ­lÇ yÇ tÃ­gÄo dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (LLM) de qiÃ¡nlÃ¬. RÃ¡n'Ã©r, dÃ duÅshÃ¹ tuÃ­lÇ yÃ¡njiÅ« jÃ­zhÅng zÃ i shÃ¹xuÃ© rÃ¨nwÃ¹ shÃ ng, yÄ«xuÃ© lÇngyÃ¹ rÃ©ng wÃ¨i dÃ©dÃ o chÅngfÃ¨n tÃ nsuÇ’. YÄ«xuÃ© lÇngyÃ¹ xÅ«yÃ o qiÃ¡ngdÃ  de tuÃ­lÇ nÃ©nglÃ¬ lÃ¡i tÃ­gÅng kÄ›kÃ o de dÃ¡'Ã n, dÃ n yÃ nzhÃ¨ng yÄ«xuÃ© tuÃ­lÇ bÇ shÃ¹xuÃ© tuÃ­lÇ gÃ¨ng jÃ¹ tiÇozhÃ nxÃ¬ng. WÃ¨i cÇ, zuÃ²zhÄ› tÃ­chÅ« le kÄ› yÃ nzhÃ¨ng de yÄ«xuÃ© wÃ¨ntÃ­ hÃ© yÄ«gÃ¨ yÄ«xuÃ© yÃ nzhÃ¨ngqÃ¬ lÃ¡i jiÇnchÃ¡ mÃ³xÃ­ng shÅ«chÅ« de zhÃ¨ngquÃ¨xÃ¬ng. ZhÃ¨ zhÇ’ng kÄ› yÃ nzhÃ¨ngxÃ¬ng shÇ dÃ© tÅngguÃ² liÇng jiÄ“duÃ n fÄngfÇ jÃ¬nxÃ­ng yÄ«xuÃ© tuÃ­lÇ chÃ©ngwÃ©i kÄ›nÃ©ng.",
        "vocab": "[{'word': 'çªç ´', 'pinyin': 'tÅ«pÃ²', 'trans': 'breakthrough'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ«lÇ', 'trans': 'reasoning'}, {'word': 'æ½œåŠ›', 'pinyin': 'qiÃ¡nlÃ¬', 'trans': 'potential'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ”yÃ¡n mÃ³xÃ­ng', 'trans': 'large language model'}, {'word': 'é›†ä¸­', 'pinyin': 'jÃ­zhÅng', 'trans': 'focus'}, {'word': 'æ•°å­¦', 'pinyin': 'shÃ¹xuÃ©', 'trans': 'mathematics'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'}, {'word': 'åŒ»å­¦', 'pinyin': 'yÄ«xuÃ©', 'trans': 'medicine'}, {'word': 'é¢†åŸŸ', 'pinyin': 'lÇngyÃ¹', 'trans': 'field'}, {'word': 'æ¢ç´¢', 'pinyin': 'tÃ nsuÇ’', 'trans': 'explore'}, {'word': 'å¯é ', 'pinyin': 'kÄ›kÃ o', 'trans': 'reliable'}, {'word': 'ç­”æ¡ˆ', 'pinyin': 'dÃ¡'Än', 'trans': 'answer'}, {'word': 'éªŒè¯', 'pinyin': 'yÃ nzhÃ¨ng', 'trans': 'verification'}, {'word': 'æŒ‘æˆ˜æ€§', 'pinyin': 'tiÇozhÃ nxÃ¬ng', 'trans': 'challenging'}, {'word': 'æå‡º', 'pinyin': 'tÃ­chÅ«', 'trans': 'propose'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄngfÇ', 'trans': 'method'}, {'word': 'æ¨è¿›', 'pinyin': 'tuÄ«jÃ¬n', 'trans': 'advance'}]",
        "trans": "This article introduces the breakthroughs of OpenAI o1, emphasizing the enhancement of reasoning to increase the potential of large language models (LLMs). However, most reasoning research focuses on mathematical tasks, and the medical field remains relatively unexplored. The medical field requires strong reasoning capabilities to provide reliable answers, but verifying medical reasoning is more challenging than verifying mathematical reasoning. To address this, the authors propose verifiable medical problems and a medical verifier to check the correctness of model outputs. This verifiability makes it possible to further advance medical reasoning through a two-stage method.",
        "update_ts": "2024-12-30 09:11"
    }
}