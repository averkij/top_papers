
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 16 papers. June 9.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">9 июня</span> | <span id="title-articles-count">16 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-06.html">⬅️ <span id="prev-date">06.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-10.html">➡️ <span id="next-date">10.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'};
        let feedDateNext = {'ru': '10.06', 'en': '06/10', 'zh': '6月10日'};
        let feedDatePrev = {'ru': '06.06', 'en': '06/06', 'zh': '6月6日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.21115', 'title': 'Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA', 'url': 'https://huggingface.co/papers/2505.21115', 'abstract': 'EverGreenQA, a multilingual QA dataset with evergreen labels, is introduced to benchmark LLMs on temporality encoding and assess their performance through verbalized judgments and uncertainty signals.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) often hallucinate in question answering (QA) tasks. A key yet underexplored factor contributing to this is the temporality of questions -- whether they are evergreen (answers remain stable over time) or mutable (answers change). In this work, we introduce EverGreenQA, the first multilingual QA dataset with evergreen labels, supporting both evaluation and training. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they encode question temporality explicitly (via verbalized judgments) or implicitly (via uncertainty signals). We also train EG-E5, a lightweight multilingual classifier that achieves SoTA performance on this task. Finally, we demonstrate the practical utility of evergreen classification across three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4o retrieval behavior.', 'score': 71, 'issue_id': 4192, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'cbbff6b511a277fa', 'authors': ['Sergey Pletenev', 'Maria Marina', 'Nikolay Ivanov', 'Daria Galimzianova', 'Nikita Krayko', 'Mikhail Salnikov', 'Vasily Konovalov', 'Alexander Panchenko', 'Viktor Moskvoretskii'], 'affiliations': ['AIRI', 'HSE University', 'MIPT', 'MTS AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.21115.jpg', 'data': {'categories': ['#benchmark', '#training', '#low_resource', '#dataset', '#multilingual', '#long_context', '#hallucinations'], 'emoji': '🌳', 'ru': {'title': 'EverGreenQA: новый подход к оценке темпоральности в вопросно-ответных системах', 'desc': "EverGreenQA - это новый многоязычный набор данных для задач вопросно-ответных систем, который включает метки 'вечнозеленых' вопросов. Он используется для оценки способности больших языковых моделей (LLM) кодировать временные аспекты информации. Исследователи протестировали 12 современных LLM на этом наборе данных, оценивая их явное и неявное понимание темпоральности вопросов. Также был разработан классификатор EG-E5, достигающий наилучших результатов в определении 'вечнозеленых' вопросов."}, 'en': {'title': 'Understanding Question Timeliness with EverGreenQA', 'desc': 'The paper introduces EverGreenQA, a new multilingual question answering (QA) dataset designed to evaluate how well large language models (LLMs) understand the concept of temporality in questions. It distinguishes between evergreen questions, which have stable answers, and mutable questions, which can change over time. The authors benchmark 12 LLMs using this dataset to see if they can explicitly or implicitly recognize question temporality through verbalized judgments and uncertainty signals. Additionally, they present EG-E5, a lightweight multilingual classifier that achieves state-of-the-art performance and demonstrate its usefulness in various applications, such as enhancing self-knowledge estimation and filtering QA datasets.'}, 'zh': {'title': '揭示问答中的时间性：EverGreenQA数据集', 'desc': 'EverGreenQA是一个多语言问答数据集，专注于时间性编码，特别是问题的持久性。该数据集通过永恒标签来评估大型语言模型（LLMs）在问答任务中的表现。研究表明，问题的时间性（如永恒性或可变性）对LLMs的回答准确性有重要影响。我们还开发了EG-E5，一个轻量级的多语言分类器，在这一任务上达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.01111', 'title': 'FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion', 'url': 'https://huggingface.co/papers/2506.01111', 'abstract': 'A novel two-stage pipeline using specialized pretrained models and a large language model enhances audio caption quality by integrating diverse multimodal cues and contextual information.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality, large-scale audio captioning is crucial for advancing audio understanding, yet current automated methods often generate captions that lack fine-grained detail and contextual accuracy, primarily due to their reliance on limited unimodal or superficial multimodal information. Drawing inspiration from human auditory perception, which adeptly integrates cross-modal cues and performs sophisticated auditory scene analysis, we introduce a novel two-stage automated pipeline. This pipeline first employs specialized pretrained models to extract diverse contextual cues (e.g., speech, music, general sounds, and visual information from associated video). A large language model (LLM) then synthesizes these rich, multimodal inputs to generate detailed and context-aware audio captions. Key contributions of this work include: (1) the proposed scalable method for fine-grained audio caption generation; (2) FusionAudio, a new large-scale dataset comprising 1.2 million such detailed captions, combined with 6 million QA pairs; and (3) enhanced audio models developed using FusionAudio, specifically a CLAP-based audio encoder with superior audio-text alignment and instruction following. This paper paves the way for more nuanced and accurate automated understanding of complex audio environments. Code and data can be found in https://github.com/satsuki2486441738/FusionAudio.', 'score': 23, 'issue_id': 4186, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'a649684de588a812', 'authors': ['Shunian Chen', 'Xinyuan Xie', 'Zheshu Chen', 'Liyan Zhao', 'Owen Lee', 'Zhan Su', 'Qilin Sun', 'Benyou Wang'], 'affiliations': ['South China University of Technology', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.01111.jpg', 'data': {'categories': ['#dataset', '#open_source', '#audio', '#multimodal', '#optimization', '#data', '#games'], 'emoji': '🎧', 'ru': {'title': 'Революция в аудио-подписях: мультимодальный ИИ для точного описания звуков', 'desc': 'Статья представляет новый двухэтапный конвейер для улучшения качества аудио-подписей, используя специализированные предобученные модели и большую языковую модель (LLM). Этот метод интегрирует разнообразные мультимодальные сигналы и контекстную информацию для создания более детальных и точных описаний аудио. Авторы также представляют FusionAudio - новый крупномасштабный датасет, содержащий 1,2 миллиона подробных аудио-подписей и 6 миллионов пар вопросов-ответов. Исследование демонстрирует улучшенные аудио-модели, разработанные с использованием FusionAudio, включая аудио-энкодер на основе CLAP с улучшенным выравниванием аудио и текста.'}, 'en': {'title': 'Enhancing Audio Captions with Multimodal Insights', 'desc': 'This paper presents a two-stage pipeline that improves the quality of audio captions by using specialized pretrained models alongside a large language model (LLM). The first stage extracts various contextual cues from audio, such as speech and music, as well as visual information from related videos. In the second stage, the LLM synthesizes these multimodal inputs to create detailed and contextually accurate captions. The work introduces a new dataset, FusionAudio, which contains 1.2 million detailed captions and enhances audio models for better audio-text alignment.'}, 'zh': {'title': '提升音频字幕质量的创新方法', 'desc': '本文提出了一种新颖的两阶段管道，利用专门的预训练模型和大型语言模型来提高音频字幕的质量。该方法通过提取多样的上下文线索，如语音、音乐和视觉信息，来增强音频理解。然后，使用大型语言模型综合这些多模态输入，生成详细且具有上下文意识的音频字幕。此研究的关键贡献包括可扩展的细粒度音频字幕生成方法和一个新的大规模数据集FusionAudio，包含120万条详细字幕和600万对问答。'}}}, {'id': 'https://huggingface.co/papers/2506.01872', 'title': 'Is Extending Modality The Right Path Towards Omni-Modality?', 'url': 'https://huggingface.co/papers/2506.01872', 'abstract': 'Research investigates the impact of extending modality and model merging on maintaining language abilities and generalization in omni-modal language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Omni-modal language models (OLMs) aim to integrate and reason over diverse input modalities--such as text, images, video, and audio--while maintaining strong language capabilities. Despite recent advancements, existing models, especially open-source ones, remain far from true omni-modality, struggling to generalize beyond the specific modality pairs they are trained on or to achieve strong performance when processing multi-modal inputs. We study the effect of extending modality, the dominant technique for training multimodal models, where an off-the-shelf language model is fine-tuned on target-domain and language data. Specifically, we investigate three key questions: (1) Does modality extension compromise core language abilities? (2) Can model merging effectively integrate independently fine-tuned modality-specific models to achieve omni-modality? (3) Does omni-modality extension lead to better knowledge sharing and generalization compared to sequential extension? Through extensive experiments, we analyze these trade-offs and provide insights into the feasibility of achieving true omni-modality using current approaches.', 'score': 14, 'issue_id': 4193, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'f876844db3f1bfbd', 'authors': ['Tinghui Zhu', 'Kai Zhang', 'Muhao Chen', 'Yu Su'], 'affiliations': ['The Ohio State University', 'University of California, Davis'], 'pdf_title_img': 'assets/pdf/title_img/2506.01872.jpg', 'data': {'categories': ['#training', '#agi', '#multimodal', '#open_source', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'Путь к истинной омни-модальности: баланс между расширением и обобщением', 'desc': 'Исследование посвящено изучению влияния расширения модальности и слияния моделей на сохранение языковых способностей и обобщение в омни-модальных языковых моделях. Авторы анализируют, как расширение модальности влияет на основные языковые навыки модели. Они также исследуют эффективность слияния моделей для достижения омни-модальности и сравнивают омни-модальное расширение с последовательным. Эксперименты предоставляют важные выводы о возможности достижения истинной омни-модальности с использованием современных подходов.'}, 'en': {'title': 'Unlocking True Omni-Modality in Language Models', 'desc': 'This research explores how extending the types of data (modalities) and merging different models can help omni-modal language models (OLMs) maintain their language skills and improve their ability to generalize across various inputs. OLMs are designed to work with multiple forms of data, like text and images, but often struggle to perform well when faced with new combinations of these inputs. The study examines whether adding new modalities affects the language capabilities of these models, if merging models trained on different modalities can create a more effective omni-modal model, and whether this approach enhances knowledge sharing and generalization. Through experiments, the paper provides valuable insights into the challenges and potential solutions for achieving true omni-modality in language models.'}, 'zh': {'title': '实现真正的全模态能力', 'desc': '本研究探讨了扩展模态和模型合并对全模态语言模型在保持语言能力和泛化能力方面的影响。全模态语言模型旨在整合和推理多种输入模态，如文本、图像、视频和音频。尽管已有进展，现有模型在真正的全模态能力上仍然存在不足，尤其是在处理多模态输入时的泛化能力较弱。我们通过实验分析了扩展模态对核心语言能力的影响，以及模型合并是否能有效整合独立微调的模态特定模型，以实现全模态能力。'}}}, {'id': 'https://huggingface.co/papers/2506.05629', 'title': 'Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs', 'url': 'https://huggingface.co/papers/2506.05629', 'abstract': 'A new method using input-dependent soft prompting with a self-attention mechanism improves parameter-efficient fine-tuning for large language models, enhancing zero-shot domain transfer.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of large language models in domain-specific tasks necessitates fine-tuning, which is computationally expensive and technically challenging. This paper focuses on parameter-efficient fine-tuning using soft prompting, a promising approach that adapts pre-trained models to downstream tasks by learning a small set of parameters. We propose a novel Input Dependent Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) that generates soft prompts based on the input tokens and attends different tokens with varying importance. Our method is simple and efficient, keeping the number of trainable parameters small. We show the merits of the proposed approach compared to state-of-the-art techniques on various tasks and show the improved zero shot domain transfer capability.', 'score': 13, 'issue_id': 4186, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c88ec16aee1c43d5', 'authors': ['Ananth Muppidi', 'Abhilash Nandy', 'Sambaran Bandyopadhyay'], 'affiliations': ['Adobe Research, India', 'IIIT Hyderabad, India', 'IIT Kharagpur, India'], 'pdf_title_img': 'assets/pdf/title_img/2506.05629.jpg', 'data': {'categories': ['#training', '#optimization', '#transfer_learning', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Эффективная донастройка языковых моделей с помощью динамических промптов', 'desc': 'Статья представляет новый метод параметрически-эффективной донастройки больших языковых моделей. Предложенная техника Input Dependent Soft Prompting with self-Attention Mechanism (ID-SPAM) генерирует мягкие промпты на основе входных токенов. Метод использует механизм самовнимания для присвоения различной важности разным токенам. ID-SPAM показывает улучшенные результаты по сравнению с современными подходами на различных задачах, особенно в zero-shot переносе на новые домены.'}, 'en': {'title': 'Efficient Fine-Tuning with Input-Dependent Soft Prompts', 'desc': 'This paper introduces a new technique called Input Dependent Soft Prompting with a Self-Attention Mechanism (ID-SPAM) to enhance fine-tuning of large language models. It focuses on making the fine-tuning process more efficient by using a small set of parameters that adapt the model to specific tasks. The self-attention mechanism allows the model to weigh the importance of different input tokens when generating soft prompts. The results demonstrate that ID-SPAM outperforms existing methods, particularly in zero-shot domain transfer scenarios.'}, 'zh': {'title': '输入依赖的软提示，提升微调效率', 'desc': '本文提出了一种新的方法，利用输入依赖的软提示和自注意力机制，来提高大语言模型的参数高效微调能力。这种方法通过学习一小组参数，适应预训练模型到下游任务，减少了计算成本。我们的方法生成基于输入标记的软提示，并对不同的重要性标记进行关注，从而实现了高效的微调。实验结果表明，该方法在多个任务上优于现有技术，并提升了零-shot领域迁移能力。'}}}, {'id': 'https://huggingface.co/papers/2506.05984', 'title': 'Audio-Aware Large Language Models as Judges for Speaking Styles', 'url': 'https://huggingface.co/papers/2506.05984', 'abstract': "Audio-aware large language models can assess speaking styles in audio inputs, demonstrating performance comparable to human judges in evaluating synthesized speech along dimensions like emotion, volume, and pitch.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-aware large language models (ALLMs) can understand the textual and non-textual information in the audio input. In this paper, we explore using ALLMs as an automatic judge to assess the speaking styles of speeches. We use ALLM judges to evaluate the speeches generated by SLMs on two tasks: voice style instruction following and role-playing. The speaking style we consider includes emotion, volume, speaking pace, word emphasis, pitch control, and non-verbal elements. We use four spoken language models (SLMs) to complete the two tasks and use humans and ALLMs to judge the SLMs' responses. We compare two ALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and show that the agreement between Gemini and human judges is comparable to the agreement between human evaluators. These promising results show that ALLMs can be used as a judge to evaluate SLMs. Our results also reveal that current SLMs, even GPT-4o-audio, still have room for improvement in controlling the speaking style and generating natural dialogues.", 'score': 10, 'issue_id': 4185, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '10dcc4567ff634c1', 'authors': ['Cheng-Han Chiang', 'Xiaofei Wang', 'Chung-Ching Lin', 'Kevin Lin', 'Linjie Li', 'Radu Kopetz', 'Yao Qian', 'Zhendong Wang', 'Zhengyuan Yang', 'Hung-yi Lee', 'Lijuan Wang'], 'affiliations': ['Microsoft', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05984.jpg', 'data': {'categories': ['#multimodal', '#audio', '#interpretability', '#games'], 'emoji': '🎙️', 'ru': {'title': 'АОБЛМ как объективные судьи стиля речи', 'desc': 'Аудио-осведомленные большие языковые модели (АОБЛМ) способны оценивать стили речи в аудиовходах, демонстрируя производительность, сравнимую с оценками людей-судей. В исследовании АОБЛМ использовались для оценки речей, сгенерированных разговорными языковыми моделями (РЯМ) в задачах следования инструкциям по стилю голоса и ролевой игры. Оценивались такие аспекты, как эмоции, громкость, темп речи, выделение слов, контроль высоты тона и невербальные элементы. Результаты показали, что согласованность между оценками Gemini и человеческих судей сопоставима с согласованностью между оценками разных людей.'}, 'en': {'title': 'Evaluating Speech Styles with AI: ALLMs vs. Human Judges', 'desc': 'This paper discusses the capabilities of audio-aware large language models (ALLMs) in evaluating speaking styles from audio inputs. The authors demonstrate that ALLMs can assess synthesized speech similarly to human judges, focusing on aspects like emotion, volume, and pitch. They compare the performance of two ALLMs, GPT-4o-audio and Gemini-2.5-pro, against human evaluations in tasks involving voice style instruction and role-playing. The findings indicate that while ALLMs can effectively judge speaking styles, there is still potential for improvement in the speaking style control of current spoken language models (SLMs).'}, 'zh': {'title': '音频感知模型：评估说话风格的新工具', 'desc': '音频感知的大型语言模型（ALLMs）能够评估音频输入中的说话风格，其表现与人类评审在情感、音量和音调等维度上的评估相当。本文探讨了使用ALLMs作为自动评审者来评估演讲的说话风格。我们使用四个口语语言模型（SLMs）完成两个任务，并通过人类和ALLMs对SLMs的响应进行评估。研究结果表明，ALLMs可以作为评审工具来评估SLMs，但当前的SLMs在控制说话风格和生成自然对话方面仍有改进空间。'}}}, {'id': 'https://huggingface.co/papers/2506.05523', 'title': 'MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2506.05523', 'abstract': 'MORSE-500, a video benchmark with 500 scripted clips, evaluates multimodal reasoning across six categories, highlighting performance gaps in abstract and planning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in vision-language models (VLMs), current benchmarks for multimodal reasoning fall short in three key dimensions. First, they overwhelmingly rely on static images, failing to capture the temporal complexity of real-world environments. Second, they narrowly focus on mathematical problem-solving, neglecting the broader spectrum of reasoning skills -- including abstract, physical, planning, spatial, and temporal capabilities -- required for robust multimodal intelligence. Third, many benchmarks quickly saturate, offering limited headroom for diagnosing failure modes or measuring continued progress. We introduce MORSE-500 (Multimodal Reasoning Stress-test Environment), a video benchmark composed of 500 fully scripted clips with embedded questions spanning six complementary reasoning categories. Each instance is programmatically generated using deterministic Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and curated real footage. This script-driven design allows fine-grained control over visual complexity, distractor density, and temporal dynamics -- enabling difficulty to be scaled systematically as models improve. Unlike static benchmarks that become obsolete once saturated, MORSE-500 is built to evolve: its controllable generation pipeline supports the creation of arbitrarily challenging new instances, making it ideally suited for stress-testing next-generation models. Initial experiments with state-of-the-art systems -- including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest available at the time, alongside strong open-source models -- reveal substantial performance gaps across all categories, with particularly large deficits in abstract and planning tasks. We release the full dataset, generation scripts, and evaluation harness to support transparent, reproducible, and forward-looking multimodal reasoning research.', 'score': 10, 'issue_id': 4188, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '524394ef06ba3ba1', 'authors': ['Zikui Cai', 'Andrew Wang', 'Anirudh Satheesh', 'Ankit Nakhawa', 'Hyunwoo Jae', 'Keenan Powell', 'Minghui Liu', 'Neel Jay', 'Sungbin Oh', 'Xiyao Wang', 'Yongyuan Liang', 'Tom Goldstein', 'Furong Huang'], 'affiliations': ['Capital One', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.05523.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#survey', '#video', '#open_source', '#dataset', '#reasoning'], 'emoji': '🎬', 'ru': {'title': 'MORSE-500: Новый рубеж в оценке мультимодального ИИ', 'desc': 'MORSE-500 - это новый видеобенчмарк для оценки мультимодального рассуждения моделей искусственного интеллекта. Он включает 500 специально созданных видеоклипов с вопросами по шести категориям рассуждений, включая абстрактное мышление и планирование. Бенчмарк позволяет систематически увеличивать сложность тестов по мере улучшения моделей. Эксперименты показали значительные пробелы в производительности современных систем, особенно в абстрактных задачах и планировании.'}, 'en': {'title': 'MORSE-500: Evolving Benchmark for Multimodal Reasoning', 'desc': 'MORSE-500 is a new video benchmark designed to evaluate multimodal reasoning in AI across six different categories. It addresses limitations in existing benchmarks by incorporating dynamic video clips instead of static images, allowing for a more realistic assessment of reasoning skills. The benchmark includes a variety of reasoning tasks, such as abstract thinking and planning, which are essential for advanced multimodal intelligence. By providing a scalable and evolving dataset, MORSE-500 aims to facilitate ongoing research and development in multimodal reasoning capabilities.'}, 'zh': {'title': 'MORSE-500：多模态推理的新基准', 'desc': 'MORSE-500是一个包含500个脚本化视频片段的基准测试，旨在评估多模态推理能力。该基准涵盖六个互补的推理类别，强调了在抽象和规划任务中的性能差距。与静态图像基准不同，MORSE-500能够捕捉现实环境的时间复杂性，并支持生成具有不同难度的新实例。通过提供完整的数据集和生成脚本，MORSE-500为多模态推理研究提供了透明和可重复的支持。'}}}, {'id': 'https://huggingface.co/papers/2506.06276', 'title': 'STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis', 'url': 'https://huggingface.co/papers/2506.06276', 'abstract': 'STARFlow, a generative model combining normalizing flows with autoregressive Transformers, achieves competitive image synthesis performance with innovations in architecture and latent space modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution.', 'score': 9, 'issue_id': 4189, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '14f98c6826d7c6bb', 'authors': ['Jiatao Gu', 'Tianrong Chen', 'David Berthelot', 'Huangjie Zheng', 'Yuyang Wang', 'Ruixiang Zhang', 'Laurent Dinh', 'Miguel Angel Bautista', 'Josh Susskind', 'Shuangfei Zhai'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2506.06276.jpg', 'data': {'categories': ['#architecture', '#cv', '#diffusion'], 'emoji': '🌊', 'ru': {'title': 'Нормализующие потоки покоряют высоты генерации изображений', 'desc': 'STARFlow - это генеративная модель, объединяющая нормализующие потоки и авторегрессивные трансформеры для синтеза изображений высокого разрешения. Модель использует глубоко-мелкую архитектуру и моделирование в латентном пространстве предобученных автоэнкодеров. Ключевые инновации включают новый алгоритм управления для повышения качества сэмплов и обучение методом максимального правдоподобия без дискретизации. STARFlow достигает конкурентоспособных результатов в задачах генерации изображений по классу и тексту, приближаясь к современным диффузионным моделям.'}, 'en': {'title': 'STARFlow: Merging Flows and Transformers for High-Quality Image Generation', 'desc': 'STARFlow is a generative model that merges normalizing flows with autoregressive Transformers to create high-quality images. It introduces the Transformer Autoregressive Flow (TARFlow), which effectively models continuous distributions while maintaining scalability. Key innovations include a deep-shallow architecture for efficient computation, latent space modeling using pretrained autoencoders, and a novel guidance algorithm to enhance sample quality. This model achieves competitive results in both class-conditional and text-conditional image generation, marking a significant advancement in the use of normalizing flows for high-resolution image synthesis.'}, 'zh': {'title': 'STARFlow：高效图像合成的新纪元', 'desc': 'STARFlow是一种结合了归一化流和自回归变换器的生成模型，能够在高分辨率图像合成中实现强大的性能。其核心是变换器自回归流（TARFlow），将归一化流的表达能力与自回归变换器的结构建模能力相结合。通过深浅设计、在预训练自编码器的潜在空间建模以及新颖的引导算法，STARFlow显著提高了可扩展性和样本质量。该模型能够在连续空间中进行精确的最大似然训练，且在类条件和文本条件的图像生成任务中表现出色，接近最先进的扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2506.05573', 'title': 'PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.05573', 'abstract': 'PartCrafter generates complex 3D scenes from single images using a unified compositional architecture with a diffusion transformer, enabling part-aware generation and hierarchical attention.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PartCrafter, the first structured 3D generative model that jointly synthesizes multiple semantically meaningful and geometrically distinct 3D meshes from a single RGB image. Unlike existing methods that either produce monolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an image and then reconstructing each segment, PartCrafter adopts a unified, compositional generation architecture that does not rely on pre-segmented inputs. Conditioned on a single image, it simultaneously denoises multiple 3D parts, enabling end-to-end part-aware generation of both individual objects and complex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh diffusion transformer (DiT) trained on whole objects, inheriting the pretrained weights, encoder, and decoder, and introduces two key innovations: (1) A compositional latent space, where each 3D part is represented by a set of disentangled latent tokens; (2) A hierarchical attention mechanism that enables structured information flow both within individual parts and across all parts, ensuring global coherence while preserving part-level detail during generation. To support part-level supervision, we curate a new dataset by mining part-level annotations from large-scale 3D object datasets. Experiments show that PartCrafter outperforms existing approaches in generating decomposable 3D meshes, including parts that are not directly visible in input images, demonstrating the strength of part-aware generative priors for 3D understanding and synthesis. Code and training data will be released.', 'score': 9, 'issue_id': 4191, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'af089266a7086a2d', 'authors': ['Yuchen Lin', 'Chenguo Lin', 'Panwang Pan', 'Honglei Yan', 'Yiqiang Feng', 'Yadong Mu', 'Katerina Fragkiadaki'], 'affiliations': ['ByteDance', 'Carnegie Mellon University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05573.jpg', 'data': {'categories': ['#3d', '#diffusion', '#open_source', '#architecture', '#dataset'], 'emoji': '🧩', 'ru': {'title': 'Создание сложных 3D-сцен из одного изображения', 'desc': 'PartCrafter - это новая модель для генерации структурированных 3D-сцен из одиночных изображений. Она использует композиционную архитектуру на основе диффузионного трансформера для одновременной генерации нескольких семантически значимых 3D-объектов. Модель применяет иерархический механизм внимания для обеспечения глобальной согласованности при сохранении детализации на уровне отдельных частей. PartCrafter превосходит существующие подходы в генерации декомпозируемых 3D-моделей, включая части, не видимые напрямую на входном изображении.'}, 'en': {'title': 'Transforming Images into 3D Worlds with PartCrafter!', 'desc': 'PartCrafter is a novel 3D generative model that creates detailed 3D scenes from a single RGB image without needing pre-segmented inputs. It uses a unified architecture that combines part-aware generation with hierarchical attention, allowing it to generate multiple distinct 3D meshes simultaneously. The model leverages a pretrained diffusion transformer to enhance the quality of the generated parts and maintains coherence across the entire scene. By introducing a compositional latent space and a new dataset for part-level supervision, PartCrafter significantly improves the generation of complex 3D structures, even including parts not visible in the original image.'}, 'zh': {'title': 'PartCrafter：从单图像生成复杂3D场景的创新模型', 'desc': 'PartCrafter是一种新型的3D生成模型，可以从单张RGB图像生成多个语义明确且几何上不同的3D网格。与传统方法不同，PartCrafter采用统一的生成架构，无需预先分割输入图像，能够同时去噪多个3D部分。该模型利用预训练的3D网格扩散变换器，并引入了组合潜在空间和层次注意机制，以确保生成的3D场景在全局一致性的同时保留部分细节。实验结果表明，PartCrafter在生成可分解的3D网格方面优于现有方法，展示了其在3D理解和合成中的优势。'}}}, {'id': 'https://huggingface.co/papers/2506.06253', 'title': 'Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\n  with Egocentric-Exocentric Vision', 'url': 'https://huggingface.co/papers/2506.06253', 'abstract': 'A survey reviews advancements in video understanding from both egocentric and exocentric perspectives, highlighting applications, tasks, joint learning frameworks, and limitations, with the aim of enhancing machine perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Perceiving the world from both egocentric (first-person) and exocentric (third-person) perspectives is fundamental to human cognition, enabling rich and complementary understanding of dynamic environments. In recent years, allowing the machines to leverage the synergistic potential of these dual perspectives has emerged as a compelling research direction in video understanding. In this survey, we provide a comprehensive review of video understanding from both exocentric and egocentric viewpoints. We begin by highlighting the practical applications of integrating egocentric and exocentric techniques, envisioning their potential collaboration across domains. We then identify key research tasks to realize these applications. Next, we systematically organize and review recent advancements into three main research directions: (1) leveraging egocentric data to enhance exocentric understanding, (2) utilizing exocentric data to improve egocentric analysis, and (3) joint learning frameworks that unify both perspectives. For each direction, we analyze a diverse set of tasks and relevant works. Additionally, we discuss benchmark datasets that support research in both perspectives, evaluating their scope, diversity, and applicability. Finally, we discuss limitations in current works and propose promising future research directions. By synthesizing insights from both perspectives, our goal is to inspire advancements in video understanding and artificial intelligence, bringing machines closer to perceiving the world in a human-like manner. A GitHub repo of related works can be found at https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.', 'score': 5, 'issue_id': 4192, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '7ddc25331945068e', 'authors': ['Yuping He', 'Yifei Huang', 'Guo Chen', 'Lidong Lu', 'Baoqi Pei', 'Jilan Xu', 'Tong Lu', 'Yoichi Sato'], 'affiliations': ['Fudan University, Shanghai 200433, China', 'State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China', 'University of Tokyo, Tokyo, Japan', 'Zhejiang University, Zhejiang 310027, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.06253.jpg', 'data': {'categories': ['#multimodal', '#video', '#survey', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Синергия эгоцентрического и экзоцентрического зрения для прорыва в понимании видео машинами', 'desc': 'Статья представляет обзор достижений в области понимания видео с эгоцентрической и экзоцентрической точек зрения. Авторы рассматривают практические приложения, ключевые исследовательские задачи и совместные обучающие фреймворки для интеграции обоих подходов. В работе анализируются три основных направления исследований: улучшение экзоцентрического понимания с помощью эгоцентрических данных, улучшение эгоцентрического анализа с использованием экзоцентрических данных и объединенные фреймворки обучения. Обзор также охватывает наборы данных, ограничения текущих работ и перспективные направления будущих исследований в области машинного восприятия видео.'}, 'en': {'title': 'Bridging Perspectives for Enhanced Video Understanding', 'desc': 'This paper surveys the progress in video understanding by examining both egocentric (first-person) and exocentric (third-person) perspectives. It emphasizes the importance of combining these viewpoints to enhance machine perception of dynamic environments. The authors categorize recent advancements into three main research directions: improving exocentric understanding with egocentric data, enhancing egocentric analysis with exocentric data, and developing joint learning frameworks. The survey also discusses practical applications, key research tasks, benchmark datasets, and identifies limitations in current research while suggesting future directions.'}, 'zh': {'title': '融合视角，提升视频理解', 'desc': '这篇论文综述了视频理解领域的最新进展，重点关注自我中心（第一人称）和外部中心（第三人称）视角的结合。通过整合这两种视角，研究者们希望提升机器对动态环境的感知能力。论文还识别了实现这些应用的关键研究任务，并系统地组织了最近的研究进展。最后，作者讨论了当前工作的局限性，并提出了未来的研究方向，以推动视频理解和人工智能的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.05433', 'title': 'Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward', 'url': 'https://huggingface.co/papers/2506.05433', 'abstract': 'Prefix Grouper reduces computational overhead in GRPO by encoding shared prefixes only once, improving scalability in long-context scenarios without altering training dynamics or policy performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) enhances policy learning by computing gradients from relative comparisons among candidate outputs that share a common input prefix. Despite its effectiveness, GRPO introduces substantial computational overhead when processing long shared prefixes, which must be redundantly encoded for each group member. This inefficiency becomes a major scalability bottleneck in long-context learning scenarios. We propose Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant prefix computation via a Shared-Prefix Forward strategy. In particular, by restructuring self-attention into two parts, our method enables the shared prefix to be encoded only once, while preserving full differentiability and compatibility with end-to-end training. We provide both theoretical and empirical evidence that Prefix Grouper is training-equivalent to standard GRPO: it yields identical forward outputs and backward gradients, ensuring that the optimization dynamics and final policy performance remain unchanged. Empirically, our experiments confirm that Prefix Grouper achieves consistent results while significantly reducing the computational cost of training, particularly in long-prefix scenarios. The proposed method is fully plug-and-play: it is compatible with existing GRPO-based architectures and can be seamlessly integrated into current training pipelines as a drop-in replacement, requiring no structural modifications and only minimal changes to input construction and attention computation. Prefix Grouper enables the use of larger group sizes under the same computational budget, thereby improving the scalability of GRPO to more complex tasks and larger models. Code is now available at https://github.com/johncaged/PrefixGrouper', 'score': 4, 'issue_id': 4192, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'fe9abfe1e500f3e2', 'authors': ['Zikang Liu', 'Tongtian Yue', 'Yepeng Tang', 'Longteng Guo', 'Junxian Cai', 'Qingbin Liu', 'Xi Chen', 'Jing Liu'], 'affiliations': ['Basic Algorithm Center, Tencent', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'School of Computer Science and Technology, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05433.jpg', 'data': {'categories': ['#long_context', '#optimization', '#architecture', '#training'], 'emoji': '🧩', 'ru': {'title': 'Префикс Группер: эффективное масштабирование GRPO для длинных контекстов', 'desc': 'Префикс Группер - это эффективный алгоритм обучения для GRPO (Group Relative Policy Optimization), который устраняет избыточные вычисления общих префиксов. Метод реструктурирует самовнимание на две части, позволяя кодировать общий префикс только один раз, сохраняя при этом полную дифференцируемость и совместимость с обучением от начала до конца. Префикс Группер не меняет динамику оптимизации и конечную производительность политики, что подтверждается теоретически и эмпирически. Этот подход значительно снижает вычислительные затраты на обучение, особенно в сценариях с длинными префиксами, и может быть легко интегрирован в существующие архитектуры на основе GRPO.'}, 'en': {'title': 'Efficiently Scaling GRPO with Prefix Grouper', 'desc': 'The paper introduces Prefix Grouper, a novel algorithm designed to enhance the efficiency of Group Relative Policy Optimization (GRPO) by reducing computational overhead associated with encoding shared prefixes. By implementing a Shared-Prefix Forward strategy, it allows the shared prefix to be encoded only once, which significantly improves scalability in long-context scenarios without compromising the training dynamics or policy performance. The method maintains full differentiability and is compatible with end-to-end training, ensuring that the optimization process remains unchanged. Empirical results demonstrate that Prefix Grouper not only achieves consistent performance but also allows for larger group sizes within the same computational budget, making it a valuable addition to GRPO-based architectures.'}, 'zh': {'title': 'Prefix Grouper：提升 GRPO 的可扩展性', 'desc': 'Prefix Grouper 是一种高效的 GRPO 训练算法，通过共享前缀的前向策略，消除了冗余的前缀计算，从而减少了计算开销。该方法将自注意力结构重组为两个部分，使得共享前缀只需编码一次，同时保持完全的可微性和与端到端训练的兼容性。实验结果表明，Prefix Grouper 在长前缀场景中显著降低了训练的计算成本，同时确保优化动态和最终策略性能不变。该方法可以无缝集成到现有的 GRPO 架构中，支持更大的组大小，从而提高 GRPO 在复杂任务和大模型中的可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2506.06199', 'title': '3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model', 'url': 'https://huggingface.co/papers/2506.06199', 'abstract': 'A 3D flow world model learned from human and robot manipulation data, using video diffusion and GPT-4o, enables robots to perform diverse manipulation tasks with strong generalization and cross-embodiment adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t Manipulation has long been a challenging task for robots, while humans can effortlessly perform complex interactions with objects, such as hanging a cup on the mug rack. A key reason is the lack of a large and uniform dataset for teaching robots manipulation skills. Current robot datasets often record robot action in different action spaces within a simple scene. This hinders the robot to learn a unified and robust action representation for different robots within diverse scenes. Observing how humans understand a manipulation task, we find that understanding how the objects should move in the 3D space is a critical clue for guiding actions. This clue is embodiment-agnostic and suitable for both humans and different robots. Motivated by this, we aim to learn a 3D flow world model from both human and robot manipulation data. This model predicts the future movement of the interacting objects in 3D space, guiding action planning for manipulation. Specifically, we synthesize a large-scale 3D optical flow dataset, named ManiFlow-110k, through a moving object auto-detect pipeline. A video diffusion-based world model then learns manipulation physics from these data, generating 3D optical flow trajectories conditioned on language instructions. With the generated 3D object optical flow, we propose a flow-guided rendering mechanism, which renders the predicted final state and leverages GPT-4o to assess whether the predicted flow aligns with the task description. This equips the robot with a closed-loop planning ability. Finally, we consider the predicted 3D optical flow as constraints for an optimization policy to determine a chunk of robot actions for manipulation. Extensive experiments demonstrate strong generalization across diverse robotic manipulation tasks and reliable cross-embodiment adaptation without hardware-specific training.', 'score': 3, 'issue_id': 4186, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'c1b9d0a9c29bdf3b', 'authors': ['Hongyan Zhi', 'Peihao Chen', 'Siyuan Zhou', 'Yubo Dong', 'Quanxi Wu', 'Lei Han', 'Mingkui Tan'], 'affiliations': ['Hong Kong University of Science and Technology', 'Pazhou Laboratory', 'South China University of Technology', 'Tencent Robotics'], 'pdf_title_img': 'assets/pdf/title_img/2506.06199.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#robotics', '#optimization', '#3d', '#games'], 'emoji': '🤖', 'ru': {'title': 'Универсальная модель 3D-потока для роботизированных манипуляций', 'desc': 'Статья представляет модель 3D-потока для манипуляций роботов, обученную на данных человеческих и роботизированных манипуляций. Модель использует видеодиффузию и GPT-4o для прогнозирования движения объектов в 3D-пространстве. Авторы создали датасет ManiFlow-110k и разработали механизм рендеринга на основе потока для оценки соответствия предсказанных действий поставленной задаче. Эксперименты показали сильную обобщающую способность модели и возможность адаптации к различным роботизированным системам без специфического обучения.'}, 'en': {'title': 'Empowering Robots with 3D Flow for Versatile Manipulation', 'desc': 'This paper presents a novel 3D flow world model that enables robots to learn manipulation tasks by leveraging both human and robot data. The model predicts how objects move in 3D space, which helps robots plan their actions more effectively. By creating a large dataset called ManiFlow-110k and using a video diffusion approach, the researchers teach robots to understand manipulation physics and generate action plans based on language instructions. The results show that this method allows robots to generalize well across various tasks and adapt to different robotic embodiments without needing specific training for each hardware type.'}, 'zh': {'title': '学习3D流动模型，提升机器人操作能力', 'desc': '本论文提出了一种从人类和机器人操作数据中学习的3D流动世界模型，旨在帮助机器人执行多样化的操作任务。通过合成一个名为ManiFlow-110k的大规模3D光流数据集，模型能够预测交互对象在3D空间中的未来运动。利用视频扩散技术和GPT-4o，模型生成的3D光流轨迹可以指导机器人的操作规划。实验结果表明，该模型在不同的机器人操作任务中具有强大的泛化能力和跨实体适应性。'}}}, {'id': 'https://huggingface.co/papers/2506.05817', 'title': 'CodeContests+: High-Quality Test Case Generation for Competitive\n  Programming', 'url': 'https://huggingface.co/papers/2506.05817', 'abstract': 'An LLM-based system generates high-quality test cases for competitive programming problems, enhancing the accuracy of model evaluation and RL performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Competitive programming, due to its high reasoning difficulty and precise correctness feedback, has become a key task for both training and evaluating the reasoning capabilities of large language models (LLMs). However, while a large amount of public problem data, such as problem statements and solutions, is available, the test cases of these problems are often difficult to obtain. Therefore, test case generation is a necessary task for building large-scale datasets, and the quality of the test cases directly determines the accuracy of the evaluation. In this paper, we introduce an LLM-based agent system that creates high-quality test cases for competitive programming problems. We apply this system to the CodeContests dataset and propose a new version with improved test cases, named CodeContests+. We evaluated the quality of test cases in CodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels to examine the accuracy of these test cases in evaluation. The results indicated that CodeContests+ achieves significantly higher accuracy than CodeContests, particularly with a notably higher True Positive Rate (TPR). Subsequently, our experiments in LLM Reinforcement Learning (RL) further confirmed that improvements in test case quality yield considerable advantages for RL.', 'score': 2, 'issue_id': 4195, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'f0f3a151758192f0', 'authors': ['Zihan Wang', 'Siyao Liu', 'Yang Sun', 'Hongyan Li', 'Kai Shen'], 'affiliations': ['ByteDance Seed', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05817.jpg', 'data': {'categories': ['#benchmark', '#rl', '#optimization', '#reasoning', '#dataset', '#data'], 'emoji': '🏆', 'ru': {'title': 'ИИ создает тесты для соревновательного программирования: повышение точности оценки и эффективности обучения', 'desc': 'Представлена система на основе языковой модели (LLM) для генерации высококачественных тестовых примеров для задач соревновательного программирования. Система применяется к набору данных CodeContests, создавая улучшенную версию CodeContests+. Оценка качества тестовых примеров показала значительно более высокую точность CodeContests+, особенно в отношении истинно положительного коэффициента (TPR). Эксперименты с обучением с подкреплением (RL) подтвердили преимущества улучшенных тестовых примеров для RL.'}, 'en': {'title': 'Enhancing Competitive Programming Evaluation with LLM-Generated Test Cases', 'desc': 'This paper presents a system that uses large language models (LLMs) to generate high-quality test cases for competitive programming problems. The generation of these test cases is crucial because they enhance the evaluation accuracy of models and improve reinforcement learning (RL) performance. The authors introduce a new dataset, CodeContests+, which contains improved test cases derived from the original CodeContests dataset. Their evaluation shows that the new test cases significantly increase the True Positive Rate (TPR) and overall accuracy, demonstrating the benefits of high-quality test case generation for model training and assessment.'}, 'zh': {'title': '基于LLM的高质量测试用例生成', 'desc': '本文介绍了一种基于大型语言模型（LLM）的系统，该系统能够为竞争编程问题生成高质量的测试用例。这项技术提高了模型评估的准确性，并增强了强化学习（RL）的性能。由于竞争编程问题的测试用例难以获取，生成测试用例成为构建大规模数据集的必要任务。我们的实验表明，改进后的测试用例在准确性上显著优于原始数据集，尤其在真实正例率（TPR）方面表现突出。'}}}, {'id': 'https://huggingface.co/papers/2506.04120', 'title': 'Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot\n  Data', 'url': 'https://huggingface.co/papers/2506.04120', 'abstract': 'A hybrid real-to-sim framework combining 3D Gaussian Splatting and physics simulation with MuJoCo allows simultaneous high-fidelity object reconstruction and accurate robot pose calibration from raw trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating accurate, physical simulations directly from real-world robot motion holds great value for safe, scalable, and affordable robot learning, yet remains exceptionally challenging. Real robot data suffers from occlusions, noisy camera poses, dynamic scene elements, which hinder the creation of geometrically accurate and photorealistic digital twins of unseen objects. We introduce a novel real-to-sim framework tackling all these challenges at once. Our key insight is a hybrid scene representation merging the photorealistic rendering of 3D Gaussian Splatting with explicit object meshes suitable for physics simulation within a single representation. We propose an end-to-end optimization pipeline that leverages differentiable rendering and differentiable physics within MuJoCo to jointly refine all scene components - from object geometry and appearance to robot poses and physical parameters - directly from raw and imprecise robot trajectories. This unified optimization allows us to simultaneously achieve high-fidelity object mesh reconstruction, generate photorealistic novel views, and perform annotation-free robot pose calibration. We demonstrate the effectiveness of our approach both in simulation and on challenging real-world sequences using an ALOHA 2 bi-manual manipulator, enabling more practical and robust real-to-simulation pipelines.', 'score': 2, 'issue_id': 4194, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'd808acd50e3bfd0c', 'authors': ['Ben Moran', 'Mauro Comi', 'Steven Bohez', 'Tom Erez', 'Zhibin Li', 'Leonard Hasenclever'], 'affiliations': ['Google DeepMind', 'University College London', 'University of Bristol'], 'pdf_title_img': 'assets/pdf/title_img/2506.04120.jpg', 'data': {'categories': ['#optimization', '#3d', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Гибридная реконструкция сцен для точного моделирования роботов', 'desc': 'Статья представляет новую гибридную систему для создания реалистичных симуляций роботов на основе реальных данных. Она объединяет 3D Gaussian Splatting для фотореалистичного рендеринга с физическим моделированием в MuJoCo. Система позволяет одновременно реконструировать геометрию объектов с высокой точностью и калибровать положение робота без дополнительных аннотаций. Этот подход решает проблемы окклюзий, шума в данных и динамических элементов сцены, делая создание цифровых двойников более точным и практичным.'}, 'en': {'title': 'Bridging Reality and Simulation for Enhanced Robot Learning', 'desc': 'This paper presents a new framework that combines 3D Gaussian Splatting and physics simulation to improve robot learning from real-world data. It addresses challenges like occlusions and noisy camera poses that make it hard to create accurate digital models of objects. The authors introduce a hybrid representation that merges photorealistic rendering with physics-compatible object meshes, allowing for better simulations. Their end-to-end optimization process refines object geometry, appearance, and robot poses from raw data, achieving high-fidelity reconstructions and accurate pose calibration without needing annotations.'}, 'zh': {'title': '实现真实与仿真的完美结合', 'desc': '本论文提出了一种混合的真实到仿真框架，结合了3D高斯点云和MuJoCo物理仿真，能够从原始轨迹中同时实现高保真物体重建和准确的机器人姿态校准。该框架解决了真实机器人数据中的遮挡、噪声相机姿态和动态场景元素等挑战，创建几何上准确且逼真的数字双胞胎。我们提出了一种新的场景表示方法，将3D高斯点云的光照真实渲染与适合物理仿真的物体网格结合在一起。通过端到端的优化流程，我们能够直接从不精确的机器人轨迹中优化所有场景组件，提升了物体重建的精度和机器人姿态的校准效果。'}}}, {'id': 'https://huggingface.co/papers/2506.04255', 'title': 'HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization', 'url': 'https://huggingface.co/papers/2506.04255', 'abstract': 'HASHIRU, a novel MAS framework, enhances flexibility, resource efficiency, and adaptability by dynamically managing specialized agents and using a hybrid intelligence approach with smaller, local LLMs and external APIs.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid Large Language Model (LLM) advancements are fueling autonomous Multi-Agent System (MAS) development. However, current frameworks often lack flexibility, resource awareness, model diversity, and autonomous tool creation. This paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent Resource Utilization), a novel MAS framework enhancing flexibility, resource efficiency, and adaptability. HASHIRU features a "CEO" agent dynamically managing specialized "employee" agents, instantiated based on task needs and resource constraints (cost, memory). Its hybrid intelligence prioritizes smaller, local LLMs (via Ollama) while flexibly using external APIs and larger models when necessary. An economic model with hiring/firing costs promotes team stability and efficient resource allocation. The system also includes autonomous API tool creation and a memory function. Evaluations on tasks like academic paper review (58% success), safety assessments (100% on a JailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash on GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate HASHIRU\'s capabilities. Case studies illustrate its self-improvement via autonomous cost model generation, tool integration, and budget management. HASHIRU offers a promising approach for more robust, efficient, and adaptable MAS through dynamic hierarchical control, resource-aware hybrid intelligence, and autonomous functional extension. Source code and benchmarks are available at https://github.com/HASHIRU-AI/HASHIRU and https://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is available at https://hashiruagentx-hashiruai.hf.space upon request.', 'score': 2, 'issue_id': 4189, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'd3d7d73af3533148', 'authors': ['Kunal Pai', 'Parth Shah', 'Harshil Patel'], 'affiliations': ['Independent Researcher', 'UC Davis'], 'pdf_title_img': 'assets/pdf/title_img/2506.04255.jpg', 'data': {'categories': ['#architecture', '#agi', '#optimization', '#benchmark', '#agents', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Гибкая и эффективная мультиагентная система нового поколения', 'desc': 'HASHIRU - это новая система мультиагентного взаимодействия, которая повышает гибкость, эффективность использования ресурсов и адаптивность. Она использует динамическое управление специализированными агентами и гибридный подход к интеллекту, сочетая небольшие локальные языковые модели и внешние API. HASHIRU включает экономическую модель с затратами на найм/увольнение агентов для стабильности команды и эффективного распределения ресурсов. Система продемонстрировала высокую эффективность в различных задачах, превзойдя некоторые существующие модели.'}, 'en': {'title': 'HASHIRU: Dynamic Intelligence for Efficient Multi-Agent Systems', 'desc': "HASHIRU is a new framework for Multi-Agent Systems (MAS) that improves flexibility and resource efficiency by managing specialized agents dynamically. It uses a hybrid intelligence approach, combining smaller local Large Language Models (LLMs) with external APIs to adapt to different tasks. The framework includes a 'CEO' agent that oversees 'employee' agents based on the specific needs and available resources, promoting efficient team management. Evaluations show HASHIRU's strong performance in various tasks, highlighting its ability to autonomously create tools and manage resources effectively."}, 'zh': {'title': 'HASHIRU：灵活高效的多智能体系统新框架', 'desc': 'HASHIRU是一个新颖的多智能体系统（MAS）框架，旨在提高灵活性、资源效率和适应性。它通过动态管理专门的代理（如“CEO”代理和“员工”代理）来满足任务需求和资源限制。该框架采用混合智能，优先使用较小的本地大语言模型（LLM），并在必要时灵活调用外部API和更大的模型。HASHIRU的评估结果显示其在多个任务上表现出色，展示了其在动态控制和资源感知方面的优势。'}}}, {'id': 'https://huggingface.co/papers/2506.04755', 'title': 'Truth in the Few: High-Value Data Selection for Efficient Multi-Modal\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.04755', 'abstract': "A new data selection paradigm, Reasoning Activation Potential (RAP), enhances multi-modal reasoning in large language models using minimal high-value datasets, improving performance and reducing computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t While multi-modal large language models (MLLMs) have made significant progress in complex reasoning tasks via reinforcement learning, it is commonly believed that extensive training data is necessary for improving multi-modal reasoning ability, inevitably leading to data redundancy and substantial computational costs. However, can smaller high-value datasets match or outperform full corpora for multi-modal reasoning in MLLMs? In this work, we challenge this assumption through a key observation: meaningful multi-modal reasoning is triggered by only a sparse subset of training samples, termed cognitive samples, whereas the majority contribute marginally. Building on this insight, we propose a novel data selection paradigm termed Reasoning Activation Potential (RAP), which identifies cognitive samples by estimating each sample's potential to stimulate genuine multi-modal reasoning by two complementary estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential outcome model principle, eliminates samples that overly rely on language priors by comparing outputs between multi-modal and text-only inputs; 2) Attention Confidence Estimator (ACE), which exploits token-level self-attention to discard samples dominated by irrelevant but over-emphasized tokens in intermediate reasoning stages. Moreover, we introduce a Difficulty-aware Replacement Module (DRM) to substitute trivial instances with cognitively challenging ones, thereby ensuring complexity for robust multi-modal reasoning. Experiments on six datasets show that our RAP method consistently achieves superior performance using only 9.3% of the training data, while reducing computational costs by over 43%. Our code is available at https://github.com/Leo-ssl/RAP.", 'score': 1, 'issue_id': 4194, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'e4a5cd694d56b2a0', 'authors': ['Shenshen Li', 'Kaiyuan Deng', 'Lei Wang', 'Hao Yang', 'Chong Peng', 'Peng Yan', 'Fumin Shen', 'Heng Tao Shen', 'Xing Xu'], 'affiliations': ['Meituan', 'Salesforce AI Research', 'School of Computer Science and Technology, Tongji University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04755.jpg', 'data': {'categories': ['#multimodal', '#data', '#optimization', '#dataset', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное мультимодальное обучение: меньше данных, больше рассуждений', 'desc': "Статья представляет новую парадигму отбора данных под названием Reasoning Activation Potential (RAP) для улучшения мультимодального рассуждения в больших языковых моделях. RAP идентифицирует ключевые 'когнитивные' образцы данных, которые стимулируют подлинное мультимодальное рассуждение, используя два оценщика: Causal Discrepancy Estimator и Attention Confidence Estimator. Метод также включает модуль Difficulty-aware Replacement для замены тривиальных примеров на более сложные. Эксперименты показывают, что RAP достигает превосходных результатов, используя всего 9.3% обучающих данных, при этом снижая вычислительные затраты на 43%."}, 'en': {'title': 'Unlocking Multi-Modal Reasoning with Less Data: The RAP Approach', 'desc': 'The paper introduces a new method called Reasoning Activation Potential (RAP) to improve multi-modal reasoning in large language models (MLLMs) using smaller, high-value datasets. It challenges the belief that large amounts of training data are necessary for effective reasoning, showing that only a small subset of samples, known as cognitive samples, can trigger meaningful reasoning. RAP employs two estimators: the Causal Discrepancy Estimator (CDE) to filter out less relevant samples and the Attention Confidence Estimator (ACE) to focus on important tokens during reasoning. The results demonstrate that RAP can enhance performance while significantly reducing the amount of data and computational resources needed.'}, 'zh': {'title': '推理激活潜力：高效的多模态推理新方法', 'desc': '本文提出了一种新的数据选择范式，称为推理激活潜力（RAP），旨在通过使用最小的高价值数据集来增强大型语言模型的多模态推理能力。研究表明，真正的多模态推理只需少量关键样本，而大多数样本的贡献微乎其微。RAP通过两个互补的估计器来识别这些关键样本，从而提高模型的性能并降低计算成本。实验结果显示，RAP方法在仅使用9.3%的训练数据的情况下，性能显著优于传统方法，同时计算成本降低超过43%。'}}}, {'id': 'https://huggingface.co/papers/2506.00649', 'title': 'GuideX: Guided Synthetic Data Generation for Zero-Shot Information\n  Extraction', 'url': 'https://huggingface.co/papers/2506.00649', 'abstract': 'GUIDEX enhances zero-shot Named Entity Recognition by automatically defining schemas and inferring guidelines, setting new benchmarks without extensive human-labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Information Extraction (IE) systems are traditionally domain-specific, requiring costly adaptation that involves expert schema design, data annotation, and model training. While Large Language Models have shown promise in zero-shot IE, performance degrades significantly in unseen domains where label definitions differ. This paper introduces GUIDEX, a novel method that automatically defines domain-specific schemas, infers guidelines, and generates synthetically labeled instances, allowing for better out-of-domain generalization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art across seven zeroshot Named Entity Recognition benchmarks. Models trained with GUIDEX gain up to 7 F1 points over previous methods without humanlabeled data, and nearly 2 F1 points higher when combined with it. Models trained on GUIDEX demonstrate enhanced comprehension of complex, domain-specific annotation schemas. Code, models, and synthetic datasets are available at neilus03.github.io/guidex.com', 'score': 1, 'issue_id': 4194, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '32639eb393594459', 'authors': ['Neil De La Fuente', 'Oscar Sainz', 'Iker García-Ferrero', 'Eneko Agirre'], 'affiliations': ['HiTZ Basque Center for Language Technology - Ixa NLP Group', 'Technical University of Munich (TUM)', 'University of the Basque Country (UPV/EHU)'], 'pdf_title_img': 'assets/pdf/title_img/2506.00649.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#synthetic', '#training', '#benchmark'], 'emoji': '🏷️', 'ru': {'title': 'Автоматическое создание схем и руководств для улучшения zero-shot NER', 'desc': 'GUIDEX - это новый метод, который автоматически определяет схемы для конкретных доменов, выводит руководства и генерирует синтетически размеченные примеры для задачи распознавания именованных сущностей (NER). Он позволяет улучшить обобщение на новые домены без использования размеченных человеком данных. Дообучение модели Llama 3.1 с помощью GUIDEX устанавливает новый state-of-the-art результат на семи бенчмарках по zero-shot NER. Модели, обученные с GUIDEX, демонстрируют улучшенное понимание сложных, специфичных для доменов схем аннотации.'}, 'en': {'title': 'Revolutionizing Zero-Shot NER with GUIDEX!', 'desc': 'GUIDEX is a new approach that improves zero-shot Named Entity Recognition (NER) by automatically creating schemas and guidelines for different domains. Traditional NER systems need a lot of human effort to design schemas and label data, which can be expensive and time-consuming. GUIDEX uses Large Language Models to generate synthetic labeled data, helping models perform better in new, unseen domains. By fine-tuning Llama 3.1 with GUIDEX, researchers achieved significant improvements in NER performance, setting new records without relying on extensive human-labeled datasets.'}, 'zh': {'title': 'GUIDEX：零-shot NER的新突破', 'desc': 'GUIDEX是一种新方法，旨在增强零-shot命名实体识别（NER）的能力。它通过自动定义领域特定的模式和推断指导方针，减少了对大量人工标注数据的依赖。使用GUIDEX微调Llama 3.1模型，在七个零-shot NER基准测试中创造了新的最佳成绩。与之前的方法相比，使用GUIDEX训练的模型在没有人工标注数据的情况下，F1分数提高了7分，结合使用时也提高了近2分。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (1)', '#agi (2)', '#alignment', '#architecture (4)', '#audio (2)', '#benchmark (6)', '#cv (1)', '#data (3)', '#dataset (8)', '#diffusion (2)', '#ethics', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference', '#interpretability (1)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (6)', '#open_source (5)', '#optimization (8)', '#plp', '#rag', '#reasoning (3)', '#rl (1)', '#rlhf', '#robotics (2)', '#science', '#security', '#small_models (1)', '#story_generation', '#survey (2)', '#synthetic (2)', '#training (6)', '#transfer_learning (3)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-09 15:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-09 15:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-09 15:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    