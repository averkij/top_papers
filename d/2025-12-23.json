{
    "date": {
        "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 23",
        "zh": "12æœˆ23æ—¥"
    },
    "time_utc": "2025-12-23 08:32",
    "weekday": 1,
    "issue_id": 196,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.16676",
            "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
            "url": "https://huggingface.co/papers/2512.16676",
            "abstract": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
            "score": 74,
            "issue_id": 192,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "22eafee308563f25",
            "authors": [
                "Hao Liang",
                "Xiaochen Ma",
                "Zhou Liu",
                "Zhen Hao Wong",
                "Zhengyang Zhao",
                "Zimo Meng",
                "Runming He",
                "Chengyu Shen",
                "Qifeng Cai",
                "Zhaoyang Han",
                "Meiyi Qiang",
                "Yalin Feng",
                "Tianyi Bai",
                "Zewei Pan",
                "Ziyi Guo",
                "Yizhen Jiang",
                "Jingwen Deng",
                "Qijie You",
                "Peichao Lai",
                "Tianyu Guo",
                "Chi Hsu Tsai",
                "Hengyi Feng",
                "Rui Hu",
                "Wenkai Yu",
                "Junbo Niu",
                "Bohan Zeng",
                "Ruichuan An",
                "Lu Ma",
                "Jihao Huang",
                "Yaowei Zheng",
                "Conghui He",
                "Linpeng Tang",
                "Bin Cui",
                "Weinan E",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Institute for Advanced Algorithms Research, Shanghai",
                "LLaMA-Factory Team",
                "OpenDataLab",
                "OriginHub Technology",
                "Peking University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.16676.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#synthetic",
                    "#optimization",
                    "#data",
                    "#rag",
                    "#dataset",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "DataFlow â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 200 Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ 6 Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. DataFlow-Agent Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LLM Ğ½Ğ° 3-7% Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Data Preparation for LLMs with DataFlow",
                    "desc": "DataFlow is a framework that uses Large Language Models (LLMs) to improve the quality and reproducibility of data preparation for various tasks. It addresses the limitations of current data preparation methods, which often rely on inconsistent scripts and workflows. By providing a modular and extensible system with nearly 200 reusable operators, DataFlow allows users to create optimized data pipelines easily. The framework has shown significant performance improvements in LLM tasks, outperforming traditional datasets and establishing a strong foundation for future advancements in data-centric AI."
                },
                "zh": {
                    "title": "DataFlowï¼šæå‡LLMæ€§èƒ½çš„æ•°æ®å‡†å¤‡æ–°æ¡†æ¶",
                    "desc": "DataFlowæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°æ®å‡†å¤‡æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ•°æ®è´¨é‡å’Œå¯é‡å¤æ€§ã€‚å®ƒé€šè¿‡è‡ªåŠ¨ç”Ÿæˆæ•°æ®å¤„ç†ç®¡é“ï¼Œå¢å¼ºäº†LLMçš„æ€§èƒ½ï¼Œè§£å†³äº†å½“å‰æ•°æ®å‡†å¤‡ä¸­å­˜åœ¨çš„ä¸´æ—¶è„šæœ¬å’Œä¸è§„èŒƒå·¥ä½œæµçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æä¾›äº†è¿‘200ä¸ªå¯é‡ç”¨çš„æ“ä½œç¬¦å’Œå¤šä¸ªé¢†åŸŸé€šç”¨çš„ç®¡é“ï¼Œæ”¯æŒæ–‡æœ¬ã€æ•°å­¦æ¨ç†ã€ä»£ç ç­‰å¤šç§ä»»åŠ¡ã€‚é€šè¿‡DataFlow-Agentï¼Œç”¨æˆ·å¯ä»¥å°†è‡ªç„¶è¯­è¨€è§„èŒƒè‡ªåŠ¨è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„ç®¡é“ï¼Œä»è€Œç®€åŒ–æ•°æ®å‡†å¤‡è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19693",
            "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
            "url": "https://huggingface.co/papers/2512.19693",
            "abstract": "Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.",
            "score": 40,
            "issue_id": 192,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "a014d388da9b25ca",
            "authors": [
                "Weichen Fan",
                "Haiwen Diao",
                "Quan Wang",
                "Dahua Lin",
                "Ziwei Liu"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19693.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸŒˆ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ°, Ğ° Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Unified Autoencoding, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¾Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ImageNet Ğ¸ MS-COCO Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ĞµĞ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "Harmonizing Semantic and Pixel Information in Unified Autoencoding",
                    "desc": "This paper introduces Unified Autoencoding (UAE), a novel approach that integrates semantic and pixel-level information using a frequency-band modulator. The authors analyze how different encoders capture various frequency components, revealing that semantic encoders focus on low-frequency data while pixel encoders retain high-frequency details. They propose the Prism Hypothesis, suggesting that different data modalities project onto a shared feature spectrum, similar to how a prism disperses light. Through extensive testing on image benchmarks like ImageNet and MS-COCO, the UAE demonstrates superior performance by effectively merging abstract meanings with detailed pixel information in a unified latent space."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è‡ªç¼–ç ï¼šèåˆè¯­ä¹‰ä¸åƒç´ çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç»Ÿä¸€è‡ªç¼–ç ï¼ˆUnified Autoencoding, UAEï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°†è¯­ä¹‰ä¿¡æ¯å’Œåƒç´ çº§ä¿¡æ¯ç»“åˆèµ·æ¥ã€‚é€šè¿‡é¢‘ç‡å¸¦è°ƒåˆ¶å™¨ï¼ŒUAEèƒ½å¤Ÿåœ¨åŒä¸€ä¸ªæ½œåœ¨ç©ºé—´ä¸­æœ‰æ•ˆåœ°èåˆæŠ½è±¡çš„è¯­ä¹‰ç»“æ„å’Œç»†è‡´çš„åƒç´ ç»†èŠ‚ã€‚ç ”ç©¶å‘ç°ï¼Œè¯­ä¹‰ç¼–ç å™¨ä¸»è¦æ•æ‰ä½é¢‘æˆåˆ†ï¼Œè€Œåƒç´ ç¼–ç å™¨åˆ™ä¿ç•™é«˜é¢‘ä¿¡æ¯ï¼Œè¿™ä¸ºç¼–ç å™¨çš„è¡Œä¸ºæä¾›äº†æ–°çš„è§†è§’ã€‚é€šè¿‡åœ¨ImageNetå’ŒMS-COCOåŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†UAEåœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17650",
            "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
            "url": "https://huggingface.co/papers/2512.17650",
            "abstract": "ReCo is a novel instructional video editing paradigm that enhances accuracy and reduces token interference by incorporating constraint modeling and regularization techniques during in-context generation.  \t\t\t\t\tAI-generated summary \t\t\t\t The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.",
            "score": 31,
            "issue_id": 192,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "95db48ea9c82c952",
            "authors": [
                "Zhongwei Zhang",
                "Fuchen Long",
                "Wei Li",
                "Zhaofan Qiu",
                "Wu Liu",
                "Ting Yao",
                "Tao Mei"
            ],
            "affiliations": [
                "HiDream.ai Inc.",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17650.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#video",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "ReCo â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½ĞµÑ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ â€” Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¸ attention Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ â€” Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¼ĞµÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ›Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ·Ğ¾Ğ½Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ²Ğ½Ğµ ÑÑ‚Ğ¸Ñ… Ğ·Ğ¾Ğ½. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ReCo-Data Ñ 500K Ğ¿Ğ°Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ-Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "ReCo: Precision in Instructional Video Editing through Constraint Modeling",
                    "desc": "ReCo is a new approach to instructional video editing that improves the accuracy of edits and minimizes unwanted interference between different parts of the video. It uses constraint modeling to clearly define which areas of the video should be edited and which should not, enhancing the in-context generation process. By applying regularization techniques, ReCo ensures that the editing focus is maintained while reducing noise from non-editing areas. Additionally, it introduces a comprehensive dataset, ReCo-Data, to support the training of models for better video editing outcomes."
                },
                "zh": {
                    "title": "ReCoï¼šæå‡è§†é¢‘ç¼–è¾‘å‡†ç¡®æ€§çš„åˆ›æ–°èŒƒå¼",
                    "desc": "ReCoæ˜¯ä¸€ç§æ–°é¢–çš„æ•™å­¦è§†é¢‘ç¼–è¾‘èŒƒå¼ï¼Œé€šè¿‡åœ¨ä¸Šä¸‹æ–‡ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥çº¦æŸå»ºæ¨¡å’Œæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œæé«˜äº†ç¼–è¾‘çš„å‡†ç¡®æ€§å¹¶å‡å°‘äº†æ ‡è®°å¹²æ‰°ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨æ²¡æœ‰æ˜ç¡®æŒ‡å®šç¼–è¾‘åŒºåŸŸæ—¶ï¼Œç¼–è¾‘åŒºåŸŸä¸å‡†ç¡®å’Œç¼–è¾‘ä¸éç¼–è¾‘åŒºåŸŸä¹‹é—´çš„å¹²æ‰°é—®é¢˜ã€‚ReCoé€šè¿‡å®½åº¦æ‹¼æ¥æºè§†é¢‘å’Œç›®æ ‡è§†é¢‘è¿›è¡Œè”åˆå»å™ªï¼Œå¹¶åˆ©ç”¨æ½œåœ¨å’Œæ³¨æ„åŠ›æ­£åˆ™åŒ–æ¥æ ¡å‡†è§†é¢‘æ‰©æ•£å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReCoåœ¨å››ä¸ªä¸»è¦çš„åŸºäºæŒ‡ä»¤çš„è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17040",
            "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation",
            "url": "https://huggingface.co/papers/2512.17040",
            "abstract": "InfCam generates high-fidelity videos with accurate camera poses by using infinite homography warping and augmenting synthetic datasets with diverse trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/",
            "score": 23,
            "issue_id": 194,
            "pub_date": "2025-12-18",
            "pub_date_card": {
                "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 18",
                "zh": "12æœˆ18æ—¥"
            },
            "hash": "e6c880bb6a736d5f",
            "authors": [
                "Min-Jung Kim",
                "Jeongho Kim",
                "Hoiyeong Jin",
                "Junha Hyung",
                "Jaegul Choo"
            ],
            "affiliations": [
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17040.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#diffusion",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¾Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹",
                    "desc": "InfCam â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½ÑƒÑ Ğ³Ğ¾Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² 2D Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»Ğ°ĞºÑ Ñ‡ĞµÑ€ĞµĞ· ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²ÑŒÑĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ InfCam Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‡ĞµÑ‚ĞºĞ¾ÑÑ‚Ğ¸, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑÑŒ Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "InfCam: High-Fidelity Video Generation with Accurate Camera Control",
                    "desc": "InfCam is a novel framework for generating high-quality videos that accurately reflect specified camera poses without relying on depth estimation. It utilizes infinite homography warping to directly incorporate 3D camera rotations into the 2D latent space of a video diffusion model, enhancing pose fidelity. Additionally, it employs a data augmentation strategy to create diverse camera trajectories from existing synthetic datasets, addressing the limitations of previous methods. Experimental results show that InfCam significantly improves camera-pose accuracy and visual quality, effectively bridging the gap between synthetic and real-world video generation."
                },
                "zh": {
                    "title": "é«˜ä¿çœŸè§†é¢‘ç”Ÿæˆä¸ç›¸æœºæ§åˆ¶çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "InfCam æ˜¯ä¸€ç§æ— æ·±åº¦ã€ç›¸æœºæ§åˆ¶çš„è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„è§†é¢‘å¹¶å‡†ç¡®æ§åˆ¶ç›¸æœºå§¿æ€ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ— é™å•åº”æ€§å˜æ¢ï¼Œå°†ä¸‰ç»´ç›¸æœºæ—‹è½¬ä¿¡æ¯ç›´æ¥ç¼–ç åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„äºŒç»´æ½œåœ¨ç©ºé—´ä¸­ã€‚é€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒï¼Œé¢„æµ‹æ®‹å·®è§†å·®é¡¹ï¼Œä»¥å®ç°é«˜ç›¸æœºå§¿æ€ä¿çœŸåº¦ã€‚åŒæ—¶ï¼Œæ•°æ®å¢å¼ºç®¡é“å°†ç°æœ‰çš„åˆæˆå¤šè§†å›¾æ•°æ®é›†è½¬åŒ–ä¸ºå…·æœ‰å¤šæ ·åŒ–è½¨è¿¹å’Œç„¦è·çš„åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInfCam åœ¨ç›¸æœºå§¿æ€å‡†ç¡®æ€§å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°ä»åˆæˆæ•°æ®æ¨å¹¿åˆ°çœŸå®ä¸–ç•Œæ•°æ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19134",
            "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2512.19134",
            "abstract": "QuCo-RAG uses objective corpus statistics to mitigate hallucinations in large language models during generation, improving accuracy across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.",
            "score": 20,
            "issue_id": 192,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "fe346e71e1011948",
            "authors": [
                "Dehai Min",
                "Kailin Zhang",
                "Tongtong Wu",
                "Lu Cheng"
            ],
            "affiliations": [
                "Monash University",
                "New York University",
                "University of Illinois at Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19134.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#hallucinations",
                    "#benchmark",
                    "#rag"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞÑ‚ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞµ Ğ² RAG",
                    "desc": "QuCo-RAG â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ»Ğ¾Ğ² (ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…), Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ¾Ğ³Ğ´Ğ° ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½ÑƒĞ»ĞµĞ²Ğ°Ñ, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ RAG. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 5-14 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ EM Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Mitigating Hallucinations with Objective Corpus Insights",
                    "desc": "QuCo-RAG is a novel approach designed to reduce hallucinations in large language models (LLMs) during text generation by utilizing objective statistics from the training corpus. Instead of relying on the model's internal confidence signals, which can be unreliable, QuCo-RAG assesses uncertainty through two key stages: identifying low-frequency entities before generation and verifying entity co-occurrence during generation. This method employs Infini-gram for rapid queries over a vast dataset, allowing for dynamic retrieval when uncertainty is detected. Experimental results demonstrate significant improvements in accuracy across various benchmarks, showcasing QuCo-RAG's effectiveness in enhancing the reliability of LLM outputs."
                },
                "zh": {
                    "title": "ç”¨å®¢è§‚ç»Ÿè®¡æ¶ˆé™¤è¯­è¨€æ¨¡å‹çš„å¹»è§‰",
                    "desc": "QuCo-RAGæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°çš„å¹»è§‰ç°è±¡ã€‚å®ƒé€šè¿‡ä½¿ç”¨å®¢è§‚çš„è¯­æ–™åº“ç»Ÿè®¡æ•°æ®æ¥æé«˜ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œè€Œä¸æ˜¯ä¾èµ–æ¨¡å‹å†…éƒ¨ä¿¡å·ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè¯†åˆ«ä½é¢‘å®ä½“ä»¥å‘ç°çŸ¥è¯†ç¼ºå£ï¼Œå…¶æ¬¡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­éªŒè¯å®ä½“çš„å…±ç°æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQuCo-RAGåœ¨å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨åŠ¨æ€æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.18880",
            "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
            "url": "https://huggingface.co/papers/2512.18880",
            "abstract": "Large Language Models struggle to accurately estimate human cognitive difficulty due to a misalignment with human perceptions and a lack of introspection regarding their own limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.",
            "score": 15,
            "issue_id": 192,
            "pub_date": "2025-12-21",
            "pub_date_card": {
                "ru": "21 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 21",
                "zh": "12æœˆ21æ—¥"
            },
            "hash": "e0ec22a61e469da2",
            "authors": [
                "Ming Li",
                "Han Chen",
                "Yunze Xiao",
                "Jian Chen",
                "Hong Jiao",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "University at Buffalo",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.18880.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 20 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ: ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸. Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ğ½Ğ¸ Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ»ĞµĞ¿Ñ‹Ğµ Ğ¿ÑÑ‚Ğ½Ğ°."
                },
                "en": {
                    "title": "Bridging the Gap: Aligning AI with Human Cognitive Difficulty",
                    "desc": "This paper investigates how well Large Language Models (LLMs) can estimate the difficulty of tasks from a human perspective. It highlights a significant misalignment between the models' assessments and actual human cognitive challenges, particularly in educational contexts. The study reveals that simply increasing the size of the models does not improve their ability to understand human difficulty levels, as they tend to converge on a machine-centric view. Additionally, the models show a lack of self-awareness regarding their limitations, which complicates their use for accurate difficulty prediction in educational assessments."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»è®¤çŸ¥éš¾åº¦çš„å¯¹é½æŒ‘æˆ˜",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å‡†ç¡®ä¼°è®¡äººç±»è®¤çŸ¥éš¾åº¦æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ä¸äººç±»çš„æ„ŸçŸ¥ä¸ä¸€è‡´ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹è‡ªèº«å±€é™æ€§çš„åæ€ã€‚æ•™è‚²è¯„ä¼°ä¸­ï¼Œå‡†ç¡®ä¼°è®¡é¢˜ç›®ï¼ˆé—®é¢˜æˆ–ä»»åŠ¡ï¼‰éš¾åº¦è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´å†·å¯åŠ¨é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹20ç§ä¸åŒé¢†åŸŸçš„æ¨¡å‹è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯åˆ†æï¼Œå‘ç°æ¨¡å‹çš„è§„æ¨¡æ‰©å¤§å¹¶æœªæœ‰æ•ˆæ”¹å–„ä¸äººç±»çš„éš¾åº¦å¯¹é½ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹çš„é«˜æ€§èƒ½å¾€å¾€å¦¨ç¢äº†å‡†ç¡®çš„éš¾åº¦ä¼°è®¡ï¼Œå› ä¸ºå®ƒä»¬éš¾ä»¥æ¨¡æ‹Ÿå­¦ç”Ÿçš„èƒ½åŠ›é™åˆ¶ï¼Œå³ä½¿åœ¨æ˜ç¡®æç¤ºç‰¹å®šç†Ÿç»ƒåº¦æ—¶ä¹Ÿå¦‚æ­¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19678",
            "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
            "url": "https://huggingface.co/papers/2512.19678",
            "abstract": "WorldWarp addresses the challenge of generating consistent long-range videos by integrating a 3D geometric cache with a spatio-temporal diffusion model, ensuring structural consistency and textural refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: https://hyokong.github.io/worldwarp-page/{https://hyokong.github.io/worldwarp-page/}.",
            "score": 13,
            "issue_id": 192,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "223c96ada398b23f",
            "authors": [
                "Hanyang Kong",
                "Xingyi Yang",
                "Xiaoxu Zheng",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19678.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ: ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ĞºÑÑˆĞ°",
                    "desc": "WorldWarp Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ 3D ĞºÑÑˆ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Gaussian Splatting Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¼ ÑˆÑƒĞ¼Ğ°, Ğ³Ğ´Ğµ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ° ÑƒĞ¶Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ â€” Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹. Ğ¢Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ ĞºÑÑˆ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹."
                },
                "en": {
                    "title": "WorldWarp: Bridging 3D Geometry and Video Fidelity",
                    "desc": "WorldWarp is a novel framework designed to generate long-range videos that maintain both geometric consistency and high-quality textures. It combines a 3D geometric cache, built using Gaussian Splatting, with a spatio-temporal diffusion model to ensure that new frames adhere to the established 3D structure. The framework addresses challenges like occlusions and complex camera movements by dynamically updating the 3D cache and employing a unique noise schedule for generating and refining video content. This approach allows WorldWarp to produce videos with superior fidelity, where 3D geometry guides the overall structure and diffusion techniques enhance the visual quality."
                },
                "zh": {
                    "title": "ç”Ÿæˆä¸€è‡´æ€§é•¿è§†é¢‘çš„æ–°æ–¹æ³•",
                    "desc": "WorldWarp æ˜¯ä¸€ä¸ªè§£å†³ç”Ÿæˆä¸€è‡´æ€§é•¿è§†é¢‘æŒ‘æˆ˜çš„æ¡†æ¶ã€‚å®ƒç»“åˆäº† 3D å‡ ä½•ç¼“å­˜å’Œæ—¶ç©ºæ‰©æ•£æ¨¡å‹ï¼Œç¡®ä¿ç»“æ„ä¸€è‡´æ€§å’Œçº¹ç†ç»†åŒ–ã€‚é€šè¿‡é«˜æ–¯å–·æº…æŠ€æœ¯æ„å»ºçš„åœ¨çº¿ 3D å‡ ä½•ç¼“å­˜ï¼Œèƒ½å¤Ÿå°†å†å²å†…å®¹æ‰­æ›²åˆ°æ–°è§†è§’ï¼Œä»è€Œä¿æŒå‡ ä½•ç»“æ„ã€‚è¯¥æ¡†æ¶çš„åˆ›æ–°ä¹‹å¤„åœ¨äºé‡‡ç”¨æ—¶ç©ºå˜åŒ–çš„å™ªå£°è°ƒåº¦ï¼ŒåŠ¨æ€æ›´æ–° 3D ç¼“å­˜ï¼Œä»¥å®ç°è§†é¢‘ç‰‡æ®µä¹‹é—´çš„ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17385",
            "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
            "url": "https://huggingface.co/papers/2512.17385",
            "abstract": "IPC is an unsupervised framework that uses internal probing of large language models to generate code without labeled datasets, achieving competitive performance with reduced resource dependency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.",
            "score": 12,
            "issue_id": 194,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "5278dcaf16d288a0",
            "authors": [
                "Jiajun Wu",
                "Jian Yang",
                "Wei Zhang",
                "Lin Jing",
                "Yuqing Ma",
                "Ensheng Shi",
                "Yuchi Ma",
                "Zhoujun Li",
                "Xianglong Liu"
            ],
            "affiliations": [
                "Beihang University",
                "Huawei"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17385.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ IPC â€” Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº: Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ UCoder â€” ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ñ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Unleashing Code Generation: IPC's Unsupervised Approach",
                    "desc": "The paper presents IPC, an unsupervised framework that enables code generation using large language models (LLMs) without the need for labeled datasets. It explores various probing techniques to extract internal knowledge and confidence patterns from LLMs, allowing for effective code generation. By employing self-consistency mechanisms and representation-based quality estimation, IPC identifies reliable code candidates and trains a model called UCoder. The results show that IPC can achieve competitive performance in code generation tasks while significantly reducing reliance on labeled data and computational resources."
                },
                "zh": {
                    "title": "æ— ç›‘ç£å­¦ä¹ ï¼šä»£ç ç”Ÿæˆçš„æ–°æ–¹å‘",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºIPCçš„æ— ç›‘ç£æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…éƒ¨æ¢æµ‹æ¥ç”Ÿæˆä»£ç ï¼Œè€Œæ— éœ€ä¾èµ–æ ‡è®°æ•°æ®é›†ã€‚è¯¥æ–¹æ³•é€šè¿‡é—®é¢˜ç©ºé—´æ¢æµ‹ã€ç†è§£æ¢æµ‹ã€è§£å†³æ–¹æ¡ˆç©ºé—´æ¢æµ‹å’ŒçŸ¥è¯†å·©å›ºç­‰æŠ€æœ¯ï¼ŒæŒ–æ˜æ¨¡å‹å†…éƒ¨çš„çŸ¥è¯†å’Œä¿¡å¿ƒæ¨¡å¼ã€‚IPCé€šè¿‡è‡ªä¸€è‡´æ€§æœºåˆ¶å’ŒåŸºäºè¡¨ç¤ºçš„è´¨é‡è¯„ä¼°æ¥è¯†åˆ«å¯é çš„ä»£ç å€™é€‰ï¼Œä»è€Œè®­ç»ƒæ— ç›‘ç£å­¦ä¹ çš„ç¼–ç å™¨UCoderã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIPCåœ¨å¤šä¸ªä»£ç åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸ç›‘ç£æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å¯¹æ ‡è®°æ•°æ®å’Œè®¡ç®—èµ„æºçš„ä¾èµ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19682",
            "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
            "url": "https://huggingface.co/papers/2512.19682",
            "abstract": "GenEnv, a framework using a co-evolutionary game with a generative environment simulator, enhances LLM agent performance by 40.3% over 7B baselines and uses less data than offline augmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective Î±-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to +40.3\\% over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3times less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
            "score": 9,
            "issue_id": 192,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "db555e252bf52944",
            "authors": [
                "Jiacheng Guo",
                "Ling Yang",
                "Peter Chen",
                "Qixin Xiao",
                "Yinjie Wang",
                "Xinzhe Juan",
                "Jiahao Qiu",
                "Ke Shen",
                "Mengdi Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Columbia University",
                "Princeton University",
                "University of Chicago",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19682.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#small_models",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ curriculum learning Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¸Ğ³Ñ€Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°",
                    "desc": "GenEnv â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¸Ğ³Ñ€Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ curriculum learning, Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğº Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¼Ñƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ»ÑŒÑ„Ğ°-Curriculum Reward Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 40.3% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ 7B baseline Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² 3.3 Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "GenEnv: Adaptive Learning for Enhanced LLM Performance",
                    "desc": "GenEnv is a novel framework that enhances the performance of Large Language Model (LLM) agents by utilizing a co-evolutionary game with a generative environment simulator. This approach allows the simulator to create dynamic tasks that adapt to the agent's current skill level, promoting efficient learning. By implementing an Î±-Curriculum Reward, GenEnv aligns task difficulty with the agent's capabilities, leading to significant performance improvements. The framework demonstrates a 40.3% increase in performance over 7B baselines while using 3.3 times less data compared to traditional offline augmentation methods."
                },
                "zh": {
                    "title": "GenEnvï¼šåŠ¨æ€ç”Ÿæˆç¯å¢ƒæå‡LLMæ€§èƒ½çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "GenEnvæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡ä¸ç”Ÿæˆç¯å¢ƒæ¨¡æ‹Ÿå™¨çš„å…±åŒè¿›åŒ–æ¸¸æˆï¼Œæ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒGenEnvä½¿ç”¨åŠ¨æ€çš„è¯¾ç¨‹ç­–ç•¥ï¼ŒæŒç»­ç”Ÿæˆä¸ä»£ç†èƒ½åŠ›ç›¸åŒ¹é…çš„ä»»åŠ¡ï¼Œä»è€Œå®ç°æ•°æ®çš„åŠ¨æ€æ¼”å˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç®€å•æœ‰æ•ˆçš„Î±-è¯¾ç¨‹å¥–åŠ±æœºåˆ¶ï¼Œç¡®ä¿ä»»åŠ¡éš¾åº¦ä¸ä»£ç†çš„å½“å‰èƒ½åŠ›ç›¸ä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenEnvåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä»£ç†æ€§èƒ½æé«˜äº†40.3%ï¼Œå¹¶ä¸”ä½¿ç”¨çš„æ•°æ®é‡æ¯”ç¦»çº¿å¢å¼ºæ–¹æ³•å°‘3.3å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19629",
            "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
            "url": "https://huggingface.co/papers/2512.19629",
            "abstract": "LoGoPlanner is an end-to-end navigation framework that improves trajectory planning in unstructured environments by integrating localization, scene geometry reconstruction, and policy conditioning.  \t\t\t\t\tAI-generated summary \t\t\t\t Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io/{project page}.",
            "score": 8,
            "issue_id": 194,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "07e1fc293c9f3d5e",
            "authors": [
                "Jiaqi Peng",
                "Wenzhe Cai",
                "Yuqiang Yang",
                "Tai Wang",
                "Yuan Shen",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "OpenGVLab",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19629.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#agents",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹",
                    "desc": "LoGoPlanner Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞºĞ²Ğ¾Ğ·Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½ĞµÑ†-Ğ²-ĞºĞ¾Ğ½ĞµÑ† Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 27.3% Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Robot Navigation with LoGoPlanner",
                    "desc": "LoGoPlanner is a novel navigation framework designed for mobile robots operating in unstructured environments. It integrates localization, scene geometry reconstruction, and policy conditioning into a single end-to-end system, which helps to minimize errors and improve efficiency. By fine-tuning a visual-geometry backbone and reconstructing scene geometry, LoGoPlanner enhances obstacle avoidance and provides accurate localization without relying on separate modules. The framework has shown significant performance improvements in both simulations and real-world tests, outperforming traditional methods by over 27.3%."
                },
                "zh": {
                    "title": "LoGoPlannerï¼šæå‡éç»“æ„åŒ–ç¯å¢ƒä¸­çš„å¯¼èˆªèƒ½åŠ›",
                    "desc": "LoGoPlanner æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¯¼èˆªæ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­çš„è½¨è¿¹è§„åˆ’ã€‚å®ƒé€šè¿‡æ•´åˆå®šä½ã€åœºæ™¯å‡ ä½•é‡å»ºå’Œç­–ç•¥æ¡ä»¶åŒ–ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å—åŒ–ç®¡é“ä¸­çš„å»¶è¿Ÿå’Œé”™è¯¯ä¼ æ’­é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒè§†è§‰å‡ ä½•éª¨å¹²ç½‘ç»œï¼Œå®ç°äº†å‡†ç¡®çš„å®šä½å’Œç¯å¢ƒæ„ŸçŸ¥ï¼Œä»è€Œæé«˜äº†éšœç¢ç‰©è§„é¿çš„å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLoGoPlanner åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†è§„åˆ’çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.17206",
            "title": "Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs",
            "url": "https://huggingface.co/papers/2512.17206",
            "abstract": "Reasoning Palette enhances large language models by using a latent-modulation framework to guide internal planning and improve both inference and reinforcement learning performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Exploration capacity shapes both inference-time performance and reinforcement learning (RL) training for large (vision-) language models, as stochastic sampling often yields redundant reasoning paths with little high-level diversity. This paper proposes Reasoning Palette, a novel latent-modulation framework that endows the model with a stochastic latent variable for strategic contextualization, guiding its internal planning prior to token generation. This latent context is inferred from the mean-pooled embedding of a question-answer pair via a variational autoencoder (VAE), where each sampled latent potentially encodes a distinct reasoning context. During inference, a sampled latent is decoded into learnable token prefixes and prepended to the input prompt, modulating the model's internal reasoning trajectory. In this way, the model performs internal sampling over reasoning strategies prior to output generation, which shapes the style and structure of the entire response sequence. A brief supervised fine-tuning (SFT) warm-up phase allows the model to adapt to this latent conditioning. Within RL optimization, Reasoning Palette facilitates structured exploration by enabling on-demand injection for diverse reasoning modes, significantly enhancing exploration efficiency and sustained learning capability. Experiments across multiple reasoning benchmarks demonstrate that our method enables interpretable and controllable control over the (vision-) language model's strategic behavior, thereby achieving consistent performance gains over standard RL methods.",
            "score": 8,
            "issue_id": 193,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "2f0c4cbd2ba00bdf",
            "authors": [
                "Rujiao Long",
                "Yang Li",
                "Xingyao Zhang",
                "Weixun Wang",
                "Tianqianjin Lin",
                "Xi Zhao",
                "Yuchi Xu",
                "Wenbo Su",
                "Junchi Yan",
                "Bo Zheng"
            ],
            "affiliations": [
                "Alibaba Group",
                "Shanghai Jiao Tong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.17206.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#rlhf",
                    "#rl",
                    "#interpretability",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞŸĞ°Ğ»Ğ¸Ñ‚Ñ€Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Reasoning Palette, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ, Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑÑ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ğ²ÑÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Reasoning Diversity in Language Models",
                    "desc": "The paper introduces Reasoning Palette, a framework that improves large language models by using a latent-modulation approach. This method incorporates a stochastic latent variable that helps the model plan its reasoning before generating responses. By utilizing a variational autoencoder, the model can sample different reasoning contexts, which enhances its ability to explore diverse strategies during inference and reinforcement learning. The results show that this framework leads to better performance and more interpretable behavior in language models compared to traditional reinforcement learning techniques."
                },
                "zh": {
                    "title": "æå‡æ¨ç†èƒ½åŠ›çš„æ½œåœ¨è°ƒåˆ¶æ¡†æ¶",
                    "desc": "Reasoning Palette æ˜¯ä¸€ç§æ–°é¢–çš„æ½œåœ¨è°ƒåˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡å¼•å…¥éšæœºæ½œåœ¨å˜é‡æ¥æŒ‡å¯¼æ¨¡å‹çš„å†…éƒ¨è§„åˆ’ï¼Œä»è€Œæ”¹å–„æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä»é—®ç­”å¯¹çš„å‡å€¼æ± åµŒå…¥ä¸­æ¨æ–­æ½œåœ¨ä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©æ¨¡å‹åœ¨ç”Ÿæˆè¾“å‡ºä¹‹å‰è¿›è¡Œå†…éƒ¨é‡‡æ ·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasoning Palette èƒ½å¤Ÿæé«˜æ¨¡å‹çš„æ¢ç´¢æ•ˆç‡å’Œå­¦ä¹ èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19539",
            "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
            "url": "https://huggingface.co/papers/2512.19539",
            "abstract": "StoryMem enhances multi-shot video generation with cinematic quality and long-range consistency using a memory bank and pre-trained single-shot video diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.",
            "score": 4,
            "issue_id": 192,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "afe7f52e42f30a9f",
            "authors": [
                "Kaiwen Zhang",
                "Liming Jiang",
                "Angtian Wang",
                "Jacob Zhiyuan Fang",
                "Tiancheng Zhi",
                "Qing Yan",
                "Hao Kang",
                "Xin Lu",
                "Xingang Pan"
            ],
            "affiliations": [
                "Intelligent Creation, ByteDance",
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19539.jpg",
            "data": {
                "categories": [
                    "#story_generation",
                    "#diffusion",
                    "#benchmark",
                    "#video",
                    "#dataset",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸĞ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ°ÑÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "StoryMem â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ°Ğ½Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ°ÑÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. Ğ˜Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Memory-to-Video Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ±Ğ°Ğ½Ğº ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ÑÑ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ RoPE ÑĞ´Ğ²Ğ¸Ğ³Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LoRA Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Transforming Video Storytelling with Memory-Enhanced Generation",
                    "desc": "StoryMem is a new approach for creating multi-shot videos that look cinematic and maintain consistency over long durations. It uses a memory bank to store important frames from previous shots, which helps in generating new shots that are coherent with the story. By combining this memory with single-shot video diffusion models, StoryMem can produce high-quality videos that transition smoothly between scenes. The framework also includes a method for selecting keyframes and filtering for aesthetic quality, ensuring that the generated videos are both informative and visually appealing."
                },
                "zh": {
                    "title": "StoryMemï¼šæå‡å¤šé•œå¤´è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§ä¸è´¨é‡",
                    "desc": "StoryMem æ˜¯ä¸€ç§å¢å¼ºå¤šé•œå¤´è§†é¢‘ç”Ÿæˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°ç”µå½±è´¨é‡å’Œé•¿è·ç¦»ä¸€è‡´æ€§ã€‚å®ƒé€šè¿‡ä¸€ä¸ªè®°å¿†åº“å’Œé¢„è®­ç»ƒçš„å•é•œå¤´è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå°†é•¿ç¯‡è§†é¢‘å™äº‹é‡æ–°æ„é€ æˆè¿­ä»£é•œå¤´åˆæˆã€‚è¯¥æ–¹æ³•ä½¿ç”¨äº†ä¸€ç§æ–°é¢–çš„è®°å¿†åˆ°è§†é¢‘ï¼ˆM2Vï¼‰è®¾è®¡ï¼ŒåŠ¨æ€æ›´æ–°å†å²ç”Ÿæˆé•œå¤´çš„å…³é”®å¸§è®°å¿†ã€‚å®éªŒè¡¨æ˜ï¼ŒStoryMem åœ¨è·¨é•œå¤´ä¸€è‡´æ€§å’Œç¾å­¦è´¨é‡æ–¹é¢ä¼˜äºä»¥å¾€çš„æ–¹æ³•ï¼Œæ¨åŠ¨äº†è¿è´¯çš„é•¿è§†é¢‘å™äº‹çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.18003",
            "title": "Name That Part: 3D Part Segmentation and Naming",
            "url": "https://huggingface.co/papers/2512.18003",
            "abstract": "ALIGN-Parts addresses semantic 3D part segmentation by aligning implicit 3D part representations with part descriptions using geometric, appearance, and semantic cues, supporting open-vocabulary part naming and creating a unified ontology for multiple datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.",
            "score": 3,
            "issue_id": 192,
            "pub_date": "2025-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "7796f8dea8db72a1",
            "authors": [
                "Soumava Paul",
                "Prakhar Kaushik",
                "Ankit Vaidya",
                "Anand Bhattad",
                "Alan Yuille"
            ],
            "affiliations": [
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.18003.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#3d",
                    "#interpretability",
                    "#multimodal"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ALIGN-Parts â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ¼Ñ‘Ğ½. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ 3D-Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ (Ğ¿Ğ°Ñ€Ñ‚Ğ»ĞµÑ‚Ñ‹) Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ¸Ğ½Ğ³-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¾Ğ½Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing 3D Part Segmentation with ALIGN-Parts",
                    "desc": "ALIGN-Parts is a method for semantic 3D part segmentation, which means it helps break down 3D objects into meaningful parts with names. It tackles the problem of inconsistent part definitions across different datasets, which makes training models difficult. The approach uses a combination of geometric information, visual features, and semantic descriptions to align 3D part representations with their names. This allows for flexible part naming and creates a unified system for categorizing parts across various datasets, enhancing the model's ability to recognize and name parts accurately."
                },
                "zh": {
                    "title": "ALIGN-Partsï¼šç»Ÿä¸€3Déƒ¨ä»¶åˆ†å‰²ä¸å‘½åçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "ALIGN-Parts è®ºæ–‡æå‡ºäº†ä¸€ç§è¯­ä¹‰3Déƒ¨ä»¶åˆ†å‰²çš„æ–¹æ³•ï¼Œé€šè¿‡å‡ ä½•ã€å¤–è§‚å’Œè¯­ä¹‰çº¿ç´¢å°†éšå¼3Déƒ¨ä»¶è¡¨ç¤ºä¸éƒ¨ä»¶æè¿°å¯¹é½ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰æ•°æ®é›†ä¸­éƒ¨ä»¶æ³¨é‡Šä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæ”¯æŒå¼€æ”¾è¯æ±‡çš„éƒ¨ä»¶å‘½åã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†éƒ¨ä»¶å‘½åè§†ä¸ºç›´æ¥çš„é›†åˆå¯¹é½ä»»åŠ¡ï¼Œåˆ©ç”¨åŒå‘åˆ†é…å°†å½¢çŠ¶åˆ†è§£ä¸ºéƒ¨ä»¶å°å—ï¼Œå¹¶ä¸éƒ¨ä»¶æè¿°åŒ¹é…ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æœ¬ä½“ï¼Œæ•´åˆäº†å¤šä¸ªæ•°æ®é›†çš„éƒ¨ä»¶ä¿¡æ¯ï¼Œæ”¯æŒé›¶æ ·æœ¬åŒ¹é…å’Œç½®ä¿¡åº¦æ ¡å‡†çš„é¢„æµ‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19432",
            "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
            "url": "https://huggingface.co/papers/2512.19432",
            "abstract": "MobileWorld, a more challenging benchmark than AndroidWorld, includes diverse real-world mobile tasks and interactions, revealing significant gaps in current model capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.",
            "score": 2,
            "issue_id": 192,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "81607d027cddfd45",
            "authors": [
                "Quyu Kong",
                "Xu Zhang",
                "Zhenyu Yang",
                "Nolan Gao",
                "Chen Liu",
                "Panrong Tong",
                "Chenglin Cai",
                "Hanzhang Zhou",
                "Jianan Zhang",
                "Liangyu Chen",
                "Zhidan Liu",
                "Steven Hoi",
                "Yue Wang"
            ],
            "affiliations": [
                "HKUST (GZ)",
                "Tongyi Lab, Alibaba Group",
                "University of Florida"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19432.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#games",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MobileWorld Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ AndroidWorld. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 201 Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ² 20 Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ agentĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ MCP-Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 51.7% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²."
                },
                "en": {
                    "title": "MobileWorld: Elevating Mobile AI Benchmarking to Real-World Challenges",
                    "desc": "MobileWorld is a new benchmark designed to challenge existing mobile AI models by simulating real-world tasks and interactions more accurately than AndroidWorld. It includes 201 tasks across 20 applications, focusing on long-horizon tasks and cross-application interactions, which are essential for realistic mobile usage. The benchmark introduces novel task categories that go beyond simple GUI manipulation, highlighting the need for improved user interaction capabilities. Results show that current models perform poorly on this benchmark, indicating significant gaps in their ability to handle complex mobile tasks and interactions."
                },
                "zh": {
                    "title": "ç§»åŠ¨æ™ºèƒ½çš„æ–°æŒ‘æˆ˜ï¼šMobileWorldåŸºå‡†æµ‹è¯•",
                    "desc": "MobileWorldæ˜¯ä¸€ä¸ªæ¯”AndroidWorldæ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ›´å¥½åœ°åæ˜ çœŸå®çš„ç§»åŠ¨ä½¿ç”¨åœºæ™¯ã€‚å®ƒåŒ…å«201ä¸ªä»»åŠ¡ï¼Œæ¶µç›–20ä¸ªåº”ç”¨ç¨‹åºï¼Œå¼ºè°ƒè·¨åº”ç”¨çš„é•¿æ—¶é—´ä»»åŠ¡å’Œå¤šåº”ç”¨ä»»åŠ¡ã€‚ä¸AndroidWorldç›¸æ¯”ï¼ŒMobileWorldçš„ä»»åŠ¡å®Œæˆæ­¥éª¤å‡ ä¹ç¿»å€ï¼Œå¹¶å¼•å…¥äº†æ–°çš„ä»»åŠ¡ç±»åˆ«ï¼Œå¦‚ä»£ç†-ç”¨æˆ·äº¤äº’å’ŒMCPå¢å¼ºä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨ç”¨æˆ·äº¤äº’å’ŒMCPè°ƒç”¨æ–¹é¢è¡¨ç°ä¸ä½³ï¼ŒæŒ‡æ˜äº†æœªæ¥ç§»åŠ¨æ™ºèƒ½å‘å±•çš„æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19402",
            "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
            "url": "https://huggingface.co/papers/2512.19402",
            "abstract": "A framework called Real2Edit2Real generates new manipulation demonstrations by using 3D reconstruction, editing, and video synthesis, improving data efficiency in robot learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.",
            "score": 2,
            "issue_id": 193,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "1447bb035b98c75e",
            "authors": [
                "Yujie Zhao",
                "Hongwei Fan",
                "Di Chen",
                "Shengcong Chen",
                "Liliang Chen",
                "Xiaoqi Li",
                "Guanghui Ren",
                "Hao Dong"
            ],
            "affiliations": [
                "AgiBot",
                "CFCS, School of Computer Science, Peking University",
                "PKU-AgiBot Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19402.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#training",
                    "#video",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· 3D Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ¸Ğ½Ñ‚ĞµĞ·",
                    "desc": "Real2Edit2Real â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· RGB-Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ·Ñ‹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ñ‹, Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 1-5 Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 50 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 10-50 Ñ€Ğ°Ğ·. Ğ“Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Robot Learning with Efficient Data Generation",
                    "desc": "The paper introduces Real2Edit2Real, a novel framework designed to enhance robot learning by generating new manipulation demonstrations. It utilizes 3D reconstruction to create a detailed geometric model from 2D images, allowing for depth-reliable editing of point clouds. This enables the generation of new manipulation trajectories while ensuring that robot poses remain physically consistent. The framework significantly improves data efficiency, allowing policies trained on a few demonstrations to perform as well as those trained on many, showcasing its potential for broad applications in robot learning."
                },
                "zh": {
                    "title": "æå‡æœºå™¨äººå­¦ä¹ çš„æ•°æ®æ•ˆç‡",
                    "desc": "Real2Edit2Realæ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œé€šè¿‡3Dé‡å»ºã€ç¼–è¾‘å’Œè§†é¢‘åˆæˆç”Ÿæˆæ–°çš„æ“ä½œæ¼”ç¤ºï¼Œæå‡äº†æœºå™¨äººå­¦ä¹ ä¸­çš„æ•°æ®æ•ˆç‡ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»å¤šè§†è§’RGBè§‚å¯Ÿä¸­é‡å»ºåœºæ™¯å‡ ä½•ï¼Œç„¶ååœ¨ç‚¹äº‘ä¸Šè¿›è¡Œæ·±åº¦å¯é çš„3Dç¼–è¾‘ï¼Œç”Ÿæˆæ–°çš„æ“ä½œè½¨è¿¹ã€‚æ¥ç€ï¼Œåˆ©ç”¨æ·±åº¦ä½œä¸ºä¸»è¦æ§åˆ¶ä¿¡å·ï¼Œç»“åˆåŠ¨ä½œã€è¾¹ç¼˜å’Œå…‰çº¿å›¾ï¼Œåˆæˆç©ºé—´å¢å¼ºçš„å¤šè§†è§’æ“ä½œè§†é¢‘ã€‚å®éªŒè¡¨æ˜ï¼Œä»…ç”¨1-5ä¸ªæºæ¼”ç¤ºç”Ÿæˆçš„æ•°æ®ï¼Œè®­ç»ƒå‡ºçš„ç­–ç•¥å¯ä»¥ä¸ä½¿ç”¨50ä¸ªçœŸå®æ¼”ç¤ºçš„æ•°æ®ç›¸åª²ç¾ï¼Œæ•°æ®æ•ˆç‡æé«˜äº†10-50å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.19399",
            "title": "Brain-Grounded Axes for Reading and Steering LLM States",
            "url": "https://huggingface.co/papers/2512.19399",
            "abstract": "Neurophysiological brain activity is used to create interpretable axes for large language models, enhancing their controllability and interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
            "score": 1,
            "issue_id": 195,
            "pub_date": "2025-12-22",
            "pub_date_card": {
                "ru": "22 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 22",
                "zh": "12æœˆ22æ—¥"
            },
            "hash": "11544f5b5e821595",
            "authors": [
                "Sandro Andric"
            ],
            "affiliations": [
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.19399.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#architecture",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ¾Ğ·Ğ³ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°Ñ: ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¹Ñ€Ğ¾Ñ„Ğ¸Ğ·Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ Ğ°Ñ‚Ğ»Ğ°Ñ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MEG, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ»Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¾ÑĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑĞ²ÑĞ·Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ñ„Ğ¸Ğ·Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾ÑÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ·Ğ³Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ (Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° ÑĞ»Ğ¾Ğ², Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ»Ğ°ÑÑ) Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ LLM Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ñ„Ğ¸Ğ·Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¹ Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Harnessing Brain Activity for Interpretable Language Models",
                    "desc": "This paper introduces a novel approach to enhance the interpretability and controllability of large language models (LLMs) by using neurophysiological brain activity as a framework. Instead of relying solely on textual data, the authors create a brain atlas that maps brain activity patterns to word-level representations, allowing for the extraction of meaningful latent axes. They validate these axes through various linguistic checks and demonstrate that steering LLMs along these brain-derived directions leads to consistent and interpretable outcomes. This method provides a new interface for understanding LLM behavior, bridging the gap between human cognition and machine learning."
                },
                "zh": {
                    "title": "ç¥ç»ç”Ÿç†å­¦é©±åŠ¨çš„å¯è§£é‡Šæ€§ä¸å¯æ§æ€§",
                    "desc": "æœ¬ç ”ç©¶åˆ©ç”¨ç¥ç»ç”Ÿç†å­¦çš„è„‘æ´»åŠ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ›å»ºå¯è§£é‡Šçš„åæ ‡è½´ï¼Œä»è€Œå¢å¼ºå…¶å¯æ§æ€§å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨äººç±»è„‘æ´»åŠ¨ä½œä¸ºåæ ‡ç³»ç»Ÿï¼Œè€Œä¸æ˜¯è®­ç»ƒä¿¡å·ï¼Œä»¥è¯»å–å’Œå¼•å¯¼LLMçŠ¶æ€ã€‚é€šè¿‡æ„å»ºåŸºäºç›¸ä½é”å®šå€¼ï¼ˆPLVï¼‰æ¨¡å¼çš„è¯çº§è„‘å›¾è°±ï¼Œå¹¶æå–æ½œåœ¨è½´ï¼Œæˆ‘ä»¬éªŒè¯äº†è¿™äº›è½´çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è®­ç»ƒè½»é‡çº§é€‚é…å™¨å°†LLMçš„éšè—çŠ¶æ€æ˜ å°„åˆ°è¿™äº›è„‘è½´ä¸Šã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºè„‘çš„æ–¹å‘èƒ½å¤Ÿæä¾›ç¨³å®šä¸”å¯è§£é‡Šçš„æ§åˆ¶æ‰‹æ®µï¼Œæ”¹å–„LLMçš„è¡Œä¸ºç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.12620",
            "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives",
            "url": "https://huggingface.co/papers/2512.12620",
            "abstract": "Research explores syllogistic reasoning in LLMs, examining their symbolic inference and natural language understanding capabilities, with some models showing perfect symbolic performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.",
            "score": 1,
            "issue_id": 195,
            "pub_date": "2025-12-14",
            "pub_date_card": {
                "ru": "14 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 14",
                "zh": "12æœˆ14æ—¥"
            },
            "hash": "dbf457880d70429f",
            "authors": [
                "Aheli Poddar",
                "Saptarshi Sahoo",
                "Sujata Ghosh"
            ],
            "affiliations": [
                "Indian Statistical Institute, Chennai",
                "Institute of Engineering & Management, Kolkata"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.12620.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ†Ğ¸Ğ¸ Ğº Ğ»Ğ¾Ğ³Ğ¸ĞºĞµ: ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ»Ğ¸ LLM Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸?",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº ÑĞ¸Ğ»Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 14 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ½Ğ° ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ‚Ğ°ĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ° Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ ÑÑ€ĞµĞ´Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ° Ğ¼Ñ‹ÑĞ»ÑŒ, Ñ‡Ñ‚Ğ¾ LLM Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑĞºĞ¾Ñ€ĞµĞµ ĞºĞ°Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ñ‡ĞµĞ¼ ĞºĞ°Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ½ÑĞ°Ğ½ÑĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Exploring Logical Reasoning in Large Language Models",
                    "desc": "This research investigates how large language models (LLMs) perform syllogistic reasoning, which is a form of logical deduction. The study evaluates 14 different LLMs to understand their abilities in both symbolic inference and natural language comprehension. Some models demonstrate perfect performance in symbolic reasoning, suggesting that they may be evolving into more formal reasoning systems. However, the variability in performance across models raises questions about their ability to capture the complexities of human reasoning."
                },
                "zh": {
                    "title": "æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸‰æ®µè®ºæ¨ç†æ–¹é¢çš„è¡¨ç°ï¼Œåˆ†æäº†å®ƒä»¬çš„ç¬¦å·æ¨ç†å’Œè‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨äº†14ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯„ä¼°å®ƒä»¬åœ¨ç¬¦å·æ¨ç†å’Œè‡ªç„¶è¯­è¨€ç†è§£ä¸­çš„è¡¨ç°ã€‚å°½ç®¡è¿™ç§æ¨ç†æœºåˆ¶åœ¨ä¸åŒçš„LLMsä¸­å¹¶ä¸å‡åŒ€å‡ºç°ï¼Œä½†æŸäº›æ¨¡å‹å±•ç°å‡ºçš„å®Œç¾ç¬¦å·è¡¨ç°å¼•å‘äº†æˆ‘ä»¬å¯¹LLMsæ˜¯å¦æ­£åœ¨æˆä¸ºæ›´æ­£å¼çš„æ¨ç†æœºåˆ¶çš„æ€è€ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶æ–¹å‘æ—¨åœ¨æ·±å…¥ç†è§£LLMsçš„åŸºæœ¬æ¨ç†èƒ½åŠ›åŠå…¶æœªæ¥å‘å±•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-22.html",
    "link_next": "2025-12-24.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "22.12",
        "en": "12/22",
        "zh": "12æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "24.12",
        "en": "12/24",
        "zh": "12æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 4,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 1,
        "#inference": 0,
        "#3d": 4,
        "#audio": 0,
        "#video": 5,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 2,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 4,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 0
    }
}