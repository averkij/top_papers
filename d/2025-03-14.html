
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. March 14.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">14 –º–∞—Ä—Ç–∞</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-13.html">‚¨ÖÔ∏è <span id="prev-date">13.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-17.html">‚û°Ô∏è <span id="next-date">17.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '14 –º–∞—Ä—Ç–∞', 'en': 'March 14', 'zh': '3Êúà14Êó•'};
        let feedDateNext = {'ru': '17.03', 'en': '03/17', 'zh': '3Êúà17Êó•'};
        let feedDatePrev = {'ru': '13.03', 'en': '03/13', 'zh': '3Êúà13Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.09669', 'title': 'Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2503.09669', 'abstract': 'Text-to-image diffusion models have achieved remarkable success in generating high-quality contents from text prompts. However, their reliance on publicly available data and the growing trend of data sharing for fine-tuning make these models particularly vulnerable to data poisoning attacks. In this work, we introduce the Silent Branding Attack, a novel data poisoning method that manipulates text-to-image diffusion models to generate images containing specific brand logos or symbols without any text triggers. We find that when certain visual patterns are repeatedly in the training data, the model learns to reproduce them naturally in its outputs, even without prompt mentions. Leveraging this, we develop an automated data poisoning algorithm that unobtrusively injects logos into original images, ensuring they blend naturally and remain undetected. Models trained on this poisoned dataset generate images containing logos without degrading image quality or text alignment. We experimentally validate our silent branding attack across two realistic settings on large-scale high-quality image datasets and style personalization datasets, achieving high success rates even without a specific text trigger. Human evaluation and quantitative metrics including logo detection show that our method can stealthily embed logos.', 'score': 14, 'issue_id': 2701, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 –º–∞—Ä—Ç–∞', 'en': 'March 12', 'zh': '3Êúà12Êó•'}, 'hash': 'f54f510a4cf48d22', 'authors': ['Sangwon Jang', 'June Suk Choi', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.09669.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#security', '#diffusion', '#data'], 'emoji': 'üïµÔ∏è', 'ru': {'title': '–ù–µ–≤–∏–¥–∏–º—ã–µ –ª–æ–≥–æ—Ç–∏–ø—ã: —Å–∫—Ä—ã—Ç–æ–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏', 'desc': "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞—Ç–∞–∫–∏ –Ω–∞ –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—É—Ç–µ–º –æ—Ç—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π '–¢–∏—Ö–∞—è –±—Ä–µ–Ω–¥–∏–Ω–≥–æ–≤–∞—è –∞—Ç–∞–∫–∞', –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –ª–æ–≥–æ—Ç–∏–ø–∞–º–∏ –±–µ–∑ —è–≤–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–ª–≥–æ—Ä–∏—Ç–º, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ–∑–∞–º–µ—Ç–Ω–æ –≤–Ω–µ–¥—Ä—è–µ—Ç –ª–æ–≥–æ—Ç–∏–ø—ã –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –Ω–∞ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –∫–∞–∫ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π, —Ç–∞–∫ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏."}, 'en': {'title': 'Stealthy Logo Injection in Image Generation', 'desc': "This paper presents a new method called the Silent Branding Attack, which targets text-to-image diffusion models by subtly injecting brand logos into training data. The attack exploits the model's tendency to reproduce visual patterns it has seen, allowing logos to appear in generated images without any explicit text prompts. The authors developed an automated algorithm that seamlessly integrates these logos into original images, making them difficult to detect. Their experiments demonstrate that models trained on this manipulated data can produce high-quality images containing logos, achieving significant success rates in various settings."}, 'zh': {'title': 'ÈùôÈªòÂìÅÁâåÊîªÂáªÔºöÈöêÁßòÊ§çÂÖ•ÂìÅÁâåÊ†áÂøóÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'ÊñáÊú¨Âà∞ÂõæÂÉèÁöÑÊâ©Êï£Ê®°ÂûãÂú®Ê†πÊçÆÊñáÊú¨ÊèêÁ§∫ÁîüÊàêÈ´òË¥®ÈáèÂÜÖÂÆπÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÊàêÂäü„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°Âûã‰æùËµñ‰∫éÂÖ¨ÂºÄÊï∞ÊçÆÔºåÂπ∂‰∏îÊï∞ÊçÆÂÖ±‰∫´ÁöÑË∂ãÂäø‰ΩøÂÖ∂ÁâπÂà´ÂÆπÊòìÂèóÂà∞Êï∞ÊçÆ‰∏≠ÊØíÊîªÂáª„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆ‰∏≠ÊØíÊñπÊ≥ï‚Äî‚ÄîÈùôÈªòÂìÅÁâåÊîªÂáªÔºåËÉΩÂ§üÊìçÊéßÊñáÊú¨Âà∞ÂõæÂÉèÁöÑÊâ©Êï£Ê®°ÂûãÁîüÊàêÂåÖÂê´ÁâπÂÆöÂìÅÁâåÊ†áÂøóÊàñÁ¨¶Âè∑ÁöÑÂõæÂÉèÔºåËÄåÊó†ÈúÄ‰ªª‰ΩïÊñáÊú¨Ëß¶Âèë„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçËá™Âä®ÂåñÁöÑÊï∞ÊçÆ‰∏≠ÊØíÁÆóÊ≥ïÔºåÂèØ‰ª•Âú®ÂéüÂßãÂõæÂÉè‰∏≠ÊÇÑÊó†Â£∞ÊÅØÂú∞Ê≥®ÂÖ•Ê†áÂøóÔºåÁ°Æ‰øùÂÆÉ‰ª¨Ëá™ÁÑ∂ËûçÂêà‰∏î‰∏çË¢´Ê£ÄÊµã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10480', 'title': 'World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning', 'url': 'https://huggingface.co/papers/2503.10480', 'abstract': 'Recent advances in large vision-language models (LVLMs) have shown promise for embodied task planning, yet they struggle with fundamental challenges like dependency constraints and efficiency. Existing approaches either solely optimize action selection or leverage world models during inference, overlooking the benefits of learning to model the world as a way to enhance planning capabilities. We propose Dual Preference Optimization (D^2PO), a new learning framework that jointly optimizes state prediction and action selection through preference learning, enabling LVLMs to understand environment dynamics for better planning. To automatically collect trajectories and stepwise preference data without human annotation, we introduce a tree search mechanism for extensive exploration via trial-and-error. Extensive experiments on VoTa-Bench demonstrate that our D^2PO-based method significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B), achieving superior task success rates with more efficient execution paths.', 'score': 11, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': '2d3e6b8c84c51e69', 'authors': ['Siyin Wang', 'Zhaoye Fei', 'Qinyuan Cheng', 'Shiduo Zhang', 'Panpan Cai', 'Jinlan Fu', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'National University of Singapore', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10480.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#agents', '#optimization', '#rl', '#training'], 'emoji': 'ü§ñ', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π —É LVLM —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ –≤—ã–±–æ—Ä–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LVLM) –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π –≤ –≤–æ–ø–ª–æ—â–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ Dual Preference Optimization (D^2PO) —Å–æ–≤–º–µ—Å—Ç–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –≤—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º. –î–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–∏—Å–∫–∞ –ø–æ –¥–µ—Ä–µ–≤—É. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ D^2PO –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π.'}, 'en': {'title': 'Enhancing Planning in LVLMs with Dual Preference Optimization', 'desc': 'This paper introduces Dual Preference Optimization (D^2PO), a novel framework designed to improve the planning capabilities of large vision-language models (LVLMs) by jointly optimizing state prediction and action selection. The approach addresses key challenges in embodied task planning, such as dependency constraints and efficiency, which have been inadequately handled by existing methods. By employing a tree search mechanism, D^2PO enables the automatic collection of trajectories and preference data, facilitating extensive exploration through trial-and-error without the need for human annotation. Experimental results on VoTa-Bench show that D^2PO significantly enhances task success rates and execution efficiency compared to previous methods and GPT-4o across various LVLMs.'}, 'zh': {'title': 'ÂèåÈáçÂÅèÂ•Ω‰ºòÂåñÔºöÊèêÂçá‰ªªÂä°ËßÑÂàíËÉΩÂäõÁöÑÂÖ≥ÈîÆ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁß∞‰∏∫ÂèåÈáçÂÅèÂ•Ω‰ºòÂåñÔºàD^2POÔºâÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂú®‰ªªÂä°ËßÑÂàí‰∏≠ÁöÑËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂÅèÂ•ΩÂ≠¶‰π†ÂêåÊó∂‰ºòÂåñÁä∂ÊÄÅÈ¢ÑÊµãÂíåÂä®‰ΩúÈÄâÊã©Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÁéØÂ¢ÉÂä®ÊÄÅ„ÄÇ‰∏∫‰∫ÜËá™Âä®Êî∂ÈõÜËΩ®ËøπÂíåÈÄêÊ≠•ÂÅèÂ•ΩÊï∞ÊçÆÔºåÊú¨ÊñáÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊ†ëÊêúÁ¥¢Êú∫Âà∂Ôºå‰ª•‰æøÈÄöËøáËØïÈîôËøõË°åÂπøÊ≥õÊé¢Á¥¢„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂü∫‰∫éD^2POÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÂíåGPT-4oÔºåËææÂà∞‰∫ÜÊõ¥È´òÁöÑ‰ªªÂä°ÊàêÂäüÁéáÂíåÊõ¥È´òÊïàÁöÑÊâßË°åË∑ØÂæÑ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10613', 'title': 'CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing', 'url': 'https://huggingface.co/papers/2503.10613', 'abstract': 'Text-to-image models like stable diffusion and DALLE-3 still struggle with multi-turn image editing. We decompose such a task as an agentic workflow (path) of tool use that addresses a sequence of subtasks by AI tools of varying costs. Conventional search algorithms require expensive exploration to find tool paths. While large language models (LLMs) possess prior knowledge of subtask planning, they may lack accurate estimations of capabilities and costs of tools to determine which to apply in each subtask. Can we combine the strengths of both LLMs and graph search to find cost-efficient tool paths? We propose a three-stage approach "CoSTA*" that leverages LLMs to create a subtask tree, which helps prune a graph of AI tools for the given task, and then conducts A* search on the small subgraph to find a tool path. To better balance the total cost and quality, CoSTA* combines both metrics of each tool on every subtask to guide the A* search. Each subtask\'s output is then evaluated by a vision-language model (VLM), where a failure will trigger an update of the tool\'s cost and quality on the subtask. Hence, the A* search can recover from failures quickly to explore other paths. Moreover, CoSTA* can automatically switch between modalities across subtasks for a better cost-quality trade-off. We build a novel benchmark of challenging multi-turn image editing, on which CoSTA* outperforms state-of-the-art image-editing models or agents in terms of both cost and quality, and performs versatile trade-offs upon user preference.', 'score': 6, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': '7f2d9ee971af97a8', 'authors': ['Advait Gupta', 'NandaKiran Velaga', 'Dang Nguyen', 'Tianyi Zhou'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2503.10613.jpg', 'data': {'categories': ['#games', '#benchmark', '#multimodal', '#agents', '#optimization'], 'emoji': 'üé®', 'ru': {'title': 'CoSTA*: –£–º–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò –∏ –ø–æ–∏—Å–∫–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ CoSTA* –¥–ª—è –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ—Ä–µ–≤–∞ –ø–æ–¥–∑–∞–¥–∞—á –∏ –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ A* –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ò–ò. CoSTA* –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞ –∏ –º–æ–∂–µ—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–µ—É–¥–∞—á–∞–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ CoSTA* –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –∞–≥–µ–Ω—Ç–∞–º–∏ –≤ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Optimizing Multi-Turn Image Editing with CoSTA*', 'desc': 'This paper addresses the challenges faced by text-to-image models in multi-turn image editing by introducing a new approach called CoSTA*. It combines large language models (LLMs) with graph search techniques to efficiently plan and execute a sequence of subtasks using AI tools. CoSTA* creates a subtask tree to streamline the selection of tools based on their costs and capabilities, and employs A* search to find optimal tool paths. The method also adapts to failures by updating tool metrics, allowing for quick recovery and improved cost-quality trade-offs in image editing tasks.'}, 'zh': {'title': 'È´òÊïàÁöÑÂ§öËΩÆÂõæÂÉèÁºñËæëÂ∑•ÂÖ∑Ë∑ØÂæÑ‰ºòÂåñ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫CoSTA*ÁöÑ‰∏âÈò∂ÊÆµÊñπÊ≥ïÔºåÁî®‰∫éËß£ÂÜ≥ÊñáÊú¨Âà∞ÂõæÂÉèÊ®°ÂûãÂú®Â§öËΩÆÂõæÂÉèÁºñËæë‰∏≠ÁöÑÊåëÊàò„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåÂõæÊêúÁ¥¢ÁöÑ‰ºòÁÇπÔºåÈÄöËøáÂàõÂª∫Â≠ê‰ªªÂä°Ê†ëÊù•‰ºòÂåñAIÂ∑•ÂÖ∑ÁöÑ‰ΩøÁî®Ë∑ØÂæÑ„ÄÇCoSTA*Âú®ÊØè‰∏™Â≠ê‰ªªÂä°‰∏≠ÁªºÂêàËÄÉËôëÂ∑•ÂÖ∑ÁöÑÊàêÊú¨ÂíåË¥®ÈáèÔºå‰ª•ÊåáÂØºA*ÊêúÁ¥¢Ôºå‰ªéËÄåÊâæÂà∞È´òÊïàÁöÑÂ∑•ÂÖ∑Ë∑ØÂæÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCoSTA*Âú®Â§öËΩÆÂõæÂÉèÁºñËæë‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊ®°ÂûãÔºåËÉΩÂ§üÊ†πÊçÆÁî®Êà∑ÂÅèÂ•ΩËøõË°åÁÅµÊ¥ªÁöÑÊàêÊú¨‰∏éË¥®ÈáèÊùÉË°°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10351', 'title': 'New Trends for Modern Machine Translation with Large Reasoning Models', 'url': 'https://huggingface.co/papers/2503.10351', 'abstract': 'Recent advances in Large Reasoning Models (LRMs), particularly those leveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility for Machine Translation (MT). This position paper argues that LRMs substantially transformed traditional neural MT as well as LLMs-based MT paradigms by reframing translation as a dynamic reasoning task that requires contextual, cultural, and linguistic understanding and reasoning. We identify three foundational shifts: 1) contextual coherence, where LRMs resolve ambiguities and preserve discourse structure through explicit reasoning over cross-sentence and complex context or even lack of context; 2) cultural intentionality, enabling models to adapt outputs by inferring speaker intent, audience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can perform self-reflection during the inference time to correct the potential errors in translation especially extremely noisy cases, showing better robustness compared to simply mapping X->Y translation. We explore various scenarios in translation including stylized translation, document-level translation and multimodal translation by showcasing empirical examples that demonstrate the superiority of LRMs in translation. We also identify several interesting phenomenons for LRMs for MT including auto-pivot translation as well as the critical challenges such as over-localisation in translation and inference efficiency. In conclusion, we think that LRMs redefine translation systems not merely as text converters but as multilingual cognitive agents capable of reasoning about meaning beyond the text. This paradigm shift reminds us to think of problems in translation beyond traditional translation scenarios in a much broader context with LRMs - what we can achieve on top of it.', 'score': 6, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': 'c0a1b109c05698cc', 'authors': ['Sinuo Liu', 'Chenyang Lyu', 'Minghao Wu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['MarcoPolo Team, Alibaba International Digital Commerce', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.10351.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multilingual', '#machine_translation'], 'emoji': 'üß†', 'ru': {'title': 'LRM: –û—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º –∞–≥–µ–Ω—Ç–∞–º', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –Ω–∞ –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ LRM —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É—é—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—è –ø–µ—Ä–µ–≤–æ–¥ –∫–∞–∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∑–∞–¥–∞—á—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç—Ä–µ–±—É—é—â—É—é –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–≥–æ, –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –∏ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è. –í—ã–¥–µ–ª—è—é—Ç—Å—è —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è: –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, –∫—É–ª—å—Ç—É—Ä–Ω–∞—è –∏–Ω—Ç–µ–Ω—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—è. –°—Ç–∞—Ç—å—è —Ç–∞–∫–∂–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LRM –≤ –ø–µ—Ä–µ–≤–æ–¥–µ –∏ –æ–±—Å—É–∂–¥–∞–µ—Ç –≤–æ–∑–Ω–∏–∫–∞—é—â–∏–µ —Ñ–µ–Ω–æ–º–µ–Ω—ã –∏ –ø—Ä–æ–±–ª–µ–º—ã.'}, 'en': {'title': 'Transforming Translation: LRMs as Cognitive Agents', 'desc': 'This paper discusses how Large Reasoning Models (LRMs) are changing the way we think about Machine Translation (MT). It highlights three key shifts: first, LRMs improve contextual coherence by understanding complex contexts and resolving ambiguities; second, they incorporate cultural intentionality, allowing translations to reflect speaker intent and social norms; and third, LRMs can self-reflect during translation to correct errors, making them more robust. The authors provide examples of how LRMs excel in various translation scenarios, suggesting that these models should be seen as cognitive agents that reason about meaning rather than just text converters.'}, 'zh': {'title': 'Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÈáçÂ°ëÊú∫Âô®ÁøªËØëÁöÑÊú™Êù•', 'desc': 'Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂú®Êú∫Âô®ÁøªËØëÔºàMTÔºâÈ¢ÜÂüüÂ∏¶Êù•‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄßÔºåÁâπÂà´ÊòØÈÄöËøáÈìæÂºèÊÄùÁª¥Êé®ÁêÜÔºàCoTÔºâ„ÄÇËøôÁØáËÆ∫ÊñáÊåáÂá∫ÔºåLRMsÂ∞Ü‰º†ÁªüÁ•ûÁªèÊú∫Âô®ÁøªËØëËΩ¨Âèò‰∏∫‰∏ÄÁßçÂä®ÊÄÅÊé®ÁêÜ‰ªªÂä°ÔºåÂº∫Ë∞É‰∏ä‰∏ãÊñá„ÄÅÊñáÂåñÂíåËØ≠Ë®ÄÁêÜËß£ÁöÑÈáçË¶ÅÊÄß„ÄÇÊàë‰ª¨ËØÜÂà´Âá∫‰∏â‰∏™Âü∫Á°ÄËΩ¨ÂèòÔºö‰∏ä‰∏ãÊñáËøûË¥ØÊÄß„ÄÅÊñáÂåñÊÑèÂõæÊÄßÂíåËá™ÊàëÂèçÊÄùËÉΩÂäõÔºå‰ΩøÂæóLRMsÂú®ÁøªËØë‰∏≠Ë°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ËÆ§‰∏∫LRMsÈáçÊñ∞ÂÆö‰πâ‰∫ÜÁøªËØëÁ≥ªÁªüÔºå‰ΩøÂÖ∂‰∏ç‰ªÖ‰ªÖÊòØÊñáÊú¨ËΩ¨Êç¢Âô®ÔºåËÄåÊòØËÉΩÂ§üË∂ÖË∂äÊñáÊú¨ËøõË°åÊÑè‰πâÊé®ÁêÜÁöÑÂ§öËØ≠Ë®ÄËÆ§Áü•‰ª£ÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10596', 'title': 'GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding', 'url': 'https://huggingface.co/papers/2503.10596', 'abstract': 'Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., 4.5 times faster than the GLaMM.', 'score': 5, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': '1b9ea337d198c741', 'authors': ['Rui Hu', 'Lianghui Zhu', 'Yuxuan Zhang', 'Tianheng Cheng', 'Lei Liu', 'Heng Liu', 'Longjin Ran', 'Xiaoxin Chen', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['School of EIC, Huazhong University of Science & Technology', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.10596.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset'], 'emoji': 'üîç', 'ru': {'title': 'GroundingSuite: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–∏–∫—Å–µ–ª—å–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GroundingSuite - –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–¥–∞—á–∏ –ø–∏–∫—Å–µ–ª—å–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è. GroundingSuite —Ç–∞–∫–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å 9,56 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∏–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–π, –∞ —Ç–∞–∫–∂–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏–∑ 3800 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —ç—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é.'}, 'en': {'title': 'Revolutionizing Pixel Grounding with GroundingSuite', 'desc': 'This paper introduces GroundingSuite, a new framework designed to enhance pixel grounding tasks like Referring Expression Segmentation (RES) by addressing the limitations of existing datasets. GroundingSuite features an automated data annotation system that utilizes multiple Vision-Language Model (VLM) agents, resulting in a large-scale dataset with 9.56 million diverse referring expressions and their segmentations. The framework not only improves the quality and diversity of training data but also provides a curated evaluation benchmark with 3,800 images. Models trained on this dataset achieve state-of-the-art performance, significantly outperforming previous methods in both efficiency and accuracy.'}, 'zh': {'title': 'GroundingSuiteÔºöÊèêÂçáËßÜËßâ‰∏éËØ≠Ë®ÄÁöÑÊ°•Ê¢Å', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫GroundingSuiteÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊï∞ÊçÆÈõÜÂú®ÂÉèÁ¥†ÂÆö‰Ωç‰ªªÂä°‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨‰∏Ä‰∏™Ëá™Âä®ÂåñÊï∞ÊçÆÊ≥®ÈáäÁ≥ªÁªüÔºåÂà©Áî®Â§ö‰∏™ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâ‰ª£ÁêÜÁîüÊàêÊï∞ÊçÆ„ÄÇGroundingSuiteÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂåÖÂê´956‰∏áÁßçÂ§öÊ†∑ÂåñÊåáÁß∞Ë°®ËææÂèäÂÖ∂ÂØπÂ∫îÂàÜÂâ≤ÁöÑÂ§ßËßÑÊ®°ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÂπ∂Âª∫Á´ã‰∫Ü‰∏Ä‰∏™ÂåÖÂê´3800Âº†ÂõæÂÉèÁöÑËØÑ‰º∞Âü∫ÂáÜ„ÄÇÈÄöËøá‰ΩøÁî®GroundingSuiteËÆ≠ÁªÉÁöÑÊï∞ÊçÆÈõÜÔºåÊ®°ÂûãÁöÑÊÄßËÉΩÊòæËëóÊèêÂçáÔºåËææÂà∞‰∫ÜÊúÄÊñ∞ÁöÑÁ†îÁ©∂ÊàêÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10582', 'title': 'VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search', 'url': 'https://huggingface.co/papers/2503.10582', 'abstract': "Vision-Language Models have made significant progress on many perception-focused tasks, however, their progress on reasoning-focused tasks seem to be limited due to the lack of high-quality and diverse training data. In this work, we aim to address the scarcity issue of reasoning-focused multimodal datasets. We propose VisualWebInstruct - a novel approach that leverages search engine to create a diverse, and high-quality dataset spanning multiple disciplines like math, physics, finance, chemistry, etc. Starting with meticulously selected 30,000 seed images, we employ Google Image search to identify websites containing similar images. We collect and process the HTMLs from over 700K unique URL sources. Through a pipeline of content extraction, filtering and synthesis, we build a dataset of approximately 900K question-answer pairs, with 40% being visual QA pairs and the rest as text QA pairs. Models fine-tuned on VisualWebInstruct demonstrate significant performance gains: (1) training from Llava-OV-mid shows 10-20% absolute point gains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain. Our best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B parameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath (55.7%). These remarkable results highlight the effectiveness of our dataset in enhancing VLMs' reasoning capabilities for complex multimodal tasks.", 'score': 5, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': 'ff1e79c1589f8602', 'authors': ['Yiming Jia', 'Jiachen Li', 'Xiang Yue', 'Bo Li', 'Ping Nie', 'Kai Zou', 'Wenhu Chen'], 'affiliations': ['CMU', 'Independent', 'NUS', 'Netmind.ai', 'UC Santa Barbara', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.10582.jpg', 'data': {'categories': ['#dataset', '#data', '#reasoning', '#multimodal'], 'emoji': 'üß†', 'ru': {'title': 'VisualWebInstruct: –ø—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ VisualWebInstruct - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É—è –ø–æ–∏—Å–∫–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É HTML-—Å—Ç—Ä–∞–Ω–∏—Ü, –æ–Ω–∏ —Å–æ–±—Ä–∞–ª–∏ –æ–∫–æ–ª–æ 900 —Ç—ã—Å—è—á –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º. –ú–æ–¥–µ–ª–∏, –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —ç—Ç–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å MAmmoTH-VL2 –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–≤–æ–µ–º –∫–ª–∞—Å—Å–µ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á.'}, 'en': {'title': 'Enhancing Reasoning in Vision-Language Models with VisualWebInstruct', 'desc': 'This paper introduces VisualWebInstruct, a new dataset aimed at improving the reasoning abilities of Vision-Language Models (VLMs). The dataset is created by leveraging search engines to gather diverse and high-quality multimodal data, specifically focusing on reasoning tasks across various disciplines. By processing over 700,000 unique web sources, the authors compile approximately 900,000 question-answer pairs, enhancing the training data available for VLMs. The results show that models trained on this dataset achieve significant performance improvements on reasoning benchmarks, demonstrating its effectiveness in advancing VLM capabilities.'}, 'zh': {'title': 'ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞Êï∞ÊçÆÈõÜ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïVisualWebInstructÔºåÊó®Âú®Ëß£ÂÜ≥Êé®ÁêÜÂØºÂêëÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜÁ®ÄÁº∫ÈóÆÈ¢ò„ÄÇÊàë‰ª¨Âà©Áî®ÊêúÁ¥¢ÂºïÊìéÂàõÂª∫‰∫Ü‰∏Ä‰∏™Â§öÂ≠¶ÁßëÁöÑÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜÔºåÂåÖÊã¨Êï∞Â≠¶„ÄÅÁâ©ÁêÜ„ÄÅÈáëËûçÂíåÂåñÂ≠¶Á≠âÈ¢ÜÂüü„ÄÇÈÄöËøá‰ªé30,000‰∏™ÁßçÂ≠êÂõæÂÉèÂºÄÂßãÔºåÊî∂ÈõÜÂíåÂ§ÑÁêÜË∂ÖËøá70‰∏á‰∏™Áã¨ÁâπÁΩëÂùÄÁöÑHTMLÂÜÖÂÆπÔºåÊàë‰ª¨ÊûÑÂª∫‰∫ÜÁ∫¶90‰∏á‰∏™ÈóÆÁ≠îÂØπÁöÑÊï∞ÊçÆÈõÜ„ÄÇÁªèËøáÂú®VisualWebInstruct‰∏äÂæÆË∞ÉÁöÑÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåËØÅÊòé‰∫ÜËØ•Êï∞ÊçÆÈõÜÂú®Â¢ûÂº∫ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10630', 'title': 'UniGoal: Towards Universal Zero-shot Goal-oriented Navigation', 'url': 'https://huggingface.co/papers/2503.10630', 'abstract': 'In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods.', 'score': 3, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': '9e7667a37c699f4c', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#reasoning', '#long_context', '#games', '#graphs', '#benchmark', '#agents', '#rl'], 'emoji': 'üß≠', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ–æ–≤ –∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ü–µ–ª—è–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –µ–¥–∏–Ω–æ–µ –≥—Ä–∞—Ñ–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ü–µ–ª–µ–π –∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É—è –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ–æ–≤, —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å—Ü–µ–Ω—ã –∏ –≥—Ä–∞—Ñ–∞ —Ü–µ–ª–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ UniGoal –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.'}, 'en': {'title': 'Navigating Goals Universally with Graphs!', 'desc': 'This paper introduces a new framework for universal zero-shot goal-oriented navigation, which allows an agent to navigate towards various goals without prior training on specific tasks. Unlike existing methods that rely heavily on large language models (LLMs) tailored for individual tasks, this approach uses a uniform graph representation to integrate different types of goals, such as object categories and text descriptions. The agent maintains an online scene graph that captures the environment, enabling it to perform graph-based reasoning and match its observations with the goals. The proposed method demonstrates superior performance in navigation tasks, surpassing both task-specific and supervised approaches, by effectively managing goal exploration and verification through innovative strategies.'}, 'zh': {'title': 'ÈÄöÁî®Èõ∂-shotÂØºËà™ÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöÁî®ÁöÑÈõ∂-shotÁõÆÊ†áÂØºÂêëÂØºËà™Ê°ÜÊû∂„ÄÇÁé∞ÊúâÁöÑÈõ∂-shotÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËøõË°åÁâπÂÆö‰ªªÂä°ÁöÑÊé®ÁêÜÔºåÂØºËá¥Âú®‰∏çÂêåÁõÆÊ†á‰πãÈó¥ÁöÑÊ≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÂõæË°®Á§∫ÊñπÊ≥ïÔºåÂ∞Ü‰∏çÂêåÁöÑÁõÆÊ†áÔºàÂ¶ÇÁâ©‰ΩìÁ±ªÂà´„ÄÅÂÆû‰æãÂõæÂÉèÂíåÊñáÊú¨ÊèèËø∞ÔºâÊï¥ÂêàÂú®‰∏ÄËµ∑ÔºåÂπ∂Â∞Ü‰ª£ÁêÜÁöÑËßÇÂØüÁªìÊûúËΩ¨Êç¢‰∏∫Âú®Á∫øÁª¥Êä§ÁöÑÂú∫ÊôØÂõæ„ÄÇÈÄöËøáËøôÁßç‰∏ÄËá¥ÁöÑÂú∫ÊôØÂíåÁõÆÊ†áË°®Á§∫ÔºåÊàë‰ª¨ËÉΩÂ§üÂà©Áî®LLMËøõË°åÂü∫‰∫éÂõæÁöÑÊé®ÁêÜÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Â±ïÁ§∫‰∫ÜÊàë‰ª¨ÁöÑUniGoalÂú®Èõ∂-shotÂØºËà™‰ªªÂä°‰∏äÁöÑ‰ºòË∂äÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10639', 'title': 'GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing', 'url': 'https://huggingface.co/papers/2503.10639', 'abstract': 'Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/rongyaofang/GoT.', 'score': 2, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': '70c9d741eedd181c', 'authors': ['Rongyao Fang', 'Chengqi Duan', 'Kun Wang', 'Linjiang Huang', 'Hao Li', 'Shilin Yan', 'Hao Tian', 'Xingyu Zeng', 'Rui Zhao', 'Jifeng Dai', 'Xihui Liu', 'Hongsheng Li'], 'affiliations': ['BUAA', 'CUHK MMLab', 'HKU', 'SenseTime', 'Shanghai AI Laboratory', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2503.10639.jpg', 'data': {'categories': ['#multimodal', '#cv', '#dataset', '#reasoning', '#diffusion', '#open_source'], 'emoji': 'üß†', 'ru': {'title': '–†–∞–∑—É–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –º—ã—à–ª–µ–Ω–∏—é', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É Generation Chain-of-Thought (GoT) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. GoT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —è–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —è–∑—ã–∫–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –≤—ã–≤–æ–¥–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –º–∞—Å—à—Ç–∞–±–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö GoT –∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –æ–±—ä–µ–¥–∏–Ω—è—é—â—É—é Qwen2.5-VL –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.'}, 'en': {'title': 'Revolutionizing Image Generation with Reasoning!', 'desc': 'The paper introduces Generation Chain-of-Thought (GoT), a new method for image generation and editing that incorporates reasoning about visual composition. Instead of simply processing text prompts, GoT uses a reasoning-guided framework to analyze semantic relationships and spatial arrangements before generating images. It includes a large dataset with over 9 million samples that capture detailed reasoning chains, enhancing the understanding of how different elements relate visually. The results show that GoT significantly improves the quality of generated and edited images, allowing for interactive adjustments based on user-defined reasoning steps.'}, 'zh': {'title': 'Êé®ÁêÜÈ©±Âä®ÁöÑÂõæÂÉèÁîüÊàê‰∏éÁºñËæëÊñ∞ÊñπÂêë', 'desc': 'ÂΩìÂâçÁöÑÂõæÂÉèÁîüÊàêÂíåÁºñËæëÊñπÊ≥ï‰∏ªË¶ÅÂ∞ÜÊñáÊú¨ÊèêÁ§∫‰Ωú‰∏∫Áõ¥Êé•ËæìÂÖ•ÔºåËÄåÊ≤°ÊúâËÄÉËôëËßÜËßâÊûÑÂõæÂíåÊòéÁ°ÆÁöÑÊìç‰Ωú„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁîüÊàêÊÄùÁª¥ÈìæÔºàGoTÔºâËåÉÂºèÔºåÈÄöËøáÂú®ËæìÂá∫ÂõæÂÉè‰πãÂâçËøõË°åÊòéÁ°ÆÁöÑËØ≠Ë®ÄÊé®ÁêÜËøáÁ®ãÊù•ÂÆûÁé∞ÁîüÊàêÂíåÁºñËæë„ÄÇËØ•ÊñπÊ≥ïÂ∞Ü‰º†ÁªüÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÂíåÁºñËæëËΩ¨Âèò‰∏∫‰∏Ä‰∏™Âü∫‰∫éÊé®ÁêÜÁöÑÊ°ÜÊû∂ÔºåÂàÜÊûêËØ≠‰πâÂÖ≥Á≥ªÂíåÁ©∫Èó¥ÊéíÂàó„ÄÇÊàë‰ª¨ÁöÑGoTÊ°ÜÊû∂Âú®ÁîüÊàêÂíåÁºñËæë‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÔºåÂπ∂ÂÖÅËÆ∏Áî®Êà∑ÈÄöËøáÊòéÁ°Æ‰øÆÊîπÊé®ÁêÜÊ≠•È™§Êù•ËøõË°å‰∫§‰∫íÂºèËßÜËßâÁîüÊàê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10637', 'title': 'Distilling Diversity and Control in Diffusion Models', 'url': 'https://huggingface.co/papers/2503.10637', 'abstract': 'Distilled diffusion models suffer from a critical limitation: reduced sample diversity compared to their base counterparts. In this work, we uncover that despite this diversity loss, distilled models retain the fundamental concept representations of base models. We demonstrate control distillation - where control mechanisms like Concept Sliders and LoRAs trained on base models can be seamlessly transferred to distilled models and vice-versa, effectively distilling control without any retraining. This preservation of representational structure prompted our investigation into the mechanisms of diversity collapse during distillation. To understand how distillation affects diversity, we introduce Diffusion Target (DT) Visualization, an analysis and debugging tool that reveals how models predict final outputs at intermediate steps. Through DT-Visualization, we identify generation artifacts, inconsistencies, and demonstrate that initial diffusion timesteps disproportionately determine output diversity, while later steps primarily refine details. Based on these insights, we introduce diversity distillation - a hybrid inference approach that strategically employs the base model for only the first critical timestep before transitioning to the efficient distilled model. Our experiments demonstrate that this simple modification not only restores the diversity capabilities from base to distilled models but surprisingly exceeds it, while maintaining nearly the computational efficiency of distilled inference, all without requiring additional training or model modifications. Our code and data are available at https://distillation.baulab.info', 'score': 2, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': '5313c06d699a1a7f', 'authors': ['Rohit Gandikota', 'David Bau'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.10637.jpg', 'data': {'categories': ['#dataset', '#inference', '#training', '#optimization', '#diffusion', '#data'], 'emoji': 'üî¨', 'ru': {'title': '–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å—ç–º–ø–ª–æ–≤ –≤ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –º–µ–∂–¥—É –Ω–∏–º–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–∏—á–∏–Ω –ø–æ—Ç–µ—Ä–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ —Ç–µ—Ö–Ω–∏–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ Diffusion Target, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –Ω–∞—á–∞–ª—å–Ω—ã–µ —à–∞–≥–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –Ω–µ–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ diversity distillation, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —à–∞–≥–∞, —á—Ç–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∏ –¥–∞–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞.'}, 'en': {'title': 'Restoring Diversity in Distilled Diffusion Models', 'desc': 'This paper addresses the issue of reduced sample diversity in distilled diffusion models compared to their original versions. The authors reveal that distilled models still maintain the essential concept representations of their base models. They introduce a method called control distillation, which allows for the transfer of control mechanisms between models without retraining. Additionally, they present Diffusion Target (DT) Visualization to analyze how diversity is affected during the distillation process, leading to a new approach called diversity distillation that enhances output diversity while keeping computational efficiency intact.'}, 'zh': {'title': 'ÊÅ¢Â§çËí∏È¶èÊ®°ÂûãÁöÑÂ§öÊ†∑ÊÄß', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜËí∏È¶èÊâ©Êï£Ê®°ÂûãÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÈôêÂà∂ÔºåÂç≥‰∏éÂü∫Á°ÄÊ®°ÂûãÁõ∏ÊØîÔºåÊ†∑Êú¨Â§öÊ†∑ÊÄßÈôç‰Ωé„ÄÇÂ∞ΩÁÆ°Â≠òÂú®ËøôÁßçÂ§öÊ†∑ÊÄßÊçüÂ§±ÔºåËí∏È¶èÊ®°Âûã‰ªçÁÑ∂‰øùÁïô‰∫ÜÂü∫Á°ÄÊ®°ÂûãÁöÑÂü∫Êú¨Ê¶ÇÂøµË°®Á§∫„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÊéßÂà∂Ëí∏È¶èÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Â∞ÜÊéßÂà∂Êú∫Âà∂Êó†ÁºùËΩ¨ÁßªÂà∞Ëí∏È¶èÊ®°ÂûãÔºåÂπ∂‰∏î‰∏çÈúÄË¶ÅÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇÈÄöËøáÂºïÂÖ•Êâ©Êï£ÁõÆÊ†áÂèØËßÜÂåñÂ∑•ÂÖ∑ÔºåÊàë‰ª¨ÂàÜÊûê‰∫ÜËí∏È¶èÂØπÂ§öÊ†∑ÊÄßÁöÑÂΩ±ÂìçÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ∑∑ÂêàÊé®ÁêÜÊñπÊ≥ïÔºåËÉΩÂ§üÂú®‰øùÊåÅËÆ°ÁÆóÊïàÁéáÁöÑÂêåÊó∂ÊÅ¢Â§çÂ§öÊ†∑ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.04723', 'title': 'Shifting Long-Context LLMs Research from Input to Output', 'url': 'https://huggingface.co/papers/2503.04723', 'abstract': 'Recent advancements in long-context Large Language Models (LLMs) have primarily concentrated on processing extended input contexts, resulting in significant strides in long-context comprehension. However, the equally critical aspect of generating long-form outputs has received comparatively less attention. This paper advocates for a paradigm shift in NLP research toward addressing the challenges of long-output generation. Tasks such as novel writing, long-term planning, and complex reasoning require models to understand extensive contexts and produce coherent, contextually rich, and logically consistent extended text. These demands highlight a critical gap in current LLM capabilities. We underscore the importance of this under-explored domain and call for focused efforts to develop foundational LLMs tailored for generating high-quality, long-form outputs, which hold immense potential for real-world applications.', 'score': 2, 'issue_id': 2700, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 –º–∞—Ä—Ç–∞', 'en': 'March 6', 'zh': '3Êúà6Êó•'}, 'hash': '009f3e654e927dc3', 'authors': ['Yuhao Wu', 'Yushi Bai', 'Zhiqing Hu', 'Shangqing Tu', 'Ming Shan Hee', 'Juanzi Li', 'Roy Ka-Wei Lee'], 'affiliations': ['Singapore University', 'Singapore University of Technology and Design', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04723.jpg', 'data': {'categories': ['#data', '#long_context', '#multimodal'], 'emoji': 'üìù', 'ru': {'title': '–ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –ò–ò: —Å–æ–∑–¥–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –æ–±—Å—É–∂–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–∑–≤–∏—Ç–∏—è –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤, –Ω–æ –Ω–µ –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ–±—ä–µ–º–Ω—ã—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç—Å—è –≤–∞–∂–Ω–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–≤—è–∑–Ω—ã–µ –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã. –¢–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ä–æ–º–∞–Ω–æ–≤, –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Bridging the Gap: Enhancing Long-Form Output in LLMs', 'desc': 'This paper discusses the need for improvements in long-form output generation by Large Language Models (LLMs). While recent advancements have focused on understanding long input contexts, generating coherent and contextually rich long outputs remains underexplored. The authors emphasize that tasks like novel writing and complex reasoning require models to produce logically consistent extended text. They call for more research efforts to develop LLMs that can effectively handle these long-output challenges, which are crucial for various real-world applications.'}, 'zh': {'title': 'Êé®Âä®ÈïøÊñáÊú¨ÁîüÊàêÁöÑÁ†îÁ©∂ËΩ¨Âûã', 'desc': 'ÊúÄËøëÔºåÈïø‰∏ä‰∏ãÊñáÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÊâ©Â±ïËæìÂÖ•‰∏ä‰∏ãÊñáÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÁîüÊàêÈïøÊñáÊú¨ËæìÂá∫ÁöÑÁ†îÁ©∂Áõ∏ÂØπËæÉÂ∞ë„ÄÇÊú¨ÊñáÊèêÂÄ°Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâÁ†îÁ©∂ÂêëËß£ÂÜ≥ÈïøËæìÂá∫ÁîüÊàêÁöÑÊåëÊàòËΩ¨Âèò„ÄÇÈïøÁØáÂ∞èËØ¥ÂÜô‰Ωú„ÄÅÈïøÊúüËßÑÂàíÂíåÂ§çÊùÇÊé®ÁêÜÁ≠â‰ªªÂä°ÈúÄË¶ÅÊ®°ÂûãÁêÜËß£ÂπøÊ≥õÁöÑ‰∏ä‰∏ãÊñáÔºåÂπ∂ÁîüÊàêËøûË¥Ø„ÄÅ‰∏∞ÂØå‰∏îÈÄªËæë‰∏ÄËá¥ÁöÑÊâ©Â±ïÊñáÊú¨„ÄÇÊàë‰ª¨Âº∫Ë∞ÉËøô‰∏ÄÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢È¢ÜÂüüÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂ÂëºÂêÅÈõÜ‰∏≠ÂäõÈáèÂºÄÂèë‰∏ìÈó®Áî®‰∫éÁîüÊàêÈ´òË¥®ÈáèÈïøÊñáÊú¨ËæìÂá∫ÁöÑÂü∫Á°ÄLLMÔºå‰ª•Êª°Ë∂≥Áé∞ÂÆûÂ∫îÁî®ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10636', 'title': 'The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation', 'url': 'https://huggingface.co/papers/2503.10636', 'abstract': 'Minibatch optimal transport coupling straightens paths in unconditional flow matching. This leads to computationally less demanding inference as fewer integration steps and less complex numerical solvers can be employed when numerically solving an ordinary differential equation at test time. However, in the conditional setting, minibatch optimal transport falls short. This is because the default optimal transport mapping disregards conditions, resulting in a conditionally skewed prior distribution during training. In contrast, at test time, we have no access to the skewed prior, and instead sample from the full, unbiased prior distribution. This gap between training and testing leads to a subpar performance. To bridge this gap, we propose conditional optimal transport C^2OT that adds a conditional weighting term in the cost matrix when computing the optimal transport assignment. Experiments demonstrate that this simple fix works with both discrete and continuous conditions in 8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method performs better overall compared to the existing baselines across different function evaluation budgets. Code is available at https://hkchengrex.github.io/C2OT', 'score': 1, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': '93d6f410c35246be', 'authors': ['Ho Kei Cheng', 'Alexander Schwing'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.10636.jpg', 'data': {'categories': ['#cv', '#inference', '#optimization', '#training'], 'emoji': 'üîÄ', 'ru': {'title': '–£—Å–ª–æ–≤–Ω—ã–π –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ —É—Å–ª–æ–≤–Ω–æ–≥–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞ (C^2OT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —É—Å–ª–æ–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º –∏ —Ç–µ—Å—Ç–æ–≤—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–∏–Ω–∏-–±–∞—Ç—á –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞ –≤ —É—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. C^2OT —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É –ø—É—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —É—Å–ª–æ–≤–Ω–æ–≥–æ –≤–µ—Å–æ–≤–æ–≥–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –≤ –º–∞—Ç—Ä–∏—Ü—É —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–∞–∑–æ–≤—ã—Ö –ª–∏–Ω–∏–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –±—é–¥–∂–µ—Ç–∞—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.'}, 'en': {'title': 'Bridging the Gap in Conditional Flow Matching with C^2OT', 'desc': 'This paper introduces a method called conditional optimal transport (C^2OT) to improve the performance of flow matching in machine learning. The authors highlight that while minibatch optimal transport simplifies computations during inference, it fails in conditional settings due to a mismatch between training and testing distributions. By incorporating a conditional weighting term in the cost matrix, C^2OT effectively aligns the training and testing conditions. Experiments show that this approach outperforms existing methods across various datasets, demonstrating its effectiveness in both discrete and continuous scenarios.'}, 'zh': {'title': 'Êù°‰ª∂ÊúÄ‰ºò‰º†ËæìÔºöÁº©Â∞èËÆ≠ÁªÉ‰∏éÊµãËØïÁöÑÊÄßËÉΩÂ∑ÆË∑ù', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊù°‰ª∂ÊúÄ‰ºò‰º†ËæìÊñπÊ≥ïC^2OTÔºå‰ª•Ëß£ÂÜ≥Âú®Êù°‰ª∂ËÆæÁΩÆ‰∏ãÂ∞èÊâπÈáèÊúÄ‰ºò‰º†ËæìÁöÑ‰∏çË∂≥„ÄÇ‰º†ÁªüÁöÑÊúÄ‰ºò‰º†ËæìÊò†Â∞ÑÂú®ËÆ≠ÁªÉÊó∂ÂøΩÁï•‰∫ÜÊù°‰ª∂ÔºåÂØºËá¥ËÆ≠ÁªÉÊúüÈó¥ÁöÑÂÖàÈ™åÂàÜÂ∏ÉÂÅèÊñúÔºåËÄåÊµãËØïÊó∂Âç¥Êó†Ê≥ïËÆøÈóÆËøôÁßçÂÅèÊñúÁöÑÂÖàÈ™å„ÄÇÈÄöËøáÂú®ÊàêÊú¨Áü©Èòµ‰∏≠Ê∑ªÂä†Êù°‰ª∂Âä†ÊùÉÈ°πÔºåC^2OTËÉΩÂ§üÊõ¥Â•ΩÂú∞ËÆ°ÁÆóÊúÄ‰ºò‰º†ËæìÂàÜÈÖçÔºå‰ªéËÄåÁº©Â∞èËÆ≠ÁªÉ‰∏éÊµãËØï‰πãÈó¥ÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºò‰∫éÁé∞ÊúâÂü∫Á∫ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10568', 'title': 'Autoregressive Image Generation with Randomized Parallel Decoding', 'url': 'https://huggingface.co/papers/2503.10568', 'abstract': 'We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel guided decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only 64 sampling steps, achieving over a 20-fold increase in throughput while reducing memory consumption by over 75% compared to representative recent autoregressive models at a similar scale.', 'score': 1, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': '44a1e551455870ac', 'authors': ['Haopeng Li', 'Jinyue Yang', 'Guoqi Li', 'Huan Wang'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10568.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#cv', '#architecture'], 'emoji': 'üé®', 'ru': {'title': 'ARPG: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Å–ª—É—á–∞–π–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –≤—ã–≤–æ–¥–æ–º', 'desc': 'ARPG - —ç—Ç–æ –Ω–æ–≤–∞—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å —Ä–∞–Ω–¥–æ–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, –ø—Ä–µ–¥–ª–∞–≥–∞—è –º–µ—Ö–∞–Ω–∏–∑–º —É–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. ARPG –æ–±—É—á–∞–µ—Ç—Å—è –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–ª—É—á–∞–π–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π –ª–µ–≥–∫–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –∑–∞–¥–∞—á–∞–º –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫–∏–º –∫–∞–∫ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∞–Ω–∞–ª–æ–≥–∞–º–∏, –¥–æ—Å—Ç–∏–≥–∞—è FID 1.94 –Ω–∞ ImageNet-1K 256 –≤—Å–µ–≥–æ –∑–∞ 64 —à–∞–≥–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Revolutionizing Image Generation with Randomized Parallel Processing', 'desc': 'The paper presents ARPG, a new visual autoregressive model designed for efficient and flexible image generation. Unlike traditional methods that generate tokens in a fixed order, ARPG allows for randomized parallel generation, improving inference speed and enabling zero-shot generalization. It introduces a guided decoding framework that separates positional guidance from content representation, enhancing the causal attention mechanism. As a result, ARPG excels in tasks like image inpainting and outpainting, achieving significant performance improvements on benchmarks while using less memory.'}, 'zh': {'title': 'ARPGÔºöÂÆûÁé∞ÈöèÊú∫Âπ∂Ë°åÁîüÊàêÁöÑËá™ÂõûÂΩíÊ®°Âûã', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËßÜËßâËá™ÂõûÂΩíÊ®°ÂûãARPGÔºåËÉΩÂ§üÂÆûÁé∞ÈöèÊú∫Âπ∂Ë°åÁîüÊàêÔºåËß£ÂÜ≥‰∫Ü‰º†ÁªüÂÖâÊ†ÖÈ°∫Â∫èÊñπÊ≥ïÂú®Êé®ÁêÜÊïàÁéáÂíåÈõ∂-shotÊ≥õÂåñÊñπÈù¢ÁöÑÂõ∫ÊúâÈôêÂà∂„ÄÇÊàë‰ª¨ÁöÑÂÖ≥ÈîÆËßÅËß£ÊòØÔºåÊúâÊïàÁöÑÈöèÊú∫È°∫Â∫èÂª∫Ê®°ÈúÄË¶ÅÊòéÁ°ÆÁöÑÊåáÂØºÊù•Á°ÆÂÆö‰∏ã‰∏Ä‰∏™È¢ÑÊµãÊ†áËÆ∞ÁöÑ‰ΩçÁΩÆ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂºïÂØºËß£Á†ÅÊ°ÜÊû∂ÔºåÂ∞Ü‰ΩçÁΩÆÊåáÂØº‰∏éÂÜÖÂÆπË°®Á§∫ÂàÜÂºÄÁºñÁ†ÅÔºåÂàÜÂà´‰Ωú‰∏∫Êü•ËØ¢ÂíåÈîÆÂÄºÂØπ„ÄÇÈÄöËøáÂ∞ÜËøôÁßçÊåáÂØºÁõ¥Êé•ËûçÂÖ•Âõ†ÊûúÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂÆûÁé∞‰∫ÜÂÆåÂÖ®ÈöèÊú∫È°∫Â∫èÁöÑËÆ≠ÁªÉÂíåÁîüÊàêÔºåÊ∂àÈô§‰∫ÜÂØπÂèåÂêëÊ≥®ÊÑèÂäõÁöÑÈúÄÊ±Ç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10391', 'title': 'CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance', 'url': 'https://huggingface.co/papers/2503.10391', 'abstract': 'Video generation has witnessed remarkable progress with the advent of deep generative models, particularly diffusion models. While existing methods excel in generating high-quality videos from text prompts or single images, personalized multi-subject video generation remains a largely unexplored challenge. This task involves synthesizing videos that incorporate multiple distinct subjects, each defined by separate reference images, while ensuring temporal and spatial consistency. Current approaches primarily rely on mapping subject images to keywords in text prompts, which introduces ambiguity and limits their ability to model subject relationships effectively. In this paper, we propose CINEMA, a novel framework for coherent multi-subject video generation by leveraging Multimodal Large Language Model (MLLM). Our approach eliminates the need for explicit correspondences between subject images and text entities, mitigating ambiguity and reducing annotation effort. By leveraging MLLM to interpret subject relationships, our method facilitates scalability, enabling the use of large and diverse datasets for training. Furthermore, our framework can be conditioned on varying numbers of subjects, offering greater flexibility in personalized content creation. Through extensive evaluations, we demonstrate that our approach significantly improves subject consistency, and overall video coherence, paving the way for advanced applications in storytelling, interactive media, and personalized video generation.', 'score': 1, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': 'd7f2c49b29951ace', 'authors': ['Yufan Deng', 'Xun Guo', 'Yizhi Wang', 'Jacob Zhiyuan Fang', 'Angtian Wang', 'Shenghai Yuan', 'Yiding Yang', 'Bo Liu', 'Haibin Huang', 'Chongyang Ma'], 'affiliations': ['ByteDance Intelligent Creation', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10391.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#story_generation', '#video', '#diffusion'], 'emoji': 'üé¨', 'ru': {'title': 'CINEMA: –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —Å—É–±—ä–µ–∫—Ç–æ–≤ –ø—Ä–∏ –ø–æ–º–æ—â–∏ MLLM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç CINEMA - –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å (MLLM). CINEMA —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ —è–≤–Ω–æ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Å—É–±—ä–µ–∫—Ç–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏, —á—Ç–æ —É–º–µ–Ω—å—à–∞–µ—Ç –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å –∏ —É–ø—Ä–æ—â–∞–µ—Ç –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å, –∏—Å–ø–æ–ª—å–∑—É—è –±–æ–ª—å—à–∏–µ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. CINEMA –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ –æ–±—â–µ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'CINEMA: Coherent Multi-Subject Video Generation Made Easy', 'desc': 'This paper introduces CINEMA, a new framework for generating videos that feature multiple distinct subjects from separate reference images. Unlike traditional methods that rely on mapping images to text prompts, CINEMA uses a Multimodal Large Language Model (MLLM) to understand and interpret relationships between subjects, which reduces ambiguity. The framework allows for flexible conditioning on different numbers of subjects, making it easier to create personalized content. Extensive evaluations show that CINEMA enhances subject consistency and overall video coherence, opening up new possibilities for applications in storytelling and interactive media.'}, 'zh': {'title': '‰∏™ÊÄßÂåñÂ§ö‰∏ª‰ΩìËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'ËßÜÈ¢ëÁîüÊàêÂú®Ê∑±Â∫¶ÁîüÊàêÊ®°ÂûãÔºåÁâπÂà´ÊòØÊâ©Êï£Ê®°ÂûãÁöÑÊé®Âä®‰∏ãÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®‰ªéÊñáÊú¨ÊèêÁ§∫ÊàñÂçïÂº†ÂõæÂÉèÁîüÊàêÈ´òË¥®ÈáèËßÜÈ¢ëÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜ‰∏™ÊÄßÂåñÁöÑÂ§ö‰∏ª‰ΩìËßÜÈ¢ëÁîüÊàê‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Êú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜCINEMAÊ°ÜÊû∂ÔºåÈÄöËøáÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÔºåÂÆûÁé∞‰∫Ü‰∏ÄËá¥ÁöÑÂ§ö‰∏ª‰ΩìËßÜÈ¢ëÁîüÊàêÔºåÊ∂àÈô§‰∫Ü‰∏ª‰ΩìÂõæÂÉè‰∏éÊñáÊú¨ÂÆû‰Ωì‰πãÈó¥ÁöÑÊòéÁ°ÆÂØπÂ∫îÂÖ≥Á≥ªÔºå‰ªéËÄåÂáèÂ∞ë‰∫ÜÊ≠ß‰πâÂíåÊ†áÊ≥®Â∑•‰Ωú„ÄÇÊàë‰ª¨ÁöÑÊ°ÜÊû∂ËÉΩÂ§üÊ†πÊçÆ‰∏çÂêåÊï∞ÈáèÁöÑ‰∏ª‰ΩìËøõË°åÊù°‰ª∂ÁîüÊàêÔºåÊèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑ‰∏™ÊÄßÂåñÂÜÖÂÆπÂàõ‰ΩúÁÅµÊ¥ªÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10072', 'title': '"Silent Is Not Actually Silent": An Investigation of Toxicity on Bug\n  Report Discussion', 'url': 'https://huggingface.co/papers/2503.10072', 'abstract': 'Toxicity in bug report discussions poses significant challenges to the collaborative dynamics of open-source software development. Bug reports are crucial for identifying and resolving defects, yet their inherently problem-focused nature and emotionally charged context make them susceptible to toxic interactions. This study explores toxicity in GitHub bug reports through a qualitative analysis of 203 bug threads, including 81 toxic ones. Our findings reveal that toxicity frequently arises from misaligned perceptions of bug severity and priority, unresolved frustrations with tools, and lapses in professional communication. These toxic interactions not only derail productive discussions but also reduce the likelihood of actionable outcomes, such as linking issues with pull requests. Our preliminary findings offer actionable recommendations to improve bug resolution by mitigating toxicity.', 'score': 1, 'issue_id': 2699, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': 'a91b6bc8232847a3', 'authors': ['Mia Mohammad Imran', 'Jaydeb Sarker'], 'affiliations': ['Missouri University of Science and Technology, Rolla, Missouri, USA', 'University of Nebraska Omaha, Omaha, Nebraska, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.10072.jpg', 'data': {'categories': ['#ethics', '#open_source'], 'emoji': 'üêõ', 'ru': {'title': '–¢–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å –≤ –æ—Ç—á–µ—Ç–∞—Ö –æ–± –æ—à–∏–±–∫–∞—Ö: –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–µ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –≤ –æ–±—Å—É–∂–¥–µ–Ω–∏—è—Ö –æ—Ç—á–µ—Ç–æ–≤ –æ–± –æ—à–∏–±–∫–∞—Ö –≤ –ø—Ä–æ–µ–∫—Ç–∞—Ö —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ 203 –≤–µ—Ç–æ–∫ –æ–±—Å—É–∂–¥–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –Ω–∞ GitHub, –≤–∫–ª—é—á–∞—è 81 —Ç–æ–∫—Å–∏—á–Ω—É—é. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å —á–∞—Å—Ç–æ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –≤ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç–∏ –æ—à–∏–±–æ–∫, –Ω–µ—Ä–µ—à–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –∏ –Ω–∞—Ä—É—à–µ–Ω–∏–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏. –¢–æ–∫—Å–∏—á–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–∞—Ä—É—à–∞—é—Ç –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ã–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è, –Ω–æ –∏ —Å–Ω–∏–∂–∞—é—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø—Ä–∏–≤—è–∑–∫–∞ –ø—Ä–æ–±–ª–µ–º –∫ –∑–∞–ø—Ä–æ—Å–∞–º –Ω–∞ —Å–ª–∏—è–Ω–∏–µ.'}, 'en': {'title': 'Mitigating Toxicity for Better Bug Resolution in Open Source', 'desc': 'This paper investigates the issue of toxicity in bug report discussions within open-source software development on GitHub. It analyzes 203 bug threads, identifying 81 as toxic, and highlights that toxicity often stems from differing views on bug severity, frustrations with tools, and poor communication. The study emphasizes that such toxic interactions hinder productive collaboration and decrease the chances of resolving issues effectively. The authors provide recommendations aimed at reducing toxicity to enhance the bug resolution process.'}, 'zh': {'title': 'ÂáèÂ∞ëÊØíÊÄßÔºåÊèêÂçáÂºÄÊ∫êÂçè‰ΩúÊïàÁéá', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂºÄÊ∫êËΩØ‰ª∂ÂºÄÂèë‰∏≠ÔºåGitHub bug Êä•ÂëäËÆ®ËÆ∫‰∏≠ÁöÑÊØíÊÄßÈóÆÈ¢ò„ÄÇÈÄöËøáÂØπ203‰∏™bugÁ∫øÁ®ãÁöÑÂÆöÊÄßÂàÜÊûêÔºåÂèëÁé∞81‰∏™Á∫øÁ®ãÂ≠òÂú®ÊØíÊÄß‰∫íÂä®„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊØíÊÄßÂæÄÂæÄÊ∫ê‰∫éÂØπbug‰∏•ÈáçÊÄßÂíå‰ºòÂÖàÁ∫ßÁöÑËØØËß£„ÄÅÂØπÂ∑•ÂÖ∑ÁöÑ‰∏çÊª°‰ª•Âèä‰∏ì‰∏öÊ≤üÈÄöÁöÑÁº∫Â§±„ÄÇËøô‰∫õÊØíÊÄß‰∫íÂä®‰∏ç‰ªÖÂΩ±Âìç‰∫ÜÊúâÊïàËÆ®ËÆ∫ÔºåËøòÈôç‰Ωé‰∫ÜÂ∞ÜÈóÆÈ¢ò‰∏éÊãâÂèñËØ∑Ê±ÇÂÖ≥ËÅîÁöÑÂèØËÉΩÊÄßÔºåÂõ†Ê≠§ÊèêÂá∫‰∫ÜÂáèÂ∞ëÊØíÊÄß‰ª•ÊîπÂñÑbugËß£ÂÜ≥ÁöÑÂª∫ËÆÆ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.09905', 'title': "Quantization for OpenAI's Whisper Models: A Comparative Analysis", 'url': 'https://huggingface.co/papers/2503.09905', 'abstract': 'Automated speech recognition (ASR) models have gained prominence for applications such as captioning, speech translation, and live transcription. This paper studies Whisper and two model variants: one optimized for live speech streaming and another for offline transcription. Notably, these models have been found to generate hallucinated content, reducing transcription reliability. Furthermore, larger model variants exhibit increased latency and pose challenges for deployment on resource-constrained devices. This study analyzes the similarities and differences between three Whisper models, qualitatively examining their distinct capabilities. Next, this study quantifies the impact of model quantization on latency and evaluates its viability for edge deployment. Using the open source LibriSpeech dataset, this paper evaluates the word error rate (WER) along with latency analysis of whispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that quantization reduces latency by 19\\% and model size by 45\\%, while preserving transcription accuracy. These findings provide insights into the optimal use cases of different Whisper models and edge device deployment possibilities. All code, datasets, and implementation details are available in a public GitHub repository: https://github.com/allisonandreyev/WhisperQuantization.git', 'score': 1, 'issue_id': 2700, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 –º–∞—Ä—Ç–∞', 'en': 'March 12', 'zh': '3Êúà12Êó•'}, 'hash': '8d7b17a97a4cbb6e', 'authors': ['Allison Andreyev'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.09905.jpg', 'data': {'categories': ['#inference', '#audio', '#open_source', '#optimization', '#hallucinations', '#dataset'], 'emoji': 'üéôÔ∏è', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π Whisper –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –Ω–∞ –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ Whisper –∏ –∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –∏ –æ—Ñ–ª–∞–π–Ω-—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –≤—ã–≤–æ–¥–µ –º–æ–¥–µ–ª–µ–π –∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ü—Ä–æ–≤–æ–¥–∏—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –≤–ª–∏—è–Ω–∏—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–¥–µ—Ä–∂–∫—É –∏ —Ç–æ—á–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö LibriSpeech. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ —É–º–µ–Ω—å—à–∞–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –Ω–∞ 19% –∏ —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –Ω–∞ 45%, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Ç–æ—á–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.'}, 'en': {'title': 'Optimizing Whisper Models for Efficient Speech Recognition', 'desc': 'This paper investigates the performance of Whisper, an automated speech recognition (ASR) model, along with its two variants tailored for live streaming and offline transcription. It highlights the issue of hallucinated content in the transcriptions, which can compromise reliability, especially in larger models that also face latency challenges. The study further explores the effects of model quantization on latency and size, demonstrating a significant reduction in both while maintaining transcription accuracy. By analyzing the word error rate (WER) using the LibriSpeech dataset, the research provides valuable insights for deploying Whisper models on resource-constrained devices.'}, 'zh': {'title': '‰ºòÂåñWhisperÊ®°ÂûãÔºåÊèêÂçáËØ≠Èü≥ËØÜÂà´ÊïàÁéá', 'desc': 'ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜËá™Âä®ËØ≠Èü≥ËØÜÂà´ÔºàASRÔºâÊ®°ÂûãWhisperÂèäÂÖ∂‰∏§‰∏™Âèò‰ΩìÔºå‰∏Ä‰∏™ÈíàÂØπÂÆûÊó∂ËØ≠Èü≥ÊµÅ‰ºòÂåñÔºåÂè¶‰∏Ä‰∏™Áî®‰∫éÁ¶ªÁ∫øËΩ¨ÂΩï„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåËøô‰∫õÊ®°ÂûãÂèØËÉΩ‰ºöÁîüÊàêËôöÂÅáÂÜÖÂÆπÔºå‰ªéËÄåÈôç‰ΩéËΩ¨ÂΩïÁöÑÂèØÈù†ÊÄß„ÄÇÊ≠§Â§ñÔºåËæÉÂ§ßÁöÑÊ®°ÂûãÂèò‰ΩìÂú®Âª∂ËøüÊñπÈù¢Ë°®Áé∞ËæÉÂ∑ÆÔºåÁªôËµÑÊ∫êÂèóÈôêÁöÑËÆæÂ§áÈÉ®ÁΩ≤Â∏¶Êù•‰∫ÜÊåëÊàò„ÄÇÈÄöËøáÂØπÊØîÂàÜÊûêÔºåËÆ∫ÊñáÈáèÂåñ‰∫ÜÊ®°ÂûãÈáèÂåñÂØπÂª∂ËøüÁöÑÂΩ±ÂìçÔºåÂπ∂ËØÑ‰º∞‰∫ÜÂÖ∂Âú®ËæπÁºòËÆæÂ§á‰∏äÁöÑÂèØË°åÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10615', 'title': 'R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization', 'url': 'https://huggingface.co/papers/2503.10615', 'abstract': 'Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks.', 'score': 0, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': '857ef8e4c110da7f', 'authors': ['Yi Yang', 'Xiaoxuan He', 'Hongkun Pan', 'Xiyan Jiang', 'Yan Deng', 'Xingtao Yang', 'Haoyu Lu', 'Dacheng Yin', 'Fengyun Rao', 'Minfeng Zhu', 'Bo Zhang', 'Wei Chen'], 'affiliations': ['Renmin University of China', 'WeChat Vision, Tencent Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10615.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#rl', '#reasoning', '#training'], 'emoji': 'üß†', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –±–∞—Ä—å–µ—Ä –º–µ–∂–¥—É –∑—Ä–µ–Ω–∏–µ–º –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –≤ –ò–ò', 'desc': 'R1-Onevision - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—é—â–∞—è —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –≤–∏–∑—É–∞–ª—å–Ω—ã–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º –∏ –≥–ª—É–±–æ–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω–≤–µ–π–µ—Ä –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—â–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç R1-Onevision —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –ø–æ—à–∞–≥–æ–≤—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏–∑ —Ä–∞–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π. –ú–æ–¥–µ–ª—å R1-Onevision, –æ–±—É—á–µ–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö.'}, 'en': {'title': 'Bridging Visual and Textual Reasoning with R1-Onevision', 'desc': 'This paper presents R1-Onevision, a new model for multimodal reasoning that combines visual and textual information. It addresses the limitations of existing visual-language models by introducing a cross-modal reasoning pipeline that converts images into structured text representations. The authors also create the R1-Onevision dataset, which includes detailed annotations for multimodal reasoning tasks across various domains. Additionally, they establish R1-Onevision-Bench, a benchmark for evaluating multimodal reasoning abilities at different educational levels, demonstrating that their model outperforms others like GPT-4o and Qwen2.5-VL.'}, 'zh': {'title': 'R1-OnevisionÔºöËßÜËßâ‰∏éËØ≠Ë®ÄÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫R1-OnevisionÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ®°ÂûãÔºåÊó®Âú®Ëß£ÂÜ≥ËßÜËßâÂíåÊñáÊú¨‰ø°ÊÅØÊï¥ÂêàÁöÑÊåëÊàò„ÄÇËØ•Ê®°ÂûãÈÄöËøáË∑®Ê®°ÊÄÅÊé®ÁêÜÁÆ°ÈÅìÔºåÂ∞ÜÂõæÂÉèËΩ¨Êç¢‰∏∫Ê≠£ÂºèÁöÑÊñáÊú¨Ë°®Á§∫Ôºå‰ªéËÄåÂÆûÁé∞Á≤æÁ°ÆÁöÑÂü∫‰∫éËØ≠Ë®ÄÁöÑÊé®ÁêÜ„ÄÇÊàë‰ª¨ËøòÊûÑÂª∫‰∫ÜR1-OnevisionÊï∞ÊçÆÈõÜÔºåÊèê‰æõ‰∫ÜËØ¶ÁªÜÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÊ≥®ÈáäÔºå‰ª•ÊîØÊåÅ‰∏çÂêåÈ¢ÜÂüüÁöÑÁ†îÁ©∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåR1-OnevisionÂú®Â§ö‰∏™Â§çÊùÇÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂÖàËøõÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.10460', 'title': 'Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond', 'url': 'https://huggingface.co/papers/2503.10460', 'abstract': 'This paper presents our work on the Light-R1 series, with models, data, and code all released.   We first focus on training long COT models from scratch, specifically starting from models initially lacking long COT capabilities. Using a curriculum training recipe consisting of two-stage SFT and semi-on-policy DPO, we train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in superior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite being trained exclusively on math data, Light-R1-32B shows strong generalization across other domains. In the subsequent phase of this work, we highlight the significant benefit of the 3k dataset constructed for the second SFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled models using this dataset, we obtain new SOTA models in 7B and 14B, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying reinforcement learning, specifically GRPO, on long-COT models to further improve reasoning performance. We successfully train our final Light-R1-14B-DS with RL, achieving SOTA performance among 14B parameter models in math. With AIME24 & 25 scores of 74.0 and 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and DeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected behavior, showing simultaneous increase in response length and reward score.   The Light-R1 series of work validates training long-COT models from scratch, showcases the art in SFT data and releases SOTA models from RL.', 'score': 0, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 –º–∞—Ä—Ç–∞', 'en': 'March 13', 'zh': '3Êúà13Êó•'}, 'hash': '503c1d89e949bb41', 'authors': ['Liang Wen', 'Yunke Cai', 'Fenrui Xiao', 'Xin He', 'Qi An', 'Zhenyu Duan', 'Yimin Du', 'Junchen Liu', 'Lifu Tang', 'Xiaowei Lv', 'Haosheng Zou', 'Yongchao Deng', 'Shousheng Jia', 'Xiangzheng Zhang'], 'affiliations': ['Qiyuan Tech', 'Renmin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10460.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#rl', '#reasoning', '#training', '#optimization', '#long_context', '#open_source'], 'emoji': 'üß†', 'ru': {'title': '–ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ò –¥–ª–∏–Ω–Ω—ã–º —Ü–µ–ø–æ—á–∫–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π Light-R1, –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤—ã–ø–æ–ª–Ω—è—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (COT) —Å –Ω—É–ª—è. –ò—Å–ø–æ–ª—å–∑—É—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (SFT) –∏ –ø–æ–ª—É-–æ–Ω–ª–∞–π–Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ (DPO), –∞–≤—Ç–æ—Ä—ã –ø–æ–ª—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å Light-R1-32B, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â—É—é –∞–Ω–∞–ª–æ–≥–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –î–∞–ª—å–Ω–µ–π—à–µ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å Light-R1-14B-DS, –¥–æ—Å—Ç–∏–≥—à—É—é –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–µ–¥–∏ 14B –º–æ–¥–µ–ª–µ–π –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –¥–ª–∏–Ω–Ω—ã–º COT —Å –Ω—É–ª—è –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è SFT.'}, 'en': {'title': 'Revolutionizing Long COT Models with Light-R1 Series', 'desc': "This paper introduces the Light-R1 series, focusing on training long Chain of Thought (COT) models from scratch. The authors employ a two-stage Supervised Fine-Tuning (SFT) and semi-on-policy Direct Preference Optimization (DPO) to enhance the model's math capabilities, resulting in the Light-R1-32B model that outperforms existing models. They also demonstrate that a specially constructed 3k dataset significantly boosts the performance of other models, achieving state-of-the-art (SOTA) results in various parameter sizes. Additionally, the application of reinforcement learning (RL) further improves reasoning performance, with the Light-R1-14B-DS model achieving impressive scores in math tasks, surpassing even larger models."}, 'zh': {'title': '‰ªéÈõ∂ÂºÄÂßãËÆ≠ÁªÉÈïøÈìæÊé®ÁêÜÊ®°ÂûãÁöÑÊàêÂäü‰πãË∑Ø', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜLight-R1Á≥ªÂàóÁöÑÁ†îÁ©∂Â∑•‰ΩúÔºåÂèëÂ∏É‰∫ÜÊ®°Âûã„ÄÅÊï∞ÊçÆÂíå‰ª£Á†Å„ÄÇÊàë‰ª¨‰∏ìÊ≥®‰∫é‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉÈïøÈìæÊé®ÁêÜÔºàCOTÔºâÊ®°ÂûãÔºåÈááÁî®‰∏§Èò∂ÊÆµÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåÂçäÂú®Á∫øÁ≠ñÁï•‰ºòÂåñÔºàDPOÔºâËøõË°åËØæÁ®ãËÆ≠ÁªÉ„ÄÇLight-R1-32BÊ®°ÂûãÂú®Êï∞Â≠¶ÊÄßËÉΩ‰∏ä‰ºò‰∫éDeepSeek-R1-Distill-Qwen-32BÔºåÂ∞ΩÁÆ°‰ªÖÂú®Êï∞Â≠¶Êï∞ÊçÆ‰∏äËÆ≠ÁªÉÔºå‰ΩÜÂú®ÂÖ∂‰ªñÈ¢ÜÂüü‰πüË°®Áé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËøõ‰∏ÄÊ≠•ÊèêÂçáÊé®ÁêÜÊÄßËÉΩÔºåÊàë‰ª¨ÊàêÂäüËÆ≠ÁªÉ‰∫ÜLight-R1-14B-DSÔºåËææÂà∞‰∫Ü14BÂèÇÊï∞Ê®°Âûã‰∏≠ÁöÑÊúÄÊñ∞ÊäÄÊúØÊ∞¥Âπ≥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2503.09642', 'title': 'Open-Sora 2.0: Training a Commercial-Level Video Generation Model in\n  $200k', 'url': 'https://huggingface.co/papers/2503.09642', 'abstract': 'Video generation models have achieved remarkable progress in the past year. The quality of AI video continues to improve, but at the cost of larger model size, increased data quantity, and greater demand for training compute. In this report, we present Open-Sora 2.0, a commercial-level video generation model trained for only $200k. With this model, we demonstrate that the cost of training a top-performing video generation model is highly controllable. We detail all techniques that contribute to this efficiency breakthrough, including data curation, model architecture, training strategy, and system optimization. According to human evaluation results and VBench scores, Open-Sora 2.0 is comparable to global leading video generation models including the open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By making Open-Sora 2.0 fully open-source, we aim to democratize access to advanced video generation technology, fostering broader innovation and creativity in content creation. All resources are publicly available at: https://github.com/hpcaitech/Open-Sora.', 'score': 0, 'issue_id': 2701, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 –º–∞—Ä—Ç–∞', 'en': 'March 12', 'zh': '3Êúà12Êó•'}, 'hash': '8987bc0bbdde3b5a', 'authors': ['Xiangyu Peng', 'Zangwei Zheng', 'Chenhui Shen', 'Tom Young', 'Xinying Guo', 'Binluo Wang', 'Hang Xu', 'Hongxin Liu', 'Mingyan Jiang', 'Wenjun Li', 'Yuhui Wang', 'Anbang Ye', 'Gang Ren', 'Qianran Ma', 'Wanying Liang', 'Xiang Lian', 'Xiwen Wu', 'Yuting Zhong', 'Zhuangyan Li', 'Chaoyu Gong', 'Guojun Lei', 'Leijun Cheng', 'Limin Zhang', 'Minghao Li', 'Ruijie Zhang', 'Silan Hu', 'Shijie Huang', 'Xiaokang Wang', 'Yuanheng Zhao', 'Yuqi Wang', 'Ziang Wei', 'Yang You'], 'affiliations': ['HPC-AI Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.09642.jpg', 'data': {'categories': ['#training', '#optimization', '#open_source', '#architecture', '#video', '#data'], 'emoji': 'üé¨', 'ru': {'title': '–î–æ—Å—Ç—É–ø–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ: –∫–∞—á–µ—Å—Ç–≤–æ –º–∏—Ä–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø–æ —Ä–∞–∑—É–º–Ω–æ–π —Ü–µ–Ω–µ', 'desc': 'Open-Sora 2.0 - —ç—Ç–æ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –æ–±—É—á–µ–Ω–Ω–∞—è –≤—Å–µ–≥–æ –∑–∞ $200 —Ç—ã—Å. –û–Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ —Å–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–ª–∏ —Ä—è–¥ —Ç–µ—Ö–Ω–∏–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –≤–∫–ª—é—á–∞—è –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è. –ü–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º VBench, Open-Sora 2.0 —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–∞ —Å –≤–µ–¥—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Affordable Excellence in Video Generation', 'desc': 'Open-Sora 2.0 is a new video generation model that has been developed to produce high-quality videos while keeping training costs low. It was trained for only $200,000, showcasing that effective video generation can be achieved without massive resources. The model incorporates various techniques such as optimized data curation, innovative model architecture, and efficient training strategies to enhance performance. By making Open-Sora 2.0 open-source, the authors aim to provide wider access to advanced video generation tools, encouraging creativity and innovation in the field.'}, 'zh': {'title': 'Open-Sora 2.0ÔºöÈ´òÊïàËßÜÈ¢ëÁîüÊàêÁöÑÊú™Êù•', 'desc': 'ËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®ËøáÂéª‰∏ÄÂπ¥ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇËôΩÁÑ∂AIËßÜÈ¢ëÁöÑË¥®Èáè‰∏çÊñ≠ÊèêÈ´òÔºå‰ΩÜËøô‰πüÂØºËá¥‰∫ÜÊ®°ÂûãËßÑÊ®°Â¢ûÂ§ß„ÄÅÊï∞ÊçÆÈáèÂ¢ûÂä†ÂíåËÆ≠ÁªÉËÆ°ÁÆóÈúÄÊ±ÇÂä†Â§ß„ÄÇÊàë‰ª¨‰ªãÁªç‰∫ÜOpen-Sora 2.0ÔºåËøôÊòØ‰∏Ä‰∏™ÂïÜ‰∏öÁ∫ßÁöÑËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºå‰ªÖÁî®20‰∏áÁæéÂÖÉËøõË°åËÆ≠ÁªÉ„ÄÇÈÄöËøá‰ºòÂåñÊï∞ÊçÆÁÆ°ÁêÜ„ÄÅÊ®°ÂûãÊû∂ÊûÑ„ÄÅËÆ≠ÁªÉÁ≠ñÁï•ÂíåÁ≥ªÁªüÊÄßËÉΩÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÈ´òÊïàËÆ≠ÁªÉÈ°∂Á∫ßËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑÂèØÊéßÊÄßÔºåÂπ∂Â∏åÊúõÈÄöËøáÂºÄÊ∫êËøô‰∏ÄÊ®°ÂûãÊù•‰øÉËøõÂÜÖÂÆπÂàõ‰ΩúÁöÑÂàõÊñ∞‰∏éÂèëÂ±ï„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (4)', '#agi', '#alignment', '#architecture (2)', '#audio (1)', '#benchmark (6)', '#cv (3)', '#data (6)', '#dataset (8)', '#diffusion (4)', '#ethics (1)', '#games (2)', '#graphs (1)', '#hallucinations (1)', '#healthcare', '#inference (3)', '#interpretability', '#leakage', '#long_context (3)', '#low_resource', '#machine_translation (1)', '#math', '#multilingual (1)', '#multimodal (7)', '#open_source (5)', '#optimization (8)', '#plp', '#rag', '#reasoning (7)', '#rl (4)', '#rlhf (1)', '#robotics', '#science', '#security (1)', '#small_models', '#story_generation (1)', '#survey', '#synthetic (1)', '#training (6)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2025-03-14 04:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-14 04:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-14 04:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    