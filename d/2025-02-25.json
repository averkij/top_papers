{
    "date": {
        "ru": "25 февраля",
        "en": "February 25",
        "zh": "2月25日"
    },
    "time_utc": "2025-02-25 04:13",
    "weekday": 1,
    "issue_id": 2387,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.16614",
            "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models",
            "url": "https://huggingface.co/papers/2502.16614",
            "abstract": "The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks have been proposed. However, existing critique benchmarks usually have the following limitations: (1). Focusing on diverse reasoning tasks in general domains and insufficient evaluation on code tasks (e.g., only covering code generation task), where the difficulty of queries is relatively easy (e.g., the code queries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive evaluation from different dimensions. To address these limitations, we introduce a holistic code critique benchmark for LLMs called CodeCriticBench. Specifically, our CodeCriticBench includes two mainstream code tasks (i.e., code generation and code QA) with different difficulties. Besides, the evaluation protocols include basic critique evaluation and advanced critique evaluation for different characteristics, where fine-grained evaluation checklists are well-designed for advanced settings. Finally, we conduct extensive experimental results of existing LLMs, which show the effectiveness of CodeCriticBench.",
            "score": 7,
            "issue_id": 2386,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 февраля",
                "en": "February 23",
                "zh": "2月23日"
            },
            "hash": "c70e868ad4c1b726",
            "authors": [
                "Alexander Zhang",
                "Marcus Dong",
                "Jiaheng Liu",
                "Wei Zhang",
                "Yejie Wang",
                "Jian Yang",
                "Ge Zhang",
                "Tianyu Liu",
                "Zhongyuan Peng",
                "Yingshui Tan",
                "Yuanxing Zhang",
                "Zhexu Wang",
                "Weixun Wang",
                "Yancheng He",
                "Ken Deng",
                "Wangchunshu Zhou",
                "Wenhao Huang",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "Alibaba",
                "BUAA",
                "BUPT",
                "CASIA",
                "Kuaishou",
                "M-A-P",
                "NJU",
                "OPPO"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16614.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "CodeCriticBench: комплексная оценка критических способностей LLM в задачах работы с кодом",
                    "desc": "Статья представляет новый бенчмарк CodeCriticBench для оценки способностей больших языковых моделей (LLM) критиковать код. Бенчмарк включает задачи генерации кода и ответов на вопросы о коде разной сложности. CodeCriticBench предлагает базовую и продвинутую оценку критических способностей с использованием детальных чек-листов. Авторы провели обширные эксперименты с существующими LLM, показавшие эффективность предложенного бенчмарка."
                },
                "en": {
                    "title": "Enhancing Code Critique with CodeCriticBench",
                    "desc": "This paper introduces CodeCriticBench, a new benchmark designed to evaluate the critique capacity of Large Language Models (LLMs) specifically in the context of code tasks. Unlike existing benchmarks that primarily focus on general reasoning tasks, CodeCriticBench encompasses both code generation and code question-answering tasks, offering a range of difficulties. The evaluation framework includes both basic and advanced critique assessments, utilizing detailed checklists to ensure comprehensive analysis. Experimental results demonstrate that CodeCriticBench effectively measures the critique abilities of various LLMs, highlighting its importance in enhancing model reasoning capabilities."
                },
                "zh": {
                    "title": "全面评估LLMs的代码批评能力",
                    "desc": "大型语言模型（LLMs）的批评能力对于推理能力至关重要，可以提供必要的建议，如详细分析和建设性反馈。为了评估LLMs的批评能力，研究者们提出了多个批评基准，但现有基准存在一些局限性，例如对代码任务的评估不足。为了解决这些问题，我们引入了一个全面的代码批评基准，称为CodeCriticBench，涵盖了代码生成和代码问答两种主流任务，并设计了细致的评估标准。通过对现有LLMs的广泛实验结果，我们验证了CodeCriticBench的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17157",
            "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
            "url": "https://huggingface.co/papers/2502.17157",
            "abstract": "Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonstrate that DICEPTION effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models. We achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images). Inspired by Wang et al., DICEPTION formulates the outputs of various perception tasks using color encoding; and we show that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation. Unifying various perception tasks as conditional image generation enables us to fully leverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently trained at a cost of orders of magnitude lower, compared to conventional models that were trained from scratch. When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION provides valuable insights and a more promising solution for visual generalist models.",
            "score": 6,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "f8790b30bb553397",
            "authors": [
                "Canyu Zhao",
                "Mingyu Liu",
                "Huanyi Zheng",
                "Muzhi Zhu",
                "Zhiyue Zhao",
                "Hao Chen",
                "Tong He",
                "Chunhua Shen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17157.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#diffusion",
                    "#cv",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Универсальное восприятие через диффузию текста в изображения",
                    "desc": "Статья представляет DICEPTION - универсальную модель восприятия, основанную на предобученных диффузионных моделях преобразования текста в изображение. DICEPTION эффективно решает множество задач восприятия, достигая производительности на уровне современных моделей, но используя значительно меньше данных и вычислительных ресурсов. Модель кодирует выходные данные различных задач восприятия с помощью цветового кодирования, что оказывается эффективным для сегментации объектов и семантической сегментации. DICEPTION может быть адаптирована к новым задачам путем дообучения на небольшом количестве изображений, что делает ее перспективным решением для создания универсальных моделей компьютерного зрения."
                },
                "en": {
                    "title": "DICEPTION: Efficient Generalist Perception with Minimal Data",
                    "desc": "This paper presents DICEPTION, a versatile perception model designed to perform multiple tasks efficiently while minimizing the need for extensive computational resources and training data. By leveraging pre-trained text-to-image diffusion models, DICEPTION achieves competitive performance on various perception tasks using only a fraction of the data required by traditional models. The innovative use of color encoding for output representation enhances the model's effectiveness in both entity and semantic segmentation. Overall, DICEPTION demonstrates that it is possible to create a powerful generalist model that requires minimal fine-tuning and can adapt quickly to new tasks."
                },
                "zh": {
                    "title": "DICEPTION：高效的通用感知模型",
                    "desc": "本文的主要目标是创建一个通用的感知模型，能够在计算资源和训练数据有限的情况下处理多种任务。我们使用了在数十亿张图像上预训练的文本到图像扩散模型。通过全面的评估指标，DICEPTION在多个感知任务上表现出色，达到了与最先进模型相当的性能。该模型在适应其他任务时，仅需对50张图像和1%的参数进行微调，显示出其高效性和潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17129",
            "title": "Thus Spake Long-Context Large Language Model",
            "url": "https://huggingface.co/papers/2502.17129",
            "abstract": "Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs) giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is accompanied by numerous obstacles. Nevertheless, long context remains a core competitive advantage for LLMs. In the past two years, the context length of LLMs has achieved a breakthrough extension to millions of tokens. Moreover, the research on long-context LLMs has expanded from length extrapolation to a comprehensive focus on architecture, infrastructure, training, and evaluation technologies.   Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy between the journey of extending the context of LLM and the attempts of humans to transcend its mortality. In this survey, We will illustrate how LLM struggles between the tremendous need for a longer context and its equal need to accept the fact that it is ultimately finite. To achieve this, we give a global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation, showcasing the full spectrum of long-context technologies. At the end of this survey, we will present 10 unanswered questions currently faced by long-context LLMs. We hope this survey can serve as a systematic introduction to the research on long-context LLMs.",
            "score": 6,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "8b44dbb5e39d9b38",
            "authors": [
                "Xiaoran Liu",
                "Ruixiao Li",
                "Mianqiu Huang",
                "Zhigeng Liu",
                "Yuerong Song",
                "Qipeng Guo",
                "Siyang He",
                "Qiqi Wang",
                "Linlin Li",
                "Qun Liu",
                "Yaqian Zhou",
                "Xuanjing Huang",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "School of Computer Science Fudan University",
                "Shanghai AI Lab",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17129.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context",
                    "#survey",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Преодолевая границы: путь к LLM с длинным контекстом",
                    "desc": "Данная статья представляет обзор исследований в области обработки длинного контекста в больших языковых моделях (LLM). Авторы рассматривают жизненный цикл LLM с длинным контекстом с точки зрения архитектуры, инфраструктуры, обучения и оценки. В работе проводится аналогия между стремлением расширить контекст LLM и попытками человека преодолеть свою смертность. Статья завершается 10 открытыми вопросами, стоящими перед LLM с длинным контекстом."
                },
                "en": {
                    "title": "Unlocking the Power of Long Context in Language Models",
                    "desc": "This paper discusses the importance of long context in Natural Language Processing (NLP) and its impact on Large Language Models (LLMs). It highlights the advancements in extending context length to millions of tokens, which enhances the models' capabilities. The authors explore the challenges faced by LLMs in balancing the need for longer context with their inherent limitations. Additionally, the paper provides a comprehensive overview of the lifecycle of long-context LLMs, covering aspects such as architecture, infrastructure, training, and evaluation, while also posing ten critical questions for future research."
                },
                "zh": {
                    "title": "长上下文：大型语言模型的核心竞争力",
                    "desc": "长上下文是自然语言处理（NLP）中的一个重要主题，对大型语言模型（LLMs）的发展具有重要意义。尽管追求长上下文面临许多挑战，但它仍然是LLMs的核心竞争优势。近年来，LLMs的上下文长度已突破到数百万个标记，研究也从长度延展扩展到架构、基础设施、训练和评估技术的全面关注。本文将从四个角度展示长上下文LLMs的生命周期，并提出当前面临的十个未解问题，以期为长上下文LLMs的研究提供系统性的介绍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.17110",
            "title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration",
            "url": "https://huggingface.co/papers/2502.17110",
            "abstract": "The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we introduce Mobile-Agent-V, a framework that leverages video guidance to provide rich and cost-effective operational knowledge for mobile automation. Mobile-Agent-V enhances task execution capabilities by leveraging video inputs without requiring specialized sampling or preprocessing. Mobile-Agent-V integrates a sliding window strategy and incorporates a video agent and deep-reflection agent to ensure that actions align with user instructions. Through this innovative approach, users can record task processes with guidance, enabling the system to autonomously learn and execute tasks efficiently. Experimental results show that Mobile-Agent-V achieves a 30% performance improvement compared to existing frameworks.",
            "score": 4,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "1900233d7723f824",
            "authors": [
                "Junyang Wang",
                "Haiyang Xu",
                "Xi Zhang",
                "Ming Yan",
                "Ji Zhang",
                "Fei Huang",
                "Jitao Sang"
            ],
            "affiliations": [
                "Alibaba Group",
                "Beijing Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.17110.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#agents"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "Видео-инструкции для ИИ: революция в автоматизации мобильных устройств",
                    "desc": "Mobile-Agent-V - это новая система автоматизации мобильных устройств, использующая видеоинструкции для обучения. Она применяет стратегию скользящего окна и включает видеоагента и агента глубокой рефлексии для выполнения задач в соответствии с указаниями пользователя. Система позволяет пользователям записывать процессы выполнения задач, что дает возможность автономного обучения и эффективного выполнения. Экспериментальные результаты показывают 30% улучшение производительности по сравнению с существующими фреймворками."
                },
                "en": {
                    "title": "Empowering Mobile Automation with Video Guidance",
                    "desc": "The paper presents Mobile-Agent-V, a novel framework designed to enhance task management on mobile devices through automation. It addresses the limitations of existing AI frameworks that lack sufficient operational knowledge by utilizing video guidance to provide rich, actionable insights. The framework employs a sliding window strategy along with a video agent and deep-reflection agent to ensure that the system's actions are in line with user instructions. Experimental results demonstrate that Mobile-Agent-V significantly improves performance by 30% over traditional methods, making it a more efficient solution for mobile automation."
                },
                "zh": {
                    "title": "移动自动化的新突破：Mobile-Agent-V",
                    "desc": "随着移动设备使用的快速增长，任务管理的自动化需求也在增加。许多基于人工智能的框架由于缺乏足够的操作知识而面临挑战。我们提出的Mobile-Agent-V框架利用视频指导提供丰富且经济的操作知识，从而改善移动自动化。通过集成滑动窗口策略和视频代理，Mobile-Agent-V能够高效地学习和执行任务，实验结果显示其性能比现有框架提高了30%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16894",
            "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment",
            "url": "https://huggingface.co/papers/2502.16894",
            "abstract": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose Great LoRA Mixture-of-Expert (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT's state-of-the-art performance, closing the gap with Full FT.",
            "score": 3,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "020c0f54f92a9238",
            "authors": [
                "Chenghao Fan",
                "Zhenyi Lu",
                "Sichen Liu",
                "Xiaoye Qu",
                "Wei Wei",
                "Chengfeng Gu",
                "Yu Cheng"
            ],
            "affiliations": [
                "School of Computer Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16894.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "🐐",
                "ru": {
                    "title": "GOAT: Эффективная низкоранговая адаптация на уровне полной тонкой настройки",
                    "desc": "Этa статья представляет новый метод под названием GOAT (Great LoRA Mixture-of-Expert) для улучшения эффективности низкоранговой адаптации (LoRA) в больших языковых моделях. GOAT использует адаптивную интеграцию релевантных приоров с помощью SVD-структурированной архитектуры Mixture-of-Experts и теоретически обоснованный масштабирующий фактор. Метод позволяет преодолеть ограничения стандартной LoRA и приблизиться по эффективности к полной тонкой настройке модели. Эксперименты на 25 наборах данных показали превосходство GOAT над существующими методами в различных задачах обработки естественного языка и компьютерного зрения."
                },
                "en": {
                    "title": "Boosting LoRA with GOAT: A New Path to Efficiency in LLMs",
                    "desc": "This paper introduces Great LoRA Mixture-of-Expert (GOAT), a new framework designed to enhance the performance of Low-Rank Adaptation (LoRA) for Large Language Models (LLMs). GOAT addresses the limitations of existing methods by integrating adaptive priors through a singular value decomposition (SVD)-structured Mixture-of-Experts (MoE) architecture. It also aligns the optimization process with that of fully fine-tuned MoE models by introducing a theoretical scaling factor. The results show that GOAT significantly improves efficiency and performance across various tasks, effectively bridging the gap between LoRA and Full Fine-Tuning."
                },
                "zh": {
                    "title": "提升LoRA性能的全新框架：GOAT",
                    "desc": "本文提出了一种名为Great LoRA Mixture-of-Expert（GOAT）的框架，旨在提高低秩适应（LoRA）在大型语言模型（LLMs）中的性能。GOAT通过自适应整合相关的先验知识，使用奇异值分解（SVD）结构的专家混合（MoE）来优化LoRA。该框架还通过推导理论缩放因子，使优化过程与完全微调（Full FT）的MoE对齐。实验结果表明，GOAT在25个数据集上的表现优于现有方法，缩小了与完全微调的差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16033",
            "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
            "url": "https://huggingface.co/papers/2502.16033",
            "abstract": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.",
            "score": 3,
            "issue_id": 2386,
            "pub_date": "2025-02-22",
            "pub_date_card": {
                "ru": "22 февраля",
                "en": "February 22",
                "zh": "2月22日"
            },
            "hash": "3e5fc69b8713e252",
            "authors": [
                "Qianqi Yan",
                "Yue Fan",
                "Hongquan Li",
                "Shan Jiang",
                "Yang Zhao",
                "Xinze Guan",
                "Ching-Chen Kuo",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "University of California, Santa Cruz",
                "eBay"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16033.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый рубеж в мультимодальном анализе: оценка способности ИИ распознавать несоответствия",
                    "desc": "Статья представляет новый бенчмарк MMIR для оценки способности мультимодальных больших языковых моделей (MLLM) обнаруживать и анализировать семантические несоответствия в визуально-текстовом контенте. MMIR включает 534 сложных примера с синтетически внедренными ошибками в пяти категориях, требующих рассуждений. Авторы оценили шесть современных MLLM, показав, что модели с специализированными возможностями мультимодальных рассуждений значительно превосходят другие, в то время как модели с открытым исходным кодом особенно уязвимы к ошибкам несоответствия. Результаты исследования подчеркивают необходимость улучшения мультимодальных рассуждений в MLLM."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning: Tackling Inconsistencies in Real-World Content",
                    "desc": "This paper introduces the Multimodal Inconsistency Reasoning (MMIR) benchmark to evaluate how well Multimodal Large Language Models (MLLMs) can identify and reason about inconsistencies in complex visual-textual content. The benchmark consists of 534 samples with various types of semantic mismatches, such as factual contradictions and contextual mismatches. The study finds that models designed for multimodal reasoning perform significantly better than others, but many open-source models struggle with inconsistencies, especially those that span multiple modalities. The results indicate a need for improved cross-modal reasoning capabilities in MLLMs, as current methods show limited effectiveness in handling complex layouts and cross-modal conflicts."
                },
                "zh": {
                    "title": "提升多模态推理能力，解决现实世界的不一致性",
                    "desc": "现有的多模态大型语言模型（MLLMs）主要在一致的视觉-文本输入上进行训练和测试，尚不清楚它们能否处理现实世界中布局丰富内容的不一致性。为此，我们提出了多模态不一致性推理（MMIR）基准，以评估MLLMs在检测和推理语义不匹配方面的能力。MMIR包含534个具有挑战性的样本，涵盖五个推理密集型类别的合成错误。我们的研究表明，具备专门多模态推理能力的模型在处理不一致性时表现优异，而开源模型则特别容易受到不一致性错误的影响。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16922",
            "title": "Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties",
            "url": "https://huggingface.co/papers/2502.16922",
            "abstract": "Temporal reasoning is fundamental to human cognition and is crucial for various real-world applications. While recent advances in Large Language Models have demonstrated promising capabilities in temporal reasoning, existing benchmarks primarily rely on rule-based construction, lack contextual depth, and involve a limited range of temporal entities. To address these limitations, we introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate LLMs on temporal reasoning within the extensive scope of Chinese dynastic chronology. CTM emphasizes cross-entity relationships, pairwise temporal alignment, and contextualized and culturally-grounded reasoning, providing a comprehensive evaluation. Extensive experimental results reveal the challenges posed by CTM and highlight potential avenues for improvement.",
            "score": 2,
            "issue_id": 2387,
            "pub_date": "2025-02-24",
            "pub_date_card": {
                "ru": "24 февраля",
                "en": "February 24",
                "zh": "2月24日"
            },
            "hash": "8e0389428d77685b",
            "authors": [
                "Zhenglin Wang",
                "Jialong Wu",
                "Pengfei LI",
                "Yong Jiang",
                "Deyu Zhou"
            ],
            "affiliations": [
                "School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16922.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "⏳",
                "ru": {
                    "title": "CTM: Новый рубеж в оценке временных рассуждений языковых моделей",
                    "desc": "Статья представляет новый бенчмарк для оценки способностей больших языковых моделей (LLM) в области временных рассуждений на основе китайской династической хронологии. Бенчмарк Chinese Time Reasoning (CTM) фокусируется на отношениях между временными сущностями, попарном временном выравнивании и контекстуализированных рассуждениях с учетом культурных особенностей. CTM преодолевает ограничения существующих бенчмарков, которые часто основаны на правилах и имеют ограниченный диапазон временных сущностей. Экспериментальные результаты выявляют сложности, связанные с CTM, и указывают на потенциальные направления для улучшения временных рассуждений в LLM."
                },
                "en": {
                    "title": "Enhancing Temporal Reasoning with Chinese Time Benchmark",
                    "desc": "This paper presents the Chinese Time Reasoning (CTM) benchmark, which aims to enhance the evaluation of Large Language Models (LLMs) in the area of temporal reasoning. Unlike existing benchmarks that are rule-based and limited in scope, CTM focuses on the rich context of Chinese dynastic history, allowing for a deeper assessment of temporal relationships. It emphasizes the importance of cross-entity relationships and pairwise temporal alignment, ensuring that the reasoning is both contextualized and culturally relevant. The results from extensive experiments indicate significant challenges for LLMs in this domain, suggesting areas for future research and improvement."
                },
                "zh": {
                    "title": "中文时间推理：提升机器学习的时间理解能力",
                    "desc": "时间推理是人类认知的基础，对许多实际应用至关重要。尽管大型语言模型在时间推理方面取得了进展，但现有基准主要依赖于基于规则的构建，缺乏上下文深度，并且涉及的时间实体范围有限。为了解决这些问题，我们引入了中文时间推理（CTM），这是一个旨在评估大型语言模型在中国历史时间推理方面的基准。CTM强调跨实体关系、成对时间对齐以及上下文化和文化基础的推理，提供了全面的评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16701",
            "title": "Beyond Release: Access Considerations for Generative AI Systems",
            "url": "https://huggingface.co/papers/2502.16701",
            "abstract": "Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access refers to practical needs, infrastructurally, technically, and societally, in order to use available components in some way. We deconstruct access along three axes: resourcing, technical usability, and utility. Within each category, a set of variables per system component clarify tradeoffs. For example, resourcing requires access to computing infrastructure to serve model weights. We also compare the accessibility of four high performance language models, two open-weight and two closed-weight, showing similar considerations for all based instead on access variables. Access variables set the foundation for being able to scale or increase access to users; we examine the scale of access and how scale affects ability to manage and intervene on risks. This framework better encompasses the landscape and risk-benefit tradeoffs of system releases to inform system release decisions, research, and policy.",
            "score": 2,
            "issue_id": 2386,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 февраля",
                "en": "February 23",
                "zh": "2月23日"
            },
            "hash": "db3e8ec873d10ddb",
            "authors": [
                "Irene Solaiman",
                "Rishi Bommasani",
                "Dan Hendrycks",
                "Ariel Herbert-Voss",
                "Yacine Jernite",
                "Aviya Skowron",
                "Andrew Trask"
            ],
            "affiliations": [
                "Center for AI Safety",
                "EleutherAI",
                "Hugging Face",
                "OpenMined",
                "RunSybil",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16701.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#open_source"
                ],
                "emoji": "🔓",
                "ru": {
                    "title": "Доступ к ИИ: больше, чем просто релиз",
                    "desc": "Статья рассматривает вопросы доступа к компонентам систем искусственного интеллекта, выходящие за рамки простого решения о релизе. Авторы предлагают фреймворк для анализа доступности по трем осям: ресурсное обеспечение, техническая удобность использования и полезность. На примере четырех языковых моделей демонстрируются схожие соображения по переменным доступа. Исследуется, как масштаб доступа влияет на возможность управления рисками и вмешательства."
                },
                "en": {
                    "title": "Understanding Access: Key to Responsible AI Release",
                    "desc": "This paper discusses how the release of generative AI systems is not just about making components available, but also about how users can effectively engage with these systems. It introduces a framework that breaks down access into three main areas: resourcing, technical usability, and utility, highlighting the importance of each in utilizing AI components. The authors analyze four high-performance language models to illustrate that access variables impact both the risks and benefits associated with these systems. Ultimately, the framework aims to guide better decision-making in AI system releases by considering the broader implications of access."
                },
                "zh": {
                    "title": "解构生成性AI的访问与风险管理",
                    "desc": "这篇论文探讨了生成性人工智能发布决策对系统组件可用性的影响。作者指出，发布并不能解决用户和利益相关者与系统互动的所有问题，访问系统组件的方式也会影响潜在的风险和收益。论文将访问分为三个方面：资源、技术可用性和效用，并在每个类别中明确了不同变量的权衡。通过比较四种高性能语言模型的可访问性，作者展示了如何通过访问变量来评估和管理风险。"
                }
            }
        }
    ],
    "link_prev": "2025-02-24.html",
    "link_next": "2025-02-26.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "24.02",
        "en": "02/24",
        "zh": "2月24日"
    },
    "short_date_next": {
        "ru": "26.02",
        "en": "02/26",
        "zh": "2月26日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种名为 SurveyX 的自动化问卷生成系统。SurveyX 通过引入在线参考检索、预处理方法 AttributeTree 和重新润色过程，显著提高了问卷生成的效果。实验结果表明，SurveyX 在内容质量和引用质量方面优于现有系统，接近人类专家的表现。文章还提到了 SurveyX 生成的问卷示例可以在 www.surveyx.cn 上找到。",
        "title": "SurveyX: Academic Survey Automation via Large Language Models",
        "pinyin": "这篇文章介绍了一种名为 SurveyX 的自动化问卷生成系统。SurveyX 通过引入在线参考检索、预处理方法 AttributeTree 和重新润色过程，显著提高了问卷生成的效果。实验结果表明，SurveyX 在内容质量和引用质量方面优于现有系统，接近人类专家的表现。文章还提到了 SurveyX 生成的问卷示例可以在 www.surveyx.cn 上找到。\n\nZhè piān wénzhāng jièshào le yīzhǒng míngwèi SurveyX de zìdònghuà wènjuàn shēngchéng xìtǒng. SurveyX tōngguò yǐnrù zàixiàn cānkǎo jiǎnsuǒ, yùchǔlǐ fāngfǎ AttributeTree hé chóngxīn rùnsè guòchéng, xiǎnzhù tīgāo le wènjuàn shēngchéng de xiàoguǒ. Shíyàn jiéguǒ biǎomíng, SurveyX zài nèiróng zhìliàng hé yǐnyòng zhìliàng fāngmiàn yōuyú xiànyǒu xìtǒng, jiējìn rénlèi zhuānjiā de biǎoxiàn. Wénzhāng hái tí dào le SurveyX shēngchéng de wènjuàn shìlì kěyǐ zài www.surveyx.cn shàng zhǎo dào.",
        "vocab": "[{'word': '自动化', 'pinyin': 'zìdònghuà', 'trans': 'automated'},\n{'word': '问卷', 'pinyin': 'wènjuàn', 'trans': 'questionnaire'},\n{'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'},\n{'word': '系统', 'pinyin': 'xìtǒng', 'trans': 'system'},\n{'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'},\n{'word': '在线', 'pinyin': 'zàixiàn', 'trans': 'online'},\n{'word': '参考', 'pinyin': 'cānkǎo', 'trans': 'reference'},\n{'word': '检索', 'pinyin': 'jiǎnsuǒ', 'trans': 'retrieval'},\n{'word': '预处理', 'pinyin': 'yùchǔlǐ', 'trans': 'preprocessing'},\n{'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'},\n{'word': 'AttributeTree', 'pinyin': '', 'trans': 'AttributeTree'},\n{'word': '润色', 'pinyin': 'rùnsè', 'trans': 'polish'},\n{'word': '过程', 'pinyin': 'guòchéng', 'trans': 'process'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'},\n{'word': '效果', 'pinyin': 'xiàoguǒ', 'trans': 'effect'},\n{'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'},\n{'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'},\n{'word': '表明', 'pinyin': 'biǎomíng', 'trans': 'indicate'},\n{'word': '内容', 'pinyin': 'nèiróng', 'trans': 'content'},\n{'word': '质量', 'pinyin': 'zhìliàng', 'trans': 'quality'},\n{'word': '引用', 'pinyin': 'yǐnyòng', 'trans': 'citation'},\n{'word': '现有', 'pinyin': 'xiànyǒu', 'trans': 'existing'},\n{'word': '接近', 'pinyin': 'jiējìn', 'trans': 'close to'},\n{'word': '人类', 'pinyin': 'rénlèi', 'trans': 'human'},\n{'word': '专家', 'pinyin': 'zhuānjiā', 'trans': 'expert'},\n{'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'},\n{'word': '提到', 'pinyin': 'tídào', 'trans': 'mention'},\n{'word': '示例', 'pinyin': 'shìlì', 'trans': 'example'},\n{'word': '找到', 'pinyin': 'zhǎodào', 'trans': 'find'}]",
        "trans": "This article introduces an automated survey generation system called SurveyX. SurveyX significantly enhances the effectiveness of survey generation by incorporating online reference retrieval, the preprocessing method AttributeTree, and a refinement process. Experimental results indicate that SurveyX outperforms existing systems in terms of content quality and citation quality, approaching the performance of human experts. The article also mentions that examples of surveys generated by SurveyX can be found at www.surveyx.cn.",
        "update_ts": "2025-02-24 09:12"
    }
}