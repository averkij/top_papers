{
    "date": {
        "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 25",
        "zh": "2æœˆ25æ—¥"
    },
    "time_utc": "2025-02-25 03:18",
    "weekday": 1,
    "issue_id": 2386,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.16614",
            "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models",
            "url": "https://huggingface.co/papers/2502.16614",
            "abstract": "The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks have been proposed. However, existing critique benchmarks usually have the following limitations: (1). Focusing on diverse reasoning tasks in general domains and insufficient evaluation on code tasks (e.g., only covering code generation task), where the difficulty of queries is relatively easy (e.g., the code queries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive evaluation from different dimensions. To address these limitations, we introduce a holistic code critique benchmark for LLMs called CodeCriticBench. Specifically, our CodeCriticBench includes two mainstream code tasks (i.e., code generation and code QA) with different difficulties. Besides, the evaluation protocols include basic critique evaluation and advanced critique evaluation for different characteristics, where fine-grained evaluation checklists are well-designed for advanced settings. Finally, we conduct extensive experimental results of existing LLMs, which show the effectiveness of CodeCriticBench.",
            "score": 1,
            "issue_id": 2386,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 23",
                "zh": "2æœˆ23æ—¥"
            },
            "hash": "c70e868ad4c1b726",
            "authors": [
                "Alexander Zhang",
                "Marcus Dong",
                "Jiaheng Liu",
                "Wei Zhang",
                "Yejie Wang",
                "Jian Yang",
                "Ge Zhang",
                "Tianyu Liu",
                "Zhongyuan Peng",
                "Yingshui Tan",
                "Yuanxing Zhang",
                "Zhexu Wang",
                "Weixun Wang",
                "Yancheng He",
                "Ken Deng",
                "Wangchunshu Zhou",
                "Wenhao Huang",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "Alibaba",
                "BUAA",
                "BUPT",
                "CASIA",
                "Kuaishou",
                "M-A-P",
                "NJU",
                "OPPO"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16614.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "CodeCriticBench: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CodeCriticBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ ĞºĞ¾Ğ´Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. CodeCriticBench Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‡ĞµĞº-Ğ»Ğ¸ÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ LLM, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°."
                },
                "en": {
                    "title": "Enhancing Code Critique with CodeCriticBench",
                    "desc": "This paper introduces CodeCriticBench, a new benchmark designed to evaluate the critique capacity of Large Language Models (LLMs) specifically in the context of code tasks. Unlike existing benchmarks that primarily focus on general reasoning tasks, CodeCriticBench encompasses both code generation and code question-answering tasks, offering a range of difficulties. The evaluation framework includes both basic and advanced critique assessments, utilizing detailed checklists to ensure comprehensive analysis. Experimental results demonstrate that CodeCriticBench effectively measures the critique abilities of various LLMs, highlighting its importance in enhancing model reasoning capabilities."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°LLMsçš„ä»£ç æ‰¹è¯„èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ‰¹è¯„èƒ½åŠ›å¯¹äºæ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå¯ä»¥æä¾›å¿…è¦çš„å»ºè®®ï¼Œå¦‚è¯¦ç»†åˆ†æå’Œå»ºè®¾æ€§åé¦ˆã€‚ä¸ºäº†è¯„ä¼°LLMsçš„æ‰¹è¯„èƒ½åŠ›ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†å¤šä¸ªæ‰¹è¯„åŸºå‡†ï¼Œä½†ç°æœ‰åŸºå‡†å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚å¯¹ä»£ç ä»»åŠ¡çš„è¯„ä¼°ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„ä»£ç æ‰¹è¯„åŸºå‡†ï¼Œç§°ä¸ºCodeCriticBenchï¼Œæ¶µç›–äº†ä»£ç ç”Ÿæˆå’Œä»£ç é—®ç­”ä¸¤ç§ä¸»æµä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†ç»†è‡´çš„è¯„ä¼°æ ‡å‡†ã€‚é€šè¿‡å¯¹ç°æœ‰LLMsçš„å¹¿æ³›å®éªŒç»“æœï¼Œæˆ‘ä»¬éªŒè¯äº†CodeCriticBenchçš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16033",
            "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
            "url": "https://huggingface.co/papers/2502.16033",
            "abstract": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.",
            "score": 1,
            "issue_id": 2386,
            "pub_date": "2025-02-22",
            "pub_date_card": {
                "ru": "22 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 22",
                "zh": "2æœˆ22æ—¥"
            },
            "hash": "3e5fc69b8713e252",
            "authors": [
                "Qianqi Yan",
                "Yue Fan",
                "Hongquan Li",
                "Shan Jiang",
                "Yang Zhao",
                "Xinze Guan",
                "Ching-Chen Kuo",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "University of California, Santa Cruz",
                "eBay"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16033.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ: Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MMIR Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ. MMIR Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 534 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸ Ğ² Ğ¿ÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ ÑˆĞµÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ², Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğº Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² MLLM."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning: Tackling Inconsistencies in Real-World Content",
                    "desc": "This paper introduces the Multimodal Inconsistency Reasoning (MMIR) benchmark to evaluate how well Multimodal Large Language Models (MLLMs) can identify and reason about inconsistencies in complex visual-textual content. The benchmark consists of 534 samples with various types of semantic mismatches, such as factual contradictions and contextual mismatches. The study finds that models designed for multimodal reasoning perform significantly better than others, but many open-source models struggle with inconsistencies, especially those that span multiple modalities. The results indicate a need for improved cross-modal reasoning capabilities in MLLMs, as current methods show limited effectiveness in handling complex layouts and cross-modal conflicts."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œè§£å†³ç°å®ä¸–ç•Œçš„ä¸ä¸€è‡´æ€§",
                    "desc": "ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦åœ¨ä¸€è‡´çš„è§†è§‰-æ–‡æœ¬è¾“å…¥ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œå°šä¸æ¸…æ¥šå®ƒä»¬èƒ½å¦å¤„ç†ç°å®ä¸–ç•Œä¸­å¸ƒå±€ä¸°å¯Œå†…å®¹çš„ä¸ä¸€è‡´æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€ä¸ä¸€è‡´æ€§æ¨ç†ï¼ˆMMIRï¼‰åŸºå‡†ï¼Œä»¥è¯„ä¼°MLLMsåœ¨æ£€æµ‹å’Œæ¨ç†è¯­ä¹‰ä¸åŒ¹é…æ–¹é¢çš„èƒ½åŠ›ã€‚MMIRåŒ…å«534ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œæ¶µç›–äº”ä¸ªæ¨ç†å¯†é›†å‹ç±»åˆ«çš„åˆæˆé”™è¯¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå…·å¤‡ä¸“é—¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„æ¨¡å‹åœ¨å¤„ç†ä¸ä¸€è‡´æ€§æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œè€Œå¼€æºæ¨¡å‹åˆ™ç‰¹åˆ«å®¹æ˜“å—åˆ°ä¸ä¸€è‡´æ€§é”™è¯¯çš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16701",
            "title": "Beyond Release: Access Considerations for Generative AI Systems",
            "url": "https://huggingface.co/papers/2502.16701",
            "abstract": "Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access refers to practical needs, infrastructurally, technically, and societally, in order to use available components in some way. We deconstruct access along three axes: resourcing, technical usability, and utility. Within each category, a set of variables per system component clarify tradeoffs. For example, resourcing requires access to computing infrastructure to serve model weights. We also compare the accessibility of four high performance language models, two open-weight and two closed-weight, showing similar considerations for all based instead on access variables. Access variables set the foundation for being able to scale or increase access to users; we examine the scale of access and how scale affects ability to manage and intervene on risks. This framework better encompasses the landscape and risk-benefit tradeoffs of system releases to inform system release decisions, research, and policy.",
            "score": 1,
            "issue_id": 2386,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 23",
                "zh": "2æœˆ23æ—¥"
            },
            "hash": "db3e8ec873d10ddb",
            "authors": [
                "Irene Solaiman",
                "Rishi Bommasani",
                "Dan Hendrycks",
                "Ariel Herbert-Voss",
                "Yacine Jernite",
                "Aviya Skowron",
                "Andrew Trask"
            ],
            "affiliations": [
                "Center for AI Safety",
                "EleutherAI",
                "Hugging Face",
                "OpenMined",
                "RunSybil",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16701.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#open_source"
                ],
                "emoji": "ğŸ”“",
                "ru": {
                    "title": "Ğ”Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ˜Ğ˜: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ€ĞµĞ»Ğ¸Ğ·",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ Ñ€ĞµĞ»Ğ¸Ğ·Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ¾ÑÑĞ¼: Ñ€ĞµÑÑƒÑ€ÑĞ½Ğ¾Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑƒĞ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ ÑÑ…Ğ¾Ğ¶Ğ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°. Ğ˜ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Understanding Access: Key to Responsible AI Release",
                    "desc": "This paper discusses how the release of generative AI systems is not just about making components available, but also about how users can effectively engage with these systems. It introduces a framework that breaks down access into three main areas: resourcing, technical usability, and utility, highlighting the importance of each in utilizing AI components. The authors analyze four high-performance language models to illustrate that access variables impact both the risks and benefits associated with these systems. Ultimately, the framework aims to guide better decision-making in AI system releases by considering the broader implications of access."
                },
                "zh": {
                    "title": "è§£æ„ç”Ÿæˆæ€§AIçš„è®¿é—®ä¸é£é™©ç®¡ç†",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å‘å¸ƒå†³ç­–å¯¹ç³»ç»Ÿç»„ä»¶å¯ç”¨æ€§çš„å½±å“ã€‚ä½œè€…æŒ‡å‡ºï¼Œå‘å¸ƒå¹¶ä¸èƒ½è§£å†³ç”¨æˆ·å’Œåˆ©ç›Šç›¸å…³è€…ä¸ç³»ç»Ÿäº’åŠ¨çš„æ‰€æœ‰é—®é¢˜ï¼Œè®¿é—®ç³»ç»Ÿç»„ä»¶çš„æ–¹å¼ä¹Ÿä¼šå½±å“æ½œåœ¨çš„é£é™©å’Œæ”¶ç›Šã€‚è®ºæ–‡å°†è®¿é—®åˆ†ä¸ºä¸‰ä¸ªæ–¹é¢ï¼šèµ„æºã€æŠ€æœ¯å¯ç”¨æ€§å’Œæ•ˆç”¨ï¼Œå¹¶åœ¨æ¯ä¸ªç±»åˆ«ä¸­æ˜ç¡®äº†ä¸åŒå˜é‡çš„æƒè¡¡ã€‚é€šè¿‡æ¯”è¾ƒå››ç§é«˜æ€§èƒ½è¯­è¨€æ¨¡å‹çš„å¯è®¿é—®æ€§ï¼Œä½œè€…å±•ç¤ºäº†å¦‚ä½•é€šè¿‡è®¿é—®å˜é‡æ¥è¯„ä¼°å’Œç®¡ç†é£é™©ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-24.html",
    "link_next": "2025-02-26.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "24.02",
        "en": "02/24",
        "zh": "2æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "26.02",
        "en": "02/26",
        "zh": "2æœˆ26æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸º SurveyX çš„è‡ªåŠ¨åŒ–é—®å·ç”Ÿæˆç³»ç»Ÿã€‚SurveyX é€šè¿‡å¼•å…¥åœ¨çº¿å‚è€ƒæ£€ç´¢ã€é¢„å¤„ç†æ–¹æ³• AttributeTree å’Œé‡æ–°æ¶¦è‰²è¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†é—®å·ç”Ÿæˆçš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSurveyX åœ¨å†…å®¹è´¨é‡å’Œå¼•ç”¨è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œæ¥è¿‘äººç±»ä¸“å®¶çš„è¡¨ç°ã€‚æ–‡ç« è¿˜æåˆ°äº† SurveyX ç”Ÿæˆçš„é—®å·ç¤ºä¾‹å¯ä»¥åœ¨ www.surveyx.cn ä¸Šæ‰¾åˆ°ã€‚",
        "title": "SurveyX: Academic Survey Automation via Large Language Models",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸º SurveyX çš„è‡ªåŠ¨åŒ–é—®å·ç”Ÿæˆç³»ç»Ÿã€‚SurveyX é€šè¿‡å¼•å…¥åœ¨çº¿å‚è€ƒæ£€ç´¢ã€é¢„å¤„ç†æ–¹æ³• AttributeTree å’Œé‡æ–°æ¶¦è‰²è¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†é—®å·ç”Ÿæˆçš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSurveyX åœ¨å†…å®¹è´¨é‡å’Œå¼•ç”¨è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œæ¥è¿‘äººç±»ä¸“å®¶çš„è¡¨ç°ã€‚æ–‡ç« è¿˜æåˆ°äº† SurveyX ç”Ÿæˆçš„é—®å·ç¤ºä¾‹å¯ä»¥åœ¨ www.surveyx.cn ä¸Šæ‰¾åˆ°ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ«zhÇ’ng mÃ­ngwÃ¨i SurveyX de zÃ¬dÃ²nghuÃ  wÃ¨njuÃ n shÄ“ngchÃ©ng xÃ¬tÇ’ng. SurveyX tÅngguÃ² yÇnrÃ¹ zÃ ixiÃ n cÄnkÇo jiÇnsuÇ’, yÃ¹chÇ”lÇ fÄngfÇ AttributeTree hÃ© chÃ³ngxÄ«n rÃ¹nsÃ¨ guÃ²chÃ©ng, xiÇnzhÃ¹ tÄ«gÄo le wÃ¨njuÃ n shÄ“ngchÃ©ng de xiÃ oguÇ’. ShÃ­yÃ n jiÃ©guÇ’ biÇomÃ­ng, SurveyX zÃ i nÃ¨irÃ³ng zhÃ¬liÃ ng hÃ© yÇnyÃ²ng zhÃ¬liÃ ng fÄngmiÃ n yÅuyÃº xiÃ nyÇ’u xÃ¬tÇ’ng, jiÄ“jÃ¬n rÃ©nlÃ¨i zhuÄnjiÄ de biÇoxiÃ n. WÃ©nzhÄng hÃ¡i tÃ­ dÃ o le SurveyX shÄ“ngchÃ©ng de wÃ¨njuÃ n shÃ¬lÃ¬ kÄ›yÇ zÃ i www.surveyx.cn shÃ ng zhÇo dÃ o.",
        "vocab": "[{'word': 'è‡ªåŠ¨åŒ–', 'pinyin': 'zÃ¬dÃ²nghuÃ ', 'trans': 'automated'},\n{'word': 'é—®å·', 'pinyin': 'wÃ¨njuÃ n', 'trans': 'questionnaire'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generate'},\n{'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬tÇ’ng', 'trans': 'system'},\n{'word': 'å¼•å…¥', 'pinyin': 'yÇnrÃ¹', 'trans': 'introduce'},\n{'word': 'åœ¨çº¿', 'pinyin': 'zÃ ixiÃ n', 'trans': 'online'},\n{'word': 'å‚è€ƒ', 'pinyin': 'cÄnkÇo', 'trans': 'reference'},\n{'word': 'æ£€ç´¢', 'pinyin': 'jiÇnsuÇ’', 'trans': 'retrieval'},\n{'word': 'é¢„å¤„ç†', 'pinyin': 'yÃ¹chÇ”lÇ', 'trans': 'preprocessing'},\n{'word': 'æ–¹æ³•', 'pinyin': 'fÄngfÇ', 'trans': 'method'},\n{'word': 'AttributeTree', 'pinyin': '', 'trans': 'AttributeTree'},\n{'word': 'æ¶¦è‰²', 'pinyin': 'rÃ¹nsÃ¨', 'trans': 'polish'},\n{'word': 'è¿‡ç¨‹', 'pinyin': 'guÃ²chÃ©ng', 'trans': 'process'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'},\n{'word': 'æé«˜', 'pinyin': 'tÃ­gÄo', 'trans': 'improve'},\n{'word': 'æ•ˆæœ', 'pinyin': 'xiÃ oguÇ’', 'trans': 'effect'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ©guÇ’', 'trans': 'result'},\n{'word': 'è¡¨æ˜', 'pinyin': 'biÇomÃ­ng', 'trans': 'indicate'},\n{'word': 'å†…å®¹', 'pinyin': 'nÃ¨irÃ³ng', 'trans': 'content'},\n{'word': 'è´¨é‡', 'pinyin': 'zhÃ¬liÃ ng', 'trans': 'quality'},\n{'word': 'å¼•ç”¨', 'pinyin': 'yÇnyÃ²ng', 'trans': 'citation'},\n{'word': 'ç°æœ‰', 'pinyin': 'xiÃ nyÇ’u', 'trans': 'existing'},\n{'word': 'æ¥è¿‘', 'pinyin': 'jiÄ“jÃ¬n', 'trans': 'close to'},\n{'word': 'äººç±»', 'pinyin': 'rÃ©nlÃ¨i', 'trans': 'human'},\n{'word': 'ä¸“å®¶', 'pinyin': 'zhuÄnjiÄ', 'trans': 'expert'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇoxiÃ n', 'trans': 'performance'},\n{'word': 'æåˆ°', 'pinyin': 'tÃ­dÃ o', 'trans': 'mention'},\n{'word': 'ç¤ºä¾‹', 'pinyin': 'shÃ¬lÃ¬', 'trans': 'example'},\n{'word': 'æ‰¾åˆ°', 'pinyin': 'zhÇodÃ o', 'trans': 'find'}]",
        "trans": "This article introduces an automated survey generation system called SurveyX. SurveyX significantly enhances the effectiveness of survey generation by incorporating online reference retrieval, the preprocessing method AttributeTree, and a refinement process. Experimental results indicate that SurveyX outperforms existing systems in terms of content quality and citation quality, approaching the performance of human experts. The article also mentions that examples of surveys generated by SurveyX can be found at www.surveyx.cn.",
        "update_ts": "2025-02-24 09:12"
    }
}