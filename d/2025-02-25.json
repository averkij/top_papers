{
    "date": {
        "ru": "25 февраля",
        "en": "February 25",
        "zh": "2月25日"
    },
    "time_utc": "2025-02-25 03:18",
    "weekday": 1,
    "issue_id": 2386,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.16614",
            "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models",
            "url": "https://huggingface.co/papers/2502.16614",
            "abstract": "The critique capacity of Large Language Models (LLMs) is essential for reasoning abilities, which can provide necessary suggestions (e.g., detailed analysis and constructive feedback). Therefore, how to evaluate the critique capacity of LLMs has drawn great attention and several critique benchmarks have been proposed. However, existing critique benchmarks usually have the following limitations: (1). Focusing on diverse reasoning tasks in general domains and insufficient evaluation on code tasks (e.g., only covering code generation task), where the difficulty of queries is relatively easy (e.g., the code queries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive evaluation from different dimensions. To address these limitations, we introduce a holistic code critique benchmark for LLMs called CodeCriticBench. Specifically, our CodeCriticBench includes two mainstream code tasks (i.e., code generation and code QA) with different difficulties. Besides, the evaluation protocols include basic critique evaluation and advanced critique evaluation for different characteristics, where fine-grained evaluation checklists are well-designed for advanced settings. Finally, we conduct extensive experimental results of existing LLMs, which show the effectiveness of CodeCriticBench.",
            "score": 1,
            "issue_id": 2386,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 февраля",
                "en": "February 23",
                "zh": "2月23日"
            },
            "hash": "c70e868ad4c1b726",
            "authors": [
                "Alexander Zhang",
                "Marcus Dong",
                "Jiaheng Liu",
                "Wei Zhang",
                "Yejie Wang",
                "Jian Yang",
                "Ge Zhang",
                "Tianyu Liu",
                "Zhongyuan Peng",
                "Yingshui Tan",
                "Yuanxing Zhang",
                "Zhexu Wang",
                "Weixun Wang",
                "Yancheng He",
                "Ken Deng",
                "Wangchunshu Zhou",
                "Wenhao Huang",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "Alibaba",
                "BUAA",
                "BUPT",
                "CASIA",
                "Kuaishou",
                "M-A-P",
                "NJU",
                "OPPO"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16614.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "CodeCriticBench: комплексная оценка критических способностей LLM в задачах работы с кодом",
                    "desc": "Статья представляет новый бенчмарк CodeCriticBench для оценки способностей больших языковых моделей (LLM) критиковать код. Бенчмарк включает задачи генерации кода и ответов на вопросы о коде разной сложности. CodeCriticBench предлагает базовую и продвинутую оценку критических способностей с использованием детальных чек-листов. Авторы провели обширные эксперименты с существующими LLM, показавшие эффективность предложенного бенчмарка."
                },
                "en": {
                    "title": "Enhancing Code Critique with CodeCriticBench",
                    "desc": "This paper introduces CodeCriticBench, a new benchmark designed to evaluate the critique capacity of Large Language Models (LLMs) specifically in the context of code tasks. Unlike existing benchmarks that primarily focus on general reasoning tasks, CodeCriticBench encompasses both code generation and code question-answering tasks, offering a range of difficulties. The evaluation framework includes both basic and advanced critique assessments, utilizing detailed checklists to ensure comprehensive analysis. Experimental results demonstrate that CodeCriticBench effectively measures the critique abilities of various LLMs, highlighting its importance in enhancing model reasoning capabilities."
                },
                "zh": {
                    "title": "全面评估LLMs的代码批评能力",
                    "desc": "大型语言模型（LLMs）的批评能力对于推理能力至关重要，可以提供必要的建议，如详细分析和建设性反馈。为了评估LLMs的批评能力，研究者们提出了多个批评基准，但现有基准存在一些局限性，例如对代码任务的评估不足。为了解决这些问题，我们引入了一个全面的代码批评基准，称为CodeCriticBench，涵盖了代码生成和代码问答两种主流任务，并设计了细致的评估标准。通过对现有LLMs的广泛实验结果，我们验证了CodeCriticBench的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16033",
            "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
            "url": "https://huggingface.co/papers/2502.16033",
            "abstract": "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.",
            "score": 1,
            "issue_id": 2386,
            "pub_date": "2025-02-22",
            "pub_date_card": {
                "ru": "22 февраля",
                "en": "February 22",
                "zh": "2月22日"
            },
            "hash": "3e5fc69b8713e252",
            "authors": [
                "Qianqi Yan",
                "Yue Fan",
                "Hongquan Li",
                "Shan Jiang",
                "Yang Zhao",
                "Xinze Guan",
                "Ching-Chen Kuo",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "University of California, Santa Cruz",
                "eBay"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16033.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый рубеж в мультимодальном анализе: оценка способности ИИ распознавать несоответствия",
                    "desc": "Статья представляет новый бенчмарк MMIR для оценки способности мультимодальных больших языковых моделей (MLLM) обнаруживать и анализировать семантические несоответствия в визуально-текстовом контенте. MMIR включает 534 сложных примера с синтетически внедренными ошибками в пяти категориях, требующих рассуждений. Авторы оценили шесть современных MLLM, показав, что модели с специализированными возможностями мультимодальных рассуждений значительно превосходят другие, в то время как модели с открытым исходным кодом особенно уязвимы к ошибкам несоответствия. Результаты исследования подчеркивают необходимость улучшения мультимодальных рассуждений в MLLM."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning: Tackling Inconsistencies in Real-World Content",
                    "desc": "This paper introduces the Multimodal Inconsistency Reasoning (MMIR) benchmark to evaluate how well Multimodal Large Language Models (MLLMs) can identify and reason about inconsistencies in complex visual-textual content. The benchmark consists of 534 samples with various types of semantic mismatches, such as factual contradictions and contextual mismatches. The study finds that models designed for multimodal reasoning perform significantly better than others, but many open-source models struggle with inconsistencies, especially those that span multiple modalities. The results indicate a need for improved cross-modal reasoning capabilities in MLLMs, as current methods show limited effectiveness in handling complex layouts and cross-modal conflicts."
                },
                "zh": {
                    "title": "提升多模态推理能力，解决现实世界的不一致性",
                    "desc": "现有的多模态大型语言模型（MLLMs）主要在一致的视觉-文本输入上进行训练和测试，尚不清楚它们能否处理现实世界中布局丰富内容的不一致性。为此，我们提出了多模态不一致性推理（MMIR）基准，以评估MLLMs在检测和推理语义不匹配方面的能力。MMIR包含534个具有挑战性的样本，涵盖五个推理密集型类别的合成错误。我们的研究表明，具备专门多模态推理能力的模型在处理不一致性时表现优异，而开源模型则特别容易受到不一致性错误的影响。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.16701",
            "title": "Beyond Release: Access Considerations for Generative AI Systems",
            "url": "https://huggingface.co/papers/2502.16701",
            "abstract": "Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access refers to practical needs, infrastructurally, technically, and societally, in order to use available components in some way. We deconstruct access along three axes: resourcing, technical usability, and utility. Within each category, a set of variables per system component clarify tradeoffs. For example, resourcing requires access to computing infrastructure to serve model weights. We also compare the accessibility of four high performance language models, two open-weight and two closed-weight, showing similar considerations for all based instead on access variables. Access variables set the foundation for being able to scale or increase access to users; we examine the scale of access and how scale affects ability to manage and intervene on risks. This framework better encompasses the landscape and risk-benefit tradeoffs of system releases to inform system release decisions, research, and policy.",
            "score": 1,
            "issue_id": 2386,
            "pub_date": "2025-02-23",
            "pub_date_card": {
                "ru": "23 февраля",
                "en": "February 23",
                "zh": "2月23日"
            },
            "hash": "db3e8ec873d10ddb",
            "authors": [
                "Irene Solaiman",
                "Rishi Bommasani",
                "Dan Hendrycks",
                "Ariel Herbert-Voss",
                "Yacine Jernite",
                "Aviya Skowron",
                "Andrew Trask"
            ],
            "affiliations": [
                "Center for AI Safety",
                "EleutherAI",
                "Hugging Face",
                "OpenMined",
                "RunSybil",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.16701.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#open_source"
                ],
                "emoji": "🔓",
                "ru": {
                    "title": "Доступ к ИИ: больше, чем просто релиз",
                    "desc": "Статья рассматривает вопросы доступа к компонентам систем искусственного интеллекта, выходящие за рамки простого решения о релизе. Авторы предлагают фреймворк для анализа доступности по трем осям: ресурсное обеспечение, техническая удобность использования и полезность. На примере четырех языковых моделей демонстрируются схожие соображения по переменным доступа. Исследуется, как масштаб доступа влияет на возможность управления рисками и вмешательства."
                },
                "en": {
                    "title": "Understanding Access: Key to Responsible AI Release",
                    "desc": "This paper discusses how the release of generative AI systems is not just about making components available, but also about how users can effectively engage with these systems. It introduces a framework that breaks down access into three main areas: resourcing, technical usability, and utility, highlighting the importance of each in utilizing AI components. The authors analyze four high-performance language models to illustrate that access variables impact both the risks and benefits associated with these systems. Ultimately, the framework aims to guide better decision-making in AI system releases by considering the broader implications of access."
                },
                "zh": {
                    "title": "解构生成性AI的访问与风险管理",
                    "desc": "这篇论文探讨了生成性人工智能发布决策对系统组件可用性的影响。作者指出，发布并不能解决用户和利益相关者与系统互动的所有问题，访问系统组件的方式也会影响潜在的风险和收益。论文将访问分为三个方面：资源、技术可用性和效用，并在每个类别中明确了不同变量的权衡。通过比较四种高性能语言模型的可访问性，作者展示了如何通过访问变量来评估和管理风险。"
                }
            }
        }
    ],
    "link_prev": "2025-02-24.html",
    "link_next": "2025-02-26.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "24.02",
        "en": "02/24",
        "zh": "2月24日"
    },
    "short_date_next": {
        "ru": "26.02",
        "en": "02/26",
        "zh": "2月26日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种名为 SurveyX 的自动化问卷生成系统。SurveyX 通过引入在线参考检索、预处理方法 AttributeTree 和重新润色过程，显著提高了问卷生成的效果。实验结果表明，SurveyX 在内容质量和引用质量方面优于现有系统，接近人类专家的表现。文章还提到了 SurveyX 生成的问卷示例可以在 www.surveyx.cn 上找到。",
        "title": "SurveyX: Academic Survey Automation via Large Language Models",
        "pinyin": "这篇文章介绍了一种名为 SurveyX 的自动化问卷生成系统。SurveyX 通过引入在线参考检索、预处理方法 AttributeTree 和重新润色过程，显著提高了问卷生成的效果。实验结果表明，SurveyX 在内容质量和引用质量方面优于现有系统，接近人类专家的表现。文章还提到了 SurveyX 生成的问卷示例可以在 www.surveyx.cn 上找到。\n\nZhè piān wénzhāng jièshào le yīzhǒng míngwèi SurveyX de zìdònghuà wènjuàn shēngchéng xìtǒng. SurveyX tōngguò yǐnrù zàixiàn cānkǎo jiǎnsuǒ, yùchǔlǐ fāngfǎ AttributeTree hé chóngxīn rùnsè guòchéng, xiǎnzhù tīgāo le wènjuàn shēngchéng de xiàoguǒ. Shíyàn jiéguǒ biǎomíng, SurveyX zài nèiróng zhìliàng hé yǐnyòng zhìliàng fāngmiàn yōuyú xiànyǒu xìtǒng, jiējìn rénlèi zhuānjiā de biǎoxiàn. Wénzhāng hái tí dào le SurveyX shēngchéng de wènjuàn shìlì kěyǐ zài www.surveyx.cn shàng zhǎo dào.",
        "vocab": "[{'word': '自动化', 'pinyin': 'zìdònghuà', 'trans': 'automated'},\n{'word': '问卷', 'pinyin': 'wènjuàn', 'trans': 'questionnaire'},\n{'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'},\n{'word': '系统', 'pinyin': 'xìtǒng', 'trans': 'system'},\n{'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'},\n{'word': '在线', 'pinyin': 'zàixiàn', 'trans': 'online'},\n{'word': '参考', 'pinyin': 'cānkǎo', 'trans': 'reference'},\n{'word': '检索', 'pinyin': 'jiǎnsuǒ', 'trans': 'retrieval'},\n{'word': '预处理', 'pinyin': 'yùchǔlǐ', 'trans': 'preprocessing'},\n{'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'},\n{'word': 'AttributeTree', 'pinyin': '', 'trans': 'AttributeTree'},\n{'word': '润色', 'pinyin': 'rùnsè', 'trans': 'polish'},\n{'word': '过程', 'pinyin': 'guòchéng', 'trans': 'process'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'},\n{'word': '效果', 'pinyin': 'xiàoguǒ', 'trans': 'effect'},\n{'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'},\n{'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'},\n{'word': '表明', 'pinyin': 'biǎomíng', 'trans': 'indicate'},\n{'word': '内容', 'pinyin': 'nèiróng', 'trans': 'content'},\n{'word': '质量', 'pinyin': 'zhìliàng', 'trans': 'quality'},\n{'word': '引用', 'pinyin': 'yǐnyòng', 'trans': 'citation'},\n{'word': '现有', 'pinyin': 'xiànyǒu', 'trans': 'existing'},\n{'word': '接近', 'pinyin': 'jiējìn', 'trans': 'close to'},\n{'word': '人类', 'pinyin': 'rénlèi', 'trans': 'human'},\n{'word': '专家', 'pinyin': 'zhuānjiā', 'trans': 'expert'},\n{'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'},\n{'word': '提到', 'pinyin': 'tídào', 'trans': 'mention'},\n{'word': '示例', 'pinyin': 'shìlì', 'trans': 'example'},\n{'word': '找到', 'pinyin': 'zhǎodào', 'trans': 'find'}]",
        "trans": "This article introduces an automated survey generation system called SurveyX. SurveyX significantly enhances the effectiveness of survey generation by incorporating online reference retrieval, the preprocessing method AttributeTree, and a refinement process. Experimental results indicate that SurveyX outperforms existing systems in terms of content quality and citation quality, approaching the performance of human experts. The article also mentions that examples of surveys generated by SurveyX can be found at www.surveyx.cn.",
        "update_ts": "2025-02-24 09:12"
    }
}