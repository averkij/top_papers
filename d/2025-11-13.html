
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. November 13.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">13 Ğ½Ğ¾ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-11-12.html">â¬…ï¸ <span id="prev-date">12.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-11-14.html">â¡ï¸ <span id="next-date">14.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-11.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'};
        let feedDateNext = {'ru': '14.11', 'en': '11/14', 'zh': '11æœˆ14æ—¥'};
        let feedDatePrev = {'ru': '12.11', 'en': '11/12', 'zh': '11æœˆ12æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2511.08892', 'title': 'Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds', 'url': 'https://huggingface.co/papers/2511.08892', 'abstract': "Lumine, a vision-language model-based agent, completes complex missions in real-time across different 3D open-world environments with human-like efficiency and zero-shot cross-game generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", 'score': 194, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '979bf2a9c14bc65e', 'authors': ['Weihao Tan', 'Xiangyang Li', 'Yunhao Fang', 'Heyuan Yao', 'Shi Yan', 'Hao Luo', 'Tenglong Ao', 'Huihui Li', 'Hongbin Ren', 'Bairen Yi', 'Yujia Qin', 'Bo An', 'Libin Liu', 'Guang Shi'], 'affiliations': ['ByteDance', 'Nanyang Technological University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08892.jpg', 'data': {'categories': ['#games', '#open_source', '#3d', '#transfer_learning', '#cv', '#agents', '#multimodal'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ¼ĞµĞ¶Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Lumine â€” Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vision-language model, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‡Ğ°ÑĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¸ÑÑĞ¸Ğ¸ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ°Ñ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ†Ğµ, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¸Ğ³Ñ€Ğµ Genshin Impact Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑÑĞ¶ĞµÑ‚Ğ° Ğ´Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ â€” Ğ½ÑƒĞ»ĞµĞ²Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ (zero-shot) Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¸Ğ³Ñ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Lumine: A Generalist Agent for Real-Time 3D Missions', 'desc': 'Lumine is a vision-language model designed to act as a generalist agent capable of completing complex tasks in real-time within 3D open-world environments. It integrates perception, reasoning, and action in a seamless manner, processing visual data to generate precise actions at a high frequency. Trained on the game Genshin Impact, Lumine can follow natural language instructions and perform various tasks, demonstrating human-like efficiency. Notably, it exhibits zero-shot cross-game generalization, successfully completing missions in different games without additional training, showcasing its versatility and adaptability.'}, 'zh': {'title': 'Lumineï¼šé€šç”¨æ™ºèƒ½ä½“çš„æœªæ¥', 'desc': 'Lumineæ˜¯ä¸€ç§åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„3Då¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å®æ—¶å®Œæˆä»»åŠ¡ï¼Œè¡¨ç°å‡ºç±»ä¼¼äººç±»çš„æ•ˆç‡ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§äººæ€§åŒ–çš„äº¤äº’æ–¹å¼ï¼Œå°†æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ç»Ÿä¸€åœ¨ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ä¸­ã€‚Lumineåœ¨ã€ŠåŸç¥ã€‹ä¸­ç»è¿‡è®­ç»ƒï¼Œèƒ½å¤Ÿé«˜æ•ˆå®Œæˆäº”å°æ—¶çš„ä¸»çº¿ä»»åŠ¡ï¼Œå¹¶æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ‰§è¡Œå¤šç§ä»»åŠ¡ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒLumineåœ¨ä¸åŒæ¸¸æˆä¹‹é—´å±•ç°å‡ºå¼ºå¤§çš„é›¶-shotè·¨æ¸¸æˆæ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¼€æ”¾ç¯å¢ƒä¸­ä½œä¸ºé€šç”¨æ™ºèƒ½ä½“çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08923', 'title': 'TiDAR: Think in Diffusion, Talk in Autoregression', 'url': 'https://huggingface.co/papers/2511.08923', 'abstract': 'TiDAR combines diffusion and autoregressive models to achieve high throughput and quality in language generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.', 'score': 113, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '99289bf144839df6', 'authors': ['Jingyu Liu', 'Xin Dong', 'Zhifan Ye', 'Rishabh Mehta', 'Yonggan Fu', 'Vartika Singh', 'Jan Kautz', 'Ce Zhang', 'Pavlo Molchanov'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08923.jpg', 'data': {'categories': ['#diffusion', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ: Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'TiDAR Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² (ÑÑ‚Ğ°Ğ¿ Thinking) Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² (ÑÑ‚Ğ°Ğ¿ Talking), Ğ²ÑĞµ ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ GPU-Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², TiDAR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ² 4.71-5.91 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‡ĞµĞ¼ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ñ Ğ½Ğ¸Ğ¼Ğ¸.'}, 'en': {'title': 'TiDAR: Bridging Speed and Quality in Language Generation', 'desc': 'TiDAR is a novel language generation model that merges diffusion and autoregressive techniques to enhance both speed and quality. By utilizing a hybrid architecture, it drafts tokens using diffusion methods and samples final outputs with autoregressive processes in a single forward pass. This approach maximizes GPU utilization and improves throughput while maintaining high-quality outputs, effectively addressing the limitations of existing models. Extensive evaluations show that TiDAR significantly outperforms traditional autoregressive models and diffusion variants in both efficiency and quality, achieving a remarkable increase in token generation speed.'}, 'zh': {'title': 'TiDARï¼šé«˜æ•ˆé«˜è´¨é‡çš„è¯­è¨€ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'TiDARæ˜¯ä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹çš„è¯­è¨€ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨å®ç°é«˜ååé‡å’Œé«˜è´¨é‡çš„æ–‡æœ¬ç”Ÿæˆã€‚æ‰©æ•£è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿå¹¶è¡Œç”Ÿæˆï¼Œè€Œè‡ªå›å½’æ¨¡å‹åœ¨è´¨é‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚TiDARé€šè¿‡åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­ä½¿ç”¨ç‰¹æ®Šè®¾è®¡çš„ç»“æ„åŒ–æ³¨æ„åŠ›æ©ç ï¼Œå¹³è¡¡äº†è‰æ‹Ÿå’ŒéªŒè¯èƒ½åŠ›ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒTiDARåœ¨ç”Ÿæˆå’Œä¼¼ç„¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆæ•ˆç‡å’Œè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08217', 'title': 'MADD: Multi-Agent Drug Discovery Orchestra', 'url': 'https://huggingface.co/papers/2511.08217', 'abstract': 'MADD, a multi-agent system integrating large language models and specialized models, streamlines hit identification in early drug discovery with superior performance and accessibility.  \t\t\t\t\tAI-generated summary \t\t\t\t Hit identification is a central challenge in early drug discovery, traditionally requiring substantial experimental resources. Recent advances in artificial intelligence, particularly large language models (LLMs), have enabled virtual screening methods that reduce costs and improve efficiency. However, the growing complexity of these tools has limited their accessibility to wet-lab researchers. Multi-agent systems offer a promising solution by combining the interpretability of LLMs with the precision of specialized models and tools. In this work, we present MADD, a multi-agent system that builds and executes customized hit identification pipelines from natural language queries. MADD employs four coordinated agents to handle key subtasks in de novo compound generation and screening. We evaluate MADD across seven drug discovery cases and demonstrate its superior performance compared to existing LLM-based solutions. Using MADD, we pioneer the application of AI-first drug design to five biological targets and release the identified hit molecules. Finally, we introduce a new benchmark of query-molecule pairs and docking scores for over three million compounds to contribute to the agentic future of drug design.', 'score': 55, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '2cfc090b8623b2f3', 'authors': ['Gleb V. Solovev', 'Alina B. Zhidkovskaya', 'Anastasia Orlova', 'Nina Gubina', 'Anastasia Vepreva', 'Rodion Golovinskii', 'Ilya Tonkii', 'Ivan Dubrovsky', 'Ivan Gurev', 'Dmitry Gilemkhanov', 'Denis Chistiakov', 'Timur A. Aliev', 'Ivan Poddiakov', 'Galina Zubkova', 'Ekaterina V. Skorb', 'Vladimir Vinogradov', 'Alexander Boukhanovsky', 'Nikolay Nikitin', 'Andrei Dmitrenko', 'Anna Kalyuzhnaya', 'Andrey Savchenko'], 'affiliations': ['D ONE AG, Zurich, Switzerland', 'HSE University, Moscow, Russia', 'ITMO University, Saint Petersburg, Russia', 'Sber AI Lab, Moscow, Russia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08217.jpg', 'data': {'categories': ['#healthcare', '#benchmark', '#open_source', '#agents', '#dataset', '#science'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²', 'desc': 'MADD â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞºÑ€Ğ¸Ğ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ´ĞµĞ»Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ MADD Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ„Ğ°Ñ€Ğ¼Ğ°ĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ñ Ñ‚Ñ€ĞµĞ¼Ñ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ AI-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ñƒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ².'}, 'en': {'title': 'MADD: Revolutionizing Drug Discovery with AI Agents', 'desc': 'MADD is a multi-agent system that enhances hit identification in early drug discovery by integrating large language models (LLMs) with specialized models. It simplifies the process by allowing researchers to create customized pipelines through natural language queries, making advanced AI tools more accessible. The system employs four coordinated agents to efficiently manage tasks like compound generation and screening. In evaluations across seven drug discovery cases, MADD outperformed existing LLM-based methods, paving the way for AI-driven drug design and providing a new benchmark for future research.'}, 'zh': {'title': 'MADDï¼šè¯ç‰©å‘ç°çš„æ–°æ™ºèƒ½åŠ©æ‰‹', 'desc': 'MADDæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸“ä¸šæ¨¡å‹ï¼Œæ—¨åœ¨ç®€åŒ–æ—©æœŸè¯ç‰©å‘ç°ä¸­çš„å‘½ä¸­è¯†åˆ«è¿‡ç¨‹ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ„å»ºå’Œæ‰§è¡Œå®šåˆ¶çš„å‘½ä¸­è¯†åˆ«æµç¨‹ï¼Œæå‡äº†æ•ˆç‡å’Œå¯è®¿é—®æ€§ã€‚MADDä½¿ç”¨å››ä¸ªåè°ƒçš„æ™ºèƒ½ä½“æ¥å¤„ç†æ–°åŒ–åˆç‰©ç”Ÿæˆå’Œç­›é€‰çš„å…³é”®å­ä»»åŠ¡ï¼Œå¹¶åœ¨ä¸ƒä¸ªè¯ç‰©å‘ç°æ¡ˆä¾‹ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜ä¸ºè¶…è¿‡ä¸‰ç™¾ä¸‡ä¸ªåŒ–åˆç‰©å¼•å…¥äº†æ–°çš„æŸ¥è¯¢-åˆ†å­å¯¹å’Œå¯¹æ¥åˆ†æ•°åŸºå‡†ï¼Œæ¨åŠ¨äº†è¯ç‰©è®¾è®¡çš„æ™ºèƒ½åŒ–æœªæ¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08633', 'title': 'Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising', 'url': 'https://huggingface.co/papers/2511.08633', 'abstract': "Time-to-Move (TTM) is a plug-and-play framework for motion- and appearance-controlled video generation using image-to-video (I2V) diffusion models, offering precise control over video content without requiring additional training.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.", 'score': 53, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': 'eab63b7df1ce2181', 'authors': ['Assaf Singer', 'Noam Rotstein', 'Amir Mann', 'Ron Kimmel', 'Or Litany'], 'affiliations': ['NVIDIA', 'Technion Israel Institute of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08633.jpg', 'data': {'categories': ['#open_source', '#video', '#diffusion', '#multimodal', '#inference'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Time-to-Move â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€ÑƒĞ±Ñ‹Ğµ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… ĞºĞ°Ğº Ğ³Ñ€ÑƒĞ±Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ dual-clock denoising, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ° Ğ² Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ½ÑƒĞ»ĞµĞ²ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Effortless Motion and Appearance Control in Video Generation', 'desc': 'Time-to-Move (TTM) is a novel framework that enhances video generation by allowing users to control both motion and appearance without needing additional training. It leverages image-to-video (I2V) diffusion models and utilizes simple user manipulations to create reference animations that guide the video generation process. TTM introduces a dual-clock denoising technique that ensures strong alignment in specified motion areas while maintaining flexibility in others, achieving a balance between user intent and natural movement. Extensive testing shows that TTM performs comparably or better than traditional methods that require extensive training, offering precise control over video content.'}, 'zh': {'title': 'ç²¾å‡†æ§åˆ¶è§†é¢‘ç”Ÿæˆçš„å…¨æ–°æ¡†æ¶', 'desc': 'Time-to-Move (TTM) æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¡†æ¶ï¼Œç”¨äºé€šè¿‡å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ‰©æ•£æ¨¡å‹ç”Ÿæˆå—è¿åŠ¨å’Œå¤–è§‚æ§åˆ¶çš„è§†é¢‘ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶è§†é¢‘å†…å®¹ï¼Œå…‹æœäº†ç°æœ‰åŸºäºå›¾åƒå’Œæ–‡æœ¬çš„æ¡ä»¶ç”Ÿæˆåœ¨è¿åŠ¨æ§åˆ¶ä¸Šçš„ä¸è¶³ã€‚TTM é€šè¿‡ç”¨æˆ·å‹å¥½çš„æ“ä½œè·å–ç²—ç•¥çš„å‚è€ƒåŠ¨ç”»ï¼Œå¹¶å°†å…¶ä½œä¸ºç²—ç•¥è¿åŠ¨çº¿ç´¢ï¼Œé€‚åº”è§†é¢‘ç”Ÿæˆé¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTTM åœ¨çœŸå®æ„Ÿå’Œè¿åŠ¨æ§åˆ¶æ–¹é¢ä¸ç°æœ‰çš„è®­ç»ƒåŸºç¡€æ–¹æ³•ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼ŒåŒæ—¶æä¾›äº†åƒç´ çº§çš„å¤–è§‚æ§åˆ¶èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07464', 'title': 'Motif 2 12.7B technical report', 'url': 'https://huggingface.co/papers/2511.07464', 'abstract': 'Motif-2-12.7B combines architectural innovations and system optimizations to enhance efficiency and performance in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.', 'score': 38, 'issue_id': 1, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '9d81b847377089b9', 'authors': ['Junghwan Lim', 'Sungmin Lee', 'Dongseok Kim', 'Taehyun Kim', 'Eunhwan Park', 'Jeesoo Lee', 'Jeongdoo Lee', 'Junhyeok Lee', 'Wai Ting Cheung', 'Dahye Choi', 'Jaeheui Her', 'Jaeyeon Huh', 'Hanbin Jung', 'Changjin Kang', 'Beomgyu Kim', 'Minjae Kim', 'Taewhan Kim', 'Youngrok Kim', 'Hyukjin Kweon', 'Haesol Lee', 'Kungyu Lee', 'Dongpin Oh', 'Yeongjae Park', 'Bokki Ryu', 'Dongjoo Weon'], 'affiliations': ['Motif Technologies'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07464.jpg', 'data': {'categories': ['#open_source', '#optimization', '#architecture', '#training', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº: Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Motif-2-12.7B, Ğ½Ğ¾Ğ²ÑƒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Grouped Differential Attention, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¿ÑƒÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¸ ÑˆÑƒĞ¼Ğ°. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° 5.5 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° MuonClip Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ´Ñ€Ğ°Ğ¼Ğ¸. Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… LLM.'}, 'en': {'title': 'Efficiency Meets Performance in Language Models', 'desc': 'Motif-2-12.7B is a new large language model that improves efficiency and performance through innovative architecture and system optimizations. It features Grouped Differential Attention (GDA), which enhances how the model processes information by separating important signals from noise. The model is trained on a vast dataset of 5.5 trillion tokens, using a smart scheduling method to gradually adjust the data it learns from. After training, a three-stage fine-tuning process is applied to improve its ability to follow instructions and understand language, allowing it to perform well on various tasks compared to larger models.'}, 'zh': {'title': 'é«˜æ•ˆä¸æ€§èƒ½çš„å®Œç¾ç»“åˆï¼šMotif-2-12.7B', 'desc': 'Motif-2-12.7B æ˜¯ä¸€ç§æ–°å‹çš„å¼€æºåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆæ¶æ„åˆ›æ–°å’Œç³»ç»Ÿä¼˜åŒ–ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†åˆ†ç»„å·®å¼‚æ³¨æ„åŠ›ï¼ˆGDAï¼‰æœºåˆ¶ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ†ç¦»ä¿¡å·å’Œå™ªå£°ï¼Œä»è€Œæé«˜è¡¨ç¤ºæ•ˆç‡ã€‚å®ƒåœ¨5.5ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–äº†å¤šç§è¯­è¨€ã€æ•°å­¦ã€ç§‘å­¦å’Œç¼–ç¨‹é¢†åŸŸï¼Œå¹¶ä½¿ç”¨é€æ­¥è°ƒæ•´çš„æ•°æ®è°ƒåº¦å™¨æ¥ä¼˜åŒ–æ•°æ®ç»„æˆæ¯”ä¾‹ã€‚ç»è¿‡ä¸‰é˜¶æ®µçš„ç›‘ç£å¾®è°ƒï¼ŒMotif-2-12.7B åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†ç²¾å¿ƒè®¾è®¡çš„æ¶æ„æ‰©å±•å’Œä¼˜åŒ–è®­ç»ƒæ–¹æ³•å¯ä»¥ä¸æ›´å¤§æ¨¡å‹çš„èƒ½åŠ›ç›¸åª²ç¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09515', 'title': 'WMPO: World Model-based Policy Optimization for Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2511.09515', 'abstract': 'WMPO, a pixel-based world-model framework for VLA RL, enhances sample efficiency, performance, self-correction, and generalization in robotic manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.', 'score': 18, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '4c71f55c00430bbb', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#rl', '#multimodal', '#training', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ WMPO â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¸Ñ€Ğ°, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ VLA, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²ĞµĞ±-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ on-policy Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ off-policy Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'WMPO: Revolutionizing Robotic Learning with Pixel-Based World Models', 'desc': 'WMPO is a new framework designed to improve reinforcement learning (RL) for robotic manipulation tasks by using pixel-based world models. Unlike traditional methods that rely on expert demonstrations, WMPO allows robots to learn from their own experiences, enhancing their ability to self-correct and adapt. This approach significantly reduces the number of interactions needed with the real environment, making learning more efficient. Through extensive testing, WMPO has shown to outperform existing methods in terms of performance, generalization, and the ability to learn continuously over time.'}, 'zh': {'title': 'WMPOï¼šæå‡æœºå™¨äººæ“ä½œçš„æ ·æœ¬æ•ˆç‡ä¸æ€§èƒ½', 'desc': 'WMPOæ˜¯ä¸€ç§åŸºäºåƒç´ çš„ä¸–ç•Œæ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ä¸ä¸çœŸå®ç¯å¢ƒäº’åŠ¨çš„æƒ…å†µä¸‹è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œå…‹æœäº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨çœŸå®æœºå™¨äººä¸Šé«˜æ ·æœ¬å¤æ‚åº¦çš„é—®é¢˜ã€‚WMPOä¸“æ³¨äºåƒç´ çº§é¢„æµ‹ï¼Œä½¿å¾—â€œæƒ³è±¡â€çš„è½¨è¿¹ä¸é¢„è®­ç»ƒçš„VLAç‰¹å¾å¯¹é½ï¼Œä»è€Œå®ç°æ›´å¼ºçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWMPOåœ¨æ ·æœ¬æ•ˆç‡ã€æ•´ä½“æ€§èƒ½ã€è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09148', 'title': 'LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls', 'url': 'https://huggingface.co/papers/2511.09148', 'abstract': "A fully automated data evolution framework, LoopTool, enhances tool-use capabilities of Large Language Models by iteratively refining data and model through a closed-loop process.  \t\t\t\t\tAI-generated summary \t\t\t\t Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.", 'score': 16, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '731a46a95599a2a7', 'authors': ['Kangning Zhang', 'Wenxiang Jiao', 'Kounianhua Du', 'Yuan Lu', 'Weiwen Liu', 'Weinan Zhang', 'Yong Yu'], 'affiliations': ['Shanghai Jiao Tong University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09148.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#data', '#training', '#synthetic'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ—Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM', 'desc': 'LoopTool â€” ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ—Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³Ğ´Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ 8B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LoopTool, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 32B Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… BFCL-v3 Ğ¸ ACEBench Ğ² ÑĞ²Ğ¾Ñ‘Ğ¼ ĞºĞ»Ğ°ÑÑĞµ.'}, 'en': {'title': 'Revolutionizing LLMs with Closed-Loop Data Evolution', 'desc': 'LoopTool is a novel framework designed to improve the performance of Large Language Models (LLMs) by integrating data generation and model training into a closed-loop system. It addresses the limitations of traditional static data pipelines by allowing the model to iteratively refine both its training data and its own capabilities. The framework includes three key components: Greedy Capability Probing to identify strengths and weaknesses, Judgement-Guided Label Verification to correct data errors, and Error-Driven Data Expansion to create new training samples based on model failures. This approach not only enhances training efficiency but also leads to superior performance on benchmark tasks compared to larger models using conventional methods.'}, 'zh': {'title': 'é—­ç¯è‡ªæˆ‘ä¼˜åŒ–ï¼Œæå‡LLMså·¥å…·ä½¿ç”¨èƒ½åŠ›', 'desc': 'LoopToolæ˜¯ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„æ•°æ®æ¼”åŒ–æ¡†æ¶ï¼Œé€šè¿‡é—­ç¯è¿‡ç¨‹è¿­ä»£åœ°ä¼˜åŒ–æ•°æ®å’Œæ¨¡å‹ï¼Œä»è€Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªååŒæ¨¡å—å®ç°æ•°æ®å’Œæ¨¡å‹çš„ç²¾ç»†åŒ–ï¼šè´ªå©ªèƒ½åŠ›æ¢æµ‹ï¼ˆGCPï¼‰è¯Šæ–­æ¨¡å‹çš„æŒæ¡å’Œå¤±è´¥èƒ½åŠ›ï¼›åˆ¤æ–­å¼•å¯¼çš„æ ‡ç­¾éªŒè¯ï¼ˆJGLVï¼‰åˆ©ç”¨å¼€æºè¯„åˆ¤æ¨¡å‹å‘ç°å¹¶çº æ­£æ ‡æ³¨é”™è¯¯ï¼Œé€æ­¥å‡€åŒ–æ•°æ®é›†ï¼›é”™è¯¯é©±åŠ¨çš„æ•°æ®æ‰©å±•ï¼ˆEDDEï¼‰åŸºäºè¯†åˆ«çš„å¤±è´¥ç”Ÿæˆæ–°çš„æŒ‘æˆ˜æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨LoopToolè®­ç»ƒçš„8Bæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†å…¶32Bæ•°æ®ç”Ÿæˆå™¨ï¼Œå±•ç¤ºäº†é—­ç¯è‡ªæˆ‘ä¼˜åŒ–æ•°æ®ç®¡é“å¯¹LLMså·¥å…·ä½¿ç”¨èƒ½åŠ›çš„æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06251', 'title': 'WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation', 'url': 'https://huggingface.co/papers/2511.06251', 'abstract': 'WebVIA is an agentic framework for generating executable and interactive UI code from design mockups, improving stability and accuracy over existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at https://zheny2751-dotcom.github.io/webvia.github.io/{https://webvia.github.io}.', 'score': 13, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': 'a6c8eaf212a56433', 'authors': ['Mingde Xu', 'Zhen Yang', 'Wenyi Hong', 'Lihang Pan', 'Xinyue Fan', 'Yan Wang', 'Xiaotao Gu', 'Bin Xu', 'Jie Tang'], 'affiliations': ['Faculty of Mathematics, University of Waterloo', 'The Knowledge Engineering Group (KEG), Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06251.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#cv', '#agents', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹ Ğ¸Ğ· Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°', 'desc': 'WebVIA â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½-Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ UI, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ UI2Code Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ WebVIA-Agent Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ². Ğ¢Ğ¾Ğ½ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ WebVIA-UI2Code Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾Ğ³Ğ¾ HTML/CSS/JavaScript ĞºĞ¾Ğ´Ğ° ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… UI-ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Transforming Design Mockups into Interactive Code with WebVIA', 'desc': 'WebVIA is a novel framework designed to convert design mockups into interactive user interface (UI) code, enhancing the stability and accuracy of this process. Unlike traditional Vision-Language Models (VLMs) that only produce static layouts, WebVIA introduces an exploration agent to capture dynamic UI states and a UI2Code model that generates executable code. Additionally, it includes a validation module to ensure the generated code is interactive. Experiments show that WebVIA outperforms existing models in both stability and accuracy for UI exploration and code generation.'}, 'zh': {'title': 'WebVIAï¼šäº¤äº’å¼ç”¨æˆ·ç•Œé¢ä»£ç ç”Ÿæˆçš„æ–°æ¡†æ¶', 'desc': 'WebVIAæ˜¯ä¸€ä¸ªç”¨äºä»è®¾è®¡æ¨¡å‹ç”Ÿæˆå¯æ‰§è¡Œå’Œäº¤äº’å¼ç”¨æˆ·ç•Œé¢ä»£ç çš„æ¡†æ¶ï¼Œæå‡äº†ç°æœ‰æ¨¡å‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šæ¢ç´¢ä»£ç†ã€UIåˆ°ä»£ç æ¨¡å‹å’ŒéªŒè¯æ¨¡å—ã€‚æ¢ç´¢ä»£ç†è´Ÿè´£æ•æ‰å¤šçŠ¶æ€çš„ç”¨æˆ·ç•Œé¢æˆªå›¾ï¼ŒUIåˆ°ä»£ç æ¨¡å‹ç”Ÿæˆå¯æ‰§è¡Œçš„äº¤äº’å¼ä»£ç ï¼Œè€ŒéªŒè¯æ¨¡å—åˆ™ç¡®ä¿ç”Ÿæˆä»£ç çš„äº¤äº’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWebVIAåœ¨ç”¨æˆ·ç•Œé¢æ¢ç´¢æ–¹é¢çš„è¡¨ç°ä¼˜äºä¸€èˆ¬ç›®çš„çš„ä»£ç†ï¼Œä¸”ç”Ÿæˆçš„ä»£ç åœ¨äº¤äº’æ€§å’Œå¯æ‰§è¡Œæ€§ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06805', 'title': 'MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning', 'url': 'https://huggingface.co/papers/2511.06805', 'abstract': "A proposed Mathematical Self-Evolving framework iteratively refines multimodal large language models through inference, reflection, and reward-based feedback, achieving superior performance in mathematical reasoning tasks compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \\method, a Mathematical Self-Evolving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \\method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \\method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at https://zheny2751\\allowbreak-dotcom.github.io/\\allowbreak MathSE.github.io/.", 'score': 12, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '815ba7d57552083e', 'authors': ['Jinhao Chen', 'Zhen Yang', 'Jianxin Shi', 'Tianyu Wo', 'Jie Tang'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University', 'School of Software, Beihang University', 'Zhipu AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06805.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#multimodal', '#training', '#math', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Mathematical Self-Evolving, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¾Ñ‚ĞºĞ°Ğ·Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² (ORM) Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¹ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Evolving Math Models for Superior Reasoning', 'desc': 'The paper introduces a new framework called Mathematical Self-Evolving (MSE) for improving multimodal large language models (MLLMs) in mathematical reasoning tasks. Unlike traditional methods that rely on static datasets, MSE uses an iterative process of inference, reflection, and reward-based feedback to enhance model performance. This approach allows the model to learn from its previous reasoning attempts and adapt to more complex problems. Experimental results show that MSE significantly outperforms existing models, demonstrating its effectiveness in tackling challenging mathematical benchmarks.'}, 'zh': {'title': 'æ•°å­¦è‡ªæˆ‘è¿›åŒ–ï¼šæå‡æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°å­¦è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œé€šè¿‡æ¨ç†ã€åæ€å’ŒåŸºäºå¥–åŠ±çš„åé¦ˆï¼Œè¿­ä»£åœ°ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ä¾èµ–äºé™æ€çš„æ•™å¸ˆæ¨¡å‹æ•°æ®é›†ï¼Œé™åˆ¶äº†æ¨¡å‹å¯¹å¤æ‚é—®é¢˜çš„é€‚åº”èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å…¥æ­£ç¡®çš„æ¨ç†è·¯å¾„å’Œä¸“é—¨çš„ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰è¿›è¡Œè¿­ä»£å¾®è°ƒï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè¶…è¶Šäº†é¢†å…ˆçš„å¼€æºå¤šæ¨¡æ€æ•°å­¦æ¨ç†æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06101', 'title': 'Adapting Web Agents with Synthetic Supervision', 'url': 'https://huggingface.co/papers/2511.06101', 'abstract': 'SynthAgent is a synthetic supervision framework that refines both tasks and trajectories to improve data quality and enhance web agent adaptation to new websites.  \t\t\t\t\tAI-generated summary \t\t\t\t Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': 'bc445af93f363e06', 'authors': ['Zhaoyang Wang', 'Yiming Liang', 'Xuchao Zhang', 'Qianhui Wu', 'Siwei Han', 'Anson Bastos', 'Rujia Wang', 'Chetan Bansal', 'Baolin Peng', 'Jianfeng Gao', 'Saravan Rajmohan', 'Huaxiu Yao'], 'affiliations': ['Microsoft', 'Purdue University', 'UNC-Chapel Hill'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06101.jpg', 'data': {'categories': ['#open_source', '#dataset', '#agents', '#data', '#hallucinations', '#synthetic'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'SynthAgent â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ°Ğ¹Ñ‚Ğ°Ğ¼, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ±Ğ¾Ñ€Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ fine-tuning Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Enhancing Web Agent Adaptation with SynthAgent', 'desc': 'SynthAgent is a framework designed to enhance the quality of synthetic data used for training web agents, which often struggle to adapt to new websites. It addresses the common issues of data quality, such as hallucinations in synthesized tasks and noisy trajectories. The framework employs a dual refinement process, first by generating diverse tasks through systematic exploration of web elements, and then by refining these tasks and trajectories based on real observations. By fine-tuning web agents on this improved synthetic data, SynthAgent significantly boosts their adaptability to new environments, outperforming previous methods in synthetic data generation.'}, 'zh': {'title': 'åˆæˆç›‘ç£ï¼Œæå‡ç½‘ç»œä»£ç†é€‚åº”èƒ½åŠ›', 'desc': 'SynthAgent æ˜¯ä¸€ä¸ªåˆæˆç›‘ç£æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŒé‡ä¼˜åŒ–ä»»åŠ¡å’Œè½¨è¿¹æ¥æé«˜æ•°æ®è´¨é‡ï¼Œå¢å¼ºç½‘ç»œä»£ç†å¯¹æ–°ç½‘ç«™çš„é€‚åº”èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹ç½‘é¡µå…ƒç´ çš„åˆ†ç±»æ¢ç´¢ï¼Œåˆæˆå¤šæ ·åŒ–çš„ä»»åŠ¡ï¼Œç¡®ä¿å¯¹ç›®æ ‡ç¯å¢ƒçš„æœ‰æ•ˆè¦†ç›–ã€‚åœ¨è½¨è¿¹æ”¶é›†è¿‡ç¨‹ä¸­ï¼Œå½“æ£€æµ‹åˆ°ä¸å®é™…è§‚å¯Ÿçš„å†²çªæ—¶ï¼Œä¼šå¯¹ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œå‡å°‘è™šå‡ä¿¡æ¯å¹¶ä¿æŒä»»åŠ¡çš„ä¸€è‡´æ€§ã€‚æœ€åï¼Œé€šè¿‡å¯¹ç²¾ç‚¼åçš„åˆæˆæ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæå‡å¼€æºç½‘ç»œä»£ç†åœ¨ç›®æ ‡ç¯å¢ƒä¸­çš„é€‚åº”æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07499', 'title': 'Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance', 'url': 'https://huggingface.co/papers/2511.07499', 'abstract': 'A new guidance method called Adversarial Sinkhorn Attention Guidance improves diffusion model performance by injecting adversarial costs into self-attention layers, enhancing sample quality and controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'b941ff95191efab4', 'authors': ['Kwanyoung Kim'], 'affiliations': ['Samsung Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07499.jpg', 'data': {'categories': ['#diffusion', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ±Ğ¾Ñ€ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Adversarial Sinkhorn Attention Guidance (ASAG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ±Ğ¾Ñ€ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¸ self-attention, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Sinkhorn Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ ĞºĞ»ÑÑ‡Ğ°Ğ¼Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğµ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ASAG Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ‚Ğ¸Ğ¿Ğ° IP-Adapter Ğ¸ ControlNet, Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¼ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Diffusion Models with Adversarial Guidance', 'desc': 'The paper introduces Adversarial Sinkhorn Attention Guidance (ASAG), a new method that enhances diffusion models by modifying self-attention layers with adversarial costs. This approach leverages the Sinkhorn algorithm to disrupt the transport cost, which helps to reduce pixel-wise similarity between queries and keys in the attention mechanism. By intentionally degrading misleading attention alignments, ASAG improves both conditional and unconditional sample quality. The method is lightweight and can be easily integrated into existing models without the need for retraining, showing significant improvements in applications like text-to-image generation.'}, 'zh': {'title': 'å¯¹æŠ—Sinkhornæ³¨æ„åŠ›æŒ‡å¯¼ï¼šæå‡æ‰©æ•£æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŒ‡å¯¼æ–¹æ³•ï¼Œç§°ä¸ºå¯¹æŠ—Sinkhornæ³¨æ„åŠ›æŒ‡å¯¼ï¼ˆASAGï¼‰ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚ASAGé€šè¿‡åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­æ³¨å…¥å¯¹æŠ—æˆæœ¬ï¼Œå¢å¼ºäº†æ ·æœ¬çš„è´¨é‡å’Œå¯æ§æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒASAGåˆ©ç”¨æœ€ä¼˜ä¼ è¾“çš„è§†è§’é‡æ–°è§£é‡Šæ³¨æ„åŠ›åˆ†æ•°ï¼Œå¹¶é€šè¿‡Sinkhornç®—æ³•æ•…æ„ç ´åä¼ è¾“æˆæœ¬ã€‚è¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸€è‡´çš„æ”¹è¿›ï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå…·æœ‰è½»é‡çº§å’Œå³æ’å³ç”¨çš„ç‰¹ç‚¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04824', 'title': 'Agentic Refactoring: An Empirical Study of AI Coding Agents', 'url': 'https://huggingface.co/papers/2511.04824', 'abstract': 'AI agents frequently perform refactoring in open-source Java projects, focusing on low-level consistency edits and improving code quality metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are transforming the software engineering landscape. These AI-powered systems function as autonomous teammates capable of planning and executing complex development tasks. Agents have become active participants in refactoring, a cornerstone of sustainable software development aimed at improving internal code quality without altering observable behavior. Despite their increasing adoption, there is a critical lack of empirical understanding regarding how agentic refactoring is utilized in practice, how it compares to human-driven refactoring, and what impact it has on code quality. To address this empirical gap, we present a large-scale study of AI agent-generated refactorings in real-world open-source Java projects, analyzing 15,451 refactoring instances across 12,256 pull requests and 14,988 commits derived from the AIDev dataset. Our empirical analysis shows that refactoring is a common and intentional activity in this development paradigm, with agents explicitly targeting refactoring in 26.1% of commits. Analysis of refactoring types reveals that agentic efforts are dominated by low-level, consistency-oriented edits, such as Change Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable (8.5%), reflecting a preference for localized improvements over the high-level design changes common in human refactoring. Additionally, the motivations behind agentic refactoring focus overwhelmingly on internal quality concerns, with maintainability (52.5%) and readability (28.1%). Furthermore, quantitative evaluation of code quality metrics shows that agentic refactoring yields small but statistically significant improvements in structural metrics, particularly for medium-level changes, reducing class size and complexity (e.g., Class LOC median Î” = -15.25).', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '568c78587497ae69', 'authors': ['Kosei Horikawa', 'Hao Li', 'Yutaro Kashiwa', 'Bram Adams', 'Hajimu Iida', 'Ahmed E. Hassan'], 'affiliations': ['Nara Institute of Science and Technology', 'Queens University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04824.jpg', 'data': {'categories': ['#agents', '#dataset', '#open_source', '#plp'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‚ ĞºĞ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¸ĞºÑ€Ğ¾-Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ (Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº OpenAI Codex Ğ¸ Claude) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Java-Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. Ğ£Ñ‡Ñ‘Ğ½Ñ‹Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 15 Ñ‚Ñ‹ÑÑÑ‡ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ°Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ…, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¿ĞµÑ€ĞµĞ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ², Ğ° Ğ½Ğµ Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑÑ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² â€” Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ° (52.5%) Ğ¸ ĞµĞ³Ğ¾ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ (28.1%), Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ. ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğ¼, Ğ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº ĞºĞ¾Ğ´Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ¸ Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'AI Agents: Transforming Code Quality Through Smart Refactoring', 'desc': 'This paper investigates the role of AI agents in refactoring Java code within open-source projects, highlighting their focus on low-level consistency edits to enhance code quality. The study analyzes a substantial dataset of refactoring instances, revealing that AI agents target refactoring in over a quarter of their commits, primarily making localized changes like renaming variables. The findings indicate that these agents prioritize internal quality improvements, such as maintainability and readability, over broader design alterations typical of human refactoring. Additionally, the research shows that while the improvements in code quality metrics are modest, they are statistically significant, particularly in reducing class size and complexity.'}, 'zh': {'title': 'AIä»£ç†é‡æ„ï¼šæå‡ä»£ç è´¨é‡çš„æ–°åŠ›é‡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†AIä»£ç†åœ¨å¼€æºJavaé¡¹ç›®ä¸­çš„é‡æ„å®è·µï¼Œåˆ†æäº†15451ä¸ªé‡æ„å®ä¾‹ã€‚AIç”Ÿæˆçš„é‡æ„ä¸»è¦é›†ä¸­åœ¨ä½çº§åˆ«çš„ä¸€è‡´æ€§ç¼–è¾‘ä¸Šï¼Œå¦‚å˜é‡é‡å‘½åå’Œç±»å‹æ›´æ”¹ï¼Œæ˜¾ç¤ºå‡ºå¯¹å±€éƒ¨æ”¹è¿›çš„åå¥½ã€‚ç ”ç©¶å‘ç°ï¼ŒAIä»£ç†çš„é‡æ„æ´»åŠ¨åœ¨å¼€å‘ä¸­å æ®äº†26.1%çš„æäº¤ï¼Œä¸»è¦å…³æ³¨ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œå¯è¯»æ€§ã€‚å°½ç®¡AIé‡æ„å¸¦æ¥äº†å°å¹…ä½†æ˜¾è‘—çš„ç»“æ„æ€§è´¨é‡æå‡ï¼Œä½†ä¸äººç±»é©±åŠ¨çš„é‡æ„ç›¸æ¯”ï¼ŒAIçš„é‡æ„æ–¹å¼ä»ç„¶å­˜åœ¨å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06073', 'title': 'Stemming Hallucination in Language Models Using a Licensing Oracle', 'url': 'https://huggingface.co/papers/2511.06073', 'abstract': "The Licensing Oracle, an architectural solution, eliminates hallucinations in language models by enforcing truth constraints through formal validation against structured knowledge graphs, achieving perfect abstention precision and zero false answers.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.", 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': '67987a4264f316db', 'authors': ['Simeon Emanuilov', 'Richard Ackermann'], 'affiliations': ['Department of Software Technologies, Faculty of Mathematics and Informatics, Sofia University St. Kliment Ohridski, Bulgaria', 'RA Software, San Diego, United States'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06073.jpg', 'data': {'categories': ['#rag', '#hallucinations', '#architecture', '#graphs'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜ÑÑ‚Ğ¸Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ: Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Licensing Oracle, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ fine-tuning, ÑÑ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ RAG Ğ¸ fine-tuning, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Licensing Oracle Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ (AP = 1.0) Ğ¸ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² (FAR-NE = 0.0). Ğ”Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‡Ğ¸ÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Eliminating Hallucinations with the Licensing Oracle', 'desc': 'The paper presents the Licensing Oracle, a new architectural solution aimed at reducing hallucinations in language models (LMs). Hallucinations refer to instances where LMs generate incorrect information despite sounding plausible. The Licensing Oracle enforces truth constraints by validating outputs against structured knowledge graphs, ensuring that only accurate information is produced. Experimental results show that this approach achieves perfect abstention precision and zero false answers, outperforming traditional methods like fine-tuning and retrieval-augmented generation.'}, 'zh': {'title': 'è®¸å¯Oracleï¼šæ¶ˆé™¤è¯­è¨€æ¨¡å‹å¹»è§‰çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºè®¸å¯Oracleçš„æ¶æ„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨æ¶ˆé™¤è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰ç°è±¡ã€‚é€šè¿‡å¯¹ç»“æ„åŒ–çŸ¥è¯†å›¾è¿›è¡Œæ­£å¼éªŒè¯ï¼Œè®¸å¯Oracleå¼ºåˆ¶æ‰§è¡ŒçœŸç›¸çº¦æŸï¼Œä»è€Œç¡®ä¿ç”Ÿæˆçš„å†…å®¹æ˜¯äº‹å®å‡†ç¡®çš„ã€‚ä¸ä¾èµ–æ•°æ®æ‰©å±•æˆ–å¾®è°ƒçš„ç»Ÿè®¡æ–¹æ³•ä¸åŒï¼Œè®¸å¯Oracleåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åµŒå…¥äº†ç¡®å®šæ€§çš„éªŒè¯æ­¥éª¤ï¼Œç¡®ä¿åªç”Ÿæˆæœ‰æ•ˆçš„å£°æ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè®¸å¯Oracleåœ¨äº‹å®å“åº”çš„å‡†ç¡®æ€§ä¸Šè¾¾åˆ°äº†89.1%ï¼Œå¹¶å®ç°äº†å®Œç¾çš„å¼ƒæƒç²¾åº¦å’Œé›¶é”™è¯¯ç­”æ¡ˆï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†ç»“æ„åŒ–çŸ¥è¯†è¡¨ç¤ºé¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (5)', '#agi', '#alignment', '#architecture (2)', '#audio', '#benchmark (4)', '#cv (2)', '#data (2)', '#dataset (3)', '#diffusion (3)', '#ethics', '#games (1)', '#graphs (1)', '#hallucinations (2)', '#healthcare (1)', '#inference (2)', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (4)', '#open_source (9)', '#optimization (5)', '#plp (1)', '#rag (1)', '#reasoning (1)', '#rl (1)', '#rlhf', '#robotics (1)', '#science (1)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (2)', '#training (5)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-11-13 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-11-13 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-11-13 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    