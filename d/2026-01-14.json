{
    "date": {
        "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 14",
        "zh": "1æœˆ14æ—¥"
    },
    "time_utc": "2026-01-14 16:33",
    "weekday": 2,
    "issue_id": 579,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.06789",
            "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
            "url": "https://huggingface.co/papers/2601.06789",
            "abstract": "MemGovern framework transforms unstructured GitHub data into structured experiential memory for autonomous software engineering agents, improving bug resolution rates through enhanced experience retrieval.  \t\t\t\t\tAI-generated summary \t\t\t\t While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.",
            "score": 54,
            "issue_id": 567,
            "pub_date": "2026-01-11",
            "pub_date_card": {
                "ru": "11 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 11",
                "zh": "1æœˆ11æ—¥"
            },
            "hash": "0aaa42ea5e9dedf0",
            "authors": [
                "Qihao Wang",
                "Ziming Cheng",
                "Shuo Zhang",
                "Fan Liu",
                "Rui Xu",
                "Heng Lian",
                "Kunyi Wang",
                "Xiaoming Yu",
                "Jianghao Yin",
                "Sen Hu",
                "Yue Hu",
                "Shaolei Zhang",
                "Yanbing Liu",
                "Ronghao Chen",
                "Huacan Wang"
            ],
            "affiliations": [
                "ECNU",
                "FDU",
                "HKUST(GZ)",
                "NUS",
                "PKU",
                "QuantaAlpha",
                "RUC",
                "UBC",
                "UCAS",
                "XDU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.06789.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‹Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° ĞºĞ°Ğº Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° MemGovern, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· GitHub Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ² ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸, Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ 135 Ñ‚Ñ‹ÑÑÑ‡ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° 4.65% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-bench Verified. MemGovern Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑƒĞ´Ğ¾Ğ±Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transforming GitHub Data into Smart Memory for Better Bug Fixing",
                    "desc": "The MemGovern framework enhances autonomous software engineering agents by converting unstructured GitHub data into structured experiential memory. This transformation allows agents to leverage historical human experiences, overcoming the limitations of relying solely on local context for bug resolution. By creating 135,000 governed experience cards, MemGovern facilitates logic-driven retrieval of relevant expertise, significantly improving bug resolution rates. This innovative approach provides a plug-in solution for integrating memory infrastructure into software engineering agents."
                },
                "zh": {
                    "title": "æå‡è‡ªä¸»è½¯ä»¶å·¥ç¨‹ä»£ç†çš„ç»éªŒè®°å¿†",
                    "desc": "MemGovernæ¡†æ¶å°†éç»“æ„åŒ–çš„GitHubæ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–çš„ç»éªŒè®°å¿†ï¼Œä»¥æ”¯æŒè‡ªä¸»è½¯ä»¶å·¥ç¨‹ä»£ç†çš„å·¥ä½œã€‚é€šè¿‡å¢å¼ºç»éªŒæ£€ç´¢ï¼ŒMemGovernæ˜¾è‘—æé«˜äº†é”™è¯¯è§£å†³ç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»éªŒæ²»ç†å°†äººç±»ç»éªŒè½¬æ¢ä¸ºé€‚åˆä»£ç†ä½¿ç”¨çš„ç»éªŒå¡ç‰‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºé€»è¾‘çš„ç»éªŒæœç´¢ç­–ç•¥ã€‚æœ€ç»ˆï¼ŒMemGovernç”Ÿæˆäº†135,000ä¸ªç»éªŒå¡ç‰‡ï¼Œä½¿å¾—åœ¨SWE-bench Verifiedä¸Šçš„è§£å†³ç‡æå‡äº†4.65%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.07022",
            "title": "Solar Open Technical Report",
            "url": "https://huggingface.co/papers/2601.07022",
            "abstract": "Solar Open presents a 102B-parameter bilingual Mixture-of-Experts language model that addresses data scarcity in underserved languages through synthetic data generation, progressive curriculum coordination, and scalable reinforcement learning optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.",
            "score": 46,
            "issue_id": 568,
            "pub_date": "2026-01-11",
            "pub_date_card": {
                "ru": "11 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 11",
                "zh": "1æœˆ11æ—¥"
            },
            "hash": "f88e2ba7e40d5615",
            "authors": [
                "Sungrae Park",
                "Sanghoon Kim",
                "Jungho Cho",
                "Gyoungjin Gim",
                "Dawoon Jung",
                "Mikyoung Cha",
                "Eunhae Choo",
                "Taekgyu Hong",
                "Minbyul Jeong",
                "SeHwan Joo",
                "Minsoo Khang",
                "Eunwon Kim",
                "Minjeong Kim",
                "Sujeong Kim",
                "Yunsu Kim",
                "Hyeonju Lee",
                "Seunghyun Lee",
                "Sukyung Lee",
                "Siyoung Park",
                "Gyungin Shin",
                "Inseo Song",
                "Wonho Song",
                "Seonghoon Yang",
                "Seungyoun Yi",
                "Sanghoon Yoon",
                "Jeonghyun Ko",
                "Seyoung Song",
                "Keunwoo Choi",
                "Hwalsuk Lee",
                "Sunghun Kim",
                "Du-Seong Chang",
                "Kyunghyun Cho",
                "Junsuk Choe",
                "Hwaran Lee",
                "Jae-Gil Lee",
                "KyungTae Lim",
                "Alice Oh"
            ],
            "affiliations": [
                "Upstage"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.07022.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#training",
                    "#data",
                    "#rlhf",
                    "#synthetic",
                    "#optimization",
                    "#open_source",
                    "#multilingual",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğº Ğ¼Ğ¾ÑÑ‚ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Solar Open Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ 102 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ´Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 4.5 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ°Ğ², ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 20 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ñ‘Ğ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ reinforcement learning Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SnapPO, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Underserved Languages with Solar Open's 102B-Parameter Model",
                    "desc": "The paper introduces Solar Open, a large bilingual Mixture-of-Experts language model with 102 billion parameters, designed to improve AI capabilities in underserved languages. It tackles the challenge of data scarcity by generating 4.5 trillion tokens of synthetic, high-quality data tailored for specific domains. The model employs a progressive curriculum that optimizes the composition and quality of the training data across a vast dataset of 20 trillion tokens. Additionally, it utilizes a scalable reinforcement learning framework called SnapPO to enhance reasoning abilities, achieving competitive results in benchmarks for English and Korean."
                },
                "zh": {
                    "title": "ä¸ºæ¬ æœåŠ¡è¯­è¨€å¼€è¾Ÿæ–°å¤©åœ°çš„Solar Open",
                    "desc": "Solar Open æ˜¯ä¸€ä¸ªæ‹¥æœ‰1020äº¿å‚æ•°çš„åŒè¯­æ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³èµ„æºåŒ®ä¹è¯­è¨€çš„æ•°æ®ä¸è¶³é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆæˆæ•°æ®ç”Ÿæˆã€æ¸è¿›å¼è¯¾ç¨‹åè°ƒå’Œå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¥å®ç°é«˜æ•ˆè®­ç»ƒã€‚æˆ‘ä»¬åˆæˆäº†4.5ä¸‡äº¿ä¸ªé«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸå’Œé¢å‘å¼ºåŒ–å­¦ä¹ çš„æ•°æ®ï¼Œä»¥åº”å¯¹æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚é€šè¿‡åœ¨è‹±è¯­å’ŒéŸ©è¯­çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒSolar Open å±•ç¤ºäº†å…¶åœ¨æ¬ æœåŠ¡è¯­è¨€AIå¼€å‘ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04745",
            "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions",
            "url": "https://huggingface.co/papers/2601.04745",
            "abstract": "Long-horizon memory benchmarks based on autobiographical narratives evaluate models' ability to infer stable motivations and decision principles through evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \\BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \\BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.",
            "score": 41,
            "issue_id": 571,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "9ef26640427ffeb7",
            "authors": [
                "Tingyu Wu",
                "Zhisheng Chen",
                "Ziyan Weng",
                "Shuhe Wang",
                "Chenglong Li",
                "Shuo Zhang",
                "Sen Hu",
                "Silin Wu",
                "Qizhen Lan",
                "Huacan Wang",
                "Ronghao Chen"
            ],
            "affiliations": [
                "CITYU-DG",
                "HAINNU",
                "NUS",
                "PKU",
                "QuantaAlpha",
                "THU",
                "UCAS",
                "UTHealth"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04745.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#long_context"
                ],
                "emoji": "ğŸ“–",
                "ru": {
                    "title": "ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ñ‹: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ¾Ğ¸ÑĞº Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº KnowMeBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ±Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‚Ñ€Ñ‘Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ñ„Ğ°ĞºÑ‚-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾-Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞ²ĞµÑ€Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Unlocking Long-Term Memory in AI with Autobiographical Narratives",
                    "desc": "This paper introduces a new benchmark called \\\\BenchName, designed to assess machine learning models' understanding of long-term memory through autobiographical narratives. Unlike previous benchmarks that relied on dialogues or synthetic histories, \\\\BenchName uses real-life stories to provide rich context for evaluating models' abilities to infer motivations and decision-making principles. The benchmark includes questions that require factual recall, understanding of subjective states, and reasoning at a principle level. The findings indicate that while retrieval-augmented systems can improve factual accuracy, they struggle with temporally grounded explanations and higher-level reasoning, suggesting a need for advanced memory mechanisms in AI."
                },
                "zh": {
                    "title": "é•¿æ—¶è®°å¿†è¯„ä¼°çš„æ–°åŸºå‡†æµ‹è¯•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸º\\BenchNameï¼Œæ—¨åœ¨è¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨é•¿æ—¶é—´è®°å¿†æ–¹é¢çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æµ‹è¯•åŸºäºè‡ªä¼ å™è¿°ï¼Œæä¾›äº†ä¸°å¯Œçš„è¯æ®æ¥æ¨æ–­ç¨³å®šçš„åŠ¨æœºå’Œå†³ç­–åŸåˆ™ã€‚ä¸ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼Œ\\BenchNameå…³æ³¨äºäº‹å®å›å¿†ã€ä¸»è§‚çŠ¶æ€å½’å› å’ŒåŸåˆ™çº§æ¨ç†ç­‰æ–¹é¢ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„æ£€ç´¢å¢å¼ºç³»ç»Ÿåœ¨äº‹å®å‡†ç¡®æ€§ä¸Šæœ‰æ‰€æé«˜ï¼Œä½†åœ¨æ—¶é—´åŸºç¡€çš„è§£é‡Šå’Œæ›´é«˜å±‚æ¬¡çš„æ¨ç†ä¸Šä»å­˜åœ¨é”™è¯¯ï¼Œå¼ºè°ƒäº†éœ€è¦è¶…è¶Šç®€å•æ£€ç´¢çš„è®°å¿†æœºåˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08225",
            "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
            "url": "https://huggingface.co/papers/2601.08225",
            "abstract": "Large reasoning models enable scalable multi-turn dialogue generation through automated task-oriented simulation and user-oriented behavioral modeling for enhanced human-agent interaction datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.",
            "score": 39,
            "issue_id": 568,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "4d4a176d79a2d78d",
            "authors": [
                "Jungho Cho",
                "Minbyul Jeong",
                "Sungrae Park"
            ],
            "affiliations": [
                "Upstage AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08225.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#reasoning"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒĞ³ÑƒĞ±Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing Human-Agent Dialogue with Dynamic User Simulation",
                    "desc": "This paper discusses the development of large reasoning models (LRMs) for improving multi-turn dialogue generation in human-agent interactions. It highlights the limitations of existing datasets that rely on static tools, which do not capture the complexity of real-world conversations. The authors propose a new framework that separates task generation from user simulation, allowing for more realistic and extended dialogues that mimic human behavior. This approach results in a scalable system that can generate diverse, high-density datasets for training agents in task-oriented scenarios."
                },
                "zh": {
                    "title": "æå‡äººæœºäº¤äº’çš„å¤šè½®å¯¹è¯ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æå‡äººæœºäº¤äº’çš„æ•°æ®é›†è´¨é‡ã€‚ç°æœ‰çš„æ•°æ®é›†å’Œç”Ÿæˆæ–¹æ³•å—é™äºé™æ€çš„å·¥å…·é›†ï¼Œæ— æ³•æ»¡è¶³å¼€æ”¾å¼äººæœºåä½œçš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…ä»¬å¼€å‘äº†ä¸€ç§åŸºäºLRMçš„è‡ªåŠ¨åŒ–ä»»åŠ¡å¯¼å‘å¯¹è¯ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆé«˜ä»·å€¼çš„é¢†åŸŸç‰¹å®šå·¥å…·ã€‚é€šè¿‡å¼•å…¥ç”¨æˆ·å¯¼å‘çš„æ¨¡æ‹Ÿæ–¹æ³•ï¼Œç ”ç©¶è€…ä»¬å®ç°äº†æ›´çœŸå®çš„å¤šè½®å¯¹è¯ï¼Œåæ˜ äº†ç°å®ä¸–ç•Œé—®é¢˜è§£å†³çš„è¿­ä»£ç‰¹æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.24965",
            "title": "ShowUI-Ï€: Flow-based Generative Models as GUI Dexterous Hands",
            "url": "https://huggingface.co/papers/2512.24965",
            "abstract": "Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-Ï€, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-Ï€ achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.",
            "score": 33,
            "issue_id": 568,
            "pub_date": "2026-12-31",
            "pub_date_card": {
                "ru": "31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 31",
                "zh": "12æœˆ31æ—¥"
            },
            "hash": "eee0e07745c9406a",
            "authors": [
                "Siyuan Hu",
                "Kevin Qinghong Lin",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.24965.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#agents",
                    "#small_models",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸĞ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…: Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ShowUI-Ï€ â€” Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ĞºĞ»Ğ¸ĞºĞ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºÑƒÑ€ÑĞ¾Ñ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 20 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ScreenDrag Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğº Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ShowUI-Ï€ Ñ 450 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ²ĞºĞ¸Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ."
                },
                "en": {
                    "title": "Empowering GUI Agents with Human-like Dexterity",
                    "desc": "This paper introduces ShowUI-Ï€, a novel flow-based generative model designed to enhance GUI agents' dexterous manipulation capabilities. Unlike traditional models that only predict discrete click actions, ShowUI-Ï€ integrates both discrete and continuous actions, allowing for more fluid interactions like dragging. The model utilizes a lightweight action expert to generate smooth cursor movements based on real-time visual input, improving the agent's adaptability across various tasks. The authors also present a new benchmark, ScreenDrag, to evaluate drag performance, demonstrating that ShowUI-Ï€ outperforms existing agents significantly in this challenging domain."
                },
                "zh": {
                    "title": "æå‡GUIæ™ºèƒ½ä»£ç†çš„çµæ´»æ“æ§èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºShowUI-Ï€çš„æµå¼ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æå‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ™ºèƒ½ä»£ç†çš„çµæ´»æ€§å’Œæ“æ§èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ç»“åˆäº†ç¦»æ•£ç‚¹å‡»å’Œè¿ç»­æ‹–åŠ¨çš„åŠ¨ä½œï¼Œèƒ½å¤Ÿåœ¨å¤šç§äº¤äº’æ¨¡å¼ä¸‹è‡ªå¦‚é€‚åº”ã€‚é€šè¿‡æµå¼åŠ¨ä½œç”Ÿæˆï¼ŒShowUI-Ï€èƒ½å¤Ÿæ ¹æ®å®æ—¶è§†è§‰ä¿¡æ¯é¢„æµ‹å…‰æ ‡çš„ç»†å¾®è°ƒæ•´ï¼Œä»è€Œå®ç°å¹³æ»‘ç¨³å®šçš„æ‹–åŠ¨è½¨è¿¹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«2ä¸‡æ¡æ‹–åŠ¨è½¨è¿¹çš„æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ScreenDragï¼Œä»¥æµ‹è¯•GUIä»£ç†çš„æ‹–åŠ¨èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08079",
            "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
            "url": "https://huggingface.co/papers/2601.08079",
            "abstract": "Memory management in tool-augmented agents is crucial for maintaining coherent, goal-directed reasoning over extended tasks, requiring explicit mechanisms to track and organize reasoning steps within constrained contexts.  \t\t\t\t\tAI-generated summary \t\t\t\t Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.   We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.   We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.",
            "score": 30,
            "issue_id": 574,
            "pub_date": "2026-01-12",
            "pub_date_card": {
                "ru": "12 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 12",
                "zh": "1æœˆ12æ—¥"
            },
            "hash": "d40e8e9bfd93aabf",
            "authors": [
                "Hongjin Qian",
                "Zhao Cao",
                "Zheng Liu"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08079.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#long_context",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° MemoBrain â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ³Ñ€Ğ°Ñ„ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ°Ğ´ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ñ€ÑĞ´Ğ¾Ğ¼ Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ LLM Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ², ÑĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ÑÑ‚Ğ¾Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ±ĞµĞ· Ğ½ĞµĞ³Ğ¾ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½Ğ°Ñ€ÑƒÑˆĞ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ†ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GAIA, WebWalker Ğ¸ BrowseComp-Plus Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Long-Horizon Reasoning with MemoBrain",
                    "desc": "This paper addresses the challenge of memory management in tool-augmented agents, which is essential for maintaining effective reasoning over long tasks. It introduces MemoBrain, a memory model that organizes and tracks reasoning steps while managing the limited context of large language models. MemoBrain enhances cognitive control by pruning irrelevant steps and preserving important reasoning states, ensuring logical continuity. The model is evaluated on various benchmarks, showing significant improvements in performance compared to existing methods."
                },
                "zh": {
                    "title": "å†…å­˜ç®¡ç†ï¼šå·¥å…·å¢å¼ºä»£ç†çš„æ ¸å¿ƒ",
                    "desc": "åœ¨å·¥å…·å¢å¼ºä»£ç†ä¸­ï¼Œå†…å­˜ç®¡ç†å¯¹äºç»´æŒè¿è´¯çš„ç›®æ ‡å¯¼å‘æ¨ç†è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†MemoBrainï¼Œä¸€ä¸ªæ‰§è¡Œå†…å­˜æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ­¥éª¤ä¸­æ„å»ºä¾èµ–æ„ŸçŸ¥çš„å†…å­˜ï¼Œæ•æ‰é‡è¦çš„ä¸­é—´çŠ¶æ€åŠå…¶é€»è¾‘å…³ç³»ã€‚MemoBrainä½œä¸ºæ¨ç†ä»£ç†çš„å‰¯é©¾é©¶ï¼Œç»„ç»‡æ¨ç†è¿›å±•å¹¶ä¸»åŠ¨ç®¡ç†å·¥ä½œä¸Šä¸‹æ–‡ï¼Œé¿å…æ— æ•ˆæ­¥éª¤çš„ç§¯ç´¯ã€‚é€šè¿‡åœ¨é•¿æ—¶é—´åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°ï¼ŒMemoBrainæ˜¾ç¤ºå‡ºç›¸è¾ƒäºå¼ºåŸºçº¿çš„ä¸€è‡´æ€§æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.06487",
            "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
            "url": "https://huggingface.co/papers/2601.06487",
            "abstract": "Reinforcement learning for large language model agents suffers from discrimination collapse in open-ended tasks due to pointwise scalar scoring, which ArenaRL addresses through relative ranking and pairwise evaluation mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
            "score": 27,
            "issue_id": 566,
            "pub_date": "2026-01-10",
            "pub_date_card": {
                "ru": "10 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 10",
                "zh": "1æœˆ10æ—¥"
            },
            "hash": "fa021a43db4c9cb5",
            "authors": [
                "Qiang Zhang",
                "Boli Chen",
                "Fanrui Zhang",
                "Ruixue Ding",
                "Shihang Wang",
                "Qiuchen Wang",
                "Yinfeng Huang",
                "Haonan Zhang",
                "Rongxiang Zhu",
                "Pengyong Wang",
                "Ailin Ren",
                "Xin Li",
                "Pengjun Xie",
                "Jiawei Liu",
                "Ning Guo",
                "Jingren Zhou",
                "Zheng-Jun Zha"
            ],
            "affiliations": [
                "Amap, Alibaba Group",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.06487.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#dataset",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğº Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ArenaRL Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ğ³Ğ´Ğ° ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑ‚Ğ°Ğ³Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ O(N) Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ O(NÂ²). Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ´Ğ²Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Open-Travel Ğ¸ Open-DeepResearch, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "ArenaRL: Elevating Reinforcement Learning for Language Models through Relative Ranking",
                    "desc": "This paper discusses the challenges of using reinforcement learning (RL) for large language model (LLM) agents in open-ended tasks, where traditional pointwise scalar scoring leads to discrimination collapse. The authors introduce ArenaRL, a new RL framework that replaces scalar scoring with relative ranking and pairwise evaluations to better differentiate between various solutions. By implementing a tournament-based ranking system and multi-level rubrics, ArenaRL provides more stable and precise reward signals, enhancing the optimization process. The results demonstrate that ArenaRL significantly improves the performance of LLM agents on complex tasks compared to standard RL methods."
                },
                "zh": {
                    "title": "ArenaRLï¼šæå‡å¼€æ”¾å¼ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ æ•ˆæœ",
                    "desc": "å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å¼€æ”¾å¼ä»»åŠ¡ä¸­é¢ä¸´ç€è¯„åˆ†æ­§è§†å´©æºƒçš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„ç‚¹å¯¹ç‚¹æ ‡é‡è¯„åˆ†æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåŒºåˆ†ä¸åŒè·¯å¾„ä¹‹é—´çš„ç»†å¾®ä¼˜åŠ¿ï¼Œå¯¼è‡´å¥–åŠ±ä¿¡å·è¢«å™ªå£°ä¸»å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒArenaRLæå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡ç»„å†…ç›¸å¯¹æ’åå’Œæˆå¯¹è¯„ä¼°æœºåˆ¶æ¥ä¼˜åŒ–è¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒArenaRLåœ¨å¤æ‚ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæ ‡å‡†å¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å¼ºå¥çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.07264",
            "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
            "url": "https://huggingface.co/papers/2601.07264",
            "abstract": "Tool-integrated language model agents exhibit different calibration behaviors based on tool type, with a reinforcement learning framework improving both task accuracy and reliable uncertainty estimation across diverse domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.",
            "score": 19,
            "issue_id": 566,
            "pub_date": "2026-01-12",
            "pub_date_card": {
                "ru": "12 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 12",
                "zh": "1æœˆ12æ—¥"
            },
            "hash": "3b69d6831de81c4b",
            "authors": [
                "Weihao Xuan",
                "Qingcheng Zeng",
                "Heli Qi",
                "Yunze Xiao",
                "Junjue Wang",
                "Naoto Yokoya"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Northwestern University",
                "RIKEN AIP",
                "The University of Tokyo",
                "Waseda University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.07264.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ (ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ²Ğ¾Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ…) Ğ² LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºÑƒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·-Ğ·Ğ° ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ´Ğ°) Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞµĞ±-ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Enhancing Trustworthiness in AI: Calibration through Reinforcement Learning",
                    "desc": "This paper explores how tool-integrated language model agents behave differently in terms of calibration, which is their ability to express confidence accurately about their performance. It identifies that different types of tools, like evidence tools and verification tools, affect the agents' confidence levels, with evidence tools often leading to overconfidence due to noisy information. The authors propose a reinforcement learning framework that enhances both the accuracy of tasks and the reliability of uncertainty estimation across various domains. Their findings emphasize the importance of tailored calibration strategies for agents that use different tools, paving the way for more trustworthy AI systems in real-world applications."
                },
                "zh": {
                    "title": "æå‡å·¥å…·é›†æˆä»£ç†çš„æ ¡å‡†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†é›†æˆå·¥å…·çš„è¯­è¨€æ¨¡å‹ä»£ç†åœ¨ä¸åŒå·¥å…·ç±»å‹ä¸‹çš„æ ¡å‡†è¡Œä¸ºã€‚æˆ‘ä»¬å‘ç°ï¼Œè¯æ®å·¥å…·ï¼ˆå¦‚ç½‘ç»œæœç´¢ï¼‰ä¼šå¯¼è‡´è¿‡åº¦è‡ªä¿¡ï¼Œè€ŒéªŒè¯å·¥å…·ï¼ˆå¦‚ä»£ç è§£é‡Šå™¨ï¼‰åˆ™èƒ½é€šè¿‡ç¡®å®šæ€§åé¦ˆæ¥æ”¹å–„æ ¡å‡†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶ä¼˜åŒ–ä»»åŠ¡å‡†ç¡®æ€§å’Œæ ¡å‡†æ•ˆæœã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡è®­ç»ƒçš„ä»£ç†åœ¨å¤šç§é¢†åŸŸä¸­è¡¨ç°å‡ºæ›´å¥½çš„æ ¡å‡†èƒ½åŠ›å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08584",
            "title": "Ministral 3",
            "url": "https://huggingface.co/papers/2601.08584",
            "abstract": "The Ministral 3 series consists of parameter-efficient dense language models with three sizes (3B, 8B, 14B) and three variants per size, trained using cascade distillation for compute-constrained applications.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.",
            "score": 16,
            "issue_id": 566,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "3895109cdeca63b2",
            "authors": [
                "Alexander H. Liu",
                "Kartik Khandelwal",
                "Sandeep Subramanian",
                "Victor Jouault",
                "Abhinav Rastogi",
                "Adrien SadÃ©",
                "Alan Jeffares",
                "Albert Jiang",
                "Alexandre Cahill",
                "Alexandre Gavaudan",
                "Alexandre Sablayrolles",
                "AmÃ©lie HÃ©liou",
                "Amos You",
                "Andy Ehrenberg",
                "Andy Lo",
                "Anton Eliseev",
                "Antonia Calvi",
                "Avinash Sooriyarachchi",
                "Baptiste Bout",
                "Baptiste RoziÃ¨re",
                "Baudouin De Monicault",
                "ClÃ©mence Lanfranchi",
                "Corentin Barreau",
                "Cyprien Courtot",
                "Daniele Grattarola",
                "Darius Dabert",
                "Diego de las Casas",
                "Elliot Chane-Sane",
                "Faruk Ahmed",
                "Gabrielle Berrada",
                "GaÃ«tan Ecrepont",
                "Gauthier Guinet",
                "Georgii Novikov",
                "Guillaume Kunsch",
                "Guillaume Lample",
                "Guillaume Martin",
                "Gunshi Gupta",
                "Jan Ludziejewski",
                "Jason Rute",
                "Joachim Studnia",
                "Jonas Amar",
                "JosÃ©phine Delas",
                "Josselin Somerville Roberts",
                "Karmesh Yadav",
                "Khyathi Chandu",
                "Kush Jain",
                "Laurence Aitchison",
                "Laurent Fainsin",
                "LÃ©onard Blier",
                "Lingxiao Zhao",
                "Louis Martin",
                "Lucile Saulnier",
                "Luyu Gao",
                "Maarten Buyl",
                "Margaret Jennings",
                "Marie Pellat",
                "Mark Prins",
                "Mathieu PoirÃ©e",
                "Mathilde Guillaumin",
                "Matthieu Dinot",
                "Matthieu Futeral",
                "Maxime Darrin",
                "Maximilian Augustin",
                "Mia Chiquier",
                "Michel Schimpf",
                "Nathan Grinsztajn",
                "Neha Gupta",
                "Nikhil Raghuraman",
                "Olivier Bousquet",
                "Olivier Duchenne",
                "Patricia Wang",
                "Patrick von Platen",
                "Paul Jacob",
                "Paul Wambergue",
                "Paula Kurylowicz",
                "Pavankumar Reddy Muddireddy",
                "PhilomÃ¨ne Chagniot",
                "Pierre Stock",
                "Pravesh Agrawal",
                "Quentin Torroba",
                "Romain Sauvestre",
                "Roman Soletskyi",
                "Rupert Menneer",
                "Sagar Vaze",
                "Samuel Barry",
                "Sanchit Gandhi",
                "Siddhant Waghjale",
                "Siddharth Gandhi",
                "Soham Ghosh",
                "Srijan Mishra",
                "Sumukh Aithal",
                "Szymon Antoniak",
                "Teven Le Scao",
                "ThÃ©o Cachet",
                "Theo Simon Sorg",
                "Thibaut Lavril",
                "Thiziri Nait Saada",
                "Thomas Chabal",
                "Thomas Foubert",
                "Thomas Robert",
                "Thomas Wang",
                "Tim Lawson",
                "Tom Bewley",
                "Tom Bewley",
                "Tom Edwards",
                "Umar Jamil",
                "Umberto Tomasini",
                "Valeriia Nemychnikova",
                "Van Phung",
                "Vincent MaladiÃ¨re",
                "Virgile Richard",
                "Wassim Bouaziz",
                "Wen-Ding Li",
                "William Marshall",
                "Xinghui Li",
                "Xinyu Yang",
                "Yassine El Ouahidi",
                "Yihan Wang",
                "Yunhao Tang",
                "Zaccharie Ramzi"
            ],
            "affiliations": [
                "Mistral AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08584.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ministral 3 â€” ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 3 Ğ´Ğ¾ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°: Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ²ĞµÑ€ÑĞ¸Ñ Ñ fine-tuning Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ cascade distillation â€” Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹ Apache 2.0."
                },
                "en": {
                    "title": "Efficient Language Models for Resource-Constrained Applications",
                    "desc": "The Ministral 3 series introduces a set of dense language models that are efficient in terms of parameters, making them suitable for applications with limited computational resources. Each model size, which includes 3B, 8B, and 14B parameters, offers three distinct variants: a general-purpose pretrained model, an instruction-tuned model for specific tasks, and a reasoning model aimed at solving complex problems. The models are developed using a technique called Cascade Distillation, which involves iterative pruning and continued training to enhance performance. Additionally, these models are equipped with image understanding capabilities and are released under the Apache 2.0 license."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¯†é›†è¯­è¨€æ¨¡å‹ï¼ŒåŠ©åŠ›è®¡ç®—å—é™åº”ç”¨",
                    "desc": "Ministral 3ç³»åˆ—æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¯†é›†è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œä¸“ä¸ºè®¡ç®—å’Œå†…å­˜å—é™çš„åº”ç”¨è€Œè®¾è®¡ã€‚è¯¥ç³»åˆ—åŒ…æ‹¬ä¸‰ç§æ¨¡å‹å¤§å°ï¼š3Bã€8Bå’Œ14Bå‚æ•°ï¼Œæ¯ç§å¤§å°éƒ½æœ‰ä¸‰ä¸ªå˜ä½“ï¼Œåˆ†åˆ«æ˜¯é€šç”¨é¢„è®­ç»ƒæ¨¡å‹ã€æŒ‡ä»¤å¾®è°ƒæ¨¡å‹å’Œå¤æ‚é—®é¢˜è§£å†³æ¨¡å‹ã€‚æˆ‘ä»¬é‡‡ç”¨çº§è”è’¸é¦çš„æ–¹æ³•æ¥è®­ç»ƒè¿™äº›æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§è¿­ä»£å‰ªæå’ŒæŒç»­è®­ç»ƒçš„æŠ€æœ¯ã€‚æ¯ä¸ªæ¨¡å‹éƒ½å…·å¤‡å›¾åƒç†è§£èƒ½åŠ›ï¼Œå¹¶åœ¨Apache 2.0è®¸å¯è¯ä¸‹å‘å¸ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08831",
            "title": "3AM: Segment Anything with Geometric Consistency in Videos",
            "url": "https://huggingface.co/papers/2601.08831",
            "abstract": "3AM enhances video object segmentation by integrating 3D-aware features from MUSt3R into SAM2, achieving improved viewpoint consistency with only RGB input at inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/",
            "score": 15,
            "issue_id": 578,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "5da7a87f80f776f3",
            "authors": [
                "Yang-Che Sun",
                "Cheng Sun",
                "Chin-Yang Lin",
                "Fu-En Yang",
                "Min-Hung Chen",
                "Yen-Yu Lin",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "NVIDIA Research",
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08831.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#3d",
                    "#optimization",
                    "#video",
                    "#cv"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· 3D-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²",
                    "desc": "3AM ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MUSt3R Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ SAM2. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ°Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Feature Merger, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞĞ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ, Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "3AM: Enhancing Video Segmentation with 3D Awareness",
                    "desc": "The paper presents 3AM, a novel approach to video object segmentation that enhances the existing SAM2 model by incorporating 3D-aware features from the MUSt3R dataset. This integration allows the model to maintain viewpoint consistency without the need for camera poses or depth maps, which are typically required in traditional methods. By using a lightweight Feature Merger, 3AM combines geometric and appearance features, enabling more accurate recognition of objects based on their spatial and visual characteristics. The proposed field-of-view aware sampling strategy further ensures that the model learns reliable 3D correspondences, leading to significant performance improvements on challenging datasets."
                },
                "zh": {
                    "title": "3AMï¼šæå‡è§†é¢‘ç‰©ä½“åˆ†å‰²çš„ä¸€è‡´æ€§",
                    "desc": "3AMæ˜¯ä¸€ç§è§†é¢‘ç‰©ä½“åˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡å°†MUSt3Rçš„3Dç‰¹å¾é›†æˆåˆ°SAM2ä¸­ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒè§†è§’ä¸‹çš„ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨æ¨ç†æ—¶ä»…éœ€RGBè¾“å…¥ï¼Œæ— éœ€ç›¸æœºä½å§¿æˆ–æ·±åº¦å›¾ï¼Œé™ä½äº†é¢„å¤„ç†çš„å¤æ‚æ€§ã€‚3AMä½¿ç”¨è½»é‡çº§çš„ç‰¹å¾åˆå¹¶å™¨ï¼Œèåˆå¤šå±‚MUSt3Rç‰¹å¾ï¼Œç¡®ä¿å‡ ä½•ä¸€è‡´æ€§å’Œè§†è§‰ç›¸ä¼¼æ€§ã€‚åœ¨å…·æœ‰å¤§åŸºçº¿è¿åŠ¨çš„æŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šï¼Œ3AMæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„VOSæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08670",
            "title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
            "url": "https://huggingface.co/papers/2601.08670",
            "abstract": "Parallel Context-of-Experts Decoding enables multi-document reasoning in retrieval-augmented generation by treating retrieved documents as isolated experts and synchronizing predictions through a retrieval-aware contrastive decoding rule.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated \"experts\", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.",
            "score": 15,
            "issue_id": 577,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "1620b58b79613f44",
            "authors": [
                "Giulio Corallo",
                "Paolo Papotti"
            ],
            "affiliations": [
                "EURECOM, France",
                "SAP Labs, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08670.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‘ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Parallel Context-of-Experts Decoding Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² retrieval-augmented generation. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ° Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑƒĞ·ĞºĞ¸Ñ… Ğ¼ĞµÑÑ‚ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Empowering Multi-Document Reasoning with Expert Synchronization",
                    "desc": "The paper introduces Parallel Context-of-Experts Decoding (Pced), a new method for improving multi-document reasoning in retrieval-augmented generation tasks. Pced treats each retrieved document as an independent expert and synchronizes their outputs using a contrastive decoding rule that considers the model's prior knowledge. This method allows for effective evidence aggregation during the decoding phase, avoiding the limitations of traditional attention mechanisms that can hinder performance. By implementing Pced, the authors demonstrate that it is possible to achieve cross-document reasoning without the need for a shared attention structure, thus enhancing the efficiency of the generation process."
                },
                "zh": {
                    "title": "å¹¶è¡Œä¸“å®¶è§£ç ï¼šæå‡å¤šæ–‡æ¡£æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¹¶è¡Œä¸“å®¶ä¸Šä¸‹æ–‡è§£ç ï¼ˆPcedï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„å¤šæ–‡æ¡£æ¨ç†é—®é¢˜ã€‚Pcedå°†æ£€ç´¢åˆ°çš„æ–‡æ¡£è§†ä¸ºç‹¬ç«‹çš„â€œä¸“å®¶â€ï¼Œé€šè¿‡ä¸€ç§æ–°çš„æ£€ç´¢æ„ŸçŸ¥å¯¹æ¯”è§£ç è§„åˆ™æ¥åŒæ­¥å®ƒä»¬çš„é¢„æµ‹ã€‚è¯¥æ–¹æ³•é¿å…äº†åœ¨æ–‡æ¡£ä¹‹é—´æ„å»ºå…±äº«æ³¨æ„åŠ›ï¼Œä»è€Œæ¢å¤äº†è·¨æ–‡æ¡£æ¨ç†èƒ½åŠ›ã€‚Pcedä¸éœ€è¦è®­ç»ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°èšåˆè¯æ®ï¼Œæå‡ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08665",
            "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
            "url": "https://huggingface.co/papers/2601.08665",
            "abstract": "VLingNav enhances embodied navigation through linguistic-driven cognition with adaptive reasoning and visual-assisted memory, achieving state-of-the-art performance and zero-shot transfer to real robots.  \t\t\t\t\tAI-generated summary \t\t\t\t VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.",
            "score": 6,
            "issue_id": 566,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "80a64f457ad37ab4",
            "authors": [
                "Shaoan Wang",
                "Yuanfei Luo",
                "Xingyu Chen",
                "Aocheng Luo",
                "Dongyue Li",
                "Chang Liu",
                "Sheng Chen",
                "Yangang Zhang",
                "Junzhi Yu"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Peking University",
                "Zhongguancun Academy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08665.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#transfer_learning",
                    "#multimodal",
                    "#reasoning",
                    "#dataset",
                    "#robotics",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ: ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚ Ğ´ÑƒĞ¼Ğ°ĞµÑ‚, Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ Ñ‡ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ",
                    "desc": "VLingNav â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ³Ğ´Ğ° ÑÑ‚Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ”Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ÑƒĞ»ĞµĞ²ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹."
                },
                "en": {
                    "title": "Empowering Navigation with Linguistic Reasoning and Memory",
                    "desc": "VLingNav is a novel model designed to improve embodied navigation by integrating linguistic-driven cognition with advanced reasoning and memory capabilities. It introduces an adaptive chain-of-thought mechanism that allows the agent to switch between quick, intuitive actions and more thoughtful planning as needed. Additionally, it features a visual-assisted memory module that helps the agent remember past observations, reducing repetitive actions and enhancing navigation in dynamic environments. The model has been trained on a large dataset with reasoning annotations and has shown exceptional performance in both simulated and real-world robotic navigation tasks, achieving zero-shot transfer capabilities."
                },
                "zh": {
                    "title": "è¯­è¨€é©±åŠ¨çš„æ™ºèƒ½å¯¼èˆªæ–°çªç ´",
                    "desc": "VLingNavæ˜¯ä¸€ç§åŸºäºè¯­è¨€é©±åŠ¨è®¤çŸ¥çš„ä½“æ„Ÿå¯¼èˆªæ¨¡å‹ï¼Œæ—¨åœ¨æå‡å¯¼èˆªèƒ½åŠ›ã€‚å®ƒå¼•å…¥äº†è‡ªé€‚åº”æ€ç»´é“¾æœºåˆ¶ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¿«é€Ÿç›´è§‰æ‰§è¡Œå’Œæ…¢é€Ÿæ·±æ€ç†Ÿè™‘ä¹‹é—´çµæ´»åˆ‡æ¢ã€‚ä¸ºäº†å¤„ç†é•¿æ—¶é—´çš„ç©ºé—´ä¾èµ–ï¼ŒVLingNavå¼€å‘äº†è§†è§‰è¾…åŠ©è¯­è¨€è®°å¿†æ¨¡å—ï¼Œå¸®åŠ©æ™ºèƒ½ä½“å›å¿†è¿‡å»çš„è§‚å¯Ÿï¼Œé¿å…é‡å¤æ¢ç´¢ã€‚é€šè¿‡æ„å»ºNav-AdaCoT-2.9Mæ•°æ®é›†å’Œåœ¨çº¿ä¸“å®¶æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼ŒVLingNavåœ¨å¤šç§ä½“æ„Ÿå¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶èƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹è¿ç§»åˆ°çœŸå®æœºå™¨äººä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08620",
            "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
            "url": "https://huggingface.co/papers/2601.08620",
            "abstract": "ViDoRe v3 is a multimodal retrieval-augmented generation benchmark with diverse document types and languages, revealing that visual retrieval and late interaction models improve performance while current systems still face challenges with non-textual elements and complex queries.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.",
            "score": 6,
            "issue_id": 578,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "89b240795c30bec1",
            "authors": [
                "AntÃ³nio Loison",
                "Quentin MacÃ©",
                "Antoine Edy",
                "Victor Xing",
                "Tom Balough",
                "Gabriel Moreira",
                "Bo Liu",
                "Manuel Faysse",
                "CÃ©line Hudelot",
                "Gautier Viaud"
            ],
            "affiliations": [
                "CentraleSupÃ©lec, Paris-Saclay",
                "Illuin Technology",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08620.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multilingual",
                    "#survey",
                    "#open_source",
                    "#rag",
                    "#multimodal",
                    "#low_resource"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "ViDoRe v3 â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 10 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 3000 Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° 6 ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ, ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾-Ğ¿Ñ€ĞµĞ¶Ğ½ĞµĞ¼Ñƒ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ½ĞµÑ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Multimodal Retrieval with ViDoRe v3",
                    "desc": "ViDoRe v3 is a new benchmark designed for multimodal retrieval-augmented generation (RAG) that includes various document types and languages. It highlights the importance of visual retrieval and late interaction models, which enhance performance in processing complex queries. The benchmark addresses the limitations of existing systems that often overlook non-textual elements and multi-document synthesis. With extensive human annotations and a diverse dataset, ViDoRe v3 aims to improve the capabilities of RAG systems in handling visually rich information."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ£€ç´¢çš„æœªæ¥ï¼šViDoRe v3",
                    "desc": "ViDoRe v3 æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”ŸæˆåŸºå‡†ï¼Œæ¶µç›–å¤šç§æ–‡æ¡£ç±»å‹å’Œè¯­è¨€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰æ£€ç´¢å’ŒåæœŸäº¤äº’æ¨¡å‹èƒ½å¤Ÿæé«˜æ€§èƒ½ï¼Œä½†å½“å‰ç³»ç»Ÿåœ¨å¤„ç†éæ–‡æœ¬å…ƒç´ å’Œå¤æ‚æŸ¥è¯¢æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ªä¸åŒä¸“ä¸šé¢†åŸŸçš„ 10 ä¸ªæ•°æ®é›†ï¼Œçº¦ 26,000 é¡µæ–‡æ¡£å’Œ 3,099 ä¸ªç»è¿‡äººå·¥éªŒè¯çš„æŸ¥è¯¢ï¼Œæ”¯æŒ 6 ç§è¯­è¨€ã€‚é€šè¿‡ 12,000 å°æ—¶çš„äººåŠ›æ ‡æ³¨ï¼Œæˆ‘ä»¬æä¾›äº†é«˜è´¨é‡çš„æ£€ç´¢ç›¸å…³æ€§ã€è¾¹ç•Œæ¡†å®šä½å’ŒéªŒè¯å‚è€ƒç­”æ¡ˆçš„æ³¨é‡Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08828",
            "title": "Motion Attribution for Video Generation",
            "url": "https://huggingface.co/papers/2601.08828",
            "abstract": "Motive is a gradient-based data attribution framework that identifies influential video clips for motion improvement in text-to-video models through motion-weighted loss masking.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
            "score": 5,
            "issue_id": 566,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "b846910920901a8a",
            "authors": [
                "Xindi Wu",
                "Despoina Paschalidou",
                "Jun Gao",
                "Antonio Torralba",
                "Laura Leal-TaixÃ©",
                "Olga Russakovsky",
                "Sanja Fidler",
                "Jonathan Lorraine"
            ],
            "affiliations": [
                "MIT CSAIL",
                "NVIDIA",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08828.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Motive â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ²Ğ»Ğ¸ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… text-to-video. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ÑƒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Motive Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»Ğ¸Ğ¿Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¸Ğ»Ğ¸ ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Motive, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ğ³Ğ»Ğ°Ğ´ĞºĞ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° 74.1% ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ»ÑĞ´ĞµĞ¹."
                },
                "en": {
                    "title": "Motive: Enhancing Motion in Video Generation through Data Attribution",
                    "desc": "Motive is a novel framework designed to analyze and improve motion in text-to-video generation models. It uses a gradient-based approach to identify which video clips significantly influence motion dynamics, separating motion effects from static visuals. By applying motion-weighted loss masking, Motive efficiently computes the impact of different clips on temporal consistency. This method not only enhances the quality of generated videos but also guides the selection of training data to optimize motion characteristics, achieving notable improvements in user preference tests."
                },
                "zh": {
                    "title": "Motiveï¼šæå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿åŠ¨è¡¨ç°çš„å…³é”®",
                    "desc": "Motiveæ˜¯ä¸€ä¸ªåŸºäºæ¢¯åº¦çš„æ•°æ®å½’å› æ¡†æ¶ï¼Œæ—¨åœ¨è¯†åˆ«å¯¹æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸­è¿åŠ¨æ”¹è¿›æœ‰å½±å“çš„è§†é¢‘ç‰‡æ®µã€‚è¯¥æ¡†æ¶é€šè¿‡è¿åŠ¨åŠ æƒæŸå¤±æ©ç ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»é™æ€å¤–è§‚ä¸­åˆ†ç¦»å‡ºæ—¶é—´åŠ¨æ€ã€‚Motiveå¯ä»¥å¤„ç†ç°ä»£å¤§å‹é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ï¼Œå¸®åŠ©ç ”ç©¶å“ªäº›å¾®è°ƒç‰‡æ®µèƒ½å¤Ÿæ”¹å–„æˆ–æ¶åŒ–æ—¶é—´åŠ¨æ€ã€‚é€šè¿‡Motiveé€‰æ‹©çš„é«˜å½±å“æ•°æ®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿åŠ¨å¹³æ»‘æ€§å’ŒåŠ¨æ€ç¨‹åº¦ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08587",
            "title": "End-to-End Video Character Replacement without Structural Guidance",
            "url": "https://huggingface.co/papers/2601.08587",
            "abstract": "MoCha enables controllable video character replacement using a single frame mask through condition-aware RoPE and a comprehensive data construction pipeline with specialized datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha",
            "score": 5,
            "issue_id": 566,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "07a6935bace7df25",
            "authors": [
                "Zhengbo Xu",
                "Jie Ma",
                "Ziheng Wang",
                "Zhan Peng",
                "Jun Liang",
                "Jing Li"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2601.08587.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#multimodal",
                    "#data",
                    "#dataset",
                    "#video",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ—Ğ°Ğ¼ĞµĞ½Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞµ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹",
                    "desc": "MoCha â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ñƒ Ğ¼Ğ°ÑĞºÑƒ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ€Ğ°Ğ½ĞµĞµ Ğ¼Ğ°ÑĞ¾Ğº Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ condition-aware RoPE Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²: ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² Unreal Engine 5, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ°ÑĞ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Video Character Replacement with MoCha",
                    "desc": "MoCha is a novel framework for replacing characters in videos using just a single frame mask, addressing the challenges of previous methods that needed detailed per-frame segmentation. It introduces a condition-aware RoPE to adapt to various input conditions and enhance facial identity, while also incorporating a reinforcement learning-based post-training stage for better results. The framework is supported by a comprehensive data construction pipeline that includes three specialized datasets to mitigate the lack of paired video data. Experimental results show that MoCha significantly outperforms existing methods, making it a promising solution for controllable video character replacement."
                },
                "zh": {
                    "title": "MoChaï¼šå•å¸§æ©ç å®ç°å¯æ§è§†é¢‘è§’è‰²æ›¿æ¢",
                    "desc": "MoChaæ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡å•å¸§æ©ç å®ç°å¯æ§çš„è§†é¢‘è§’è‰²æ›¿æ¢ã€‚è¯¥æ–¹æ³•å…‹æœäº†ä»¥å¾€éœ€è¦é€å¸§åˆ†å‰²æ©ç å’Œç»“æ„æŒ‡å¯¼çš„é™åˆ¶ï¼Œé€‚åº”å¤æ‚åœºæ™¯ä¸­çš„é®æŒ¡å’Œè§’è‰²äº¤äº’ã€‚æˆ‘ä»¬å¼•å…¥äº†æ¡ä»¶æ„ŸçŸ¥çš„RoPEå’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒé˜¶æ®µï¼Œä»¥å¢å¼ºé¢éƒ¨èº«ä»½çš„é€‚åº”æ€§ã€‚é€šè¿‡æ„å»ºé«˜ä¿çœŸåº¦çš„æ•°æ®é›†å’Œå…¶ä»–ä¸“é—¨æ•°æ®é›†ï¼ŒMoChaåœ¨å®éªŒä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08303",
            "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
            "url": "https://huggingface.co/papers/2601.08303",
            "abstract": "An efficient diffusion transformer framework for mobile and edge devices that maintains high-generation quality while reducing computational costs through compact architecture, elastic training, and knowledge-guided distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.",
            "score": 5,
            "issue_id": 566,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "2bf6eff80c892659",
            "authors": [
                "Dongting Hu",
                "Aarush Gupta",
                "Magzhan Gabidolla",
                "Arpit Sahni",
                "Huseyin Coskun",
                "Yanyu Li",
                "Yerlan Idelbayev",
                "Ahsan Mahmood",
                "Aleksei Lebedev",
                "Dishani Lahiri",
                "Anujraaj Goyal",
                "Ju Hu",
                "Mingming Gong",
                "Sergey Tulyakov",
                "Anil Kag"
            ],
            "affiliations": [
                "MBZUAI",
                "Snap Inc.",
                "The University of Melbourne"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08303.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#small_models",
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ° Ğ»Ğ°Ğ´Ğ¾Ğ½Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑÑƒĞ¿ĞµÑ€ÑĞµÑ‚Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Knowledge-Guided Distribution Matching Distillation Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Efficient Diffusion Transformers for Mobile and Edge Devices",
                    "desc": "This paper introduces an efficient diffusion transformer (DiT) framework designed specifically for mobile and edge devices, addressing the high computational demands of traditional models. It features a compact architecture that utilizes a global-local sparse attention mechanism to effectively balance the need for global context and local detail in image generation. The framework also includes an elastic training approach that allows the model to adapt its capacity based on the hardware it runs on, optimizing performance without sacrificing quality. Additionally, the authors implement a Knowledge-Guided Distribution Matching Distillation method to enhance the model's efficiency and fidelity, enabling real-time image generation on resource-constrained devices."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ‰©æ•£å˜æ¢å™¨ï¼šç§»åŠ¨è®¾å¤‡ä¸Šçš„é«˜è´¨é‡ç”Ÿæˆ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ‰©æ•£å˜æ¢å™¨æ¡†æ¶ï¼Œä¸“ä¸ºç§»åŠ¨å’Œè¾¹ç¼˜è®¾å¤‡è®¾è®¡ï¼Œæ—¨åœ¨åœ¨ä¸¥æ ¼çš„èµ„æºé™åˆ¶ä¸‹ä¿æŒé«˜è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç´§å‡‘çš„æ‰©æ•£å˜æ¢å™¨æ¶æ„ï¼Œç»“åˆäº†è‡ªé€‚åº”çš„å…¨å±€-å±€éƒ¨ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¹³è¡¡å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œå±€éƒ¨ç»†èŠ‚ä¿ç•™ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼¹æ€§è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ç»Ÿä¸€çš„è¶…ç½‘ç»œä¸­ä¼˜åŒ–ä¸åŒå®¹é‡çš„å­æ‰©æ•£å˜æ¢å™¨ï¼Œä»è€Œä½¿å•ä¸ªæ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸åŒç¡¬ä»¶åŠ¨æ€è°ƒæ•´ä»¥å®ç°é«˜æ•ˆæ¨ç†ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†çŸ¥è¯†å¼•å¯¼çš„åˆ†å¸ƒåŒ¹é…è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡ä¸å°‘æ­¥æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è½¬ç§»ç›¸ç»“åˆï¼Œç”Ÿæˆé«˜ä¿çœŸã€ä½å»¶è¿Ÿçš„å›¾åƒï¼Œé€‚åˆå®æ—¶è®¾å¤‡ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08468",
            "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
            "url": "https://huggingface.co/papers/2601.08468",
            "abstract": "Reinforcement learning with verifiable rewards is enhanced through a judge-then-generate paradigm that improves both efficiency and accuracy in mathematical problem-solving.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.",
            "score": 2,
            "issue_id": 566,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "3af6b50faf280f19",
            "authors": [
                "Jiangshan Duo",
                "Hanyu Li",
                "Hailin Zhang",
                "Yudong Wang",
                "Sujian Li",
                "Liang Zhao"
            ],
            "affiliations": [
                "CFCS, School of Computer Science, Peking University",
                "LLM-Core Xiaomi",
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08468.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#math",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ¡ÑƒĞ´ÑŒÑ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ°, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ JudgeRLVR â€” Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑÑƒĞ´ÑŒĞ¸), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑƒÑĞ²Ğ¾Ğ¸Ñ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚Ğ° Ğ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ ÑÑƒĞ´ÑŒĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ JudgeRLVR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ: ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° +3.7 Ğ±Ğ°Ğ»Ğ»Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 42% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼."
                },
                "en": {
                    "title": "JudgeRLVR: Efficient and Accurate Reinforcement Learning for Math Problem-Solving",
                    "desc": "This paper introduces JudgeRLVR, a novel approach in reinforcement learning that enhances the efficiency and accuracy of mathematical problem-solving. It operates in two stages: first, the model learns to evaluate solution responses for correctness, and then it generates solutions based on this learned judgment. By distinguishing valid solutions, the model reduces unnecessary exploration and focuses on more promising paths, improving both the quality of answers and the speed of generation. The results show significant improvements in accuracy and reduced verbosity compared to traditional methods, demonstrating better generalization across different problem domains."
                },
                "zh": {
                    "title": "åˆ¤åˆ«-ç”ŸæˆèŒƒå¼æå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºJudgeRLVRçš„ä¸¤é˜¶æ®µåˆ¤åˆ«-ç”ŸæˆèŒƒå¼ï¼Œä»¥æé«˜å¼ºåŒ–å­¦ä¹ ä¸­å¯éªŒè¯å¥–åŠ±çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ åˆ¤æ–­å…·æœ‰å¯éªŒè¯ç­”æ¡ˆçš„è§£å†³æ–¹æ¡ˆå“åº”ï¼›ç¬¬äºŒé˜¶æ®µï¼Œæ¨¡å‹åœ¨åˆ¤åˆ«çš„åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒJudgeRLVRåœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­å®ç°äº†æ›´å¥½çš„è´¨é‡ä¸æ•ˆç‡å¹³è¡¡ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡å¹¶å‡å°‘äº†ç”Ÿæˆé•¿åº¦ã€‚è¯¥æ–¹æ³•åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–çš„åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.07290",
            "title": "VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding",
            "url": "https://huggingface.co/papers/2601.07290",
            "abstract": "VideoLoom is a unified video large language model that achieves state-of-the-art performance in spatial-temporal video understanding through a specialized dataset and benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents VideoLoom, a unified Video Large Language Model (Video LLM) for joint spatial-temporal understanding. To facilitate the development of fine-grained spatial and temporal localization capabilities, we curate LoomData-8.7k, a human-centric video dataset with temporally grounded and spatially localized captions. With this, VideoLoom achieves state-of-the-art or highly competitive performance across a variety of spatial and temporal benchmarks (e.g., 63.1 J&F on ReVOS for referring video object segmentation, and 48.3 R1@0.7 on Charades-STA for temporal grounding). In addition, we introduce LoomBench, a novel benchmark consisting of temporal, spatial, and compositional video-question pairs, enabling a comprehensive evaluation of Video LLMs from diverse aspects. Collectively, these contributions offer a universal and effective suite for joint spatial-temporal video understanding, setting a new standard in multimodal intelligence.",
            "score": 2,
            "issue_id": 579,
            "pub_date": "2026-01-12",
            "pub_date_card": {
                "ru": "12 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 12",
                "zh": "1æœˆ12æ—¥"
            },
            "hash": "120e8b203796ca39",
            "authors": [
                "Jiapeng Shi",
                "Junke Wang",
                "Zuyao You",
                "Bo He",
                "Zuxuan Wu"
            ],
            "affiliations": [
                "Fudan University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.07290.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#open_source",
                    "#video",
                    "#benchmark",
                    "#synthetic",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸ Ğ³Ğ´Ğµ ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚",
                    "desc": "VideoLoom â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑÑ€Ğ°Ğ·Ñƒ Ğ² Ğ´Ğ²ÑƒÑ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ…: Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ (Ğ³Ğ´Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹) Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ (ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LoomData-8.7k Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LoomBench Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ LLM Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "VideoLoom: Redefining Video Understanding with Unified Language Models",
                    "desc": "VideoLoom is a cutting-edge Video Large Language Model (LLM) designed for advanced understanding of video content in both space and time. It utilizes a specially curated dataset called LoomData-8.7k, which includes human-centric videos with detailed captions that are both temporally and spatially grounded. The model demonstrates exceptional performance on various benchmarks, achieving top scores in tasks like referring video object segmentation and temporal grounding. Additionally, the introduction of LoomBench provides a new way to evaluate Video LLMs through diverse video-question pairs, enhancing the assessment of multimodal intelligence capabilities."
                },
                "zh": {
                    "title": "è§†é¢‘ç†è§£çš„æ–°æ ‡å‡†ï¼šVideoLoom",
                    "desc": "VideoLoomæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºæ—¶ç©ºè§†é¢‘ç†è§£ã€‚ä¸ºäº†æå‡ç»†ç²’åº¦çš„æ—¶ç©ºå®šä½èƒ½åŠ›ï¼Œç ”ç©¶è€…ä»¬åˆ›å»ºäº†LoomData-8.7kï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å«æ—¶é—´å’Œç©ºé—´å®šä½çš„å­—å¹•ã€‚VideoLoomåœ¨å¤šä¸ªæ—¶ç©ºåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥LoomBenchåŸºå‡†ï¼Œç ”ç©¶è€…èƒ½å¤Ÿä»ä¸åŒæ–¹é¢å…¨é¢è¯„ä¼°è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.06786",
            "title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs",
            "url": "https://huggingface.co/papers/2601.06786",
            "abstract": "Training large language models for improved reasoning while maintaining calibration through epistemic learning reduces inference compute requirements and enhances performance on mathematical and coding tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K=30 performance of STaR with only K=10 samples in capable models.",
            "score": 1,
            "issue_id": 575,
            "pub_date": "2026-01-11",
            "pub_date_card": {
                "ru": "11 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 11",
                "zh": "1æœˆ11æ—¥"
            },
            "hash": "c93dbc2828071573",
            "authors": [
                "Jewon Yeom",
                "Jaewon Sok",
                "Seonghyeon Park",
                "Jeongjae Park",
                "Taesup Kim"
            ],
            "affiliations": [
                "Department of Aerospace Engineering, Seoul National University",
                "Department of Rural Systems Engineering, Seoul National University",
                "Graduate School of Data Science, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.06786.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#alignment",
                    "#plp",
                    "#math",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ñ‡Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ, Ğ½Ğ¾ Ğ¸ Ğ·Ğ½Ğ°Ñ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸Ğ¼ Ğ¼Ñ‹ÑĞ»ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ EpiCaR Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€” ĞµÑ‘ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ, Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° ĞµÑ‘ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ fine-tuning Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. Ğ­Ñ‚Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ² Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing Reasoning and Calibration in Language Models with EpiCaR",
                    "desc": "This paper introduces a new method for training large language models (LLMs) to improve their reasoning abilities while ensuring they remain well-calibrated in their predictions. The authors identify a common problem where models become overconfident and fail to represent uncertainty, leading to poor performance in reasoning tasks. They propose a training objective called epistemically-calibrated reasoning (EpiCaR), which focuses on teaching models not only how to reason but also when to trust their reasoning. Their experiments show that this approach significantly enhances both accuracy and calibration, while also reducing the computational resources needed for inference."
                },
                "zh": {
                    "title": "æå‡æ¨ç†èƒ½åŠ›ä¸æ ¡å‡†æ€§çš„åŒé‡ä¼˜åŒ–",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ ¡å‡†æ€§ã€‚æˆ‘ä»¬å°†æ¨ç†è®­ç»ƒé‡æ–°å®šä¹‰ä¸ºä¸€ç§è®¤è¯†å­¦ä¹ é—®é¢˜ï¼Œä½¿æ¨¡å‹ä¸ä»…å­¦ä¹ å¦‚ä½•æ¨ç†ï¼Œè¿˜è¦å­¦ä¼šä½•æ—¶ä¿¡ä»»å…¶æ¨ç†ç»“æœã€‚æˆ‘ä»¬æå‡ºçš„EpiCaRç›®æ ‡å¯ä»¥åŒæ—¶ä¼˜åŒ–æ¨ç†æ€§èƒ½å’Œæ ¡å‡†æ€§ï¼Œå¹¶åœ¨è¿­ä»£ç›‘ç£å¾®è°ƒæ¡†æ¶ä¸­å®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ ¡å‡†æ€§ä¸Šä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰è¶³å¤Ÿæ¨ç†èƒ½åŠ›çš„æ¨¡å‹ä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04582",
            "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",
            "url": "https://huggingface.co/papers/2601.04582",
            "abstract": "A reinforcement learning framework for text-to-visualization generation that improves chart quality and code execution by optimizing multiple objectives using post-execution feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.",
            "score": 1,
            "issue_id": 566,
            "pub_date": "2026-01-08",
            "pub_date_card": {
                "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 8",
                "zh": "1æœˆ8æ—¥"
            },
            "hash": "596be1c607cd146a",
            "authors": [
                "Mizanur Rahman",
                "Mohammed Saidul Islam",
                "Md Tahmid Rahman Laskar",
                "Shafiq Joty",
                "Enamul Hoque"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Salesforce AI Research",
                "York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04582.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RL-Text2Vis â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ°Ğ´ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Group Relative Policy Optimization (GRPO) Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ´Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ 22% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GPT-4o Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ñ 78% Ğ´Ğ¾ 97%. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ GRPO Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Reinforcement Learning for Enhanced Text-to-Visualization Generation",
                    "desc": "This paper introduces RL-Text2Vis, a novel reinforcement learning framework designed to enhance the generation of visualizations from text queries. It addresses the limitations of existing models by optimizing multiple objectives, including textual accuracy, code validity, and visualization quality, using feedback obtained after code execution. The framework employs Group Relative Policy Optimization (GRPO) to improve the quality of charts generated, achieving a significant increase in both chart quality and code execution success rates. The results demonstrate that RL-Text2Vis outperforms traditional methods and shows strong generalization capabilities across different datasets."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ æå‡æ–‡æœ¬åˆ°å¯è§†åŒ–ç”Ÿæˆçš„è´¨é‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ–‡æœ¬åˆ°å¯è§†åŒ–ç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºRL-Text2Visã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–å¤šä¸ªç›®æ ‡ï¼Œåˆ©ç”¨æ‰§è¡Œåçš„åé¦ˆæ¥æé«˜å›¾è¡¨è´¨é‡å’Œä»£ç æ‰§è¡Œæ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ä¸åŒï¼ŒRL-Text2Visé‡‡ç”¨äº†ä¸€ç§æ–°çš„å¤šç›®æ ‡å¥–åŠ±æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶ä¼˜åŒ–æ–‡æœ¬å‡†ç¡®æ€§ã€ä»£ç æœ‰æ•ˆæ€§å’Œå¯è§†åŒ–è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRL-Text2Visåœ¨å›¾è¡¨è´¨é‡ä¸Šç›¸è¾ƒäºGPT-4oæœ‰22%çš„ç›¸å¯¹æå‡ï¼Œä»£ç æ‰§è¡ŒæˆåŠŸç‡ä»78%æé«˜åˆ°97%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02669",
            "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
            "url": "https://huggingface.co/papers/2601.02669",
            "abstract": "FactArena is an automated evaluation framework that comprehensively assesses large language models across all stages of the fact-checking pipeline, revealing gaps between claim verification accuracy and overall fact-checking capability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.",
            "score": 0,
            "issue_id": 571,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "9a7c27380caaf64c",
            "authors": [
                "Hongzhan Lin",
                "Zixin Chen",
                "Zhiqi Shen",
                "Ziyang Luo",
                "Zhen Ye",
                "Jing Ma",
                "Tat-Seng Chua",
                "Guandong Xu"
            ],
            "affiliations": [
                "Department of Computer Science, Hong Kong Baptist University",
                "National University of Singapore",
                "Salesforce AI Research",
                "The Education University of Hong Kong",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02669.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğº Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ñ‹",
                    "desc": "FactArena Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ„Ğ°ĞºÑ‚-Ñ‡ĞµĞºĞ¸Ğ½Ğ³Ğ° Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ°Ñ€ĞµĞ½Ñ‹-ÑÑ‚Ğ¸Ğ»Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° 16 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ„Ğ°ĞºÑ‚-Ñ‡ĞµĞºĞ¸Ğ½Ğ³Ğ°."
                },
                "en": {
                    "title": "FactArena: Holistic Evaluation for Reliable Fact-Checking in LLMs",
                    "desc": "FactArena is a new evaluation framework designed to assess large language models (LLMs) throughout the entire fact-checking process, not just their ability to verify claims. It identifies weaknesses in LLMs by examining their performance in claim extraction, evidence retrieval, and justification of verdicts. The framework includes a standardized process for fact-checking, a fair judgment system for comparing models, and a module that generates challenging claims to test LLMs' robustness. By revealing gaps between claim verification accuracy and overall fact-checking ability, FactArena aims to improve the reliability of LLMs in critical applications."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°è¯­è¨€æ¨¡å‹çš„äº‹å®æ ¸æŸ¥èƒ½åŠ›",
                    "desc": "FactArenaæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œå…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äº‹å®æ ¸æŸ¥æµç¨‹ä¸­çš„å„ä¸ªé˜¶æ®µï¼Œæ­ç¤ºäº†å£°æ˜éªŒè¯å‡†ç¡®æ€§ä¸æ•´ä½“äº‹å®æ ¸æŸ¥èƒ½åŠ›ä¹‹é—´çš„å·®è·ã€‚ç°æœ‰è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨å£°æ˜éªŒè¯ï¼Œå¿½è§†äº†æ›´å¹¿æ³›çš„äº‹å®æ ¸æŸ¥å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬å£°æ˜æå–å’Œè¯æ®æ£€ç´¢ã€‚FactArenaé€šè¿‡æ ‡å‡†åŒ–çš„å£°æ˜åˆ†è§£ã€å·¥å…·å¢å¼ºçš„è¯æ®æ£€ç´¢å’ŒåŸºäºç†ç”±çš„åˆ¤å†³é¢„æµ‹ï¼Œæä¾›äº†å…¨é¢çš„é˜¶æ®µæ€§åŸºå‡†æµ‹è¯•ã€‚è¯¥æ¡†æ¶ä¸ºè¯Šæ–­å¤§å‹è¯­è¨€æ¨¡å‹çš„äº‹å®æ¨ç†æä¾›äº†å¯æ‰©å±•å’Œå¯é çš„èŒƒå¼ï¼Œæ¨åŠ¨äº†åœ¨å®‰å…¨å…³é”®çš„äº‹å®æ ¸æŸ¥åº”ç”¨ä¸­å¯é éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›å±•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-13.html",
    "link_next": "2026-01-15.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "13.01",
        "en": "01/13",
        "zh": "1æœˆ13æ—¥"
    },
    "short_date_next": {
        "ru": "15.01",
        "en": "01/15",
        "zh": "1æœˆ15æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 7,
        "#agents": 5,
        "#cv": 1,
        "#rl": 6,
        "#rlhf": 2,
        "#rag": 1,
        "#plp": 1,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 8,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 3,
        "#science": 0,
        "#low_resource": 2
    }
}