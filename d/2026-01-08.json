{
    "date": {
        "ru": "8 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 8",
        "zh": "1æœˆ8æ—¥"
    },
    "time_utc": "2026-01-08 19:21",
    "weekday": 3,
    "issue_id": 486,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.02151",
            "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting",
            "url": "https://huggingface.co/papers/2601.02151",
            "abstract": "Entropy-Adaptive Fine-Tuning addresses catastrophic forgetting in supervised fine-tuning by using token-level entropy to distinguish uncertainty from knowledge conflict, enabling better preservation of general capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.",
            "score": 63,
            "issue_id": 471,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "9d40d1471e675eaa",
            "authors": [
                "Muxi Diao",
                "Lele Yang",
                "Wuxuan Gong",
                "Yutong Zhang",
                "Zhonghao Yan",
                "Yufei Han",
                "Kongming Liang",
                "Weiran Xu",
                "Zhanyu Ma"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Zhongguancun Academy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02151.jpg",
            "data": {
                "categories": [],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ - ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ½Ğ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ° Ğ² ÑĞ²Ğ¾Ñ‘Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸, Ğ½Ğ¾ Ğ²Ñ‹Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Entropy-Adaptive Fine-Tuning (EAFT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ EAFT ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Preserving Knowledge with Entropy-Aware Fine-Tuning",
                    "desc": "Entropy-Adaptive Fine-Tuning (EAFT) is a method designed to combat catastrophic forgetting during supervised fine-tuning of machine learning models. It uses token-level entropy to differentiate between uncertainty in predictions and conflicts with external labels, allowing the model to retain its general capabilities. By focusing on uncertain samples and minimizing updates on conflicting data, EAFT helps maintain performance across various tasks. Experiments show that EAFT achieves comparable results to traditional fine-tuning while preserving the model's overall knowledge."
                },
                "zh": {
                    "title": "ç†µè‡ªé€‚åº”å¾®è°ƒï¼šè§£å†³ç¾éš¾æ€§é—å¿˜çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç†µè‡ªé€‚åº”å¾®è°ƒï¼ˆEAFTï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç›‘ç£å¾®è°ƒä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚EAFTé€šè¿‡ä½¿ç”¨ä»¤ç‰Œçº§ç†µæ¥åŒºåˆ†ä¸ç¡®å®šæ€§å’ŒçŸ¥è¯†å†²çªï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ä¸åŒï¼ŒEAFTèƒ½å¤Ÿåœ¨ä¸ç¡®å®šæ ·æœ¬ä¸­å­¦ä¹ ï¼ŒåŒæ—¶æŠ‘åˆ¶å¯¹å†²çªæ•°æ®çš„æ¢¯åº¦æ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEAFTåœ¨å¤šä¸ªé¢†åŸŸçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå‡è½»é€šç”¨èƒ½åŠ›çš„é€€åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03509",
            "title": "Evolving Programmatic Skill Networks",
            "url": "https://huggingface.co/papers/2601.03509",
            "abstract": "Programmatic Skill Network enables continual skill acquisition through executable symbolic programs that evolve via reflection, progressive optimization, and structural refactoring mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.",
            "score": 51,
            "issue_id": 481,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "5bf0162d41201122",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Programmatic Skill Network (PSN) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ³Ğ´Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ ĞºĞ°Ğº Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ²: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚ĞºĞ°Ñ‚Ğ¾Ğ². PSN Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ, ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½ÑƒÑ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… MineDojo Ğ¸ Crafter Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Evolving Skills for Endless Learning",
                    "desc": "The Programmatic Skill Network (PSN) is a framework designed for continual skill acquisition in dynamic environments. It allows agents to create and refine a library of executable skills through three main mechanisms: REFLECT for identifying issues in skill combinations, progressive optimization to balance skill reliability and adaptability, and structural refactoring to keep the skill network efficient. PSN utilizes large language models to enhance these processes, enabling agents to learn from their experiences effectively. Experiments show that PSN can adapt quickly and generalize well across various tasks, demonstrating its potential in open-ended learning scenarios."
                },
                "zh": {
                    "title": "ç¨‹åºæŠ€èƒ½ç½‘ç»œï¼šæŒç»­æŠ€èƒ½è·å–çš„æ–°æ–¹æ³•",
                    "desc": "ç¨‹åºæŠ€èƒ½ç½‘ç»œï¼ˆPSNï¼‰æ˜¯ä¸€ç§æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯æ‰§è¡Œçš„ç¬¦å·ç¨‹åºå®ç°æŒç»­çš„æŠ€èƒ½è·å–ã€‚å®ƒé€šè¿‡åæ€ã€æ¸è¿›ä¼˜åŒ–å’Œç»“æ„é‡æ„æœºåˆ¶ï¼Œä½¿æŠ€èƒ½åº“ä¸æ–­æ‰©å±•å’Œæ¼”åŒ–ã€‚PSNåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å®ç°ä¸‰ç§æ ¸å¿ƒæœºåˆ¶ï¼šç»“æ„åŒ–æ•…éšœå®šä½ã€æˆç†Ÿåº¦æ„ŸçŸ¥çš„ä¼˜åŒ–æ›´æ–°å’Œåœ¨å›æ»šéªŒè¯ä¸‹çš„ç»“æ„é‡æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPSNåœ¨å¼€æ”¾å¼ä»»åŠ¡åˆ†å¸ƒä¸­å±•ç°å‡ºå¼ºå¤§çš„æŠ€èƒ½é‡ç”¨ã€å¿«é€Ÿé€‚åº”å’Œè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03872",
            "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
            "url": "https://huggingface.co/papers/2601.03872",
            "abstract": "ATLAS is a dual-path framework that dynamically selects optimal model-tool combinations for cross-domain reasoning through cluster-based routing and reinforcement learning-based multi-step routing, achieving superior performance on complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.",
            "score": 30,
            "issue_id": 474,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "1f593458df41b2b9",
            "authors": [
                "Jinyang Wu",
                "Guocheng Zhai",
                "Ruihan Jin",
                "Jiahao Yuan",
                "Yuhao Shen",
                "Shuai Zhang",
                "Zhengqi Wen",
                "Jianhua Tao"
            ],
            "affiliations": [
                "East China Normal University",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03872.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#agents",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "ATLAS â€” ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…Ğ¿ÑƒÑ‚Ñ‘Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4o, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 10,1% Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 13,1% Ğ´Ğ»Ñ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Dynamic Model-Tool Selection for Superior Reasoning Performance",
                    "desc": "ATLAS is a novel framework designed to enhance cross-domain reasoning by dynamically selecting the best combinations of models and tools. It utilizes a dual-path approach that includes cluster-based routing for domain-specific alignment and reinforcement learning for exploring optimal tool usage. This method addresses the challenges of high-dimensional optimization in selecting model-tool pairs, leading to improved performance on complex reasoning tasks. Experiments show that ATLAS significantly outperforms existing models and routing methods, particularly in visual reasoning tasks."
                },
                "zh": {
                    "title": "ATLASï¼šåŠ¨æ€é€‰æ‹©æœ€ä½³æ¨¡å‹å·¥å…·ç»„åˆçš„åŒè·¯å¾„æ¡†æ¶",
                    "desc": "ATLASæ˜¯ä¸€ç§åŒè·¯å¾„æ¡†æ¶ï¼Œèƒ½å¤ŸåŠ¨æ€é€‰æ‹©æœ€ä½³çš„æ¨¡å‹å’Œå·¥å…·ç»„åˆï¼Œä»¥åº”å¯¹è·¨é¢†åŸŸæ¨ç†çš„å¤æ‚ä»»åŠ¡ã€‚å®ƒé€šè¿‡åŸºäºèšç±»çš„è·¯ç”±å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ­¥è·¯ç”±æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œä»è€Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„è¡¨ç°ã€‚ATLASçš„åŒè·¯å¾„æ–¹æ³•åŒ…æ‹¬æ— è®­ç»ƒçš„åŸºäºèšç±»çš„è·¯ç”±å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ­¥è·¯ç”±ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨é¢†åŸŸç‰¹å®šçš„ç»éªŒå…ˆéªŒå’Œè‡ªä¸»æ¢ç´¢çš„è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒATLASåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„é—­æºæ¨¡å‹å’Œè·¯ç”±æ–¹æ³•ï¼Œå°¤å…¶åœ¨è§†è§‰æ¨ç†æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03986",
            "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
            "url": "https://huggingface.co/papers/2601.03986",
            "abstract": "Researchers developed Benchmark^2, a framework with three metrics to evaluate benchmark quality for large language models, revealing significant variations in existing benchmarks and enabling more efficient evaluation through selective benchmark construction.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.",
            "score": 28,
            "issue_id": 471,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "daa162d270fd4a3e",
            "authors": [
                "Qi Qian",
                "Chengsong Huang",
                "Jingwen Xu",
                "Changze Lv",
                "Muling Wu",
                "Wenhao Liu",
                "Xiaohua Wang",
                "Zhenghua Wang",
                "Zisu Huang",
                "Muzhao Tian",
                "Jianhan Xu",
                "Kun Hu",
                "He-Da Wang",
                "Yao Hu",
                "Xuanjing Huang",
                "Xiaoqing Zheng"
            ],
            "affiliations": [
                "College of Computer Science and Artificial Intelligence, Fudan University",
                "Washington University in St. Louis",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03986.jpg",
            "data": {
                "categories": [
                    "#benchmark"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹: Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ BenchmarkÂ², Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸: ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹, ĞºĞ¾Ğ³Ğ´Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ°Ğ±Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚, Ñ‡ĞµĞ¼ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° 15 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ 11 LLM-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Benchmark^2: Elevating Benchmark Quality for Language Models",
                    "desc": "The paper introduces Benchmark^2, a new framework designed to evaluate the quality of benchmarks used for large language models (LLMs). It includes three key metrics: Cross-Benchmark Ranking Consistency, which checks if a benchmark's rankings match those of other benchmarks; Discriminability Score, which measures how well a benchmark can distinguish between different models; and Capability Alignment Deviation, which highlights cases where stronger models underperform compared to weaker ones. Through experiments on 15 different benchmarks and 11 LLMs, the researchers found significant differences in benchmark quality and showed that using their metrics allows for more efficient benchmark selection while maintaining evaluation effectiveness."
                },
                "zh": {
                    "title": "Benchmark^2ï¼šæå‡åŸºå‡†è¯„ä¼°è´¨é‡çš„æ¡†æ¶",
                    "desc": "ç ”ç©¶äººå‘˜å¼€å‘äº†Benchmark^2ï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†è´¨é‡çš„æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæŒ‡æ ‡ã€‚ç¬¬ä¸€ä¸ªæŒ‡æ ‡æ˜¯è·¨åŸºå‡†æ’åä¸€è‡´æ€§ï¼Œç”¨äºæµ‹é‡åŸºå‡†æ˜¯å¦èƒ½ä¸åŒè¡ŒåŸºå‡†äº§ç”Ÿä¸€è‡´çš„æ¨¡å‹æ’åã€‚ç¬¬äºŒä¸ªæŒ‡æ ‡æ˜¯å¯åŒºåˆ†æ€§å¾—åˆ†ï¼Œé‡åŒ–åŸºå‡†åŒºåˆ†æ¨¡å‹çš„èƒ½åŠ›ã€‚ç¬¬ä¸‰ä¸ªæŒ‡æ ‡æ˜¯èƒ½åŠ›å¯¹é½åå·®ï¼Œè¯†åˆ«åœ¨åŒä¸€æ¨¡å‹å®¶æ—ä¸­å¼ºæ¨¡å‹å¤±è´¥è€Œå¼±æ¨¡å‹æˆåŠŸçš„æƒ…å†µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04151",
            "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
            "url": "https://huggingface.co/papers/2601.04151",
            "abstract": "Klear addresses audio-video joint generation challenges through a unified model architecture, progressive multitask training, and large-scale dense-caption data construction, achieving superior alignment and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.",
            "score": 9,
            "issue_id": 470,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "a6b0d69f2c3ead36",
            "authors": [
                "Jun Wang",
                "Chunyu Qiang",
                "Yuxin Guo",
                "Yiran Wang",
                "Xijuan Zeng",
                "Chen Zhang",
                "Pengfei Wan"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04151.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#dataset",
                    "#audio",
                    "#video",
                    "#multimodal",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ°: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Klear â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°ÑÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ³ÑƒĞ±. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Omni-Full Attention Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµÑĞ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½ÑƒÑ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼ curriculum learning, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ ÑƒĞ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ¾Ğ¹Ğ¾Ğº (Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚ĞµĞºÑÑ‚)."
                },
                "en": {
                    "title": "Klear: Unifying Audio-Video Generation for Superior Alignment and Generalization",
                    "desc": "Klear is a new model designed to improve the generation of audio and video together. It uses a single architecture with advanced attention mechanisms to ensure that audio and video are closely aligned. The training process involves multitasking and progressive learning, which helps the model understand and generate better representations of audio-visual content. Additionally, Klear introduces a large dataset with detailed captions, allowing it to perform exceptionally well in generating high-quality audio-video outputs, even in challenging scenarios."
                },
                "zh": {
                    "title": "Klearï¼šéŸ³è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "Klear æ˜¯ä¸€ç§æ–°å‹çš„éŸ³é¢‘è§†é¢‘è”åˆç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³éŸ³è§†é¢‘ä¸åŒæ­¥å’Œå£å‹ä¸è¯­éŸ³ä¸åŒ¹é…ç­‰é—®é¢˜ã€‚å®ƒé‡‡ç”¨ç»Ÿä¸€çš„æ¨¡å‹æ¶æ„å’Œæ¸è¿›å¼å¤šä»»åŠ¡è®­ç»ƒç­–ç•¥ï¼Œèƒ½å¤Ÿå®ç°æ›´å¥½çš„éŸ³è§†é¢‘å¯¹é½å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡çš„å¯†é›†æ ‡æ³¨æ•°æ®é›†ï¼ŒKlear æä¾›äº†é«˜è´¨é‡çš„éŸ³è§†é¢‘-å­—å¹•ä¸‰å…ƒç»„ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKlear åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04194",
            "title": "Choreographing a World of Dynamic Objects",
            "url": "https://huggingface.co/papers/2601.04194",
            "abstract": "CHORD is a universal generative framework that extracts Lagrangian motion information from Eulerian video representations to synthesize diverse 4D dynamic scenes without requiring category-specific rules or large datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord",
            "score": 7,
            "issue_id": 470,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "f62cc6c228aa9b1c",
            "authors": [
                "Yanzhe Lyu",
                "Chen Geng",
                "Karthik Dharmarajan",
                "Yunzhi Zhang",
                "Hadi Alzayer",
                "Shangzhe Wu",
                "Jiajun Wu"
            ],
            "affiliations": [
                "Stanford University",
                "University of Cambridge",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04194.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#synthetic",
                    "#3d",
                    "#multimodal",
                    "#robotics"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 4D Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸Ğ· 2D Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»",
                    "desc": "CHORD â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· 2D Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ­Ğ¹Ğ»ĞµÑ€Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ĞµÑ‘ Ğ² Ğ›Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶ĞµĞ²Ñ‹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 4D ÑÑ†ĞµĞ½. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ½Ğµ Ğ½ÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ĞµĞ½ Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµĞ½, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° 4D ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ½Ğ¾ Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "CHORD: Universal Synthesis of Dynamic 4D Scenes",
                    "desc": "The paper introduces CHORD, a generative framework that synthesizes dynamic 4D scenes by extracting motion information from 2D video representations. Unlike traditional methods that rely on specific rules for different object categories, CHORD operates without needing large datasets or category-specific heuristics. It utilizes a distillation-based approach to capture Lagrangian motion, making it versatile and applicable to various scenarios. The effectiveness of CHORD is demonstrated through experiments that showcase its ability to generate diverse multi-body dynamics and its potential in robotics applications."
                },
                "zh": {
                    "title": "CHORDï¼šæ— ç±»åˆ«é™åˆ¶çš„å››ç»´åŠ¨æ€åœºæ™¯ç”Ÿæˆ",
                    "desc": "CHORDæ˜¯ä¸€ç§é€šç”¨çš„ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿä»æ¬§æ‹‰è§†é¢‘è¡¨ç¤ºä¸­æå–æ‹‰æ ¼æœ—æ—¥è¿åŠ¨ä¿¡æ¯ï¼Œä»¥åˆæˆå¤šæ ·çš„å››ç»´åŠ¨æ€åœºæ™¯ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦ç‰¹å®šç±»åˆ«çš„è§„åˆ™æˆ–å¤§å‹æ•°æ®é›†ï¼Œå…·æœ‰æ™®éæ€§å’Œçµæ´»æ€§ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„å›¾å½¢ç®¡é“ç›¸æ¯”ï¼ŒCHORDèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ç”ŸæˆåŠ¨æ€åœºæ™¯ï¼Œé¿å…äº†ç¹ççš„äººå·¥è®¾è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCHORDåœ¨ç”Ÿæˆå¤šä½“å››ç»´åŠ¨æ€æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå¹¶ä¸”å¯ä»¥åº”ç”¨äºæœºå™¨äººæ“ä½œç­–ç•¥çš„ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04171",
            "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
            "url": "https://huggingface.co/papers/2601.04171",
            "abstract": "Agentic Rubrics enable efficient and scalable verification for software engineering agents by creating context-aware checklists that outperform traditional methods while maintaining interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.",
            "score": 6,
            "issue_id": 470,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "31d275e8d765ca8b",
            "authors": [
                "Mohit Raghavendra",
                "Anisha Gunjal",
                "Bing Liu",
                "Yunzhong He"
            ],
            "affiliations": [
                "Scale AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04171.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#interpretability",
                    "#plp",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "âœ…",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ñ‡ĞµĞºĞ»Ğ¸ÑÑ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ´Ğ°: Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Agentic Rubrics Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¹ Ñ‡ĞµĞºĞ»Ğ¸ÑÑ‚ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğº ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‡Ğ°Ğ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ +3.5 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-Bench. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·Ğ° ÑÑ‡ĞµÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Agentic Rubrics: Context-Aware Checklists for Efficient Software Verification",
                    "desc": "Agentic Rubrics are a new method for verifying software engineering agents by using context-aware checklists. This approach improves upon traditional verification methods by allowing agents to score candidate patches without needing to execute code, which can be time-consuming. The study shows that Agentic Rubrics outperform existing methods, achieving higher scores on benchmark tests while maintaining interpretability. Additionally, the rubrics help identify issues that standard tests might miss, making them a valuable tool for enhancing the performance of software engineering agents."
                },
                "zh": {
                    "title": "Agentic Rubricsï¼šé«˜æ•ˆå¯æ‰©å±•çš„è½¯ä»¶éªŒè¯æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAgentic Rubricsçš„æ–¹æ³•ï¼Œç”¨äºæé«˜è½¯ä»¶å·¥ç¨‹ä»£ç†çš„éªŒè¯æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ›å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ£€æŸ¥æ¸…å•ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–ä»£ç æ‰§è¡Œçš„æƒ…å†µä¸‹å¯¹å€™é€‰è¡¥ä¸è¿›è¡Œè¯„åˆ†ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒAgentic Rubricsåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¾—åˆ†æ˜¾è‘—é«˜äºç°æœ‰çš„æœ€å¼ºåŸºçº¿ã€‚é€šè¿‡åˆ†æï¼Œå‘ç°è¿™ç§æ–¹æ³•ä¸ä»…ä¸çœŸå®æµ‹è¯•ç»“æœä¸€è‡´ï¼Œè¿˜èƒ½è¯†åˆ«æµ‹è¯•æœªèƒ½æ•æ‰åˆ°çš„é—®é¢˜ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02075",
            "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
            "url": "https://huggingface.co/papers/2601.02075",
            "abstract": "MDAgent2 enables automated molecular dynamics code generation and question answering through domain-adapted language models and a multi-agent runtime system.  \t\t\t\t\tAI-generated summary \t\t\t\t Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2",
            "score": 6,
            "issue_id": 470,
            "pub_date": "2026-01-05",
            "pub_date_card": {
                "ru": "5 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 5",
                "zh": "1æœˆ5æ—¥"
            },
            "hash": "aa0562dc0d93c752",
            "authors": [
                "Zhuofan Shi",
                "Hubao A",
                "Yufei Shao",
                "Dongliang Huang",
                "Hongxu An",
                "Chunxiao Xin",
                "Haiyang Shen",
                "Zhenyu Wang",
                "Yunshan Na",
                "Gang Huang",
                "Xiang Jing"
            ],
            "affiliations": [
                "Liaoning Technical University",
                "National Key Laboratory of Data Space Technology and System",
                "Peking University",
                "The Hong Kong University of Science and Technology",
                "Wenjing Future Lab (Beijing) Technology Co., Ltd."
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.02075.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#agents",
                    "#dataset",
                    "#plp",
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#open_source",
                    "#science",
                    "#small_models"
                ],
                "emoji": "âš›ï¸",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹Ğº Ğ´Ğ»Ñ Ğ½Ğ°ÑƒĞºĞ¸: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸",
                    "desc": "MDAgent2 â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğº Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MD-Instruct Ğ¸ MD-Code, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MD-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹ ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. MDAgent2-RUNTIME Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° LAMMPS, Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MD-EvalBench."
                },
                "en": {
                    "title": "Automating Molecular Dynamics with MDAgent2: Code Generation and Q&A Simplified!",
                    "desc": "MDAgent2 is an advanced framework designed to automate the generation of molecular dynamics (MD) code and facilitate question answering in the MD domain. It addresses the challenges of writing LAMMPS scripts by utilizing domain-adapted language models and a multi-agent runtime system. The framework employs a three-stage training approach, including continued pre-training, supervised fine-tuning, and reinforcement learning, to enhance model performance. Additionally, MDAgent2 introduces a closed-loop reinforcement learning method that improves code generation through feedback from simulation outcomes, making it a significant step forward in AI applications for scientific simulations."
                },
                "zh": {
                    "title": "MDAgent2ï¼šè‡ªåŠ¨åŒ–åˆ†å­åŠ¨åŠ›å­¦çš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "MDAgent2 æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„åˆ†å­åŠ¨åŠ›å­¦ä»£ç ç”Ÿæˆå’Œé—®ç­”ç³»ç»Ÿï¼Œåˆ©ç”¨é¢†åŸŸé€‚åº”çš„è¯­è¨€æ¨¡å‹å’Œå¤šæ™ºèƒ½ä½“è¿è¡Œæ—¶ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿè§£å†³äº†ç¼–å†™ LAMMPS è„šæœ¬çš„ä¸“ä¸šæ€§å’Œè€—æ—¶é—®é¢˜ï¼Œé€šè¿‡æ„å»ºé«˜è´¨é‡çš„æ•°æ®é›†æ¥æ”¯æŒçŸ¥è¯†é—®ç­”å’Œä»£ç ç”Ÿæˆã€‚MDAgent2 é‡‡ç”¨äº†ä¸‰é˜¶æ®µçš„åè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ç»§ç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œè®­ç»ƒå‡ºé€‚åº”åˆ†å­åŠ¨åŠ›å­¦é¢†åŸŸçš„æ¨¡å‹ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å·¥ä¸šæ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºç§‘å­¦å’Œå·¥ä¸šè§„æ¨¡æ¨¡æ‹Ÿä¸­çš„è‡ªåŠ¨ä»£ç ç”Ÿæˆå¥ å®šäº†æ–¹æ³•è®ºåŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.00423",
            "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models",
            "url": "https://huggingface.co/papers/2601.00423",
            "abstract": "Entropy-aware policy optimization method for reinforcement learning in flow matching models that improves exploration through SDE and ODE sampling strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.",
            "score": 6,
            "issue_id": 470,
            "pub_date": "2026-01-01",
            "pub_date_card": {
                "ru": "1 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 1",
                "zh": "1æœˆ1æ—¥"
            },
            "hash": "d7930acb619d200c",
            "authors": [
                "Shengjun Zhang",
                "Zhang Zhang",
                "Chensheng Dai",
                "Yueqi Duan"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.00423.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#alignment",
                    "#optimization"
                ],
                "emoji": "ğŸ²",
                "ru": {
                    "title": "Ğ­Ğ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ E-GRPO â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ, Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… flow matching. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑˆĞ°Ğ³Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑˆĞ°Ğ³Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ´Ğ°ÑÑ‚ Ğ½ĞµÑ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ² Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ´Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ (ODE). Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿ Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°."
                },
                "en": {
                    "title": "Enhancing Exploration with Entropy in Reinforcement Learning",
                    "desc": "This paper presents E-GRPO, an entropy-aware policy optimization method designed to enhance exploration in reinforcement learning for flow matching models. The authors identify that high entropy steps lead to better exploration, while low entropy steps can produce unclear outcomes. To address the challenges of sparse rewards in stochastic differential equation (SDE) sampling, they propose merging consecutive low entropy steps into a single high entropy step. Additionally, they introduce a multi-step group normalized advantage to improve the computation of advantages in samples that share the same SDE denoising step, demonstrating improved performance in various reward settings."
                },
                "zh": {
                    "title": "ç†µæ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼Œæå‡æ¢ç´¢æ•ˆç‡ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œç§°ä¸ºE-GRPOï¼Œæ—¨åœ¨æé«˜æµåŒ¹é…æ¨¡å‹ä¸­çš„æ¢ç´¢æ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å…³æ³¨ç†µçš„å˜åŒ–ï¼Œä¼˜åŒ–éšæœºå¾®åˆ†æ–¹ç¨‹(SDE)çš„é‡‡æ ·æ­¥éª¤ï¼Œä»¥å…‹æœç°æœ‰æ–¹æ³•åœ¨å¤šæ­¥å»å™ªä¸­é¢ä¸´çš„ç¨€ç–å’Œæ¨¡ç³Šå¥–åŠ±ä¿¡å·é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°é«˜ç†µæ­¥éª¤èƒ½å¤Ÿä¿ƒè¿›æ›´æœ‰æ•ˆçš„æ¢ç´¢ï¼Œè€Œä½ç†µæ­¥éª¤åˆ™å¯¼è‡´ç»“æœä¸æ˜æ˜¾ã€‚é€šè¿‡å°†è¿ç»­çš„ä½ç†µæ­¥éª¤åˆå¹¶ä¸ºä¸€ä¸ªé«˜ç†µæ­¥éª¤ï¼Œå¹¶åœ¨å…¶ä»–æ­¥éª¤ä¸Šåº”ç”¨å¸¸å¾®åˆ†æ–¹ç¨‹(ODE)é‡‡æ ·ï¼ŒE-GRPOæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03471",
            "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
            "url": "https://huggingface.co/papers/2601.03471",
            "abstract": "EpiQAL presents a novel benchmark for evaluating epidemiological reasoning in language models through three distinct subsets measuring factual recall, multi-step inference, and conclusion reconstruction from scientific literature.  \t\t\t\t\tAI-generated summary \t\t\t\t Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction.",
            "score": 5,
            "issue_id": 471,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "14c81969b5c87a07",
            "authors": [
                "Mingyang Wei",
                "Dehai Min",
                "Zewen Liu",
                "Yuzhang Xie",
                "Guanchen Wu",
                "Carl Yang",
                "Max S. Y. Lau",
                "Qi He",
                "Lu Cheng",
                "Wei Jin"
            ],
            "affiliations": [
                "Emory University",
                "Microsoft",
                "University of Illinois Chicago"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03471.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#benchmark",
                    "#healthcare",
                    "#science"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿Ğ¸Ğ´ĞµĞ¼Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ EpiQAL â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¸Ğ´ĞµĞ¼Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ‚Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸĞµÑ€Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ„Ğ°ĞºÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¸Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑĞ¿Ğ¸Ğ´ĞµĞ¼Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ñ€ĞµÑ‚ÑŒĞµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğµ Discussion. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑÑÑ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¿Ğ¸Ğ´ĞµĞ¼Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ¿ĞµÑ… Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ° Chain-of-Thought Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ğ´Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°Ñ…."
                },
                "en": {
                    "title": "EpiQAL: Advancing Epidemiological Reasoning in Language Models",
                    "desc": "EpiQAL is a new benchmark designed to assess how well language models can perform epidemiological reasoning. It includes three specific tests that measure a model's ability to recall facts, make multi-step inferences, and reconstruct conclusions from scientific texts. This benchmark is unique because it focuses on understanding population-level health issues rather than just clinical or patient-level questions. The findings show that current language models struggle with these tasks, especially with multi-step reasoning, highlighting the need for improved methods in this area."
                },
                "zh": {
                    "title": "EpiQALï¼šæµè¡Œç—…å­¦æ¨ç†çš„æ–°åŸºå‡†",
                    "desc": "EpiQALæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨æµè¡Œç—…å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å®ƒé€šè¿‡ä¸‰ä¸ªä¸åŒçš„å­é›†æ¥æµ‹é‡äº‹å®å›å¿†ã€å¤šæ­¥æ¨ç†å’Œä»ç§‘å­¦æ–‡çŒ®ä¸­é‡å»ºç»“è®ºã€‚ç°æœ‰çš„åŒ»å­¦é—®ç­”åŸºå‡†ä¸»è¦å…³æ³¨ä¸´åºŠçŸ¥è¯†æˆ–æ‚£è€…å±‚é¢çš„æ¨ç†ï¼Œè€Œå¾ˆå°‘ç³»ç»Ÿåœ°è¯„ä¼°åŸºäºè¯æ®çš„æµè¡Œç—…å­¦æ¨ç†ã€‚EpiQALä¸ºå¤šç§ç–¾ç—…æä¾›äº†ç¬¬ä¸€ä¸ªè¯Šæ–­åŸºå‡†ï¼Œæ­ç¤ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æµè¡Œç—…å­¦æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ­¥æ¨ç†æ–¹é¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03699",
            "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
            "url": "https://huggingface.co/papers/2601.03699",
            "abstract": "RedBench presents a unified dataset with standardized risk categorization for evaluating LLM vulnerabilities across multiple domains and attack types.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against adversarial prompts is paramount. However, existing red teaming datasets suffer from inconsistent risk categorizations, limited domain coverage, and outdated evaluations, hindering systematic vulnerability assessments. To address these challenges, we introduce RedBench, a universal dataset aggregating 37 benchmark datasets from leading conferences and repositories, comprising 29,362 samples across attack and refusal prompts. RedBench employs a standardized taxonomy with 22 risk categories and 19 domains, enabling consistent and comprehensive evaluations of LLM vulnerabilities. We provide a detailed analysis of existing datasets, establish baselines for modern LLMs, and open-source the dataset and evaluation code. Our contributions facilitate robust comparisons, foster future research, and promote the development of secure and reliable LLMs for real-world deployment. Code: https://github.com/knoveleng/redeval",
            "score": 4,
            "issue_id": 470,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "b9041e4ef3eab3d5",
            "authors": [
                "Quy-Anh Dang",
                "Chris Ngo",
                "Truong-Son Hy"
            ],
            "affiliations": [
                "Knovel Engineering Lab",
                "University of Alabama at Birmingham",
                "VNU University of Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03699.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#security",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ”’",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "RedBench â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ 37 ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ² Ğ¾Ğ´Ğ¸Ğ½ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ 29,362 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ 22 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ¸ÑĞºĞ° Ğ¸ 19 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ LLM Ğº adversarial Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ°Ñ‚Ğ°ĞºÑƒÑÑ‰Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "RedBench: A Unified Dataset for Evaluating LLM Vulnerabilities",
                    "desc": "RedBench is a new dataset designed to evaluate the vulnerabilities of large language models (LLMs) against various types of attacks. It combines 37 existing benchmark datasets into one unified resource, containing over 29,000 samples of attack and refusal prompts. The dataset uses a standardized risk categorization system with 22 categories and covers 19 different domains, allowing for consistent assessments of LLM robustness. By providing this comprehensive resource and evaluation code, RedBench aims to enhance the security and reliability of LLMs in critical applications."
                },
                "zh": {
                    "title": "RedBenchï¼šè¯„ä¼°LLMè„†å¼±æ€§çš„ç»Ÿä¸€æ•°æ®é›†",
                    "desc": "RedBenchæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸå’Œæ”»å‡»ç±»å‹ä¸‹çš„è„†å¼±æ€§ã€‚è¯¥æ•°æ®é›†æ•´åˆäº†æ¥è‡ªé¡¶çº§ä¼šè®®å’Œå­˜å‚¨åº“çš„37ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œå…±åŒ…å«29,362ä¸ªæ ·æœ¬ï¼Œæ¶µç›–æ”»å‡»å’Œæ‹’ç»æç¤ºã€‚RedBenché‡‡ç”¨æ ‡å‡†åŒ–çš„åˆ†ç±»æ³•ï¼Œè®¾æœ‰22ä¸ªé£é™©ç±»åˆ«å’Œ19ä¸ªé¢†åŸŸï¼Œä½¿å¾—å¯¹LLMè„†å¼±æ€§çš„è¯„ä¼°æ›´åŠ ä¸€è‡´å’Œå…¨é¢ã€‚é€šè¿‡æä¾›ç°æœ‰æ•°æ®é›†çš„è¯¦ç»†åˆ†æå’Œç°ä»£LLMçš„åŸºå‡†ï¼ŒRedBenchä¿ƒè¿›äº†ç¨³å¥çš„æ¯”è¾ƒï¼Œæ¨åŠ¨äº†æœªæ¥çš„ç ”ç©¶ï¼Œå¹¶æ”¯æŒå®‰å…¨å¯é çš„LLMåœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03315",
            "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
            "url": "https://huggingface.co/papers/2601.03315",
            "abstract": "A case study of four attempts to autonomously generate ML research papers using LLM agents reveals recurring failure modes and proposes design principles for robust AI-scientist systems.  \t\t\t\t\tAI-generated summary \t\t\t\t We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1",
            "score": 4,
            "issue_id": 479,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "e24727d5beee24ef",
            "authors": [
                "Dhruv Trehan",
                "Paras Chopra"
            ],
            "affiliations": [
                "Lossfunk"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03315.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#open_source",
                    "#hallucinations",
                    "#science"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ° Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ AI-ÑƒÑ‡ĞµĞ½Ñ‹Ğ¼: ÑƒÑ€Ğ¾ĞºĞ¸ Ğ¸Ğ· Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¿Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸Ğ· ÑˆĞµÑÑ‚Ğ¸ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ»Ğ°ÑÑŒ Ğ¸ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ° Ğ½Ğ° ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Agents4Science 2025, Ğ¿Ñ€Ğ¾Ğ¹Ğ´Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ĞºĞ°Ğº Ñƒ Ğ»ÑĞ´ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ Ñƒ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… AI ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ ÑˆĞµÑÑ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºÑ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼: ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ñ€ĞµĞ¹Ñ„ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½ĞµĞ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¼ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… AI-ÑƒÑ‡ĞµĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Building Better AI Scientists: Learning from Failures",
                    "desc": "This paper explores the challenges faced when using large language models (LLMs) to autonomously generate machine learning research papers. The authors conducted a case study with four attempts, where three failed due to various issues such as bias in training data and memory degradation during long tasks. One attempt succeeded and was accepted for publication, demonstrating the potential of AI in scientific research. The study identifies six common failure modes and proposes four design principles to enhance the reliability of AI systems in scientific discovery."
                },
                "zh": {
                    "title": "æå‡AIç§‘å­¦å®¶ç³»ç»Ÿçš„è®¾è®¡åŸåˆ™",
                    "desc": "æœ¬ç ”ç©¶æ¡ˆä¾‹åˆ†æäº†å››æ¬¡å°è¯•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è‡ªä¸»ç”Ÿæˆæœºå™¨å­¦ä¹ ç ”ç©¶è®ºæ–‡çš„è¿‡ç¨‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸‰æ¬¡å°è¯•åœ¨å®æ–½æˆ–è¯„ä¼°é˜¶æ®µå¤±è´¥ï¼Œåªæœ‰ä¸€æ¬¡æˆåŠŸå®Œæˆå¹¶è¢«æ¥å—åˆ°Agents4Science 2025ä¼šè®®ã€‚ç ”ç©¶ä¸­æ€»ç»“äº†å…­ç§å¸¸è§çš„å¤±è´¥æ¨¡å¼ï¼ŒåŒ…æ‹¬å¯¹è®­ç»ƒæ•°æ®çš„åè§ã€æ‰§è¡Œå‹åŠ›ä¸‹çš„å®ç°æ¼‚ç§»ã€é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„è®°å¿†å’Œä¸Šä¸‹æ–‡é€€åŒ–ç­‰ã€‚æœ€åï¼Œæå‡ºäº†å››é¡¹è®¾è®¡åŸåˆ™ï¼Œä»¥å¢å¼ºAIç§‘å­¦å®¶ç³»ç»Ÿçš„é²æ£’æ€§ï¼Œå¹¶æ¢è®¨äº†å¯¹è‡ªä¸»ç§‘å­¦å‘ç°çš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03467",
            "title": "ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing",
            "url": "https://huggingface.co/papers/2601.03467",
            "abstract": "ThinkRL-Edit enhances reasoning-centric image editing through reinforcement learning by expanding visual reasoning exploration beyond denoising stochasticity and using unbiased reward strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)-based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.",
            "score": 3,
            "issue_id": 472,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "bb71677f72564096",
            "authors": [
                "Hengjia Li",
                "Liming Jiang",
                "Qing Yan",
                "Yizhi Song",
                "Hao Kang",
                "Zichuan Liu",
                "Xin Lu",
                "Boxi Wu",
                "Deng Cai"
            ],
            "affiliations": [
                "Intelligent Creation, ByteDance",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03467.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#cv",
                    "#rl",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ThinkRL-Edit â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ reinforcement learning Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Chain-of-Thought Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ´Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾ĞºĞ¾Ğ½Ñ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼. Ğ”Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ—Ğ°Ğ¼ĞµĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… VLM Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½Ğ° Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ Ñ‡ĞµĞº-Ğ»Ğ¸ÑÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Image Editing with Reasoning-Driven Reinforcement Learning",
                    "desc": "ThinkRL-Edit is a novel framework that enhances image editing by focusing on reasoning through reinforcement learning. It addresses limitations in traditional methods by allowing for broader exploration of visual reasoning beyond just denoising. The framework employs Chain-of-Thought reasoning, which involves planning and reflection to evaluate different semantic possibilities before generating images. Additionally, it introduces an unbiased reward strategy that improves the accuracy and interpretability of the editing process, leading to better results in reasoning-centric image edits."
                },
                "zh": {
                    "title": "æ¨ç†é©±åŠ¨çš„å›¾åƒç¼–è¾‘æ–°çªç ´",
                    "desc": "ThinkRL-Edit æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„å›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚å®ƒé€šè¿‡å°†è§†è§‰æ¨ç†ä¸å›¾åƒåˆæˆè§£è€¦ï¼Œæ‰©å±•äº†æ¨ç†æ¢ç´¢çš„èŒƒå›´ï¼Œè¶…è¶Šäº†å»å™ªçš„éšæœºæ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†åŸºäºæ€ç»´é“¾çš„æ¨ç†é‡‡æ ·ï¼Œä¿ƒä½¿æ¨¡å‹åœ¨ç”Ÿæˆä¹‹å‰è¿›è¡Œå¤šç§è¯­ä¹‰å‡è®¾çš„æ¢ç´¢å’ŒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒThinkRL-Edit åœ¨æ¨ç†ä¸­å¿ƒçš„å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œç”Ÿæˆçš„ç¼–è¾‘ç»“æœæ›´ç¬¦åˆæŒ‡ä»¤è¦æ±‚ï¼Œè§†è§‰ä¸Šæ›´è¿è´¯ï¼Œè¯­ä¹‰ä¸Šæ›´æ‰å®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03448",
            "title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
            "url": "https://huggingface.co/papers/2601.03448",
            "abstract": "Language models pre-trained with a framework combining standard next-token prediction and structured language learning tasks show enhanced linguistic competence without sacrificing general reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) are pre-trained on raw text datasets to generate text sequences token-by-token. While this approach facilitates the learning of world knowledge and reasoning, it does not explicitly optimize for linguistic competence. To bridge this gap, we propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. Inspired by human language acquisition, L2T transforms raw text into structured input-output pairs to provide explicit linguistic stimulation. Pre-training LMs on a mixture of raw text and L2T data not only improves overall performance on linguistic competence benchmarks but accelerates its acquisition, while maintaining competitive performance on general reasoning tasks.",
            "score": 3,
            "issue_id": 479,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "2a965cc0eaea825a",
            "authors": [
                "Atsuki Yamaguchi",
                "Maggie Mi",
                "Nikolaos Aletras"
            ],
            "affiliations": [
                "School of Computer Science, University of Sheffield, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03448.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ L2T â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ÑÑÑÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ¾Ğ²Ğ»Ğ°Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ÑÑ‹Ñ€Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Â«Ğ²Ñ…Ğ¾Ğ´-Ğ²Ñ‹Ñ…Ğ¾Ğ´Â» Ğ´Ğ»Ñ ÑĞºÑĞ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¼ĞµÑĞ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… L2T ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ĞµÑ‘ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ğµ. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Language Models with Structured Learning for Better Linguistic Competence",
                    "desc": "This paper introduces L2T, a novel pre-training framework for language models that combines traditional next-token prediction with structured language learning tasks. By transforming raw text into structured input-output pairs, L2T enhances the linguistic competence of models, mimicking aspects of human language acquisition. The approach allows models to learn linguistic structures explicitly, leading to improved performance on linguistic benchmarks. Importantly, L2T maintains strong general reasoning capabilities, ensuring that the model remains versatile across various tasks."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€èƒ½åŠ›ä¸æ¨ç†èƒ½åŠ›çš„å¹³è¡¡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶L2Tï¼Œç»“åˆäº†æ ‡å‡†çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œç»“æ„åŒ–è¯­è¨€å­¦ä¹ ä»»åŠ¡ï¼Œä»¥æé«˜è¯­è¨€æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›ã€‚é€šè¿‡å°†åŸå§‹æ–‡æœ¬è½¬åŒ–ä¸ºç»“æ„åŒ–çš„è¾“å…¥è¾“å‡ºå¯¹ï¼ŒL2Tä¸ºæ¨¡å‹æä¾›äº†æ˜ç¡®çš„è¯­è¨€åˆºæ¿€ï¼Œæ¨¡ä»¿äººç±»è¯­è¨€ä¹ å¾—çš„è¿‡ç¨‹ã€‚é¢„è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨åŸå§‹æ–‡æœ¬å’ŒL2Tæ•°æ®çš„æ··åˆï¼Œä¸ä»…æå‡äº†è¯­è¨€èƒ½åŠ›åŸºå‡†æµ‹è¯•çš„æ•´ä½“è¡¨ç°ï¼Œè¿˜åŠ å¿«äº†è¯­è¨€èƒ½åŠ›çš„è·å–ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨ä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸Šä»ä¿æŒäº†ç«äº‰åŠ›çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.02933",
            "title": "Pearmut: Human Evaluation of Translation Made Trivial",
            "url": "https://huggingface.co/papers/2601.02933",
            "abstract": "Pearmut is a platform that simplifies human evaluation in multilingual NLP by providing a lightweight solution for end-to-end evaluation with support for various protocols and learning strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort.",
            "score": 2,
            "issue_id": 482,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "0b135526779a2ba3",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#open_source",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "ğŸ‘¥",
                "ru": {
                    "title": "Ğ§ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ: ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ NLP Ñ‡ĞµÑ€ĞµĞ· Pearmut",
                    "desc": "Pearmut â€” ÑÑ‚Ğ¾ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ»Ñ‘Ğ³ĞºĞ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ². ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ñ‹Ğ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… NLP ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ½Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ·-Ğ·Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Pearmut Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ (DA, ESA, MQM), Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ´ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½ÑƒÑ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑƒÑĞ¸Ğ»Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Simplifying Human Evaluation in Multilingual NLP with Pearmut",
                    "desc": "Pearmut is a platform designed to streamline human evaluation in multilingual natural language processing (NLP). It addresses the challenges of traditional evaluation methods, which are often complex and time-consuming, by offering a lightweight solution that supports various evaluation protocols and learning strategies. The platform focuses on machine translation tasks and allows for both standard and customizable evaluation approaches, making it easier for researchers to incorporate human evaluation into their workflows. By simplifying the process, Pearmut aims to make reliable human evaluation a regular part of model development and assessment."
                },
                "zh": {
                    "title": "ç®€åŒ–å¤šè¯­è¨€NLPçš„äººç±»è¯„ä¼°",
                    "desc": "Pearmutæ˜¯ä¸€ä¸ªç®€åŒ–å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰äººç±»è¯„ä¼°çš„å¹³å°ï¼Œæä¾›è½»é‡çº§çš„ç«¯åˆ°ç«¯è¯„ä¼°è§£å†³æ–¹æ¡ˆã€‚å®ƒæ”¯æŒå¤šç§è¯„ä¼°åè®®å’Œå­¦ä¹ ç­–ç•¥ï¼Œç‰¹åˆ«å…³æ³¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚é€šè¿‡å®ç°æ ‡å‡†è¯„ä¼°åè®®ï¼ŒPearmutæ¶ˆé™¤äº†å¸¸è§çš„å…¥é—¨éšœç¢ï¼Œä½¿äººç±»è¯„ä¼°å˜å¾—åƒè‡ªåŠ¨è¯„ä¼°ä¸€æ ·ç®€å•ã€‚è¯¥å¹³å°ä½¿å¾—å¯é çš„äººç±»è¯„ä¼°æˆä¸ºæ¨¡å‹å¼€å‘å’Œè¯Šæ–­çš„å¸¸è§„ç»„æˆéƒ¨åˆ†ï¼Œè€Œä¸æ˜¯å¶å°”çš„åŠªåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03236",
            "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
            "url": "https://huggingface.co/papers/2601.03236",
            "abstract": "MAGMA is a multi-graph memory architecture that improves long-context reasoning in language models by separating memory representation from retrieval logic across semantic, temporal, causal, and entity dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.",
            "score": 1,
            "issue_id": 475,
            "pub_date": "2026-01-06",
            "pub_date_card": {
                "ru": "6 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 6",
                "zh": "1æœˆ6æ—¥"
            },
            "hash": "89b8c625aa26fb05",
            "authors": [
                "Dongming Jiang",
                "Yi Li",
                "Guanpeng Li",
                "Bingzhe Li"
            ],
            "affiliations": [
                "University of Florida",
                "University of Texas at Dallas"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03236.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#graphs",
                    "#long_context",
                    "#interpretability",
                    "#reasoning",
                    "#agents",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "MAGMA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒÑ ĞµÑ‘ Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼, Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ±Ğ»Ğ¸Ğ·Ğ¾ÑÑ‚ÑŒ, MAGMA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ»Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "MAGMA: Enhancing Long-Context Reasoning with Multi-Graph Memory",
                    "desc": "MAGMA is a new memory architecture designed to enhance long-context reasoning in language models. It separates how memory is represented from how it is retrieved, using different graphs for semantic, temporal, causal, and entity information. This separation allows for better interpretability and alignment between what a query is asking and the evidence retrieved, improving reasoning accuracy. Experiments show that MAGMA outperforms existing memory systems in tasks that require understanding over long contexts."
                },
                "zh": {
                    "title": "MAGMAï¼šæå‡é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„å¤šå›¾è®°å¿†æ¶æ„",
                    "desc": "MAGMAæ˜¯ä¸€ç§å¤šå›¾è®°å¿†æ¶æ„ï¼Œæ—¨åœ¨æ”¹å–„è¯­è¨€æ¨¡å‹ä¸­çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚å®ƒé€šè¿‡åœ¨è¯­ä¹‰ã€æ—¶é—´ã€å› æœå’Œå®ä½“ç»´åº¦ä¸Šåˆ†ç¦»è®°å¿†è¡¨ç¤ºä¸æ£€ç´¢é€»è¾‘ï¼Œæå‡äº†æ¨ç†çš„å‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå•ä¸€è®°å¿†å­˜å‚¨çš„è¯­ä¹‰ç›¸ä¼¼æ€§ä¸åŒï¼ŒMAGMAå…è®¸æ›´çµæ´»çš„æŸ¥è¯¢é€‚åº”æ€§é€‰æ‹©å’Œç»“æ„åŒ–ä¸Šä¸‹æ–‡æ„å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAGMAåœ¨é•¿æ—¶é—´æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„è®°å¿†ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.04090",
            "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
            "url": "https://huggingface.co/papers/2601.04090",
            "abstract": "Gen3R combines foundational reconstruction models with video diffusion models to generate 3D scenes with RGB videos and geometric information through aligned latents.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.",
            "score": 0,
            "issue_id": 480,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "5b7a5d3b7d8f9455",
            "authors": [
                "Jiaxin Huang",
                "Yuanbo Yang",
                "Bangbang Yang",
                "Lin Ma",
                "Yuewen Ma",
                "Yiyi Liao"
            ],
            "affiliations": [
                "ByteDance",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.04090.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#video",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Gen3R Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… VGGT Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Gen3R Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ RGB Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€, ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Bridging 3D Reconstruction and Video Generation with Gen3R",
                    "desc": "Gen3R is a novel method that integrates foundational reconstruction models with video diffusion models to create 3D scenes from RGB videos and geometric data. It utilizes the VGGT reconstruction model to generate geometric latents, which are then aligned with appearance latents from pre-trained video diffusion models. By generating these aligned latents together, Gen3R can produce both realistic RGB videos and detailed 3D geometry, including camera poses and depth maps. The results show that Gen3R outperforms existing methods in 3D scene generation and enhances reconstruction robustness by combining generative and reconstruction techniques."
                },
                "zh": {
                    "title": "Gen3Rï¼šé‡å»ºä¸ç”Ÿæˆæ¨¡å‹çš„å®Œç¾ç»“åˆ",
                    "desc": "Gen3Ræ˜¯ä¸€ç§ç»“åˆåŸºç¡€é‡å»ºæ¨¡å‹å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå…·æœ‰RGBè§†é¢‘å’Œå‡ ä½•ä¿¡æ¯çš„3Dåœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒé€‚é…å™¨ï¼Œå°†VGGTé‡å»ºæ¨¡å‹çš„å‡ ä½•æ½œå˜é‡ä¸é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„å¤–è§‚æ½œå˜é‡å¯¹é½ã€‚é€šè¿‡å…±åŒç”Ÿæˆè¿™äº›è§£è€¦ä½†å¯¹é½çš„æ½œå˜é‡ï¼ŒGen3Rèƒ½å¤Ÿç”ŸæˆRGBè§†é¢‘åŠå…¶å¯¹åº”çš„3Då‡ ä½•ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç›¸æœºå§¿æ€ã€æ·±åº¦å›¾å’Œå…¨å±€ç‚¹äº‘ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•å›¾åƒå’Œå¤šå›¾åƒæ¡ä»¶ä¸‹çš„3Dåœºæ™¯ç”Ÿæˆä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶ä¸”é€šè¿‡åˆ©ç”¨ç”Ÿæˆå…ˆéªŒå¢å¼ºäº†é‡å»ºçš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.03955",
            "title": "ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2601.03955",
            "abstract": "A novel 1D visual tokenizer called Residual Tokenizer is introduced that incorporates hierarchical residuals to improve autoregressive image generation by leveraging vision-specific design principles rather than language modeling approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring \"vision\" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at https://github.com/Kwai-Kolors/ResTok.",
            "score": 0,
            "issue_id": 482,
            "pub_date": "2026-01-07",
            "pub_date_card": {
                "ru": "7 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 7",
                "zh": "1æœˆ7æ—¥"
            },
            "hash": "89b963ed293fb10d",
            "authors": [
                "Xu Zhang",
                "Cheng Da",
                "Huan Yang",
                "Kun Gai",
                "Ming Lu",
                "Zhan Ma"
            ],
            "affiliations": [
                "Kolors Team, Kuaishou Technology",
                "Vision Lab, Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.03955.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ—ï¸",
                "ru": {
                    "title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‚ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Residual Tokenizer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ·Ğ°Ğ¸Ğ¼ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¸Ğ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸. Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ ĞºĞ°Ğº Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºÑ€Ğ¾ÑÑ-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ñ†ĞµĞ»Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° Ñ€Ğ°Ğ· Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ gFID 2.34 Ğ½Ğ° ImageNet-256 Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 9 ÑˆĞ°Ğ³Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Hierarchical Residuals",
                    "desc": "The paper presents a new visual tokenizer called Residual Tokenizer (ResTok) designed to enhance autoregressive image generation. Unlike traditional tokenizers that mimic language models, ResTok utilizes hierarchical residuals to better capture the structure of visual data. This approach allows for improved feature fusion and reduces information overlap, leading to more effective latent distributions for modeling. Additionally, a hierarchical autoregressive generator is introduced, which accelerates the generation process by predicting multiple latent tokens simultaneously, resulting in significant performance improvements."
                },
                "zh": {
                    "title": "å¼•å…¥å±‚æ¬¡æ®‹å·®ï¼Œæå‡å›¾åƒç”Ÿæˆèƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„1Dè§†è§‰æ ‡è®°å™¨ï¼Œç§°ä¸ºæ®‹å·®æ ‡è®°å™¨ï¼ˆResidual Tokenizerï¼‰ï¼Œå®ƒé€šè¿‡å¼•å…¥å±‚æ¬¡æ®‹å·®æ¥æ”¹å–„è‡ªå›å½’å›¾åƒç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè¯­è¨€å»ºæ¨¡çš„æ–¹æ³•ä¸åŒï¼Œæ®‹å·®æ ‡è®°å™¨åˆ©ç”¨è§†è§‰ç‰¹å®šçš„è®¾è®¡åŸåˆ™ï¼Œæ„å»ºå±‚æ¬¡åŒ–çš„æ®‹å·®ç»“æ„ï¼Œä»¥å¢å¼ºå›¾åƒå’Œæ½œåœ¨æ ‡è®°çš„è¡¨ç¤ºèƒ½åŠ›ã€‚é€šè¿‡é€æ­¥åˆå¹¶è·å¾—çš„å±‚æ¬¡è¡¨ç¤ºï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸€å±‚å®ç°è·¨å±‚ç‰¹å¾èåˆï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¡¨ç°åŠ›ã€‚åŒæ—¶ï¼Œå±‚æ¬¡ä¹‹é—´çš„è¯­ä¹‰æ®‹å·®é˜²æ­¢äº†ä¿¡æ¯é‡å ï¼Œä½¿å¾—æ½œåœ¨åˆ†å¸ƒæ›´åŠ é›†ä¸­ï¼Œä¾¿äºè‡ªå›å½’å»ºæ¨¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.00705",
            "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
            "url": "https://huggingface.co/papers/2601.00705",
            "abstract": "RGS-SLAM presents a robust Gaussian-splatting SLAM framework that uses dense multi-view correspondences and DINOv3 descriptors for efficient, stable mapping with improved rendering fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.",
            "score": 0,
            "issue_id": 476,
            "pub_date": "2026-12-28",
            "pub_date_card": {
                "ru": "28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 28",
                "zh": "12æœˆ28æ—¥"
            },
            "hash": "1549408a2d070aca",
            "authors": [
                "Wei-Tse Cheng",
                "Yen-Jen Chiou",
                "Yuan-Fu Yang"
            ],
            "affiliations": [
                "National Yang Ming Chiao Tung University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.00705.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ğ° SLAM",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ RGS-SLAM, Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ñ‚Ñ€Ğ¸Ğ°Ğ½Ğ³ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· DINOv3 Ğ´ĞµÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ¿ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 20%, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… TUM RGB-D Ğ¸ Replica, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ´Ğ¾ 925 FPS."
                },
                "en": {
                    "title": "RGS-SLAM: Fast and Accurate Mapping with Gaussian Splatting",
                    "desc": "RGS-SLAM is a new framework for Simultaneous Localization and Mapping (SLAM) that enhances the process of creating 3D maps using Gaussian splatting. It introduces a method that initializes mapping using dense correspondences from DINOv3 descriptors, eliminating the need for a complex training phase. This approach allows for faster and more stable mapping by quickly generating a well-distributed Gaussian representation of the environment. RGS-SLAM has shown to improve accuracy and rendering quality in challenging scenes while maintaining high-speed performance, making it compatible with existing SLAM systems."
                },
                "zh": {
                    "title": "RGS-SLAMï¼šé«˜æ•ˆç¨³å®šçš„SLAMæ–°æ¡†æ¶",
                    "desc": "RGS-SLAMæ˜¯ä¸€ç§ç¨³å¥çš„é«˜æ–¯ç‚¹äº‘SLAMæ¡†æ¶ï¼Œåˆ©ç”¨å¯†é›†çš„å¤šè§†è§’å¯¹åº”å…³ç³»å’ŒDINOv3æè¿°ç¬¦è¿›è¡Œé«˜æ•ˆã€ç¨³å®šçš„åœ°å›¾æ„å»ºã€‚å®ƒé€šè¿‡ä¸€ç§æ— è®­ç»ƒçš„å¯¹åº”åˆ°é«˜æ–¯åˆå§‹åŒ–æ–¹æ³•ï¼Œæ›¿ä»£äº†GS-SLAMä¸­çš„æ®‹å·®é©±åŠ¨å¯†é›†åŒ–é˜¶æ®µã€‚RGS-SLAMä¸€æ¬¡æ€§ä¸‰è§’åŒ–å¯†é›†çš„å¤šè§†è§’å¯¹åº”å…³ç³»ï¼Œç”Ÿæˆç»“æ„æ„ŸçŸ¥çš„é«˜æ–¯ç§å­ï¼Œä»è€ŒåŠ é€Ÿæ”¶æ•›å¹¶æé«˜æ¸²æŸ“è´¨é‡ã€‚ç»è¿‡TUM RGB-Då’ŒReplicaæ•°æ®é›†çš„è¯„ä¼°ï¼ŒRGS-SLAMåœ¨å®šä½å’Œé‡å»ºç²¾åº¦ä¸Šä¸æœ€å…ˆè¿›çš„SLAMç³»ç»Ÿç›¸æ¯”è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨é«˜è¾¾925 FPSçš„é€Ÿåº¦ä¸‹å®ç°å®æ—¶åœ°å›¾æ„å»ºã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-07.html",
    "link_next": "2026-01-09.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "07.01",
        "en": "01/07",
        "zh": "1æœˆ7æ—¥"
    },
    "short_date_next": {
        "ru": "09.01",
        "en": "01/09",
        "zh": "1æœˆ9æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 7,
        "#agents": 4,
        "#cv": 2,
        "#rl": 5,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 2,
        "#inference": 0,
        "#3d": 2,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 3,
        "#low_resource": 0
    }
}