{
    "date": {
        "ru": "18 декабря",
        "en": "December 18",
        "zh": "12月18日"
    },
    "time_utc": "2024-12-18 08:14",
    "weekday": 2,
    "issue_id": 1186,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.13147",
            "title": "Are Your LLMs Capable of Stable Reasoning?",
            "url": "https://huggingface.co/papers/2412.13147",
            "abstract": "The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks. However, a significant discrepancy persists between benchmark performances and real-world applications. We identify this gap as primarily stemming from current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, particularly in complex reasoning tasks where both accuracy and consistency are crucial. This work makes two key contributions. First, we introduce G-Pass@k, a novel evaluation metric that provides a continuous assessment of model performance across multiple sampling attempts, quantifying both the model's peak performance potential and its stability. Second, we present LiveMathBench, a dynamic benchmark comprising challenging, contemporary mathematical problems designed to minimize data leakage risks during evaluation. Through extensive experiments using G-Pass@k on state-of-the-art LLMs with LiveMathBench, we provide comprehensive insights into both their maximum capabilities and operational consistency. Our findings reveal substantial room for improvement in LLMs' \"realistic\" reasoning capabilities, highlighting the need for more robust evaluation methods. The benchmark and detailed results are available at: https://github.com/open-compass/GPassK.",
            "score": 45,
            "issue_id": 1181,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "a030a3cb6cc36da2",
            "authors": [
                "Junnan Liu",
                "Hongwei Liu",
                "Linchen Xiao",
                "Ziyi Wang",
                "Kuikun Liu",
                "Songyang Gao",
                "Wenwei Zhang",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13147.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#benchmark",
                    "#evaluation",
                    "#reasoning",
                    "#leakage"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Новый подход к оценке способностей языковых моделей в сложных математических задачах",
                    "desc": "Статья представляет новый метод оценки больших языковых моделей (LLM) в задачах сложных рассуждений. Авторы вводят метрику G-Pass@k, которая оценивает как пиковую производительность модели, так и её стабильность при многократных попытках. Также представлен динамический бенчмарк LiveMathBench с современными математическими задачами для минимизации утечки данных при оценке. Эксперименты показывают значительный потенциал для улучшения 'реалистичных' способностей LLM к рассуждениям."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing LLM Evaluation for Real-World Reasoning",
                    "desc": "This paper addresses the gap between the performance of Large Language Models (LLMs) on benchmarks and their effectiveness in real-world scenarios, particularly in complex reasoning tasks. The authors propose a new evaluation metric called G-Pass@k, which assesses model performance over multiple attempts, focusing on both peak performance and stability. Additionally, they introduce LiveMathBench, a benchmark of challenging mathematical problems that reduces data leakage during evaluation. The study reveals that current LLMs have significant room for improvement in their reasoning capabilities, emphasizing the need for better evaluation methods."
                },
                "zh": {
                    "title": "提升大型语言模型推理能力的评估新方法",
                    "desc": "本论文探讨了大型语言模型（LLMs）在复杂推理任务中的表现与实际应用之间的差距。我们认为，这一差距主要源于当前的评估协议和指标无法全面反映LLMs的能力，尤其是在准确性和一致性至关重要的复杂推理任务中。为此，我们提出了G-Pass@k这一新评估指标，能够在多次采样中持续评估模型性能，并量化模型的最佳表现潜力和稳定性。此外，我们还推出了LiveMathBench，这是一个动态基准，包含具有挑战性的现代数学问题，旨在减少评估过程中的数据泄露风险。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12606",
            "title": "Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models",
            "url": "https://huggingface.co/papers/2412.12606",
            "abstract": "The rapidly developing field of large multimodal models (LMMs) has led to the emergence of diverse models with remarkable capabilities. However, existing benchmarks fail to comprehensively, objectively and accurately evaluate whether LMMs align with the diverse needs of humans in real-world scenarios. To bridge this gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which includes over 500 images covering six common scenarios of human life. Notably, the MDI-Benchmark offers two significant advantages over existing evaluations: (1) Each image is accompanied by two types of questions: simple questions to assess the model's understanding of the image, and complex questions to evaluate the model's ability to analyze and reason beyond basic content. (2) Recognizing that people of different age groups have varying needs and perspectives when faced with the same scenario, our benchmark stratifies questions into three age categories: young people, middle-aged people, and older people. This design allows for a detailed assessment of LMMs' capabilities in meeting the preferences and needs of different age groups. With MDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related tasks, indicating that existing LMMs still have considerable room for improvement in addressing real-world applications. Looking ahead, we anticipate that the MDI-Benchmark will open new pathways for aligning real-world personalization in LMMs. The MDI-Benchmark data and evaluation code are available at https://mdi-benchmark.github.io/",
            "score": 20,
            "issue_id": 1182,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "75fba478b54ada06",
            "authors": [
                "YiFan Zhang",
                "Shanglin Lei",
                "Runqi Qiao",
                "Zhuoma GongQue",
                "Xiaoshuai Song",
                "Guanting Dong",
                "Qiuna Tan",
                "Zhe Wei",
                "Peiqing Yang",
                "Ye Tian",
                "Yadong Xue",
                "Xiaofei Wang",
                "Honggang Zhang"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12606.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#alignment",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Многомерная оценка мультимодальных моделей для реальных задач",
                    "desc": "Предложен новый бенчмарк Multi-Dimensional Insights (MDI) для оценки мультимодальных моделей. MDI включает более 500 изображений с простыми и сложными вопросами, охватывающими 6 сценариев человеческой жизни. Бенчмарк учитывает потребности людей разных возрастных групп, что позволяет оценить способность моделей адаптироваться к различным пользователям. Даже передовые модели вроде GPT-4 достигают лишь 79% точности на задачах с учетом возраста, что указывает на потенциал для улучшения мультимодальных моделей."
                },
                "en": {
                    "title": "Enhancing LMM Evaluation with Age-Responsive Insights",
                    "desc": "This paper introduces the Multi-Dimensional Insights (MDI) benchmark, designed to evaluate large multimodal models (LMMs) in a more comprehensive way. It includes over 500 images and features two types of questions: simple ones for basic understanding and complex ones for deeper analysis and reasoning. The benchmark also categorizes questions by age groups, recognizing that different ages have unique perspectives and needs. The results show that while models like GPT-4o perform well, there is still significant room for improvement in aligning LMMs with real-world applications."
                },
                "zh": {
                    "title": "多维洞察基准：提升多模态模型的评估标准",
                    "desc": "大型多模态模型（LMMs）正在迅速发展，但现有的评估标准无法全面、客观地评估这些模型是否满足人类在现实场景中的多样化需求。为了解决这个问题，我们提出了多维洞察（MDI）基准，包含500多张图片，涵盖六种常见的人类生活场景。MDI基准的两个主要优势是：每张图片配有简单和复杂两种问题，评估模型对图像的理解和分析推理能力；同时，基准根据不同年龄段的需求，将问题分为年轻人、中年人和老年人三类。通过MDI基准，我们希望推动LMMs在现实应用中的个性化对齐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13018",
            "title": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain",
            "url": "https://huggingface.co/papers/2412.13018",
            "abstract": "As a typical and practical application of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) techniques have gained extensive attention, particularly in vertical domains where LLMs may lack domain-specific knowledge. In this paper, we introduce an omnidirectional and automatic RAG benchmark, OmniEval, in the financial domain. Our benchmark is characterized by its multi-dimensional evaluation framework, including (1) a matrix-based RAG scenario evaluation system that categorizes queries into five task classes and 16 financial topics, leading to a structured assessment of diverse query scenarios; (2) a multi-dimensional evaluation data generation approach, which combines GPT-4-based automatic generation and human annotation, achieving an 87.47\\% acceptance ratio in human evaluations on generated instances; (3) a multi-stage evaluation system that evaluates both retrieval and generation performance, result in a comprehensive evaluation on the RAG pipeline; and (4) robust evaluation metrics derived from rule-based and LLM-based ones, enhancing the reliability of assessments through manual annotations and supervised fine-tuning of an LLM evaluator. Our experiments demonstrate the comprehensiveness of OmniEval, which includes extensive test datasets and highlights the performance variations of RAG systems across diverse topics and tasks, revealing significant opportunities for RAG models to improve their capabilities in vertical domains. We open source the code of our benchmark in https://github.com/RUC-NLPIR/OmniEval{https://github.com/RUC-NLPIR/OmniEval}.",
            "score": 19,
            "issue_id": 1184,
            "pub_date": "2024-12-17",
            "pub_date_card": {
                "ru": "17 декабря",
                "en": "December 17",
                "zh": "12月17日"
            },
            "hash": "293aa1b03b853973",
            "authors": [
                "Shuting Wang",
                "Jiejun Tan",
                "Zhicheng Dou",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13018.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#rag",
                    "#science"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "OmniEval: Всесторонняя оценка RAG-систем в финансовой сфере",
                    "desc": "Статья представляет OmniEval - многомерный бенчмарк для оценки методов Retrieval-Augmented Generation (RAG) в финансовой сфере. Авторы разработали матричную систему оценки сценариев RAG, включающую 5 классов задач и 16 финансовых тем. Бенчмарк использует многоэтапную систему оценки, анализирующую как извлечение информации, так и генерацию текста. OmniEval демонстрирует различия в производительности систем RAG для разных тем и задач, выявляя потенциал для улучшения моделей в узкоспециализированных областях."
                },
                "en": {
                    "title": "OmniEval: Elevating RAG Evaluation in Finance",
                    "desc": "This paper presents OmniEval, a new benchmark for evaluating Retrieval-Augmented Generation (RAG) techniques specifically in the financial domain. It features a multi-dimensional evaluation framework that categorizes queries into various task classes and financial topics, allowing for structured assessments. The benchmark combines automatic data generation using GPT-4 with human annotations to ensure high-quality evaluation instances. The study reveals significant performance variations in RAG systems, highlighting areas for improvement and providing a comprehensive resource for future research in this area."
                },
                "zh": {
                    "title": "OmniEval：金融领域的全方位RAG评估基准",
                    "desc": "本文介绍了一种名为OmniEval的全方位自动化检索增强生成（RAG）基准，专注于金融领域。该基准具有多维评估框架，包括基于矩阵的RAG场景评估系统，能够将查询分类为五个任务类别和16个金融主题。我们还采用了结合GPT-4自动生成和人工标注的多维评估数据生成方法，确保生成实例的高接受率。实验结果表明，OmniEval在评估RAG系统的性能方面具有全面性，揭示了RAG模型在特定领域提升能力的显著机会。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.12276",
            "title": "Emergence of Abstractions: Concept Encoding and Decoding Mechanism for In-Context Learning in Transformers",
            "url": "https://huggingface.co/papers/2412.12276",
            "abstract": "Humans distill complex experiences into fundamental abstractions that enable rapid learning and adaptation. Similarly, autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. In this paper, we propose concept encoding-decoding mechanism to explain ICL by studying how transformers form and use internal abstractions in their representations. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of concept encoding and decoding. As the model learns to encode different latent concepts (e.g., ``Finding the first noun in a sentence.\") into distinct, separable representations, it concureently builds conditional decoding algorithms and improve its ICL performance. We validate the existence of this mechanism across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B). Further, through mechanistic interventions and controlled finetuning, we demonstrate that the quality of concept encoding is causally related and predictive of ICL performance. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.",
            "score": 2,
            "issue_id": 1184,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "9baa157dac26994a",
            "authors": [
                "Seungwook Han",
                "Jinyeop Song",
                "Jeff Gore",
                "Pulkit Agrawal"
            ],
            "affiliations": [
                "Improbable AI",
                "Massachusetts Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.12276.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#interpretability",
                    "#transfer_learning",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрытие тайн обучения в контексте: как трансформеры формируют и используют абстракции",
                    "desc": "Статья исследует механизмы обучения в контексте (ICL) у трансформеров, предлагая концепцию кодирования-декодирования. Авторы анализируют, как модели формируют внутренние абстракции в своих представлениях на синтетических задачах ICL. Исследование показывает, что по мере обучения модели кодировать различные латентные концепты, она одновременно улучшает алгоритмы декодирования и производительность ICL. Эти выводы подтверждаются на предобученных моделях разного масштаба и через механистические вмешательства."
                },
                "en": {
                    "title": "Unlocking In-Context Learning: The Power of Concept Encoding in Transformers",
                    "desc": "This paper explores how autoregressive transformers, like those used in natural language processing, learn and adapt through a process called in-context learning (ICL). The authors introduce a concept encoding-decoding mechanism that helps explain how these models form and utilize internal abstractions in their representations. By analyzing a small transformer on synthetic ICL tasks, they observe that as the model encodes different concepts, it simultaneously develops decoding strategies that enhance its performance. The study confirms that the quality of concept encoding is crucial for ICL success, providing insights into the workings of large language models."
                },
                "zh": {
                    "title": "揭示自回归变换器的学习机制",
                    "desc": "本文探讨了自回归变换器如何通过上下文学习（ICL）进行适应性学习。我们提出了一种概念编码-解码机制，以解释变换器如何在其表示中形成和使用内部抽象。通过对合成ICL任务的分析，我们发现模型在学习编码不同潜在概念的同时，构建条件解码算法，从而提高ICL性能。我们的研究结果揭示了概念编码质量与ICL表现之间的因果关系，帮助我们更好地理解大型语言模型的成功与失败模式。"
                }
            }
        }
    ],
    "link_prev": "2024-12-17.html",
    "link_next": "2024-12-19.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "17.12",
        "en": "12/17",
        "zh": "12月17日"
    },
    "short_date_next": {
        "ru": "19.12",
        "en": "12/19",
        "zh": "12月19日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0,
        "#evaluation": 1
    },
    "zh": {
        "text": "大型语言模型（LLMs）展示了显著的生成能力，但常常出现幻觉。检索增强生成（RAG）通过引入外部知识提供了有效解决方案，但现有方法仍面临一些限制：额外的部署成本、检索文本块中的冗余输入标记以及检索和生成的联合优化缺乏。为解决这些问题，我们提出了RetroLLM，一个统一的框架，将检索和生成整合为单一的过程，使LLMs能够直接从语料库中生成细粒度的证据。此外，我们引入了分层FM-Index约束和前瞻性约束解码策略，以减少不相关的解码空间并提高证据准确性。实验结果表明，RetroLLM在五个开放域问答数据集上表现优异。代码可在https://github.com/sunnynexus/RetroLLM获取。",
        "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation",
        "pinyin": "大型语言模型（LLMs）展示了显著的生成能力，但常常出现幻觉。检索增强生成（RAG）通过引入外部知识提供了有效解决方案，但现有方法仍面临一些限制：额外的部署成本、检索文本块中的冗余输入标记以及检索和生成的联合优化缺乏。为解决这些问题，我们提出了RetroLLM，一个统一的框架，将检索和生成整合为单一的过程，使LLMs能够直接从语料库中生成细粒度的证据。此外，我们引入了分层FM-Index约束和前瞻性约束解码策略，以减少不相关的解码空间并提高证据准确性。实验结果表明，RetroLLM在五个开放域问答数据集上表现优异。代码可在https://github.com/sunnynexus/RetroLLM获取。\n\ndà xíng yǔ yán mó xíng (LLMs) zhǎn shì le xiǎn zhù de shēng chéng néng lì, dàn cháng cháng chū xiàn huàn jué. Jiǎn suǒ zēng qiáng shēng chéng (RAG) tōng guò yǐn rù wài bù zhī shi tí gōng le yǒu xiào jiě jué fāng ān, dàn xiàn yǒu fāng fǎ réng miàn lìng yī xiē xiàn zhì: é wài de bù shù chéng běn, jiǎn suǒ wén běn kuài zhōng de róng yù shū rù biāo jì yǐ jí jiǎn suǒ hé shēng chéng de lián hé yōu huà quē fǎ. Wèi jiě jué zhè xiē wèn tí, wǒ men tí chū le RetroLLM, yī gè tǒng yī de kuàng jià, jiāng jiǎn suǒ hé shēng chéng zhěng hé wéi dān yī de guò chéng, shǐ LLMs néng gòu zhí jiē kù zhōng shēng chéng xì lì dù de zhèng jù. Cǐ wài, wǒ men yǐn rù le fēn céng FM-Index yuē shuō hé qián zhān xìng yuē shuō jiě mǎ zhuàn lüè, yǐ jiǎn shǎo bù xiāng guān de jiě mǎ kōng jiān yǐng tí gāo zhèng jù zhǔn què xìng. Shí yàn jié guǒ biǎo míng, RetroLLM zài wǔ gè kāi fàng yù qiú shù jù shù jù shàng biǎo xiàn yōu yán. Dài mǎ kě zài https://github.com/sunnynexus/RetroLLM huò qǔ.",
        "vocab": "[{'word': '大型', 'pinyin': 'dàxíng', 'trans': 'large-scale'},\n{'word': '语言模型', 'pinyin': 'yǔyán móxíng', 'trans': 'language model'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generation'},\n{'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'},\n{'word': '幻觉', 'pinyin': 'huànjué', 'trans': 'hallucination'},\n{'word': '检索', 'pinyin': 'jiǎnsuǒ', 'trans': 'retrieval'},\n{'word': '增强', 'pinyin': 'zēngqiáng', 'trans': 'enhanced'},\n{'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'},\n{'word': '外部', 'pinyin': 'wàibù', 'trans': 'external'},\n{'word': '知识', 'pinyin': 'zhīshi', 'trans': 'knowledge'},\n{'word': '提供', 'pinyin': 'tígōng', 'trans': 'provide'},\n{'word': '有效', 'pinyin': 'yǒuxiào', 'trans': 'effective'},\n{'word': '解决', 'pinyin': 'jiějué', 'trans': 'solution'},\n{'word': '方案', 'pinyin': 'fāngàn', 'trans': 'scheme'},\n{'word': '面临', 'pinyin': 'miànlín', 'trans': 'face'},\n{'word': '限制', 'pinyin': 'xiànzhì', 'trans': 'limitation'},\n{'word': '额外', 'pinyin': 'éwài', 'trans': 'additional'},\n{'word': '部署', 'pinyin': 'bùshǔ', 'trans': 'deployment'},\n{'word': '成本', 'pinyin': 'chéngběn', 'trans': 'cost'},\n{'word': '冗余', 'pinyin': 'róngyú', 'trans': 'redundant'},\n{'word': '输入', 'pinyin': 'shūrù', 'trans': 'input'},\n{'word': '标记', 'pinyin': 'biāojì', 'trans': 'token'},\n{'word': '联合', 'pinyin': 'liánhé', 'trans': 'joint'},\n{'word': '优化', 'pinyin': 'yōuhuà', 'trans': 'optimization'},\n{'word': '缺乏', 'pinyin': 'quēfá', 'trans': 'lack'},\n{'word': '提出', 'pinyin': 'tíchū', 'trans': 'propose'},\n{'word': '统一', 'pinyin': 'tǒngyī', 'trans': 'unified'},\n{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '整合', 'pinyin': 'zhěnghé', 'trans': 'integrate'},\n{'word': '过程', 'pinyin': 'guòchéng', 'trans': 'process'},\n{'word': '使', 'pinyin': 'shǐ', 'trans': 'make'},\n{'word': '直接', 'pinyin': 'zhíjiē', 'trans': 'directly'},\n{'word': '语料库', 'pinyin': 'yǔliào kù', 'trans': 'corpus'},\n{'word': '细粒度', 'pinyin': 'xìlìdù', 'trans': 'fine-grained'},\n{'word': '证据', 'pinyin': 'zhèngjù', 'trans': 'evidence'},\n{'word': '分层', 'pinyin': 'fēncéng', 'trans': 'hierarchical'},\n{'word': 'FM-Index', 'pinyin': 'FM-Index', 'trans': 'FM-Index'},\n{'word': '约束', 'pinyin': 'yuēshù', 'trans': 'constraint'},\n{'word': '前瞻性', 'pinyin': 'qiánzhānxìng', 'trans': 'forward-looking'},\n{'word': '解码', 'pinyin': 'jiěmǎ', 'trans': 'decoding'},\n{'word': '策略', 'pinyin': 'cèlüè', 'trans': 'strategy'},\n{'word': '减少', 'pinyin': 'jiǎnshǎo', 'trans': 'reduce'},\n{'word': '不相关', 'pinyin': 'bùxiāngguān', 'trans': 'irrelevant'},\n{'word': '空间', 'pinyin': 'kōngjiān', 'trans': 'space'},\n{'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'},\n{'word': '准确性', 'pinyin': 'zhǔnquèxìng', 'trans': 'accuracy'},\n{'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'},\n{'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'},\n{'word': '表明', 'pinyin': 'biǎomíng', 'trans': 'indicate'},\n{'word': '开放域', 'pinyin': 'kāifàng yù', 'trans': 'open-domain'},\n{'word': '问答', 'pinyin': 'wèndá', 'trans': 'question-answering'},\n{'word': '数据集', 'pinyin': 'shùjùjí', 'trans': 'dataset'},\n{'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'},\n{'word': '优异', 'pinyin': 'yōuyì', 'trans': 'excellent'},\n{'word': '代码', 'pinyin': 'dàimǎ', 'trans': 'code'},\n{'word': '获取', 'pinyin': 'huòqǔ', 'trans': 'obtain'}]",
        "trans": "Large Language Models (LLMs) have demonstrated significant generative capabilities but often suffer from hallucinations. Retrieval-Augmented Generation (RAG) offers an effective solution by introducing external knowledge; however, existing methods still face several limitations: additional deployment costs, redundant input tokens in retrieved text blocks, and a lack of joint optimization for retrieval and generation. To address these issues, we propose RetroLLM, a unified framework that integrates retrieval and generation into a single process, enabling LLMs to generate fine-grained evidence directly from a corpus. Additionally, we introduce hierarchical FM-Index constraints and look-ahead constraint decoding strategies to reduce irrelevant decoding space and enhance evidence accuracy. Experimental results show that RetroLLM performs exceptionally well on five open-domain question-answering datasets. The code is available at https://github.com/sunnynexus/RetroLLM.",
        "update_ts": "2024-12-17 09:11"
    }
}