
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 24 papers. October 15.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">15 октября</span> | <span id="title-articles-count">24 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-14.html">⬅️ <span id="prev-date">14.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-16.html">➡️ <span id="next-date">16.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'};
        let feedDateNext = {'ru': '16.10', 'en': '10/16', 'zh': '10月16日'};
        let feedDatePrev = {'ru': '14.10', 'en': '10/14', 'zh': '10月14日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.12586', 'title': 'Advancing End-to-End Pixel Space Generative Modeling via Self-supervised\n  Pre-training', 'url': 'https://huggingface.co/papers/2510.12586', 'abstract': 'Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our training framework demonstrates strong empirical performance on ImageNet dataset. Specifically, our diffusion model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE), surpassing prior pixel-space methods by a large margin in both generation quality and efficiency while rivaling leading VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our consistency model achieves an impressive FID of 8.82 in a single sampling step, significantly surpassing its latent-space counterpart. To the best of our knowledge, this marks the first successful training of a consistency model directly on high-resolution images without relying on pre-trained VAEs or diffusion models.', 'score': 66, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'bf296825624ed354', 'authors': ['Jiachen Lei', 'Keli Liu', 'Julius Berner', 'Haiming Yu', 'Hongkai Zheng', 'Jiahong Wu', 'Xiangxiang Chu'], 'affiliations': ['AMAP, Alibaba Group', 'Caltech', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2510.12586.jpg', 'data': {'categories': ['#training', '#diffusion', '#cv', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'Двухэтапное обучение закрывает разрыв между пиксельными и латентными генеративными моделями', 'desc': 'Исследователи предложили новый двухэтапный подход для обучения диффузионных моделей и consistency models напрямую в пространстве пикселей. На первом этапе энкодер предобучается извлекать семантику из чистых изображений и выравнивать их вдоль траектории сэмплирования, на втором — энкодер объединяется с декодером для end-to-end обучения полной модели. Метод достигает FID 2.04 на ImageNet-256 для диффузионной модели и впечатляющий FID 8.82 за один шаг для consistency model, что превосходит латентные аналоги. Это первая успешная реализация обучения consistency model на изображениях высокого разрешения без использования предобученных VAE или диффузионных моделей.'}, 'en': {'title': 'Bridging the Gap: Enhanced Pixel-Space Generative Models', 'desc': 'This paper presents a new two-stage training framework aimed at improving pixel-space generative models, which traditionally struggle with performance compared to latent-space models. In the first stage, encoders are pre-trained to understand the semantics of clean images while aligning them with a deterministic sampling path. The second stage involves integrating the encoder with a decoder and fine-tuning the entire model for both diffusion and consistency tasks. The proposed method shows significant improvements in image generation quality and efficiency on the ImageNet dataset, achieving state-of-the-art results without relying on pre-trained models.'}, 'zh': {'title': '缩小像素空间生成模型的性能差距', 'desc': '本文提出了一种新颖的两阶段训练框架，旨在缩小像素空间生成模型与潜在空间模型之间的性能和效率差距。在第一阶段，我们预训练编码器，从干净图像中捕捉有意义的语义，并将其与沿着确定性采样轨迹的点对齐。第二阶段中，我们将编码器与随机初始化的解码器结合，并对整个模型进行端到端的微调，以适应扩散和一致性模型。我们的训练框架在ImageNet数据集上表现出色，尤其是在生成质量和效率方面，超越了之前的像素空间方法。'}}}, {'id': 'https://huggingface.co/papers/2510.09116', 'title': 'DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel\n  Translation', 'url': 'https://huggingface.co/papers/2510.09116', 'abstract': 'A new evaluation framework, DITING, and a reasoning-driven multi-agent evaluation framework, AgentEval, are introduced to assess the quality of web novel translations, revealing that Chinese-trained LLMs outperform larger foreign models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have substantially advanced machine translation (MT), yet their effectiveness in translating web novels remains unclear. Existing benchmarks rely on surface-level metrics that fail to capture the distinctive traits of this genre. To address these gaps, we introduce DITING, the first comprehensive evaluation framework for web novel translation, assessing narrative and cultural fidelity across six dimensions: idiom translation, lexical ambiguity, terminology localization, tense consistency, zero-pronoun resolution, and cultural safety, supported by over 18K expert-annotated Chinese-English sentence pairs. We further propose AgentEval, a reasoning-driven multi-agent evaluation framework that simulates expert deliberation to assess translation quality beyond lexical overlap, achieving the highest correlation with human judgments among seven tested automatic metrics. To enable metric comparison, we develop MetricAlign, a meta-evaluation dataset of 300 sentence pairs annotated with error labels and scalar quality scores. Comprehensive evaluation of fourteen open, closed, and commercial models reveals that Chinese-trained LLMs surpass larger foreign counterparts, and that DeepSeek-V3 delivers the most faithful and stylistically coherent translations. Our work establishes a new paradigm for exploring LLM-based web novel translation and provides public resources to advance future research.', 'score': 49, 'issue_id': 6421, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'a37bd28d84a1c152', 'authors': ['Enze Zhang', 'Jiaying Wang', 'Mengxi Xiao', 'Jifei Liu', 'Ziyan Kuang', 'Rui Dong', 'Eric Dong', 'Sophia Ananiadou', 'Min Peng', 'Qianqian Xie'], 'affiliations': ['Center for Language and Information Research, Wuhan University', 'Jiangxi Normal University', 'Malvern College Chengdu', 'School of Artificial Intelligence, Wuhan University', 'The University of Manchester', 'Yunnan Trrans Technology Co., Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2510.09116.jpg', 'data': {'categories': ['#machine_translation', '#open_source', '#reasoning', '#dataset', '#multilingual', '#benchmark'], 'emoji': '📚', 'ru': {'title': 'Китайские LLM побеждают в переводе веб-романов', 'desc': 'Исследователи представили DITING — первую комплексную систему оценки качества перевода веб-романов, которая анализирует шесть ключевых аспектов: идиомы, лексическую неоднозначность, терминологию, согласованность времён, разрешение нулевых местоимений и культурную безопасность. Для автоматической оценки был разработан AgentEval — мультиагентный фреймворк, имитирующий экспертное обсуждение и показавший наилучшую корреляцию с человеческими оценками среди семи протестированных метрик. Эксперименты на четырнадцати моделях показали, что китайские LLM превосходят более крупные зарубежные модели в переводе веб-романов. Модель DeepSeek-V3 продемонстрировала наиболее точный и стилистически связный перевод в этом жанре.'}, 'en': {'title': 'Revolutionizing Web Novel Translation Evaluation with DITING and AgentEval', 'desc': 'This paper introduces two new frameworks, DITING and AgentEval, to evaluate the quality of web novel translations. DITING focuses on assessing narrative and cultural fidelity through six specific dimensions, using a large dataset of expert-annotated sentence pairs. AgentEval enhances evaluation by simulating expert deliberation, providing a more nuanced assessment of translation quality beyond simple lexical matching. The findings indicate that Chinese-trained large language models (LLMs) outperform larger foreign models in translation tasks, establishing a new standard for evaluating web novel translations.'}, 'zh': {'title': '中文训练模型翻译网络小说更胜一筹', 'desc': '本文介绍了一个新的评估框架DITING和一个基于推理的多智能体评估框架AgentEval，用于评估网络小说翻译的质量。现有的评估方法往往只关注表面指标，无法有效捕捉网络小说的独特特征。DITING框架通过六个维度评估翻译的叙事和文化忠实度，显示出中文训练的大型语言模型在翻译质量上优于更大的外文模型。我们的研究为基于大型语言模型的网络小说翻译探索建立了新的范式，并提供了公共资源以推动未来的研究。'}}}, {'id': 'https://huggingface.co/papers/2510.11693', 'title': 'Scaling Language-Centric Omnimodal Representation Learning', 'url': 'https://huggingface.co/papers/2510.11693', 'abstract': "Recent multimodal embedding approaches leveraging multimodal large language models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising results, yet the underlying reasons behind their superiority remain underexplored. This work argues that a crucial advantage of MLLM-based approaches stems from implicit cross-modal alignment achieved during generative pretraining, where the language decoder learns to exploit multimodal signals within a shared representation space for generating unimodal outputs. Through analysis of anisotropy and kernel similarity structure, we empirically confirm that latent alignment emerges within MLLM representations, allowing CL to serve as a lightweight refinement stage. Leveraging this insight, we propose a Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive experiments across diverse backbones and benchmarks demonstrate its effectiveness, achieving state-of-the-art performance across modalities. Furthermore, we identify a Generation-Representation Scaling Law (GRSL), showing that the representational capabilities gained through contrastive refinement scales positively with the MLLM's generative capabilities. This suggests that improving generative abilities evolves as an effective paradigm for enhancing representation quality. We provide a theoretical explanation of GRSL, which formally links the MLLM's generative quality to the upper bound on its representation performance, and validate it on a challenging, low-resource visual-document retrieval task, showing that continual generative pretraining before CL can further enhance the potential of a model's embedding capabilities. Codes, models, and resources are available at https://github.com/LCO-Embedding/LCO-Embedding.", 'score': 47, 'issue_id': 6422, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '29d98b7946fd646d', 'authors': ['Chenghao Xiao', 'Hou Pong Chan', 'Hao Zhang', 'Weiwen Xu', 'Mahani Aljunied', 'Yu Rong'], 'affiliations': ['DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.11693.jpg', 'data': {'categories': ['#low_resource', '#data', '#benchmark', '#multimodal', '#transfer_learning', '#training', '#alignment'], 'emoji': '🔗', 'ru': {'title': 'Генеративное предобучение как ключ к качественным мультимодальным эмбеддингам', 'desc': 'Исследователи обнаружили, что превосходство мультимодальных эмбеддингов на основе LLM объясняется неявным выравниванием модальностей во время генеративного предобучения, когда языковой декодер учится использовать мультимодальные сигналы в общем пространстве представлений. На основе этого инсайта предложен фреймворк LCO-Emb, где contrastive learning выступает лишь как легковесный этап финальной настройки поверх уже выровненных представлений. Авторы выявили закон масштабирования Generation-Representation Scaling Law (GRSL), показывающий что качество представлений напрямую зависит от генеративных способностей модели. Эксперименты подтверждают state-of-the-art результаты across модальностей и демонстрируют, что улучшение генеративных способностей является эффективной парадигмой для повышения качества эмбеддингов.'}, 'en': {'title': 'Unlocking Multimodal Potential with LCO-Emb', 'desc': 'This paper explores the advantages of multimodal large language models (MLLMs) that use contrastive learning (CL) for better performance in embedding tasks. It highlights that the strength of these models comes from their ability to align different types of data (like text and images) during the initial training phase, which helps them generate more accurate outputs. The authors introduce a new framework called Language-Centric Omnimodal Embedding (LCO-Emb) that builds on these insights and shows improved results across various tasks. Additionally, they present a Generation-Representation Scaling Law (GRSL) that connects the generative capabilities of MLLMs to their representation quality, suggesting that enhancing generative skills can lead to better embeddings.'}, 'zh': {'title': '提升生成能力，增强表示质量的有效途径', 'desc': '本文探讨了多模态大语言模型（MLLM）在对比学习（CL）下的优越性及其原因。研究表明，MLLM的一个重要优势在于生成预训练过程中实现的隐式跨模态对齐，使得语言解码器能够在共享表示空间中利用多模态信号生成单模态输出。我们提出了一种以语言为中心的全模态嵌入框架LCO-Emb，并通过大量实验验证了其在多种基准测试中的有效性，达到了最先进的性能。此外，我们发现生成-表示缩放法则（GRSL），表明通过对比精炼获得的表示能力与MLLM的生成能力呈正相关。'}}}, {'id': 'https://huggingface.co/papers/2510.12747', 'title': 'FlashVSR: Towards Real-Time Diffusion-Based Streaming Video\n  Super-Resolution', 'url': 'https://huggingface.co/papers/2510.12747', 'abstract': 'Diffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical by achieving efficiency, scalability, and real-time performance. To this end, we propose FlashVSR, the first diffusion-based one-step streaming framework towards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408 videos on a single A100 GPU by combining three complementary innovations: (i) a train-friendly three-stage distillation pipeline that enables streaming super-resolution, (ii) locality-constrained sparse attention that cuts redundant computation while bridging the train-test resolution gap, and (iii) a tiny conditional decoder that accelerates reconstruction without sacrificing quality. To support large-scale training, we also construct VSR-120K, a new dataset with 120k videos and 180k images. Extensive experiments show that FlashVSR scales reliably to ultra-high resolutions and achieves state-of-the-art performance with up to 12x speedup over prior one-step diffusion VSR models. We will release the code, pretrained models, and dataset to foster future research in efficient diffusion-based VSR.', 'score': 24, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'a1e0314eef58561f', 'authors': ['Junhao Zhuang', 'Shi Guo', 'Xin Cai', 'Xiaohui Li', 'Yihao Liu', 'Chun Yuan', 'Tianfan Xue'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.12747.jpg', 'data': {'categories': ['#video', '#open_source', '#inference', '#training', '#dataset', '#diffusion'], 'emoji': '⚡', 'ru': {'title': 'Диффузионное видео super-resolution в реальном времени', 'desc': 'FlashVSR — первая диффузионная модель для видео super-resolution в реальном времени, работающая со скоростью 17 FPS на разрешениях вплоть до 768x1408 на одной GPU A100. Авторы используют трёхэтапную дистилляцию для streaming обработки, разреженное внимание с локальными ограничениями и компактный декодер для ускорения работы. Для обучения создан новый датасет VSR-120K с 120 тысячами видео и 180 тысячами изображений. Модель достигает state-of-the-art качества с ускорением в 12 раз по сравнению с предыдущими one-step диффузионными моделями для VSR.'}, 'en': {'title': 'FlashVSR: Real-Time Video Super-Resolution with Diffusion Models', 'desc': 'This paper introduces FlashVSR, a novel framework that enhances video super-resolution (VSR) using diffusion models while addressing challenges like latency and computation. FlashVSR achieves real-time performance by implementing a three-stage distillation pipeline, which allows for efficient streaming of super-resolution. It also utilizes locality-constrained sparse attention to minimize unnecessary computations and improve generalization across different resolutions. The framework is supported by a new large-scale dataset, VSR-120K, and demonstrates significant speed improvements and high-quality results compared to existing models.'}, 'zh': {'title': 'FlashVSR：实时视频超分辨率的新突破', 'desc': '扩散模型在视频修复方面取得了进展，但在实际视频超分辨率（VSR）中应用仍面临挑战。本文提出了FlashVSR，这是第一个基于扩散的一步流媒体框架，旨在实现实时VSR。FlashVSR通过三项创新实现了高效性和可扩展性，能够在单个A100 GPU上以约17帧每秒的速度处理768x1408的视频。我们还构建了一个新的数据集VSR-120K，以支持大规模训练，并展示了FlashVSR在超高分辨率下的可靠扩展性和领先性能。'}}}, {'id': 'https://huggingface.co/papers/2510.11057', 'title': 'Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models', 'url': 'https://huggingface.co/papers/2510.11057', 'abstract': "Diffusion models have achieved remarkable success as generative models. However, even a well-trained model can accumulate errors throughout the generation process. These errors become particularly problematic when arbitrary guidance is applied to steer samples toward desired properties, which often breaks sample fidelity. In this paper, we propose a general solution to address the off-manifold phenomenon observed in diffusion models. Our approach leverages a time predictor to estimate deviations from the desired data manifold at each timestep, identifying that a larger time gap is associated with reduced generation quality. We then design a novel guidance mechanism, `Temporal Alignment Guidance' (TAG), attracting the samples back to the desired manifold at every timestep during generation. Through extensive experiments, we demonstrate that TAG consistently produces samples closely aligned with the desired manifold at each timestep, leading to significant improvements in generation quality across various downstream tasks.", 'score': 22, 'issue_id': 6425, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '5ec8a5cf8c358245', 'authors': ['Youngrok Park', 'Hojung Jung', 'Sangmin Bae', 'Se-Young Yun'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.11057.jpg', 'data': {'categories': ['#training', '#optimization', '#diffusion', '#multimodal', '#cv'], 'emoji': '🎯', 'ru': {'title': 'Временное выравнивание для точной генерации в диффузионных моделях', 'desc': 'Диффузионные модели показывают отличные результаты в генерации, но накапливают ошибки в процессе работы, особенно при использовании guidance для управления свойствами генерируемых объектов. Авторы предлагают решение проблемы отклонения от целевого многообразия данных с помощью предиктора времени, который оценивает степень отклонения на каждом шаге генерации. Разработанный механизм Temporal Alignment Guidance (TAG) возвращает сэмплы на нужное многообразие на каждом временном шаге. Эксперименты показывают, что TAG значительно улучшает качество генерации в различных задачах, удерживая сэмплы близко к целевому распределению данных.'}, 'en': {'title': 'Aligning Generative Samples with Temporal Guidance', 'desc': "This paper addresses the issue of error accumulation in diffusion models during the sample generation process. When arbitrary guidance is applied, it can lead to a loss of fidelity in the generated samples. The authors introduce a method called 'Temporal Alignment Guidance' (TAG), which uses a time predictor to correct deviations from the desired data manifold at each generation step. Their experiments show that TAG significantly enhances the quality of generated samples by ensuring they remain aligned with the target manifold throughout the process."}, 'zh': {'title': '时间对齐引导：提升扩散模型生成质量的关键', 'desc': '扩散模型作为生成模型取得了显著成功，但在生成过程中，即使是经过良好训练的模型也可能积累错误。这些错误在应用任意引导以调整样本属性时尤为严重，常常导致样本的真实性下降。本文提出了一种通用解决方案，旨在解决扩散模型中观察到的离散现象。我们设计了一种新的引导机制——时间对齐引导（TAG），在生成的每个时间步将样本吸引回期望的数据流形，从而显著提高生成质量。'}}}, {'id': 'https://huggingface.co/papers/2510.12798', 'title': 'Detect Anything via Next Point Prediction', 'url': 'https://huggingface.co/papers/2510.12798', 'abstract': "Object detection has long been dominated by traditional coordinate regression-based models, such as YOLO, DETR, and Grounding DINO. Although recent efforts have attempted to leverage MLLMs to tackle this task, they face challenges like low recall rate, duplicate predictions, coordinate misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a 3B-scale MLLM that achieves state-of-the-art object perception performance. On benchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or exceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot setting. This is enabled by three key designs: 1) Task Formulation: we use special tokens to represent quantized coordinates from 0 to 999, reducing the model's learning difficulty and improving token efficiency for coordinate prediction; 2) Data Engines: we construct multiple data engines to generate high-quality grounding, referring, and pointing data, providing semantically rich supervision for training; \\3) Training Pipelines: we employ a two-stage training process, combining supervised fine-tuning on 22 million data with GRPO-based reinforcement post-training. This RL post-training leverages geometry-aware rewards to effectively bridge the discrete-to-continuous coordinate prediction gap, improve box accuracy, and mitigate undesirable behaviors like duplicate predictions that stem from the teacher-guided nature of the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent language understanding enables versatile capabilities such as object referring, pointing, visual prompting, GUI grounding, spatial referring, OCR and key-pointing, all systematically evaluated on dedicated benchmarks. We believe that Rex-Omni paves the way for more versatile and language-aware visual perception systems.", 'score': 21, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'ac3650578301e79e', 'authors': ['Qing Jiang', 'Junan Huo', 'Xingyu Chen', 'Yuda Xiong', 'Zhaoyang Zeng', 'Yihao Chen', 'Tianhe Ren', 'Junzhi Yu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)'], 'pdf_title_img': 'assets/pdf/title_img/2510.12798.jpg', 'data': {'categories': ['#cv', '#multimodal', '#training', '#optimization', '#rl'], 'emoji': '🔍', 'ru': {'title': 'Rex-Omni: новый уровень в обнаружении объектов', 'desc': 'В статье рассматривается новая модель Rex-Omni, которая улучшает задачи обнаружения объектов, используя подходы LLM. Rex-Omni достигает высоких результатов на тестах COCO и LVIS, превосходя традиционные модели на основе регрессии. Это достигается благодаря использованию специальных токенов для представления координат, созданию качественных наборов данных и двухэтапному обучению с использованием RL. Модель также обладает возможностями понимания языка, что позволяет ей выполнять разнообразные задачи, такие как указание объектов и визуальные подсказки.'}, 'en': {'title': 'Rex-Omni: Bridging Language and Vision for Superior Object Detection', 'desc': "This paper introduces Rex-Omni, a 3B-scale Multi-Modal Language Model (MLLM) designed to enhance object detection performance, traditionally dominated by coordinate regression models. Rex-Omni addresses common challenges such as low recall rates and coordinate misalignment by employing innovative strategies like special token representation for coordinates and multiple data engines for high-quality training data. The model achieves state-of-the-art results on benchmarks like COCO and LVIS, even in zero-shot scenarios, by utilizing a two-stage training process that combines supervised fine-tuning with reinforcement learning. Additionally, Rex-Omni's language understanding capabilities allow it to perform various tasks beyond standard detection, including object referring and visual prompting, marking a significant advancement in visual perception systems."}, 'zh': {'title': 'Rex-Omni：开创多模态物体检测新纪元', 'desc': '本文提出了一种新的物体检测模型Rex-Omni，它是一个规模为3B的多模态大语言模型（MLLM），在物体感知性能上达到了最先进的水平。Rex-Omni在COCO和LVIS等基准测试中表现出色，能够在零样本设置下与传统的回归模型相媲美或超越。该模型通过三项关键设计实现了这一目标：使用特殊标记表示量化坐标、构建多个数据引擎生成高质量的标注数据，以及采用两阶段训练流程结合强化学习后训练。Rex-Omni不仅在传统检测任务中表现优异，还具备多种语言理解能力，能够进行物体指向、视觉提示等多种应用。'}}}, {'id': 'https://huggingface.co/papers/2510.12773', 'title': 'Dr.LLM: Dynamic Layer Routing in LLMs', 'url': 'https://huggingface.co/papers/2510.12773', 'abstract': 'Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. We introduce Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights.', 'score': 21, 'issue_id': 6422, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'a2001c3f3c9bbeab', 'authors': ['Ahmed Heakl', 'Martin Gubri', 'Salman Khan', 'Sangdoo Yun', 'Seong Joon Oh'], 'affiliations': ['MBZUAI', 'NAVER AI Lab', 'Paramter Lab', 'Tubingen AI Center', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2510.12773.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#optimization', '#training', '#inference'], 'emoji': '🧭', 'ru': {'title': 'Умная маршрутизация слоёв: LLM учатся пропускать ненужные вычисления', 'desc': 'В статье представлен Dr.LLM — метод динамической маршрутизации слоёв для больших языковых моделей, который решает проблему избыточных вычислений. Вместо обработки каждого токена через все слои трансформера, система использует лёгкие роутеры для каждого слоя, которые решают: пропустить блок, выполнить его или повторить. Роутеры обучаются с явным контролем через Monte Carlo Tree Search (MCTS), находя оптимальные конфигурации слоёв с сохранением точности при ограниченном бюджете вычислений. Метод улучшает точность на логических и математических задачах до +3.4% при экономии в среднем 5 слоёв на пример, и хорошо обобщается на новые задачи без переобучения базовой модели.'}, 'en': {'title': 'Dynamic Layer Routing for Efficient and Accurate LLMs', 'desc': 'The paper presents Dr.LLM, a framework designed to enhance the efficiency of Large Language Models (LLMs) by dynamically routing layers during inference. Instead of processing every token through all layers, Dr.LLM uses lightweight routers that can decide to skip, execute, or repeat layers based on the complexity of the query. This approach is trained with explicit supervision using Monte Carlo Tree Search, allowing it to maintain or improve accuracy while reducing computational costs. The results show that Dr.LLM not only saves resources but also generalizes well to various tasks, outperforming previous methods in both efficiency and accuracy.'}, 'zh': {'title': 'Dr.LLM：高效灵活的动态层路由', 'desc': '大型语言模型（LLMs）在处理每个标记时会经过所有变换器层，这导致在简单查询时计算资源浪费，而在需要更深层次推理的复杂查询时灵活性不足。我们提出了Dr.LLM，这是一种动态层路由框架，可以为预训练模型添加轻量级的每层路由器，决定跳过、执行或重复某个模块。通过使用蒙特卡洛树搜索（MCTS）进行显式监督训练，Dr.LLM能够在计算预算内获得高质量的层配置，从而在保持或提高准确性的同时提高效率。实验结果表明，Dr.LLM在多个任务上显著提高了准确性，同时在计算资源使用上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2510.12399', 'title': 'A Survey of Vibe Coding with Large Language Models', 'url': 'https://huggingface.co/papers/2510.12399', 'abstract': 'The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed "Vibe Coding" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension. Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration. To address this gap, this survey provides the first comprehensive and systematic review of Vibe Coding with large language models, establishing both theoretical foundations and practical frameworks for this transformative development approach. Drawing from systematic analysis of over 1000 research papers, we survey the entire vibe coding ecosystem, examining critical infrastructure components including LLMs for coding, LLM-based coding agent, development environment of coding agent, and feedback mechanisms. We first introduce Vibe Coding as a formal discipline by formalizing it through a Constrained Markov Decision Process that captures the dynamic triadic relationship among human developers, software projects, and coding agents. Building upon this theoretical foundation, we then synthesize existing practices into five distinct development models: Unconstrained Automation, Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and Context-Enhanced Models, thus providing the first comprehensive taxonomy in this domain. Critically, our analysis reveals that successful Vibe Coding depends not merely on agent capabilities but on systematic context engineering, well-established development environments, and human-agent collaborative development models.', 'score': 21, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '45776dc32839b14f', 'authors': ['Yuyao Ge', 'Lingrui Mei', 'Zenghao Duan', 'Tianhao Li', 'Yujia Zheng', 'Yiwei Wang', 'Lexin Wang', 'Jiayu Yao', 'Tianyu Liu', 'Yujun Cai', 'Baolong Bi', 'Fangda Guo', 'Jiafeng Guo', 'Shenghua Liu', 'Xueqi Cheng'], 'affiliations': ['Duke University', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Peking University', 'University of California, Merced', 'University of Queensland'], 'pdf_title_img': 'assets/pdf/title_img/2510.12399.jpg', 'data': {'categories': ['#multimodal', '#training', '#survey', '#agi', '#agents', '#rl'], 'emoji': '🎵', 'ru': {'title': 'Vibe Coding: когда разработчик проверяет результат, а не читает код', 'desc': 'Статья исследует новую парадигму разработки под названием "Vibe Coding", где разработчики проверяют работоспособность кода, сгенерированного AI, через наблюдение за результатами, а не через построчное чтение. Авторы формализуют этот подход через Constrained Markov Decision Process, описывающий взаимодействие между человеком, проектом и coding-агентом на основе LLM. На основе анализа более 1000 научных работ выделены пять моделей разработки: от полной автоматизации до контекстно-обогащённых подходов. Исследование показывает, что успех Vibe Coding зависит не только от возможностей агента, но и от правильной организации контекста, окружения разработки и моделей человеко-агентной коллаборации.'}, 'en': {'title': 'Empowering Developers with Vibe Coding: A New Era of AI Collaboration', 'desc': "This paper explores a new approach to software development called 'Vibe Coding', which leverages large language models (LLMs) to assist in coding tasks. Instead of developers understanding every line of code, they validate AI-generated code by observing the outcomes it produces. The study reviews over 1000 research papers to create a comprehensive framework for Vibe Coding, identifying key components like coding agents and feedback mechanisms. It also introduces a formal model to describe the interactions between developers and AI, highlighting that successful implementation relies on effective collaboration and context management."}, 'zh': {'title': 'Vibe Coding：人机协作的新篇章', 'desc': '大型语言模型（LLMs）的进步促使了从代码生成辅助到自主编码代理的转变，形成了一种新的开发方法论，称为“Vibe Coding”。尽管这一新兴范式具有变革潜力，但其有效性仍未得到充分探索，实证研究显示出意想不到的生产力损失和人机协作的基本挑战。本文提供了对Vibe Coding的首次全面系统评审，建立了理论基础和实践框架，并分析了超过1000篇研究论文，探讨了Vibe Coding生态系统的关键基础设施组件。我们的研究表明，成功的Vibe Coding不仅依赖于代理的能力，还需要系统的上下文工程、良好的开发环境和人机协作开发模型。'}}}, {'id': 'https://huggingface.co/papers/2510.12276', 'title': 'Spatial Forcing: Implicit Spatial Representation Alignment for\n  Vision-language-action Model', 'url': 'https://huggingface.co/papers/2510.12276', 'abstract': 'Vision-language-action (VLA) models have recently shown strong potential in enabling robots to follow language instructions and execute precise actions. However, most VLAs are built upon vision-language models pretrained solely on 2D data, which lack accurate spatial awareness and hinder their ability to operate in the 3D physical world. Existing solutions attempt to incorporate explicit 3D sensor inputs such as depth maps or point clouds, but these approaches face challenges due to sensor noise, hardware heterogeneity, and incomplete depth coverage in existing datasets. Alternative methods that estimate 3D cues from 2D images also suffer from the limited performance of depth estimators.We propose Spatial Forcing (SF), a simple yet effective alignment strategy that implicitly forces VLA models to develop spatial comprehension capabilities without relying on explicit 3D inputs or depth estimators. SF aligns intermediate visual embeddings of VLAs with geometric representations produced by pretrained 3D foundation models. By enforcing alignment at intermediate layers, SF guides VLAs to encode richer spatial representations that enhance action precision.Extensive experiments in simulation and real-world environments demonstrate that SF achieves state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further accelerates training by up to 3.8x and improves data efficiency across diverse robotic tasks. Project page is at https://spatial-forcing.github.io/', 'score': 18, 'issue_id': 6422, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '93d66257d7564b11', 'authors': ['Fuhao Li', 'Wenxuan Song', 'Han Zhao', 'Jingbo Wang', 'Pengxiang Ding', 'Donglin Wang', 'Long Zeng', 'Haoang Li'], 'affiliations': ['South China University of Technology', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Tsinghua University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.12276.jpg', 'data': {'categories': ['#3d', '#optimization', '#training', '#agents', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'Обучение роботов пространственному мышлению без 3D сенсоров', 'desc': 'Статья представляет Spatial Forcing (SF) — метод для улучшения vision-language-action (VLA) моделей в робототехнике. Проблема существующих VLA в том, что они обучены на 2D данных и плохо понимают пространство, а явное использование 3D сенсоров вносит шум и зависит от оборудования. SF решает это через выравнивание промежуточных визуальных эмбеддингов VLA с геометрическими представлениями от предобученных 3D foundation моделей, неявно обучая пространственному пониманию. Метод показывает state-of-the-art результаты, ускоряет обучение в 3.8 раза и улучшает эффективность использования данных в роботизированных задачах.'}, 'en': {'title': 'Enhancing Spatial Awareness in Robots with Spatial Forcing', 'desc': 'This paper introduces Spatial Forcing (SF), a novel alignment strategy for vision-language-action (VLA) models that enhances their spatial awareness without the need for explicit 3D inputs. By aligning intermediate visual embeddings with geometric representations from pretrained 3D models, SF helps VLAs develop better spatial comprehension. The approach addresses limitations of existing methods that rely on noisy 3D sensor data or depth estimators, which often struggle with accuracy. Experimental results show that SF not only improves action precision but also accelerates training and increases data efficiency in various robotic tasks.'}, 'zh': {'title': '空间强制：提升机器人空间理解的创新方法', 'desc': '本文提出了一种名为空间强制（Spatial Forcing, SF）的新方法，旨在提高视觉-语言-动作（VLA）模型的空间理解能力。传统的VLA模型依赖于仅在2D数据上预训练的视觉-语言模型，缺乏对3D物理世界的准确空间感知。SF通过将VLA模型的中间视觉嵌入与预训练的3D基础模型生成的几何表示进行对齐，来增强模型的空间表示能力。实验结果表明，SF在多种机器人任务中表现出色，训练速度提高了3.8倍，并且在数据效率上也有显著改善。'}}}, {'id': 'https://huggingface.co/papers/2510.12693', 'title': 'ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning\n  and Online Reinforcement Learning', 'url': 'https://huggingface.co/papers/2510.12693', 'abstract': 'Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary knowledge and skills to succeed. To bridge this gap, we present Embodied Reasoning Agent (ERA), a two-stage framework that integrates prior knowledge learning and online reinforcement learning (RL). The first stage, Embodied Prior Learning, distills foundational knowledge from three types of data: (1) Trajectory-Augmented Priors, which enrich existing trajectory data with structured reasoning generated by stronger models; (2) Environment-Anchored Priors, which provide in-environment knowledge and grounding supervision; and (3) External Knowledge Priors, which transfer general knowledge from out-of-environment datasets. In the second stage, we develop an online RL pipeline that builds on these priors to further enhance agent performance. To overcome the inherent challenges in agent RL, including long horizons, sparse rewards, and training instability, we introduce three key designs: self-summarization for context management, dense reward shaping, and turn-level policy optimization. Extensive experiments on both high-level planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate that ERA-3B surpasses both prompting-based large models and previous training-based baselines. Specifically, it achieves overall improvements of 8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits strong generalization to unseen tasks. Overall, ERA offers a practical path toward scalable embodied intelligence, providing methodological insights for future embodied AI systems.', 'score': 16, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'becfc2b44cce8996', 'authors': ['Hanyang Chen', 'Mark Zhao', 'Rui Yang', 'Qinwei Ma', 'Ke Yang', 'Jiarui Yao', 'Kangrui Wang', 'Hao Bai', 'Zhenhailong Wang', 'Rui Pan', 'Mengchao Zhang', 'Jose Barreiros', 'Aykut Onol', 'ChengXiang Zhai', 'Heng Ji', 'Manling Li', 'Huan Zhang', 'Tong Zhang'], 'affiliations': ['Northwestern University', 'Toyota Research Institute', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2510.12693.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#transfer_learning', '#agents', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Маленькие модели учатся действовать как большие', 'desc': 'Статья представляет ERA — двухэтапный фреймворк для обучения компактных vision language models действовать в физических средах. На первом этапе модель получает базовые знания из трёх источников: обогащённых траекторий с рассуждениями от более сильных моделей, знаний о конкретной среде и внешних датасетов. На втором этапе применяется онлайн reinforcement learning с самосуммаризацией для управления контекстом, плотным формированием наград и оптимизацией на уровне отдельных действий. Компактная модель ERA-3B превосходит GPT-4o на 8.4% в задачах планирования и на 19.4% в задачах манипуляции объектами.'}, 'en': {'title': 'Empowering VLMs with ERA: Bridging Knowledge and Action', 'desc': "This paper introduces the Embodied Reasoning Agent (ERA), a two-stage framework designed to enhance the capabilities of vision language models (VLMs) in complex environments. The first stage, Embodied Prior Learning, gathers foundational knowledge from various data sources to improve the model's understanding and reasoning. The second stage employs online reinforcement learning (RL) to refine the agent's performance using the acquired knowledge. The proposed methods, including self-summarization and dense reward shaping, address common challenges in RL, leading to significant performance improvements in both planning and control tasks compared to existing models."}, 'zh': {'title': '具身智能的实用路径', 'desc': '本论文介绍了一种名为“具身推理代理”（ERA）的新框架，旨在提升视觉语言模型（VLMs）在复杂环境中的表现。ERA通过两个阶段的学习，首先从多种数据中提取基础知识，然后利用在线强化学习进一步提升代理的性能。该框架解决了代理强化学习中的挑战，如长时间跨度、稀疏奖励和训练不稳定性。实验结果表明，ERA在高层规划和低层控制任务上均优于现有的大型模型和训练基线，展示了其在具身智能领域的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.12784', 'title': 'SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2510.12784', 'abstract': "Recently, remarkable progress has been made in Unified Multimodal Models (UMMs), which integrate vision-language generation and understanding capabilities within a single framework. However, a significant gap exists where a model's strong visual understanding often fails to transfer to its visual generation. A model might correctly understand an image based on user instructions, yet be unable to generate a faithful image from text prompts. This phenomenon directly raises a compelling question: Can a model achieve self-improvement by using its understanding module to reward its generation module? To bridge this gap and achieve self-improvement, we introduce SRUM, a self-rewarding post-training framework that can be directly applied to existing UMMs of various designs. SRUM creates a feedback loop where the model's own understanding module acts as an internal ``evaluator'', providing corrective signals to improve its generation module, without requiring additional human-labeled data. To ensure this feedback is comprehensive, we designed a global-local dual reward system. To tackle the inherent structural complexity of images, this system offers multi-scale guidance: a global reward ensures the correctness of the overall visual semantics and layout, while a local reward refines fine-grained, object-level fidelity. SRUM leads to powerful capabilities and shows strong generalization, boosting performance on T2I-CompBench from 82.18 to 88.37 and on T2I-ReasonBench from 43.82 to 46.75. Overall, our work establishes a powerful new paradigm for enabling a UMMs' understanding module to guide and enhance its own generation via self-rewarding.", 'score': 11, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '97647c923a10f143', 'authors': ['Weiyang Jin', 'Yuwei Niu', 'Jiaqi Liao', 'Chengqi Duan', 'Aoxue Li', 'Shenghua Gao', 'Xihui Liu'], 'affiliations': ['Noahs Ark Lab, Huawei'], 'pdf_title_img': 'assets/pdf/title_img/2510.12784.jpg', 'data': {'categories': ['#training', '#optimization', '#multimodal', '#transfer_learning'], 'emoji': '🔄', 'ru': {'title': 'Самообучение мультимодальных моделей через внутреннюю самооценку', 'desc': 'Исследователи обнаружили парадокс: мультимодальные модели могут хорошо понимать изображения, но плохо их генерировать по текстовым описаниям. Предложен метод SRUM, где модуль понимания изображений выступает внутренним оценщиком для улучшения модуля генерации без дополнительной человеческой разметки. Система использует двухуровневую систему наград: глобальную для общей семантики и композиции, и локальную для детализации объектов. Эксперименты показали значительное улучшение качества генерации изображений на популярных бенчмарках.'}, 'en': {'title': 'Empowering Models to Self-Improve: SRUM Framework for UMMs', 'desc': "This paper presents SRUM, a self-rewarding framework designed to improve Unified Multimodal Models (UMMs) by enhancing the relationship between visual understanding and visual generation. The framework allows the model's understanding module to evaluate and provide feedback to its generation module, creating a self-improvement loop without needing extra human-labeled data. SRUM employs a dual reward system that offers both global and local feedback, ensuring that the generated images are semantically correct and detailed. The results show significant performance improvements on benchmark tasks, demonstrating the effectiveness of this self-guided approach in multimodal learning."}, 'zh': {'title': '自我奖励，提升生成能力！', 'desc': '最近，统一多模态模型（UMMs）在视觉-语言生成和理解能力方面取得了显著进展。然而，模型的强视觉理解往往无法有效转移到视觉生成上。为了解决这个问题，我们提出了SRUM，一个自我奖励的后训练框架，可以直接应用于各种设计的UMMs。SRUM通过创建反馈循环，使模型的理解模块作为内部“评估者”，为生成模块提供纠正信号，从而实现自我改进。'}}}, {'id': 'https://huggingface.co/papers/2510.12635', 'title': 'Memory as Action: Autonomous Context Curation for Long-Horizon Agentic\n  Tasks', 'url': 'https://huggingface.co/papers/2510.12635', 'abstract': "Large Language Models face challenges in long-horizon agentic tasks as their constrained memory is easily overwhelmed by distracting or irrelevant context. Existing working memory methods typically rely on external, heuristic mechanisms that are decoupled from the agent's core policy. In this work, we reframe working memory management as a learnable, intrinsic capability. We propose a novel framework, Memory-as-Action, where an agent actively manages its working memory by executing explicit editing operations as part of a unified policy. This formulation allows an agent, trained via reinforcement learning, to balance memory curation against long-term task objectives under given resource constraints. However, such memory editing actions break the standard assumption of a continuously growing prefix in LLM interactions, leading to what we call trajectory fractures. These non-prefix changes disrupt the causal continuity required by standard policy gradient methods, making those methods inapplicable. To address this, we propose a new algorithm, Dynamic Context Policy Optimization, which enables stable end-to-end reinforcement learning by segmenting trajectories at memory action points and applying trajectory-level advantages to the resulting action segments. Our results demonstrate that jointly optimizing for task reasoning and memory management in an end-to-end fashion not only reduces overall computational consumption but also improves task performance, driven by adaptive context curation strategies tailored to the model's intrinsic capabilities.", 'score': 11, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '30ef35f2c0624828', 'authors': ['Yuxiang Zhang', 'Jiangming Shu', 'Ye Ma', 'Xueyuan Lin', 'Shangxi Wu', 'Jitao Sang'], 'affiliations': ['Hithink Research', 'Huawei Noahs Ark Lab', 'School of Computer Science and Technology, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.12635.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#training', '#optimization', '#agents', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Память как действие: LLM учатся управлять своим контекстом', 'desc': 'Большие языковые модели сталкиваются с проблемой ограниченной памяти при решении длинных задач, когда контекст переполняется ненужной информацией. Исследователи предлагают подход Memory-as-Action, где агент активно управляет своей рабочей памятью через явные операции редактирования как часть единой политики, обученной с помощью reinforcement learning. Для обучения разработан новый алгоритм Dynamic Context Policy Optimization, который решает проблему "разрывов траектории", возникающих при редактировании контекста. Результаты показывают, что совместная оптимизация рассуждений и управления памятью снижает вычислительные затраты и улучшает качество решения задач.'}, 'en': {'title': 'Empowering LLMs with Learnable Memory Management', 'desc': 'This paper addresses the limitations of Large Language Models (LLMs) in handling long-term tasks due to their limited memory capacity. It introduces a new approach called Memory-as-Action, where the model learns to manage its working memory through specific editing actions integrated into its decision-making policy. The authors propose a novel algorithm, Dynamic Context Policy Optimization, to overcome challenges posed by non-continuous memory updates, which disrupt standard reinforcement learning methods. The findings show that optimizing memory management alongside task reasoning enhances performance and reduces computational costs.'}, 'zh': {'title': '记忆管理与任务推理的统一优化', 'desc': '本论文探讨了大型语言模型在处理长期任务时面临的挑战，尤其是由于内存限制而导致的干扰问题。我们提出了一种新的框架，称为“记忆作为行动”，使代理能够通过执行显式编辑操作来主动管理其工作记忆。通过强化学习训练，代理可以在资源限制下平衡记忆管理与长期任务目标。我们还提出了一种新算法，动态上下文策略优化，解决了传统策略梯度方法在处理非前缀变化时的局限性，从而实现稳定的端到端强化学习。'}}}, {'id': 'https://huggingface.co/papers/2510.11683', 'title': 'Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion\n  Large Language Models', 'url': 'https://huggingface.co/papers/2510.11683', 'abstract': 'Boundary-Guided Policy Optimization (BGPO) improves reinforcement learning for diffusion large language models by efficiently approximating likelihoods with a memory-efficient lower bound, enhancing performance in tasks like math problem solving, code generation, and planning.  \t\t\t\t\tAI-generated summary \t\t\t\t A key challenge in applying reinforcement learning (RL) to diffusion large language models (dLLMs) lies in the intractability of their likelihood functions, which are essential for the RL objective, necessitating corresponding approximation in each training step. While existing methods approximate the log-likelihoods by their evidence lower bounds (ELBOs) via customized Monte Carlo (MC) sampling, the forward computational graphs of all MC samples need to be retained for the gradient computation of non-linear terms in the RL objective, resulting in significant memory overhead. This constraint restricts feasible sample sizes, leading to imprecise likelihood approximations and ultimately distorting the RL objective. To overcome this limitation, we propose Boundary-Guided Policy Optimization (BGPO), a memory-efficient RL algorithm that maximizes a specially constructed lower bound of the ELBO-based objective. This lower bound is carefully designed to satisfy two key properties: (1) Linearity: it is formulated in a linear sum where each term depends only on a single MC sample, thereby enabling gradient accumulation across samples and ensuring constant memory usage; (2) Equivalence: Both the value and gradient of this lower bound are equal to those of the ELBO-based objective in on-policy training, making it also an effective approximation for the original RL objective. These properties allow BGPO to adopt a large MC sample size, resulting in more accurate likelihood approximations and improved RL objective estimation, which in turn leads to enhanced performance. Experiments show that BGPO significantly outperforms previous RL algorithms for dLLMs in math problem solving, code generation, and planning tasks.', 'score': 11, 'issue_id': 6421, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '4eb85eafb417ade6', 'authors': ['Nianyi Lin', 'Jiajie Zhang', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.11683.jpg', 'data': {'categories': ['#games', '#rlhf', '#training', '#optimization', '#rl'], 'emoji': '🎯', 'ru': {'title': 'Эффективное обучение диффузионных языковых моделей с помощью линейной аппроксимации', 'desc': 'Статья представляет метод BGPO для reinforcement learning в диффузионных LLM, которые сталкиваются с проблемой вычисления функций правдоподобия. Существующие методы используют Monte Carlo сэмплирование для аппроксимации, но это требует огромного количества памяти для хранения градиентов всех сэмплов. BGPO решает эту проблему через специальную линейную нижнюю границу ELBO, которая позволяет накапливать градиенты последовательно и использовать больше сэмплов при константном потреблении памяти. Эксперименты показывают значительное улучшение производительности в задачах решения математических задач, генерации кода и планирования по сравнению с предыдущими RL-алгоритмами.'}, 'en': {'title': 'Efficient Reinforcement Learning for Language Models with BGPO', 'desc': 'Boundary-Guided Policy Optimization (BGPO) is a novel reinforcement learning algorithm designed to enhance the performance of diffusion large language models (dLLMs) by efficiently approximating likelihoods. It addresses the challenge of high memory usage in existing methods that rely on Monte Carlo sampling for estimating log-likelihoods, which can distort the RL objective. BGPO introduces a memory-efficient lower bound that maintains linearity and equivalence with the original objective, allowing for larger sample sizes and more accurate likelihood approximations. As a result, BGPO demonstrates significant improvements in tasks such as math problem solving, code generation, and planning compared to previous RL approaches.'}, 'zh': {'title': '边界引导策略优化：提升强化学习性能的利器', 'desc': '边界引导策略优化（BGPO）是一种改进强化学习的方法，专门用于扩散大语言模型（dLLMs）。它通过高效地近似似然函数的下界，解决了在训练过程中内存开销过大的问题。BGPO的设计确保了线性和等价性，使得在每个训练步骤中可以使用较大的样本量，从而提高了似然近似的准确性。实验结果表明，BGPO在数学问题解决、代码生成和规划任务中显著优于之前的强化学习算法。'}}}, {'id': 'https://huggingface.co/papers/2510.12789', 'title': 'UniFusion: Vision-Language Model as Unified Encoder in Image Generation', 'url': 'https://huggingface.co/papers/2510.12789', 'abstract': "Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large unified models jointly for text and image generation, which demands substantial computational resources and large-scale data, limiting its accessibility.We present UniFusion, a diffusion-based generative model conditioned on a frozen large vision-language model (VLM) that serves as a unified multimodal encoder. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high level semantics and low level details from text and visual tokens of a frozen VLM to condition a diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithful transfer of visual information from VLM to the diffusion model which is key for editing. We propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI), which conditions a diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. VERIFI combines the alignment of the conditioning distribution with the VLM's reasoning capabilities for increased capabilities and flexibility at inference. In addition, finetuning on editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities. Our model when trained on single image editing, zero-shot generalizes to multiple image references further motivating the unified encoder design of UniFusion.", 'score': 10, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '68fefbfc1c762f27', 'authors': ['Kevin Li', 'Manuel Brack', 'Sudeep Katakol', 'Hareesh Ravi', 'Ajinkya Kale'], 'affiliations': ['Adobe Applied Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.12789.jpg', 'data': {'categories': ['#cv', '#multimodal', '#reasoning', '#training', '#transfer_learning', '#diffusion'], 'emoji': '🔗', 'ru': {'title': 'Единый энкодер для текста и изображений в диффузионных моделях', 'desc': 'UniFusion — это диффузионная модель для генерации изображений, которая использует замороженную vision-language модель (VLM) в качестве единого мультимодального энкодера вместо раздельных энкодеров для текста и изображений. Ключевой механизм Layerwise Attention Pooling (LAP) извлекает семантику высокого уровня и детали низкого уровня из токенов VLM для управления генерацией. Метод VERIFI использует текстовые токены, сгенерированные VLM при переписывании промптов внутри модели, что усиливает reasoning-способности и гибкость. Модель демонstrирует отличный transfer знаний между модальностями и zero-shot обобщается с редактирования одного изображения на работу с несколькими референсами.'}, 'en': {'title': 'Unified Multimodal Generation with UniFusion', 'desc': "The paper introduces UniFusion, a novel diffusion-based generative model that utilizes a frozen large vision-language model (VLM) as a unified multimodal encoder. This approach addresses the limitations of existing architectures that rely on separate encoders for images and text, which restricts their ability to perform cross-modal reasoning. The core innovation is the Layerwise Attention Pooling (LAP) mechanism, which effectively extracts both high-level semantics and low-level details from the VLM, enhancing text-image alignment and enabling better visual information transfer. Additionally, the proposed VLM-Enabled Rewriting Injection with Flexible Inference (VERIFI) improves the model's flexibility and generalization capabilities, allowing it to perform well on various editing tasks with minimal additional training."}, 'zh': {'title': '统一编码器，跨模态生成的未来', 'desc': '尽管视觉生成的最新进展显著，但大多数现有架构仍依赖于独立的图像和文本编码器。这种分离限制了扩散模型在跨模态推理和知识转移方面的能力。我们提出的UniFusion是一种基于扩散的生成模型，利用冻结的大型视觉语言模型（VLM）作为统一的多模态编码器。通过层级注意力池化机制（LAP），UniFusion能够提取文本和视觉标记的高层语义和低层细节，从而在生成和编辑任务中实现更好的文本-图像对齐和信息转移。'}}}, {'id': 'https://huggingface.co/papers/2510.12709', 'title': 'SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model', 'url': 'https://huggingface.co/papers/2510.12709', 'abstract': "Multimodal embedding models aim to yield informative unified representations that empower diverse cross-modal tasks. Despite promising developments in the evolution from CLIP-based dual-tower architectures to large vision-language models, prior works still face unavoidable challenges in real-world applications and business scenarios, such as the limited modality support, unstable training mechanisms, and industrial domain gaps. In this work, we introduce SAIL-Embedding, an omni-modal embedding foundation model that addresses these issues through tailored training strategies and architectural design. In the optimization procedure, we propose a multi-stage training scheme to boost the multifaceted effectiveness of representation learning. Specifically, the content-aware progressive training aims to enhance the model's adaptability to diverse downstream tasks and master enriched cross-modal proficiency. The collaboration-aware recommendation enhancement training further adapts multimodal representations for recommendation scenarios by distilling knowledge from sequence-to-item and ID-to-item embeddings while mining user historical interests. Concurrently, we develop the stochastic specialization and dataset-driven pattern matching to strengthen model training flexibility and generalizability. Experimental results show that SAIL-Embedding achieves SOTA performance compared to other methods in different retrieval tasks. In online experiments across various real-world scenarios integrated with our model, we observe a significant increase in Lifetime (LT), which is a crucial indicator for the recommendation experience. For instance, the model delivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the Douyin-Selected scenario. For the Douyin feed rank model, the match features produced by SAIL-Embedding yield a +0.08% AUC gain.", 'score': 7, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'ea07490eae7bd8d4', 'authors': ['Lin Lin', 'Jiefeng Long', 'Zhihe Wan', 'Yuchi Wang', 'Dingkang Yang', 'Shuang Yang', 'Yueyang Yao', 'Xu Chen', 'Zirui Guo', 'Shengqiang Li', 'Weiran Li', 'Hanyu Li', 'Yaling Mou', 'Yan Qiu', 'Haiyang Yu', 'Xiao Liang', 'Hongsheng Li', 'Chao Feng'], 'affiliations': ['ByteDance', 'CUHK MMLab'], 'pdf_title_img': 'assets/pdf/title_img/2510.12709.jpg', 'data': {'categories': ['#training', '#optimization', '#multimodal'], 'emoji': '⛵', 'ru': {'title': 'SAIL-Embedding: Универсальная мультимодальная модель для поиска и рекомендаций', 'desc': 'Статья представляет SAIL-Embedding — омнимодальную модель эмбеддингов, которая решает проблемы ограниченной поддержки модальностей и нестабильного обучения в реальных бизнес-сценариях. Авторы предлагают многоэтапную схему обучения: content-aware прогрессивный тренинг для адаптации к разным задачам и collaboration-aware тренинг для рекомендательных систем с дистилляцией знаний из последовательностей и ID-эмбеддингов. Модель использует стохастическую специализацию и паттерн-матчинг для повышения гибкости и обобщающей способности. В онлайн-экспериментах на платформе Douyin модель показывает значительный прирост метрики Lifetime (+0.158% за 7 дней) и улучшение AUC (+0.08%) в ранжирующих моделях.'}, 'en': {'title': 'SAIL-Embedding: Bridging Modalities for Enhanced Learning and Recommendations', 'desc': 'This paper presents SAIL-Embedding, a new omni-modal embedding model designed to improve cross-modal tasks in machine learning. It addresses challenges like limited modality support and unstable training by using a multi-stage training scheme that enhances representation learning. The model incorporates content-aware and collaboration-aware training strategies to adapt to various tasks and improve recommendation systems. Experimental results demonstrate that SAIL-Embedding outperforms existing methods, showing significant improvements in user engagement metrics in real-world applications.'}, 'zh': {'title': 'SAIL-Embedding：全模态嵌入的未来', 'desc': '多模态嵌入模型旨在生成信息丰富的统一表示，以支持多样的跨模态任务。尽管从CLIP基础的双塔架构到大型视觉-语言模型的演变取得了良好进展，但在实际应用中仍面临诸多挑战，如有限的模态支持和不稳定的训练机制。本文提出了SAIL-Embedding，这是一种全模态嵌入基础模型，通过定制的训练策略和架构设计来解决这些问题。实验结果表明，SAIL-Embedding在不同检索任务中实现了最先进的性能，并在实际场景中显著提高了推荐体验。'}}}, {'id': 'https://huggingface.co/papers/2510.12801', 'title': 'DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search', 'url': 'https://huggingface.co/papers/2510.12801', 'abstract': 'Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on a two-stage training pipeline: a cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across a range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search.', 'score': 4, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '78d4634c4d91c679', 'authors': ['Kartik Narayan', 'Yang Xu', 'Tian Cao', 'Kavya Nerella', 'Vishal M. Patel', 'Navid Shiee', 'Peter Grasch', 'Chao Jia', 'Yinfei Yang', 'Zhe Gan'], 'affiliations': ['Apple', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2510.12801.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#training', '#dataset', '#agi', '#optimization', '#rag', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Мультимодальный поиск в интернете с обучением через подкрепление', 'desc': 'Статья представляет DeepMMSearch-R1 — первую мультимодальную LLM, способную самостоятельно выполнять многоэтапный веб-поиск по запросу, используя как текстовые, так и визуальные инструменты. Модель может инициировать поиск на основе фрагментов входного изображения и итеративно адаптировать текстовые запросы, что позволяет ей самостоятельно корректировать свои действия. Для обучения используется двухэтапный подход: supervised fine-tuning с последующей оптимизацией через reinforcement learning на специально созданном датасете DeepMMSearchVQA с мультимодальными вопросами, требующими поиска информации. Эксперименты показывают превосходство подхода над существующими RAG-методами и поисковыми агентами на задачах, требующих внешних знаний.'}, 'en': {'title': 'Revolutionizing Web Search with Dynamic Multimodal Queries', 'desc': 'This paper introduces DeepMMSearch-R1, a novel multimodal large language model (MLLM) designed to enhance web search capabilities for both text and images. Unlike traditional methods that rely on fixed pipelines, DeepMMSearch-R1 can perform on-demand, multi-turn searches and dynamically generate queries based on the input data. The model is trained using a two-stage process, starting with supervised finetuning followed by reinforcement learning, and utilizes a new dataset called DeepMMSearchVQA for training. The results show that this approach significantly improves the efficiency and effectiveness of information retrieval in real-world applications.'}, 'zh': {'title': '动态多模态搜索，提升信息获取效率', 'desc': '本论文介绍了一种名为DeepMMSearch-R1的多模态大型语言模型（MLLM），它能够进行按需的多轮网络搜索，并动态生成图像和文本搜索查询。该模型通过对输入图像的相关部分进行搜索，提高了图像搜索的有效性，并能够根据检索到的信息迭代调整文本搜索查询，实现自我反思和自我纠正。我们采用了两阶段的训练流程，包括冷启动的监督微调和在线强化学习优化，并引入了DeepMMSearchVQA数据集，以支持多模态视觉问答（VQA）任务。通过广泛的实验，我们证明了该方法在知识密集型基准测试中的优越性，推动了多模态网络搜索的发展。'}}}, {'id': 'https://huggingface.co/papers/2510.12225', 'title': 'HoneyBee: Data Recipes for Vision-Language Reasoners', 'url': 'https://huggingface.co/papers/2510.12225', 'abstract': 'Recent advances in vision-language models (VLMs) have made them highly effective at reasoning tasks. However, the principles underlying the construction of performant VL reasoning training datasets remain poorly understood. In this work, we introduce several data curation approaches and study their impacts on VL reasoning capabilities by carefully controlling training and evaluation setups. We analyze the effects of context (image and question pair) sources, implement targeted data interventions, and explore scaling up images, questions, and chain-of-thought (CoT) solutions. Our findings reveal that (a) context source strategies significantly affect VLM performance, (b) interventions such as auxiliary signals from image captions and the inclusion of text-only reasoning yield substantial gains, and (c) scaling all data dimensions (e.g., unique questions per image and unique CoTs per image-question pair) consistently improves reasoning capability. Motivated by these insights, we introduce HoneyBee, a large-scale, high-quality CoT reasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs trained with HoneyBee outperform state-of-the-art models across model sizes. For instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA model and the base model by 7.8% and 24.8%, respectively, on MathVerse. Furthermore, we propose a test-time scaling strategy that reduces decoding cost by 73% without sacrificing accuracy. Overall, this work presents improved strategies for VL reasoning dataset curation research.', 'score': 4, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'a1704fba84103a6e', 'authors': ['Hritik Bansal', 'Devandra Singh Sachan', 'Kai-Wei Chang', 'Aditya Grover', 'Gargi Ghosh', 'Wen-tau Yih', 'Ramakanth Pasunuru'], 'affiliations': ['FAIR at Meta', 'University of California Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2510.12225.jpg', 'data': {'categories': ['#cv', '#multimodal', '#reasoning', '#dataset', '#benchmark', '#data'], 'emoji': '🐝', 'ru': {'title': 'HoneyBee: Как правильно готовить данные для обучения визуального мышления', 'desc': 'Исследователи изучили принципы создания эффективных датасетов для обучения vision-language моделей логическому мышлению. Они обнаружили, что источник контекста (пары изображение-вопрос), добавление вспомогательных сигналов вроде описаний изображений и текстовых рассуждений, а также масштабирование всех измерений данных значительно улучшают способности к рассуждениям. На основе этих находок был создан HoneyBee — высококачественный датасет с 2.5 миллионами примеров chain-of-thought рассуждений на 350 тысячах пар изображение-вопрос. Модели, обученные на HoneyBee, превосходят state-of-the-art решения: например, 3B-параметровая модель показала улучшение на 7.8% по сравнению с лучшим аналогом и на 24.8% по сравнению с базовой моделью на бенчмарке MathVerse.'}, 'en': {'title': 'Enhancing VLM Reasoning with HoneyBee Dataset', 'desc': 'This paper explores how to create better training datasets for vision-language models (VLMs) to enhance their reasoning abilities. The authors introduce various data curation methods and analyze their effects on VLM performance, focusing on context sources and targeted data interventions. They find that using auxiliary signals and scaling data dimensions significantly boosts reasoning capabilities. The study culminates in the creation of HoneyBee, a large dataset that improves VLM performance, demonstrating substantial gains over existing models.'}, 'zh': {'title': '提升视觉-语言模型推理能力的新策略', 'desc': '最近在视觉-语言模型（VLMs）方面的进展使其在推理任务中表现出色。然而，构建高效VLM推理训练数据集的原则仍然不够清晰。本文介绍了几种数据整理方法，并研究了它们对VLM推理能力的影响，分析了上下文来源、数据干预和数据规模的效果。我们的研究表明，上下文来源策略显著影响VLM性能，数据干预和数据规模的增加均能有效提升推理能力。'}}}, {'id': 'https://huggingface.co/papers/2510.11919', 'title': 'LLM Reasoning for Machine Translation: Synthetic Data Generation over\n  Thinking Tokens', 'url': 'https://huggingface.co/papers/2510.11919', 'abstract': 'Large reasoning models (LRMs) have led to new possibilities in terms of problem-solving, through the devising of a natural language thought process prior to answering a query. While their capabilities are well known across mathematics and coding tasks, their impact on the task of machine translation (MT) remains underexplored. In this work, we explore the benefits of the generation of intermediate tokens when performing MT across multiple language pairs of different levels of resourcedness and multiple setups. We find that "thinking tokens" do not help LRMs better perform MT. This result generalizes to models fine-tuned to reason before translating using distilled chain of thought (CoT) inspired by human translators\' practices. Specifically, fine-tuning a model with synthetic CoT explanations detailing how to translate step-by-step does not outperform standard input-output fine-tuning. However, constructing the intermediate tokens by combining the outputs of modular translation-specific prompting strategies results in improvements. Our findings underscore that the contribution of intermediate tokens during fine-tuning highly depends on the presence of translation attempts within them. More broadly, our results suggest that using a teacher to refine target translations or to expand parallel corpora is more impactful than distilling their CoT explanations into "thinking" MT models.', 'score': 4, 'issue_id': 6421, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': 'ea9cdd45bedd4fcf', 'authors': ['Armel Zebaze', 'Rachel Bawden', 'Benoît Sagot'], 'affiliations': ['Inria Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2510.11919.jpg', 'data': {'categories': ['#low_resource', '#reasoning', '#training', '#multilingual', '#machine_translation', '#synthetic'], 'emoji': '🤔', 'ru': {'title': 'Размышления не помогают LLM лучше переводить', 'desc': 'Исследователи изучили, помогают ли "токены размышлений" (thinking tokens) большим reasoning моделям (LRM) лучше справляться с машинным переводом. Оказалось, что генерация промежуточных рассуждений перед переводом, включая дистилляцию chain of thought (CoT) от учителя, не улучшает качество перевода по сравнению со стандартным fine-tuning. Улучшения наблюдались только когда промежуточные токены содержали непосредственно попытки перевода через модульные стратегии промптинга. Результаты показывают, что для машинного перевода эффективнее использовать учителя для улучшения целевых переводов или расширения параллельных корпусов, чем дистиллировать объяснения CoT в модели.'}, 'en': {'title': 'Intermediate Tokens: Not the Key to Better Machine Translation', 'desc': "This paper investigates the role of large reasoning models (LRMs) in machine translation (MT) by examining the use of intermediate tokens during the translation process. The authors find that generating 'thinking tokens' does not enhance the performance of LRMs in MT tasks, even when models are fine-tuned with chain of thought (CoT) explanations. Instead, they discover that combining outputs from translation-specific prompting strategies leads to better results. Overall, the study highlights that refining translations through teacher models or expanding training data is more beneficial than simply using reasoning-based approaches in MT."}, 'zh': {'title': '中间标记在机器翻译中的重要性', 'desc': '大型推理模型（LRMs）在解决问题方面展现了新的可能性，尤其是在自然语言思维过程的引导下进行回答。尽管它们在数学和编程任务中的能力已被广泛认可，但在机器翻译（MT）任务中的影响仍然未被充分探讨。我们的研究发现，生成中间标记并未帮助LRMs在机器翻译中表现更好，尤其是在使用人类翻译者实践启发的思维链（CoT）进行微调时。相反，通过结合模块化翻译特定提示策略的输出构建中间标记则能带来改进，表明中间标记的贡献高度依赖于其中的翻译尝试。'}}}, {'id': 'https://huggingface.co/papers/2510.12777', 'title': 'What If : Understanding Motion Through Sparse Interactions', 'url': 'https://huggingface.co/papers/2510.12777', 'abstract': 'Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on sparse interactions termed "pokes". Unlike traditional methods that typically only enable dense sampling of a single realization of scene dynamics, FPT provides an interpretable directly accessible representation of multi-modal scene motion, its dependency on physical interactions and the inherent uncertainties of scene dynamics. We also evaluate our model on several downstream tasks to enable comparisons with prior methods and highlight the flexibility of our approach. On dense face motion generation, our generic pre-trained model surpasses specialized baselines. FPT can be fine-tuned in strongly out-of-distribution tasks such as synthetic datasets to enable significant improvements over in-domain methods in articulated object motion estimation. Additionally, predicting explicit motion distributions directly enables our method to achieve competitive performance on tasks like moving part segmentation from pokes which further demonstrates the versatility of our FPT. Code and models are publicly available at https://compvis.github.io/flow-poke-transformer.', 'score': 2, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'bb4a1b3538707a8b', 'authors': ['Stefan Andreas Baumann', 'Nick Stracke', 'Timy Phan', 'Björn Ommer'], 'affiliations': ['CompVis @ LMU Munich', 'Munich Center for Machine Learning (MCML)'], 'pdf_title_img': 'assets/pdf/title_img/2510.12777.jpg', 'data': {'categories': ['#cv', '#interpretability', '#open_source', '#reasoning', '#training', '#optimization', '#synthetic'], 'emoji': '🔄', 'ru': {'title': 'Flow Poke Transformer: новая эра в предсказании движения', 'desc': 'В статье представлена новая модель Flow Poke Transformer (FPT), которая предсказывает распределение локального движения на основе взаимодействий, называемых "поками". В отличие от традиционных методов, FPT позволяет интерпретировать многомодальное движение сцены и его зависимость от физических взаимодействий. Модель превосходит специализированные базовые методы в задачах генерации движения лица и оценки движения сложных объектов. FPT также демонстрирует высокую гибкость и конкурентоспособность в задачах сегментации движущихся частей.'}, 'en': {'title': 'Predicting Scene Dynamics with Flow Poke Transformer', 'desc': "The Flow Poke Transformer (FPT) is a new machine learning framework designed to predict how physical scenes change based on local interactions called 'pokes'. Unlike traditional models that only provide a single outcome, FPT offers a clear representation of multiple possible motions and their uncertainties. This model has been tested on various tasks, showing that it can outperform specialized models in generating dense face motion and improve performance in estimating articulated object motion. By directly predicting motion distributions, FPT also excels in tasks like moving part segmentation, showcasing its adaptability and effectiveness."}, 'zh': {'title': '流动戳击变换器：理解物理场景动态的新方法', 'desc': '本文介绍了一种新的框架，称为流动戳击变换器（FPT），用于直接预测物理场景中局部运动的分布。与传统方法不同，FPT能够提供多模态场景运动的可解释表示，并考虑物理交互和场景动态的不确定性。我们在多个下游任务上评估了该模型，显示出其灵活性和优越性，尤其是在稠密面部运动生成和关节物体运动估计方面。FPT的直接运动分布预测能力使其在移动部件分割等任务中表现出色，展示了其广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.12793', 'title': 'ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution', 'url': 'https://huggingface.co/papers/2510.12793', 'abstract': "Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens. The key idea behind our method is to employ multiple MLP connectors, each with a different image compression ratio, to downsample the vision tokens based on the semantic complexity of the image. During training, we minimize the KL divergence between the responses conditioned on different MLP connectors. At inference time, we introduce an image router, termed Visual Resolution Router (ViR), that automatically selects the appropriate compression rate for each image patch. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the model's perception, reasoning, and OCR capabilities. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research.", 'score': 1, 'issue_id': 6424, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'faa0a7ba8d794172', 'authors': ['Long Cui', 'Weiyun Wang', 'Jie Shao', 'Zichen Wen', 'Gen Luo', 'Linfeng Zhang', 'Yanting Zhang', 'Yu Qiao', 'Wenhai Wang'], 'affiliations': ['Donghua University', 'Fudan University', 'Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.12793.jpg', 'data': {'categories': ['#cv', '#training', '#inference', '#interpretability', '#agi', '#optimization', '#multimodal'], 'emoji': '🎯', 'ru': {'title': 'Умное сжатие визуальных токенов по сложности изображения', 'desc': 'Исследователи предложили метод Visual Consistency Learning (ViCO), который позволяет multimodal LLM использовать разное количество визуальных токенов в зависимости от семантической сложности изображения. Система использует несколько MLP-коннекторов с различными коэффициентами сжатия и минимизирует KL-дивергенцию между их ответами во время обучения. Специальный роутер Visual Resolution Router автоматически выбирает оптимальный уровень сжатия для каждого патча изображения на этапе инференса. Метод позволяет сократить количество визуальных токенов до 50% при сохранении способностей модели к восприятию, рассуждению и распознаванию текста.'}, 'en': {'title': 'Efficient Vision Token Management for MLLMs', 'desc': "This paper introduces Visual Consistency Learning (ViCO), a new training approach for Multimodal Large Language Models (MLLMs) that addresses high inference costs caused by image inputs. ViCO allows the model to use different numbers of vision tokens based on the semantic complexity of images, improving efficiency. The method employs multiple MLP connectors to downsample vision tokens and minimizes KL divergence during training. At inference, the Visual Resolution Router (ViR) selects the optimal compression rate for image patches, reducing vision tokens by up to 50% while preserving the model's performance in perception and reasoning tasks."}, 'zh': {'title': '动态适应语义复杂性的视觉标记选择', 'desc': '现有的多模态大型语言模型（MLLMs）在处理图像输入时，由于引入了额外的视觉标记，导致推理成本增加。我们提出了一种新颖的训练算法，称为视觉一致性学习（ViCO），使模型能够根据图像的语义复杂性使用不同数量的视觉标记。我们的方法通过多个具有不同图像压缩比的多层感知器（MLP）连接器来下采样视觉标记，从而适应图像的语义复杂性。在推理时，我们引入了一种称为视觉分辨率路由器（ViR）的图像路由器，自动选择每个图像块的适当压缩率。'}}}, {'id': 'https://huggingface.co/papers/2510.12088', 'title': 'One Life to Learn: Inferring Symbolic World Models for Stochastic\n  Environments from Unguided Exploration', 'url': 'https://huggingface.co/papers/2510.12088', 'abstract': 'OneLife framework models complex, stochastic environments using conditionally-activated programmatic laws within a probabilistic programming framework, enabling learning from minimal, unguided interaction and outperforming baselines in state ranking and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Symbolic world modeling requires inferring and representing an environment\'s transitional dynamics as an executable program. Prior work has focused on largely deterministic environments with abundant interaction data, simple mechanics, and human guidance. We address a more realistic and challenging setting, learning in a complex, stochastic environment where the agent has only "one life" to explore a hostile environment without human guidance. We introduce OneLife, a framework that models world dynamics through conditionally-activated programmatic laws within a probabilistic programming framework. Each law operates through a precondition-effect structure, activating in relevant world states. This creates a dynamic computation graph that routes inference and optimization only through relevant laws, avoiding scaling challenges when all laws contribute to predictions about a complex, hierarchical state, and enabling the learning of stochastic dynamics even with sparse rule activation. To evaluate our approach under these demanding constraints, we introduce a new evaluation protocol that measures (a) state ranking, the ability to distinguish plausible future states from implausible ones, and (b) state fidelity, the ability to generate future states that closely resemble reality. We develop and evaluate our framework on Crafter-OO, our reimplementation of the Crafter environment that exposes a structured, object-oriented symbolic state and a pure transition function that operates on that state alone. OneLife can successfully learn key environment dynamics from minimal, unguided interaction, outperforming a strong baseline on 16 out of 23 scenarios tested. We also test OneLife\'s planning ability, with simulated rollouts successfully identifying superior strategies. Our work establishes a foundation for autonomously constructing programmatic world models of unknown, complex environments.', 'score': 1, 'issue_id': 6426, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'd8a451b665853cec', 'authors': ['Zaid Khan', 'Archiki Prasad', 'Elias Stengel-Eskin', 'Jaemin Cho', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2510.12088.jpg', 'data': {'categories': ['#rl', '#benchmark', '#reasoning', '#optimization', '#games', '#agents'], 'emoji': '🎮', 'ru': {'title': 'Обучение модели мира за одну жизнь без подсказок', 'desc': 'Статья представляет OneLife — фреймворк для символьного моделирования динамики сложных стохастических сред через программируемые законы с условной активацией. Система работает в рамках вероятностного программирования, где каждый закон имеет структуру «предусловие-эффект» и активируется только в релевантных состояниях мира. Это позволяет агенту учиться из минимального количества неконтролируемых взаимодействий в враждебной среде, где у него есть только «одна жизнь» для исследования. Подход демонстрирует превосходство над базовыми методами в задачах ранжирования состояний и точности их генерации, успешно изучая ключевую динамику среды и планируя эффективные стратегии.'}, 'en': {'title': 'OneLife: Learning in Complex Environments with Minimal Interaction', 'desc': 'The OneLife framework is designed to model complex environments that are unpredictable and require minimal interaction for learning. It uses conditionally-activated programmatic laws within a probabilistic programming structure to represent the dynamics of the environment. This approach allows the agent to learn effectively even when it has only one chance to explore, without any human guidance. The framework has been shown to outperform existing methods in distinguishing future states and generating realistic outcomes, demonstrating its potential for autonomous learning in challenging settings.'}, 'zh': {'title': 'OneLife：在复杂环境中实现自主学习的框架', 'desc': 'OneLife框架通过条件激活的程序法则在概率编程框架中建模复杂的随机环境，使得智能体能够在没有人类指导的情况下，从最小的交互中学习。该方法特别适用于在复杂和敌对的环境中进行学习，智能体只有一次探索的机会。OneLife通过动态计算图来优化推理过程，避免了在复杂状态下所有法则都参与预测时的扩展挑战。实验结果表明，OneLife在16个测试场景中超越了强基线，成功学习了环境的关键动态。'}}}, {'id': 'https://huggingface.co/papers/2510.11606', 'title': 'ExpVid: A Benchmark for Experiment Video Understanding & Reasoning', 'url': 'https://huggingface.co/papers/2510.11606', 'abstract': 'ExpVid, a new benchmark for evaluating multimodal large language models on scientific experiment videos, highlights gaps in fine-grained perception, procedural understanding, and scientific reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) hold promise for accelerating scientific discovery by interpreting complex experimental procedures. However, their true capabilities are poorly understood, as existing benchmarks neglect the fine-grained and long-horizon nature of authentic laboratory work, especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the first benchmark designed to systematically evaluate MLLMs on scientific experiment videos. Curated from peer-reviewed video publications, ExpVid features a new three-level task hierarchy that mirrors the scientific process: (1) Fine-grained Perception of tools, materials, and actions; (2) Procedural Understanding of step order and completeness; and (3) Scientific Reasoning that connects the full experiment to its published conclusions. Our vision-centric annotation pipeline, combining automated generation with multi-disciplinary expert validation, ensures that tasks require visual grounding. We evaluate 19 leading MLLMs on ExpVid and find that while they excel at coarse-grained recognition, they struggle with disambiguating fine details, tracking state changes over time, and linking experimental procedures to scientific outcomes. Our results reveal a notable performance gap between proprietary and open-source models, particularly in high-order reasoning. ExpVid not only provides a diagnostic tool but also charts a roadmap for developing MLLMs capable of becoming trustworthy partners in scientific experimentation.', 'score': 1, 'issue_id': 6424, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '4f72ba62e40a520c', 'authors': ['Yicheng Xu', 'Yue Wu', 'Jiashuo Yu', 'Ziang Yan', 'Tianxiang Jiang', 'Yinan He', 'Qingsong Zhao', 'Kai Chen', 'Yu Qiao', 'Limin Wang', 'Manabu Okumura', 'Yi Wang'], 'affiliations': ['Institute of Science Tokyo', 'Nanjing University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2510.11606.jpg', 'data': {'categories': ['#cv', '#science', '#benchmark', '#open_source', '#reasoning', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'Научные эксперименты — слабое место современных AI', 'desc': 'Исследователи представили ExpVid — первый бенчмарк для оценки мультимодальных LLM на видео научных экспериментов. Бенчмарк включает трёхуровневую иерархию задач: детальное восприятие инструментов и действий, понимание процедурной последовательности и научное рассуждение. Тестирование 19 ведущих MLLM показало, что модели хорошо справляются с общим распознаванием, но испытывают трудности с различением мелких деталей, отслеживанием изменений состояний и связыванием процедур с научными выводами. ExpVid выявляет значительный разрыв между проприетарными и открытыми моделями, особенно в задачах высокоуровневого рассуждения.'}, 'en': {'title': 'ExpVid: Bridging Gaps in MLLM Evaluation for Scientific Discovery', 'desc': 'ExpVid is a new benchmark designed to evaluate multimodal large language models (MLLMs) specifically on scientific experiment videos. It addresses the shortcomings of existing benchmarks by focusing on fine-grained perception, procedural understanding, and scientific reasoning in laboratory settings. The benchmark features a three-level task hierarchy that reflects the scientific process, ensuring that models are tested on their ability to perceive details, understand procedures, and reason scientifically. Our evaluation of 19 leading MLLMs reveals significant gaps in their performance, particularly in high-order reasoning, highlighting the need for improved models in scientific contexts.'}, 'zh': {'title': 'ExpVid：科学实验视频评估的新基准', 'desc': 'ExpVid是一个新的基准，用于评估多模态大型语言模型在科学实验视频上的表现。它揭示了在细粒度感知、程序理解和科学推理方面的不足。该基准通过三层任务层次结构，系统地评估模型在实验过程中的表现，包括工具和材料的识别、步骤的理解以及实验与结论的关联。我们的研究表明，尽管现有模型在粗粒度识别上表现良好，但在处理细节和高阶推理时仍存在显著差距。'}}}, {'id': 'https://huggingface.co/papers/2510.08783', 'title': 'MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human\n  Perception of User Interfaces', 'url': 'https://huggingface.co/papers/2510.08783', 'abstract': 'In an ideal design pipeline, user interface (UI) design is intertwined with user research to validate decisions, yet studies are often resource-constrained during early exploration. Recent advances in multimodal large language models (MLLMs) offer a promising opportunity to act as early evaluators, helping designers narrow options before formal testing. Unlike prior work that emphasizes user behavior in narrow domains such as e-commerce with metrics like clicks or conversions, we focus on subjective user evaluations across varied interfaces. We investigate whether MLLMs can mimic human preferences when evaluating individual UIs and comparing them. Using data from a crowdsourcing platform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and examine alignment with human judgments on multiple UI factors. Our results show that MLLMs approximate human preferences on some dimensions but diverge on others, underscoring both their potential and limitations in supplementing early UX research.', 'score': 1, 'issue_id': 6425, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'd90fcd4cec73cd8c', 'authors': ['Reuben A. Luera', 'Ryan Rossi', 'Franck Dernoncourt', 'Samyadeep Basu', 'Sungchul Kim', 'Subhojyoti Mukherjee', 'Puneet Mathur', 'Ruiyi Zhang', 'Jihyung Kil', 'Nedim Lipka', 'Seunghyun Yoon', 'Jiuxiang Gu', 'Zichao Wang', 'Cindy Xiong Bearfield', 'Branislav Kveton'], 'affiliations': ['Adobe Research', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2510.08783.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#multimodal', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'LLM как ранние оценщики пользовательских интерфейсов', 'desc': 'Исследование проверяет, могут ли мультимодальные LLM (большие языковые модели) оценивать пользовательские интерфейсы так же, как люди. Авторы тестировали GPT-4o, Claude и Llama на 30 различных интерфейсах, сравнивая их оценки с мнениями реальных пользователей из краудсорсинговой платформы. Результаты показали, что AI-модели частично совпадают с человеческими предпочтениями по некоторым аспектам UI, но расходятся по другим. Это демонстрирует потенциал использования LLM для ранней оценки дизайна интерфейсов, но также выявляет их ограничения.'}, 'en': {'title': 'Harnessing MLLMs for Early UI Evaluation', 'desc': 'This paper explores the use of multimodal large language models (MLLMs) as tools for early evaluation in user interface (UI) design. It investigates whether these models can replicate human preferences when assessing different UIs, using data from a crowdsourcing platform. The study benchmarks several MLLMs, including GPT-4o, Claude, and Llama, against human judgments on various UI factors. The findings reveal that while MLLMs can approximate human preferences in some areas, they also show significant divergence in others, highlighting their potential and limitations in enhancing early user experience (UX) research.'}, 'zh': {'title': '利用MLLMs提升用户界面设计的早期评估', 'desc': '本研究探讨了多模态大型语言模型（MLLMs）在用户界面（UI）设计中的应用，特别是在早期评估阶段。我们通过比较GPT-4o、Claude和Llama等模型在30个不同界面上的表现，评估它们是否能够模拟人类的偏好。研究结果表明，虽然MLLMs在某些方面能够接近人类的判断，但在其他方面则存在差异，这显示了它们在早期用户体验（UX）研究中的潜力和局限性。通过这种方式，设计师可以在正式测试之前更有效地缩小选择范围。'}}}, {'id': 'https://huggingface.co/papers/2510.12403', 'title': 'Robot Learning: A Tutorial', 'url': 'https://huggingface.co/papers/2510.12403', 'abstract': 'Robot learning transitions from model-based to data-driven methods, leveraging reinforcement learning and behavioral cloning to develop versatile, language-conditioned models for diverse tasks and robot types.  \t\t\t\t\tAI-generated summary \t\t\t\t Robot learning is at an inflection point, driven by rapid advancements in machine learning and the growing availability of large-scale robotics data. This shift from classical, model-based methods to data-driven, learning-based paradigms is unlocking unprecedented capabilities in autonomous systems. This tutorial navigates the landscape of modern robot learning, charting a course from the foundational principles of Reinforcement Learning and Behavioral Cloning to generalist, language-conditioned models capable of operating across diverse tasks and even robot embodiments. This work is intended as a guide for researchers and practitioners, and our goal is to equip the reader with the conceptual understanding and practical tools necessary to contribute to developments in robot learning, with ready-to-use examples implemented in lerobot.', 'score': 0, 'issue_id': 6426, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '6a710031eb0a9d1a', 'authors': ['Francesco Capuano', 'Caroline Pascal', 'Adil Zouitine', 'Thomas Wolf', 'Michel Aractingi'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2510.12403.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#games', '#agents', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'От классики к данным: новая эра обучения роботов', 'desc': 'Статья представляет собой учебное руководство по современному машинному обучению для роботов. Авторы описывают переход от классических модельных подходов к методам, основанным на данных, включая reinforcement learning и behavioral cloning. Особое внимание уделяется созданию универсальных моделей, управляемых естественным языком, которые могут работать с различными задачами и типами роботов. Материал содержит практические примеры на базе библиотеки lerobot и предназначен для исследователей и практиков в области робототехники.'}, 'en': {'title': 'Empowering Robots with Data-Driven Learning', 'desc': 'This paper discusses the evolution of robot learning from traditional model-based approaches to modern data-driven techniques. It highlights the importance of reinforcement learning and behavioral cloning in creating adaptable models that can understand and execute various tasks based on language input. The authors aim to provide a comprehensive overview of current methodologies and their applications in robotics, emphasizing the role of large-scale data in enhancing robot capabilities. The tutorial serves as a resource for researchers and practitioners, offering practical tools and examples to facilitate advancements in the field.'}, 'zh': {'title': '机器人学习：从模型到数据驱动的转变', 'desc': '机器人学习正经历从基于模型的方法转向数据驱动的方法。这种转变利用了强化学习和行为克隆技术，开发出适应多种任务和机器人类型的语言条件模型。随着机器学习的快速发展和大规模机器人数据的可用性，这一领域正在解锁前所未有的自主系统能力。本文旨在为研究人员和从业者提供指导，帮助他们理解机器人学习的基本概念和实用工具。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (6)', '#agi (3)', '#alignment (3)', '#architecture (1)', '#audio', '#benchmark (7)', '#cv (8)', '#data (2)', '#dataset (4)', '#diffusion (4)', '#ethics', '#games (3)', '#graphs', '#hallucinations', '#healthcare', '#inference (3)', '#interpretability (3)', '#leakage', '#long_context (1)', '#low_resource (2)', '#machine_translation (2)', '#math', '#multilingual (2)', '#multimodal (12)', '#open_source (4)', '#optimization (15)', '#plp', '#rag (1)', '#reasoning (12)', '#rl (7)', '#rlhf (1)', '#robotics (1)', '#science (1)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (18)', '#transfer_learning (4)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-15 07:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-15 07:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-15 07:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    