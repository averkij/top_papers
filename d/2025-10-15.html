
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 44 papers. October 15.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">44 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-14.html">â¬…ï¸ <span id="prev-date">14.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-16.html">â¡ï¸ <span id="next-date">16.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'};
        let feedDateNext = {'ru': '16.10', 'en': '10/16', 'zh': '10æœˆ16æ—¥'};
        let feedDatePrev = {'ru': '14.10', 'en': '10/14', 'zh': '10æœˆ14æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.12276', 'title': 'Spatial Forcing: Implicit Spatial Representation Alignment for\n  Vision-language-action Model', 'url': 'https://huggingface.co/papers/2510.12276', 'abstract': 'Vision-language-action (VLA) models have recently shown strong potential in enabling robots to follow language instructions and execute precise actions. However, most VLAs are built upon vision-language models pretrained solely on 2D data, which lack accurate spatial awareness and hinder their ability to operate in the 3D physical world. Existing solutions attempt to incorporate explicit 3D sensor inputs such as depth maps or point clouds, but these approaches face challenges due to sensor noise, hardware heterogeneity, and incomplete depth coverage in existing datasets. Alternative methods that estimate 3D cues from 2D images also suffer from the limited performance of depth estimators.We propose Spatial Forcing (SF), a simple yet effective alignment strategy that implicitly forces VLA models to develop spatial comprehension capabilities without relying on explicit 3D inputs or depth estimators. SF aligns intermediate visual embeddings of VLAs with geometric representations produced by pretrained 3D foundation models. By enforcing alignment at intermediate layers, SF guides VLAs to encode richer spatial representations that enhance action precision.Extensive experiments in simulation and real-world environments demonstrate that SF achieves state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further accelerates training by up to 3.8x and improves data efficiency across diverse robotic tasks. Project page is at https://spatial-forcing.github.io/', 'score': 130, 'issue_id': 6422, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '93d66257d7564b11', 'authors': ['Fuhao Li', 'Wenxuan Song', 'Han Zhao', 'Jingbo Wang', 'Pengxiang Ding', 'Donglin Wang', 'Long Zeng', 'Haoang Li'], 'affiliations': ['South China University of Technology', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Tsinghua University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.12276.jpg', 'data': {'categories': ['#3d', '#optimization', '#training', '#agents', '#alignment'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· 3D ÑĞµĞ½ÑĞ¾Ñ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Spatial Forcing (SF) â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ vision-language-action (VLA) Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… VLA Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° 2D Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ° ÑĞ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D ÑĞµĞ½ÑĞ¾Ñ€Ğ¾Ğ² Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ ÑˆÑƒĞ¼ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. SF Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² VLA Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… 3D foundation Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½ĞµÑĞ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² 3.8 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Enhancing Spatial Awareness in Robots with Spatial Forcing', 'desc': 'This paper introduces Spatial Forcing (SF), a novel alignment strategy for vision-language-action (VLA) models that enhances their spatial awareness without the need for explicit 3D inputs. By aligning intermediate visual embeddings with geometric representations from pretrained 3D models, SF helps VLAs develop better spatial comprehension. The approach addresses limitations of existing methods that rely on noisy 3D sensor data or depth estimators, which often struggle with accuracy. Experimental results show that SF not only improves action precision but also accelerates training and increases data efficiency in various robotic tasks.'}, 'zh': {'title': 'ç©ºé—´å¼ºåˆ¶ï¼šæå‡æœºå™¨äººç©ºé—´ç†è§£çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç©ºé—´å¼ºåˆ¶ï¼ˆSpatial Forcing, SFï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„VLAæ¨¡å‹ä¾èµ–äºä»…åœ¨2Dæ•°æ®ä¸Šé¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç¼ºä¹å¯¹3Dç‰©ç†ä¸–ç•Œçš„å‡†ç¡®ç©ºé—´æ„ŸçŸ¥ã€‚SFé€šè¿‡å°†VLAæ¨¡å‹çš„ä¸­é—´è§†è§‰åµŒå…¥ä¸é¢„è®­ç»ƒçš„3DåŸºç¡€æ¨¡å‹ç”Ÿæˆçš„å‡ ä½•è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œæ¥å¢å¼ºæ¨¡å‹çš„ç©ºé—´è¡¨ç¤ºèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSFåœ¨å¤šç§æœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜äº†3.8å€ï¼Œå¹¶ä¸”åœ¨æ•°æ®æ•ˆç‡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09116', 'title': 'DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel\n  Translation', 'url': 'https://huggingface.co/papers/2510.09116', 'abstract': 'A new evaluation framework, DITING, and a reasoning-driven multi-agent evaluation framework, AgentEval, are introduced to assess the quality of web novel translations, revealing that Chinese-trained LLMs outperform larger foreign models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have substantially advanced machine translation (MT), yet their effectiveness in translating web novels remains unclear. Existing benchmarks rely on surface-level metrics that fail to capture the distinctive traits of this genre. To address these gaps, we introduce DITING, the first comprehensive evaluation framework for web novel translation, assessing narrative and cultural fidelity across six dimensions: idiom translation, lexical ambiguity, terminology localization, tense consistency, zero-pronoun resolution, and cultural safety, supported by over 18K expert-annotated Chinese-English sentence pairs. We further propose AgentEval, a reasoning-driven multi-agent evaluation framework that simulates expert deliberation to assess translation quality beyond lexical overlap, achieving the highest correlation with human judgments among seven tested automatic metrics. To enable metric comparison, we develop MetricAlign, a meta-evaluation dataset of 300 sentence pairs annotated with error labels and scalar quality scores. Comprehensive evaluation of fourteen open, closed, and commercial models reveals that Chinese-trained LLMs surpass larger foreign counterparts, and that DeepSeek-V3 delivers the most faithful and stylistically coherent translations. Our work establishes a new paradigm for exploring LLM-based web novel translation and provides public resources to advance future research.', 'score': 89, 'issue_id': 6421, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': 'a37bd28d84a1c152', 'authors': ['Enze Zhang', 'Jiaying Wang', 'Mengxi Xiao', 'Jifei Liu', 'Ziyan Kuang', 'Rui Dong', 'Eric Dong', 'Sophia Ananiadou', 'Min Peng', 'Qianqian Xie'], 'affiliations': ['Center for Language and Information Research, Wuhan University', 'Jiangxi Normal University', 'Malvern College Chengdu', 'School of Artificial Intelligence, Wuhan University', 'The University of Manchester', 'Yunnan Trrans Technology Co., Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2510.09116.jpg', 'data': {'categories': ['#machine_translation', '#open_source', '#reasoning', '#dataset', '#multilingual', '#benchmark'], 'emoji': 'ğŸ“š', 'ru': {'title': 'ĞšĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğµ LLM Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ÑÑ‚ Ğ² Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ Ğ²ĞµĞ±-Ñ€Ğ¾Ğ¼Ğ°Ğ½Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ DITING â€” Ğ¿ĞµÑ€Ğ²ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ²ĞµĞ±-Ñ€Ğ¾Ğ¼Ğ°Ğ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆĞµÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ²: Ğ¸Ğ´Ğ¸Ğ¾Ğ¼Ñ‹, Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ‘Ğ½, Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑÑ‚Ğ¾Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. Ğ”Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ AgentEval â€” Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ¸Ğ¹ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ÑÑ€ĞµĞ´Ğ¸ ÑĞµĞ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğµ LLM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ€ÑƒĞ±ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ Ğ²ĞµĞ±-Ñ€Ğ¾Ğ¼Ğ°Ğ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ DeepSeek-V3 Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ² ÑÑ‚Ğ¾Ğ¼ Ğ¶Ğ°Ğ½Ñ€Ğµ.'}, 'en': {'title': 'Revolutionizing Web Novel Translation Evaluation with DITING and AgentEval', 'desc': 'This paper introduces two new frameworks, DITING and AgentEval, to evaluate the quality of web novel translations. DITING focuses on assessing narrative and cultural fidelity through six specific dimensions, using a large dataset of expert-annotated sentence pairs. AgentEval enhances evaluation by simulating expert deliberation, providing a more nuanced assessment of translation quality beyond simple lexical matching. The findings indicate that Chinese-trained large language models (LLMs) outperform larger foreign models in translation tasks, establishing a new standard for evaluating web novel translations.'}, 'zh': {'title': 'ä¸­æ–‡è®­ç»ƒæ¨¡å‹ç¿»è¯‘ç½‘ç»œå°è¯´æ›´èƒœä¸€ç­¹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶DITINGå’Œä¸€ä¸ªåŸºäºæ¨ç†çš„å¤šæ™ºèƒ½ä½“è¯„ä¼°æ¡†æ¶AgentEvalï¼Œç”¨äºè¯„ä¼°ç½‘ç»œå°è¯´ç¿»è¯‘çš„è´¨é‡ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€åªå…³æ³¨è¡¨é¢æŒ‡æ ‡ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰ç½‘ç»œå°è¯´çš„ç‹¬ç‰¹ç‰¹å¾ã€‚DITINGæ¡†æ¶é€šè¿‡å…­ä¸ªç»´åº¦è¯„ä¼°ç¿»è¯‘çš„å™äº‹å’Œæ–‡åŒ–å¿ å®åº¦ï¼Œæ˜¾ç¤ºå‡ºä¸­æ–‡è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¿»è¯‘è´¨é‡ä¸Šä¼˜äºæ›´å¤§çš„å¤–æ–‡æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç½‘ç»œå°è¯´ç¿»è¯‘æ¢ç´¢å»ºç«‹äº†æ–°çš„èŒƒå¼ï¼Œå¹¶æä¾›äº†å…¬å…±èµ„æºä»¥æ¨åŠ¨æœªæ¥çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12586', 'title': 'Advancing End-to-End Pixel Space Generative Modeling via Self-supervised\n  Pre-training', 'url': 'https://huggingface.co/papers/2510.12586', 'abstract': 'Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our training framework demonstrates strong empirical performance on ImageNet dataset. Specifically, our diffusion model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE), surpassing prior pixel-space methods by a large margin in both generation quality and efficiency while rivaling leading VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our consistency model achieves an impressive FID of 8.82 in a single sampling step, significantly surpassing its latent-space counterpart. To the best of our knowledge, this marks the first successful training of a consistency model directly on high-resolution images without relying on pre-trained VAEs or diffusion models.', 'score': 87, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'bf296825624ed354', 'authors': ['Jiachen Lei', 'Keli Liu', 'Julius Berner', 'Haiming Yu', 'Hongkai Zheng', 'Jiahong Wu', 'Xiangxiang Chu'], 'affiliations': ['AMAP, Alibaba Group', 'Caltech', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2510.12586.jpg', 'data': {'categories': ['#training', '#diffusion', '#cv', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ consistency models Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ²Ğ´Ğ¾Ğ»ÑŒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ â€” ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ÑÑ Ñ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ»Ñ end-to-end Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ FID 2.04 Ğ½Ğ° ImageNet-256 Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğ¹ FID 8.82 Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ğ´Ğ»Ñ consistency model, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ consistency model Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… VAE Ğ¸Ğ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Bridging the Gap: Enhanced Pixel-Space Generative Models', 'desc': 'This paper presents a new two-stage training framework aimed at improving pixel-space generative models, which traditionally struggle with performance compared to latent-space models. In the first stage, encoders are pre-trained to understand the semantics of clean images while aligning them with a deterministic sampling path. The second stage involves integrating the encoder with a decoder and fine-tuning the entire model for both diffusion and consistency tasks. The proposed method shows significant improvements in image generation quality and efficiency on the ImageNet dataset, achieving state-of-the-art results without relying on pre-trained models.'}, 'zh': {'title': 'ç¼©å°åƒç´ ç©ºé—´ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½å·®è·', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ç¼©å°åƒç´ ç©ºé—´ç”Ÿæˆæ¨¡å‹ä¸æ½œåœ¨ç©ºé—´æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å’Œæ•ˆç‡å·®è·ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é¢„è®­ç»ƒç¼–ç å™¨ï¼Œä»å¹²å‡€å›¾åƒä¸­æ•æ‰æœ‰æ„ä¹‰çš„è¯­ä¹‰ï¼Œå¹¶å°†å…¶ä¸æ²¿ç€ç¡®å®šæ€§é‡‡æ ·è½¨è¿¹çš„ç‚¹å¯¹é½ã€‚ç¬¬äºŒé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬å°†ç¼–ç å™¨ä¸éšæœºåˆå§‹åŒ–çš„è§£ç å™¨ç»“åˆï¼Œå¹¶å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯çš„å¾®è°ƒï¼Œä»¥é€‚åº”æ‰©æ•£å’Œä¸€è‡´æ€§æ¨¡å‹ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ¡†æ¶åœ¨ImageNetæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡æ–¹é¢ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„åƒç´ ç©ºé—´æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11693', 'title': 'Scaling Language-Centric Omnimodal Representation Learning', 'url': 'https://huggingface.co/papers/2510.11693', 'abstract': "Recent multimodal embedding approaches leveraging multimodal large language models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising results, yet the underlying reasons behind their superiority remain underexplored. This work argues that a crucial advantage of MLLM-based approaches stems from implicit cross-modal alignment achieved during generative pretraining, where the language decoder learns to exploit multimodal signals within a shared representation space for generating unimodal outputs. Through analysis of anisotropy and kernel similarity structure, we empirically confirm that latent alignment emerges within MLLM representations, allowing CL to serve as a lightweight refinement stage. Leveraging this insight, we propose a Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive experiments across diverse backbones and benchmarks demonstrate its effectiveness, achieving state-of-the-art performance across modalities. Furthermore, we identify a Generation-Representation Scaling Law (GRSL), showing that the representational capabilities gained through contrastive refinement scales positively with the MLLM's generative capabilities. This suggests that improving generative abilities evolves as an effective paradigm for enhancing representation quality. We provide a theoretical explanation of GRSL, which formally links the MLLM's generative quality to the upper bound on its representation performance, and validate it on a challenging, low-resource visual-document retrieval task, showing that continual generative pretraining before CL can further enhance the potential of a model's embedding capabilities. Codes, models, and resources are available at https://github.com/LCO-Embedding/LCO-Embedding.", 'score': 81, 'issue_id': 6422, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '29d98b7946fd646d', 'authors': ['Chenghao Xiao', 'Hou Pong Chan', 'Hao Zhang', 'Weiwen Xu', 'Mahani Aljunied', 'Yu Rong'], 'affiliations': ['DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.11693.jpg', 'data': {'categories': ['#low_resource', '#data', '#benchmark', '#multimodal', '#transfer_learning', '#training', '#alignment'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ÑÑ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº LCO-Emb, Ğ³Ğ´Ğµ contrastive learning Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ ĞºĞ°Ğº Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ… ÑƒĞ¶Ğµ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Generation-Representation Scaling Law (GRSL), Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ across Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Multimodal Potential with LCO-Emb', 'desc': 'This paper explores the advantages of multimodal large language models (MLLMs) that use contrastive learning (CL) for better performance in embedding tasks. It highlights that the strength of these models comes from their ability to align different types of data (like text and images) during the initial training phase, which helps them generate more accurate outputs. The authors introduce a new framework called Language-Centric Omnimodal Embedding (LCO-Emb) that builds on these insights and shows improved results across various tasks. Additionally, they present a Generation-Representation Scaling Law (GRSL) that connects the generative capabilities of MLLMs to their representation quality, suggesting that enhancing generative skills can lead to better embeddings.'}, 'zh': {'title': 'æå‡ç”Ÿæˆèƒ½åŠ›ï¼Œå¢å¼ºè¡¨ç¤ºè´¨é‡çš„æœ‰æ•ˆé€”å¾„', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰ä¸‹çš„ä¼˜è¶Šæ€§åŠå…¶åŸå› ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMLLMçš„ä¸€ä¸ªé‡è¦ä¼˜åŠ¿åœ¨äºç”Ÿæˆé¢„è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°çš„éšå¼è·¨æ¨¡æ€å¯¹é½ï¼Œä½¿å¾—è¯­è¨€è§£ç å™¨èƒ½å¤Ÿåœ¨å…±äº«è¡¨ç¤ºç©ºé—´ä¸­åˆ©ç”¨å¤šæ¨¡æ€ä¿¡å·ç”Ÿæˆå•æ¨¡æ€è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥è¯­è¨€ä¸ºä¸­å¿ƒçš„å…¨æ¨¡æ€åµŒå…¥æ¡†æ¶LCO-Embï¼Œå¹¶é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†å…¶åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­çš„æœ‰æ•ˆæ€§ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ç”Ÿæˆ-è¡¨ç¤ºç¼©æ”¾æ³•åˆ™ï¼ˆGRSLï¼‰ï¼Œè¡¨æ˜é€šè¿‡å¯¹æ¯”ç²¾ç‚¼è·å¾—çš„è¡¨ç¤ºèƒ½åŠ›ä¸MLLMçš„ç”Ÿæˆèƒ½åŠ›å‘ˆæ­£ç›¸å…³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12403', 'title': 'Robot Learning: A Tutorial', 'url': 'https://huggingface.co/papers/2510.12403', 'abstract': 'Robot learning transitions from model-based to data-driven methods, leveraging reinforcement learning and behavioral cloning to develop versatile, language-conditioned models for diverse tasks and robot types.  \t\t\t\t\tAI-generated summary \t\t\t\t Robot learning is at an inflection point, driven by rapid advancements in machine learning and the growing availability of large-scale robotics data. This shift from classical, model-based methods to data-driven, learning-based paradigms is unlocking unprecedented capabilities in autonomous systems. This tutorial navigates the landscape of modern robot learning, charting a course from the foundational principles of Reinforcement Learning and Behavioral Cloning to generalist, language-conditioned models capable of operating across diverse tasks and even robot embodiments. This work is intended as a guide for researchers and practitioners, and our goal is to equip the reader with the conceptual understanding and practical tools necessary to contribute to developments in robot learning, with ready-to-use examples implemented in lerobot.', 'score': 35, 'issue_id': 6426, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '6a710031eb0a9d1a', 'authors': ['Francesco Capuano', 'Caroline Pascal', 'Adil Zouitine', 'Thomas Wolf', 'Michel Aractingi'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2510.12403.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#games', '#agents', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ ĞºĞ»Ğ°ÑÑĞ¸ĞºĞ¸ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼: Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¾Ñ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ reinforcement learning Ğ¸ behavioral cloning. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞœĞ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ» ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ lerobot Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Empowering Robots with Data-Driven Learning', 'desc': 'This paper discusses the evolution of robot learning from traditional model-based approaches to modern data-driven techniques. It highlights the importance of reinforcement learning and behavioral cloning in creating adaptable models that can understand and execute various tasks based on language input. The authors aim to provide a comprehensive overview of current methodologies and their applications in robotics, emphasizing the role of large-scale data in enhancing robot capabilities. The tutorial serves as a resource for researchers and practitioners, offering practical tools and examples to facilitate advancements in the field.'}, 'zh': {'title': 'æœºå™¨äººå­¦ä¹ ï¼šä»æ¨¡å‹åˆ°æ•°æ®é©±åŠ¨çš„è½¬å˜', 'desc': 'æœºå™¨äººå­¦ä¹ æ­£ç»å†ä»åŸºäºæ¨¡å‹çš„æ–¹æ³•è½¬å‘æ•°æ®é©±åŠ¨çš„æ–¹æ³•ã€‚è¿™ç§è½¬å˜åˆ©ç”¨äº†å¼ºåŒ–å­¦ä¹ å’Œè¡Œä¸ºå…‹éš†æŠ€æœ¯ï¼Œå¼€å‘å‡ºé€‚åº”å¤šç§ä»»åŠ¡å’Œæœºå™¨äººç±»å‹çš„è¯­è¨€æ¡ä»¶æ¨¡å‹ã€‚éšç€æœºå™¨å­¦ä¹ çš„å¿«é€Ÿå‘å±•å’Œå¤§è§„æ¨¡æœºå™¨äººæ•°æ®çš„å¯ç”¨æ€§ï¼Œè¿™ä¸€é¢†åŸŸæ­£åœ¨è§£é”å‰æ‰€æœªæœ‰çš„è‡ªä¸»ç³»ç»Ÿèƒ½åŠ›ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›æŒ‡å¯¼ï¼Œå¸®åŠ©ä»–ä»¬ç†è§£æœºå™¨äººå­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µå’Œå®ç”¨å·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12798', 'title': 'Detect Anything via Next Point Prediction', 'url': 'https://huggingface.co/papers/2510.12798', 'abstract': "Object detection has long been dominated by traditional coordinate regression-based models, such as YOLO, DETR, and Grounding DINO. Although recent efforts have attempted to leverage MLLMs to tackle this task, they face challenges like low recall rate, duplicate predictions, coordinate misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a 3B-scale MLLM that achieves state-of-the-art object perception performance. On benchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or exceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot setting. This is enabled by three key designs: 1) Task Formulation: we use special tokens to represent quantized coordinates from 0 to 999, reducing the model's learning difficulty and improving token efficiency for coordinate prediction; 2) Data Engines: we construct multiple data engines to generate high-quality grounding, referring, and pointing data, providing semantically rich supervision for training; \\3) Training Pipelines: we employ a two-stage training process, combining supervised fine-tuning on 22 million data with GRPO-based reinforcement post-training. This RL post-training leverages geometry-aware rewards to effectively bridge the discrete-to-continuous coordinate prediction gap, improve box accuracy, and mitigate undesirable behaviors like duplicate predictions that stem from the teacher-guided nature of the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent language understanding enables versatile capabilities such as object referring, pointing, visual prompting, GUI grounding, spatial referring, OCR and key-pointing, all systematically evaluated on dedicated benchmarks. We believe that Rex-Omni paves the way for more versatile and language-aware visual perception systems.", 'score': 32, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'ac3650578301e79e', 'authors': ['Qing Jiang', 'Junan Huo', 'Xingyu Chen', 'Yuda Xiong', 'Zhaoyang Zeng', 'Yihao Chen', 'Tianhe Ren', 'Junzhi Yu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)'], 'pdf_title_img': 'assets/pdf/title_img/2510.12798.jpg', 'data': {'categories': ['#cv', '#multimodal', '#training', '#optimization', '#rl'], 'emoji': 'ğŸ”', 'ru': {'title': 'Rex-Omni: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Rex-Omni, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ LLM. Rex-Omni Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… COCO Ğ¸ LVIS, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RL. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸.'}, 'en': {'title': 'Rex-Omni: Bridging Language and Vision for Superior Object Detection', 'desc': "This paper introduces Rex-Omni, a 3B-scale Multi-Modal Language Model (MLLM) designed to enhance object detection performance, traditionally dominated by coordinate regression models. Rex-Omni addresses common challenges such as low recall rates and coordinate misalignment by employing innovative strategies like special token representation for coordinates and multiple data engines for high-quality training data. The model achieves state-of-the-art results on benchmarks like COCO and LVIS, even in zero-shot scenarios, by utilizing a two-stage training process that combines supervised fine-tuning with reinforcement learning. Additionally, Rex-Omni's language understanding capabilities allow it to perform various tasks beyond standard detection, including object referring and visual prompting, marking a significant advancement in visual perception systems."}, 'zh': {'title': 'Rex-Omniï¼šå¼€åˆ›å¤šæ¨¡æ€ç‰©ä½“æ£€æµ‹æ–°çºªå…ƒ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç‰©ä½“æ£€æµ‹æ¨¡å‹Rex-Omniï¼Œå®ƒæ˜¯ä¸€ä¸ªè§„æ¨¡ä¸º3Bçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œåœ¨ç‰©ä½“æ„ŸçŸ¥æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚Rex-Omniåœ¨COCOå’ŒLVISç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ä¸ä¼ ç»Ÿçš„å›å½’æ¨¡å‹ç›¸åª²ç¾æˆ–è¶…è¶Šã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰é¡¹å…³é”®è®¾è®¡å®ç°äº†è¿™ä¸€ç›®æ ‡ï¼šä½¿ç”¨ç‰¹æ®Šæ ‡è®°è¡¨ç¤ºé‡åŒ–åæ ‡ã€æ„å»ºå¤šä¸ªæ•°æ®å¼•æ“ç”Ÿæˆé«˜è´¨é‡çš„æ ‡æ³¨æ•°æ®ï¼Œä»¥åŠé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ç»“åˆå¼ºåŒ–å­¦ä¹ åè®­ç»ƒã€‚Rex-Omniä¸ä»…åœ¨ä¼ ç»Ÿæ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¿˜å…·å¤‡å¤šç§è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¿›è¡Œç‰©ä½“æŒ‡å‘ã€è§†è§‰æç¤ºç­‰å¤šç§åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12399', 'title': 'A Survey of Vibe Coding with Large Language Models', 'url': 'https://huggingface.co/papers/2510.12399', 'abstract': 'The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed "Vibe Coding" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension. Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration. To address this gap, this survey provides the first comprehensive and systematic review of Vibe Coding with large language models, establishing both theoretical foundations and practical frameworks for this transformative development approach. Drawing from systematic analysis of over 1000 research papers, we survey the entire vibe coding ecosystem, examining critical infrastructure components including LLMs for coding, LLM-based coding agent, development environment of coding agent, and feedback mechanisms. We first introduce Vibe Coding as a formal discipline by formalizing it through a Constrained Markov Decision Process that captures the dynamic triadic relationship among human developers, software projects, and coding agents. Building upon this theoretical foundation, we then synthesize existing practices into five distinct development models: Unconstrained Automation, Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and Context-Enhanced Models, thus providing the first comprehensive taxonomy in this domain. Critically, our analysis reveals that successful Vibe Coding depends not merely on agent capabilities but on systematic context engineering, well-established development environments, and human-agent collaborative development models.', 'score': 30, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '45776dc32839b14f', 'authors': ['Yuyao Ge', 'Lingrui Mei', 'Zenghao Duan', 'Tianhao Li', 'Yujia Zheng', 'Yiwei Wang', 'Lexin Wang', 'Jiayu Yao', 'Tianyu Liu', 'Yujun Cai', 'Baolong Bi', 'Fangda Guo', 'Jiafeng Guo', 'Shenghua Liu', 'Xueqi Cheng'], 'affiliations': ['Duke University', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Peking University', 'University of California, Merced', 'University of Queensland'], 'pdf_title_img': 'assets/pdf/title_img/2510.12399.jpg', 'data': {'categories': ['#multimodal', '#training', '#survey', '#agi', '#agents', '#rl'], 'emoji': 'ğŸµ', 'ru': {'title': 'Vibe Coding: ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚, Ğ° Ğ½Ğµ Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ ĞºĞ¾Ğ´', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ "Vibe Coding", Ğ³Ğ´Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ´Ğ°, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ AI, Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸, Ğ° Ğ½Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· Constrained Markov Decision Process, Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼, Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ¼ Ğ¸ coding-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ±Ğ¾Ğ»ĞµĞµ 1000 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ñ‹ Ğ¿ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¿ĞµÑ… Vibe Coding Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ½Ğ¾ Ğ¸ Ğ¾Ñ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Empowering Developers with Vibe Coding: A New Era of AI Collaboration', 'desc': "This paper explores a new approach to software development called 'Vibe Coding', which leverages large language models (LLMs) to assist in coding tasks. Instead of developers understanding every line of code, they validate AI-generated code by observing the outcomes it produces. The study reviews over 1000 research papers to create a comprehensive framework for Vibe Coding, identifying key components like coding agents and feedback mechanisms. It also introduces a formal model to describe the interactions between developers and AI, highlighting that successful implementation relies on effective collaboration and context management."}, 'zh': {'title': 'Vibe Codingï¼šäººæœºåä½œçš„æ–°ç¯‡ç« ', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä¿ƒä½¿äº†ä»ä»£ç ç”Ÿæˆè¾…åŠ©åˆ°è‡ªä¸»ç¼–ç ä»£ç†çš„è½¬å˜ï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„å¼€å‘æ–¹æ³•è®ºï¼Œç§°ä¸ºâ€œVibe Codingâ€ã€‚å°½ç®¡è¿™ä¸€æ–°å…´èŒƒå¼å…·æœ‰å˜é©æ½œåŠ›ï¼Œä½†å…¶æœ‰æ•ˆæ€§ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œå®è¯ç ”ç©¶æ˜¾ç¤ºå‡ºæ„æƒ³ä¸åˆ°çš„ç”Ÿäº§åŠ›æŸå¤±å’Œäººæœºåä½œçš„åŸºæœ¬æŒ‘æˆ˜ã€‚æœ¬æ–‡æä¾›äº†å¯¹Vibe Codingçš„é¦–æ¬¡å…¨é¢ç³»ç»Ÿè¯„å®¡ï¼Œå»ºç«‹äº†ç†è®ºåŸºç¡€å’Œå®è·µæ¡†æ¶ï¼Œå¹¶åˆ†æäº†è¶…è¿‡1000ç¯‡ç ”ç©¶è®ºæ–‡ï¼Œæ¢è®¨äº†Vibe Codingç”Ÿæ€ç³»ç»Ÿçš„å…³é”®åŸºç¡€è®¾æ–½ç»„ä»¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒæˆåŠŸçš„Vibe Codingä¸ä»…ä¾èµ–äºä»£ç†çš„èƒ½åŠ›ï¼Œè¿˜éœ€è¦ç³»ç»Ÿçš„ä¸Šä¸‹æ–‡å·¥ç¨‹ã€è‰¯å¥½çš„å¼€å‘ç¯å¢ƒå’Œäººæœºåä½œå¼€å‘æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12747', 'title': 'FlashVSR: Towards Real-Time Diffusion-Based Streaming Video\n  Super-Resolution', 'url': 'https://huggingface.co/papers/2510.12747', 'abstract': 'Diffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical by achieving efficiency, scalability, and real-time performance. To this end, we propose FlashVSR, the first diffusion-based one-step streaming framework towards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408 videos on a single A100 GPU by combining three complementary innovations: (i) a train-friendly three-stage distillation pipeline that enables streaming super-resolution, (ii) locality-constrained sparse attention that cuts redundant computation while bridging the train-test resolution gap, and (iii) a tiny conditional decoder that accelerates reconstruction without sacrificing quality. To support large-scale training, we also construct VSR-120K, a new dataset with 120k videos and 180k images. Extensive experiments show that FlashVSR scales reliably to ultra-high resolutions and achieves state-of-the-art performance with up to 12x speedup over prior one-step diffusion VSR models. We will release the code, pretrained models, and dataset to foster future research in efficient diffusion-based VSR.', 'score': 29, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'a1e0314eef58561f', 'authors': ['Junhao Zhuang', 'Shi Guo', 'Xin Cai', 'Xiaohui Li', 'Yihao Liu', 'Chun Yuan', 'Tianfan Xue'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.12747.jpg', 'data': {'categories': ['#video', '#open_source', '#inference', '#training', '#dataset', '#diffusion'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ super-resolution Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'FlashVSR â€” Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ super-resolution Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ°Ñ ÑĞ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ 17 FPS Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ²Ğ¿Ğ»Ğ¾Ñ‚ÑŒ Ğ´Ğ¾ 768x1408 Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ GPU A100. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ streaming Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VSR-120K Ñ 120 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 180 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ² 12 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ one-step Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ VSR.'}, 'en': {'title': 'FlashVSR: Real-Time Video Super-Resolution with Diffusion Models', 'desc': 'This paper introduces FlashVSR, a novel framework that enhances video super-resolution (VSR) using diffusion models while addressing challenges like latency and computation. FlashVSR achieves real-time performance by implementing a three-stage distillation pipeline, which allows for efficient streaming of super-resolution. It also utilizes locality-constrained sparse attention to minimize unnecessary computations and improve generalization across different resolutions. The framework is supported by a new large-scale dataset, VSR-120K, and demonstrates significant speed improvements and high-quality results compared to existing models.'}, 'zh': {'title': 'FlashVSRï¼šå®æ—¶è§†é¢‘è¶…åˆ†è¾¨ç‡çš„æ–°çªç ´', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ä¿®å¤æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨å®é™…è§†é¢‘è¶…åˆ†è¾¨ç‡ï¼ˆVSRï¼‰ä¸­åº”ç”¨ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†FlashVSRï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºæ‰©æ•£çš„ä¸€æ­¥æµåª’ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å®æ—¶VSRã€‚FlashVSRé€šè¿‡ä¸‰é¡¹åˆ›æ–°å®ç°äº†é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªA100 GPUä¸Šä»¥çº¦17å¸§æ¯ç§’çš„é€Ÿåº¦å¤„ç†768x1408çš„è§†é¢‘ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†VSR-120Kï¼Œä»¥æ”¯æŒå¤§è§„æ¨¡è®­ç»ƒï¼Œå¹¶å±•ç¤ºäº†FlashVSRåœ¨è¶…é«˜åˆ†è¾¨ç‡ä¸‹çš„å¯é æ‰©å±•æ€§å’Œé¢†å…ˆæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12773', 'title': 'Dr.LLM: Dynamic Layer Routing in LLMs', 'url': 'https://huggingface.co/papers/2510.12773', 'abstract': 'Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. We introduce Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights.', 'score': 25, 'issue_id': 6422, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'a2001c3f3c9bbeab', 'authors': ['Ahmed Heakl', 'Martin Gubri', 'Salman Khan', 'Sangdoo Yun', 'Seong Joon Oh'], 'affiliations': ['MBZUAI', 'NAVER AI Lab', 'Paramter Lab', 'Tubingen AI Center', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2510.12773.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#optimization', '#training', '#inference'], 'emoji': 'ğŸ§­', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ñ‘Ğ²: LLM ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Dr.LLM â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ñ‘Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑĞµ ÑĞ»Ğ¾Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ Ñ€Ğ¾ÑƒÑ‚ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµÑˆĞ°ÑÑ‚: Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ±Ğ»Ğ¾Ğº, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ÑŒ. Ğ Ğ¾ÑƒÑ‚ĞµÑ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ñ ÑĞ²Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Monte Carlo Tree Search (MCTS), Ğ½Ğ°Ñ…Ğ¾Ğ´Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ñ‘Ğ² Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ¾ +3.4% Ğ¿Ñ€Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 5 ÑĞ»Ğ¾Ñ‘Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Dynamic Layer Routing for Efficient and Accurate LLMs', 'desc': 'The paper presents Dr.LLM, a framework designed to enhance the efficiency of Large Language Models (LLMs) by dynamically routing layers during inference. Instead of processing every token through all layers, Dr.LLM uses lightweight routers that can decide to skip, execute, or repeat layers based on the complexity of the query. This approach is trained with explicit supervision using Monte Carlo Tree Search, allowing it to maintain or improve accuracy while reducing computational costs. The results show that Dr.LLM not only saves resources but also generalizes well to various tasks, outperforming previous methods in both efficiency and accuracy.'}, 'zh': {'title': 'Dr.LLMï¼šé«˜æ•ˆçµæ´»çš„åŠ¨æ€å±‚è·¯ç”±', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†æ¯ä¸ªæ ‡è®°æ—¶ä¼šç»è¿‡æ‰€æœ‰å˜æ¢å™¨å±‚ï¼Œè¿™å¯¼è‡´åœ¨ç®€å•æŸ¥è¯¢æ—¶è®¡ç®—èµ„æºæµªè´¹ï¼Œè€Œåœ¨éœ€è¦æ›´æ·±å±‚æ¬¡æ¨ç†çš„å¤æ‚æŸ¥è¯¢æ—¶çµæ´»æ€§ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†Dr.LLMï¼Œè¿™æ˜¯ä¸€ç§åŠ¨æ€å±‚è·¯ç”±æ¡†æ¶ï¼Œå¯ä»¥ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ·»åŠ è½»é‡çº§çš„æ¯å±‚è·¯ç”±å™¨ï¼Œå†³å®šè·³è¿‡ã€æ‰§è¡Œæˆ–é‡å¤æŸä¸ªæ¨¡å—ã€‚é€šè¿‡ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œæ˜¾å¼ç›‘ç£è®­ç»ƒï¼ŒDr.LLMèƒ½å¤Ÿåœ¨è®¡ç®—é¢„ç®—å†…è·å¾—é«˜è´¨é‡çš„å±‚é…ç½®ï¼Œä»è€Œåœ¨ä¿æŒæˆ–æé«˜å‡†ç¡®æ€§çš„åŒæ—¶æé«˜æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDr.LLMåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨è®¡ç®—èµ„æºä½¿ç”¨ä¸Šè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11057', 'title': 'Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models', 'url': 'https://huggingface.co/papers/2510.11057', 'abstract': "Diffusion models have achieved remarkable success as generative models. However, even a well-trained model can accumulate errors throughout the generation process. These errors become particularly problematic when arbitrary guidance is applied to steer samples toward desired properties, which often breaks sample fidelity. In this paper, we propose a general solution to address the off-manifold phenomenon observed in diffusion models. Our approach leverages a time predictor to estimate deviations from the desired data manifold at each timestep, identifying that a larger time gap is associated with reduced generation quality. We then design a novel guidance mechanism, `Temporal Alignment Guidance' (TAG), attracting the samples back to the desired manifold at every timestep during generation. Through extensive experiments, we demonstrate that TAG consistently produces samples closely aligned with the desired manifold at each timestep, leading to significant improvements in generation quality across various downstream tasks.", 'score': 25, 'issue_id': 6425, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '5ec8a5cf8c358245', 'authors': ['Youngrok Park', 'Hojung Jung', 'Sangmin Bae', 'Se-Young Yun'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.11057.jpg', 'data': {'categories': ['#training', '#optimization', '#diffusion', '#multimodal', '#cv'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ guidance Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Temporal Alignment Guidance (TAG) Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑĞ¼Ğ¿Ğ»Ñ‹ Ğ½Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TAG Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑƒĞ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ ÑÑĞ¼Ğ¿Ğ»Ñ‹ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾ Ğº Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Aligning Generative Samples with Temporal Guidance', 'desc': "This paper addresses the issue of error accumulation in diffusion models during the sample generation process. When arbitrary guidance is applied, it can lead to a loss of fidelity in the generated samples. The authors introduce a method called 'Temporal Alignment Guidance' (TAG), which uses a time predictor to correct deviations from the desired data manifold at each generation step. Their experiments show that TAG significantly enhances the quality of generated samples by ensuring they remain aligned with the target manifold throughout the process."}, 'zh': {'title': 'æ—¶é—´å¯¹é½å¼•å¯¼ï¼šæå‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆè´¨é‡çš„å…³é”®', 'desc': 'æ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå³ä½¿æ˜¯ç»è¿‡è‰¯å¥½è®­ç»ƒçš„æ¨¡å‹ä¹Ÿå¯èƒ½ç§¯ç´¯é”™è¯¯ã€‚è¿™äº›é”™è¯¯åœ¨åº”ç”¨ä»»æ„å¼•å¯¼ä»¥è°ƒæ•´æ ·æœ¬å±æ€§æ—¶å°¤ä¸ºä¸¥é‡ï¼Œå¸¸å¸¸å¯¼è‡´æ ·æœ¬çš„çœŸå®æ€§ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹ä¸­è§‚å¯Ÿåˆ°çš„ç¦»æ•£ç°è±¡ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„å¼•å¯¼æœºåˆ¶â€”â€”æ—¶é—´å¯¹é½å¼•å¯¼ï¼ˆTAGï¼‰ï¼Œåœ¨ç”Ÿæˆçš„æ¯ä¸ªæ—¶é—´æ­¥å°†æ ·æœ¬å¸å¼•å›æœŸæœ›çš„æ•°æ®æµå½¢ï¼Œä»è€Œæ˜¾è‘—æé«˜ç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12693', 'title': 'ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning\n  and Online Reinforcement Learning', 'url': 'https://huggingface.co/papers/2510.12693', 'abstract': 'Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary knowledge and skills to succeed. To bridge this gap, we present Embodied Reasoning Agent (ERA), a two-stage framework that integrates prior knowledge learning and online reinforcement learning (RL). The first stage, Embodied Prior Learning, distills foundational knowledge from three types of data: (1) Trajectory-Augmented Priors, which enrich existing trajectory data with structured reasoning generated by stronger models; (2) Environment-Anchored Priors, which provide in-environment knowledge and grounding supervision; and (3) External Knowledge Priors, which transfer general knowledge from out-of-environment datasets. In the second stage, we develop an online RL pipeline that builds on these priors to further enhance agent performance. To overcome the inherent challenges in agent RL, including long horizons, sparse rewards, and training instability, we introduce three key designs: self-summarization for context management, dense reward shaping, and turn-level policy optimization. Extensive experiments on both high-level planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate that ERA-3B surpasses both prompting-based large models and previous training-based baselines. Specifically, it achieves overall improvements of 8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits strong generalization to unseen tasks. Overall, ERA offers a practical path toward scalable embodied intelligence, providing methodological insights for future embodied AI systems.', 'score': 20, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'becfc2b44cce8996', 'authors': ['Hanyang Chen', 'Mark Zhao', 'Rui Yang', 'Qinwei Ma', 'Ke Yang', 'Jiarui Yao', 'Kangrui Wang', 'Hao Bai', 'Zhenhailong Wang', 'Rui Pan', 'Mengchao Zhang', 'Jose Barreiros', 'Aykut Onol', 'ChengXiang Zhai', 'Heng Ji', 'Manling Li', 'Huan Zhang', 'Tong Zhang'], 'affiliations': ['Northwestern University', 'Toyota Research Institute', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2510.12693.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#transfer_learning', '#agents', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ERA â€” Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… vision language models Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²: Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ reinforcement learning Ñ ÑĞ°Ğ¼Ğ¾ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ERA-3B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4o Ğ½Ğ° 8.4% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ° 19.4% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering VLMs with ERA: Bridging Knowledge and Action', 'desc': "This paper introduces the Embodied Reasoning Agent (ERA), a two-stage framework designed to enhance the capabilities of vision language models (VLMs) in complex environments. The first stage, Embodied Prior Learning, gathers foundational knowledge from various data sources to improve the model's understanding and reasoning. The second stage employs online reinforcement learning (RL) to refine the agent's performance using the acquired knowledge. The proposed methods, including self-summarization and dense reward shaping, address common challenges in RL, leading to significant performance improvements in both planning and control tasks compared to existing models."}, 'zh': {'title': 'å…·èº«æ™ºèƒ½çš„å®ç”¨è·¯å¾„', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œå…·èº«æ¨ç†ä»£ç†â€ï¼ˆERAï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚ERAé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„å­¦ä¹ ï¼Œé¦–å…ˆä»å¤šç§æ•°æ®ä¸­æå–åŸºç¡€çŸ¥è¯†ï¼Œç„¶ååˆ©ç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æå‡ä»£ç†çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä»£ç†å¼ºåŒ–å­¦ä¹ ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚é•¿æ—¶é—´è·¨åº¦ã€ç¨€ç–å¥–åŠ±å’Œè®­ç»ƒä¸ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒERAåœ¨é«˜å±‚è§„åˆ’å’Œä½å±‚æ§åˆ¶ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰çš„å¤§å‹æ¨¡å‹å’Œè®­ç»ƒåŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨å…·èº«æ™ºèƒ½é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12784', 'title': 'SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2510.12784', 'abstract': "Recently, remarkable progress has been made in Unified Multimodal Models (UMMs), which integrate vision-language generation and understanding capabilities within a single framework. However, a significant gap exists where a model's strong visual understanding often fails to transfer to its visual generation. A model might correctly understand an image based on user instructions, yet be unable to generate a faithful image from text prompts. This phenomenon directly raises a compelling question: Can a model achieve self-improvement by using its understanding module to reward its generation module? To bridge this gap and achieve self-improvement, we introduce SRUM, a self-rewarding post-training framework that can be directly applied to existing UMMs of various designs. SRUM creates a feedback loop where the model's own understanding module acts as an internal ``evaluator'', providing corrective signals to improve its generation module, without requiring additional human-labeled data. To ensure this feedback is comprehensive, we designed a global-local dual reward system. To tackle the inherent structural complexity of images, this system offers multi-scale guidance: a global reward ensures the correctness of the overall visual semantics and layout, while a local reward refines fine-grained, object-level fidelity. SRUM leads to powerful capabilities and shows strong generalization, boosting performance on T2I-CompBench from 82.18 to 88.37 and on T2I-ReasonBench from 43.82 to 46.75. Overall, our work establishes a powerful new paradigm for enabling a UMMs' understanding module to guide and enhance its own generation via self-rewarding.", 'score': 15, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '97647c923a10f143', 'authors': ['Weiyang Jin', 'Yuwei Niu', 'Jiaqi Liao', 'Chengqi Duan', 'Aoxue Li', 'Shenghua Gao', 'Xihui Liu'], 'affiliations': ['Noahs Ark Lab, Huawei'], 'pdf_title_img': 'assets/pdf/title_img/2510.12784.jpg', 'data': {'categories': ['#training', '#optimization', '#multimodal', '#transfer_learning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºÑƒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SRUM, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Empowering Models to Self-Improve: SRUM Framework for UMMs', 'desc': "This paper presents SRUM, a self-rewarding framework designed to improve Unified Multimodal Models (UMMs) by enhancing the relationship between visual understanding and visual generation. The framework allows the model's understanding module to evaluate and provide feedback to its generation module, creating a self-improvement loop without needing extra human-labeled data. SRUM employs a dual reward system that offers both global and local feedback, ensuring that the generated images are semantically correct and detailed. The results show significant performance improvements on benchmark tasks, demonstrating the effectiveness of this self-guided approach in multimodal learning."}, 'zh': {'title': 'è‡ªæˆ‘å¥–åŠ±ï¼Œæå‡ç”Ÿæˆèƒ½åŠ›ï¼', 'desc': 'æœ€è¿‘ï¼Œç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ˆUMMsï¼‰åœ¨è§†è§‰-è¯­è¨€ç”Ÿæˆå’Œç†è§£èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæ¨¡å‹çš„å¼ºè§†è§‰ç†è§£å¾€å¾€æ— æ³•æœ‰æ•ˆè½¬ç§»åˆ°è§†è§‰ç”Ÿæˆä¸Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SRUMï¼Œä¸€ä¸ªè‡ªæˆ‘å¥–åŠ±çš„åè®­ç»ƒæ¡†æ¶ï¼Œå¯ä»¥ç›´æ¥åº”ç”¨äºå„ç§è®¾è®¡çš„UMMsã€‚SRUMé€šè¿‡åˆ›å»ºåé¦ˆå¾ªç¯ï¼Œä½¿æ¨¡å‹çš„ç†è§£æ¨¡å—ä½œä¸ºå†…éƒ¨â€œè¯„ä¼°è€…â€ï¼Œä¸ºç”Ÿæˆæ¨¡å—æä¾›çº æ­£ä¿¡å·ï¼Œä»è€Œå®ç°è‡ªæˆ‘æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12789', 'title': 'UniFusion: Vision-Language Model as Unified Encoder in Image Generation', 'url': 'https://huggingface.co/papers/2510.12789', 'abstract': "Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large unified models jointly for text and image generation, which demands substantial computational resources and large-scale data, limiting its accessibility.We present UniFusion, a diffusion-based generative model conditioned on a frozen large vision-language model (VLM) that serves as a unified multimodal encoder. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high level semantics and low level details from text and visual tokens of a frozen VLM to condition a diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithful transfer of visual information from VLM to the diffusion model which is key for editing. We propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI), which conditions a diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. VERIFI combines the alignment of the conditioning distribution with the VLM's reasoning capabilities for increased capabilities and flexibility at inference. In addition, finetuning on editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities. Our model when trained on single image editing, zero-shot generalizes to multiple image references further motivating the unified encoder design of UniFusion.", 'score': 14, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '68fefbfc1c762f27', 'authors': ['Kevin Li', 'Manuel Brack', 'Sudeep Katakol', 'Hareesh Ravi', 'Ajinkya Kale'], 'affiliations': ['Adobe Applied Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.12789.jpg', 'data': {'categories': ['#cv', '#multimodal', '#reasoning', '#training', '#transfer_learning', '#diffusion'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'UniFusion â€” ÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½ÑƒÑ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (VLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Layerwise Attention Pooling (LAP) Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸Ğ· Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² VLM Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ VERIFI Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ VLM Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ reasoning-ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½strĞ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ transfer Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ zero-shot Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unified Multimodal Generation with UniFusion', 'desc': "The paper introduces UniFusion, a novel diffusion-based generative model that utilizes a frozen large vision-language model (VLM) as a unified multimodal encoder. This approach addresses the limitations of existing architectures that rely on separate encoders for images and text, which restricts their ability to perform cross-modal reasoning. The core innovation is the Layerwise Attention Pooling (LAP) mechanism, which effectively extracts both high-level semantics and low-level details from the VLM, enhancing text-image alignment and enabling better visual information transfer. Additionally, the proposed VLM-Enabled Rewriting Injection with Flexible Inference (VERIFI) improves the model's flexibility and generalization capabilities, allowing it to perform well on various editing tasks with minimal additional training."}, 'zh': {'title': 'ç»Ÿä¸€ç¼–ç å™¨ï¼Œè·¨æ¨¡æ€ç”Ÿæˆçš„æœªæ¥', 'desc': 'å°½ç®¡è§†è§‰ç”Ÿæˆçš„æœ€æ–°è¿›å±•æ˜¾è‘—ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ¶æ„ä»ä¾èµ–äºç‹¬ç«‹çš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ã€‚è¿™ç§åˆ†ç¦»é™åˆ¶äº†æ‰©æ•£æ¨¡å‹åœ¨è·¨æ¨¡æ€æ¨ç†å’ŒçŸ¥è¯†è½¬ç§»æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºçš„UniFusionæ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œåˆ©ç”¨å†»ç»“çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºç»Ÿä¸€çš„å¤šæ¨¡æ€ç¼–ç å™¨ã€‚é€šè¿‡å±‚çº§æ³¨æ„åŠ›æ± åŒ–æœºåˆ¶ï¼ˆLAPï¼‰ï¼ŒUniFusionèƒ½å¤Ÿæå–æ–‡æœ¬å’Œè§†è§‰æ ‡è®°çš„é«˜å±‚è¯­ä¹‰å’Œä½å±‚ç»†èŠ‚ï¼Œä»è€Œåœ¨ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­å®ç°æ›´å¥½çš„æ–‡æœ¬-å›¾åƒå¯¹é½å’Œä¿¡æ¯è½¬ç§»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12635', 'title': 'Memory as Action: Autonomous Context Curation for Long-Horizon Agentic\n  Tasks', 'url': 'https://huggingface.co/papers/2510.12635', 'abstract': "Large Language Models face challenges in long-horizon agentic tasks as their constrained memory is easily overwhelmed by distracting or irrelevant context. Existing working memory methods typically rely on external, heuristic mechanisms that are decoupled from the agent's core policy. In this work, we reframe working memory management as a learnable, intrinsic capability. We propose a novel framework, Memory-as-Action, where an agent actively manages its working memory by executing explicit editing operations as part of a unified policy. This formulation allows an agent, trained via reinforcement learning, to balance memory curation against long-term task objectives under given resource constraints. However, such memory editing actions break the standard assumption of a continuously growing prefix in LLM interactions, leading to what we call trajectory fractures. These non-prefix changes disrupt the causal continuity required by standard policy gradient methods, making those methods inapplicable. To address this, we propose a new algorithm, Dynamic Context Policy Optimization, which enables stable end-to-end reinforcement learning by segmenting trajectories at memory action points and applying trajectory-level advantages to the resulting action segments. Our results demonstrate that jointly optimizing for task reasoning and memory management in an end-to-end fashion not only reduces overall computational consumption but also improves task performance, driven by adaptive context curation strategies tailored to the model's intrinsic capabilities.", 'score': 12, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '30ef35f2c0624828', 'authors': ['Yuxiang Zhang', 'Jiangming Shu', 'Ye Ma', 'Xueyuan Lin', 'Shangxi Wu', 'Jitao Sang'], 'affiliations': ['Hithink Research', 'Huawei Noahs Ark Lab', 'School of Computer Science and Technology, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.12635.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#training', '#optimization', '#agents', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ğ¼ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ: LLM ÑƒÑ‡Ğ°Ñ‚ÑÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¾Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ğ³Ğ´Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Memory-as-Action, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ²Ğ¾ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ñ‡Ğ°ÑÑ‚ÑŒ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Dynamic Context Policy Optimization, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ "Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ¾Ğ² Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸", Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Empowering LLMs with Learnable Memory Management', 'desc': 'This paper addresses the limitations of Large Language Models (LLMs) in handling long-term tasks due to their limited memory capacity. It introduces a new approach called Memory-as-Action, where the model learns to manage its working memory through specific editing actions integrated into its decision-making policy. The authors propose a novel algorithm, Dynamic Context Policy Optimization, to overcome challenges posed by non-continuous memory updates, which disrupt standard reinforcement learning methods. The findings show that optimizing memory management alongside task reasoning enhances performance and reduces computational costs.'}, 'zh': {'title': 'è®°å¿†ç®¡ç†ä¸ä»»åŠ¡æ¨ç†çš„ç»Ÿä¸€ä¼˜åŒ–', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æœŸä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç”±äºå†…å­˜é™åˆ¶è€Œå¯¼è‡´çš„å¹²æ‰°é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºâ€œè®°å¿†ä½œä¸ºè¡ŒåŠ¨â€ï¼Œä½¿ä»£ç†èƒ½å¤Ÿé€šè¿‡æ‰§è¡Œæ˜¾å¼ç¼–è¾‘æ“ä½œæ¥ä¸»åŠ¨ç®¡ç†å…¶å·¥ä½œè®°å¿†ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä»£ç†å¯ä»¥åœ¨èµ„æºé™åˆ¶ä¸‹å¹³è¡¡è®°å¿†ç®¡ç†ä¸é•¿æœŸä»»åŠ¡ç›®æ ‡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ï¼ŒåŠ¨æ€ä¸Šä¸‹æ–‡ç­–ç•¥ä¼˜åŒ–ï¼Œè§£å†³äº†ä¼ ç»Ÿç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨å¤„ç†éå‰ç¼€å˜åŒ–æ—¶çš„å±€é™æ€§ï¼Œä»è€Œå®ç°ç¨³å®šçš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11683', 'title': 'Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion\n  Large Language Models', 'url': 'https://huggingface.co/papers/2510.11683', 'abstract': 'Boundary-Guided Policy Optimization (BGPO) improves reinforcement learning for diffusion large language models by efficiently approximating likelihoods with a memory-efficient lower bound, enhancing performance in tasks like math problem solving, code generation, and planning.  \t\t\t\t\tAI-generated summary \t\t\t\t A key challenge in applying reinforcement learning (RL) to diffusion large language models (dLLMs) lies in the intractability of their likelihood functions, which are essential for the RL objective, necessitating corresponding approximation in each training step. While existing methods approximate the log-likelihoods by their evidence lower bounds (ELBOs) via customized Monte Carlo (MC) sampling, the forward computational graphs of all MC samples need to be retained for the gradient computation of non-linear terms in the RL objective, resulting in significant memory overhead. This constraint restricts feasible sample sizes, leading to imprecise likelihood approximations and ultimately distorting the RL objective. To overcome this limitation, we propose Boundary-Guided Policy Optimization (BGPO), a memory-efficient RL algorithm that maximizes a specially constructed lower bound of the ELBO-based objective. This lower bound is carefully designed to satisfy two key properties: (1) Linearity: it is formulated in a linear sum where each term depends only on a single MC sample, thereby enabling gradient accumulation across samples and ensuring constant memory usage; (2) Equivalence: Both the value and gradient of this lower bound are equal to those of the ELBO-based objective in on-policy training, making it also an effective approximation for the original RL objective. These properties allow BGPO to adopt a large MC sample size, resulting in more accurate likelihood approximations and improved RL objective estimation, which in turn leads to enhanced performance. Experiments show that BGPO significantly outperforms previous RL algorithms for dLLMs in math problem solving, code generation, and planning tasks.', 'score': 12, 'issue_id': 6421, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '4eb85eafb417ade6', 'authors': ['Nianyi Lin', 'Jiajie Zhang', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.11683.jpg', 'data': {'categories': ['#games', '#rlhf', '#training', '#optimization', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ BGPO Ğ´Ğ»Ñ reinforcement learning Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Monte Carlo ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ ÑÑ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²ÑĞµÑ… ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ². BGPO Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ½Ğ¸Ğ¶Ğ½ÑÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ ELBO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ RL-Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficient Reinforcement Learning for Language Models with BGPO', 'desc': 'Boundary-Guided Policy Optimization (BGPO) is a novel reinforcement learning algorithm designed to enhance the performance of diffusion large language models (dLLMs) by efficiently approximating likelihoods. It addresses the challenge of high memory usage in existing methods that rely on Monte Carlo sampling for estimating log-likelihoods, which can distort the RL objective. BGPO introduces a memory-efficient lower bound that maintains linearity and equivalence with the original objective, allowing for larger sample sizes and more accurate likelihood approximations. As a result, BGPO demonstrates significant improvements in tasks such as math problem solving, code generation, and planning compared to previous RL approaches.'}, 'zh': {'title': 'è¾¹ç•Œå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼šæå‡å¼ºåŒ–å­¦ä¹ æ€§èƒ½çš„åˆ©å™¨', 'desc': 'è¾¹ç•Œå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆBGPOï¼‰æ˜¯ä¸€ç§æ”¹è¿›å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºæ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ã€‚å®ƒé€šè¿‡é«˜æ•ˆåœ°è¿‘ä¼¼ä¼¼ç„¶å‡½æ•°çš„ä¸‹ç•Œï¼Œè§£å†³äº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å†…å­˜å¼€é”€è¿‡å¤§çš„é—®é¢˜ã€‚BGPOçš„è®¾è®¡ç¡®ä¿äº†çº¿æ€§å’Œç­‰ä»·æ€§ï¼Œä½¿å¾—åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­å¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„æ ·æœ¬é‡ï¼Œä»è€Œæé«˜äº†ä¼¼ç„¶è¿‘ä¼¼çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBGPOåœ¨æ•°å­¦é—®é¢˜è§£å†³ã€ä»£ç ç”Ÿæˆå’Œè§„åˆ’ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¹‹å‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11602', 'title': 'Deconstructing Attention: Investigating Design Principles for Effective\n  Language Modeling', 'url': 'https://huggingface.co/papers/2510.11602', 'abstract': "Systematic analysis of attention mechanisms in Transformer models shows that token mixing is essential, while other aspects like sequence dependency and mathematical form can be relaxed or interleaved to maintain performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The success of Transformer language models is widely credited to their dot-product attention mechanism, which interweaves a set of key design principles: mixing information across positions (enabling multi-token interactions), sequence-dependent activations (where attention weights adapt to each input), a specific mathematical form (dot-product similarities plus softmax weighting), and coupling of queries and keys to evolving hidden states (grounding attention in the current layer). However, the necessity of each of these principles remains largely untested. In this work, we systematically deconstruct attention by designing controlled variants that selectively relax these principles, applied both uniformly across all layers and in hybrid architectures where only some layers retain standard attention. Our empirical analysis reveals that mechanisms for mixing tokens are indispensable, as their absence collapses models to near-random behavior, while the exact mathematical form and sequence dependency can be substantially relaxed, especially when preserved in just a subset of layers. Surprisingly, even variants that fail in isolation can achieve robust performance when interleaved with standard attention, highlighting a cooperative effect. These findings deepen our understanding of what truly underpins attention's effectiveness and open new avenues for simplifying language models without sacrificing performance.", 'score': 12, 'issue_id': 6434, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': 'a83dc866ac4e61b8', 'authors': ['Huiyin Xue', 'Nafise Sadat Moosavi', 'Nikolaos Aletras'], 'affiliations': ['School of Computer Science, University of Sheffield, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2510.11602.jpg', 'data': {'categories': ['#math', '#architecture', '#optimization', '#interpretability', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¡Ğ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ°Ğ¶Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ° attention', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Transformer Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ€Ğ°Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ ĞµĞ³Ğ¾ Ğ½Ğ° ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ - Ğ±ĞµĞ· Ğ½ĞµĞ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ° dot-product attention, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ĞµÑĞ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ Ñ…Ğ¾Ñ‚Ñ Ğ±Ñ‹ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ñ‘Ğ². Ğ”Ğ°Ğ¶Ğµ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ attention, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… language models.'}, 'en': {'title': 'Token Mixing is Key in Transformer Attention!', 'desc': "This paper investigates the role of attention mechanisms in Transformer models, focusing on how different design principles contribute to their performance. The authors find that mixing information across tokens is crucial, while other aspects like sequence dependency and the specific mathematical formulation can be relaxed without significantly affecting results. They conduct experiments with modified attention mechanisms, showing that even less effective variants can perform well when combined with standard attention. This research enhances our understanding of attention's effectiveness and suggests ways to simplify language models while maintaining their performance."}, 'zh': {'title': 'ä»¤ç‰Œæ··åˆæ˜¯Transformeræ¨¡å‹çš„å…³é”®', 'desc': 'è¿™ç¯‡è®ºæ–‡ç³»ç»Ÿåˆ†æäº†Transformeræ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå‘ç°ä»¤ç‰Œæ··åˆæ˜¯è‡³å…³é‡è¦çš„ã€‚è™½ç„¶åºåˆ—ä¾èµ–æ€§å’Œæ•°å­¦å½¢å¼å¯ä»¥æ”¾å®½æˆ–äº¤é”™ï¼Œä½†ä¿æŒæ€§èƒ½ä»ç„¶æœ‰æ•ˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œç¼ºå°‘ä»¤ç‰Œæ··åˆæœºåˆ¶ä¼šå¯¼è‡´æ¨¡å‹è¡¨ç°æ¥è¿‘éšæœºï¼Œè€Œæ•°å­¦å½¢å¼å’Œåºåˆ—ä¾èµ–æ€§å¯ä»¥åœ¨æŸäº›å±‚ä¸­å¤§å¹…æ”¾å®½ã€‚è®ºæ–‡çš„å‘ç°ä¸ºç®€åŒ–è¯­è¨€æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01171', 'title': 'Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity', 'url': 'https://huggingface.co/papers/2510.01171', 'abstract': "Typicality bias in preference data causes mode collapse in LLMs, and Verbalized Sampling is introduced as a prompting strategy to enhance diversity without compromising accuracy or safety.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., ``Generate 5 jokes about coffee and their corresponding probabilities''). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.", 'score': 11, 'issue_id': 6433, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'e7b0d88de08244ab', 'authors': ['Jiayi Zhang', 'Simon Yu', 'Derek Chong', 'Anthony Sicilia', 'Michael R. Tomz', 'Christopher D. Manning', 'Weiyan Shi'], 'affiliations': ['Northeastern University', 'Stanford University', 'West Virginia University'], 'pdf_title_img': 'assets/pdf/title_img/2510.01171.jpg', 'data': {'categories': ['#hallucinations', '#data', '#inference', '#training', '#alignment', '#story_generation', '#synthetic'], 'emoji': 'ğŸ²', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹: Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ğµ, Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸Ğ·-Ğ·Ğ° ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ ĞµÑ‘ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Verbalized Sampling â€” Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ prompting-ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ¸Ñ… Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² 1.6-2.1 Ñ€Ğ°Ğ·Ğ° Ğ² Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ²Ñ‹Ğ³Ğ¾Ğ´Ñƒ Ğ¾Ñ‚ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Unlocking Diversity in LLMs with Verbalized Sampling', 'desc': 'This paper addresses the issue of mode collapse in large language models (LLMs), which occurs when models generate repetitive outputs due to typicality bias in preference data. Typicality bias leads annotators to prefer familiar text, limiting the diversity of generated responses. The authors propose a new prompting strategy called Verbalized Sampling (VS), which encourages models to express a range of possible outputs along with their probabilities. Experiments demonstrate that VS significantly enhances creative writing and other tasks by increasing output diversity while maintaining accuracy and safety.'}, 'zh': {'title': 'æ‰“ç ´æ¨¡å¼å´©æºƒï¼Œæå‡ç”Ÿæˆå¤šæ ·æ€§ï¼', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åå¥½æ•°æ®ä¸­çš„å…¸å‹æ€§åè§å¦‚ä½•å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‡ºç°æ¨¡å¼å´©æºƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ³¨é‡Šè€…å€¾å‘äºé€‰æ‹©ç†Ÿæ‚‰çš„æ–‡æœ¬ï¼Œè¿™ç§ç°è±¡æºäºè®¤çŸ¥å¿ƒç†å­¦çš„ç ”ç©¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºç­–ç•¥â€”â€”å£å¤´é‡‡æ ·ï¼ˆVerbalized Samplingï¼‰ï¼Œå¯ä»¥åœ¨ä¸å½±å“å‡†ç¡®æ€§å’Œå®‰å…¨æ€§çš„æƒ…å†µä¸‹å¢å¼ºç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå£å¤´é‡‡æ ·åœ¨åˆ›æ„å†™ä½œã€å¯¹è¯æ¨¡æ‹Ÿå’Œå¼€æ”¾å¼é—®ç­”ç­‰ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12225', 'title': 'HoneyBee: Data Recipes for Vision-Language Reasoners', 'url': 'https://huggingface.co/papers/2510.12225', 'abstract': 'Recent advances in vision-language models (VLMs) have made them highly effective at reasoning tasks. However, the principles underlying the construction of performant VL reasoning training datasets remain poorly understood. In this work, we introduce several data curation approaches and study their impacts on VL reasoning capabilities by carefully controlling training and evaluation setups. We analyze the effects of context (image and question pair) sources, implement targeted data interventions, and explore scaling up images, questions, and chain-of-thought (CoT) solutions. Our findings reveal that (a) context source strategies significantly affect VLM performance, (b) interventions such as auxiliary signals from image captions and the inclusion of text-only reasoning yield substantial gains, and (c) scaling all data dimensions (e.g., unique questions per image and unique CoTs per image-question pair) consistently improves reasoning capability. Motivated by these insights, we introduce HoneyBee, a large-scale, high-quality CoT reasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs trained with HoneyBee outperform state-of-the-art models across model sizes. For instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA model and the base model by 7.8% and 24.8%, respectively, on MathVerse. Furthermore, we propose a test-time scaling strategy that reduces decoding cost by 73% without sacrificing accuracy. Overall, this work presents improved strategies for VL reasoning dataset curation research.', 'score': 9, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'a1704fba84103a6e', 'authors': ['Hritik Bansal', 'Devandra Singh Sachan', 'Kai-Wei Chang', 'Aditya Grover', 'Gargi Ghosh', 'Wen-tau Yih', 'Ramakanth Pasunuru'], 'affiliations': ['FAIR at Meta', 'University of California Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2510.12225.jpg', 'data': {'categories': ['#cv', '#multimodal', '#reasoning', '#dataset', '#benchmark', '#data'], 'emoji': 'ğŸ', 'ru': {'title': 'HoneyBee: ĞšĞ°Ğº Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° (Ğ¿Ğ°Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ), Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ÑĞµÑ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¾Ğº Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ HoneyBee â€” Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 2.5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² chain-of-thought Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 350 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° HoneyBee, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ state-of-the-art Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ: Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, 3B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 7.8% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ¼ Ğ¸ Ğ½Ğ° 24.8% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MathVerse.'}, 'en': {'title': 'Enhancing VLM Reasoning with HoneyBee Dataset', 'desc': 'This paper explores how to create better training datasets for vision-language models (VLMs) to enhance their reasoning abilities. The authors introduce various data curation methods and analyze their effects on VLM performance, focusing on context sources and targeted data interventions. They find that using auxiliary signals and scaling data dimensions significantly boosts reasoning capabilities. The study culminates in the creation of HoneyBee, a large dataset that improves VLM performance, demonstrating substantial gains over existing models.'}, 'zh': {'title': 'æå‡è§†è§‰-è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°ç­–ç•¥', 'desc': 'æœ€è¿‘åœ¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ–¹é¢çš„è¿›å±•ä½¿å…¶åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œæ„å»ºé«˜æ•ˆVLMæ¨ç†è®­ç»ƒæ•°æ®é›†çš„åŸåˆ™ä»ç„¶ä¸å¤Ÿæ¸…æ™°ã€‚æœ¬æ–‡ä»‹ç»äº†å‡ ç§æ•°æ®æ•´ç†æ–¹æ³•ï¼Œå¹¶ç ”ç©¶äº†å®ƒä»¬å¯¹VLMæ¨ç†èƒ½åŠ›çš„å½±å“ï¼Œåˆ†æäº†ä¸Šä¸‹æ–‡æ¥æºã€æ•°æ®å¹²é¢„å’Œæ•°æ®è§„æ¨¡çš„æ•ˆæœã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä¸Šä¸‹æ–‡æ¥æºç­–ç•¥æ˜¾è‘—å½±å“VLMæ€§èƒ½ï¼Œæ•°æ®å¹²é¢„å’Œæ•°æ®è§„æ¨¡çš„å¢åŠ å‡èƒ½æœ‰æ•ˆæå‡æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12709', 'title': 'SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model', 'url': 'https://huggingface.co/papers/2510.12709', 'abstract': "Multimodal embedding models aim to yield informative unified representations that empower diverse cross-modal tasks. Despite promising developments in the evolution from CLIP-based dual-tower architectures to large vision-language models, prior works still face unavoidable challenges in real-world applications and business scenarios, such as the limited modality support, unstable training mechanisms, and industrial domain gaps. In this work, we introduce SAIL-Embedding, an omni-modal embedding foundation model that addresses these issues through tailored training strategies and architectural design. In the optimization procedure, we propose a multi-stage training scheme to boost the multifaceted effectiveness of representation learning. Specifically, the content-aware progressive training aims to enhance the model's adaptability to diverse downstream tasks and master enriched cross-modal proficiency. The collaboration-aware recommendation enhancement training further adapts multimodal representations for recommendation scenarios by distilling knowledge from sequence-to-item and ID-to-item embeddings while mining user historical interests. Concurrently, we develop the stochastic specialization and dataset-driven pattern matching to strengthen model training flexibility and generalizability. Experimental results show that SAIL-Embedding achieves SOTA performance compared to other methods in different retrieval tasks. In online experiments across various real-world scenarios integrated with our model, we observe a significant increase in Lifetime (LT), which is a crucial indicator for the recommendation experience. For instance, the model delivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the Douyin-Selected scenario. For the Douyin feed rank model, the match features produced by SAIL-Embedding yield a +0.08% AUC gain.", 'score': 8, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'ea07490eae7bd8d4', 'authors': ['Lin Lin', 'Jiefeng Long', 'Zhihe Wan', 'Yuchi Wang', 'Dingkang Yang', 'Shuang Yang', 'Yueyang Yao', 'Xu Chen', 'Zirui Guo', 'Shengqiang Li', 'Weiran Li', 'Hanyu Li', 'Yaling Mou', 'Yan Qiu', 'Haiyang Yu', 'Xiao Liang', 'Hongsheng Li', 'Chao Feng'], 'affiliations': ['ByteDance', 'CUHK MMLab'], 'pdf_title_img': 'assets/pdf/title_img/2510.12709.jpg', 'data': {'categories': ['#training', '#optimization', '#multimodal'], 'emoji': 'â›µ', 'ru': {'title': 'SAIL-Embedding: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SAIL-Embedding â€” Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: content-aware Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞ½Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¸ collaboration-aware Ñ‚Ñ€ĞµĞ½Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ID-ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½-Ğ¼Ğ°Ñ‚Ñ‡Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ Douyin Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Lifetime (+0.158% Ğ·Ğ° 7 Ğ´Ğ½ĞµĞ¹) Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ AUC (+0.08%) Ğ² Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'SAIL-Embedding: Bridging Modalities for Enhanced Learning and Recommendations', 'desc': 'This paper presents SAIL-Embedding, a new omni-modal embedding model designed to improve cross-modal tasks in machine learning. It addresses challenges like limited modality support and unstable training by using a multi-stage training scheme that enhances representation learning. The model incorporates content-aware and collaboration-aware training strategies to adapt to various tasks and improve recommendation systems. Experimental results demonstrate that SAIL-Embedding outperforms existing methods, showing significant improvements in user engagement metrics in real-world applications.'}, 'zh': {'title': 'SAIL-Embeddingï¼šå…¨æ¨¡æ€åµŒå…¥çš„æœªæ¥', 'desc': 'å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹æ—¨åœ¨ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„ç»Ÿä¸€è¡¨ç¤ºï¼Œä»¥æ”¯æŒå¤šæ ·çš„è·¨æ¨¡æ€ä»»åŠ¡ã€‚å°½ç®¡ä»CLIPåŸºç¡€çš„åŒå¡”æ¶æ„åˆ°å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¼”å˜å–å¾—äº†è‰¯å¥½è¿›å±•ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚æœ‰é™çš„æ¨¡æ€æ”¯æŒå’Œä¸ç¨³å®šçš„è®­ç»ƒæœºåˆ¶ã€‚æœ¬æ–‡æå‡ºäº†SAIL-Embeddingï¼Œè¿™æ˜¯ä¸€ç§å…¨æ¨¡æ€åµŒå…¥åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡å®šåˆ¶çš„è®­ç»ƒç­–ç•¥å’Œæ¶æ„è®¾è®¡æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAIL-Embeddingåœ¨ä¸åŒæ£€ç´¢ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å®é™…åœºæ™¯ä¸­æ˜¾è‘—æé«˜äº†æ¨èä½“éªŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12801', 'title': 'DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search', 'url': 'https://huggingface.co/papers/2510.12801', 'abstract': 'Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on a two-stage training pipeline: a cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across a range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search.', 'score': 7, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '78d4634c4d91c679', 'authors': ['Kartik Narayan', 'Yang Xu', 'Tian Cao', 'Kavya Nerella', 'Vishal M. Patel', 'Navid Shiee', 'Peter Grasch', 'Chao Jia', 'Yinfei Yang', 'Zhe Gan'], 'affiliations': ['Apple', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2510.12801.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#training', '#dataset', '#agi', '#optimization', '#rag', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DeepMMSearch-R1 â€” Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ LLM, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: supervised fine-tuning Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ DeepMMSearchVQA Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ RAG-Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Web Search with Dynamic Multimodal Queries', 'desc': 'This paper introduces DeepMMSearch-R1, a novel multimodal large language model (MLLM) designed to enhance web search capabilities for both text and images. Unlike traditional methods that rely on fixed pipelines, DeepMMSearch-R1 can perform on-demand, multi-turn searches and dynamically generate queries based on the input data. The model is trained using a two-stage process, starting with supervised finetuning followed by reinforcement learning, and utilizes a new dataset called DeepMMSearchVQA for training. The results show that this approach significantly improves the efficiency and effectiveness of information retrieval in real-world applications.'}, 'zh': {'title': 'åŠ¨æ€å¤šæ¨¡æ€æœç´¢ï¼Œæå‡ä¿¡æ¯è·å–æ•ˆç‡', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDeepMMSearch-R1çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒèƒ½å¤Ÿè¿›è¡ŒæŒ‰éœ€çš„å¤šè½®ç½‘ç»œæœç´¢ï¼Œå¹¶åŠ¨æ€ç”Ÿæˆå›¾åƒå’Œæ–‡æœ¬æœç´¢æŸ¥è¯¢ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹è¾“å…¥å›¾åƒçš„ç›¸å…³éƒ¨åˆ†è¿›è¡Œæœç´¢ï¼Œæé«˜äº†å›¾åƒæœç´¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶èƒ½å¤Ÿæ ¹æ®æ£€ç´¢åˆ°çš„ä¿¡æ¯è¿­ä»£è°ƒæ•´æ–‡æœ¬æœç´¢æŸ¥è¯¢ï¼Œå®ç°è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘çº æ­£ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬å†·å¯åŠ¨çš„ç›‘ç£å¾®è°ƒå’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œå¹¶å¼•å…¥äº†DeepMMSearchVQAæ•°æ®é›†ï¼Œä»¥æ”¯æŒå¤šæ¨¡æ€è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ–¹æ³•åœ¨çŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸­çš„ä¼˜è¶Šæ€§ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€ç½‘ç»œæœç´¢çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11892', 'title': 'R-WoM: Retrieval-augmented World Model For Computer-use Agents', 'url': 'https://huggingface.co/papers/2510.11892', 'abstract': "LLMs can enhance decision-making in digital environments but struggle with long-horizon simulations due to hallucination and static knowledge. R-WoM improves performance by integrating external, up-to-date knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can serve as world models to enhance agent decision-making in digital environments by simulating future states and predicting action outcomes, potentially eliminating costly trial-and-error exploration. However, this capability is fundamentally limited by LLMs' tendency toward hallucination and their reliance on static training knowledge, which can lead to compounding errors that inhibit long-horizon simulations. To systematically investigate whether LLMs are appropriate for world modeling, we probe two core capabilities of world models--future state prediction and reward estimation--through three tasks: next-state identification, full-procedure planning alignment, and milestone transition recognition. Our analysis shows that while LLMs effectively capture immediate next states and identify meaningful state transitions, their performance rapidly degrades in full-procedure planning. This highlights LLMs' limitations in reliably modeling environment dynamics over long horizons. To address these limitations, we propose the Retrieval-augmented World Model (R-WoM), which grounds LLM simulations by incorporating factual, up-to-date knowledge retrieved from external tutorials. Experiments show that R-WoM achieves substantial improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to baselines, with particular advantages in longer-horizon simulations.", 'score': 6, 'issue_id': 6432, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '816242da322279c2', 'authors': ['Kai Mei', 'Jiang Guo', 'Shuaichen Chang', 'Mingwen Dong', 'Dongkyu Lee', 'Xing Niu', 'Jiarong Jiang'], 'affiliations': ['AWS Agentic AI', 'Rutgers University'], 'pdf_title_img': 'assets/pdf/title_img/2510.11892.jpg', 'data': {'categories': ['#hallucinations', '#agents', '#long_context', '#rag'], 'emoji': 'ğŸ”®', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ½Ğ¾ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ² Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ R-WoM (Retrieval-augmented World Model) Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ LLM Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 25.3% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing LLMs with Real-Time Knowledge for Better Decision-Making', 'desc': "This paper discusses the limitations of Large Language Models (LLMs) in making long-term predictions in digital environments due to their tendency to hallucinate and rely on outdated knowledge. It evaluates LLMs' abilities in future state prediction and reward estimation through various tasks, revealing that while they perform well in short-term scenarios, their accuracy declines in complex, long-horizon planning. To overcome these challenges, the authors introduce the Retrieval-augmented World Model (R-WoM), which enhances LLMs by integrating real-time, factual information from external sources. Experimental results demonstrate that R-WoM significantly improves performance in long-horizon simulations, showcasing its effectiveness in decision-making tasks."}, 'zh': {'title': 'æ£€ç´¢å¢å¼ºä¸–ç•Œæ¨¡å‹ï¼šæå‡é•¿æ—¶é—´å†³ç­–èƒ½åŠ›çš„å…³é”®', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­—ç¯å¢ƒä¸­å¯ä»¥å¢å¼ºå†³ç­–èƒ½åŠ›ï¼Œä½†åœ¨é•¿æ—¶é—´æ¨¡æ‹Ÿä¸­å®¹æ˜“å‡ºç°å¹»è§‰å’Œä¾èµ–é™æ€çŸ¥è¯†çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ–¹æ³•ï¼Œç§°ä¸ºæ£€ç´¢å¢å¼ºä¸–ç•Œæ¨¡å‹ï¼ˆR-WoMï¼‰ï¼Œé€šè¿‡æ•´åˆå¤–éƒ¨çš„æœ€æ–°çŸ¥è¯†æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMsåœ¨é¢„æµ‹ä¸‹ä¸€çŠ¶æ€å’Œè¯†åˆ«é‡è¦çŠ¶æ€è½¬å˜æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å®Œæ•´è¿‡ç¨‹è§„åˆ’ä¸­æ€§èƒ½è¿…é€Ÿä¸‹é™ã€‚R-WoMçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”åŸºçº¿æ–¹æ³•ï¼Œåœ¨é•¿æ—¶é—´æ¨¡æ‹Ÿä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11919', 'title': 'LLM Reasoning for Machine Translation: Synthetic Data Generation over\n  Thinking Tokens', 'url': 'https://huggingface.co/papers/2510.11919', 'abstract': 'Large reasoning models (LRMs) have led to new possibilities in terms of problem-solving, through the devising of a natural language thought process prior to answering a query. While their capabilities are well known across mathematics and coding tasks, their impact on the task of machine translation (MT) remains underexplored. In this work, we explore the benefits of the generation of intermediate tokens when performing MT across multiple language pairs of different levels of resourcedness and multiple setups. We find that "thinking tokens" do not help LRMs better perform MT. This result generalizes to models fine-tuned to reason before translating using distilled chain of thought (CoT) inspired by human translators\' practices. Specifically, fine-tuning a model with synthetic CoT explanations detailing how to translate step-by-step does not outperform standard input-output fine-tuning. However, constructing the intermediate tokens by combining the outputs of modular translation-specific prompting strategies results in improvements. Our findings underscore that the contribution of intermediate tokens during fine-tuning highly depends on the presence of translation attempts within them. More broadly, our results suggest that using a teacher to refine target translations or to expand parallel corpora is more impactful than distilling their CoT explanations into "thinking" MT models.', 'score': 4, 'issue_id': 6421, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': 'ea9cdd45bedd4fcf', 'authors': ['Armel Zebaze', 'Rachel Bawden', 'BenoÃ®t Sagot'], 'affiliations': ['Inria Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2510.11919.jpg', 'data': {'categories': ['#low_resource', '#reasoning', '#training', '#multilingual', '#machine_translation', '#synthetic'], 'emoji': 'ğŸ¤”', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ LLM Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸, Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ "Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¹" (thinking tokens) Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ reasoning Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LRM) Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ¼. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ chain of thought (CoT) Ğ¾Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ fine-tuning. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ğ»Ğ¸ÑÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ»Ğ¸ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ², Ñ‡ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ CoT Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Intermediate Tokens: Not the Key to Better Machine Translation', 'desc': "This paper investigates the role of large reasoning models (LRMs) in machine translation (MT) by examining the use of intermediate tokens during the translation process. The authors find that generating 'thinking tokens' does not enhance the performance of LRMs in MT tasks, even when models are fine-tuned with chain of thought (CoT) explanations. Instead, they discover that combining outputs from translation-specific prompting strategies leads to better results. Overall, the study highlights that refining translations through teacher models or expanding training data is more beneficial than simply using reasoning-based approaches in MT."}, 'zh': {'title': 'ä¸­é—´æ ‡è®°åœ¨æœºå™¨ç¿»è¯‘ä¸­çš„é‡è¦æ€§', 'desc': 'å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨è§£å†³é—®é¢˜æ–¹é¢å±•ç°äº†æ–°çš„å¯èƒ½æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€æ€ç»´è¿‡ç¨‹çš„å¼•å¯¼ä¸‹è¿›è¡Œå›ç­”ã€‚å°½ç®¡å®ƒä»¬åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸­çš„èƒ½åŠ›å·²è¢«å¹¿æ³›è®¤å¯ï¼Œä½†åœ¨æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ä»»åŠ¡ä¸­çš„å½±å“ä»ç„¶æœªè¢«å……åˆ†æ¢è®¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œç”Ÿæˆä¸­é—´æ ‡è®°å¹¶æœªå¸®åŠ©LRMsåœ¨æœºå™¨ç¿»è¯‘ä¸­è¡¨ç°æ›´å¥½ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨äººç±»ç¿»è¯‘è€…å®è·µå¯å‘çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è¿›è¡Œå¾®è°ƒæ—¶ã€‚ç›¸åï¼Œé€šè¿‡ç»“åˆæ¨¡å—åŒ–ç¿»è¯‘ç‰¹å®šæç¤ºç­–ç•¥çš„è¾“å‡ºæ„å»ºä¸­é—´æ ‡è®°åˆ™èƒ½å¸¦æ¥æ”¹è¿›ï¼Œè¡¨æ˜ä¸­é—´æ ‡è®°çš„è´¡çŒ®é«˜åº¦ä¾èµ–äºå…¶ä¸­çš„ç¿»è¯‘å°è¯•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11000', 'title': 'ContextGen: Contextual Layout Anchoring for Identity-Consistent\n  Multi-Instance Generation', 'url': 'https://huggingface.co/papers/2510.11000', 'abstract': 'ContextGen, a Diffusion Transformer framework, enhances multi-instance image generation by integrating layout anchoring and identity consistency attention, achieving superior control and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-instance image generation (MIG) remains a significant challenge for modern diffusion models due to key limitations in achieving precise control over object layout and preserving the identity of multiple distinct subjects. To address these limitations, we introduce ContextGen, a novel Diffusion Transformer framework for multi-instance generation that is guided by both layout and reference images. Our approach integrates two key technical contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates the composite layout image into the generation context to robustly anchor the objects in their desired positions, and Identity Consistency Attention (ICA), an innovative attention mechanism that leverages contextual reference images to ensure the identity consistency of multiple instances. Recognizing the lack of large-scale, hierarchically-structured datasets for this task, we introduce IMIG-100K, the first dataset with detailed layout and identity annotations. Extensive experiments demonstrate that ContextGen sets a new state-of-the-art, outperforming existing methods in control precision, identity fidelity, and overall visual quality.', 'score': 4, 'issue_id': 6437, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': 'b1b06cbfe0048430', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#cv', '#dataset', '#diffusion'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ContextGen â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Diffusion Transformer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Contextual Layout Anchoring Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Identity Consistency Attention Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ IMIG-100K Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ContextGen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Mastering Multi-Instance Image Generation with ContextGen', 'desc': 'ContextGen is a new framework that improves the generation of images with multiple objects using a method called Diffusion Transformer. It solves problems in controlling where objects are placed and keeping their identities consistent across images. The framework uses two main techniques: Contextual Layout Anchoring to fix objects in specific locations and Identity Consistency Attention to maintain the unique characteristics of each object. Additionally, it introduces a new dataset, IMIG-100K, which helps train models by providing detailed information about object layouts and identities.'}, 'zh': {'title': 'ContextGenï¼šæå‡å¤šå®ä¾‹å›¾åƒç”Ÿæˆçš„æ§åˆ¶ä¸è´¨é‡', 'desc': 'ContextGenæ˜¯ä¸€ç§æ–°çš„æ‰©æ•£å˜æ¢å™¨æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„å¤šå®ä¾‹å›¾åƒç”Ÿæˆã€‚å®ƒé€šè¿‡å¸ƒå±€é”šå®šå’Œèº«ä»½ä¸€è‡´æ€§æ³¨æ„æœºåˆ¶ï¼Œå¢å¼ºäº†å¯¹å¯¹è±¡å¸ƒå±€çš„æ§åˆ¶å’Œå¤šä¸ªä¸»ä½“èº«ä»½çš„ä¿æŒã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸Šä¸‹æ–‡å¸ƒå±€é”šå®šï¼ˆCLAï¼‰å’Œèº«ä»½ä¸€è‡´æ€§æ³¨æ„ï¼ˆICAï¼‰ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒåœ¨å¯¹è±¡ä½ç½®å’Œèº«ä»½ä¸Šéƒ½å…·æœ‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒContextGenè¿˜å¼•å…¥äº†IMIG-100Kæ•°æ®é›†ï¼Œä¸ºå¤šå®ä¾‹ç”Ÿæˆæä¾›äº†è¯¦ç»†çš„å¸ƒå±€å’Œèº«ä»½æ³¨é‡Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12777', 'title': 'What If : Understanding Motion Through Sparse Interactions', 'url': 'https://huggingface.co/papers/2510.12777', 'abstract': 'Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on sparse interactions termed "pokes". Unlike traditional methods that typically only enable dense sampling of a single realization of scene dynamics, FPT provides an interpretable directly accessible representation of multi-modal scene motion, its dependency on physical interactions and the inherent uncertainties of scene dynamics. We also evaluate our model on several downstream tasks to enable comparisons with prior methods and highlight the flexibility of our approach. On dense face motion generation, our generic pre-trained model surpasses specialized baselines. FPT can be fine-tuned in strongly out-of-distribution tasks such as synthetic datasets to enable significant improvements over in-domain methods in articulated object motion estimation. Additionally, predicting explicit motion distributions directly enables our method to achieve competitive performance on tasks like moving part segmentation from pokes which further demonstrates the versatility of our FPT. Code and models are publicly available at https://compvis.github.io/flow-poke-transformer.', 'score': 3, 'issue_id': 6421, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'bb4a1b3538707a8b', 'authors': ['Stefan Andreas Baumann', 'Nick Stracke', 'Timy Phan', 'BjÃ¶rn Ommer'], 'affiliations': ['CompVis @ LMU Munich', 'Munich Center for Machine Learning (MCML)'], 'pdf_title_img': 'assets/pdf/title_img/2510.12777.jpg', 'data': {'categories': ['#cv', '#interpretability', '#open_source', '#reasoning', '#training', '#optimization', '#synthetic'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Flow Poke Transformer: Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Flow Poke Transformer (FPT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… "Ğ¿Ğ¾ĞºĞ°Ğ¼Ğ¸". Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², FPT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ ĞµĞ³Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ†Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². FPT Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ñ‡Ğ°ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Predicting Scene Dynamics with Flow Poke Transformer', 'desc': "The Flow Poke Transformer (FPT) is a new machine learning framework designed to predict how physical scenes change based on local interactions called 'pokes'. Unlike traditional models that only provide a single outcome, FPT offers a clear representation of multiple possible motions and their uncertainties. This model has been tested on various tasks, showing that it can outperform specialized models in generating dense face motion and improve performance in estimating articulated object motion. By directly predicting motion distributions, FPT also excels in tasks like moving part segmentation, showcasing its adaptability and effectiveness."}, 'zh': {'title': 'æµåŠ¨æˆ³å‡»å˜æ¢å™¨ï¼šç†è§£ç‰©ç†åœºæ™¯åŠ¨æ€çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºæµåŠ¨æˆ³å‡»å˜æ¢å™¨ï¼ˆFPTï¼‰ï¼Œç”¨äºç›´æ¥é¢„æµ‹ç‰©ç†åœºæ™¯ä¸­å±€éƒ¨è¿åŠ¨çš„åˆ†å¸ƒã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒFPTèƒ½å¤Ÿæä¾›å¤šæ¨¡æ€åœºæ™¯è¿åŠ¨çš„å¯è§£é‡Šè¡¨ç¤ºï¼Œå¹¶è€ƒè™‘ç‰©ç†äº¤äº’å’Œåœºæ™¯åŠ¨æ€çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¯„ä¼°äº†è¯¥æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…¶çµæ´»æ€§å’Œä¼˜è¶Šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç¨ å¯†é¢éƒ¨è¿åŠ¨ç”Ÿæˆå’Œå…³èŠ‚ç‰©ä½“è¿åŠ¨ä¼°è®¡æ–¹é¢ã€‚FPTçš„ç›´æ¥è¿åŠ¨åˆ†å¸ƒé¢„æµ‹èƒ½åŠ›ä½¿å…¶åœ¨ç§»åŠ¨éƒ¨ä»¶åˆ†å‰²ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09782', 'title': 'The Geometry of Reasoning: Flowing Logics in Representation Space', 'url': 'https://huggingface.co/papers/2510.09782', 'abstract': "We study how large language models (LLMs) ``think'' through their representation space. We propose a novel geometric framework that models an LLM's reasoning as flows -- embedding trajectories evolving where logic goes. We disentangle logical structure from semantics by employing the same natural deduction propositions with varied semantic carriers, allowing us to test whether LLMs internalize logic beyond surface form. This perspective connects reasoning with geometric quantities such as position, velocity, and curvature, enabling formal analysis in representation and concept spaces. Our theory establishes: (1) LLM reasoning corresponds to smooth flows in representation space, and (2) logical statements act as local controllers of these flows' velocities. Using learned representation proxies, we design controlled experiments to visualize and quantify reasoning flows, providing empirical validation of our theoretical framework. Our work serves as both a conceptual foundation and practical tools for studying reasoning phenomenon, offering a new lens for interpretability and formal analysis of LLMs' behavior.", 'score': 3, 'issue_id': 6439, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': '0af2abb3690ae423', 'authors': ['Yufa Zhou', 'Yixiao Wang', 'Xunjian Yin', 'Shuyan Zhou', 'Anru R. Zhang'], 'affiliations': ['Duke University'], 'pdf_title_img': 'assets/pdf/title_img/2510.09782.jpg', 'data': {'categories': ['#interpretability', '#architecture', '#math', '#reasoning'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° ĞºĞ°Ğº Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº LLM Â«Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ÑÑÑ‚Â» Ğ² ÑĞ²Ğ¾Ñ‘Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ â€” Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ² Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ, ÑƒÑĞ²Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ Ğ»Ğ¸ LLM Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹. Ğ¢ĞµĞ¾Ñ€Ğ¸Ñ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ (Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ, ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ, ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ğ°), Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ğ¼Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Understanding LLM Reasoning Through Geometric Flows', 'desc': 'This paper explores how large language models (LLMs) process information by examining their representation space through a geometric framework. It introduces the concept of reasoning as flows, which are trajectories in the embedding space that represent logical reasoning. By separating logical structure from semantics, the authors test if LLMs understand logic beyond just the words they use. The study provides a theoretical basis and practical methods for analyzing LLM reasoning, enhancing our understanding of their interpretability and behavior.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æµåŠ¨', 'desc': 'æˆ‘ä»¬ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•é€šè¿‡å…¶è¡¨ç¤ºç©ºé—´è¿›è¡Œâ€œæ€è€ƒâ€ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å‡ ä½•æ¡†æ¶ï¼Œå°†LLMçš„æ¨ç†å»ºæ¨¡ä¸ºæµåŠ¨â€”â€”åµŒå…¥è½¨è¿¹çš„æ¼”å˜ã€‚é€šè¿‡ä½¿ç”¨ç›¸åŒçš„è‡ªç„¶æ¨ç†å‘½é¢˜ä¸ä¸åŒçš„è¯­ä¹‰è½½ä½“ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæµ‹è¯•LLMsæ˜¯å¦åœ¨è¡¨é¢å½¢å¼ä¹‹å¤–å†…åŒ–äº†é€»è¾‘ã€‚æˆ‘ä»¬çš„ç†è®ºè¡¨æ˜ï¼ŒLLMçš„æ¨ç†å¯¹åº”äºè¡¨ç¤ºç©ºé—´ä¸­çš„å¹³æ»‘æµåŠ¨ï¼Œé€»è¾‘è¯­å¥ä½œä¸ºè¿™äº›æµåŠ¨é€Ÿåº¦çš„å±€éƒ¨æ§åˆ¶å™¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12088', 'title': 'One Life to Learn: Inferring Symbolic World Models for Stochastic\n  Environments from Unguided Exploration', 'url': 'https://huggingface.co/papers/2510.12088', 'abstract': 'OneLife framework models complex, stochastic environments using conditionally-activated programmatic laws within a probabilistic programming framework, enabling learning from minimal, unguided interaction and outperforming baselines in state ranking and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Symbolic world modeling requires inferring and representing an environment\'s transitional dynamics as an executable program. Prior work has focused on largely deterministic environments with abundant interaction data, simple mechanics, and human guidance. We address a more realistic and challenging setting, learning in a complex, stochastic environment where the agent has only "one life" to explore a hostile environment without human guidance. We introduce OneLife, a framework that models world dynamics through conditionally-activated programmatic laws within a probabilistic programming framework. Each law operates through a precondition-effect structure, activating in relevant world states. This creates a dynamic computation graph that routes inference and optimization only through relevant laws, avoiding scaling challenges when all laws contribute to predictions about a complex, hierarchical state, and enabling the learning of stochastic dynamics even with sparse rule activation. To evaluate our approach under these demanding constraints, we introduce a new evaluation protocol that measures (a) state ranking, the ability to distinguish plausible future states from implausible ones, and (b) state fidelity, the ability to generate future states that closely resemble reality. We develop and evaluate our framework on Crafter-OO, our reimplementation of the Crafter environment that exposes a structured, object-oriented symbolic state and a pure transition function that operates on that state alone. OneLife can successfully learn key environment dynamics from minimal, unguided interaction, outperforming a strong baseline on 16 out of 23 scenarios tested. We also test OneLife\'s planning ability, with simulated rollouts successfully identifying superior strategies. Our work establishes a foundation for autonomously constructing programmatic world models of unknown, complex environments.', 'score': 2, 'issue_id': 6426, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'd8a451b665853cec', 'authors': ['Zaid Khan', 'Archiki Prasad', 'Elias Stengel-Eskin', 'Jaemin Cho', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2510.12088.jpg', 'data': {'categories': ['#rl', '#benchmark', '#reasoning', '#optimization', '#games', '#agents'], 'emoji': 'ğŸ®', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ·Ğ° Ğ¾Ğ´Ğ½Ñƒ Ğ¶Ğ¸Ğ·Ğ½ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OneLife â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¸Ğ¼ĞµĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Â«Ğ¿Ñ€ĞµĞ´ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ-ÑÑ„Ñ„ĞµĞºÑ‚Â» Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ¼Ğ¸Ñ€Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¸Ğ· Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ñ€Ğ°Ğ¶Ğ´ĞµĞ±Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ³Ğ´Ğµ Ñƒ Ğ½ĞµĞ³Ğ¾ ĞµÑÑ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Â«Ğ¾Ğ´Ğ½Ğ° Ğ¶Ğ¸Ğ·Ğ½ÑŒÂ» Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸.'}, 'en': {'title': 'OneLife: Learning in Complex Environments with Minimal Interaction', 'desc': 'The OneLife framework is designed to model complex environments that are unpredictable and require minimal interaction for learning. It uses conditionally-activated programmatic laws within a probabilistic programming structure to represent the dynamics of the environment. This approach allows the agent to learn effectively even when it has only one chance to explore, without any human guidance. The framework has been shown to outperform existing methods in distinguishing future states and generating realistic outcomes, demonstrating its potential for autonomous learning in challenging settings.'}, 'zh': {'title': 'OneLifeï¼šåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°è‡ªä¸»å­¦ä¹ çš„æ¡†æ¶', 'desc': 'OneLifeæ¡†æ¶é€šè¿‡æ¡ä»¶æ¿€æ´»çš„ç¨‹åºæ³•åˆ™åœ¨æ¦‚ç‡ç¼–ç¨‹æ¡†æ¶ä¸­å»ºæ¨¡å¤æ‚çš„éšæœºç¯å¢ƒï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æ²¡æœ‰äººç±»æŒ‡å¯¼çš„æƒ…å†µä¸‹ï¼Œä»æœ€å°çš„äº¤äº’ä¸­å­¦ä¹ ã€‚è¯¥æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºåœ¨å¤æ‚å’Œæ•Œå¯¹çš„ç¯å¢ƒä¸­è¿›è¡Œå­¦ä¹ ï¼Œæ™ºèƒ½ä½“åªæœ‰ä¸€æ¬¡æ¢ç´¢çš„æœºä¼šã€‚OneLifeé€šè¿‡åŠ¨æ€è®¡ç®—å›¾æ¥ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ï¼Œé¿å…äº†åœ¨å¤æ‚çŠ¶æ€ä¸‹æ‰€æœ‰æ³•åˆ™éƒ½å‚ä¸é¢„æµ‹æ—¶çš„æ‰©å±•æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOneLifeåœ¨16ä¸ªæµ‹è¯•åœºæ™¯ä¸­è¶…è¶Šäº†å¼ºåŸºçº¿ï¼ŒæˆåŠŸå­¦ä¹ äº†ç¯å¢ƒçš„å…³é”®åŠ¨æ€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11661', 'title': 'SR-Scientist: Scientific Equation Discovery With Agentic AI', 'url': 'https://huggingface.co/papers/2510.11661', 'abstract': "SR-Scientist, an autonomous AI framework, leverages LLMs to generate, implement, and optimize scientific equations, outperforming baselines across multiple disciplines.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Language Models (LLMs) have been applied to scientific equation discovery, leveraging their embedded scientific knowledge for hypothesis generation. However, current methods typically confine LLMs to the role of an equation proposer within search algorithms like genetic programming. In this paper, we present SR-Scientist, a framework that elevates the LLM from a simple equation proposer to an autonomous AI scientist that writes code to analyze data, implements the equation as code, submits it for evaluation, and optimizes the equation based on experimental feedback. Specifically, we wrap the code interpreter into a set of tools for data analysis and equation evaluation. The agent is instructed to optimize the equation by utilizing these tools over a long horizon with minimal human-defined pipelines. Empirical results show that SR-Scientist outperforms baseline methods by an absolute margin of 6% to 35% on datasets covering four science disciplines. Additionally, we demonstrate our method's robustness to noise, the generalization of the discovered equations to out-of-domain data, and their symbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning framework to enhance the agent's capabilities.", 'score': 2, 'issue_id': 6435, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '4b2da4526e4f17e8', 'authors': ['Shijie Xia', 'Yuhan Sun', 'Pengfei Liu'], 'affiliations': ['GAIR', 'SII', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.11661.jpg', 'data': {'categories': ['#agents', '#rl', '#science', '#dataset', '#optimization'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'AI-ÑƒÑ‡Ñ‘Ğ½Ñ‹Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ°Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ SR-Scientist â€” Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ AI-Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ³Ğ´Ğµ LLM Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹, SR-Scientist Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ğ¹ AI-ÑƒÑ‡Ñ‘Ğ½Ñ‹Ğ¹: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ´Ğµ, Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 6-35% Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑˆÑƒĞ¼Ñƒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ end-to-end reinforcement learning Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº.'}, 'en': {'title': 'Empowering AI to Become Autonomous Scientists', 'desc': 'SR-Scientist is an advanced AI framework that utilizes Large Language Models (LLMs) to autonomously generate, implement, and optimize scientific equations. Unlike traditional methods that limit LLMs to proposing equations, SR-Scientist enables them to analyze data, write code, and refine equations based on experimental results. The framework incorporates a code interpreter and a suite of tools for data analysis, allowing the AI to operate with minimal human intervention. Empirical studies show that SR-Scientist significantly outperforms existing methods across various scientific disciplines, demonstrating its effectiveness and robustness in equation discovery and optimization.'}, 'zh': {'title': 'è‡ªä¸»AIç§‘å­¦å®¶ï¼šSR-Scientistçš„åˆ›æ–°ä¹‹è·¯', 'desc': 'SR-Scientistæ˜¯ä¸€ä¸ªè‡ªä¸»çš„äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆã€å®ç°å’Œä¼˜åŒ–ç§‘å­¦æ–¹ç¨‹ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒSR-Scientistä¸ä»…ä»…æ˜¯æ–¹ç¨‹çš„æå‡ºè€…ï¼Œè€Œæ˜¯ä¸€ä¸ªèƒ½å¤Ÿç¼–å†™ä»£ç ã€åˆ†ææ•°æ®å¹¶æ ¹æ®å®éªŒåé¦ˆä¼˜åŒ–æ–¹ç¨‹çš„è‡ªä¸»AIç§‘å­¦å®¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ä»£ç è§£é‡Šå™¨ä¸æ•°æ®åˆ†æå’Œæ–¹ç¨‹è¯„ä¼°å·¥å…·ç»“åˆï¼Œæå‡äº†æ–¹ç¨‹ä¼˜åŒ–çš„æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSR-Scientiståœ¨å››ä¸ªç§‘å­¦é¢†åŸŸçš„æ•°æ®é›†ä¸Šï¼Œæ€§èƒ½è¶…è¶Šäº†åŸºçº¿æ–¹æ³•6%åˆ°35%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11606', 'title': 'ExpVid: A Benchmark for Experiment Video Understanding & Reasoning', 'url': 'https://huggingface.co/papers/2510.11606', 'abstract': 'ExpVid, a new benchmark for evaluating multimodal large language models on scientific experiment videos, highlights gaps in fine-grained perception, procedural understanding, and scientific reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) hold promise for accelerating scientific discovery by interpreting complex experimental procedures. However, their true capabilities are poorly understood, as existing benchmarks neglect the fine-grained and long-horizon nature of authentic laboratory work, especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the first benchmark designed to systematically evaluate MLLMs on scientific experiment videos. Curated from peer-reviewed video publications, ExpVid features a new three-level task hierarchy that mirrors the scientific process: (1) Fine-grained Perception of tools, materials, and actions; (2) Procedural Understanding of step order and completeness; and (3) Scientific Reasoning that connects the full experiment to its published conclusions. Our vision-centric annotation pipeline, combining automated generation with multi-disciplinary expert validation, ensures that tasks require visual grounding. We evaluate 19 leading MLLMs on ExpVid and find that while they excel at coarse-grained recognition, they struggle with disambiguating fine details, tracking state changes over time, and linking experimental procedures to scientific outcomes. Our results reveal a notable performance gap between proprietary and open-source models, particularly in high-order reasoning. ExpVid not only provides a diagnostic tool but also charts a roadmap for developing MLLMs capable of becoming trustworthy partners in scientific experimentation.', 'score': 2, 'issue_id': 6424, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '4f72ba62e40a520c', 'authors': ['Yicheng Xu', 'Yue Wu', 'Jiashuo Yu', 'Ziang Yan', 'Tianxiang Jiang', 'Yinan He', 'Qingsong Zhao', 'Kai Chen', 'Yu Qiao', 'Limin Wang', 'Manabu Okumura', 'Yi Wang'], 'affiliations': ['Institute of Science Tokyo', 'Nanjing University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2510.11606.jpg', 'data': {'categories': ['#cv', '#science', '#benchmark', '#open_source', '#reasoning', '#multimodal'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ â€” ÑĞ»Ğ°Ğ±Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… AI', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ExpVid â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 19 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… MLLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ExpVid Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'ExpVid: Bridging Gaps in MLLM Evaluation for Scientific Discovery', 'desc': 'ExpVid is a new benchmark designed to evaluate multimodal large language models (MLLMs) specifically on scientific experiment videos. It addresses the shortcomings of existing benchmarks by focusing on fine-grained perception, procedural understanding, and scientific reasoning in laboratory settings. The benchmark features a three-level task hierarchy that reflects the scientific process, ensuring that models are tested on their ability to perceive details, understand procedures, and reason scientifically. Our evaluation of 19 leading MLLMs reveals significant gaps in their performance, particularly in high-order reasoning, highlighting the need for improved models in scientific contexts.'}, 'zh': {'title': 'ExpVidï¼šç§‘å­¦å®éªŒè§†é¢‘è¯„ä¼°çš„æ–°åŸºå‡†', 'desc': 'ExpVidæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦å®éªŒè§†é¢‘ä¸Šçš„è¡¨ç°ã€‚å®ƒæ­ç¤ºäº†åœ¨ç»†ç²’åº¦æ„ŸçŸ¥ã€ç¨‹åºç†è§£å’Œç§‘å­¦æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚è¯¥åŸºå‡†é€šè¿‡ä¸‰å±‚ä»»åŠ¡å±‚æ¬¡ç»“æ„ï¼Œç³»ç»Ÿåœ°è¯„ä¼°æ¨¡å‹åœ¨å®éªŒè¿‡ç¨‹ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬å·¥å…·å’Œææ–™çš„è¯†åˆ«ã€æ­¥éª¤çš„ç†è§£ä»¥åŠå®éªŒä¸ç»“è®ºçš„å…³è”ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ç°æœ‰æ¨¡å‹åœ¨ç²—ç²’åº¦è¯†åˆ«ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†ç»†èŠ‚å’Œé«˜é˜¶æ¨ç†æ—¶ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09259', 'title': 'Detecting Data Contamination from Reinforcement Learning Post-training\n  for Large Language Models', 'url': 'https://huggingface.co/papers/2510.09259', 'abstract': "Self-Critique addresses data contamination in the RL post-training phase of LLMs by detecting policy collapse, outperforming existing methods with significant improvements in AUC.  \t\t\t\t\tAI-generated summary \t\t\t\t Data contamination poses a significant threat to the reliable evaluation of Large Language Models (LLMs). This issue arises when benchmark samples may inadvertently appear in training sets, compromising the validity of reported performance. While detection methods have been developed for the pre-training and Supervised Fine-Tuning stages, a critical research gap exists for the increasingly significant phase of Reinforcement Learning (RL) post-training. As RL post-training becomes pivotal for advancing LLM reasoning, the absence of specialized contamination detection methods in this paradigm presents a critical vulnerability. To address this, we conduct the first systematic study of data detection within RL post-training scenario and propose Self-Critique. Our method is motivated by a key observation: after RL phase, the output entropy distribution of LLMs tends to collapse into highly specific and sparse modes. Self-Critique probes for the underlying policy collapse, i.e., the model's convergence to a narrow reasoning path, which causes this entropy reduction. To facilitate this research, we also introduce RL-MIA, a benchmark constructed to simulate this specific contamination scenario. Extensive experiments show that Self-Critique significantly outperforms baseline methods across multiple models and contamination tasks, achieving an AUC improvement of up to 30%. Whereas existing methods are close to a random guess for RL-phase contamination, our method makes detection possible.", 'score': 2, 'issue_id': 6435, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': '50b37388f9e06847', 'authors': ['Yongding Tao', 'Tian Wang', 'Yihong Dong', 'Huanyu Liu', 'Kechi Zhang', 'Xiaolong Hu', 'Ge Li'], 'affiliations': ['New H3C Technologies Co., Ltd', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2510.09259.jpg', 'data': {'categories': ['#reasoning', '#rl', '#security', '#hallucinations', '#training', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ”ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² RL', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-Critique Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ RL Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ğ½Ğ³Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»Ğµ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² LLM ÑÑ…Ğ»Ğ¾Ğ¿Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ² ÑƒĞ·ĞºĞ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹. Self-Critique Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€” ĞµÑ‘ ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğº ÑƒĞ·ĞºĞ¾Ğ¼Ñƒ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ AUC Ğ´Ğ¾ 30% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ ĞºĞ°Ğº ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Self-Critique: Enhancing RL Post-Training Integrity in LLMs', 'desc': "This paper introduces Self-Critique, a novel method designed to detect data contamination during the Reinforcement Learning (RL) post-training phase of Large Language Models (LLMs). Data contamination can lead to policy collapse, where the model's output becomes overly focused and less diverse, compromising its performance evaluation. The authors highlight a significant gap in existing detection methods, which primarily address earlier training stages but overlook the critical RL post-training phase. Through extensive experiments, Self-Critique demonstrates a substantial improvement in Area Under the Curve (AUC) metrics, outperforming traditional methods and providing a reliable solution for identifying contamination in LLMs."}, 'zh': {'title': 'è‡ªæˆ‘æ‰¹è¯„ï¼šæå‡RLåè®­ç»ƒé˜¶æ®µçš„æ£€æµ‹èƒ½åŠ›', 'desc': 'è‡ªæˆ‘æ‰¹è¯„æ–¹æ³•è§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒé˜¶æ®µçš„æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œé€šè¿‡æ£€æµ‹ç­–ç•¥å´©æºƒæ¥æé«˜æ€§èƒ½ã€‚æ•°æ®æ±¡æŸ“ä¼šå½±å“LLMsçš„å¯é è¯„ä¼°ï¼Œå°¤å…¶æ˜¯åœ¨RLåè®­ç»ƒé˜¶æ®µç¼ºä¹æœ‰æ•ˆçš„æ£€æµ‹æ–¹æ³•ã€‚æˆ‘ä»¬é¦–æ¬¡ç³»ç»Ÿç ”ç©¶äº†RLåè®­ç»ƒä¸­çš„æ•°æ®æ£€æµ‹ï¼Œå¹¶æå‡ºäº†è‡ªæˆ‘æ‰¹è¯„æ–¹æ³•ï¼Œèƒ½å¤Ÿè¯†åˆ«æ¨¡å‹æ”¶æ•›åˆ°ç‹­çª„æ¨ç†è·¯å¾„çš„ç°è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè‡ªæˆ‘æ‰¹è¯„åœ¨å¤šä¸ªæ¨¡å‹å’Œæ±¡æŸ“ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒAUCæå‡å¯è¾¾30%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.08783', 'title': 'MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human\n  Perception of User Interfaces', 'url': 'https://huggingface.co/papers/2510.08783', 'abstract': 'In an ideal design pipeline, user interface (UI) design is intertwined with user research to validate decisions, yet studies are often resource-constrained during early exploration. Recent advances in multimodal large language models (MLLMs) offer a promising opportunity to act as early evaluators, helping designers narrow options before formal testing. Unlike prior work that emphasizes user behavior in narrow domains such as e-commerce with metrics like clicks or conversions, we focus on subjective user evaluations across varied interfaces. We investigate whether MLLMs can mimic human preferences when evaluating individual UIs and comparing them. Using data from a crowdsourcing platform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and examine alignment with human judgments on multiple UI factors. Our results show that MLLMs approximate human preferences on some dimensions but diverge on others, underscoring both their potential and limitations in supplementing early UX research.', 'score': 2, 'issue_id': 6425, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': 'd90fcd4cec73cd8c', 'authors': ['Reuben A. Luera', 'Ryan Rossi', 'Franck Dernoncourt', 'Samyadeep Basu', 'Sungchul Kim', 'Subhojyoti Mukherjee', 'Puneet Mathur', 'Ruiyi Zhang', 'Jihyung Kil', 'Nedim Lipka', 'Seunghyun Yoon', 'Jiuxiang Gu', 'Zichao Wang', 'Cindy Xiong Bearfield', 'Branislav Kveton'], 'affiliations': ['Adobe Research', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2510.08783.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#multimodal', '#benchmark'], 'emoji': 'ğŸ¨', 'ru': {'title': 'LLM ĞºĞ°Ğº Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ»Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM (Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸) Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹ Ñ‚Ğ°Ğº Ğ¶Ğµ, ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ GPT-4o, Claude Ğ¸ Llama Ğ½Ğ° 30 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¼Ğ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· ĞºÑ€Ğ°ÑƒĞ´ÑĞ¾Ñ€ÑĞ¸Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AI-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ÑÑ‚ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ UI, Ğ½Ğ¾ Ñ€Ğ°ÑÑ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ¿Ğ¾ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ´Ğ»Ñ Ñ€Ğ°Ğ½Ğ½ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², Ğ½Ğ¾ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Harnessing MLLMs for Early UI Evaluation', 'desc': 'This paper explores the use of multimodal large language models (MLLMs) as tools for early evaluation in user interface (UI) design. It investigates whether these models can replicate human preferences when assessing different UIs, using data from a crowdsourcing platform. The study benchmarks several MLLMs, including GPT-4o, Claude, and Llama, against human judgments on various UI factors. The findings reveal that while MLLMs can approximate human preferences in some areas, they also show significant divergence in others, highlighting their potential and limitations in enhancing early user experience (UX) research.'}, 'zh': {'title': 'åˆ©ç”¨MLLMsæå‡ç”¨æˆ·ç•Œé¢è®¾è®¡çš„æ—©æœŸè¯„ä¼°', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰è®¾è®¡ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸè¯„ä¼°é˜¶æ®µã€‚æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒGPT-4oã€Claudeå’ŒLlamaç­‰æ¨¡å‹åœ¨30ä¸ªä¸åŒç•Œé¢ä¸Šçš„è¡¨ç°ï¼Œè¯„ä¼°å®ƒä»¬æ˜¯å¦èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»çš„åå¥½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶MLLMsåœ¨æŸäº›æ–¹é¢èƒ½å¤Ÿæ¥è¿‘äººç±»çš„åˆ¤æ–­ï¼Œä½†åœ¨å…¶ä»–æ–¹é¢åˆ™å­˜åœ¨å·®å¼‚ï¼Œè¿™æ˜¾ç¤ºäº†å®ƒä»¬åœ¨æ—©æœŸç”¨æˆ·ä½“éªŒï¼ˆUXï¼‰ç ”ç©¶ä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè®¾è®¡å¸ˆå¯ä»¥åœ¨æ­£å¼æµ‹è¯•ä¹‹å‰æ›´æœ‰æ•ˆåœ°ç¼©å°é€‰æ‹©èŒƒå›´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12793', 'title': 'ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution', 'url': 'https://huggingface.co/papers/2510.12793', 'abstract': "Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens. The key idea behind our method is to employ multiple MLP connectors, each with a different image compression ratio, to downsample the vision tokens based on the semantic complexity of the image. During training, we minimize the KL divergence between the responses conditioned on different MLP connectors. At inference time, we introduce an image router, termed Visual Resolution Router (ViR), that automatically selects the appropriate compression rate for each image patch. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the model's perception, reasoning, and OCR capabilities. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research.", 'score': 1, 'issue_id': 6424, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'faa0a7ba8d794172', 'authors': ['Long Cui', 'Weiyun Wang', 'Jie Shao', 'Zichen Wen', 'Gen Luo', 'Linfeng Zhang', 'Yanting Zhang', 'Yu Qiao', 'Wenhai Wang'], 'affiliations': ['Donghua University', 'Fudan University', 'Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.12793.jpg', 'data': {'categories': ['#cv', '#training', '#inference', '#interpretability', '#agi', '#optimization', '#multimodal'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Visual Consistency Learning (ViCO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ multimodal LLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ MLP-ĞºĞ¾Ğ½Ğ½ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ¾ÑƒÑ‚ĞµÑ€ Visual Resolution Router Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ‚Ñ‡Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ¾ 50% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Efficient Vision Token Management for MLLMs', 'desc': "This paper introduces Visual Consistency Learning (ViCO), a new training approach for Multimodal Large Language Models (MLLMs) that addresses high inference costs caused by image inputs. ViCO allows the model to use different numbers of vision tokens based on the semantic complexity of images, improving efficiency. The method employs multiple MLP connectors to downsample vision tokens and minimizes KL divergence during training. At inference, the Visual Resolution Router (ViR) selects the optimal compression rate for image patches, reducing vision tokens by up to 50% while preserving the model's performance in perception and reasoning tasks."}, 'zh': {'title': 'åŠ¨æ€é€‚åº”è¯­ä¹‰å¤æ‚æ€§çš„è§†è§‰æ ‡è®°é€‰æ‹©', 'desc': 'ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å›¾åƒè¾“å…¥æ—¶ï¼Œç”±äºå¼•å…¥äº†é¢å¤–çš„è§†è§‰æ ‡è®°ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬å¢åŠ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒç®—æ³•ï¼Œç§°ä¸ºè§†è§‰ä¸€è‡´æ€§å­¦ä¹ ï¼ˆViCOï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å›¾åƒçš„è¯­ä¹‰å¤æ‚æ€§ä½¿ç”¨ä¸åŒæ•°é‡çš„è§†è§‰æ ‡è®°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¤šä¸ªå…·æœ‰ä¸åŒå›¾åƒå‹ç¼©æ¯”çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰è¿æ¥å™¨æ¥ä¸‹é‡‡æ ·è§†è§‰æ ‡è®°ï¼Œä»è€Œé€‚åº”å›¾åƒçš„è¯­ä¹‰å¤æ‚æ€§ã€‚åœ¨æ¨ç†æ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç§°ä¸ºè§†è§‰åˆ†è¾¨ç‡è·¯ç”±å™¨ï¼ˆViRï¼‰çš„å›¾åƒè·¯ç”±å™¨ï¼Œè‡ªåŠ¨é€‰æ‹©æ¯ä¸ªå›¾åƒå—çš„é€‚å½“å‹ç¼©ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12497', 'title': 'Mitigating the Noise Shift for Denoising Generative Models via Noise\n  Awareness Guidance', 'url': 'https://huggingface.co/papers/2510.12497', 'abstract': 'Noise Awareness Guidance (NAG) addresses noise shift in diffusion models by aligning sampling trajectories with the pre-defined noise schedule, improving generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing denoising generative models rely on solving discretized reverse-time SDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue in this family of models: a misalignment between the pre-defined noise level and the actual noise level encoded in intermediate states during sampling. We refer to this misalignment as noise shift. Through empirical analysis, we demonstrate that noise shift is widespread in modern diffusion models and exhibits a systematic bias, leading to sub-optimal generation due to both out-of-distribution generalization and inaccurate denoising updates. To address this problem, we propose Noise Awareness Guidance (NAG), a simple yet effective correction method that explicitly steers sampling trajectories to remain consistent with the pre-defined noise schedule. We further introduce a classifier-free variant of NAG, which jointly trains a noise-conditional and a noise-unconditional model via noise-condition dropout, thereby eliminating the need for external classifiers. Extensive experiments, including ImageNet generation and various supervised fine-tuning tasks, show that NAG consistently mitigates noise shift and substantially improves the generation quality of mainstream diffusion models.', 'score': 1, 'issue_id': 6435, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '8c0f8dbdadc86b39', 'authors': ['Jincheng Zhong', 'Boyuan Jiang', 'Xin Tao', 'Pengfei Wan', 'Kun Gai', 'Mingsheng Long'], 'affiliations': ['Kling Team, Kuaishou Technology, China', 'School of Software, BNRist, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.12497.jpg', 'data': {'categories': ['#cv', '#diffusion', '#training', '#optimization', '#data'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ ÑˆÑƒĞ¼Ğ° Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ»Ğ¸ "ÑĞ´Ğ²Ğ¸Ğ³Ğ¾Ğ¼ ÑˆÑƒĞ¼Ğ°". Ğ­Ñ‚Ğ¾Ñ‚ ÑĞ´Ğ²Ğ¸Ğ³ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Noise Awareness Guidance (NAG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ½Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ NAG Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Aligning Noise for Better Generative Models', 'desc': 'This paper introduces Noise Awareness Guidance (NAG), a method designed to correct noise shift in diffusion models. Noise shift occurs when there is a mismatch between the expected noise level and the actual noise level during the sampling process, leading to poor generation quality. NAG aligns the sampling trajectories with a pre-defined noise schedule, ensuring that the model generates outputs that are more accurate and consistent. The authors also present a classifier-free version of NAG that simplifies the training process while maintaining high performance in generating images and other tasks.'}, 'zh': {'title': 'å™ªå£°æ„è¯†å¼•å¯¼ï¼šæå‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆè´¨é‡çš„å…³é”®', 'desc': 'å™ªå£°æ„è¯†å¼•å¯¼ï¼ˆNAGï¼‰è§£å†³äº†æ‰©æ•£æ¨¡å‹ä¸­çš„å™ªå£°åç§»é—®é¢˜ï¼Œé€šè¿‡å°†é‡‡æ ·è½¨è¿¹ä¸é¢„å®šä¹‰çš„å™ªå£°è°ƒåº¦å¯¹é½ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°æœ‰çš„å»å™ªç”Ÿæˆæ¨¡å‹åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œé¢„å®šä¹‰çš„å™ªå£°æ°´å¹³ä¸å®é™…å™ªå£°æ°´å¹³ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºå™ªå£°åç§»ã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†å™ªå£°åç§»åœ¨ç°ä»£æ‰©æ•£æ¨¡å‹ä¸­æ™®éå­˜åœ¨ï¼Œå¹¶å¯¼è‡´ç”Ÿæˆæ•ˆæœä¸ä½³ã€‚NAGæ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„ä¿®æ­£æ–¹æ³•ï¼Œèƒ½å¤Ÿæ˜¾è‘—æ”¹å–„ä¸»æµæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12402', 'title': 'Cautious Weight Decay', 'url': 'https://huggingface.co/papers/2510.12402', 'abstract': 'Cautious Weight Decay (CWD) enhances optimizer performance by applying weight decay selectively, improving accuracy and loss in large-scale models without additional tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic modification that applies weight decay only to parameter coordinates whose signs align with the optimizer update. Unlike standard decoupled decay, which implicitly optimizes a regularized or constrained objective, CWD preserves the original loss and admits a bilevel interpretation: it induces sliding-mode behavior upon reaching the stationary manifold, allowing it to search for locally Pareto-optimal stationary points of the unmodified objective. In practice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon, requiring no new hyperparameters or additional tuning. For language model pre-training and ImageNet classification, CWD consistently improves final loss and accuracy at million- to billion-parameter scales.', 'score': 1, 'issue_id': 6435, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '0edcc8534d4b352b', 'authors': ['Lizhang Chen', 'Jonathan Li', 'Kaizhao Liang', 'Baiyu Su', 'Cong Xie', 'Nuo Wang Pierse', 'Chen Liang', 'Ni Lao', 'Qiang Liu'], 'affiliations': ['Google', 'University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2510.12402.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğµ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ²: ÑƒĞ¼Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Cautious Weight Decay (CWD) â€” Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ weight decay Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğº Ñ‚ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼, Ñ‡ÑŒĞ¸ Ğ·Ğ½Ğ°ĞºĞ¸ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ÑÑ‚ Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ decoupled weight decay, CWD ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ğ¿Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº drop-in Ğ·Ğ°Ğ¼ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ñ‚Ğ¸Ğ¿Ğ° AdamW, Lion Ğ¸ Muon, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ImageNet Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ loss Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ¾ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Selective Weight Decay for Optimizer Boosting', 'desc': "Cautious Weight Decay (CWD) is a new technique that improves the performance of optimizers by applying weight decay selectively based on the direction of the optimizer's updates. This method differs from traditional weight decay by maintaining the original loss function, allowing for a more effective search for optimal solutions. CWD can be easily integrated into existing optimizers like AdamW, Lion, and Muon without needing extra tuning or new hyperparameters. In experiments, CWD has shown to enhance accuracy and reduce loss in large-scale models, particularly in language model training and ImageNet classification tasks."}, 'zh': {'title': 'è°¨æ…æƒé‡è¡°å‡ï¼šä¼˜åŒ–å™¨æ€§èƒ½çš„æ–°çªç ´', 'desc': 'è°¨æ…æƒé‡è¡°å‡ï¼ˆCWDï¼‰æ˜¯ä¸€ç§ä¼˜åŒ–å™¨æ— å…³çš„ä¿®æ”¹æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°åº”ç”¨æƒé‡è¡°å‡æ¥æå‡ä¼˜åŒ–å™¨çš„æ€§èƒ½ã€‚å®ƒåªå¯¹ä¸ä¼˜åŒ–å™¨æ›´æ–°æ–¹å‘ä¸€è‡´çš„å‚æ•°åæ ‡è¿›è¡Œæƒé‡è¡°å‡ï¼Œä»è€Œæ”¹å–„å¤§è§„æ¨¡æ¨¡å‹çš„å‡†ç¡®æ€§å’ŒæŸå¤±ã€‚ä¸æ ‡å‡†çš„è§£è€¦è¡°å‡ä¸åŒï¼ŒCWDä¿æŒäº†åŸå§‹æŸå¤±ï¼Œå¹¶å…è®¸åŒå±‚æ¬¡çš„è§£é‡Šï¼Œèƒ½å¤Ÿåœ¨è¾¾åˆ°é™æ€æµå½¢æ—¶è¯±å¯¼æ»‘æ¨¡è¡Œä¸ºã€‚CWDå¯ä»¥ç›´æ¥åº”ç”¨äºå¦‚AdamWã€Lionå’ŒMuonç­‰ä¼˜åŒ–å™¨ï¼Œæ— éœ€æ–°çš„è¶…å‚æ•°æˆ–é¢å¤–çš„è°ƒä¼˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12323', 'title': 'RAG-Anything: All-in-One RAG Framework', 'url': 'https://huggingface.co/papers/2510.12323', 'abstract': 'RAG-Anything is a unified framework that enhances multimodal knowledge retrieval by integrating cross-modal relationships and semantic matching, outperforming existing methods on complex benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inherently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content, creating fundamental gaps when processing multimodal documents. We present RAG-Anything, a unified framework that enables comprehensive knowledge retrieval across all modalities. Our approach reconceptualizes multimodal content as interconnected knowledge entities rather than isolated data types. The framework introduces dual-graph construction to capture both cross-modal relationships and textual semantics within a unified representation. We develop cross-modal hybrid retrieval that combines structural knowledge navigation with semantic matching. This enables effective reasoning over heterogeneous content where relevant evidence spans multiple modalities. RAG-Anything demonstrates superior performance on challenging multimodal benchmarks, achieving significant improvements over state-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. Our framework establishes a new paradigm for multimodal knowledge access, eliminating the architectural fragmentation that constrains current systems. Our framework is open-sourced at: https://github.com/HKUDS/RAG-Anything.', 'score': 1, 'issue_id': 6433, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '0370cbb382f40700', 'authors': ['Zirui Guo', 'Xubin Ren', 'Lingrui Xu', 'Jiahao Zhang', 'Chao Huang'], 'affiliations': ['The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.12323.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#open_source', '#reasoning', '#agi', '#benchmark', '#rag'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ RAG Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'RAG-Anything â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Retrieval-Augmented Generation (RAG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ. RAG-Anything Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unlocking Multimodal Knowledge Retrieval with RAG-Anything', 'desc': 'RAG-Anything is a new framework designed to improve how we retrieve knowledge from different types of information, like text, images, and tables. It addresses the limitations of existing methods that only focus on text, allowing for better understanding and processing of complex documents that contain various formats. By using a dual-graph structure, it captures relationships between different types of data and enhances semantic matching. This leads to better performance on challenging tasks, especially with long documents, making it a significant advancement in multimodal knowledge retrieval.'}, 'zh': {'title': 'RAG-Anythingï¼šå¤šæ¨¡æ€çŸ¥è¯†æ£€ç´¢çš„æ–°èŒƒå¼', 'desc': 'RAG-Anythingæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆè·¨æ¨¡æ€å…³ç³»å’Œè¯­ä¹‰åŒ¹é…æ¥å¢å¼ºå¤šæ¨¡æ€çŸ¥è¯†æ£€ç´¢ã€‚å®ƒè§£å†³äº†ç°æœ‰RAGæ–¹æ³•åœ¨å¤„ç†å¤æ‚åŸºå‡†æ—¶çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æ–‡æ¡£ä¸Šè¡¨ç°å‡ºè‰²ã€‚è¯¥æ¡†æ¶å°†å¤šæ¨¡æ€å†…å®¹è§†ä¸ºç›¸äº’å…³è”çš„çŸ¥è¯†å®ä½“ï¼Œè€Œä¸æ˜¯å­¤ç«‹çš„æ•°æ®ç±»å‹ï¼Œä»è€Œå®ç°å…¨é¢çš„çŸ¥è¯†æ£€ç´¢ã€‚é€šè¿‡åŒå›¾æ„å»ºå’Œè·¨æ¨¡æ€æ··åˆæ£€ç´¢ï¼ŒRAG-Anythingèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¼‚æ„å†…å®¹ï¼Œæå‡äº†å¤šæ¨¡æ€çŸ¥è¯†è®¿é—®çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12117', 'title': 'Locket: Robust Feature-Locking Technique for Language Models', 'url': 'https://huggingface.co/papers/2510.12117', 'abstract': 'Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to generate revenue, offering basic models for free users, and advanced models for paying subscribers. However, a finer-grained pay-to-unlock scheme for premium features (e.g., math, coding) is thought to be more economically viable for the providers. Such a scheme requires a feature-locking technique (FLoTE) which is (i) effective in refusing locked features, (ii) utility-preserving for unlocked features, (iii) robust against evasion or unauthorized credential sharing, and (iv) scalable to multiple features and users. However, existing FLoTEs (e.g., password-locked models) are not robust or scalable. We present Locket, the first robust and scalable FLoTE to enable pay-to-unlock schemes. Locket uses a novel merging approach to attach adapters to an LLM for refusing unauthorized features. Our comprehensive evaluation shows that Locket is effective (100% refusal on locked features), utility-preserving (leq 7% utility degradation in unlocked features), robust (leq 5% attack success rate), and scales to multiple features and clients.', 'score': 1, 'issue_id': 6437, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '18495416930a748d', 'authors': ['Lipeng He', 'Vasisht Duddu', 'N. Asokan'], 'affiliations': ['University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2510.12117.jpg', 'data': {'categories': ['#security', '#architecture', '#training', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'Locket: ĞœĞ¾Ğ½ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ AI Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞºĞµ', 'desc': 'ĞŸÑ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ñ‹ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞºĞ¸, Ğ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹Ğ³Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ…ĞµĞ¼Ğ° Ğ¾Ğ¿Ğ»Ğ°Ñ‚Ñ‹ Ğ·Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ). Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ² LLM Ğ½Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‰ĞµĞ½Ñ‹ Ğ¾Ñ‚ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Locket Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ğ»Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ 100% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ° Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 5% ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ°) Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Features with Robust Security: Introducing Locket', 'desc': 'This paper introduces Locket, a new feature-locking technique (FLoTE) designed for chatbot providers to implement a pay-to-unlock model for premium features. Locket ensures that locked features are effectively refused while maintaining high utility for unlocked features, with only a 7% degradation in performance. It also demonstrates robustness against unauthorized access attempts, achieving a low attack success rate of 5%. Additionally, Locket is scalable, allowing multiple features to be managed across various users without compromising security or functionality.'}, 'zh': {'title': 'Locketï¼šè§£é”é«˜çº§åŠŸèƒ½çš„æ–°æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLocketçš„ç‰¹å¾é”å®šæŠ€æœ¯ï¼ˆFLoTEï¼‰ï¼Œæ—¨åœ¨æ”¯æŒæŒ‰éœ€è§£é”é«˜çº§åŠŸèƒ½çš„ä»˜è´¹æ¨¡å¼ã€‚Locketèƒ½å¤Ÿæœ‰æ•ˆæ‹’ç»æœªæˆæƒçš„åŠŸèƒ½ï¼ŒåŒæ—¶ä¿æŒå·²è§£é”åŠŸèƒ½çš„å®ç”¨æ€§ã€‚é€šè¿‡åˆ›æ–°çš„åˆå¹¶æ–¹æ³•ï¼ŒLocketå®ç°äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„é€‚é…ï¼Œç¡®ä¿å…¶åœ¨å¤šç”¨æˆ·å’Œå¤šåŠŸèƒ½åœºæ™¯ä¸‹çš„å¯æ‰©å±•æ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒLocketåœ¨æ‹’ç»é”å®šåŠŸèƒ½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸”å¯¹å·²è§£é”åŠŸèƒ½çš„å½±å“æå°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11851', 'title': 'Deep Research Brings Deeper Harm', 'url': 'https://huggingface.co/papers/2510.11851', 'abstract': "DR agents based on LLMs can generate detailed reports from harmful queries, highlighting alignment failures and the need for specialized safety measures.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research (DR) agents built on Large Language Models (LLMs) can perform complex, multi-step research by decomposing tasks, retrieving online information, and synthesizing detailed reports. However, the misuse of LLMs with such powerful capabilities can lead to even greater risks. This is especially concerning in high-stakes and knowledge-intensive domains such as biosecurity, where DR can generate a professional report containing detailed forbidden knowledge. Unfortunately, we have found such risks in practice: simply submitting a harmful query, which a standalone LLM directly rejects, can elicit a detailed and dangerous report from DR agents. This highlights the elevated risks and underscores the need for a deeper safety analysis. Yet, jailbreak methods designed for LLMs fall short in exposing such unique risks, as they do not target the research ability of DR agents. To address this gap, we propose two novel jailbreak strategies: Plan Injection, which injects malicious sub-goals into the agent's plan; and Intent Hijack, which reframes harmful queries as academic research questions. We conducted extensive experiments across different LLMs and various safety benchmarks, including general and biosecurity forbidden prompts. These experiments reveal 3 key findings: (1) Alignment of the LLMs often fail in DR agents, where harmful prompts framed in academic terms can hijack agent intent; (2) Multi-step planning and execution weaken the alignment, revealing systemic vulnerabilities that prompt-level safeguards cannot address; (3) DR agents not only bypass refusals but also produce more coherent, professional, and dangerous content, compared with standalone LLMs. These results demonstrate a fundamental misalignment in DR agents and call for better alignment techniques tailored to DR agents. Code and datasets are available at https://chenxshuo.github.io/deeper-harm.", 'score': 1, 'issue_id': 6433, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '8b7627eb01fd311a', 'authors': ['Shuo Chen', 'Zonggen Li', 'Zhen Han', 'Bailan He', 'Tong Liu', 'Haokun Chen', 'Georg Groh', 'Philip Torr', 'Volker Tresp', 'Jindong Gu'], 'affiliations': ['AWS AI', 'Konrad Zuse School of Excellence in Reliable AI (relAI)', 'LMU Munich', 'Munich Center for Machine Learning (MCML)', 'Technical University of Munich (TUM)', 'University of Hong Kong (HKU)', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2510.11851.jpg', 'data': {'categories': ['#dataset', '#agents', '#ethics', '#rlhf', '#security', '#alignment', '#benchmark'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ĞºĞ¾Ğ³Ğ´Ğ° ÑƒĞ¼Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±Ñ…Ğ¾Ğ´ÑÑ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Deep Research Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼, Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ñ… Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€ÑƒĞµÑ‚. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ¶ĞµĞ¹Ğ»Ğ±Ñ€ĞµĞ¹ĞºĞ°: Plan Injection (Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ†ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ»Ğ°Ğ½ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°) Ğ¸ Intent Hijack (Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ÑĞµÑ‚ alignment Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° DR-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ñ‡ĞµĞ¼ standalone LLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ DR-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² alignment Ğ´Ğ»Ñ Ñ‚Ğ°ĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Unmasking Risks: Enhancing Safety in Deep Research Agents', 'desc': 'This paper discusses the risks associated with Deep Research (DR) agents that utilize Large Language Models (LLMs) to generate detailed reports from complex queries. It highlights how these agents can produce harmful content when given seemingly academic prompts, revealing a failure in alignment with safety protocols. The authors propose two new jailbreak strategies, Plan Injection and Intent Hijack, to expose these vulnerabilities in DR agents. The findings indicate that the multi-step capabilities of DR agents can lead to more coherent and dangerous outputs, necessitating improved alignment techniques for safety.'}, 'zh': {'title': 'æ·±åº¦ç ”ç©¶ä»£ç†çš„å®‰å…¨éšæ‚£ä¸å¯¹ç­–', 'desc': 'åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰ä»£ç†å¯ä»¥ä»æœ‰å®³æŸ¥è¯¢ä¸­ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šï¼Œæ­ç¤ºäº†å¯¹é½å¤±è´¥å’Œéœ€è¦ä¸“é—¨å®‰å…¨æªæ–½çš„å¿…è¦æ€§ã€‚å°½ç®¡LLMså…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å…¶è¯¯ç”¨å¯èƒ½å¯¼è‡´æ›´å¤§çš„é£é™©ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿç‰©å®‰å…¨ç­‰é«˜é£é™©é¢†åŸŸã€‚ç ”ç©¶å‘ç°ï¼Œç®€å•çš„æœ‰å®³æŸ¥è¯¢å¯ä»¥å¼•å‘DRä»£ç†ç”Ÿæˆå±é™©çš„æŠ¥å‘Šï¼Œè¿™çªæ˜¾äº†å®‰å…¨åˆ†æçš„å¿…è¦æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§æ–°çš„è¶Šç‹±ç­–ç•¥ï¼Œä»¥åº”å¯¹DRä»£ç†çš„ç‹¬ç‰¹é£é™©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11570', 'title': 'Bag of Tricks for Subverting Reasoning-based Safety Guardrails', 'url': 'https://huggingface.co/papers/2510.11570', 'abstract': "Reasoning-based safety guardrails in Large Reasoning Models are vulnerable to subtle prompt manipulations, leading to high attack success rates across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs), such as deliberative alignment, have shown strong defense against jailbreak attacks. By leveraging LRMs' reasoning ability, these guardrails help the models to assess the safety of user inputs before generating final responses. The powerful reasoning ability can analyze the intention of the input query and will refuse to assist once it detects the harmful intent hidden by the jailbreak methods. Such guardrails have shown a significant boost in defense, such as the near-perfect refusal rates on the open-source gpt-oss series. Unfortunately, we find that these powerful reasoning-based guardrails can be extremely vulnerable to subtle manipulation of the input prompts, and once hijacked, can lead to even more harmful results. Specifically, we first uncover a surprisingly fragile aspect of these guardrails: simply adding a few template tokens to the input prompt can successfully bypass the seemingly powerful guardrails and lead to explicit and harmful responses. To explore further, we introduce a bag of jailbreak methods that subvert the reasoning-based guardrails. Our attacks span white-, gray-, and black-box settings and range from effortless template manipulations to fully automated optimization. Along with the potential for scalable implementation, these methods also achieve alarmingly high attack success rates (e.g., exceeding 90% across 5 different benchmarks on gpt-oss series on both local host models and online API services). Evaluations across various leading open-source LRMs confirm that these vulnerabilities are systemic, underscoring the urgent need for stronger alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is open-sourced at https://chenxshuo.github.io/bag-of-tricks.", 'score': 1, 'issue_id': 6433, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '3aff1a3ca1972fc7', 'authors': ['Shuo Chen', 'Zhen Han', 'Haokun Chen', 'Bailan He', 'Shengyun Si', 'Jingpei Wu', 'Philip Torr', 'Volker Tresp', 'Jindong Gu'], 'affiliations': ['AWS AI', 'DFKI', 'Konrad Zuse School of Excellence in Reliable AI (relAI)', 'LMU Munich', 'Munich Center for Machine Learning (MCML)', 'Technical University of Berlin', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2510.11570.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#rlhf', '#security', '#alignment', '#benchmark', '#architecture'], 'emoji': 'ğŸ”“', 'ru': {'title': 'Ğ¥Ñ€ÑƒĞ¿ĞºĞ°Ñ Ğ±Ñ€Ğ¾Ğ½Ñ: ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ»Ğ¾Ğ¼Ğ°ÑÑ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ reasoning-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ (LRM), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° deliberative alignment. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ñ‚Ğ°Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ… (white-box, gray-box, black-box) Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 90% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº alignment Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LRM, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ·Ğ»Ğ¾Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Strengthening Safety: Guardrails Are Not Foolproof!', 'desc': "This paper discusses the vulnerabilities of reasoning-based safety guardrails in Large Reasoning Models (LRMs) against subtle prompt manipulations. Although these guardrails, like deliberative alignment, are designed to enhance safety by analyzing user inputs for harmful intent, they can be easily bypassed with minor changes to the input prompts. The authors demonstrate that simple template modifications can lead to high attack success rates, revealing a significant flaw in the guardrails' effectiveness. The study emphasizes the need for improved alignment techniques to safeguard LRMs from malicious exploitation."}, 'zh': {'title': 'å¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹çš„å®‰å…¨é˜²æŠ¤ï¼ŒæŠµå¾¡å¾®å¦™æ”»å‡»ï¼', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„åŸºäºæ¨ç†çš„å®‰å…¨é˜²æŠ¤æªæ–½çš„è„†å¼±æ€§ã€‚å°½ç®¡è¿™äº›é˜²æŠ¤æªæ–½åœ¨æŠµå¾¡è¶Šç‹±æ”»å‡»æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†ç ”ç©¶å‘ç°ï¼Œè¾“å…¥æç¤ºçš„å¾®å°æ“æ§å¯ä»¥è½»æ˜“ç»•è¿‡è¿™äº›é˜²æŠ¤ã€‚ä½œè€…æå‡ºäº†ä¸€ç³»åˆ—è¶Šç‹±æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åœ¨ä¸åŒçš„ç¯å¢ƒä¸‹éƒ½èƒ½æˆåŠŸæ”»å‡»ï¼ŒæˆåŠŸç‡é«˜è¾¾90%ä»¥ä¸Šã€‚è®ºæ–‡å¼ºè°ƒäº†è¿«åˆ‡éœ€è¦æ›´å¼ºçš„å¯¹é½æŠ€æœ¯ï¼Œä»¥é˜²æ­¢å¼€æºå¤§å‹æ¨ç†æ¨¡å‹çš„æ¶æ„æ»¥ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11545', 'title': 'Information-Preserving Reformulation of Reasoning Traces for\n  Antidistillation', 'url': 'https://huggingface.co/papers/2510.11545', 'abstract': "PART reformulates reasoning traces to preserve information while disrupting unauthorized distillation in Large Language Models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) show that extending the length of reasoning chains significantly improves performance on complex tasks. While revealing these reasoning traces helps users better follow, verify, and learn from the model's problem-solving process, it also makes them highly vulnerable to unauthorized distillation. To mitigate this risk, proprietary model providers often adopt aggressive protection strategies, such as replacing detailed reasoning with brief summaries, which deprive users of valuable intermediate information. To address this trade-off, we propose PART, an information-preserving antidistillation reformulation of reasoning traces. Motivated by the difference between how humans understand reasoning traces and how LLMs exploit them for supervised fine-tuning, we design a simple but effective two-step reformulation: removing self-talk behaviors and reordering sub-conclusions. A small auxiliary model is trained to perform this reformulation, incurring minimal computational overhead. Extensive experiments demonstrate that PART consistently disrupts distillation across student models of different sizes and types on various reasoning benchmarks. For instance, when training on reformulated traces, even the performance of a large 32B student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a 13.5% degradation.", 'score': 1, 'issue_id': 6427, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '01998e70fa5eff66', 'authors': ['Jiayu Ding', 'Lei Cui', 'Li Dong', 'Nanning Zheng', 'Furu Wei'], 'affiliations': ['IAIR, Xian Jiaotong University', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.11545.jpg', 'data': {'categories': ['#hallucinations', '#reasoning', '#benchmark', '#data', '#training'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ğ¾Ñ‚ ĞºÑ€Ğ°Ğ¶Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ PART Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… reasoning-Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ½ĞµÑĞ°Ğ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚Ğ°ĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑƒĞ¿Ğ°Ğ»Ğ° Ñ 54.17 Ğ´Ğ¾ 46.88 Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ AIME 2024, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ Ğ¾Ñ‚ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'PART: Protecting Reasoning While Preserving Information', 'desc': 'The paper introduces PART, a method designed to reformulate reasoning traces in Large Language Models (LLMs) to protect against unauthorized distillation while retaining essential information. It highlights the challenge of balancing user access to detailed reasoning and the risk of distillation, which can compromise model integrity. PART employs a two-step process that removes unnecessary self-talk and reorganizes conclusions to make the reasoning less exploitable. Experimental results show that this approach effectively reduces the performance of student models trained on the reformulated traces, demonstrating its effectiveness in safeguarding proprietary information.'}, 'zh': {'title': 'ä¿æŠ¤æ¨ç†ä¿¡æ¯ï¼Œé˜²æ­¢æœªç»æˆæƒçš„è’¸é¦', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPARTçš„æ–¹æ³•ï¼Œç”¨äºé‡æ–°æ„å»ºæ¨ç†è½¨è¿¹ï¼Œä»¥ä¿æŠ¤ä¿¡æ¯å¹¶é˜²æ­¢å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æœªç»æˆæƒçš„è’¸é¦ã€‚éšç€æ¨ç†é“¾é•¿åº¦çš„å¢åŠ ï¼Œæ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œä½†è¿™ä¹Ÿä½¿å¾—æ¨ç†è½¨è¿¹å®¹æ˜“å—åˆ°æ”»å‡»ã€‚PARTé€šè¿‡å»é™¤è‡ªè¨€è‡ªè¯­è¡Œä¸ºå’Œé‡æ–°æ’åºå­ç»“è®ºï¼Œè®¾è®¡äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„ä¸¤æ­¥é‡æ„è¿‡ç¨‹ï¼Œä»è€Œåœ¨ä¿æŒä¿¡æ¯çš„åŒæ—¶ï¼Œé™ä½äº†è’¸é¦çš„é£é™©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPARTåœ¨ä¸åŒå¤§å°å’Œç±»å‹çš„å­¦ç”Ÿæ¨¡å‹ä¸Šéƒ½èƒ½æœ‰æ•ˆå¹²æ‰°è’¸é¦è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.11330', 'title': 'Diffusion-Link: Diffusion Probabilistic Model for Bridging the\n  Audio-Text Modality Gap', 'url': 'https://huggingface.co/papers/2510.11330', 'abstract': 'Diffusion-Link, a diffusion-based modality-bridging module, reduces the audio-text modality gap and enhances multimodal encoder-LLM coupling, achieving state-of-the-art performance in automatic audio captioning.  \t\t\t\t\tAI-generated summary \t\t\t\t Contrastive audio-language pretraining yields powerful joint representations, yet a persistent audio-text modality gap limits the benefits of coupling multimodal encoders with large language models (LLMs). We present Diffusion-Link, a diffusion-based modality-bridging module that generatively maps audio embeddings into the text-embedding distribution. The module is trained at the output embedding from the frozen multimodal encoder and implemented as a lightweight network with three residual MLP blocks. To assess the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on Automatic Audio Captioning (AAC); to our knowledge, this is the first application of diffusion-based modality bridging to AAC. We report two results. (1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link reduces the modality gap the most among prior diffusion-based methods and shows a collective migration of audio embeddings toward the text distribution. (2) Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised captioning without external knowledge, with relative gains up to 52.5% and 7.5%, respectively. These findings show that closing the modality gap is pivotal for effective coupling between multimodal encoders and LLMs, and diffusion-based modality bridging offers a promising direction beyond knowledge-retrieval-centric designs. Code will be released upon acceptance https://github.com/DevKiHyun/Diffusion-Link', 'score': 1, 'issue_id': 6435, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': '365489ecaa99e2fc', 'authors': ['KiHyun Nam', 'Jongmin Choi', 'Hyeongkeun Lee', 'Jungwoo Heo', 'Joon Son Chung'], 'affiliations': ['Korea Advanced Institute of Science and Technology, South Korea', 'University of Seoul, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2510.11330.jpg', 'data': {'categories': ['#diffusion', '#audio', '#games', '#multimodal'], 'emoji': 'ğŸµ', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ²ÑƒĞºĞ¾Ğ¼ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Diffusion-Link â€” Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ LLM Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ (AAC). ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ AudioCaps, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ´Ğ¾ 52.5% Ğ² zero-shot Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ¸ 7.5% Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Bridging the Audio-Text Gap with Diffusion-Link', 'desc': 'The paper introduces Diffusion-Link, a novel module designed to bridge the gap between audio and text modalities in multimodal learning. By using a diffusion-based approach, it effectively maps audio embeddings to the text-embedding space, enhancing the integration of multimodal encoders with large language models (LLMs). The authors demonstrate that Diffusion-Link significantly reduces the audio-text modality gap, leading to improved performance in automatic audio captioning tasks. Their results show that this method achieves state-of-the-art performance, highlighting the importance of closing modality gaps for better multimodal interactions.'}, 'zh': {'title': 'ç¼©å°æ¨¡æ€å·®è·ï¼Œæå‡å¤šæ¨¡æ€æ€§èƒ½', 'desc': 'Diffusion-Link æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ¨¡æ€æ¡¥æ¥æ¨¡å—ï¼Œæ—¨åœ¨ç¼©å°éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€å·®è·ï¼Œä»è€Œå¢å¼ºå¤šæ¨¡æ€ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»“åˆã€‚è¯¥æ¨¡å—é€šè¿‡ç”Ÿæˆæ€§æ˜ å°„å°†éŸ³é¢‘åµŒå…¥æ˜ å°„åˆ°æ–‡æœ¬åµŒå…¥åˆ†å¸ƒï¼Œç»è¿‡è®­ç»ƒåèƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æ¨¡æ€å·®è·ã€‚æˆ‘ä»¬åœ¨è‡ªåŠ¨éŸ³é¢‘å­—å¹•ç”Ÿæˆï¼ˆAACï¼‰ä»»åŠ¡ä¸­è¯„ä¼°äº† Diffusion-Link çš„æ•ˆæœï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨é›¶æ ·æœ¬å’Œå®Œå…¨ç›‘ç£çš„å­—å¹•ç”Ÿæˆä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç¼©å°æ¨¡æ€å·®è·å¯¹äºå¤šæ¨¡æ€ç¼–ç å™¨ä¸ LLM çš„æœ‰æ•ˆç»“åˆè‡³å…³é‡è¦ï¼Œæ‰©æ•£åŸºç¡€çš„æ¨¡æ€æ¡¥æ¥æä¾›äº†ä¸€ç§è¶…è¶ŠçŸ¥è¯†æ£€ç´¢è®¾è®¡çš„æœ‰å‰æ™¯çš„æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09776', 'title': 'Why Do Transformers Fail to Forecast Time Series In-Context?', 'url': 'https://huggingface.co/papers/2510.09776', 'abstract': "Theoretical analysis reveals that Transformers, particularly Linear Self-Attention models, have limitations in time series forecasting compared to classical linear models, with predictions collapsing to the mean under Chain-of-Thought inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Time series forecasting (TSF) remains a challenging and largely unsolved problem in machine learning, despite significant recent efforts leveraging Large Language Models (LLMs), which predominantly rely on Transformer architectures. Empirical evidence consistently shows that even powerful Transformers often fail to outperform much simpler models, e.g., linear models, on TSF tasks; however, a rigorous theoretical understanding of this phenomenon remains limited. In this paper, we provide a theoretical analysis of Transformers' limitations for TSF through the lens of In-Context Learning (ICL) theory. Specifically, under AR(p) data, we establish that: (1) Linear Self-Attention (LSA) models cannot achieve lower expected MSE than classical linear models for in-context forecasting; (2) as the context length approaches to infinity, LSA asymptotically recovers the optimal linear predictor; and (3) under Chain-of-Thought (CoT) style inference, predictions collapse to the mean exponentially. We empirically validate these findings through carefully designed experiments. Our theory not only sheds light on several previously underexplored phenomena but also offers practical insights for designing more effective forecasting architectures. We hope our work encourages the broader research community to revisit the fundamental theoretical limitations of TSF and to critically evaluate the direct application of increasingly sophisticated architectures without deeper scrutiny.", 'score': 1, 'issue_id': 6439, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': '4b37b497cf2b2824', 'authors': ['Yufa Zhou', 'Yixiao Wang', 'Surbhi Goel', 'Anru R. Zhang'], 'affiliations': ['Duke University', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2510.09776.jpg', 'data': {'categories': ['#optimization', '#architecture', '#math', '#reasoning', '#training'], 'emoji': 'ğŸ“‰', 'ru': {'title': 'ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Transformer-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Linear Self-Attention, Ğ¸Ğ¼ĞµÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ². ĞĞ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LSA-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ½ĞµĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ. ĞŸÑ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Chain-of-Thought Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑÑ…Ğ»Ğ¾Ğ¿Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğº ÑÑ€ĞµĞ´Ğ½ĞµĞ¼Ñƒ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ±ĞµĞ· Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ñ… Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Transformers vs. Linear Models: A Time Series Showdown', 'desc': 'This paper analyzes the limitations of Transformer models, especially Linear Self-Attention (LSA), in the context of time series forecasting (TSF). It shows that LSA models cannot achieve better mean squared error (MSE) than classical linear models when predicting future values. The study reveals that as the context length increases, LSA approaches the performance of optimal linear predictors, but under Chain-of-Thought inference, predictions tend to collapse to the mean. The authors provide empirical evidence to support their theoretical findings and encourage further exploration of the fundamental challenges in TSF.'}, 'zh': {'title': 'å˜æ¢å™¨åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„å±€é™æ€§åˆ†æ', 'desc': 'æœ¬è®ºæ–‡åˆ†æäº†å˜æ¢å™¨ï¼ˆTransformersï¼‰åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯çº¿æ€§è‡ªæ³¨æ„åŠ›æ¨¡å‹ï¼ˆLinear Self-Attentionï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨é“¾å¼æ¨ç†ï¼ˆChain-of-Thoughtï¼‰ä¸‹ï¼Œè¿™äº›æ¨¡å‹çš„é¢„æµ‹ç»“æœè¶‹å‘äºå‡å€¼ï¼Œæ— æ³•è¶…è¶Šä¼ ç»Ÿçº¿æ€§æ¨¡å‹çš„è¡¨ç°ã€‚é€šè¿‡å¯¹AR(p)æ•°æ®çš„ç†è®ºåˆ†æï¼Œæˆ‘ä»¬å‘ç°çº¿æ€§è‡ªæ³¨æ„åŠ›æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡é¢„æµ‹ä¸­æ— æ³•å®ç°æ¯”ç»å…¸çº¿æ€§æ¨¡å‹æ›´ä½çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€‚æˆ‘ä»¬çš„å®éªŒéªŒè¯äº†è¿™äº›ç†è®ºç»“æœï¼Œå¹¶ä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„é¢„æµ‹æ¶æ„æä¾›äº†å®ç”¨è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09263', 'title': 'SynthID-Image: Image watermarking at internet scale', 'url': 'https://huggingface.co/papers/2510.09263', 'abstract': "SynthID-Image, a deep learning system for watermarking AI-generated imagery, demonstrates state-of-the-art performance in visual quality and robustness, and is deployed across Google's services.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SynthID-Image, a deep learning-based system for invisibly watermarking AI-generated imagery. This paper documents the technical desiderata, threat models, and practical challenges of deploying such a system at internet scale, addressing key requirements of effectiveness, fidelity, robustness, and security. SynthID-Image has been used to watermark over ten billion images and video frames across Google's services and its corresponding verification service is available to trusted testers. For completeness, we present an experimental evaluation of an external model variant, SynthID-O, which is available through partnerships. We benchmark SynthID-O against other post-hoc watermarking methods from the literature, demonstrating state-of-the-art performance in both visual quality and robustness to common image perturbations. While this work centers on visual media, the conclusions on deployment, constraints, and threat modeling generalize to other modalities, including audio. This paper provides a comprehensive documentation for the large-scale deployment of deep learning-based media provenance systems.", 'score': 1, 'issue_id': 6429, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': '8465fb94f5d757f3', 'authors': ['Sven Gowal', 'Rudy Bunel', 'Florian Stimberg', 'David Stutz', 'Guillermo Ortiz-Jimenez', 'Christina Kouridi', 'Mel Vecerik', 'Jamie Hayes', 'Sylvestre-Alvise Rebuffi', 'Paul Bernard', 'Chris Gamble', 'MiklÃ³s Z. HorvÃ¡th', 'Fabian Kaczmarczyck', 'Alex Kaskasoli', 'Aleksandar Petrov', 'Ilia Shumailov', 'Meghana Thotakuri', 'Olivia Wiles', 'Jessica Yung', 'Zahra Ahmed', 'Victor Martin', 'Simon Rosen', 'Christopher SavÄak', 'Armin Senoner', 'Nidhi Vyas', 'Pushmeet Kohli'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2510.09263.jpg', 'data': {'categories': ['#optimization', '#security', '#multimodal', '#benchmark', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'SynthID-Image: Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ AI-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SynthID-Image â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾Ğ¹ Ğ²Ğ¾Ğ´ÑĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ AI-ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ°ÑÑŒ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµÑ€Ğ²Ğ¸ÑĞ°Ñ… Google, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾Ñ‡Ğ°Ğ¹ÑˆĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ñ‹ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ³Ñ€Ğ¾Ğ· Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°. Ğ¥Ğ¾Ñ‚Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ğ°, Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑƒĞ³Ñ€Ğ¾Ğ· Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹ Ğ¸ Ğº Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾.'}, 'en': {'title': 'Invisible Watermarking for AI Imagery at Scale', 'desc': "SynthID-Image is a deep learning system designed to invisibly watermark images generated by AI, ensuring their authenticity and traceability. The paper discusses the technical requirements and challenges of implementing this watermarking system at a large scale, focusing on effectiveness, fidelity, robustness, and security. It highlights the successful deployment of SynthID-Image across Google's services, where it has watermarked over ten billion images and video frames. Additionally, the paper compares SynthID-O, an external model variant, with other watermarking methods, showcasing its superior performance in visual quality and resilience to image alterations."}, 'zh': {'title': 'éšå½¢æ°´å°ï¼Œä¿æŠ¤AIå›¾åƒçš„æœªæ¥', 'desc': 'SynthID-Imageæ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„ç³»ç»Ÿï¼Œç”¨äºå¯¹AIç”Ÿæˆçš„å›¾åƒè¿›è¡Œéšå½¢æ°´å°ã€‚è¯¥ç³»ç»Ÿåœ¨è§†è§‰è´¨é‡å’Œé²æ£’æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶å·²åœ¨è°·æ­Œçš„æœåŠ¡ä¸­å¹¿æ³›åº”ç”¨ã€‚è®ºæ–‡è®¨è®ºäº†ç³»ç»Ÿéƒ¨ç½²çš„æŠ€æœ¯è¦æ±‚ã€å¨èƒæ¨¡å‹å’Œå®é™…æŒ‘æˆ˜ï¼Œç¡®ä¿å…¶æœ‰æ•ˆæ€§ã€ä¿çœŸåº¦ã€é²æ£’æ€§å’Œå®‰å…¨æ€§ã€‚SynthID-Imageå·²ç”¨äºæ°´å°è¶…è¿‡100äº¿å¼ å›¾åƒå’Œè§†é¢‘å¸§ï¼Œå¹¶æä¾›äº†ç›¸åº”çš„éªŒè¯æœåŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.09062', 'title': 'ReFIne: A Framework for Trustworthy Large Reasoning Models with\n  Reliability, Faithfulness, and Interpretability', 'url': 'https://huggingface.co/papers/2510.09062', 'abstract': "ReFIne, a new training framework, enhances the trustworthiness of reasoning models by improving interpretability, faithfulness, and reliability through structured traces, decisive information disclosure, and confidence estimates.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in long chain-of-thought (CoT) reasoning have largely prioritized answer accuracy and token efficiency, while overlooking aspects critical to trustworthiness. We argue that usable reasoning systems must be trustworthy, characterized by three properties: interpretability, faithfulness, and reliability. To this end, we propose ReFIne, a new training framework that integrates supervised fine-tuning with GRPO to encourage models to: (i) improve interpretability by producing structured, tag-based traces with high-level planning that are easier for humans to follow; (ii) enhance faithfulness by explicitly disclosing the decisive information guiding each solution, with consistent cross-section references; and (iii) promote reliability by providing self-assessments of both the derivation's soundness and the confidence of the final answer. We apply ReFIne to the Qwen3 models at multiple scales (1.7B/4B/8B) and evaluate across mathematical benchmarks of varying difficulty. Our experimental results show that ReFIne models generate clearer and better-structured reasoning traces (interpretability +44.0%), more faithfully expose their underlying decision process (faithfulness +18.8%), and offer informative confidence estimates (reliability +42.4%). These findings highlight an overlooked but important direction: reasoning models should be optimized not only for accuracy, but also for broader dimensions of trustworthiness. Our code is available at: https://github.com/Trustworthy-ML-Lab/Training_Trustworthy_LRM_with_Refine", 'score': 1, 'issue_id': 6427, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': 'fc791aa1b4447a51', 'authors': ['Chung-En Sun', 'Ge Yan', 'Akshay Kulkarni', 'Tsui-Wei Weng'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2510.09062.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#math', '#training', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ReFIne â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´ĞµĞ»Ğ°ĞµÑ‚ reasoning Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ReFIne ÑƒÑ‡Ğ¸Ñ‚ LLM ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚ĞµĞ³Ğ°Ğ¼Ğ¸, ÑĞ²Ğ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen3 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 44%, faithfulness (Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼) Ğ½Ğ° 18.8% Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 42.4%. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ accuracy, Ğ½Ğ¾ Ğ¸ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ğº reasoning Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑƒ AI.'}, 'en': {'title': 'Enhancing Trustworthiness in Reasoning Models with ReFIne', 'desc': 'ReFIne is a new training framework designed to improve the trustworthiness of reasoning models in machine learning. It focuses on three key aspects: interpretability, faithfulness, and reliability, which are essential for creating usable reasoning systems. The framework uses structured traces and decisive information disclosure to enhance how models communicate their reasoning processes. Experimental results show that models trained with ReFIne significantly outperform traditional models in clarity, decision transparency, and confidence estimation.'}, 'zh': {'title': 'æå‡æ¨ç†æ¨¡å‹çš„å¯ä¿¡åº¦ä¸å¯è§£é‡Šæ€§', 'desc': 'ReFIneæ˜¯ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–çš„è¿½è¸ªã€å…³é”®ä¿¡æ¯çš„æŠ«éœ²å’Œç½®ä¿¡åº¦ä¼°è®¡æ¥å¢å¼ºæ¨ç†æ¨¡å‹çš„å¯ä¿¡åº¦ã€‚è¯¥æ¡†æ¶å¼ºè°ƒå¯è§£é‡Šæ€§ã€å¿ å®æ€§å’Œå¯é æ€§ä¸‰ä¸ªç‰¹æ€§ï¼Œä»¥ç¡®ä¿æ¨ç†ç³»ç»Ÿçš„å¯ç”¨æ€§ã€‚é€šè¿‡å¯¹Qwen3æ¨¡å‹çš„å¤šå°ºåº¦åº”ç”¨ï¼ŒReFIneæ˜¾è‘—æé«˜äº†æ¨ç†è¿‡ç¨‹çš„æ¸…æ™°åº¦å’Œç»“æ„æ€§ï¼ŒåŒæ—¶æ›´å¿ å®åœ°å±•ç¤ºäº†å†³ç­–è¿‡ç¨‹ï¼Œå¹¶æä¾›äº†æœ‰ç”¨çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨ç†æ¨¡å‹ä¸ä»…è¦ä¼˜åŒ–å‡†ç¡®æ€§ï¼Œè¿˜åº”å…³æ³¨æ›´å¹¿æ³›çš„å¯ä¿¡åº¦ç»´åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.12269', 'title': 'Tensor Logic: The Language of AI', 'url': 'https://huggingface.co/papers/2510.12269', 'abstract': 'Tensor logic unifies neural and symbolic AI by using tensor equations, enabling scalable, learnable, and transparent AI systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Progress in AI is hindered by the lack of a programming language with all the requisite features. Libraries like PyTorch and TensorFlow provide automatic differentiation and efficient GPU implementation, but are additions to Python, which was never intended for AI. Their lack of support for automated reasoning and knowledge acquisition has led to a long and costly series of hacky attempts to tack them on. On the other hand, AI languages like LISP an Prolog lack scalability and support for learning. This paper proposes tensor logic, a language that solves these problems by unifying neural and symbolic AI at a fundamental level. The sole construct in tensor logic is the tensor equation, based on the observation that logical rules and Einstein summation are essentially the same operation, and all else can be reduced to them. I show how to elegantly implement key forms of neural, symbolic and statistical AI in tensor logic, including transformers, formal reasoning, kernel machines and graphical models. Most importantly, tensor logic makes new directions possible, such as sound reasoning in embedding space. This combines the scalability and learnability of neural networks with the reliability and transparency of symbolic reasoning, and is potentially a basis for the wider adoption of AI.', 'score': 0, 'issue_id': 6429, 'pub_date': '2025-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '9d8ed85a21f130e2', 'authors': ['Pedro Domingos'], 'affiliations': ['Paul G. Allen School of Computer Science & Engineering University of Washington Seattle, WA 98195-2350, USA'], 'pdf_title_img': 'assets/pdf/title_img/2510.12269.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#interpretability', '#plp', '#agi'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ¢ĞµĞ½Ğ·Ğ¾Ñ€Ğ½Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ°: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ AI', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ tensor logic â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² AI Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ­Ğ¹Ğ½ÑˆÑ‚ĞµĞ¹Ğ½Ğ° â€” ÑÑ‚Ğ¾ Ğ¿Ğ¾ ÑÑƒÑ‚Ğ¸ Ğ¾Ğ´Ğ½Ğ° Ğ¸ Ñ‚Ğ° Ğ¶Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ»ĞµĞ³Ğ°Ğ½Ñ‚Ğ½Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹, Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ â€” Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº Ğ²Ñ€Ğ¾Ğ´Ğµ PyTorch, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼ Ğ½Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ°ĞµÑ‚ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Unifying Neural and Symbolic AI with Tensor Logic', 'desc': 'This paper introduces tensor logic, a new programming language that merges neural and symbolic AI using tensor equations. It addresses the limitations of existing AI frameworks, which either lack scalability or do not support automated reasoning. By recognizing that logical rules and Einstein summation are fundamentally similar, tensor logic simplifies the implementation of various AI models, including transformers and graphical models. This approach enhances the learnability and transparency of AI systems, paving the way for more reliable and scalable AI applications.'}, 'zh': {'title': 'å¼ é‡é€»è¾‘ï¼šç»Ÿä¸€ç¥ç»ä¸ç¬¦å·äººå·¥æ™ºèƒ½çš„è¯­è¨€', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¼–ç¨‹è¯­è¨€â€”â€”å¼ é‡é€»è¾‘ï¼Œå®ƒå°†ç¥ç»ç½‘ç»œå’Œç¬¦å·äººå·¥æ™ºèƒ½ç»Ÿä¸€åœ¨ä¸€ä¸ªåŸºç¡€å±‚é¢ä¸Šã€‚å¼ é‡é€»è¾‘çš„æ ¸å¿ƒæ˜¯å¼ é‡æ–¹ç¨‹ï¼Œè¿™ç§æ–¹ç¨‹èƒ½å¤Ÿå°†é€»è¾‘è§„åˆ™ä¸çˆ±å› æ–¯å¦æ±‚å’Œçš„æ“ä½œç›¸ç»“åˆã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¼ é‡é€»è¾‘èƒ½å¤Ÿä¼˜é›…åœ°å®ç°ç¥ç»ã€ç¬¦å·å’Œç»Ÿè®¡äººå·¥æ™ºèƒ½çš„å…³é”®å½¢å¼ï¼Œå¦‚å˜æ¢å™¨ã€å½¢å¼æ¨ç†ã€æ ¸æœºå™¨å’Œå›¾æ¨¡å‹ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œå¼ é‡é€»è¾‘ä¸ºåœ¨åµŒå…¥ç©ºé—´ä¸­è¿›è¡Œå¯é æ¨ç†å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ï¼Œç»“åˆäº†ç¥ç»ç½‘ç»œçš„å¯æ‰©å±•æ€§å’Œå­¦ä¹ èƒ½åŠ›ï¼Œä»¥åŠç¬¦å·æ¨ç†çš„å¯é æ€§å’Œé€æ˜æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.08666', 'title': 'dInfer: An Efficient Inference Framework for Diffusion Language Models', 'url': 'https://huggingface.co/papers/2510.08666', 'abstract': 'dInfer is an efficient and extensible framework for diffusion-based large language model inference, achieving significant speedups over existing systems without compromising output quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism. Even more and more open-sourced dLLM models emerge, yet their widespread adoption remains constrained by the lack of a standardized and efficient inference framework. We present dInfer, an efficient and extensible framework for dLLM inference. dInfer decomposes the inference pipeline into four modular components--model, diffusion iteration manager, decoding strategy, and KV-cache manager--and integrates novel algorithms for each component alongside system-level optimizations. Through this combination of algorithmic innovations and system enhancements, dInfer achieves substantial efficiency gains without compromising output quality on LLaDA-MoE. At batch size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800 tokens per second across six benchmarks on 8times H800 GPUs. Compared to prior systems, dInfer delivers a 10times speedup over Fast-dLLM while maintaining similar model performance. Even compared to the AR model (with a comparable number of activation parameters and performance) QWen2.5-3B, which is highly optimized with the latest vLLM inference engine, dInfer still delivers a 2-3times speedup. The implementation of dInfer is open-sourced at https://github.com/inclusionAI/dInfer.', 'score': 0, 'issue_id': 6434, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': '9378df98a01d3416', 'authors': ['Yuxin Ma', 'Lun Du', 'Lanning Wei', 'Kun Chen', 'Qian Xu', 'Kangyu Wang', 'Guofeng Feng', 'Guoshan Lu', 'Lin Liu', 'Xiaojing Qi', 'Xinyuan Zhang', 'Zhen Tao', 'Haibo Feng', 'Ziyun Jiang', 'Ying Xu', 'Zenan Huang', 'Yihong Zhuang', 'Haokai Xu', 'Jiaqi Hu', 'Zhenzhong Lan', 'Junbo Zhao', 'Jianguo Li', 'Da Zheng'], 'affiliations': ['Ant Group', 'Renmin University of China', 'Shanghai Jiao Tong University', 'University of Chinese Academy of Sciences', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.08666.jpg', 'data': {'categories': ['#open_source', '#inference', '#diffusion', '#architecture'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ LLM Ğ² 10 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ñ dInfer', 'desc': 'dInfer - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaDA-MoE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ 1100 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ½Ğ° HumanEval Ğ¿Ñ€Ğ¸ batch size 1, Ñ‡Ñ‚Ğ¾ Ğ² 10 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Fast-dLLM. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ QWen2.5-3B, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ vLLM, dInfer Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 2-3 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'dInfer: Speeding Up Diffusion-Based Language Models Efficiently!', 'desc': 'dInfer is a new framework designed for efficiently running diffusion-based large language models (dLLMs). It improves the speed of inference significantly while ensuring that the quality of the generated text remains high. The framework is modular, consisting of components like model management and decoding strategies, which allows for easy updates and enhancements. With dInfer, users can achieve over 1,100 tokens per second, making it much faster than previous systems, while still delivering comparable performance to autoregressive models.'}, 'zh': {'title': 'dInferï¼šé«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹æ¨ç†æ¡†æ¶', 'desc': 'dInferæ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”å¯æ‰©å±•çš„æ‰©æ•£åŸºç¡€å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸é™ä½è¾“å‡ºè´¨é‡çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜é€Ÿåº¦ã€‚è¯¥æ¡†æ¶å°†æ¨ç†æµç¨‹åˆ†è§£ä¸ºå››ä¸ªæ¨¡å—ç»„ä»¶ï¼šæ¨¡å‹ã€æ‰©æ•£è¿­ä»£ç®¡ç†å™¨ã€è§£ç ç­–ç•¥å’ŒKVç¼“å­˜ç®¡ç†å™¨ï¼Œå¹¶ä¸ºæ¯ä¸ªç»„ä»¶é›†æˆäº†æ–°ç®—æ³•å’Œç³»ç»Ÿçº§ä¼˜åŒ–ã€‚é€šè¿‡ç®—æ³•åˆ›æ–°å’Œç³»ç»Ÿå¢å¼ºçš„ç»“åˆï¼ŒdInferåœ¨LLaDA-MoEä¸Šå®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œèƒ½å¤Ÿåœ¨HumanEvalä¸Šè¾¾åˆ°æ¯ç§’è¶…è¿‡1100ä¸ªæ ‡è®°çš„é€Ÿåº¦ã€‚ä¸ä¹‹å‰çš„ç³»ç»Ÿç›¸æ¯”ï¼ŒdInferåœ¨ä¿æŒç›¸ä¼¼æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œæä¾›äº†10å€çš„é€Ÿåº¦æå‡ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (9)', '#agi (5)', '#alignment (6)', '#architecture (9)', '#audio (1)', '#benchmark (13)', '#cv (11)', '#data (5)', '#dataset (7)', '#diffusion (8)', '#ethics (1)', '#games (4)', '#graphs', '#hallucinations (4)', '#healthcare', '#inference (5)', '#interpretability (7)', '#leakage', '#long_context (3)', '#low_resource (2)', '#machine_translation (2)', '#math (4)', '#multilingual (2)', '#multimodal (15)', '#open_source (7)', '#optimization (23)', '#plp (1)', '#rag (3)', '#reasoning (20)', '#rl (9)', '#rlhf (3)', '#robotics (1)', '#science (2)', '#security (5)', '#small_models', '#story_generation (1)', '#survey (1)', '#synthetic (3)', '#training (27)', '#transfer_learning (4)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-10-15 20:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-15 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-15 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    