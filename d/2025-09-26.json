{
    "date": {
        "ru": "26 сентября",
        "en": "September 26",
        "zh": "9月26日"
    },
    "time_utc": "2025-09-26 02:18",
    "weekday": 4,
    "issue_id": 6098,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.21268",
            "title": "MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and\n  Open Resources",
            "url": "https://huggingface.co/papers/2509.21268",
            "abstract": "Variance-Aware Sampling and large-scale CoT data improve multimodal reasoning models by stabilizing RL fine-tuning and enhancing performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal reasoning models have achieved rapid progress, but their advancement is constrained by two major limitations: the absence of open, large-scale, high-quality long chain-of-thought (CoT) data, and the instability of reinforcement learning (RL) algorithms in post-training. Group Relative Policy Optimization (GRPO), the standard framework for RL fine-tuning, is prone to gradient vanishing when reward variance is low, which weakens optimization signals and impairs convergence. This work makes three contributions: (1) We propose Variance-Aware Sampling (VAS), a data selection strategy guided by Variance Promotion Score (VPS) that combines outcome variance and trajectory diversity to promote reward variance and stabilize policy optimization. (2) We release large-scale, carefully curated resources containing ~1.6M long CoT cold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty, and diversity, along with a fully reproducible end-to-end training codebase. (3) We open-source a family of multimodal reasoning models in multiple scales, establishing standardized baselines for the community. Experiments across mathematical reasoning benchmarks demonstrate the effectiveness of both the curated data and the proposed VAS. Comprehensive ablation studies and analyses provide further insight into the contributions of each component. In addition, we theoretically establish that reward variance lower-bounds the expected policy gradient magnitude, with VAS serving as a practical mechanism to realize this guarantee. Our code, data, and checkpoints are available at https://github.com/LengSicong/MMR1.",
            "score": 11,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 сентября",
                "en": "September 25",
                "zh": "9月25日"
            },
            "hash": "7153bc23f1974ebe",
            "authors": [
                "Sicong Leng",
                "Jing Wang",
                "Jiaxi Li",
                "Hao Zhang",
                "Zhiqiang Hu",
                "Boqiang Zhang",
                "Yuming Jiang",
                "Hang Zhang",
                "Xin Li",
                "Lidong Bing",
                "Deli Zhao",
                "Wei Lu",
                "Yu Rong",
                "Aixin Sun",
                "Shijian Lu"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Nanyang Technological University",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21268.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#architecture",
                    "#reasoning",
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#multimodal",
                    "#data",
                    "#open_source"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Стабилизация RL-обучения через управление дисперсией вознаграждений",
                    "desc": "Исследователи предложили метод Variance-Aware Sampling (VAS) для улучшения обучения больших мультимодальных моделей рассуждения с подкреплением. Основная проблема заключается в нестабильности алгоритмов RL из-за низкой дисперсии вознаграждений, что приводит к исчезновению градиентов. VAS использует показатель Variance Promotion Score для отбора данных, которые увеличивают дисперсию вознаграждений и стабилизируют оптимизацию политики. Авторы также выпустили крупномасштабный датасет с 1.6M примерами длинных цепочек рассуждений и семейство открытых мультимодальных моделей."
                },
                "en": {
                    "title": "Boosting Multimodal Reasoning with Variance-Aware Sampling and Quality Data",
                    "desc": "This paper addresses the challenges faced by large multimodal reasoning models, particularly the lack of high-quality long chain-of-thought (CoT) data and the instability of reinforcement learning (RL) during fine-tuning. It introduces Variance-Aware Sampling (VAS), a method that enhances reward variance and stabilizes policy optimization by selecting data based on outcome variance and trajectory diversity. The authors also provide a substantial dataset of approximately 1.6 million CoT examples and 15,000 RL question-answer pairs, ensuring diversity and quality for training. Additionally, they release a set of multimodal reasoning models and establish standardized benchmarks for future research in the field."
                },
                "zh": {
                    "title": "方差感知采样提升多模态推理模型性能",
                    "desc": "本文提出了一种新的数据选择策略，称为方差感知采样（VAS），旨在提高多模态推理模型的性能。通过结合结果方差和轨迹多样性，VAS可以促进奖励方差，从而稳定强化学习（RL）优化过程。我们还发布了大规模的高质量长链思维（CoT）数据集，包含约160万条冷启动数据和约15000个RL问答对，以支持模型训练。实验结果表明，VAS和新数据集显著提升了模型在数学推理基准上的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.20427",
            "title": "Seedream 4.0: Toward Next-generation Multimodal Image Generation",
            "url": "https://huggingface.co/papers/2509.20427",
            "abstract": "Seedream 4.0 is a high-performance multimodal image generation system that integrates text-to-image synthesis, image editing, and multi-image composition using a diffusion transformer and VAE, achieving state-of-the-art results with efficient training and inference.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. Seedream 4.0 is now accessible on https://www.volcengine.com/experience/ark?launch=seedream.",
            "score": 6,
            "issue_id": 6098,
            "pub_date": "2025-09-24",
            "pub_date_card": {
                "ru": "24 сентября",
                "en": "September 24",
                "zh": "9月24日"
            },
            "hash": "fb2f872386c520ce",
            "pdf_title_img": "assets/pdf/title_img/2509.20427.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#games",
                    "#multimodal",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Универсальная система для генерации и редактирования изображений нового поколения",
                    "desc": "Seedream 4.0 — это высокопроизводительная мультимодальная система генерации изображений, которая объединяет синтез изображений по тексту, редактирование изображений и композицию из нескольких изображений в единой архитектуре. Система использует эффективный диффузионный трансформер с мощным VAE, что позволяет значительно сократить количество токенов изображения и обеспечить быструю генерацию изображений высокого разрешения до 4K. Модель предобучена на миллиардах пар текст-изображение и проходит мультимодальное дообучение с использованием тщательно настроенной VLM модели. Для ускорения инференса применяются техники adversarial distillation, distribution matching, квантизация и speculative decoding, что позволяет генерировать изображение 2K за 1.8 секунды."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Seedream 4.0",
                    "desc": "Seedream 4.0 is a cutting-edge multimodal image generation system that combines text-to-image synthesis, image editing, and multi-image composition into one efficient framework. It utilizes a diffusion transformer and a variational autoencoder (VAE) to significantly reduce image token counts, enabling faster training and high-resolution image generation. The model is pretrained on a vast dataset of text-image pairs, ensuring strong generalization across various scenarios. With advanced techniques for inference acceleration, Seedream 4.0 achieves state-of-the-art performance in both T2I tasks and complex image editing, making it a powerful tool for creative and professional applications."
                },
                "zh": {
                    "title": "Seedream 4.0：多模态图像生成的新纪元",
                    "desc": "Seedream 4.0 是一个高性能的多模态图像生成系统，结合了文本到图像合成、图像编辑和多图像组合。它采用了高效的扩散变换器和变分自编码器（VAE），在训练和推理过程中表现出色。该系统经过数十亿对文本-图像对的预训练，确保了强大的泛化能力和稳定性。Seedream 4.0 不仅能快速生成高分辨率图像，还在复杂任务中展现出卓越的多模态能力，推动了生成式人工智能的边界。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21318",
            "title": "SD3.5-Flash: Distribution-Guided Distillation of Generative Flows",
            "url": "https://huggingface.co/papers/2509.21318",
            "abstract": "SD3.5-Flash is an efficient few-step distillation framework that enhances image generation on consumer devices using rectified flow models with innovations like timestep sharing and split-timestep fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SD3.5-Flash, an efficient few-step distillation framework that brings high-quality image generation to accessible consumer devices. Our approach distills computationally prohibitive rectified flow models through a reformulated distribution matching objective tailored specifically for few-step generation. We introduce two key innovations: \"timestep sharing\" to reduce gradient noise and \"split-timestep fine-tuning\" to improve prompt alignment. Combined with comprehensive pipeline optimizations like text encoder restructuring and specialized quantization, our system enables both rapid generation and memory-efficient deployment across different hardware configurations. This democratizes access across the full spectrum of devices, from mobile phones to desktop computers. Through extensive evaluation including large-scale user studies, we demonstrate that SD3.5-Flash consistently outperforms existing few-step methods, making advanced generative AI truly accessible for practical deployment.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 сентября",
                "en": "September 25",
                "zh": "9月25日"
            },
            "hash": "37d7c4e28964afd7",
            "authors": [
                "Hmrishav Bandyopadhyay",
                "Rahim Entezari",
                "Jim Scott",
                "Reshinth Adithyan",
                "Yi-Zhe Song",
                "Varun Jampani"
            ],
            "affiliations": [
                "Stability AI",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21318.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#data",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Быстрая генерация изображений для всех устройств",
                    "desc": "SD3.5-Flash представляет эффективный фреймворк дистилляции для генерации изображений за несколько шагов на обычных потребительских устройствах. Исследователи разработали специальную технику дистилляции rectified flow моделей с инновационными методами как timestep sharing и split-timestep fine-tuning. Система включает комплексные оптимизации пайплайна, включая реструктуризацию text encoder и специализированную квантизацию для эффективного развертывания. Результаты показывают превосходство над существующими методами генерации за малое количество шагов, делая продвинутые генеративные AI доступными для практического применения."
                },
                "en": {
                    "title": "Democratizing Image Generation with SD3.5-Flash",
                    "desc": "SD3.5-Flash is a new framework designed to improve image generation on everyday devices by using a few-step distillation method. It focuses on simplifying complex rectified flow models to make them more efficient for consumer hardware. The framework introduces innovative techniques like timestep sharing to minimize noise during training and split-timestep fine-tuning to enhance the alignment with user prompts. Overall, SD3.5-Flash allows for faster and more memory-efficient image generation, making advanced AI technology available to a wider range of devices."
                },
                "zh": {
                    "title": "让先进生成AI触手可及",
                    "desc": "SD3.5-Flash是一种高效的少步蒸馏框架，旨在提升消费者设备上的图像生成能力。该方法通过重新制定的分布匹配目标，蒸馏计算上昂贵的修正流模型，专门针对少步生成进行优化。我们引入了两个关键创新：时间步共享以减少梯度噪声，以及分步时间微调以改善提示对齐。通过全面的管道优化，我们的系统实现了快速生成和内存高效的部署，使得从手机到桌面电脑的各种设备都能轻松访问先进的生成AI。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21317",
            "title": "Interactive Recommendation Agent with Active User Commands",
            "url": "https://huggingface.co/papers/2509.21317",
            "abstract": "IRF, a new recommendation system using natural language commands, improves user satisfaction and business outcomes through a dual-agent architecture and simulation-augmented knowledge distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional recommender systems rely on passive feedback mechanisms that limit users to simple choices such as like and dislike. However, these coarse-grained signals fail to capture users' nuanced behavior motivations and intentions. In turn, current systems cannot also distinguish which specific item attributes drive user satisfaction or dissatisfaction, resulting in inaccurate preference modeling. These fundamental limitations create a persistent gap between user intentions and system interpretations, ultimately undermining user satisfaction and harming system effectiveness.   To address these limitations, we introduce the Interactive Recommendation Feed (IRF), a pioneering paradigm that enables natural language commands within mainstream recommendation feeds. Unlike traditional systems that confine users to passive implicit behavioral influence, IRF empowers active explicit control over recommendation policies through real-time linguistic commands. To support this paradigm, we develop RecBot, a dual-agent architecture where a Parser Agent transforms linguistic expressions into structured preferences and a Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly policy adjustment. To enable practical deployment, we employ simulation-augmented knowledge distillation to achieve efficient performance while maintaining strong reasoning capabilities. Through extensive offline and long-term online experiments, RecBot shows significant improvements in both user satisfaction and business outcomes.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 сентября",
                "en": "September 25",
                "zh": "9月25日"
            },
            "hash": "df6bcc9456addce3",
            "authors": [
                "Jiakai Tang",
                "Yujie Luo",
                "Xunke Xi",
                "Fei Sun",
                "Xueyang Feng",
                "Sunhao Dai",
                "Chao Yi",
                "Dian Chen",
                "Zhujin Gao",
                "Yang Li",
                "Xu Chen",
                "Wen Chen",
                "Jian Wu",
                "Yuning Jiang",
                "Bo Zheng"
            ],
            "affiliations": [
                "Alibaba Group, Beijing, China",
                "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China",
                "University of Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21317.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#reasoning",
                    "#optimization",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Управляй рекомендациями голосом - говори системе, что хочешь увидеть",
                    "desc": "Исследователи представили Interactive Recommendation Feed (IRF) - новую парадигму рекомендательных систем, которая позволяет пользователям управлять рекомендациями через команды на естественном языке. Система RecBot включает два агента: Parser Agent преобразует текстовые команды в структурированные предпочтения, а Planner Agent динамически адаптирует политику рекомендаций. Для эффективного развертывания используется knowledge distillation с дополнением симуляциями. Эксперименты показали значительное улучшение удовлетворенности пользователей и бизнес-метрик по сравнению с традиционными системами пассивного feedback."
                },
                "en": {
                    "title": "Empowering Users with Natural Language in Recommendations",
                    "desc": "The paper presents the Interactive Recommendation Feed (IRF), a novel recommendation system that utilizes natural language commands to enhance user engagement and satisfaction. Unlike traditional systems that rely on passive feedback, IRF allows users to actively express their preferences through real-time linguistic inputs. This is achieved using a dual-agent architecture, where a Parser Agent interprets user commands and a Planner Agent adjusts recommendation policies dynamically. The system employs simulation-augmented knowledge distillation to optimize performance while preserving robust reasoning capabilities, leading to improved user satisfaction and better business outcomes."
                },
                "zh": {
                    "title": "自然语言驱动的智能推荐系统",
                    "desc": "IRF是一种新型推荐系统，允许用户通过自然语言命令进行互动，从而提高用户满意度和商业成果。与传统推荐系统依赖被动反馈不同，IRF通过实时语言命令赋予用户主动控制推荐策略的能力。该系统采用双代理架构，解析代理将语言表达转化为结构化偏好，规划代理则动态调整推荐策略。通过模拟增强知识蒸馏，IRF在保持强大推理能力的同时，实现了高效的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21245",
            "title": "Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D\n  Assets",
            "url": "https://huggingface.co/papers/2509.21245",
            "abstract": "Hunyuan3D-Omni is a unified 3D asset generation framework that accepts multiple conditioning signals, improving controllability and robustness in production workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D-native generative models have accelerated asset creation for games, film, and design. However, most methods still rely primarily on image or text conditioning and lack fine-grained, cross-modal controls, which limits controllability and practical adoption. To address this gap, we present Hunyuan3D-Omni, a unified framework for fine-grained, controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images, Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose priors as conditioning signals, enabling precise control over geometry, topology, and pose. Instead of separate heads for each modality, our model unifies all signals in a single cross-modal architecture. We train with a progressive, difficulty-aware sampling strategy that selects one control modality per example and biases sampling toward harder signals (e.g., skeletal pose) while downweighting easier ones (e.g., point clouds), encouraging robust multi-modal fusion and graceful handling of missing inputs. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 сентября",
                "en": "September 25",
                "zh": "9月25日"
            },
            "hash": "8c4661dd2c3016bb",
            "pdf_title_img": "assets/pdf/title_img/2509.21245.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#training",
                    "#architecture",
                    "#games",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Многомодальный контроль 3D-генерации для игровой индустрии",
                    "desc": "Исследователи представили Hunyuan3D-Omni — единую систему для генерации 3D-объектов с множественным контролем. Модель принимает не только изображения и текст, но также облака точек, вокселы, ограничивающие рамки и скелетные позы для точного управления геометрией и позой. Система использует прогрессивную стратегию обучения с учетом сложности модальностей, что улучшает робастность при отсутствии некоторых входных данных. Эксперименты показали повышение точности генерации и практичности для производственных процессов в играх и дизайне."
                },
                "en": {
                    "title": "Unified Control for 3D Asset Generation",
                    "desc": "Hunyuan3D-Omni is a comprehensive framework designed for generating 3D assets with enhanced control and reliability. It allows the use of various conditioning signals, such as point clouds and skeletal poses, in addition to traditional images, which improves the precision of the generated models. The framework employs a unified cross-modal architecture, eliminating the need for separate processing heads for each type of input. By using a progressive sampling strategy that prioritizes more complex controls, it ensures better integration of different modalities and improves the overall robustness of the asset generation process."
                },
                "zh": {
                    "title": "统一多模态的3D资产生成框架",
                    "desc": "Hunyuan3D-Omni是一个统一的3D资产生成框架，能够接受多种条件信号，从而提高生产工作流程中的可控性和鲁棒性。该框架不仅支持图像，还可以处理点云、体素、边界框和骨骼姿态先验等多种输入信号，实现对几何形状、拓扑结构和姿态的精确控制。与传统方法不同，Hunyuan3D-Omni将所有信号统一在一个跨模态架构中，避免了为每种模态单独设计模型的复杂性。通过逐步的、关注难度的采样策略，我们的模型能够有效融合多模态信息，并在处理缺失输入时表现出良好的鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.20868",
            "title": "StyleBench: Evaluating thinking styles in Large Language Models",
            "url": "https://huggingface.co/papers/2509.20868",
            "abstract": "StyleBench evaluates various reasoning styles across tasks and models, revealing that strategy efficacy depends on model scale and task type.  \t\t\t\t\tAI-generated summary \t\t\t\t The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 сентября",
                "en": "September 25",
                "zh": "9月25日"
            },
            "hash": "fa050993ad9cfcf9",
            "authors": [
                "Junyu Guo",
                "Shangding Gu",
                "Ming Jin",
                "Costas Spanos",
                "Javad Lavaei"
            ],
            "affiliations": [
                "University of California, Berkeley",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.20868.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Размер модели решает, какой стиль рассуждений работает лучше",
                    "desc": "Исследование представляет StyleBench - бенчмарк для оценки различных стилей рассуждений в больших языковых моделях. Авторы протестировали пять методов промптинга (Chain of Thought, Tree of Thought и другие) на 15 моделях от 270M до 120B параметров. Результаты показали, что эффективность стратегий зависит от размера модели и типа задачи: поисковые методы лучше работают на открытых задачах с крупными моделями, а лаконичные стили более эффективны на четко определенных задачах. Маленькие модели часто не следуют инструкциям и склонны к угадыванию, тогда как устойчивость рассуждений появляется с увеличением масштаба модели."
                },
                "en": {
                    "title": "Unlocking Reasoning Styles for Optimal Model Performance",
                    "desc": "StyleBench is a new benchmark designed to evaluate different reasoning styles used in prompts for Large Language Models (LLMs). It examines how the effectiveness of these reasoning strategies varies depending on the model size and the type of task being performed. The study analyzes five reasoning styles across various tasks using 15 different models, revealing that no single style works best for all scenarios. The results indicate that larger models perform better with complex reasoning styles, while simpler styles are more efficient for straightforward tasks."
                },
                "zh": {
                    "title": "推理风格与模型规模的最佳选择",
                    "desc": "StyleBench 是一个全面的基准测试，用于系统评估不同任务和模型中的推理风格。我们研究了五种代表性的推理风格，包括思维链（CoT）、思维树（ToT）、思维算法（AoT）、思维草图（SoT）和草稿链（CoD），并在五个推理任务上进行了评估。研究发现，没有一种推理风格在所有情况下都是最佳的，其有效性高度依赖于模型规模和任务类型。我们的分析表明，基于搜索的方法在开放性问题中表现优异，但需要大规模模型，而简洁的风格在定义明确的任务中则能显著提高效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.20712",
            "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy\n  Optimization in Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.20712",
            "abstract": "A novel reinforcement learning algorithm, CE-GPPO, reintroduces gradients from clipped tokens to improve the exploration-exploitation balance in training large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose Controlling Entropy via Gradient-Preserving Policy Optimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 сентября",
                "en": "September 25",
                "zh": "9月25日"
            },
            "hash": "ad1a5016131ff587",
            "authors": [
                "Zhenpeng Su",
                "Leiyu Pan",
                "Minxuan Lv",
                "Yuntao Li",
                "Wenping Hu",
                "Fuzheng Zhang",
                "Kun Gai",
                "Guorui Zhou"
            ],
            "affiliations": [
                "Independent",
                "Klear Team, Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.20712.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Сохраняем градиенты для лучшего баланса в обучении LLM",
                    "desc": "В работе предлагается новый алгоритм обучения с подкреплением CE-GPPO для оптимизации больших языковых моделей. Основная проблема существующих методов типа PPO заключается в том, что они отбрасывают ценные градиенты от токенов с низкой вероятностью из-за механизма клиппинга. CE-GPPO решает эту проблему, повторно вводя градиенты от обрезанных токенов контролируемым образом, что улучшает баланс между исследованием и эксплуатацией. Эксперименты на задачах математических рассуждений показывают превосходство метода над существующими подходами."
                },
                "en": {
                    "title": "Enhancing Exploration-Exploitation Balance in Language Models with CE-GPPO",
                    "desc": "The paper presents a new reinforcement learning algorithm called CE-GPPO, which enhances the training of large language models by reintroducing gradients from clipped tokens. This approach addresses the challenge of managing policy entropy, which is crucial for balancing exploration and exploitation during training. By carefully controlling the gradients from low-probability tokens, CE-GPPO improves the stability of entropy dynamics, which is often overlooked in existing methods. The authors provide both theoretical insights and empirical results demonstrating that CE-GPPO outperforms traditional algorithms like PPO on various reasoning tasks."
                },
                "zh": {
                    "title": "重新引入梯度，优化探索与利用的平衡",
                    "desc": "CE-GPPO是一种新颖的强化学习算法，旨在改善大语言模型的探索与利用平衡。该算法通过重新引入被剪切的标记的梯度，解决了现有方法在训练过程中丢失有价值的梯度信号的问题。研究表明，这些被剪切的标记在调节策略熵的演变中起着重要作用。CE-GPPO在多个数学推理基准测试中表现优异，超越了强基线模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.20414",
            "title": "SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and\n  Self-Reflective Agent",
            "url": "https://huggingface.co/papers/2509.20414",
            "abstract": "SceneWeaver, a reflective agentic framework, uses a language model-based planner to iteratively refine 3D scene synthesis, achieving high physical, visual, and semantic quality across diverse instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Indoor scene synthesis has become increasingly important with the rise of Embodied AI, which requires 3D environments that are not only visually realistic but also physically plausible and functionally diverse. While recent approaches have advanced visual fidelity, they often remain constrained to fixed scene categories, lack sufficient object-level detail and physical consistency, and struggle to align with complex user instructions. In this work, we present SceneWeaver, a reflective agentic framework that unifies diverse scene synthesis paradigms through tool-based iterative refinement. At its core, SceneWeaver employs a language model-based planner to select from a suite of extensible scene generation tools, ranging from data-driven generative models to visual- and LLM-based methods, guided by self-evaluation of physical plausibility, visual realism, and semantic alignment with user input. This closed-loop reason-act-reflect design enables the agent to identify semantic inconsistencies, invoke targeted tools, and update the environment over successive iterations. Extensive experiments on both common and open-vocabulary room types demonstrate that SceneWeaver not only outperforms prior methods on physical, visual, and semantic metrics, but also generalizes effectively to complex scenes with diverse instructions, marking a step toward general-purpose 3D environment generation. Project website: https://scene-weaver.github.io/.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-24",
            "pub_date_card": {
                "ru": "24 сентября",
                "en": "September 24",
                "zh": "9月24日"
            },
            "hash": "4b1e989136f96d93",
            "authors": [
                "Yandan Yang",
                "Baoxiong Jia",
                "Shujie Zhang",
                "Siyuan Huang"
            ],
            "affiliations": [
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.20414.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#reasoning",
                    "#games",
                    "#alignment",
                    "#agents"
                ],
                "emoji": "🏠",
                "ru": {
                    "title": "Умный архитектор: AI-агент создает реалистичные 3D интерьеры через саморефлексию",
                    "desc": "SceneWeaver - это агентная система для синтеза 3D сцен, которая использует языковую модель-планировщик для итеративного улучшения качества генерируемых пространств. Система объединяет различные инструменты генерации сцен через механизм рефлексии, позволяющий агенту самостоятельно оценивать физическую правдоподобность, визуальную реалистичность и семантическое соответствие инструкциям. В отличие от предыдущих подходов, ограниченных фиксированными категориями сцен, SceneWeaver способна работать с разнообразными и сложными пользовательскими запросами. Эксперименты показывают превосходство системы по всем ключевым метрикам качества при генерации как стандартных, так и нестандартных типов помещений."
                },
                "en": {
                    "title": "SceneWeaver: Crafting Realistic 3D Environments with Iterative Refinement",
                    "desc": "SceneWeaver is a framework designed for creating 3D scenes that are not only visually appealing but also physically realistic and semantically accurate. It utilizes a language model-based planner to iteratively refine the scene generation process, allowing for adjustments based on user instructions. By integrating various scene generation tools and employing a closed-loop reasoning approach, SceneWeaver can identify and correct inconsistencies in the generated scenes. This innovative method significantly improves the quality of 3D environments, making it suitable for a wide range of applications in Embodied AI."
                },
                "zh": {
                    "title": "SceneWeaver：智能3D场景合成的新突破",
                    "desc": "SceneWeaver是一个反思性代理框架，利用基于语言模型的规划器来迭代优化3D场景合成。它能够在多样化的指令下，实现高水平的物理、视觉和语义质量。该框架通过工具驱动的迭代精炼，统一了不同的场景合成范式。实验表明，SceneWeaver在物理、视觉和语义指标上超越了以往的方法，并能有效地适应复杂场景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14662",
            "title": "Understanding the Thinking Process of Reasoning Models: A Perspective\n  from Schoenfeld's Episode Theory",
            "url": "https://huggingface.co/papers/2509.14662",
            "abstract": "A novel framework using Schoenfeld's Episode Theory is introduced to analyze the reasoning patterns of Large Reasoning Models in solving math problems, providing a benchmark for machine reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Reasoning Models (LRMs) generate extensive chain-of-thought reasoning, we lack a principled framework for understanding how these thoughts are structured. In this paper, we introduce a novel approach by applying Schoenfeld's Episode Theory, a classic cognitive framework for human mathematical problem-solving, to analyze the reasoning traces of LRMs. We annotated thousands of sentences and paragraphs from model-generated solutions to math problems using seven cognitive labels (e.g., Plan, Implement, Verify). The result is the first publicly available benchmark for the fine-grained analysis of machine reasoning, including a large annotated corpus and detailed annotation guidebooks. Our preliminary analysis reveals distinct patterns in LRM reasoning, such as the transition dynamics between cognitive states. This framework provides a theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "1ed939a345480c28",
            "authors": [
                "Ming Li",
                "Nan Zhang",
                "Chenrui Fan",
                "Hong Jiao",
                "Yanbin Fu",
                "Sydney Peters",
                "Qingshu Xu",
                "Robert Lissitz",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14662.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#interpretability",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Картография мышления AI через призму человеческого познания",
                    "desc": "Исследователи применили теорию эпизодов Шёнфельда, классическую когнитивную модель решения математических задач человеком, для анализа рассуждений больших языковых моделей. Они создали первый публично доступный бенчмарк для детального анализа машинного мышления, разметив тысячи предложений из решений моделей семью когнитивными метками. Анализ выявил характерные паттерны в рассуждениях LLM, включая динамику переходов между когнитивными состояниями. Эта работа предоставляет теоретически обоснованную методологию для понимания познавательных процессов AI и открывает путь к созданию более контролируемых и прозрачных систем рассуждений."
                },
                "en": {
                    "title": "Understanding Machine Reasoning with Cognitive Frameworks",
                    "desc": "This paper presents a new framework that uses Schoenfeld's Episode Theory to analyze how Large Reasoning Models (LRMs) approach math problems. By applying cognitive labels to thousands of sentences from model-generated solutions, the authors create a detailed benchmark for understanding machine reasoning. The study reveals specific patterns in the reasoning processes of LRMs, highlighting how they transition between different cognitive states. This framework not only aids in interpreting LRM cognition but also sets the stage for developing more transparent and controllable reasoning systems in the future."
                },
                "zh": {
                    "title": "揭示大型推理模型的推理模式",
                    "desc": "本文提出了一种新颖的框架，利用Schoenfeld的情节理论分析大型推理模型在解决数学问题时的推理模式。这种方法通过对模型生成的解决方案进行标注，使用七种认知标签（如计划、实施、验证）来分析推理轨迹。研究结果提供了第一个公开可用的机器推理细粒度分析基准，包括一个大型标注语料库和详细的标注指南。初步分析显示了大型推理模型推理中的独特模式，如认知状态之间的转变动态。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21320",
            "title": "SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines",
            "url": "https://huggingface.co/papers/2509.21320",
            "abstract": "A scientific reasoning foundation model pre-trained on diverse scientific data supports multiple tasks and enhances cross-domain generalization and fidelity through specialized training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at https://huggingface.co/SciReason and https://github.com/open-sciencelab/SciReason.",
            "score": 0,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 сентября",
                "en": "September 25",
                "zh": "9月25日"
            },
            "hash": "e33c7b540d84ad9a",
            "authors": [
                "Yizhou Wang",
                "Chen Tang",
                "Han Deng",
                "Jiabei Xiao",
                "Jiaqi Liu",
                "Jianyu Wu",
                "Jun Yao",
                "Pengze Li",
                "Encheng Su",
                "Lintao Wang",
                "Guohang Zhuang",
                "Yuchen Ren",
                "Ben Fei",
                "Ming Hu",
                "Xin Chen",
                "Dongzhan Zhou",
                "Junjun He",
                "Xiangyu Yue",
                "Zhenfei Yin",
                "Jiamin Wu",
                "Qihao Zheng",
                "Yuhao Zhou",
                "Huihui Xu",
                "Chenglong Ma",
                "Yan Lu",
                "Wenlong Zhang",
                "Chunfeng Song",
                "Philip Torr",
                "Shixiang Tang",
                "Xinzhu Ma",
                "Wanli Ouyang",
                "Lei Bai"
            ],
            "affiliations": [
                "Beihang University",
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Sydney",
                "University of Oxford",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21320.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#transfer_learning",
                    "#multimodal",
                    "#data",
                    "#science",
                    "#open_source"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Универсальный AI для научных рассуждений во всех дисциплинах",
                    "desc": "Исследователи создали foundation модель для научных рассуждений, которая может работать с различными типами научных данных - текстом, последовательностями и их парами. Модель была обучена на корпусе из 206 миллиардов токенов, а затем дообучена с помощью supervised fine-tuning на 40 миллионах инструкций и reinforcement learning с task-специфичным reward shaping. Она поддерживает пять семейств задач, включающих перевод между текстом и научными форматами, извлечение знаний, предсказание свойств, классификацию и генерацию последовательностей. Модель показывает лучшую кросс-доменную генерализацию и точность по сравнению со специализированными системами благодаря междисциплинарному обучению."
                },
                "en": {
                    "title": "Empowering Scientific Reasoning with a Versatile Foundation Model",
                    "desc": "This paper introduces a scientific reasoning foundation model that is trained on a vast dataset of scientific texts and sequences. It employs advanced techniques like supervised fine-tuning and reinforcement learning to improve its ability to perform various scientific tasks. The model can translate between different scientific formats, extract knowledge, predict properties, and generate sequences, making it versatile across multiple domains. By enhancing cross-domain generalization and fidelity, this model outperforms specialized systems in handling a wide range of scientific inquiries."
                },
                "zh": {
                    "title": "科学推理模型：跨领域的智能助手",
                    "desc": "这篇论文介绍了一种科学推理基础模型，该模型在多样的科学数据上进行预训练，支持多种任务并增强跨领域的泛化能力。模型使用了2060亿个标记的语料库，涵盖科学文本、纯序列和序列-文本对，并通过特定的训练技术进行对齐。它能够执行多达103个任务，包括文本与科学格式之间的翻译、知识提取、属性预测和分类等。与专业系统相比，该模型在指令覆盖范围、跨领域泛化和准确性方面都有显著提升。"
                }
            }
        }
    ],
    "link_prev": "2025-09-25.html",
    "link_next": "2025-09-29.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "25.09",
        "en": "09/25",
        "zh": "9月25日"
    },
    "short_date_next": {
        "ru": "29.09",
        "en": "09/29",
        "zh": "9月29日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}