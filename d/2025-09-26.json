{
    "date": {
        "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 26",
        "zh": "9æœˆ26æ—¥"
    },
    "time_utc": "2025-09-26 02:18",
    "weekday": 4,
    "issue_id": 6098,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.21268",
            "title": "MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and\n  Open Resources",
            "url": "https://huggingface.co/papers/2509.21268",
            "abstract": "Variance-Aware Sampling and large-scale CoT data improve multimodal reasoning models by stabilizing RL fine-tuning and enhancing performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal reasoning models have achieved rapid progress, but their advancement is constrained by two major limitations: the absence of open, large-scale, high-quality long chain-of-thought (CoT) data, and the instability of reinforcement learning (RL) algorithms in post-training. Group Relative Policy Optimization (GRPO), the standard framework for RL fine-tuning, is prone to gradient vanishing when reward variance is low, which weakens optimization signals and impairs convergence. This work makes three contributions: (1) We propose Variance-Aware Sampling (VAS), a data selection strategy guided by Variance Promotion Score (VPS) that combines outcome variance and trajectory diversity to promote reward variance and stabilize policy optimization. (2) We release large-scale, carefully curated resources containing ~1.6M long CoT cold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty, and diversity, along with a fully reproducible end-to-end training codebase. (3) We open-source a family of multimodal reasoning models in multiple scales, establishing standardized baselines for the community. Experiments across mathematical reasoning benchmarks demonstrate the effectiveness of both the curated data and the proposed VAS. Comprehensive ablation studies and analyses provide further insight into the contributions of each component. In addition, we theoretically establish that reward variance lower-bounds the expected policy gradient magnitude, with VAS serving as a practical mechanism to realize this guarantee. Our code, data, and checkpoints are available at https://github.com/LengSicong/MMR1.",
            "score": 11,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "7153bc23f1974ebe",
            "authors": [
                "Sicong Leng",
                "Jing Wang",
                "Jiaxi Li",
                "Hao Zhang",
                "Zhiqiang Hu",
                "Boqiang Zhang",
                "Yuming Jiang",
                "Hang Zhang",
                "Xin Li",
                "Lidong Bing",
                "Deli Zhao",
                "Wei Lu",
                "Yu Rong",
                "Aixin Sun",
                "Shijian Lu"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Nanyang Technological University",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21268.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#architecture",
                    "#reasoning",
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#multimodal",
                    "#data",
                    "#open_source"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Variance-Aware Sampling (VAS) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² RL Ğ¸Ğ·-Ğ·Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸ÑÑ‡ĞµĞ·Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². VAS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ Variance Promotion Score Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 1.6M Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Boosting Multimodal Reasoning with Variance-Aware Sampling and Quality Data",
                    "desc": "This paper addresses the challenges faced by large multimodal reasoning models, particularly the lack of high-quality long chain-of-thought (CoT) data and the instability of reinforcement learning (RL) during fine-tuning. It introduces Variance-Aware Sampling (VAS), a method that enhances reward variance and stabilizes policy optimization by selecting data based on outcome variance and trajectory diversity. The authors also provide a substantial dataset of approximately 1.6 million CoT examples and 15,000 RL question-answer pairs, ensuring diversity and quality for training. Additionally, they release a set of multimodal reasoning models and establish standardized benchmarks for future research in the field."
                },
                "zh": {
                    "title": "æ–¹å·®æ„ŸçŸ¥é‡‡æ ·æå‡å¤šæ¨¡æ€æ¨ç†æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œç§°ä¸ºæ–¹å·®æ„ŸçŸ¥é‡‡æ ·ï¼ˆVASï¼‰ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡ç»“åˆç»“æœæ–¹å·®å’Œè½¨è¿¹å¤šæ ·æ€§ï¼ŒVASå¯ä»¥ä¿ƒè¿›å¥–åŠ±æ–¹å·®ï¼Œä»è€Œç¨³å®šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†å¤§è§„æ¨¡çš„é«˜è´¨é‡é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ•°æ®é›†ï¼ŒåŒ…å«çº¦160ä¸‡æ¡å†·å¯åŠ¨æ•°æ®å’Œçº¦15000ä¸ªRLé—®ç­”å¯¹ï¼Œä»¥æ”¯æŒæ¨¡å‹è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVASå’Œæ–°æ•°æ®é›†æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.20427",
            "title": "Seedream 4.0: Toward Next-generation Multimodal Image Generation",
            "url": "https://huggingface.co/papers/2509.20427",
            "abstract": "Seedream 4.0 is a high-performance multimodal image generation system that integrates text-to-image synthesis, image editing, and multi-image composition using a diffusion transformer and VAE, achieving state-of-the-art results with efficient training and inference.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. Seedream 4.0 is now accessible on https://www.volcengine.com/experience/ark?launch=seedream.",
            "score": 6,
            "issue_id": 6098,
            "pub_date": "2025-09-24",
            "pub_date_card": {
                "ru": "24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 24",
                "zh": "9æœˆ24æ—¥"
            },
            "hash": "fb2f872386c520ce",
            "pdf_title_img": "assets/pdf/title_img/2509.20427.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#games",
                    "#multimodal",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Seedream 4.0 â€” ÑÑ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ VAE, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 4K. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ğ¿Ğ°Ñ€ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¹ VLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ”Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ adversarial distillation, distribution matching, ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ speculative decoding, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ 2K Ğ·Ğ° 1.8 ÑĞµĞºÑƒĞ½Ğ´Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Seedream 4.0",
                    "desc": "Seedream 4.0 is a cutting-edge multimodal image generation system that combines text-to-image synthesis, image editing, and multi-image composition into one efficient framework. It utilizes a diffusion transformer and a variational autoencoder (VAE) to significantly reduce image token counts, enabling faster training and high-resolution image generation. The model is pretrained on a vast dataset of text-image pairs, ensuring strong generalization across various scenarios. With advanced techniques for inference acceleration, Seedream 4.0 achieves state-of-the-art performance in both T2I tasks and complex image editing, making it a powerful tool for creative and professional applications."
                },
                "zh": {
                    "title": "Seedream 4.0ï¼šå¤šæ¨¡æ€å›¾åƒç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "Seedream 4.0 æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å¤šæ¨¡æ€å›¾åƒç”Ÿæˆç³»ç»Ÿï¼Œç»“åˆäº†æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€å›¾åƒç¼–è¾‘å’Œå¤šå›¾åƒç»„åˆã€‚å®ƒé‡‡ç”¨äº†é«˜æ•ˆçš„æ‰©æ•£å˜æ¢å™¨å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥ç³»ç»Ÿç»è¿‡æ•°åäº¿å¯¹æ–‡æœ¬-å›¾åƒå¯¹çš„é¢„è®­ç»ƒï¼Œç¡®ä¿äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚Seedream 4.0 ä¸ä»…èƒ½å¿«é€Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œè¿˜åœ¨å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„å¤šæ¨¡æ€èƒ½åŠ›ï¼Œæ¨åŠ¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¾¹ç•Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21318",
            "title": "SD3.5-Flash: Distribution-Guided Distillation of Generative Flows",
            "url": "https://huggingface.co/papers/2509.21318",
            "abstract": "SD3.5-Flash is an efficient few-step distillation framework that enhances image generation on consumer devices using rectified flow models with innovations like timestep sharing and split-timestep fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SD3.5-Flash, an efficient few-step distillation framework that brings high-quality image generation to accessible consumer devices. Our approach distills computationally prohibitive rectified flow models through a reformulated distribution matching objective tailored specifically for few-step generation. We introduce two key innovations: \"timestep sharing\" to reduce gradient noise and \"split-timestep fine-tuning\" to improve prompt alignment. Combined with comprehensive pipeline optimizations like text encoder restructuring and specialized quantization, our system enables both rapid generation and memory-efficient deployment across different hardware configurations. This democratizes access across the full spectrum of devices, from mobile phones to desktop computers. Through extensive evaluation including large-scale user studies, we demonstrate that SD3.5-Flash consistently outperforms existing few-step methods, making advanced generative AI truly accessible for practical deployment.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "37d7c4e28964afd7",
            "authors": [
                "Hmrishav Bandyopadhyay",
                "Rahim Entezari",
                "Jim Scott",
                "Reshinth Adithyan",
                "Yi-Zhe Song",
                "Varun Jampani"
            ],
            "affiliations": [
                "Stability AI",
                "University of Surrey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21318.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#data",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²",
                    "desc": "SD3.5-Flash Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ rectified flow Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº timestep sharing Ğ¸ split-timestep fine-tuning. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ text encoder Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ° Ğ¼Ğ°Ğ»Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ², Ğ´ĞµĞ»Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ AI Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Democratizing Image Generation with SD3.5-Flash",
                    "desc": "SD3.5-Flash is a new framework designed to improve image generation on everyday devices by using a few-step distillation method. It focuses on simplifying complex rectified flow models to make them more efficient for consumer hardware. The framework introduces innovative techniques like timestep sharing to minimize noise during training and split-timestep fine-tuning to enhance the alignment with user prompts. Overall, SD3.5-Flash allows for faster and more memory-efficient image generation, making advanced AI technology available to a wider range of devices."
                },
                "zh": {
                    "title": "è®©å…ˆè¿›ç”ŸæˆAIè§¦æ‰‹å¯åŠ",
                    "desc": "SD3.5-Flashæ˜¯ä¸€ç§é«˜æ•ˆçš„å°‘æ­¥è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ¶ˆè´¹è€…è®¾å¤‡ä¸Šçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡æ–°åˆ¶å®šçš„åˆ†å¸ƒåŒ¹é…ç›®æ ‡ï¼Œè’¸é¦è®¡ç®—ä¸Šæ˜‚è´µçš„ä¿®æ­£æµæ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹å°‘æ­¥ç”Ÿæˆè¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šæ—¶é—´æ­¥å…±äº«ä»¥å‡å°‘æ¢¯åº¦å™ªå£°ï¼Œä»¥åŠåˆ†æ­¥æ—¶é—´å¾®è°ƒä»¥æ”¹å–„æç¤ºå¯¹é½ã€‚é€šè¿‡å…¨é¢çš„ç®¡é“ä¼˜åŒ–ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå®ç°äº†å¿«é€Ÿç”Ÿæˆå’Œå†…å­˜é«˜æ•ˆçš„éƒ¨ç½²ï¼Œä½¿å¾—ä»æ‰‹æœºåˆ°æ¡Œé¢ç”µè„‘çš„å„ç§è®¾å¤‡éƒ½èƒ½è½»æ¾è®¿é—®å…ˆè¿›çš„ç”ŸæˆAIã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21317",
            "title": "Interactive Recommendation Agent with Active User Commands",
            "url": "https://huggingface.co/papers/2509.21317",
            "abstract": "IRF, a new recommendation system using natural language commands, improves user satisfaction and business outcomes through a dual-agent architecture and simulation-augmented knowledge distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional recommender systems rely on passive feedback mechanisms that limit users to simple choices such as like and dislike. However, these coarse-grained signals fail to capture users' nuanced behavior motivations and intentions. In turn, current systems cannot also distinguish which specific item attributes drive user satisfaction or dissatisfaction, resulting in inaccurate preference modeling. These fundamental limitations create a persistent gap between user intentions and system interpretations, ultimately undermining user satisfaction and harming system effectiveness.   To address these limitations, we introduce the Interactive Recommendation Feed (IRF), a pioneering paradigm that enables natural language commands within mainstream recommendation feeds. Unlike traditional systems that confine users to passive implicit behavioral influence, IRF empowers active explicit control over recommendation policies through real-time linguistic commands. To support this paradigm, we develop RecBot, a dual-agent architecture where a Parser Agent transforms linguistic expressions into structured preferences and a Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly policy adjustment. To enable practical deployment, we employ simulation-augmented knowledge distillation to achieve efficient performance while maintaining strong reasoning capabilities. Through extensive offline and long-term online experiments, RecBot shows significant improvements in both user satisfaction and business outcomes.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "df6bcc9456addce3",
            "authors": [
                "Jiakai Tang",
                "Yujie Luo",
                "Xunke Xi",
                "Fei Sun",
                "Xueyang Feng",
                "Sunhao Dai",
                "Chao Yi",
                "Dian Chen",
                "Zhujin Gao",
                "Yang Li",
                "Xu Chen",
                "Wen Chen",
                "Jian Wu",
                "Yuning Jiang",
                "Bo Zheng"
            ],
            "affiliations": [
                "Alibaba Group, Beijing, China",
                "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China",
                "University of Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21317.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#reasoning",
                    "#optimization",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞ¹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ¼ - Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‡ĞµÑˆÑŒ ÑƒĞ²Ğ¸Ğ´ĞµÑ‚ÑŒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Interactive Recommendation Feed (IRF) - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° RecBot Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: Parser Agent Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ, Ğ° Planner Agent Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ knowledge distillation Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ feedback."
                },
                "en": {
                    "title": "Empowering Users with Natural Language in Recommendations",
                    "desc": "The paper presents the Interactive Recommendation Feed (IRF), a novel recommendation system that utilizes natural language commands to enhance user engagement and satisfaction. Unlike traditional systems that rely on passive feedback, IRF allows users to actively express their preferences through real-time linguistic inputs. This is achieved using a dual-agent architecture, where a Parser Agent interprets user commands and a Planner Agent adjusts recommendation policies dynamically. The system employs simulation-augmented knowledge distillation to optimize performance while preserving robust reasoning capabilities, leading to improved user satisfaction and better business outcomes."
                },
                "zh": {
                    "title": "è‡ªç„¶è¯­è¨€é©±åŠ¨çš„æ™ºèƒ½æ¨èç³»ç»Ÿ",
                    "desc": "IRFæ˜¯ä¸€ç§æ–°å‹æ¨èç³»ç»Ÿï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€å‘½ä»¤è¿›è¡Œäº’åŠ¨ï¼Œä»è€Œæé«˜ç”¨æˆ·æ»¡æ„åº¦å’Œå•†ä¸šæˆæœã€‚ä¸ä¼ ç»Ÿæ¨èç³»ç»Ÿä¾èµ–è¢«åŠ¨åé¦ˆä¸åŒï¼ŒIRFé€šè¿‡å®æ—¶è¯­è¨€å‘½ä»¤èµ‹äºˆç”¨æˆ·ä¸»åŠ¨æ§åˆ¶æ¨èç­–ç•¥çš„èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åŒä»£ç†æ¶æ„ï¼Œè§£æä»£ç†å°†è¯­è¨€è¡¨è¾¾è½¬åŒ–ä¸ºç»“æ„åŒ–åå¥½ï¼Œè§„åˆ’ä»£ç†åˆ™åŠ¨æ€è°ƒæ•´æ¨èç­–ç•¥ã€‚é€šè¿‡æ¨¡æ‹Ÿå¢å¼ºçŸ¥è¯†è’¸é¦ï¼ŒIRFåœ¨ä¿æŒå¼ºå¤§æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ•ˆçš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21245",
            "title": "Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D\n  Assets",
            "url": "https://huggingface.co/papers/2509.21245",
            "abstract": "Hunyuan3D-Omni is a unified 3D asset generation framework that accepts multiple conditioning signals, improving controllability and robustness in production workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D-native generative models have accelerated asset creation for games, film, and design. However, most methods still rely primarily on image or text conditioning and lack fine-grained, cross-modal controls, which limits controllability and practical adoption. To address this gap, we present Hunyuan3D-Omni, a unified framework for fine-grained, controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images, Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose priors as conditioning signals, enabling precise control over geometry, topology, and pose. Instead of separate heads for each modality, our model unifies all signals in a single cross-modal architecture. We train with a progressive, difficulty-aware sampling strategy that selects one control modality per example and biases sampling toward harder signals (e.g., skeletal pose) while downweighting easier ones (e.g., point clouds), encouraging robust multi-modal fusion and graceful handling of missing inputs. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "8c4661dd2c3016bb",
            "pdf_title_img": "assets/pdf/title_img/2509.21245.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#training",
                    "#architecture",
                    "#games",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Hunyuan3D-Omni â€” ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚, Ğ½Ğ¾ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ²Ğ¾ĞºÑĞµĞ»Ñ‹, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¸ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ."
                },
                "en": {
                    "title": "Unified Control for 3D Asset Generation",
                    "desc": "Hunyuan3D-Omni is a comprehensive framework designed for generating 3D assets with enhanced control and reliability. It allows the use of various conditioning signals, such as point clouds and skeletal poses, in addition to traditional images, which improves the precision of the generated models. The framework employs a unified cross-modal architecture, eliminating the need for separate processing heads for each type of input. By using a progressive sampling strategy that prioritizes more complex controls, it ensures better integration of different modalities and improves the overall robustness of the asset generation process."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¤šæ¨¡æ€çš„3Dèµ„äº§ç”Ÿæˆæ¡†æ¶",
                    "desc": "Hunyuan3D-Omniæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„3Dèµ„äº§ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ¥å—å¤šç§æ¡ä»¶ä¿¡å·ï¼Œä»è€Œæé«˜ç”Ÿäº§å·¥ä½œæµç¨‹ä¸­çš„å¯æ§æ€§å’Œé²æ£’æ€§ã€‚è¯¥æ¡†æ¶ä¸ä»…æ”¯æŒå›¾åƒï¼Œè¿˜å¯ä»¥å¤„ç†ç‚¹äº‘ã€ä½“ç´ ã€è¾¹ç•Œæ¡†å’Œéª¨éª¼å§¿æ€å…ˆéªŒç­‰å¤šç§è¾“å…¥ä¿¡å·ï¼Œå®ç°å¯¹å‡ ä½•å½¢çŠ¶ã€æ‹“æ‰‘ç»“æ„å’Œå§¿æ€çš„ç²¾ç¡®æ§åˆ¶ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒHunyuan3D-Omniå°†æ‰€æœ‰ä¿¡å·ç»Ÿä¸€åœ¨ä¸€ä¸ªè·¨æ¨¡æ€æ¶æ„ä¸­ï¼Œé¿å…äº†ä¸ºæ¯ç§æ¨¡æ€å•ç‹¬è®¾è®¡æ¨¡å‹çš„å¤æ‚æ€§ã€‚é€šè¿‡é€æ­¥çš„ã€å…³æ³¨éš¾åº¦çš„é‡‡æ ·ç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¹¶åœ¨å¤„ç†ç¼ºå¤±è¾“å…¥æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.20868",
            "title": "StyleBench: Evaluating thinking styles in Large Language Models",
            "url": "https://huggingface.co/papers/2509.20868",
            "abstract": "StyleBench evaluates various reasoning styles across tasks and models, revealing that strategy efficacy depends on model scale and task type.  \t\t\t\t\tAI-generated summary \t\t\t\t The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "fa050993ad9cfcf9",
            "authors": [
                "Junyu Guo",
                "Shangding Gu",
                "Ming Jin",
                "Costas Spanos",
                "Javad Lavaei"
            ],
            "affiliations": [
                "University of California, Berkeley",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.20868.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚, ĞºĞ°ĞºĞ¾Ğ¹ ÑÑ‚Ğ¸Ğ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ StyleBench - Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° (Chain of Thought, Tree of Thought Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ) Ğ½Ğ° 15 Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ 270M Ğ´Ğ¾ 120B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ° Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚ĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unlocking Reasoning Styles for Optimal Model Performance",
                    "desc": "StyleBench is a new benchmark designed to evaluate different reasoning styles used in prompts for Large Language Models (LLMs). It examines how the effectiveness of these reasoning strategies varies depending on the model size and the type of task being performed. The study analyzes five reasoning styles across various tasks using 15 different models, revealing that no single style works best for all scenarios. The results indicate that larger models perform better with complex reasoning styles, while simpler styles are more efficient for straightforward tasks."
                },
                "zh": {
                    "title": "æ¨ç†é£æ ¼ä¸æ¨¡å‹è§„æ¨¡çš„æœ€ä½³é€‰æ‹©",
                    "desc": "StyleBench æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°ä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¸­çš„æ¨ç†é£æ ¼ã€‚æˆ‘ä»¬ç ”ç©¶äº†äº”ç§ä»£è¡¨æ€§çš„æ¨ç†é£æ ¼ï¼ŒåŒ…æ‹¬æ€ç»´é“¾ï¼ˆCoTï¼‰ã€æ€ç»´æ ‘ï¼ˆToTï¼‰ã€æ€ç»´ç®—æ³•ï¼ˆAoTï¼‰ã€æ€ç»´è‰å›¾ï¼ˆSoTï¼‰å’Œè‰ç¨¿é“¾ï¼ˆCoDï¼‰ï¼Œå¹¶åœ¨äº”ä¸ªæ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œæ²¡æœ‰ä¸€ç§æ¨ç†é£æ ¼åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½æ˜¯æœ€ä½³çš„ï¼Œå…¶æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºæ¨¡å‹è§„æ¨¡å’Œä»»åŠ¡ç±»å‹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒåŸºäºæœç´¢çš„æ–¹æ³•åœ¨å¼€æ”¾æ€§é—®é¢˜ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†éœ€è¦å¤§è§„æ¨¡æ¨¡å‹ï¼Œè€Œç®€æ´çš„é£æ ¼åœ¨å®šä¹‰æ˜ç¡®çš„ä»»åŠ¡ä¸­åˆ™èƒ½æ˜¾è‘—æé«˜æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.20712",
            "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy\n  Optimization in Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.20712",
            "abstract": "A novel reinforcement learning algorithm, CE-GPPO, reintroduces gradients from clipped tokens to improve the exploration-exploitation balance in training large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose Controlling Entropy via Gradient-Preserving Policy Optimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "ad1a5016131ff587",
            "authors": [
                "Zhenpeng Su",
                "Leiyu Pan",
                "Minxuan Lv",
                "Yuntao Li",
                "Wenping Hu",
                "Fuzheng Zhang",
                "Kun Gai",
                "Guorui Zhou"
            ],
            "affiliations": [
                "Independent",
                "Klear Team, Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.20712.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ CE-GPPO Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚Ğ¸Ğ¿Ğ° PPO Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ÑÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¾Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ·-Ğ·Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° ĞºĞ»Ğ¸Ğ¿Ğ¿Ğ¸Ğ½Ğ³Ğ°. CE-GPPO Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¾Ñ‚ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Exploration-Exploitation Balance in Language Models with CE-GPPO",
                    "desc": "The paper presents a new reinforcement learning algorithm called CE-GPPO, which enhances the training of large language models by reintroducing gradients from clipped tokens. This approach addresses the challenge of managing policy entropy, which is crucial for balancing exploration and exploitation during training. By carefully controlling the gradients from low-probability tokens, CE-GPPO improves the stability of entropy dynamics, which is often overlooked in existing methods. The authors provide both theoretical insights and empirical results demonstrating that CE-GPPO outperforms traditional algorithms like PPO on various reasoning tasks."
                },
                "zh": {
                    "title": "é‡æ–°å¼•å…¥æ¢¯åº¦ï¼Œä¼˜åŒ–æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡",
                    "desc": "CE-GPPOæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹çš„æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡ã€‚è¯¥ç®—æ³•é€šè¿‡é‡æ–°å¼•å…¥è¢«å‰ªåˆ‡çš„æ ‡è®°çš„æ¢¯åº¦ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸¢å¤±æœ‰ä»·å€¼çš„æ¢¯åº¦ä¿¡å·çš„é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›è¢«å‰ªåˆ‡çš„æ ‡è®°åœ¨è°ƒèŠ‚ç­–ç•¥ç†µçš„æ¼”å˜ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚CE-GPPOåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å¼ºåŸºçº¿æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.20414",
            "title": "SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and\n  Self-Reflective Agent",
            "url": "https://huggingface.co/papers/2509.20414",
            "abstract": "SceneWeaver, a reflective agentic framework, uses a language model-based planner to iteratively refine 3D scene synthesis, achieving high physical, visual, and semantic quality across diverse instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Indoor scene synthesis has become increasingly important with the rise of Embodied AI, which requires 3D environments that are not only visually realistic but also physically plausible and functionally diverse. While recent approaches have advanced visual fidelity, they often remain constrained to fixed scene categories, lack sufficient object-level detail and physical consistency, and struggle to align with complex user instructions. In this work, we present SceneWeaver, a reflective agentic framework that unifies diverse scene synthesis paradigms through tool-based iterative refinement. At its core, SceneWeaver employs a language model-based planner to select from a suite of extensible scene generation tools, ranging from data-driven generative models to visual- and LLM-based methods, guided by self-evaluation of physical plausibility, visual realism, and semantic alignment with user input. This closed-loop reason-act-reflect design enables the agent to identify semantic inconsistencies, invoke targeted tools, and update the environment over successive iterations. Extensive experiments on both common and open-vocabulary room types demonstrate that SceneWeaver not only outperforms prior methods on physical, visual, and semantic metrics, but also generalizes effectively to complex scenes with diverse instructions, marking a step toward general-purpose 3D environment generation. Project website: https://scene-weaver.github.io/.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-24",
            "pub_date_card": {
                "ru": "24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 24",
                "zh": "9æœˆ24æ—¥"
            },
            "hash": "4b1e989136f96d93",
            "authors": [
                "Yandan Yang",
                "Baoxiong Jia",
                "Shujie Zhang",
                "Siyuan Huang"
            ],
            "affiliations": [
                "State Key Laboratory of General Artificial Intelligence, BIGAI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.20414.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#reasoning",
                    "#games",
                    "#alignment",
                    "#agents"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚Ğ¾Ñ€: AI-Ğ°Ğ³ĞµĞ½Ñ‚ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ",
                    "desc": "SceneWeaver - ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° 3D ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ ÑÑ†ĞµĞ½, SceneWeaver ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "SceneWeaver: Crafting Realistic 3D Environments with Iterative Refinement",
                    "desc": "SceneWeaver is a framework designed for creating 3D scenes that are not only visually appealing but also physically realistic and semantically accurate. It utilizes a language model-based planner to iteratively refine the scene generation process, allowing for adjustments based on user instructions. By integrating various scene generation tools and employing a closed-loop reasoning approach, SceneWeaver can identify and correct inconsistencies in the generated scenes. This innovative method significantly improves the quality of 3D environments, making it suitable for a wide range of applications in Embodied AI."
                },
                "zh": {
                    "title": "SceneWeaverï¼šæ™ºèƒ½3Dåœºæ™¯åˆæˆçš„æ–°çªç ´",
                    "desc": "SceneWeaveræ˜¯ä¸€ä¸ªåæ€æ€§ä»£ç†æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºè¯­è¨€æ¨¡å‹çš„è§„åˆ’å™¨æ¥è¿­ä»£ä¼˜åŒ–3Dåœºæ™¯åˆæˆã€‚å®ƒèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„æŒ‡ä»¤ä¸‹ï¼Œå®ç°é«˜æ°´å¹³çš„ç‰©ç†ã€è§†è§‰å’Œè¯­ä¹‰è´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å·¥å…·é©±åŠ¨çš„è¿­ä»£ç²¾ç‚¼ï¼Œç»Ÿä¸€äº†ä¸åŒçš„åœºæ™¯åˆæˆèŒƒå¼ã€‚å®éªŒè¡¨æ˜ï¼ŒSceneWeaveråœ¨ç‰©ç†ã€è§†è§‰å’Œè¯­ä¹‰æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ä»¥å¾€çš„æ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°é€‚åº”å¤æ‚åœºæ™¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14662",
            "title": "Understanding the Thinking Process of Reasoning Models: A Perspective\n  from Schoenfeld's Episode Theory",
            "url": "https://huggingface.co/papers/2509.14662",
            "abstract": "A novel framework using Schoenfeld's Episode Theory is introduced to analyze the reasoning patterns of Large Reasoning Models in solving math problems, providing a benchmark for machine reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Reasoning Models (LRMs) generate extensive chain-of-thought reasoning, we lack a principled framework for understanding how these thoughts are structured. In this paper, we introduce a novel approach by applying Schoenfeld's Episode Theory, a classic cognitive framework for human mathematical problem-solving, to analyze the reasoning traces of LRMs. We annotated thousands of sentences and paragraphs from model-generated solutions to math problems using seven cognitive labels (e.g., Plan, Implement, Verify). The result is the first publicly available benchmark for the fine-grained analysis of machine reasoning, including a large annotated corpus and detailed annotation guidebooks. Our preliminary analysis reveals distinct patterns in LRM reasoning, such as the transition dynamics between cognitive states. This framework provides a theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems.",
            "score": 1,
            "issue_id": 6098,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "1ed939a345480c28",
            "authors": [
                "Ming Li",
                "Nan Zhang",
                "Chenrui Fan",
                "Hong Jiao",
                "Yanbin Fu",
                "Sydney Peters",
                "Qingshu Xu",
                "Robert Lissitz",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14662.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#interpretability",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ AI Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ² Ğ¨Ñ‘Ğ½Ñ„ĞµĞ»ÑŒĞ´Ğ°, ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼, Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚Ğ¸Ğ² Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ¼ÑŒÑ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² AI Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Understanding Machine Reasoning with Cognitive Frameworks",
                    "desc": "This paper presents a new framework that uses Schoenfeld's Episode Theory to analyze how Large Reasoning Models (LRMs) approach math problems. By applying cognitive labels to thousands of sentences from model-generated solutions, the authors create a detailed benchmark for understanding machine reasoning. The study reveals specific patterns in the reasoning processes of LRMs, highlighting how they transition between different cognitive states. This framework not only aids in interpreting LRM cognition but also sets the stage for developing more transparent and controllable reasoning systems in the future."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†æ¨¡å¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨Schoenfeldçš„æƒ…èŠ‚ç†è®ºåˆ†æå¤§å‹æ¨ç†æ¨¡å‹åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶çš„æ¨ç†æ¨¡å¼ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å¯¹æ¨¡å‹ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆè¿›è¡Œæ ‡æ³¨ï¼Œä½¿ç”¨ä¸ƒç§è®¤çŸ¥æ ‡ç­¾ï¼ˆå¦‚è®¡åˆ’ã€å®æ–½ã€éªŒè¯ï¼‰æ¥åˆ†ææ¨ç†è½¨è¿¹ã€‚ç ”ç©¶ç»“æœæä¾›äº†ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„æœºå™¨æ¨ç†ç»†ç²’åº¦åˆ†æåŸºå‡†ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå¤§å‹æ ‡æ³¨è¯­æ–™åº“å’Œè¯¦ç»†çš„æ ‡æ³¨æŒ‡å—ã€‚åˆæ­¥åˆ†ææ˜¾ç¤ºäº†å¤§å‹æ¨ç†æ¨¡å‹æ¨ç†ä¸­çš„ç‹¬ç‰¹æ¨¡å¼ï¼Œå¦‚è®¤çŸ¥çŠ¶æ€ä¹‹é—´çš„è½¬å˜åŠ¨æ€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21320",
            "title": "SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines",
            "url": "https://huggingface.co/papers/2509.21320",
            "abstract": "A scientific reasoning foundation model pre-trained on diverse scientific data supports multiple tasks and enhances cross-domain generalization and fidelity through specialized training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at https://huggingface.co/SciReason and https://github.com/open-sciencelab/SciReason.",
            "score": 0,
            "issue_id": 6098,
            "pub_date": "2025-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "e33c7b540d84ad9a",
            "authors": [
                "Yizhou Wang",
                "Chen Tang",
                "Han Deng",
                "Jiabei Xiao",
                "Jiaqi Liu",
                "Jianyu Wu",
                "Jun Yao",
                "Pengze Li",
                "Encheng Su",
                "Lintao Wang",
                "Guohang Zhuang",
                "Yuchen Ren",
                "Ben Fei",
                "Ming Hu",
                "Xin Chen",
                "Dongzhan Zhou",
                "Junjun He",
                "Xiangyu Yue",
                "Zhenfei Yin",
                "Jiamin Wu",
                "Qihao Zheng",
                "Yuhao Zhou",
                "Huihui Xu",
                "Chenglong Ma",
                "Yan Lu",
                "Wenlong Zhang",
                "Chunfeng Song",
                "Philip Torr",
                "Shixiang Tang",
                "Xinzhu Ma",
                "Wanli Ouyang",
                "Lei Bai"
            ],
            "affiliations": [
                "Beihang University",
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Sydney",
                "University of Oxford",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21320.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#transfer_learning",
                    "#multimodal",
                    "#data",
                    "#science",
                    "#open_source"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ foundation Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 206 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ supervised fine-tuning Ğ½Ğ° 40 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ reinforcement learning Ñ task-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ reward shaping. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ², ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼ĞµĞ¶Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Scientific Reasoning with a Versatile Foundation Model",
                    "desc": "This paper introduces a scientific reasoning foundation model that is trained on a vast dataset of scientific texts and sequences. It employs advanced techniques like supervised fine-tuning and reinforcement learning to improve its ability to perform various scientific tasks. The model can translate between different scientific formats, extract knowledge, predict properties, and generate sequences, making it versatile across multiple domains. By enhancing cross-domain generalization and fidelity, this model outperforms specialized systems in handling a wide range of scientific inquiries."
                },
                "zh": {
                    "title": "ç§‘å­¦æ¨ç†æ¨¡å‹ï¼šè·¨é¢†åŸŸçš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§ç§‘å­¦æ¨ç†åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ ·çš„ç§‘å­¦æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ”¯æŒå¤šç§ä»»åŠ¡å¹¶å¢å¼ºè·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚æ¨¡å‹ä½¿ç”¨äº†2060äº¿ä¸ªæ ‡è®°çš„è¯­æ–™åº“ï¼Œæ¶µç›–ç§‘å­¦æ–‡æœ¬ã€çº¯åºåˆ—å’Œåºåˆ—-æ–‡æœ¬å¯¹ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„è®­ç»ƒæŠ€æœ¯è¿›è¡Œå¯¹é½ã€‚å®ƒèƒ½å¤Ÿæ‰§è¡Œå¤šè¾¾103ä¸ªä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬ä¸ç§‘å­¦æ ¼å¼ä¹‹é—´çš„ç¿»è¯‘ã€çŸ¥è¯†æå–ã€å±æ€§é¢„æµ‹å’Œåˆ†ç±»ç­‰ã€‚ä¸ä¸“ä¸šç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨æŒ‡ä»¤è¦†ç›–èŒƒå›´ã€è·¨é¢†åŸŸæ³›åŒ–å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-25.html",
    "link_next": "2025-09-29.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "25.09",
        "en": "09/25",
        "zh": "9æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "29.09",
        "en": "09/29",
        "zh": "9æœˆ29æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}