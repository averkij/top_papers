{
    "date": {
        "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 30",
        "zh": "12æœˆ30æ—¥"
    },
    "time_utc": "2025-12-30 05:25",
    "weekday": 1,
    "issue_id": 311,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.23705",
            "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
            "url": "https://huggingface.co/papers/2512.23705",
            "abstract": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.",
            "score": 21,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "acdde1b848da34eb",
            "authors": [
                "Shaocong Xu",
                "Songlin Wei",
                "Qizhe Wei",
                "Zheng Geng",
                "Hong Li",
                "Licheng Shen",
                "Qianpu Sun",
                "Shu Han",
                "Bin Ma",
                "Bohan Li",
                "Chongjie Ye",
                "Yuhang Zheng",
                "Nan Wang",
                "Saining Zhang",
                "Hao Zhao"
            ],
            "affiliations": [
                "Beihang University",
                "Beijing Academy of Artificial Intelligence",
                "European Institute of Innovation and Technology Ningbo",
                "FNii, The Chinese University of Hong Kong, Shenzhen",
                "National University of Singapore",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of Southern California",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23705.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#synthetic",
                    "#robotics",
                    "#diffusion",
                    "#cv",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ·Ğ½Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ ÑÑ‚ĞµĞºĞ»ÑĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ²",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ DKT â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ¶Ğµ Ğ½Ğ°ÑƒÑ‡Ğ¸Ğ»Ğ¸ÑÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ TransPhy3D â€” ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 11 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ¸Ğ´ĞµĞ¾sequences Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¾Ñ‚Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼ ray tracing. ĞœĞ¾Ğ´ĞµĞ»ÑŒ DKT Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞ³ĞºĞ¸Ñ… LoRA Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ RGB Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ DiT Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Harnessing Diffusion for Transparent Object Perception",
                    "desc": "This paper addresses the challenges of perceiving transparent objects in computer vision, where traditional methods struggle due to optical effects like refraction and reflection. The authors introduce TransPhy3D, a synthetic video dataset that includes 11,000 sequences of transparent and reflective scenes, created using advanced rendering techniques. They develop a model called DKT, which utilizes a large video diffusion model to predict depth and normals from video inputs, achieving state-of-the-art results in transparency-related benchmarks. The findings suggest that generative models can effectively learn and apply optical principles, enhancing perception systems for real-world applications involving complex materials."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹æŒæ¡é€æ˜æ€§",
                    "desc": "é€æ˜ç‰©ä½“å¯¹æ„ŸçŸ¥ç³»ç»Ÿæ¥è¯´ä¸€ç›´å¾ˆéš¾å¤„ç†ï¼Œå› ä¸ºæŠ˜å°„ã€åå°„å’Œé€å°„æ‰“ç ´äº†ç«‹ä½“è§†è§‰ã€æ—¶é—´é£è¡Œï¼ˆToFï¼‰å’Œå•ç›®æ·±åº¦ä¼°è®¡çš„å‡è®¾ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ç°ä»£è§†é¢‘æ‰©æ•£æ¨¡å‹å·²ç»èƒ½å¤Ÿåˆæˆä»¤äººä¿¡æœçš„é€æ˜ç°è±¡ï¼Œè¡¨æ˜å®ƒä»¬å†…åŒ–äº†å…‰å­¦è§„åˆ™ã€‚æˆ‘ä»¬æ„å»ºäº†TransPhy3Dï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆçš„è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å«11,000ä¸ªé€æ˜/åå°„åœºæ™¯çš„åºåˆ—ï¼Œä½¿ç”¨Blender/Cyclesæ¸²æŸ“ã€‚é€šè¿‡è½»é‡çº§çš„LoRAé€‚é…å™¨ï¼Œæˆ‘ä»¬ä»å¤§å‹è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­å­¦ä¹ è§†é¢‘åˆ°è§†é¢‘çš„æ·±åº¦ï¼ˆå’Œæ³•çº¿ï¼‰è½¬æ¢å™¨ï¼Œæœ€ç»ˆå®ç°äº†åœ¨é€æ˜æ€§ç›¸å…³çš„åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é›¶-shotçš„æœ€å…ˆè¿›æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22096",
            "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model",
            "url": "https://huggingface.co/papers/2512.22096",
            "abstract": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.",
            "score": 21,
            "issue_id": 310,
            "pub_date": "2025-12-26",
            "pub_date_card": {
                "ru": "26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 26",
                "zh": "12æœˆ26æ—¥"
            },
            "hash": "bea639b4ce2ed908",
            "authors": [
                "Xiaofeng Mao",
                "Zhen Li",
                "Chuanhao Li",
                "Xiaojie Xu",
                "Kaining Ying",
                "Tong He",
                "Jiangmiao Pang",
                "Yu Qiao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22096.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#long_context",
                    "#diffusion"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ»Ğ°Ğ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñ‹, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Interactive World Generation with Diffusion Models",
                    "desc": "This paper introduces a new framework called \\method that enhances the generation of interactive worlds using diffusion models. It tackles significant issues like large model sizes and slow inference times, which hinder real-time applications. The framework includes innovative techniques such as context compression and bidirectional attention distillation to improve performance. Additionally, it allows users to control world events through text prompts, making the generated environments more dynamic and engaging."
                },
                "zh": {
                    "title": "ç”Ÿæˆäº’åŠ¨ä¸–ç•Œçš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶\textit{method}ï¼Œæ—¨åœ¨ä»å•å¼ å›¾åƒæˆ–æ–‡æœ¬æç¤ºç”Ÿæˆé€¼çœŸã€äº’åŠ¨å’Œè¿ç»­çš„ä¸–ç•Œã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨å‚æ•°è§„æ¨¡ã€æ¨ç†æ­¥éª¤å’Œå†å²ä¸Šä¸‹æ–‡ç­‰æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡äº†å®æ—¶æ€§èƒ½å¹¶æ”¯æŒæ–‡æœ¬æ§åˆ¶ç”Ÿæˆã€‚æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé•¿è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€å®æ—¶æµåŠ é€Ÿç­–ç•¥å’Œæ–‡æœ¬æ§åˆ¶çš„ä¸–ç•Œäº‹ä»¶ç”Ÿæˆæ–¹æ³•ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼Œ\textit{method}èƒ½å¤Ÿå®ç°é”®ç›˜åŸºç¡€çš„ä¸–ç•Œæ¢ç´¢ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22322",
            "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
            "url": "https://huggingface.co/papers/2512.22322",
            "abstract": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.",
            "score": 21,
            "issue_id": 310,
            "pub_date": "2025-12-26",
            "pub_date_card": {
                "ru": "26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 26",
                "zh": "12æœˆ26æ—¥"
            },
            "hash": "d2216b9110bb4404",
            "authors": [
                "Shaofei Cai",
                "Yulei Qin",
                "Haojia Lin",
                "Zihan Xu",
                "Gang Li",
                "Yuchen Shi",
                "Zongyi Li",
                "Yong Mao",
                "Siqi Cai",
                "Xiaoyu Tan",
                "Yitao Liang",
                "Ke Li",
                "Xing Sun"
            ],
            "affiliations": [
                "Peking University",
                "Tencent",
                "Youtu-Agent Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22322.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ“¸",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚ ÑĞ°Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ¹ ÑƒÑĞ¿ĞµÑ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ SmartSnap Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ğ½Ğ¾ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° ÑĞ²Ğ¾ĞµĞ³Ğ¾ ÑƒÑĞ¿ĞµÑ…Ğ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²ÑĞµĞ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾ÑĞ»Ğµ ĞµĞ³Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ·Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² ÑĞºÑ€Ğ°Ğ½Ğ°, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹, Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM-as-a-Judge. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 26% Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ´ĞµĞ»Ğ°Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Agents with SmartSnap: Proactive Self-Verification for Enhanced Performance",
                    "desc": "This paper introduces SmartSnap, a new approach in agentic reinforcement learning that enhances the scalability of autonomous agents in complex GUI tasks. Instead of relying on traditional post-hoc verification methods, SmartSnap empowers agents to perform proactive self-verification during task execution. The Self-Verifying Agent captures essential snapshots as evidence of task completion, guided by the 3C Principles: Completeness, Conciseness, and Creativity. Experimental results show that this method significantly improves performance in training LLM-driven agents, achieving notable gains in efficiency and reliability."
                },
                "zh": {
                    "title": "æ™ºèƒ½å¿«ç…§ï¼šä»£ç†è‡ªæˆ‘éªŒè¯çš„æ–°èŒƒå¼",
                    "desc": "ä»£ç†å¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ä»»åŠ¡ä¸­å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ï¼Œä½†å…¶å¯æ‰©å±•æ€§å—åˆ°ä»»åŠ¡å®ŒæˆéªŒè¯çš„ä¸¥é‡é™åˆ¶ã€‚ç°æœ‰çš„ä»»åŠ¡éªŒè¯é€šå¸¸æ˜¯è¢«åŠ¨çš„ï¼Œåˆ†æä»£ç†çš„æ•´ä¸ªäº¤äº’è½¨è¿¹æ¥åˆ¤æ–­å…¶æ˜¯å¦æˆåŠŸï¼Œè¿™ç§æ–¹æ³•å¤„ç†å†—é•¿ä¸”åŒ…å«æ— å…³ä¿¡æ¯çš„å†å²è®°å½•ï¼Œå¯¼è‡´éªŒè¯æˆæœ¬é«˜ä¸”å¯é æ€§ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†SmartSnapï¼Œè¿™æ˜¯ä¸€ç§ä¸»åŠ¨çš„è‡ªæˆ‘éªŒè¯æ–¹æ³•ï¼Œä»£ç†ä¸ä»…å®Œæˆä»»åŠ¡ï¼Œè¿˜èƒ½é€šè¿‡ç²¾å¿ƒç­–åˆ’çš„å¿«ç…§è¯æ®æ¥è¯æ˜å…¶å®Œæˆæƒ…å†µã€‚é€šè¿‡æˆ‘ä»¬çš„3CåŸåˆ™ï¼ˆå®Œæ•´æ€§ã€ç®€æ´æ€§å’Œåˆ›é€ æ€§ï¼‰ï¼Œä»£ç†èƒ½å¤Ÿåœ¨åœ¨çº¿ç¯å¢ƒä¸­è¿›è¡Œè‡ªæˆ‘éªŒè¯ï¼Œä»è€Œæé«˜äº†è®­ç»ƒçš„å¯æ‰©å±•æ€§å’Œæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23447",
            "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss",
            "url": "https://huggingface.co/papers/2512.23447",
            "abstract": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.",
            "score": 13,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "bb7b40a0888662c7",
            "authors": [
                "Ang Lv",
                "Jin Ma",
                "Yiyuan Ma",
                "Siyuan Qiao"
            ],
            "affiliations": [
                "ByteDance",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23447.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ¡Ğ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ expert-router coupling (ERC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mixture-of-Experts. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞ»Ğ°Ğ±ÑƒÑ Ğ´Ğ»Ñ Ñ‡ÑƒĞ¶Ğ¸Ñ…. ERC loss Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°, Ñ‚Ğ°Ğº ĞºĞ°Ğº ĞµÑ‘ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (nÂ²), Ğ° Ğ½Ğµ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ±Ğ°Ñ‚Ñ‡Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ 3B Ğ´Ğ¾ 15B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Aligning Router Decisions with Expert Capabilities for Enhanced MoE Performance",
                    "desc": "The paper introduces the expert-router coupling (ERC) loss, which enhances the performance of Mixture-of-Experts (MoE) models by aligning router decisions with the capabilities of the experts. It imposes constraints on internal activations to ensure that each expert responds more strongly to its own proxy token than to others, promoting specialization. This approach is computationally efficient, as it operates on a fixed cost related to the number of experts, rather than the number of tokens processed. The authors demonstrate the effectiveness of ERC loss through extensive experiments with large-scale MoE models, providing insights into expert specialization during training."
                },
                "zh": {
                    "title": "ä¸“å®¶ä¸è·¯ç”±å™¨çš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“å®¶-è·¯ç”±è€¦åˆï¼ˆERCï¼‰æŸå¤±ï¼Œç”¨äºæé«˜æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚ERCæŸå¤±é€šè¿‡å¯¹å†…éƒ¨æ¿€æ´»æ–½åŠ çº¦æŸï¼Œä½¿è·¯ç”±å™¨çš„å†³ç­–ä¸ä¸“å®¶çš„èƒ½åŠ›ç´§å¯†ç»“åˆã€‚å…·ä½“æ¥è¯´ï¼ŒERCæŸå¤±ç¡®ä¿æ¯ä¸ªä¸“å®¶å¯¹å…¶ä»£ç†æ ‡è®°çš„æ¿€æ´»é«˜äºå¯¹å…¶ä»–ä¸“å®¶çš„ä»£ç†æ ‡è®°çš„æ¿€æ´»ï¼Œä»è€Œä¿è¯æ¯ä¸ªè·¯ç”±åµŒå…¥å‡†ç¡®åæ˜ å…¶å¯¹åº”ä¸“å®¶çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä¸åŒè§„æ¨¡çš„MoE-LLMè¿›è¡Œé¢„è®­ç»ƒå’Œåˆ†æï¼ŒéªŒè¯äº†ERCæŸå¤±çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æä¾›äº†å¯¹ä¸“å®¶ä¸“ä¸šåŒ–æ°´å¹³çš„çµæ´»æ§åˆ¶å’Œå®šé‡è·Ÿè¸ªã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23576",
            "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation",
            "url": "https://huggingface.co/papers/2512.23576",
            "abstract": "Real-time multimodal video generation via diffusion is enabled by an improved distillation approach with multimodal conditioning and optimized scheduling, reducing inference latency while maintaining quality for interactive systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.",
            "score": 10,
            "issue_id": 311,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "d574a4e2bad83857",
            "authors": [
                "Ethan Chern",
                "Zhulin Hu",
                "Bohao Tang",
                "Jiadi Su",
                "Steffi Chern",
                "Zhijie Deng",
                "Pengfei Liu"
            ],
            "affiliations": [
                "GAIR",
                "SII",
                "SJTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23576.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#multimodal",
                    "#optimization",
                    "#training",
                    "#video",
                    "#inference"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸ (Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ°ÑƒĞ´Ğ¸Ğ¾). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ on-policy Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Self Forcing, Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞµ. Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ 20-ĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° LiveTalk Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Real-Time Multimodal Video Generation for Seamless Interaction",
                    "desc": "This paper presents a novel approach to real-time video generation using diffusion models, focusing on multimodal inputs like text, images, and audio. The authors address the limitations of existing methods that struggle with interactive human-AI communication due to high inference latency and quality issues. By improving the distillation process and optimizing the scheduling of video frame generation, they achieve significant reductions in latency while maintaining high visual quality. The resulting system, LiveTalk, demonstrates superior performance in generating coherent and high-quality video content in real-time, enhancing the interactive experience."
                },
                "zh": {
                    "title": "å®æ—¶å¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆçš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„è’¸é¦æ–¹æ³•ï¼Œç”¨äºå®æ—¶å¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆï¼Œç»“åˆæ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šç§è¾“å…¥ã€‚é€šè¿‡ä¼˜åŒ–è°ƒåº¦å’Œæé«˜æ¡ä»¶è¾“å…¥çš„è´¨é‡ï¼Œå‡å°‘äº†æ¨ç†å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„ç”Ÿæˆæ•ˆæœä¸ä¼ ç»Ÿçš„åŒå‘æ¨¡å‹ç›¸å½“ï¼Œä½†æ¨ç†æˆæœ¬é™ä½äº†20å€ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºLiveTalkçš„å®æ—¶å¤šæ¨¡æ€äº’åŠ¨ç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº†äººæœºäº¤äº’çš„æµç•…æ€§å’Œè§†é¢‘å†…å®¹çš„è¿è´¯æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23676",
            "title": "Web World Models",
            "url": "https://huggingface.co/papers/2512.23676",
            "abstract": "Web World Models (WWMs) combine web frameworks with large language models to create controllable, open-ended persistent environments by structuring world state in web code and leveraging model-driven imagination for narratives and decisions.  \t\t\t\t\tAI-generated summary \t\t\t\t Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.",
            "score": 9,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "05bf3b1ace6e32c3",
            "authors": [
                "Jichen Feng",
                "Yifan Zhang",
                "Chenggong Zhang",
                "Yifu Lu",
                "Shilong Liu",
                "Mengdi Wang"
            ],
            "affiliations": [
                "Princeton University",
                "University of California, Los Angeles",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23676.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#agents",
                    "#games",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ’ĞµĞ±-ĞºĞ¾Ğ´ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ: Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Web World Model (WWM) â€” Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ²ĞµĞ±-Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ ĞµĞ³Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ²ĞµĞ±-ĞºĞ¾Ğ´Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ñ‹ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ… ÑÑ‚Ğ¾Ğ³Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ WWM Ğ½Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ Ğ²ĞµĞ±-ÑÑ‚ĞµĞºĞµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ñ‚Ğ»Ğ°Ñ, Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ĞºĞ°Ğº Ñ‚Ğ¸Ğ¿Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞ±-ÑÑ‚ĞµĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ…, Ğ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Creating Dynamic Worlds with Web World Models",
                    "desc": "Web World Models (WWMs) merge web frameworks with large language models to create dynamic and controllable environments. This approach balances the reliability of traditional web frameworks with the creativity of generative models, allowing for structured yet imaginative interactions. By implementing world states in web code, WWMs ensure logical consistency while enabling language models to generate narratives and make decisions. The study presents various applications of WWMs, highlighting design principles that facilitate scalable and open-ended exploration in AI-generated worlds."
                },
                "zh": {
                    "title": "Webä¸–ç•Œæ¨¡å‹ï¼šå¯æ§çš„å¼€æ”¾å¼ç¯å¢ƒ",
                    "desc": "Webä¸–ç•Œæ¨¡å‹ï¼ˆWWMï¼‰ç»“åˆäº†ç½‘ç»œæ¡†æ¶å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåˆ›å»ºå¯æ§çš„ã€å¼€æ”¾å¼çš„æŒä¹…ç¯å¢ƒã€‚å®ƒé€šè¿‡åœ¨ç½‘ç»œä»£ç ä¸­æ„å»ºä¸–ç•ŒçŠ¶æ€ï¼Œå¹¶åˆ©ç”¨æ¨¡å‹é©±åŠ¨çš„æƒ³è±¡åŠ›æ¥ç”Ÿæˆå™äº‹å’Œå†³ç­–ã€‚WWMåœ¨é€»è¾‘ä¸€è‡´æ€§å’Œç”Ÿæˆèƒ½åŠ›ä¹‹é—´æ‰¾åˆ°äº†å¹³è¡¡ï¼Œç¡®ä¿äº†ç¯å¢ƒçš„å¯æ§æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç½‘ç»œå †æ ˆå¯ä»¥ä½œä¸ºä¸–ç•Œæ¨¡å‹çš„å¯æ‰©å±•åŸºç¡€ï¼Œæ”¯æŒæ— é™ä½†ç»“æ„åŒ–çš„æ¢ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22431",
            "title": "Monadic Context Engineering",
            "url": "https://huggingface.co/papers/2512.22431",
            "abstract": "The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.",
            "score": 3,
            "issue_id": 310,
            "pub_date": "2025-12-27",
            "pub_date_card": {
                "ru": "27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 27",
                "zh": "12æœˆ27æ—¥"
            },
            "hash": "472b5375fd965473",
            "authors": [
                "Yifan Zhang",
                "Mengdi Wang"
            ],
            "affiliations": [
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22431.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "ğŸ§±",
                "ru": {
                    "title": "ĞĞ»Ğ³ĞµĞ±Ñ€Ğ°Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Monadic Context Engineering (MCE) â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ»Ğ³ĞµĞ±Ñ€Ğ°Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ„ÑƒĞ½ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ°Ğ¿Ğ¿Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ½Ğ°Ğ´. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼Ğ¾Ğ½Ğ°Ğ´Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ğ°Ğ¿Ğ¿Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, Ğ° Ğ¼Ğ¾Ğ½Ğ°Ğ´Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ…, Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ Meta-Agents, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ MCE Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ğ°Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Empowering AI Agents with Monadic Context Engineering",
                    "desc": "This paper presents Monadic Context Engineering (MCE), a new way to design autonomous agents using concepts from functional programming. MCE utilizes algebraic structures like Functors and Monads to improve how agents handle tasks, manage state, and deal with errors. By treating agent workflows as computational contexts, MCE simplifies complex operations such as error handling and asynchronous execution. The framework also introduces Meta-Agents, which can dynamically create and manage workflows for sub-agents, enhancing the flexibility and robustness of AI systems."
                },
                "zh": {
                    "title": "å•å­ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼šæ„å»ºæ™ºèƒ½ä»£ç†çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¶æ„èŒƒå¼ï¼Œç§°ä¸ºå•å­ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼ˆMCEï¼‰ï¼Œæ—¨åœ¨æ”¹å–„å½“å‰è‡ªä¸»ä»£ç†çš„è®¾è®¡ã€‚MCEåˆ©ç”¨å‡½å­ã€åº”ç”¨å‡½å­å’Œå•å­çš„ä»£æ•°ç»“æ„ï¼Œä¸ºä»£ç†å·¥ä½œæµæä¾›äº†ä¸€ä¸ªå½¢å¼åŒ–çš„åŸºç¡€ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒçŠ¶æ€ç®¡ç†ã€é”™è¯¯å¤„ç†å’Œå¹¶å‘æ‰§è¡Œç­‰è·¨åˆ‡å…³æ³¨ç‚¹å¯ä»¥é€šè¿‡ä»£æ•°å±æ€§å†…åœ¨åœ°ç®¡ç†ã€‚æœ€ç»ˆï¼ŒMCEä½¿å¼€å‘è€…èƒ½å¤Ÿä»ç®€å•ã€å¯ç‹¬ç«‹éªŒè¯çš„ç»„ä»¶æ„å»ºå¤æ‚ã€ç¨³å¥ä¸”é«˜æ•ˆçš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23707",
            "title": "Training AI Co-Scientists Using Rubric Rewards",
            "url": "https://huggingface.co/papers/2512.23707",
            "abstract": "AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.",
            "score": 2,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "0f116c273b415185",
            "authors": [
                "Shashwat Goel",
                "Rishi Hazra",
                "Dulhan Jayalath",
                "Timon Willi",
                "Parag Jain",
                "William F. Shen",
                "Ilias Leontiadis",
                "Francesco Barbieri",
                "Yoram Bachrach",
                "Jonas Geiping",
                "Chenxi Whitehouse"
            ],
            "affiliations": [
                "ELLIS Institute TÃ¼bingen",
                "Max Planck Institute for Intelligent Systems",
                "Meta Superintelligence Labs",
                "University of Cambridge",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23707.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#rl",
                    "#science",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ĞºĞ¾Ğ¿Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‰Ğ¸Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen3-30B-A3B Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ĞµĞµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ² 70% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸."
                },
                "en": {
                    "title": "Empowering Research with AI: Smarter Plans for Better Science",
                    "desc": "This paper explores the development of AI co-scientists that assist researchers by generating research plans based on specified aims and constraints. The authors address the limitations of current language models in producing plans that adhere to all requirements by leveraging a large dataset of existing research papers. They employ reinforcement learning with self-grading to train models, using extracted grading rubrics to enhance the generation process without needing human supervision. The results show significant improvements in plan quality across various domains, indicating the potential for AI to effectively support research endeavors."
                },
                "zh": {
                    "title": "æå‡AIååŠ©ç§‘å­¦ç ”ç©¶çš„èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ç°æœ‰ç ”ç©¶è®ºæ–‡çš„ä¸°å¯Œè¯­æ–™åº“æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥ç”Ÿæˆæ›´å¥½çš„ç ”ç©¶è®¡åˆ’ã€‚ç ”ç©¶è€…ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡è‡ªæˆ‘è¯„åˆ†çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒç ”ç©¶è®¡åˆ’ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è‡ªåŠ¨æå–çš„ç ”ç©¶ç›®æ ‡å’Œè¯„åˆ†æ ‡å‡†æ¥æ„å»ºå¤šæ ·åŒ–çš„è®­ç»ƒè¯­æ–™åº“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨ç”Ÿæˆç ”ç©¶è®¡åˆ’æ–¹é¢ä¼˜äºåˆå§‹æ¨¡å‹ï¼Œä¸”åœ¨åŒ»å­¦ç ”ç©¶ç­‰é¢†åŸŸä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºæå‡AIååŠ©ç§‘å­¦ç ”ç©¶çš„èƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23647",
            "title": "Nested Browser-Use Learning for Agentic Information Seeking",
            "url": "https://huggingface.co/papers/2512.23647",
            "abstract": "Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.",
            "score": 2,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "ff50e7cc84f888d8",
            "authors": [
                "Baixuan Li",
                "Jialong Wu",
                "Wenbiao Yin",
                "Kuan Li",
                "Zhongwang Zhang",
                "Huifeng Yin",
                "Zhengwei Tao",
                "Liwen Zhang",
                "Pengjun Xie",
                "Jingren Zhou",
                "Yong Jiang"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23647.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ’Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· API Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ NestBrowse â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ReAct-ÑÑ‚Ğ¸Ğ»Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ²ĞµĞ±Ğ°. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NestBrowse Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Unlocking Deeper Web Insights with NestBrowse",
                    "desc": "This paper presents a new approach called Nested Browser-Use Learning (NestBrowse) for improving information-seeking agents. Traditional agents struggle with complex web interactions, limiting their ability to gather rich information. NestBrowse introduces a structured framework that separates control of browser actions from the exploration of web pages, making it easier for agents to reason and operate. The results show that this method significantly enhances the agents' performance in deep web searches, proving to be both efficient and flexible."
                },
                "zh": {
                    "title": "è§£é”æ·±å±‚ä¿¡æ¯çš„æµè§ˆå™¨äº¤äº’èƒ½åŠ›",
                    "desc": "ä¿¡æ¯æœç´¢ä»£ç†åœ¨å¤šç§æœç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬çš„å·¥å…·ä½¿ç”¨ä¸»è¦é™äºAPIçº§åˆ«çš„ç‰‡æ®µæ£€ç´¢å’ŒåŸºäºURLçš„é¡µé¢è·å–ï¼Œè¿™é™åˆ¶äº†å¯¹æ›´ä¸°å¯Œä¿¡æ¯çš„è®¿é—®ã€‚å®Œå…¨çš„æµè§ˆå™¨äº¤äº’å¯ä»¥è§£é”æ›´æ·±å±‚æ¬¡çš„èƒ½åŠ›ï¼Œä½†å…¶ç»†ç²’åº¦æ§åˆ¶å’Œå†—é•¿çš„é¡µé¢å†…å®¹å¸¦æ¥äº†å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åµŒå¥—æµè§ˆå™¨ä½¿ç”¨å­¦ä¹ ï¼ˆNestBrowseï¼‰ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªæœ€å°ä¸”å®Œæ•´çš„æµè§ˆå™¨æ“ä½œæ¡†æ¶ï¼Œé€šè¿‡åµŒå¥—ç»“æ„å°†äº¤äº’æ§åˆ¶ä¸é¡µé¢æ¢ç´¢è§£è€¦ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒNestBrowseåœ¨æ·±å±‚ä¿¡æ¯æœç´¢åŸºå‡†æµ‹è¯•ä¸­å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œè¿›ä¸€æ­¥çš„åˆ†æå¼ºè°ƒäº†å…¶æ•ˆç‡å’Œçµæ´»æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23646",
            "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding",
            "url": "https://huggingface.co/papers/2512.23646",
            "abstract": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.",
            "score": 2,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "f644f8f445308f7e",
            "authors": [
                "Keda Tao",
                "Wenjie Du",
                "Bohan Yu",
                "Weiqiang Wang",
                "Jian Liu",
                "Huan Wang"
            ],
            "affiliations": [
                "Ant Group",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23646.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#audio",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ‘‚",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑˆĞ°Ğ½Ğ¸Ñ Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ°ÑƒĞ´Ğ¸Ğ¾ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ OmniAgent â€” Ğ°Ğ³ĞµĞ½Ñ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ¼Ğ½Ğ¸modĞ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ Ğ¾Ñ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ â€” Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ³Ñ€ÑƒĞ±Ğ¾-Ğº-Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 10-20% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Audio-Visual Understanding with Active Perception",
                    "desc": "OmniAgent is a new type of active perception agent that improves how machines understand both audio and visual information together. It addresses the common issues of previous models that struggle with detailed cross-modal understanding and alignment. By using a dynamic planning approach, OmniAgent can actively decide which tools to use based on the task at hand, rather than following a fixed process. This innovative method allows it to focus on important audio cues to enhance its reasoning about events in videos, leading to significantly better performance on various benchmarks."
                },
                "zh": {
                    "title": "ä¸»åŠ¨æ„ŸçŸ¥ï¼ŒéŸ³é¢‘å¼•å¯¼çš„å¤šæ¨¡æ€æ¨ç†æ–°çºªå…ƒ",
                    "desc": "OmniAgentæ˜¯ä¸€ç§å…¨éŸ³é¢‘å¼•å¯¼çš„ä¸»åŠ¨æ„ŸçŸ¥ä»£ç†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨éŸ³é¢‘å’Œè§†è§‰ç†è§£ä¸Šçš„ä¸è¶³ã€‚å®ƒé€šè¿‡åŠ¨æ€è§„åˆ’è‡ªä¸»åè°ƒå·¥å…·çš„è°ƒç”¨ï¼Œèƒ½å¤Ÿæ›´ç²¾ç»†åœ°è¿›è¡ŒéŸ³é¢‘-è§†è§‰æ¨ç†ã€‚ä¸ä»¥å¾€ä¾èµ–é™æ€å·¥ä½œæµç¨‹çš„æ¨¡å‹ä¸åŒï¼ŒOmniAgentå¼ºè°ƒä¸»åŠ¨å¤šæ¨¡æ€æ¢è¯¢ï¼Œé›†ä¸­æ³¨æ„åŠ›äºä¸ä»»åŠ¡ç›¸å…³çš„çº¿ç´¢ã€‚é€šè¿‡æ–°é¢–çš„ç²—åˆ°ç»†çš„éŸ³é¢‘å¼•å¯¼æ„ŸçŸ¥èŒƒå¼ï¼ŒOmniAgentåœ¨éŸ³é¢‘è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡è¶…è¶Šäº†é¢†å…ˆçš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23162",
            "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
            "url": "https://huggingface.co/papers/2512.23162",
            "abstract": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.",
            "score": 2,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "798d94b026b07ca4",
            "authors": [
                "Yufan He",
                "Pengfei Guo",
                "Mengya Xu",
                "Zhaoshuo Li",
                "Andriy Myronenko",
                "Dillan Imans",
                "Bingjie Liu",
                "Dongren Yang",
                "Mingxue Gu",
                "Yongnan Ji",
                "Yueming Jin",
                "Ren Zhao",
                "Baiyong Shen",
                "Daguang Xu"
            ],
            "affiliations": [
                "NVIDIA",
                "National University of Singapore",
                "Ruijin Hospital",
                "Sung Kyun Kwan University",
                "The Chinese University of Hong Kong",
                "Wenzhou Medical University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23162.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#synthetic",
                    "#robotics",
                    "#science",
                    "#healthcare",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€ĞµÑˆĞ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ SurgWorld â€” Ğ¼Ğ¸Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ AI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ½Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ»Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ¸Ğ· ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ñ‚ĞµĞ¼ ÑĞ°Ğ¼Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° vision language action (VLA), Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ… Ğ½Ğ° Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ."
                },
                "en": {
                    "title": "Unlocking Surgical Robotics with Synthetic Data and World Models",
                    "desc": "This paper addresses the challenge of limited data in training autonomous surgical robots by introducing a novel approach that utilizes a world model called SurgeWorld. The authors created the Surgical Action Text Alignment (SATA) dataset, which provides detailed action descriptions for surgical tasks, enabling the training of policy models. By employing an inverse dynamics model, they generate synthetic paired video action data from surgical videos, enhancing the training process. The results show that their surgical vision language action (VLA) policy, trained with this augmented data, significantly outperforms traditional models, paving the way for more efficient and scalable surgical robot training."
                },
                "zh": {
                    "title": "åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æå‡å¤–ç§‘æœºå™¨äººè‡ªä¸»æŠ€èƒ½",
                    "desc": "æœ¬ç ”ç©¶è§£å†³äº†è‡ªä¸»å¤–ç§‘æœºå™¨äººé¢ä¸´çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬åˆ›å»ºäº†Surgical Action Text Alignmentï¼ˆSATAï¼‰æ•°æ®é›†ï¼Œä¸ºå¤–ç§‘æœºå™¨äººæä¾›è¯¦ç»†çš„åŠ¨ä½œæè¿°ã€‚åŸºäºSATAï¼Œæˆ‘ä»¬æ„å»ºäº†SurgeWorldï¼Œä¸€ä¸ªå…ˆè¿›çš„ç‰©ç†AIä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–å’ŒçœŸå®çš„å¤–ç§‘æ‰‹æœ¯è§†é¢‘ã€‚é€šè¿‡ä½¿ç”¨åˆæˆè§†é¢‘æ¨æ–­ä¼ªè¿åŠ¨å­¦ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨çœŸå®å¤–ç§‘æœºå™¨äººå¹³å°ä¸Šè¡¨ç°ä¼˜äºä»…ä½¿ç”¨çœŸå®ç¤ºèŒƒè®­ç»ƒçš„æ¨¡å‹ï¼Œæ¨åŠ¨äº†è‡ªä¸»å¤–ç§‘æŠ€èƒ½çš„è·å–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23044",
            "title": "Video-BrowseComp: Benchmarking Agentic Video Research on Open Web",
            "url": "https://huggingface.co/papers/2512.23044",
            "abstract": "The paper addresses the modality gap in autonomous agents for video processing by introducing a benchmark requiring proactive, open-web video reasoning, revealing limitations of current models in metadata-sparse, dynamic video domains.  \t\t\t\t\tAI-generated summary \t\t\t\t The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web's most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present Video-BrowseComp, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning.",
            "score": 2,
            "issue_id": 310,
            "pub_date": "2025-12-28",
            "pub_date_card": {
                "ru": "28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 28",
                "zh": "12æœˆ28æ—¥"
            },
            "hash": "886b1afc4bd42000",
            "authors": [
                "Zhengyang Liang",
                "Yan Shu",
                "Xiangrui Liu",
                "Minghao Qin",
                "Kaixin Liang",
                "Paolo Rota",
                "Nicu Sebe",
                "Zheng Liu",
                "Lizi Liao"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Beijing University of Posts and Telecommunications",
                "Hong Kong Polytechnic University",
                "Singapore Management University",
                "University of Trento"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23044.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚Ğ¸: Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Video-BrowseComp Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ½Ğ¾ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑĞºĞ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑˆĞºĞ°Ğ»Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ LLM Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 15% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ñ‚ĞµÑ€ÑÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ğ±ĞµĞ· Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ĞµĞ±-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Bridging the Modality Gap in Video Reasoning",
                    "desc": "This paper introduces Video-BrowseComp, a new benchmark designed to address the challenges faced by autonomous agents in processing dynamic video content. It highlights the limitations of current models that primarily rely on textual information and curated video clips, which do not require active engagement with video timelines. The benchmark consists of 210 questions that necessitate proactive video reasoning, compelling models to verify claims using temporal visual evidence rather than just text. The findings reveal that even advanced models struggle significantly in metadata-sparse environments, underscoring the need for improved visual grounding in video processing tasks."
                },
                "zh": {
                    "title": "å¡«è¡¥è‡ªä¸»ä»£ç†è§†é¢‘å¤„ç†çš„æ¨¡æ€å·®è·",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è‡ªä¸»ä»£ç†åœ¨è§†é¢‘å¤„ç†ä¸­çš„æ¨¡æ€å·®è·ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œè¦æ±‚ä¸»åŠ¨è¿›è¡Œå¼€æ”¾ç½‘ç»œè§†é¢‘æ¨ç†ã€‚å½“å‰æ¨¡å‹åœ¨ç¼ºä¹å…ƒæ•°æ®å’ŒåŠ¨æ€è§†é¢‘é¢†åŸŸä¸­å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦é›†ä¸­äºè¢«åŠ¨æ„ŸçŸ¥ã€‚ç°æœ‰çš„è§†é¢‘åŸºå‡†æµ‹è¯•é€šå¸¸åªæä¾›ç»è¿‡ç­›é€‰çš„ç‰‡æ®µï¼Œæœªèƒ½è¯„ä¼°ä»£ç†åœ¨è§†é¢‘æ—¶é—´çº¿ä¸Šçš„ä¸»åŠ¨ç ”ç©¶èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥Video-BrowseCompåŸºå‡†ï¼Œè®ºæ–‡å¼ºè°ƒäº†åœ¨è§†é¢‘æ¨ç†ä¸­ä¾èµ–æ—¶é—´è§†è§‰è¯æ®çš„é‡è¦æ€§ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸä»è¢«åŠ¨æ„ŸçŸ¥å‘ä¸»åŠ¨è§†é¢‘æ¨ç†çš„è½¬å˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21720",
            "title": "An Information Theoretic Perspective on Agentic System Design",
            "url": "https://huggingface.co/papers/2512.21720",
            "abstract": "Agentic language model (LM) systems power modern applications like \"Deep Research\" and \"Claude Code,\" and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller \"compressor\" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger \"predictor\" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, we introduce a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. We show that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, we perform a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is 1.6times more accurate, 4.6times more concise, and conveys 5.5times more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors. Applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover 99% of frontier-LM accuracy at 26% of API costs.",
            "score": 1,
            "issue_id": 310,
            "pub_date": "2025-12-25",
            "pub_date_card": {
                "ru": "25 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 25",
                "zh": "12æœˆ25æ—¥"
            },
            "hash": "20cb8de872acc0d9",
            "authors": [
                "Shizhe He",
                "Avanika Narayan",
                "Ishan S. Khare",
                "Scott W. Linderman",
                "Christopher RÃ©",
                "Dan Biderman"
            ],
            "affiliations": [
                "Department of Computer Science, Stanford University",
                "Department of Statistics, Stanford University",
                "Wu Tsai Neurosciences Institute, Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21720.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#small_models",
                    "#agents",
                    "#long_context",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€-Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚Ğ¾Ñ€",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğµ Ğ¸Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¶Ğ¸Ğ¼Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚, Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞµĞ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ĞµĞ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€Ğ°Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ²ÑĞµĞ³Ğ¾ 3B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ 99% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° API Ğ½Ğ° 26%."
                },
                "en": {
                    "title": "Maximizing Efficiency: The Power of Compressor Models in Language Systems",
                    "desc": "This paper discusses the design of agentic language model systems that use smaller 'compressor' models to condense context into compact text for larger 'predictor' models. It highlights the lack of systematic guidance in choosing these models and introduces an information-theoretic approach to evaluate the quality of compression. By estimating mutual information between the context and its compression, the authors demonstrate that higher mutual information correlates with better performance in downstream tasks. The findings suggest that scaling compressor models is more beneficial than scaling predictor models, leading to more efficient and cost-effective language processing systems."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å‹ç¼©å™¨ä»¥æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä»£ç†è¯­è¨€æ¨¡å‹ç³»ç»Ÿçš„è®¾è®¡ï¼Œç‰¹åˆ«æ˜¯å‹ç¼©å™¨å’Œé¢„æµ‹å™¨çš„å…³ç³»ã€‚å‹ç¼©å™¨æ¨¡å‹å°†åŸå§‹ä¸Šä¸‹æ–‡è½¬åŒ–ä¸ºç´§å‡‘æ–‡æœ¬ï¼Œä¾›æ›´å¤§çš„é¢„æµ‹å™¨æ¨¡å‹ä½¿ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¿¡æ¯è®ºæ¡†æ¶ï¼Œé€šè¿‡äº’ä¿¡æ¯é‡åŒ–å‹ç¼©è´¨é‡ï¼Œå‘ç°äº’ä¿¡æ¯ä¸ä¸‹æ¸¸æ€§èƒ½æœ‰å¾ˆå¼ºçš„é¢„æµ‹èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¢å¤§å‹ç¼©å™¨çš„è§„æ¨¡æ¯”å¢å¤§é¢„æµ‹å™¨çš„è§„æ¨¡æ›´æœ‰æ•ˆï¼Œèƒ½å¤Ÿåœ¨æœ¬åœ°è®¾å¤‡ä¸Šå®ç°é«˜æ•ˆçš„æ¨¡å‹ç»„åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23222",
            "title": "Bridging Your Imagination with Audio-Video Generation via a Unified Director",
            "url": "https://huggingface.co/papers/2512.23222",
            "abstract": "A unified director model leveraging a Mixture-of-Transformers architecture with interleaved and disentangled learning generates coherent video scripts and consistent keyframes through a single framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.",
            "score": 0,
            "issue_id": 311,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "64e7665d5e5a4efc",
            "authors": [
                "Jiaxu Zhang",
                "Tianshu Hu",
                "Yuan Zhang",
                "Zenan Li",
                "Linjie Luo",
                "Guosheng Lin",
                "Xin Chen"
            ],
            "affiliations": [
                "ByteDance Intelligent Creation",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23222.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#multimodal",
                    "#long_context",
                    "#training",
                    "#video",
                    "#story_generation"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸ÑÑÑ‘Ñ€: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ UniMAGE â€” ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞ¶Ğ¸ÑÑÑ‘Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² Ğ¾Ğ´Ğ½Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mixture-of-Transformers. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Â«ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµÂ», Ğ³Ğ´Ğµ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ğ°ĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ½ĞµĞ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ°Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ»ÑŒĞ¼Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UniMAGE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unifying Script and Keyframe Generation for Coherent Video Creation",
                    "desc": "This paper presents UniMAGE, a unified director model that integrates script writing and keyframe generation into a single framework using a Mixture-of-Transformers architecture. By treating these tasks as interconnected, the model enhances the logical coherence of video scripts and the visual consistency of keyframes. The training process involves interleaved concept learning to improve understanding and creativity, followed by disentangled expert learning to separate the script and image generation processes. The results show that UniMAGE outperforms existing models, making it easier for non-experts to create multi-shot films."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¯¼æ¼”æ¨¡å‹ï¼šè§†é¢‘åˆ›ä½œçš„æ–°æ–¹å¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¯¼æ¼”æ¨¡å‹UniMAGEï¼Œåˆ©ç”¨æ··åˆå˜æ¢å™¨æ¶æ„æ¥ç”Ÿæˆè¿è´¯çš„è§†é¢‘è„šæœ¬å’Œä¸€è‡´çš„å…³é”®å¸§ã€‚ä¼ ç»Ÿçš„è§†é¢‘åˆ›ä½œç³»ç»Ÿå°†è„šæœ¬æ’°å†™å’Œå…³é”®é•œå¤´è®¾è®¡è§†ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„ä»»åŠ¡ï¼Œè€Œæˆ‘ä»¬è®¤ä¸ºè¿™ä¸¤è€…åº”åœ¨ä¸€ä¸ªæ¡†æ¶å†…ç»Ÿä¸€ã€‚é€šè¿‡å¼•å…¥â€œå…ˆäº¤é”™åè§£è€¦â€çš„è®­ç»ƒèŒƒå¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ›é€ è„šæœ¬å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniMAGEåœ¨ç”Ÿæˆé€»è¾‘è¿è´¯çš„è§†é¢‘è„šæœ¬å’Œè§†è§‰ä¸€è‡´çš„å…³é”®å¸§å›¾åƒæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-29.html",
    "link_next": "2025-12-31.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "29.12",
        "en": "12/29",
        "zh": "12æœˆ29æ—¥"
    },
    "short_date_next": {
        "ru": "31.12",
        "en": "12/31",
        "zh": "12æœˆ31æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 6,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 5,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 1,
        "#training": 8,
        "#robotics": 2,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    }
}