{
    "date": {
        "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 30",
        "zh": "12æœˆ30æ—¥"
    },
    "time_utc": "2025-12-30 14:23",
    "weekday": 1,
    "issue_id": 320,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.23447",
            "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss",
            "url": "https://huggingface.co/papers/2512.23447",
            "abstract": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.",
            "score": 64,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "bb7b40a0888662c7",
            "authors": [
                "Ang Lv",
                "Jin Ma",
                "Yiyuan Ma",
                "Siyuan Qiao"
            ],
            "affiliations": [
                "ByteDance",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23447.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ¡Ğ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ expert-router coupling (ERC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Mixture-of-Experts. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞ»Ğ°Ğ±ÑƒÑ Ğ´Ğ»Ñ Ñ‡ÑƒĞ¶Ğ¸Ñ…. ERC loss Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°, Ñ‚Ğ°Ğº ĞºĞ°Ğº ĞµÑ‘ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (nÂ²), Ğ° Ğ½Ğµ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ±Ğ°Ñ‚Ñ‡Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ 3B Ğ´Ğ¾ 15B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Aligning Router Decisions with Expert Capabilities for Enhanced MoE Performance",
                    "desc": "The paper introduces the expert-router coupling (ERC) loss, which enhances the performance of Mixture-of-Experts (MoE) models by aligning router decisions with the capabilities of the experts. It imposes constraints on internal activations to ensure that each expert responds more strongly to its own proxy token than to others, promoting specialization. This approach is computationally efficient, as it operates on a fixed cost related to the number of experts, rather than the number of tokens processed. The authors demonstrate the effectiveness of ERC loss through extensive experiments with large-scale MoE models, providing insights into expert specialization during training."
                },
                "zh": {
                    "title": "ä¸“å®¶ä¸è·¯ç”±å™¨çš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“å®¶-è·¯ç”±è€¦åˆï¼ˆERCï¼‰æŸå¤±ï¼Œç”¨äºæé«˜æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚ERCæŸå¤±é€šè¿‡å¯¹å†…éƒ¨æ¿€æ´»æ–½åŠ çº¦æŸï¼Œä½¿è·¯ç”±å™¨çš„å†³ç­–ä¸ä¸“å®¶çš„èƒ½åŠ›ç´§å¯†ç»“åˆã€‚å…·ä½“æ¥è¯´ï¼ŒERCæŸå¤±ç¡®ä¿æ¯ä¸ªä¸“å®¶å¯¹å…¶ä»£ç†æ ‡è®°çš„æ¿€æ´»é«˜äºå¯¹å…¶ä»–ä¸“å®¶çš„ä»£ç†æ ‡è®°çš„æ¿€æ´»ï¼Œä»è€Œä¿è¯æ¯ä¸ªè·¯ç”±åµŒå…¥å‡†ç¡®åæ˜ å…¶å¯¹åº”ä¸“å®¶çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä¸åŒè§„æ¨¡çš„MoE-LLMè¿›è¡Œé¢„è®­ç»ƒå’Œåˆ†æï¼ŒéªŒè¯äº†ERCæŸå¤±çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æä¾›äº†å¯¹ä¸“å®¶ä¸“ä¸šåŒ–æ°´å¹³çš„çµæ´»æ§åˆ¶å’Œå®šé‡è·Ÿè¸ªã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23576",
            "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation",
            "url": "https://huggingface.co/papers/2512.23576",
            "abstract": "Real-time multimodal video generation via diffusion is enabled by an improved distillation approach with multimodal conditioning and optimized scheduling, reducing inference latency while maintaining quality for interactive systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.",
            "score": 47,
            "issue_id": 311,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "d574a4e2bad83857",
            "authors": [
                "Ethan Chern",
                "Zhulin Hu",
                "Bohao Tang",
                "Jiadi Su",
                "Steffi Chern",
                "Zhijie Deng",
                "Pengfei Liu"
            ],
            "affiliations": [
                "GAIR",
                "SII",
                "SJTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23576.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#diffusion",
                    "#multimodal",
                    "#optimization",
                    "#training",
                    "#video",
                    "#inference"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸ (Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ°ÑƒĞ´Ğ¸Ğ¾). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ on-policy Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Self Forcing, Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞµ. Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ 20-ĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° LiveTalk Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Real-Time Multimodal Video Generation for Seamless Interaction",
                    "desc": "This paper presents a novel approach to real-time video generation using diffusion models, focusing on multimodal inputs like text, images, and audio. The authors address the limitations of existing methods that struggle with interactive human-AI communication due to high inference latency and quality issues. By improving the distillation process and optimizing the scheduling of video frame generation, they achieve significant reductions in latency while maintaining high visual quality. The resulting system, LiveTalk, demonstrates superior performance in generating coherent and high-quality video content in real-time, enhancing the interactive experience."
                },
                "zh": {
                    "title": "å®æ—¶å¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆçš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„è’¸é¦æ–¹æ³•ï¼Œç”¨äºå®æ—¶å¤šæ¨¡æ€è§†é¢‘ç”Ÿæˆï¼Œç»“åˆæ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šç§è¾“å…¥ã€‚é€šè¿‡ä¼˜åŒ–è°ƒåº¦å’Œæé«˜æ¡ä»¶è¾“å…¥çš„è´¨é‡ï¼Œå‡å°‘äº†æ¨ç†å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„ç”Ÿæˆæ•ˆæœä¸ä¼ ç»Ÿçš„åŒå‘æ¨¡å‹ç›¸å½“ï¼Œä½†æ¨ç†æˆæœ¬é™ä½äº†20å€ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºLiveTalkçš„å®æ—¶å¤šæ¨¡æ€äº’åŠ¨ç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº†äººæœºäº¤äº’çš„æµç•…æ€§å’Œè§†é¢‘å†…å®¹çš„è¿è´¯æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22096",
            "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model",
            "url": "https://huggingface.co/papers/2512.22096",
            "abstract": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.",
            "score": 47,
            "issue_id": 310,
            "pub_date": "2025-12-26",
            "pub_date_card": {
                "ru": "26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 26",
                "zh": "12æœˆ26æ—¥"
            },
            "hash": "bea639b4ce2ed908",
            "authors": [
                "Xiaofeng Mao",
                "Zhen Li",
                "Chuanhao Li",
                "Xiaojie Xu",
                "Kaining Ying",
                "Tong He",
                "Jiangmiao Pang",
                "Yu Qiao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22096.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#long_context",
                    "#diffusion"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ»Ğ°Ğ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñ‹, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Interactive World Generation with Diffusion Models",
                    "desc": "This paper introduces a new framework called \\method that enhances the generation of interactive worlds using diffusion models. It tackles significant issues like large model sizes and slow inference times, which hinder real-time applications. The framework includes innovative techniques such as context compression and bidirectional attention distillation to improve performance. Additionally, it allows users to control world events through text prompts, making the generated environments more dynamic and engaging."
                },
                "zh": {
                    "title": "ç”Ÿæˆäº’åŠ¨ä¸–ç•Œçš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶\textit{method}ï¼Œæ—¨åœ¨ä»å•å¼ å›¾åƒæˆ–æ–‡æœ¬æç¤ºç”Ÿæˆé€¼çœŸã€äº’åŠ¨å’Œè¿ç»­çš„ä¸–ç•Œã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨å‚æ•°è§„æ¨¡ã€æ¨ç†æ­¥éª¤å’Œå†å²ä¸Šä¸‹æ–‡ç­‰æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡äº†å®æ—¶æ€§èƒ½å¹¶æ”¯æŒæ–‡æœ¬æ§åˆ¶ç”Ÿæˆã€‚æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé•¿è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€å®æ—¶æµåŠ é€Ÿç­–ç•¥å’Œæ–‡æœ¬æ§åˆ¶çš„ä¸–ç•Œäº‹ä»¶ç”Ÿæˆæ–¹æ³•ã€‚é€šè¿‡è¿™äº›åˆ›æ–°ï¼Œ\textit{method}èƒ½å¤Ÿå®ç°é”®ç›˜åŸºç¡€çš„ä¸–ç•Œæ¢ç´¢ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22322",
            "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
            "url": "https://huggingface.co/papers/2512.22322",
            "abstract": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.",
            "score": 31,
            "issue_id": 310,
            "pub_date": "2025-12-26",
            "pub_date_card": {
                "ru": "26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 26",
                "zh": "12æœˆ26æ—¥"
            },
            "hash": "d2216b9110bb4404",
            "authors": [
                "Shaofei Cai",
                "Yulei Qin",
                "Haojia Lin",
                "Zihan Xu",
                "Gang Li",
                "Yuchen Shi",
                "Zongyi Li",
                "Yong Mao",
                "Siqi Cai",
                "Xiaoyu Tan",
                "Yitao Liang",
                "Ke Li",
                "Xing Sun"
            ],
            "affiliations": [
                "Peking University",
                "Tencent",
                "Youtu-Agent Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22322.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ“¸",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚ ÑĞ°Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ¹ ÑƒÑĞ¿ĞµÑ…",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ SmartSnap Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ğ½Ğ¾ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° ÑĞ²Ğ¾ĞµĞ³Ğ¾ ÑƒÑĞ¿ĞµÑ…Ğ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²ÑĞµĞ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾ÑĞ»Ğµ ĞµĞ³Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ·Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² ÑĞºÑ€Ğ°Ğ½Ğ°, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹, Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM-as-a-Judge. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 26% Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ´ĞµĞ»Ğ°Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Agents with SmartSnap: Proactive Self-Verification for Enhanced Performance",
                    "desc": "This paper introduces SmartSnap, a new approach in agentic reinforcement learning that enhances the scalability of autonomous agents in complex GUI tasks. Instead of relying on traditional post-hoc verification methods, SmartSnap empowers agents to perform proactive self-verification during task execution. The Self-Verifying Agent captures essential snapshots as evidence of task completion, guided by the 3C Principles: Completeness, Conciseness, and Creativity. Experimental results show that this method significantly improves performance in training LLM-driven agents, achieving notable gains in efficiency and reliability."
                },
                "zh": {
                    "title": "æ™ºèƒ½å¿«ç…§ï¼šä»£ç†è‡ªæˆ‘éªŒè¯çš„æ–°èŒƒå¼",
                    "desc": "ä»£ç†å¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ä»»åŠ¡ä¸­å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ï¼Œä½†å…¶å¯æ‰©å±•æ€§å—åˆ°ä»»åŠ¡å®ŒæˆéªŒè¯çš„ä¸¥é‡é™åˆ¶ã€‚ç°æœ‰çš„ä»»åŠ¡éªŒè¯é€šå¸¸æ˜¯è¢«åŠ¨çš„ï¼Œåˆ†æä»£ç†çš„æ•´ä¸ªäº¤äº’è½¨è¿¹æ¥åˆ¤æ–­å…¶æ˜¯å¦æˆåŠŸï¼Œè¿™ç§æ–¹æ³•å¤„ç†å†—é•¿ä¸”åŒ…å«æ— å…³ä¿¡æ¯çš„å†å²è®°å½•ï¼Œå¯¼è‡´éªŒè¯æˆæœ¬é«˜ä¸”å¯é æ€§ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†SmartSnapï¼Œè¿™æ˜¯ä¸€ç§ä¸»åŠ¨çš„è‡ªæˆ‘éªŒè¯æ–¹æ³•ï¼Œä»£ç†ä¸ä»…å®Œæˆä»»åŠ¡ï¼Œè¿˜èƒ½é€šè¿‡ç²¾å¿ƒç­–åˆ’çš„å¿«ç…§è¯æ®æ¥è¯æ˜å…¶å®Œæˆæƒ…å†µã€‚é€šè¿‡æˆ‘ä»¬çš„3CåŸåˆ™ï¼ˆå®Œæ•´æ€§ã€ç®€æ´æ€§å’Œåˆ›é€ æ€§ï¼‰ï¼Œä»£ç†èƒ½å¤Ÿåœ¨åœ¨çº¿ç¯å¢ƒä¸­è¿›è¡Œè‡ªæˆ‘éªŒè¯ï¼Œä»è€Œæé«˜äº†è®­ç»ƒçš„å¯æ‰©å±•æ€§å’Œæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23705",
            "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
            "url": "https://huggingface.co/papers/2512.23705",
            "abstract": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.",
            "score": 30,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "acdde1b848da34eb",
            "authors": [
                "Shaocong Xu",
                "Songlin Wei",
                "Qizhe Wei",
                "Zheng Geng",
                "Hong Li",
                "Licheng Shen",
                "Qianpu Sun",
                "Shu Han",
                "Bin Ma",
                "Bohan Li",
                "Chongjie Ye",
                "Yuhang Zheng",
                "Nan Wang",
                "Saining Zhang",
                "Hao Zhao"
            ],
            "affiliations": [
                "Beihang University",
                "Beijing Academy of Artificial Intelligence",
                "European Institute of Innovation and Technology Ningbo",
                "FNii, The Chinese University of Hong Kong, Shenzhen",
                "National University of Singapore",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of Southern California",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23705.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#synthetic",
                    "#robotics",
                    "#diffusion",
                    "#cv",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ·Ğ½Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ ÑÑ‚ĞµĞºĞ»ÑĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ²",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ DKT â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ¶Ğµ Ğ½Ğ°ÑƒÑ‡Ğ¸Ğ»Ğ¸ÑÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ TransPhy3D â€” ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 11 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ¸Ğ´ĞµĞ¾sequences Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¾Ñ‚Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼ ray tracing. ĞœĞ¾Ğ´ĞµĞ»ÑŒ DKT Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞ³ĞºĞ¸Ñ… LoRA Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ RGB Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ DiT Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Harnessing Diffusion for Transparent Object Perception",
                    "desc": "This paper addresses the challenges of perceiving transparent objects in computer vision, where traditional methods struggle due to optical effects like refraction and reflection. The authors introduce TransPhy3D, a synthetic video dataset that includes 11,000 sequences of transparent and reflective scenes, created using advanced rendering techniques. They develop a model called DKT, which utilizes a large video diffusion model to predict depth and normals from video inputs, achieving state-of-the-art results in transparency-related benchmarks. The findings suggest that generative models can effectively learn and apply optical principles, enhancing perception systems for real-world applications involving complex materials."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹æŒæ¡é€æ˜æ€§",
                    "desc": "é€æ˜ç‰©ä½“å¯¹æ„ŸçŸ¥ç³»ç»Ÿæ¥è¯´ä¸€ç›´å¾ˆéš¾å¤„ç†ï¼Œå› ä¸ºæŠ˜å°„ã€åå°„å’Œé€å°„æ‰“ç ´äº†ç«‹ä½“è§†è§‰ã€æ—¶é—´é£è¡Œï¼ˆToFï¼‰å’Œå•ç›®æ·±åº¦ä¼°è®¡çš„å‡è®¾ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ç°ä»£è§†é¢‘æ‰©æ•£æ¨¡å‹å·²ç»èƒ½å¤Ÿåˆæˆä»¤äººä¿¡æœçš„é€æ˜ç°è±¡ï¼Œè¡¨æ˜å®ƒä»¬å†…åŒ–äº†å…‰å­¦è§„åˆ™ã€‚æˆ‘ä»¬æ„å»ºäº†TransPhy3Dï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆçš„è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å«11,000ä¸ªé€æ˜/åå°„åœºæ™¯çš„åºåˆ—ï¼Œä½¿ç”¨Blender/Cyclesæ¸²æŸ“ã€‚é€šè¿‡è½»é‡çº§çš„LoRAé€‚é…å™¨ï¼Œæˆ‘ä»¬ä»å¤§å‹è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­å­¦ä¹ è§†é¢‘åˆ°è§†é¢‘çš„æ·±åº¦ï¼ˆå’Œæ³•çº¿ï¼‰è½¬æ¢å™¨ï¼Œæœ€ç»ˆå®ç°äº†åœ¨é€æ˜æ€§ç›¸å…³çš„åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é›¶-shotçš„æœ€å…ˆè¿›æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22323",
            "title": "SpotEdit: Selective Region Editing in Diffusion Transformers",
            "url": "https://huggingface.co/papers/2512.22323",
            "abstract": "Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.",
            "score": 27,
            "issue_id": 312,
            "pub_date": "2025-12-26",
            "pub_date_card": {
                "ru": "26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 26",
                "zh": "12æœˆ26æ—¥"
            },
            "hash": "ea2b1f5275a5d4f3",
            "authors": [
                "Zhibin Qin",
                "Zhenxiong Tan",
                "Zeqing Wang",
                "Songhua Liu",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22323.jpg",
            "data": {
                "categories": [],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ’Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¼ĞµĞ½ÑÑ‚ÑŒ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ SpotEdit â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¼ĞµĞ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµÑ… Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ SpotSelector Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. SpotFusion Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Efficient Image Editing with Selective Region Updates",
                    "desc": "The paper introduces SpotEdit, a new framework for image editing that improves efficiency by focusing only on the areas that need changes. Instead of processing the entire image at every step, SpotEdit identifies stable regions that do not require editing and reuses their features. This selective approach minimizes redundant computations and helps maintain the quality of the unchanged parts of the image. By using a dynamic fusion mechanism, SpotEdit ensures that the edited areas blend well with the rest of the image, resulting in high-quality edits with less computational effort."
                },
                "zh": {
                    "title": "SpotEditï¼šé«˜æ•ˆç²¾å‡†çš„å›¾åƒç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "æ‰©æ•£å˜æ¢å™¨æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œé€šè¿‡ç¼–ç æ¡ä»¶å›¾åƒå¹¶å°†å…¶æ•´åˆåˆ°å˜æ¢å™¨å±‚ä¸­ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç¼–è¾‘åªæ¶‰åŠä¿®æ”¹å°åŒºåŸŸï¼Œè€Œå½“å‰æ–¹æ³•åœ¨æ¯ä¸ªæ—¶é—´æ­¥å‡åŒ€å¤„ç†å’Œå»å™ªæ‰€æœ‰æ ‡è®°ï¼Œå¯¼è‡´å†—ä½™è®¡ç®—å¹¶å¯èƒ½é™ä½æœªæ›´æ”¹åŒºåŸŸçš„è´¨é‡ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šåœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­æ˜¯å¦çœŸçš„æœ‰å¿…è¦é‡æ–°ç”Ÿæˆæ¯ä¸ªåŒºåŸŸï¼Ÿä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SpotEditï¼Œä¸€ä¸ªæ— è®­ç»ƒçš„æ‰©æ•£ç¼–è¾‘æ¡†æ¶ï¼Œä»…é€‰æ‹©æ€§åœ°æ›´æ–°ä¿®æ”¹çš„åŒºåŸŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22615",
            "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
            "url": "https://huggingface.co/papers/2512.22615",
            "abstract": "Diffusion-based vision-language models and action frameworks demonstrate superior performance in visual planning and robotic control tasks compared to autoregressive baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as Ï€_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.",
            "score": 23,
            "issue_id": 312,
            "pub_date": "2025-12-27",
            "pub_date_card": {
                "ru": "27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 27",
                "zh": "12æœˆ27æ—¥"
            },
            "hash": "ef912dcec2a4e386",
            "authors": [
                "Jiacheng Ye",
                "Shansan Gong",
                "Jiahui Gao",
                "Junming Fan",
                "Shuang Wu",
                "Wei Bi",
                "Haoli Bai",
                "Lifeng Shang",
                "Lingpeng Kong"
            ],
            "affiliations": [
                "Huawei Technologies",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22615.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#robotics",
                    "#architecture",
                    "#benchmark",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Dream-VL â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ€ĞµĞ´Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… VLM Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Dream-VL Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Dream-VLA â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Dream-VLA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LIBERO Ğ¸ SimplerEnv, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Visual Planning with Diffusion Models",
                    "desc": "This paper presents Dream-VL, a diffusion-based vision-language model (dVLM) that outperforms traditional autoregressive models in visual planning and robotic control tasks. The authors highlight the limitations of autoregressive models in handling complex tasks due to their sequential generation approach. Dream-VL leverages the bidirectional nature of diffusion models, allowing for faster convergence and improved performance in action-related tasks. Additionally, the paper introduces Dream-VLA, a model that integrates vision, language, and action, achieving state-of-the-art results on various benchmarks and demonstrating the advantages of diffusion-based architectures."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹å¼•é¢†è§†è§‰-è¯­è¨€ä¸åŠ¨ä½œçš„æ–°æ—¶ä»£",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è§†è§‰è§„åˆ’å’Œæœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬æå‡ºäº†Dream-VLï¼Œè¿™æ˜¯ä¸€ç§å¼€æ”¾çš„æ‰©æ•£åŸºç¡€VLMï¼Œè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ¨¡å‹ã€‚åŸºäºDream-VLï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†Dream-VLAï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›å¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºè‡ªå›å½’æ¨¡å‹ï¼Œæ¨åŠ¨äº†ç›¸å…³é¢†åŸŸçš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23709",
            "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion",
            "url": "https://huggingface.co/papers/2512.23709",
            "abstract": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/",
            "score": 22,
            "issue_id": 314,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "c1ef282065c67382",
            "authors": [
                "Hau-Shiang Shiu",
                "Chin-Yang Lin",
                "Zhixiang Wang",
                "Chi-Wei Hsiao",
                "Po-Fan Yu",
                "Yu-Chih Chen",
                "Yu-Lun Liu"
            ],
            "affiliations": [
                "MediaTek Inc.",
                "National Yang Ming Chiao Tung University",
                "Shanda AI Research Tokyo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23709.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞµĞº",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Stream-DiffVSR â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ‘Ñ…ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Auto-regressive Temporal Guidance, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Temporal Processor Module Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Stream-DiffVSR Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ 720p Ğ·Ğ° 0,328 ÑĞµĞºÑƒĞ½Ğ´Ñ‹ Ğ¸ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹, ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ² Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ 4600+ ÑĞµĞºÑƒĞ½Ğ´ Ğ´Ğ¾ 0,328 ÑĞµĞºÑƒĞ½Ğ´Ñ‹."
                },
                "en": {
                    "title": "Stream-DiffVSR: Fast and Efficient Video Super-Resolution for Real-Time Applications",
                    "desc": "The paper introduces Stream-DiffVSR, a novel approach to video super-resolution (VSR) that addresses the latency issues of existing diffusion-based methods. By using only past frames and a four-step distilled denoiser, it achieves fast inference times suitable for real-time applications. The Auto-regressive Temporal Guidance (ARTG) module enhances the quality of the output by incorporating motion-aligned information during the denoising process. Stream-DiffVSR significantly reduces latency while improving perceptual quality, making it the first diffusion VSR method viable for low-latency online use."
                },
                "zh": {
                    "title": "ä½å»¶è¿Ÿåœ¨çº¿è§†é¢‘è¶…åˆ†è¾¨ç‡çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºStream-DiffVSRçš„åŸºäºæ‰©æ•£çš„åœ¨çº¿è§†é¢‘è¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»…ä¾èµ–è¿‡å»çš„å¸§æ¥å®ç°é«˜æ•ˆçš„åœ¨çº¿è¶…åˆ†è¾¨ç‡ï¼Œé¿å…äº†å¯¹æœªæ¥å¸§çš„ä¾èµ–ã€‚Stream-DiffVSRç»“åˆäº†å¿«é€Ÿæ¨ç†çš„å››æ­¥æç‚¼å»å™ªå™¨å’Œè‡ªå›å½’æ—¶é—´å¼•å¯¼æ¨¡å—ï¼Œå¢å¼ºäº†ç»†èŠ‚å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨é™ä½å»¶è¿Ÿçš„åŒæ—¶æ˜¾è‘—æé«˜äº†æ„ŸçŸ¥è´¨é‡ï¼Œæˆä¸ºé¦–ä¸ªé€‚ç”¨äºä½å»¶è¿Ÿåœ¨çº¿éƒ¨ç½²çš„æ‰©æ•£è¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15560",
            "title": "GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models",
            "url": "https://huggingface.co/papers/2512.15560",
            "abstract": "The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Notably, under our experimental setup, compared with training a diffusion model from scratch, evaluating with TED-6K is about 750times faster. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our TED-6K dataset and evaluation code are available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.",
            "score": 20,
            "issue_id": 313,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "32c8b75c710f3c4e",
            "authors": [
                "Bozhou Li",
                "Sihan Yang",
                "Yushuo Guan",
                "Ruichuan An",
                "Xinlong Chen",
                "Yang Shi",
                "Pengfei Wan",
                "Wentao Zhang",
                "Yuanxing zhang"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "Peking University",
                "School of Artificial Intelligence, UCAS",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15560.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#diffusion",
                    "#video",
                    "#multimodal",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ GRAN-TED â€” Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ TED-6K â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ±ĞµĞ· Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ² 750 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ). ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° fine-tuning Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ GRAN-TED ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ TED-6K Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Text Encoders for Superior Visual Generation",
                    "desc": "This paper presents GRAN-TED, a new approach to improve text encoders used in text-to-image and text-to-video diffusion models. It addresses two main challenges: the need for a reliable evaluation framework and the adaptation of pretrained language models for visual tasks. The authors introduce TED-6K, a benchmark that allows for efficient assessment of text encoders without extensive training, showing a strong correlation with downstream performance. Additionally, they propose a two-stage training method for the text encoder that enhances its ability to generate high-quality visual content."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬ç”Ÿæˆè´¨é‡çš„åˆ›æ–°ç¼–ç å™¨",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬ç¼–ç å™¨GRAN-TEDï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†TED-6Kï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ–‡æœ¬åŸºå‡†ï¼Œå¯ä»¥é«˜æ•ˆè¯„ä¼°ç¼–ç å™¨çš„è¡¨ç°ï¼Œè€Œæ— éœ€æ˜‚è´µçš„ç«¯åˆ°ç«¯æ¨¡å‹è®­ç»ƒã€‚é€šè¿‡è½»é‡çº§çš„ç»Ÿä¸€é€‚é…å™¨ï¼Œæˆ‘ä»¬å‘ç°TED-6Kçš„è¡¨ç°ä¸ç¼–ç å™¨åœ¨ä¸‹æ¸¸ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§é«˜åº¦ç›¸å…³ã€‚æœ€ç»ˆï¼ŒGRAN-TEDç¼–ç å™¨åœ¨TED-6Kä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23541",
            "title": "Act2Goal: From World Model To General Goal-conditioned Policy",
            "url": "https://huggingface.co/papers/2512.23541",
            "abstract": "Act2Goal employs a goal-conditioned visual world model with multi-scale temporal control and cross-attention to achieve robust long-horizon robotic manipulation through structured planning and adaptive execution.  \t\t\t\t\tAI-generated summary \t\t\t\t Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/",
            "score": 18,
            "issue_id": 314,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "5e5da1e271fb6c88",
            "authors": [
                "Pengfei Zhou",
                "Liliang Chen",
                "Shengcong Chen",
                "Di Chen",
                "Wenzhi Zhao",
                "Rongjun Jin",
                "Guanghui Ren",
                "Jianlan Luo"
            ],
            "affiliations": [
                "Agibot Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23541.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#robotics",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°",
                    "desc": "Act2Goal Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ†ĞµĞ»ÑÑ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Multi-Scale Temporal Hashing (MSTH), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµĞ¼ÑƒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ğµ Ğ´Ğ°Ğ»ÑŒĞ½Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ cross-attention Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ¸Ğ¼Ñ‡Ğ¸Ğ²Ñ‹Ğ¼ Ğº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ zero-shot Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ€ĞµĞ»Ğ°Ğ±ĞµĞ»Ğ¸Ğ½Ğ³Ñƒ Ñ†ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· hindsight Ğ¸ LoRA-based finetuning, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ 30% Ğ´Ğ¾ 90%."
                },
                "en": {
                    "title": "Empowering Robots with Goal-Conditioned Visual Planning for Long-Horizon Tasks",
                    "desc": "Act2Goal is a novel approach for robotic manipulation that uses a goal-conditioned visual world model to enhance long-horizon task execution. It addresses the challenge of specifying tasks clearly by generating a sequence of intermediate visual states that guide the robot's actions over time. The method employs Multi-Scale Temporal Hashing (MSTH) to break down the planned trajectory into both detailed and broader frames, allowing for precise control while maintaining overall task coherence. This approach not only improves the robot's ability to adapt to new environments but also significantly increases its success rates in complex tasks without needing external supervision."
                },
                "zh": {
                    "title": "ç›®æ ‡å¯¼å‘çš„æœºå™¨äººæ“ä½œæ–°çªç ´",
                    "desc": "Act2Goal æ˜¯ä¸€ç§ç›®æ ‡æ¡ä»¶çš„è§†è§‰ä¸–ç•Œæ¨¡å‹ï¼Œç»“åˆäº†å¤šå°ºåº¦æ—¶é—´æ§åˆ¶å’Œäº¤å‰æ³¨æ„åŠ›ï¼Œæ—¨åœ¨å®ç°ç¨³å¥çš„é•¿æ—¶é—´æœºå™¨äººæ“ä½œã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆä¸­é—´è§†è§‰çŠ¶æ€åºåˆ—ï¼Œæ•æ‰é•¿æ—¶é—´ç»“æ„ï¼Œä»è€Œå…‹æœäº†ç°æœ‰ç­–ç•¥åœ¨é•¿æ—¶é—´æ“ä½œä¸­çš„å±€é™æ€§ã€‚é€šè¿‡å¤šå°ºåº¦æ—¶é—´å“ˆå¸Œï¼ˆMSTHï¼‰ï¼Œå°†æƒ³è±¡çš„è½¨è¿¹åˆ†è§£ä¸ºå¯†é›†çš„è¿‘ç«¯å¸§å’Œç¨€ç–çš„è¿œç«¯å¸§ï¼Œå®ç°ç²¾ç»†çš„é—­ç¯æ§åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒAct2Goal åœ¨å¤æ‚ä»»åŠ¡ä¸­æˆåŠŸç‡ä» 30% æé«˜åˆ° 90%ï¼ŒéªŒè¯äº†å…¶åœ¨é•¿æ—¶é—´æ“ä½œä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23676",
            "title": "Web World Models",
            "url": "https://huggingface.co/papers/2512.23676",
            "abstract": "Web World Models (WWMs) combine web frameworks with large language models to create controllable, open-ended persistent environments by structuring world state in web code and leveraging model-driven imagination for narratives and decisions.  \t\t\t\t\tAI-generated summary \t\t\t\t Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.",
            "score": 13,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "05bf3b1ace6e32c3",
            "authors": [
                "Jichen Feng",
                "Yifan Zhang",
                "Chenggong Zhang",
                "Yifu Lu",
                "Shilong Liu",
                "Mengdi Wang"
            ],
            "affiliations": [
                "Princeton University",
                "University of California, Los Angeles",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23676.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#agents",
                    "#games",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ’ĞµĞ±-ĞºĞ¾Ğ´ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ: Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Web World Model (WWM) â€” Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ²ĞµĞ±-Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ ĞµĞ³Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ²ĞµĞ±-ĞºĞ¾Ğ´Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ñ‹ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ… ÑÑ‚Ğ¾Ğ³Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ WWM Ğ½Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ Ğ²ĞµĞ±-ÑÑ‚ĞµĞºĞµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ñ‚Ğ»Ğ°Ñ, Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ĞºĞ°Ğº Ñ‚Ğ¸Ğ¿Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞ±-ÑÑ‚ĞµĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ…, Ğ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Creating Dynamic Worlds with Web World Models",
                    "desc": "Web World Models (WWMs) merge web frameworks with large language models to create dynamic and controllable environments. This approach balances the reliability of traditional web frameworks with the creativity of generative models, allowing for structured yet imaginative interactions. By implementing world states in web code, WWMs ensure logical consistency while enabling language models to generate narratives and make decisions. The study presents various applications of WWMs, highlighting design principles that facilitate scalable and open-ended exploration in AI-generated worlds."
                },
                "zh": {
                    "title": "Webä¸–ç•Œæ¨¡å‹ï¼šå¯æ§çš„å¼€æ”¾å¼ç¯å¢ƒ",
                    "desc": "Webä¸–ç•Œæ¨¡å‹ï¼ˆWWMï¼‰ç»“åˆäº†ç½‘ç»œæ¡†æ¶å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåˆ›å»ºå¯æ§çš„ã€å¼€æ”¾å¼çš„æŒä¹…ç¯å¢ƒã€‚å®ƒé€šè¿‡åœ¨ç½‘ç»œä»£ç ä¸­æ„å»ºä¸–ç•ŒçŠ¶æ€ï¼Œå¹¶åˆ©ç”¨æ¨¡å‹é©±åŠ¨çš„æƒ³è±¡åŠ›æ¥ç”Ÿæˆå™äº‹å’Œå†³ç­–ã€‚WWMåœ¨é€»è¾‘ä¸€è‡´æ€§å’Œç”Ÿæˆèƒ½åŠ›ä¹‹é—´æ‰¾åˆ°äº†å¹³è¡¡ï¼Œç¡®ä¿äº†ç¯å¢ƒçš„å¯æ§æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç½‘ç»œå †æ ˆå¯ä»¥ä½œä¸ºä¸–ç•Œæ¨¡å‹çš„å¯æ‰©å±•åŸºç¡€ï¼Œæ”¯æŒæ— é™ä½†ç»“æ„åŒ–çš„æ¢ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22234",
            "title": "DiRL: An Efficient Post-Training Framework for Diffusion Language Models",
            "url": "https://huggingface.co/papers/2512.22234",
            "abstract": "Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.",
            "score": 13,
            "issue_id": 312,
            "pub_date": "2025-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "faad4053471995e5",
            "authors": [
                "Ying Zhu",
                "Jiaxin Wan",
                "Xiaoran Liu",
                "Siyanag He",
                "Qiqi Wang",
                "Xu Guo",
                "Tianyi Liang",
                "Zengfeng Huang",
                "Ziwei He",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "OpenMoss Team",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22234.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#inference",
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#math",
                    "#diffusion",
                    "#reasoning"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° DiRL â€” ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ DiPO â€” Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (GRPO) Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ DiRL-8B-Instruct Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑÑ€ĞµĞ´Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing dLLMs: Efficient Post-Training for Superior Math Performance",
                    "desc": "This paper introduces DiRL, a new framework for improving the performance of Diffusion Language Models (dLLMs) after their initial training. It combines advanced training techniques with optimized inference methods to enhance efficiency and effectiveness, particularly in complex reasoning tasks like mathematics. The authors also present DiPO, a novel implementation of Group Relative Policy Optimization specifically designed for dLLMs, which helps in refining the model's capabilities. Their experiments show that the DiRL-8B-Instruct model outperforms existing dLLMs in math tasks, setting a new standard in this area."
                },
                "zh": {
                    "title": "é«˜æ•ˆåæœŸè®­ç»ƒï¼Œæå‡æ‰©æ•£è¯­è¨€æ¨¡å‹è¡¨ç°",
                    "desc": "æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºè‡ªå›å½’æ¨¡å‹çš„æœ‰åŠ›æ›¿ä»£å“ï¼Œå±•ç°äº†è‰¯å¥½çš„é¢„è®­ç»ƒæ½œåŠ›å’ŒåŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚ç„¶è€Œï¼ŒdLLMsçš„åæœŸè®­ç»ƒæ–¹æ³•ä»ç„¶ä¸å¤Ÿæˆç†Ÿï¼Œç°æœ‰æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œè®­ç»ƒä¸æ¨ç†ç›®æ ‡ä¸åŒ¹é…æ–¹é¢å­˜åœ¨é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦ï¼‰ä¸Šçš„è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiRLï¼Œä¸€ä¸ªé«˜æ•ˆçš„åæœŸè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆäº†FlexAttentionåŠ é€Ÿçš„åˆ†å—è®­ç»ƒå’ŒLMDeployä¼˜åŒ–çš„æ¨ç†ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†DiPOï¼Œè¿™æ˜¯é’ˆå¯¹dLLMsçš„é¦–ä¸ªæ— åç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å®ç°ï¼Œç»è¿‡éªŒè¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é«˜è´¨é‡æ•°å­¦æ•°æ®ä¸Šè¾¾åˆ°äº†dLLMsä¸­çš„æœ€å…ˆè¿›è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23707",
            "title": "Training AI Co-Scientists Using Rubric Rewards",
            "url": "https://huggingface.co/papers/2512.23707",
            "abstract": "AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.",
            "score": 10,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "0f116c273b415185",
            "authors": [
                "Shashwat Goel",
                "Rishi Hazra",
                "Dulhan Jayalath",
                "Timon Willi",
                "Parag Jain",
                "William F. Shen",
                "Ilias Leontiadis",
                "Francesco Barbieri",
                "Yoram Bachrach",
                "Jonas Geiping",
                "Chenxi Whitehouse"
            ],
            "affiliations": [
                "ELLIS Institute TÃ¼bingen",
                "Max Planck Institute for Intelligent Systems",
                "Meta Superintelligence Labs",
                "University of Cambridge",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23707.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#rl",
                    "#science",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ĞºĞ¾Ğ¿Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‰Ğ¸Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen3-30B-A3B Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ĞµĞµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ² 70% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸."
                },
                "en": {
                    "title": "Empowering Research with AI: Smarter Plans for Better Science",
                    "desc": "This paper explores the development of AI co-scientists that assist researchers by generating research plans based on specified aims and constraints. The authors address the limitations of current language models in producing plans that adhere to all requirements by leveraging a large dataset of existing research papers. They employ reinforcement learning with self-grading to train models, using extracted grading rubrics to enhance the generation process without needing human supervision. The results show significant improvements in plan quality across various domains, indicating the potential for AI to effectively support research endeavors."
                },
                "zh": {
                    "title": "æå‡AIååŠ©ç§‘å­¦ç ”ç©¶çš„èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ç°æœ‰ç ”ç©¶è®ºæ–‡çš„ä¸°å¯Œè¯­æ–™åº“æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥ç”Ÿæˆæ›´å¥½çš„ç ”ç©¶è®¡åˆ’ã€‚ç ”ç©¶è€…ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡è‡ªæˆ‘è¯„åˆ†çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒç ”ç©¶è®¡åˆ’ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è‡ªåŠ¨æå–çš„ç ”ç©¶ç›®æ ‡å’Œè¯„åˆ†æ ‡å‡†æ¥æ„å»ºå¤šæ ·åŒ–çš„è®­ç»ƒè¯­æ–™åº“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨ç”Ÿæˆç ”ç©¶è®¡åˆ’æ–¹é¢ä¼˜äºåˆå§‹æ¨¡å‹ï¼Œä¸”åœ¨åŒ»å­¦ç ”ç©¶ç­‰é¢†åŸŸä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºæå‡AIååŠ©ç§‘å­¦ç ”ç©¶çš„èƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23044",
            "title": "Video-BrowseComp: Benchmarking Agentic Video Research on Open Web",
            "url": "https://huggingface.co/papers/2512.23044",
            "abstract": "The paper addresses the modality gap in autonomous agents for video processing by introducing a benchmark requiring proactive, open-web video reasoning, revealing limitations of current models in metadata-sparse, dynamic video domains.  \t\t\t\t\tAI-generated summary \t\t\t\t The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web's most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present Video-BrowseComp, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning.",
            "score": 9,
            "issue_id": 310,
            "pub_date": "2025-12-28",
            "pub_date_card": {
                "ru": "28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 28",
                "zh": "12æœˆ28æ—¥"
            },
            "hash": "886b1afc4bd42000",
            "authors": [
                "Zhengyang Liang",
                "Yan Shu",
                "Xiangrui Liu",
                "Minghao Qin",
                "Kaixin Liang",
                "Paolo Rota",
                "Nicu Sebe",
                "Zheng Liu",
                "Lizi Liao"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Beijing University of Posts and Telecommunications",
                "Hong Kong Polytechnic University",
                "Singapore Management University",
                "University of Trento"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23044.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚Ğ¸: Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Video-BrowseComp Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ½Ğ¾ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑĞºĞ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑˆĞºĞ°Ğ»Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ LLM Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 15% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ñ‚ĞµÑ€ÑÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ğ±ĞµĞ· Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ĞµĞ±-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Bridging the Modality Gap in Video Reasoning",
                    "desc": "This paper introduces Video-BrowseComp, a new benchmark designed to address the challenges faced by autonomous agents in processing dynamic video content. It highlights the limitations of current models that primarily rely on textual information and curated video clips, which do not require active engagement with video timelines. The benchmark consists of 210 questions that necessitate proactive video reasoning, compelling models to verify claims using temporal visual evidence rather than just text. The findings reveal that even advanced models struggle significantly in metadata-sparse environments, underscoring the need for improved visual grounding in video processing tasks."
                },
                "zh": {
                    "title": "å¡«è¡¥è‡ªä¸»ä»£ç†è§†é¢‘å¤„ç†çš„æ¨¡æ€å·®è·",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è‡ªä¸»ä»£ç†åœ¨è§†é¢‘å¤„ç†ä¸­çš„æ¨¡æ€å·®è·ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œè¦æ±‚ä¸»åŠ¨è¿›è¡Œå¼€æ”¾ç½‘ç»œè§†é¢‘æ¨ç†ã€‚å½“å‰æ¨¡å‹åœ¨ç¼ºä¹å…ƒæ•°æ®å’ŒåŠ¨æ€è§†é¢‘é¢†åŸŸä¸­å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦é›†ä¸­äºè¢«åŠ¨æ„ŸçŸ¥ã€‚ç°æœ‰çš„è§†é¢‘åŸºå‡†æµ‹è¯•é€šå¸¸åªæä¾›ç»è¿‡ç­›é€‰çš„ç‰‡æ®µï¼Œæœªèƒ½è¯„ä¼°ä»£ç†åœ¨è§†é¢‘æ—¶é—´çº¿ä¸Šçš„ä¸»åŠ¨ç ”ç©¶èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥Video-BrowseCompåŸºå‡†ï¼Œè®ºæ–‡å¼ºè°ƒäº†åœ¨è§†é¢‘æ¨ç†ä¸­ä¾èµ–æ—¶é—´è§†è§‰è¯æ®çš„é‡è¦æ€§ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸä»è¢«åŠ¨æ„ŸçŸ¥å‘ä¸»åŠ¨è§†é¢‘æ¨ç†çš„è½¬å˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22342",
            "title": "VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs",
            "url": "https://huggingface.co/papers/2512.22342",
            "abstract": "A new benchmark for dialog-enabled navigation tasks introduces interactive learning to resolve ambiguous instructions through active dialog, enhancing real-world applicability of embodied agents.  \t\t\t\t\tAI-generated summary \t\t\t\t In most existing embodied navigation tasks, instructions are well-defined and unambiguous, such as instruction following and object searching. Under this idealized setting, agents are required solely to produce effective navigation outputs conditioned on vision and language inputs. However, real-world navigation instructions are often vague and ambiguous, requiring the agent to resolve uncertainty and infer user intent through active dialog. To address this gap, we propose Interactive Instance Object Navigation (IION), a task that requires agents not only to generate navigation actions but also to produce language outputs via active dialog, thereby aligning more closely with practical settings. IION extends Instance Object Navigation (ION) by allowing agents to freely consult an oracle in natural language while navigating. Building on this task, we present the Vision Language-Language Navigation (VL-LN) benchmark, which provides a large-scale, automatically generated dataset and a comprehensive evaluation protocol for training and assessing dialog-enabled navigation models. VL-LN comprises over 41k long-horizon dialog-augmented trajectories for training and an automatic evaluation protocol with an oracle capable of responding to agent queries. Using this benchmark, we train a navigation model equipped with dialog capabilities and show that it achieves significant improvements over the baselines. Extensive experiments and analyses further demonstrate the effectiveness and reliability of VL-LN for advancing research on dialog-enabled embodied navigation. Code and dataset: https://0309hws.github.io/VL-LN.github.io/",
            "score": 8,
            "issue_id": 316,
            "pub_date": "2025-12-26",
            "pub_date_card": {
                "ru": "26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 26",
                "zh": "12æœˆ26æ—¥"
            },
            "hash": "51d8cfcde9b02ec5",
            "authors": [
                "Wensi Huang",
                "Shaohao Zhu",
                "Meng Wei",
                "Jinming Xu",
                "Xihui Liu",
                "Hanqing Wang",
                "Tai Wang",
                "Feng Zhao",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "SenseTime Research",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22342.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#robotics",
                    "#dataset",
                    "#agents"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Interactive Instance Object Navigation (IION), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ VL-LN Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 41 Ñ‚Ñ‹ÑÑÑ‡ĞµĞ¹ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ¼ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ¾Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ Ğ¾Ñ€Ğ°ĞºÑƒĞ»Ğ¾Ğ¼ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³-ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°."
                },
                "en": {
                    "title": "Navigating Ambiguity: Dialog-Enabled Navigation for Real-World Tasks",
                    "desc": "This paper introduces a new benchmark called Vision Language-Language Navigation (VL-LN) for dialog-enabled navigation tasks, which allows agents to handle ambiguous instructions through active dialog. Unlike traditional navigation tasks that rely on clear instructions, this approach requires agents to engage in conversations to clarify user intent while navigating. The proposed Interactive Instance Object Navigation (IION) task enhances the existing Instance Object Navigation (ION) by enabling agents to ask questions in natural language during their navigation process. The authors demonstrate that using this benchmark leads to significant improvements in the performance of navigation models equipped with dialog capabilities, showcasing its potential for real-world applications."
                },
                "zh": {
                    "title": "æå‡å¯¼èˆªä»»åŠ¡çš„å¯¹è¯èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³æ¨¡ç³ŠæŒ‡ä»¤çš„å¯¹è¯å¯¼èˆªä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥äº’åŠ¨å­¦ä¹ ï¼Œä»£ç†ä¸ä»…éœ€è¦ç”Ÿæˆå¯¼èˆªåŠ¨ä½œï¼Œè¿˜éœ€è¦é€šè¿‡ä¸»åŠ¨å¯¹è¯æ¥ç†è§£ç”¨æˆ·æ„å›¾ã€‚è¯¥ä»»åŠ¡ç§°ä¸ºäº’åŠ¨å®ä¾‹å¯¹è±¡å¯¼èˆªï¼ˆIIONï¼‰ï¼Œå®ƒå…è®¸ä»£ç†åœ¨å¯¼èˆªè¿‡ç¨‹ä¸­ä½¿ç”¨è‡ªç„¶è¯­è¨€å’¨è¯¢ä¿¡æ¯æºã€‚ç ”ç©¶è¿˜æ¨å‡ºäº†è§†è§‰è¯­è¨€-è¯­è¨€å¯¼èˆªï¼ˆVL-LNï¼‰åŸºå‡†ï¼Œæä¾›äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†å’Œè¯„ä¼°åè®®ï¼Œä»¥è®­ç»ƒå’Œè¯„ä¼°æ”¯æŒå¯¹è¯çš„å¯¼èˆªæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23646",
            "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding",
            "url": "https://huggingface.co/papers/2512.23646",
            "abstract": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.",
            "score": 7,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "f644f8f445308f7e",
            "authors": [
                "Keda Tao",
                "Wenjie Du",
                "Bohan Yu",
                "Weiqiang Wang",
                "Jian Liu",
                "Huan Wang"
            ],
            "affiliations": [
                "Ant Group",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23646.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#audio",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ‘‚",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑˆĞ°Ğ½Ğ¸Ñ Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ°ÑƒĞ´Ğ¸Ğ¾ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ OmniAgent â€” Ğ°Ğ³ĞµĞ½Ñ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ¼Ğ½Ğ¸modĞ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ Ğ¾Ñ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ â€” Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ³Ñ€ÑƒĞ±Ğ¾-Ğº-Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 10-20% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Audio-Visual Understanding with Active Perception",
                    "desc": "OmniAgent is a new type of active perception agent that improves how machines understand both audio and visual information together. It addresses the common issues of previous models that struggle with detailed cross-modal understanding and alignment. By using a dynamic planning approach, OmniAgent can actively decide which tools to use based on the task at hand, rather than following a fixed process. This innovative method allows it to focus on important audio cues to enhance its reasoning about events in videos, leading to significantly better performance on various benchmarks."
                },
                "zh": {
                    "title": "ä¸»åŠ¨æ„ŸçŸ¥ï¼ŒéŸ³é¢‘å¼•å¯¼çš„å¤šæ¨¡æ€æ¨ç†æ–°çºªå…ƒ",
                    "desc": "OmniAgentæ˜¯ä¸€ç§å…¨éŸ³é¢‘å¼•å¯¼çš„ä¸»åŠ¨æ„ŸçŸ¥ä»£ç†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹åœ¨éŸ³é¢‘å’Œè§†è§‰ç†è§£ä¸Šçš„ä¸è¶³ã€‚å®ƒé€šè¿‡åŠ¨æ€è§„åˆ’è‡ªä¸»åè°ƒå·¥å…·çš„è°ƒç”¨ï¼Œèƒ½å¤Ÿæ›´ç²¾ç»†åœ°è¿›è¡ŒéŸ³é¢‘-è§†è§‰æ¨ç†ã€‚ä¸ä»¥å¾€ä¾èµ–é™æ€å·¥ä½œæµç¨‹çš„æ¨¡å‹ä¸åŒï¼ŒOmniAgentå¼ºè°ƒä¸»åŠ¨å¤šæ¨¡æ€æ¢è¯¢ï¼Œé›†ä¸­æ³¨æ„åŠ›äºä¸ä»»åŠ¡ç›¸å…³çš„çº¿ç´¢ã€‚é€šè¿‡æ–°é¢–çš„ç²—åˆ°ç»†çš„éŸ³é¢‘å¼•å¯¼æ„ŸçŸ¥èŒƒå¼ï¼ŒOmniAgentåœ¨éŸ³é¢‘è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡è¶…è¶Šäº†é¢†å…ˆçš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23273",
            "title": "YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection",
            "url": "https://huggingface.co/papers/2512.23273",
            "abstract": "Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.",
            "score": 7,
            "issue_id": 317,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "7f5fccb5cfb9cbe0",
            "authors": [
                "Xu Lin",
                "Jinlong Peng",
                "Zhenye Gan",
                "Jiawen Zhu",
                "Jun Liu"
            ],
            "affiliations": [
                "Singapore Management University",
                "Tencent Youtu Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23273.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° YOLO-Master, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ² Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ±Ğ»Ğ¾ĞºÑƒ Efficient Sparse Mixture-of-Experts. Ğ›ĞµĞ³ĞºĞ°Ñ ÑĞµÑ‚ÑŒ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑÑ Ğ¸Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ YOLO-Master Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Dynamic Resource Allocation for Enhanced Object Detection",
                    "desc": "The paper introduces YOLO-Master, a new framework for real-time object detection that enhances the traditional YOLO architecture. It addresses the inefficiencies of static dense computation by implementing instance-conditional adaptive computation, which allocates resources based on the complexity of each scene. This is achieved through an Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically selects the most relevant computational experts for each input. The results show that YOLO-Master significantly improves detection performance and speed, particularly in complex scenes, while maintaining real-time processing capabilities."
                },
                "zh": {
                    "title": "YOLO-Masterï¼šåŠ¨æ€åˆ†é…è®¡ç®—èµ„æºçš„å®æ—¶ç‰©ä½“æ£€æµ‹æ–°æ¡†æ¶",
                    "desc": "ç°æœ‰çš„å®æ—¶ç‰©ä½“æ£€æµ‹æ–¹æ³•é€šå¸¸é‡‡ç”¨ç±»ä¼¼YOLOçš„æ¶æ„ï¼Œä»¥å®ç°å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¾èµ–äºé™æ€çš„å¯†é›†è®¡ç®—ï¼Œå¯¹æ‰€æœ‰è¾“å…¥è¿›è¡Œç»Ÿä¸€å¤„ç†ï¼Œå¯¼è‡´è®¡ç®—èµ„æºçš„æµªè´¹å’Œæ£€æµ‹æ€§èƒ½çš„ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†YOLO-Masterï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„YOLOç±»æ¡†æ¶ï¼Œé‡‡ç”¨å®ä¾‹æ¡ä»¶è‡ªé€‚åº”è®¡ç®—ã€‚é€šè¿‡é«˜æ•ˆç¨€ç–ä¸“å®¶æ··åˆï¼ˆES-MoEï¼‰æ¨¡å—ï¼ŒYOLO-Masteræ ¹æ®åœºæ™¯å¤æ‚æ€§åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œä»è€Œæé«˜æ£€æµ‹æ€§èƒ½å¹¶å‡å°‘è®¡ç®—å¼€é”€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22431",
            "title": "Monadic Context Engineering",
            "url": "https://huggingface.co/papers/2512.22431",
            "abstract": "The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.",
            "score": 7,
            "issue_id": 310,
            "pub_date": "2025-12-27",
            "pub_date_card": {
                "ru": "27 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 27",
                "zh": "12æœˆ27æ—¥"
            },
            "hash": "472b5375fd965473",
            "authors": [
                "Yifan Zhang",
                "Mengdi Wang"
            ],
            "affiliations": [
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22431.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "ğŸ§±",
                "ru": {
                    "title": "ĞĞ»Ğ³ĞµĞ±Ñ€Ğ°Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Monadic Context Engineering (MCE) â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ»Ğ³ĞµĞ±Ñ€Ğ°Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ„ÑƒĞ½ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ°Ğ¿Ğ¿Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ½Ğ°Ğ´. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼Ğ¾Ğ½Ğ°Ğ´Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ğ°Ğ¿Ğ¿Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, Ğ° Ğ¼Ğ¾Ğ½Ğ°Ğ´Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ…, Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ Meta-Agents, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ MCE Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ğ°Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Empowering AI Agents with Monadic Context Engineering",
                    "desc": "This paper presents Monadic Context Engineering (MCE), a new way to design autonomous agents using concepts from functional programming. MCE utilizes algebraic structures like Functors and Monads to improve how agents handle tasks, manage state, and deal with errors. By treating agent workflows as computational contexts, MCE simplifies complex operations such as error handling and asynchronous execution. The framework also introduces Meta-Agents, which can dynamically create and manage workflows for sub-agents, enhancing the flexibility and robustness of AI systems."
                },
                "zh": {
                    "title": "å•å­ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼šæ„å»ºæ™ºèƒ½ä»£ç†çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¶æ„èŒƒå¼ï¼Œç§°ä¸ºå•å­ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼ˆMCEï¼‰ï¼Œæ—¨åœ¨æ”¹å–„å½“å‰è‡ªä¸»ä»£ç†çš„è®¾è®¡ã€‚MCEåˆ©ç”¨å‡½å­ã€åº”ç”¨å‡½å­å’Œå•å­çš„ä»£æ•°ç»“æ„ï¼Œä¸ºä»£ç†å·¥ä½œæµæä¾›äº†ä¸€ä¸ªå½¢å¼åŒ–çš„åŸºç¡€ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒçŠ¶æ€ç®¡ç†ã€é”™è¯¯å¤„ç†å’Œå¹¶å‘æ‰§è¡Œç­‰è·¨åˆ‡å…³æ³¨ç‚¹å¯ä»¥é€šè¿‡ä»£æ•°å±æ€§å†…åœ¨åœ°ç®¡ç†ã€‚æœ€ç»ˆï¼ŒMCEä½¿å¼€å‘è€…èƒ½å¤Ÿä»ç®€å•ã€å¯ç‹¬ç«‹éªŒè¯çš„ç»„ä»¶æ„å»ºå¤æ‚ã€ç¨³å¥ä¸”é«˜æ•ˆçš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23647",
            "title": "Nested Browser-Use Learning for Agentic Information Seeking",
            "url": "https://huggingface.co/papers/2512.23647",
            "abstract": "Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.",
            "score": 6,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "ff50e7cc84f888d8",
            "authors": [
                "Baixuan Li",
                "Jialong Wu",
                "Wenbiao Yin",
                "Kuan Li",
                "Zhongwang Zhang",
                "Huifeng Yin",
                "Zhengwei Tao",
                "Liwen Zhang",
                "Pengjun Xie",
                "Jingren Zhou",
                "Yong Jiang"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23647.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ’Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· API Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ NestBrowse â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ReAct-ÑÑ‚Ğ¸Ğ»Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ²ĞµĞ±Ğ°. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NestBrowse Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ°."
                },
                "en": {
                    "title": "Unlocking Deeper Web Insights with NestBrowse",
                    "desc": "This paper presents a new approach called Nested Browser-Use Learning (NestBrowse) for improving information-seeking agents. Traditional agents struggle with complex web interactions, limiting their ability to gather rich information. NestBrowse introduces a structured framework that separates control of browser actions from the exploration of web pages, making it easier for agents to reason and operate. The results show that this method significantly enhances the agents' performance in deep web searches, proving to be both efficient and flexible."
                },
                "zh": {
                    "title": "è§£é”æ·±å±‚ä¿¡æ¯çš„æµè§ˆå™¨äº¤äº’èƒ½åŠ›",
                    "desc": "ä¿¡æ¯æœç´¢ä»£ç†åœ¨å¤šç§æœç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬çš„å·¥å…·ä½¿ç”¨ä¸»è¦é™äºAPIçº§åˆ«çš„ç‰‡æ®µæ£€ç´¢å’ŒåŸºäºURLçš„é¡µé¢è·å–ï¼Œè¿™é™åˆ¶äº†å¯¹æ›´ä¸°å¯Œä¿¡æ¯çš„è®¿é—®ã€‚å®Œå…¨çš„æµè§ˆå™¨äº¤äº’å¯ä»¥è§£é”æ›´æ·±å±‚æ¬¡çš„èƒ½åŠ›ï¼Œä½†å…¶ç»†ç²’åº¦æ§åˆ¶å’Œå†—é•¿çš„é¡µé¢å†…å®¹å¸¦æ¥äº†å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åµŒå¥—æµè§ˆå™¨ä½¿ç”¨å­¦ä¹ ï¼ˆNestBrowseï¼‰ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªæœ€å°ä¸”å®Œæ•´çš„æµè§ˆå™¨æ“ä½œæ¡†æ¶ï¼Œé€šè¿‡åµŒå¥—ç»“æ„å°†äº¤äº’æ§åˆ¶ä¸é¡µé¢æ¢ç´¢è§£è€¦ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒNestBrowseåœ¨æ·±å±‚ä¿¡æ¯æœç´¢åŸºå‡†æµ‹è¯•ä¸­å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œè¿›ä¸€æ­¥çš„åˆ†æå¼ºè°ƒäº†å…¶æ•ˆç‡å’Œçµæ´»æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23162",
            "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
            "url": "https://huggingface.co/papers/2512.23162",
            "abstract": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.",
            "score": 6,
            "issue_id": 310,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "798d94b026b07ca4",
            "authors": [
                "Yufan He",
                "Pengfei Guo",
                "Mengya Xu",
                "Zhaoshuo Li",
                "Andriy Myronenko",
                "Dillan Imans",
                "Bingjie Liu",
                "Dongren Yang",
                "Mingxue Gu",
                "Yongnan Ji",
                "Yueming Jin",
                "Ren Zhao",
                "Baiyong Shen",
                "Daguang Xu"
            ],
            "affiliations": [
                "NVIDIA",
                "National University of Singapore",
                "Ruijin Hospital",
                "Sung Kyun Kwan University",
                "The Chinese University of Hong Kong",
                "Wenzhou Medical University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23162.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#synthetic",
                    "#robotics",
                    "#science",
                    "#healthcare",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€ĞµÑˆĞ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ SurgWorld â€” Ğ¼Ğ¸Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ AI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ½Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ»Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ¸Ğ· ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ñ‚ĞµĞ¼ ÑĞ°Ğ¼Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° vision language action (VLA), Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ… Ğ½Ğ° Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ."
                },
                "en": {
                    "title": "Unlocking Surgical Robotics with Synthetic Data and World Models",
                    "desc": "This paper addresses the challenge of limited data in training autonomous surgical robots by introducing a novel approach that utilizes a world model called SurgeWorld. The authors created the Surgical Action Text Alignment (SATA) dataset, which provides detailed action descriptions for surgical tasks, enabling the training of policy models. By employing an inverse dynamics model, they generate synthetic paired video action data from surgical videos, enhancing the training process. The results show that their surgical vision language action (VLA) policy, trained with this augmented data, significantly outperforms traditional models, paving the way for more efficient and scalable surgical robot training."
                },
                "zh": {
                    "title": "åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æå‡å¤–ç§‘æœºå™¨äººè‡ªä¸»æŠ€èƒ½",
                    "desc": "æœ¬ç ”ç©¶è§£å†³äº†è‡ªä¸»å¤–ç§‘æœºå™¨äººé¢ä¸´çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬åˆ›å»ºäº†Surgical Action Text Alignmentï¼ˆSATAï¼‰æ•°æ®é›†ï¼Œä¸ºå¤–ç§‘æœºå™¨äººæä¾›è¯¦ç»†çš„åŠ¨ä½œæè¿°ã€‚åŸºäºSATAï¼Œæˆ‘ä»¬æ„å»ºäº†SurgeWorldï¼Œä¸€ä¸ªå…ˆè¿›çš„ç‰©ç†AIä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–å’ŒçœŸå®çš„å¤–ç§‘æ‰‹æœ¯è§†é¢‘ã€‚é€šè¿‡ä½¿ç”¨åˆæˆè§†é¢‘æ¨æ–­ä¼ªè¿åŠ¨å­¦ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨çœŸå®å¤–ç§‘æœºå™¨äººå¹³å°ä¸Šè¡¨ç°ä¼˜äºä»…ä½¿ç”¨çœŸå®ç¤ºèŒƒè®­ç»ƒçš„æ¨¡å‹ï¼Œæ¨åŠ¨äº†è‡ªä¸»å¤–ç§‘æŠ€èƒ½çš„è·å–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21720",
            "title": "An Information Theoretic Perspective on Agentic System Design",
            "url": "https://huggingface.co/papers/2512.21720",
            "abstract": "Agentic language model (LM) systems power modern applications like \"Deep Research\" and \"Claude Code,\" and leverage multi-LM architectures to overcome context limitations. Beneath their apparent diversity lies a recurring pattern: smaller \"compressor\" LMs (that can even run locally) distill raw context into compact text that is then consumed by larger \"predictor\" LMs. Despite their popularity, the design of compressor-predictor systems remains largely ad hoc, with little guidance on how compressor and predictor choices shape downstream performance. In practice, attributing gains to compression versus prediction requires costly, task-specific pairwise sweeps. We argue that these agentic system design questions are, at root, information-theoretic. Viewing the compressor LM as a noisy channel, we introduce a simple estimator of mutual information between the context and its compression to quantify compression quality in a task-independent way. We show that mutual information strongly predicts downstream performance, independent of any specific task. Through an information-theoretic framework, we perform a comprehensive empirical analysis across five datasets and three model families. Results reveal that larger compressors not only are more accurate, but also more token-efficient, conveying more bits of information per token. A 7B Qwen-2.5 compressor, for instance, is 1.6times more accurate, 4.6times more concise, and conveys 5.5times more bits of mutual information per token than its 1.5B sibling. Across datasets, scaling compressors is substantially more effective than scaling predictors, enabling larger on-device compressors to pair with smaller cloud predictors. Applied to a Deep Research system, these principles enable local compressors as small as 3B parameters to recover 99% of frontier-LM accuracy at 26% of API costs.",
            "score": 6,
            "issue_id": 310,
            "pub_date": "2025-12-25",
            "pub_date_card": {
                "ru": "25 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 25",
                "zh": "12æœˆ25æ—¥"
            },
            "hash": "20cb8de872acc0d9",
            "authors": [
                "Shizhe He",
                "Avanika Narayan",
                "Ishan S. Khare",
                "Scott W. Linderman",
                "Christopher RÃ©",
                "Dan Biderman"
            ],
            "affiliations": [
                "Department of Computer Science, Stanford University",
                "Department of Statistics, Stanford University",
                "Wu Tsai Neurosciences Institute, Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21720.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#small_models",
                    "#agents",
                    "#long_context",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€-Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚Ğ¾Ñ€",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğµ Ğ¸Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¶Ğ¸Ğ¼Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚, Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞµĞ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ĞµĞ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€Ğ°Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ²ÑĞµĞ³Ğ¾ 3B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ 99% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° API Ğ½Ğ° 26%."
                },
                "en": {
                    "title": "Maximizing Efficiency: The Power of Compressor Models in Language Systems",
                    "desc": "This paper discusses the design of agentic language model systems that use smaller 'compressor' models to condense context into compact text for larger 'predictor' models. It highlights the lack of systematic guidance in choosing these models and introduces an information-theoretic approach to evaluate the quality of compression. By estimating mutual information between the context and its compression, the authors demonstrate that higher mutual information correlates with better performance in downstream tasks. The findings suggest that scaling compressor models is more beneficial than scaling predictor models, leading to more efficient and cost-effective language processing systems."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å‹ç¼©å™¨ä»¥æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä»£ç†è¯­è¨€æ¨¡å‹ç³»ç»Ÿçš„è®¾è®¡ï¼Œç‰¹åˆ«æ˜¯å‹ç¼©å™¨å’Œé¢„æµ‹å™¨çš„å…³ç³»ã€‚å‹ç¼©å™¨æ¨¡å‹å°†åŸå§‹ä¸Šä¸‹æ–‡è½¬åŒ–ä¸ºç´§å‡‘æ–‡æœ¬ï¼Œä¾›æ›´å¤§çš„é¢„æµ‹å™¨æ¨¡å‹ä½¿ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¿¡æ¯è®ºæ¡†æ¶ï¼Œé€šè¿‡äº’ä¿¡æ¯é‡åŒ–å‹ç¼©è´¨é‡ï¼Œå‘ç°äº’ä¿¡æ¯ä¸ä¸‹æ¸¸æ€§èƒ½æœ‰å¾ˆå¼ºçš„é¢„æµ‹èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¢å¤§å‹ç¼©å™¨çš„è§„æ¨¡æ¯”å¢å¤§é¢„æµ‹å™¨çš„è§„æ¨¡æ›´æœ‰æ•ˆï¼Œèƒ½å¤Ÿåœ¨æœ¬åœ°è®¾å¤‡ä¸Šå®ç°é«˜æ•ˆçš„æ¨¡å‹ç»„åˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.20927",
            "title": "Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting",
            "url": "https://huggingface.co/papers/2512.20927",
            "abstract": "Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.",
            "score": 5,
            "issue_id": 314,
            "pub_date": "2025-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "88507e380ed5be0b",
            "authors": [
                "Yoonwoo Jeong",
                "Cheng Sun",
                "Frank Wang",
                "Minsu Cho",
                "Jaesung Choe"
            ],
            "affiliations": [
                "NVIDIA",
                "POSTECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.20927.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#open_source",
                    "#cv",
                    "#3d"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² 3D ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° (Q-Render) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ½Ğ¾Ğ¼ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ 3D ÑÑ†ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D Gaussian Splatting. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ²ÑĞµÑ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ², Ğ¿ĞµÑ€ĞµÑĞµĞºĞ°ÑÑ‰Ğ¸Ñ… Ğ»ÑƒÑ‡, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ GS-Net, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… ScanNet Ğ¸ LeRF Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° (~43.7x) Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Efficient 3D Rendering with Quantile Sampling",
                    "desc": "This paper presents a new method called Quantile Rendering (Q-Render) to improve the efficiency of rendering high-dimensional features in 3D Gaussian Splatting for open-vocabulary segmentation. Traditional methods often lose important information due to feature compression, which negatively impacts segmentation quality. Q-Render addresses this by selectively sampling only the most influential 3D Gaussians, leading to better fidelity in rendering. The authors also introduce the Gaussian Splatting Network (GS-Net), which utilizes Q-Render to predict Gaussian features effectively, achieving significant speed improvements in real-time applications."
                },
                "zh": {
                    "title": "é«˜æ•ˆé«˜ç»´ç‰¹å¾æ¸²æŸ“çš„æ–°ç­–ç•¥",
                    "desc": "æœ€è¿‘ï¼Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„è¿›å±•æˆåŠŸå°†å¼€æ”¾è¯æ±‡åˆ†å‰²ï¼ˆOVSï¼‰æ‰©å±•åˆ°3Dé¢†åŸŸï¼Œåˆ©ç”¨äº†3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3D-GSï¼‰ã€‚ç„¶è€Œï¼Œé«˜ç»´ç‰¹å¾çš„é«˜æ•ˆæ¸²æŸ“ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä½¿ç”¨ä»£ç æœ¬æˆ–ç‰¹å¾å‹ç¼©ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼Œä»è€Œé™ä½åˆ†å‰²è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¸²æŸ“ç­–ç•¥â€”â€”åˆ†ä½æ•°æ¸²æŸ“ï¼ˆQ-Renderï¼‰ï¼Œå®ƒèƒ½å¤Ÿé«˜æ•ˆå¤„ç†é«˜ç»´ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒé«˜ä¿çœŸåº¦ã€‚é€šè¿‡å°†Q-Renderé›†æˆåˆ°ä¸€ä¸ªå¯æ³›åŒ–çš„3Dç¥ç»ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†é«˜æ–¯ç‚¹äº‘ç½‘ç»œï¼ˆGS-Netï¼‰ï¼Œè¯¥ç½‘ç»œä»¥å¯æ³›åŒ–çš„æ–¹å¼é¢„æµ‹é«˜æ–¯ç‰¹å¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23573",
            "title": "ProGuard: Towards Proactive Multimodal Safeguard",
            "url": "https://huggingface.co/papers/2512.23573",
            "abstract": "The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.",
            "score": 4,
            "issue_id": 316,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "7047b86d78755a7c",
            "authors": [
                "Shaohan Yu",
                "Lijun Li",
                "Chenyang Si",
                "Lu Sheng",
                "Jing Shao"
            ],
            "affiliations": [
                "Beihang University",
                "PRLab Nanjing University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23573.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#open_source",
                    "#multimodal",
                    "#security",
                    "#dataset",
                    "#rl",
                    "#synthetic"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ ProGuard â€” Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¸ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 87 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ¿Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (reinforcement learning) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ProGuard Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° (OOD) Ñ€Ğ¸ÑĞºĞ¾Ğ² Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 52,6% Ğ´Ğ»Ñ Ğ¸Ñ… Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ 64,8% Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "ProGuard: Proactive Safety in Multimodal Generative Models",
                    "desc": "This paper introduces ProGuard, a proactive guard designed to identify and describe out-of-distribution (OOD) safety risks in multimodal data, such as text and images. Unlike traditional methods that react to risks after they occur, ProGuard anticipates potential safety issues without needing model adjustments. The authors created a balanced dataset of 87,000 samples with safety labels and risk categories, which helps reduce bias across different modalities. By employing reinforcement learning, ProGuard enhances its ability to generate concise descriptions of unseen unsafe categories, significantly improving detection and categorization of OOD risks compared to existing models."
                },
                "zh": {
                    "title": "ProGuardï¼šä¸»åŠ¨é˜²æŠ¤ï¼Œæå‡å¤šæ¨¡æ€å®‰å…¨æ€§",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºProGuardçš„è§†è§‰-è¯­è¨€ä¸»åŠ¨é˜²æŠ¤æ¨¡å‹ï¼Œæ—¨åœ¨è¯†åˆ«å’Œæè¿°å¤šæ¨¡æ€å®‰å…¨é£é™©ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«87Kæ ·æœ¬çš„å¹³è¡¡æ•°æ®é›†ï¼Œæ ‡æ³¨äº†å®‰å…¨æ ‡ç­¾å’Œé£é™©ç±»åˆ«ï¼Œä»¥å‡å°‘æ¨¡æ€åå·®ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼ŒProGuardèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œæ¨ç†ï¼Œå¹¶åœ¨æœªè§çš„å®‰å…¨ç±»åˆ«ä¸Šç”Ÿæˆç®€æ´æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProGuardåœ¨äºŒå…ƒå®‰å…¨åˆ†ç±»å’Œä¸å®‰å…¨å†…å®¹åˆ†ç±»ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†OODé£é™©æ£€æµ‹å’Œæè¿°çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23703",
            "title": "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation",
            "url": "https://huggingface.co/papers/2512.23703",
            "abstract": "The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io",
            "score": 3,
            "issue_id": 319,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "d2c35417ed10a246",
            "authors": [
                "Huajie Tan",
                "Sixiang Chen",
                "Yijie Xu",
                "Zixiao Wang",
                "Yuheng Ji",
                "Cheng Chi",
                "Yaoxu Lyu",
                "Zhongxia Zhao",
                "Xiansheng Chen",
                "Peterson Co",
                "Shaoxuan Xie",
                "Guocai Yao",
                "Pengwei Wang",
                "Zhongyuan Wang",
                "Shanghang Zhang"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Institute of Automation, Chinese Academy of Sciences",
                "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University",
                "University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23703.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#multimodal",
                    "#dataset",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ”Ğ¾Ñ„Ğ°Ğ¼Ğ¸Ğ½ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Dopamine-Reward â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ General Reward Model (GRM), Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 3400+ Ñ‡Ğ°ÑĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Dopamine-RL â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ Policy-Invariant Reward Shaping, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ñ€Ğ¾Ğ±Ğ¾Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 95% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ·Ğ° 150 Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ (Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 1 Ñ‡Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°)."
                },
                "en": {
                    "title": "Revolutionizing Reward Functions for Robotics with Dopamine-Reward",
                    "desc": "This paper addresses the challenges of designing effective reward functions in reinforcement learning for robotics. It introduces Dopamine-Reward, a new method that creates a step-aware process reward model using multi-view inputs to enhance reward assessment. The General Reward Model (GRM) is trained on a large dataset and incorporates techniques like Step-wise Reward Discretization and Multi-Perspective Reward Fusion to improve understanding and perception. Additionally, the Dopamine-RL framework utilizes a Policy-Invariant Reward Shaping method to optimize policy learning efficiently while avoiding common pitfalls in reward shaping."
                },
                "zh": {
                    "title": "çªç ´å¼ºåŒ–å­¦ä¹ éšœç¢ï¼Œæå‡æœºå™¨äººæ™ºèƒ½",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ åœ¨ç°å®æœºå™¨äººåº”ç”¨ä¸­çš„ä¸»è¦éšœç¢ï¼Œå³æœ‰æ•ˆå¥–åŠ±å‡½æ•°çš„è®¾è®¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•Dopamine-Rewardï¼Œæ—¨åœ¨ä»å¤šè§†è§’è¾“å…¥ä¸­å­¦ä¹ é€šç”¨çš„ã€é€æ­¥æ„ŸçŸ¥çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚æ ¸å¿ƒæ˜¯æˆ‘ä»¬çš„é€šç”¨å¥–åŠ±æ¨¡å‹ï¼ˆGRMï¼‰ï¼Œé€šè¿‡ç»“æ„åŒ–ç†è§£å’Œå¤šè§†è§’å¥–åŠ±èåˆæ¥å…‹æœæ„ŸçŸ¥é™åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGRMåœ¨å¥–åŠ±è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œè€ŒåŸºäºGRMçš„Dopamine-RLæ˜¾è‘—æé«˜äº†ç­–ç•¥å­¦ä¹ æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.23222",
            "title": "Bridging Your Imagination with Audio-Video Generation via a Unified Director",
            "url": "https://huggingface.co/papers/2512.23222",
            "abstract": "A unified director model leveraging a Mixture-of-Transformers architecture with interleaved and disentangled learning generates coherent video scripts and consistent keyframes through a single framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.",
            "score": 3,
            "issue_id": 311,
            "pub_date": "2025-12-29",
            "pub_date_card": {
                "ru": "29 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 29",
                "zh": "12æœˆ29æ—¥"
            },
            "hash": "64e7665d5e5a4efc",
            "authors": [
                "Jiaxu Zhang",
                "Tianshu Hu",
                "Yuan Zhang",
                "Zenan Li",
                "Linjie Luo",
                "Guosheng Lin",
                "Xin Chen"
            ],
            "affiliations": [
                "ByteDance Intelligent Creation",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.23222.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#architecture",
                    "#multimodal",
                    "#long_context",
                    "#training",
                    "#video",
                    "#story_generation"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸ÑÑÑ‘Ñ€: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ UniMAGE â€” ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞ¶Ğ¸ÑÑÑ‘Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² Ğ¾Ğ´Ğ½Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mixture-of-Transformers. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Â«ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµÂ», Ğ³Ğ´Ğµ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ğ°ĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ½ĞµĞ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ°Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ»ÑŒĞ¼Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UniMAGE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unifying Script and Keyframe Generation for Coherent Video Creation",
                    "desc": "This paper presents UniMAGE, a unified director model that integrates script writing and keyframe generation into a single framework using a Mixture-of-Transformers architecture. By treating these tasks as interconnected, the model enhances the logical coherence of video scripts and the visual consistency of keyframes. The training process involves interleaved concept learning to improve understanding and creativity, followed by disentangled expert learning to separate the script and image generation processes. The results show that UniMAGE outperforms existing models, making it easier for non-experts to create multi-shot films."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¯¼æ¼”æ¨¡å‹ï¼šè§†é¢‘åˆ›ä½œçš„æ–°æ–¹å¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¯¼æ¼”æ¨¡å‹UniMAGEï¼Œåˆ©ç”¨æ··åˆå˜æ¢å™¨æ¶æ„æ¥ç”Ÿæˆè¿è´¯çš„è§†é¢‘è„šæœ¬å’Œä¸€è‡´çš„å…³é”®å¸§ã€‚ä¼ ç»Ÿçš„è§†é¢‘åˆ›ä½œç³»ç»Ÿå°†è„šæœ¬æ’°å†™å’Œå…³é”®é•œå¤´è®¾è®¡è§†ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„ä»»åŠ¡ï¼Œè€Œæˆ‘ä»¬è®¤ä¸ºè¿™ä¸¤è€…åº”åœ¨ä¸€ä¸ªæ¡†æ¶å†…ç»Ÿä¸€ã€‚é€šè¿‡å¼•å…¥â€œå…ˆäº¤é”™åè§£è€¦â€çš„è®­ç»ƒèŒƒå¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåˆ›é€ è„šæœ¬å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniMAGEåœ¨ç”Ÿæˆé€»è¾‘è¿è´¯çš„è§†é¢‘è„šæœ¬å’Œè§†è§‰ä¸€è‡´çš„å…³é”®å¸§å›¾åƒæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.21734",
            "title": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation",
            "url": "https://huggingface.co/papers/2512.21734",
            "abstract": "Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A \"running ahead\" mechanism that dynamically updates the reference frame's temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.",
            "score": 3,
            "issue_id": 314,
            "pub_date": "2025-12-25",
            "pub_date_card": {
                "ru": "25 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 25",
                "zh": "12æœˆ25æ—¥"
            },
            "hash": "1033759ddae4091f",
            "authors": [
                "Steven Xiao",
                "Xindi Zhang",
                "Dechao Meng",
                "Qi Wang",
                "Peng Zhang",
                "Bang Zhang"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.21734.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸĞ¾Ñ‚Ğ¾Ğº Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ¾Ğ²: Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ²",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Knot Forcing â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°ÑƒÑ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ KV-ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑƒĞ·Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ \"Ğ·Ğ°Ğ±ĞµĞ³Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¿ĞµÑ€ĞµĞ´\" Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… GPU Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Knot Forcing: Real-Time Portrait Animation Redefined",
                    "desc": "This paper introduces a new framework called Knot Forcing for real-time portrait animation, which is crucial for applications like virtual assistants. It combines chunk-wise generation with global identity preservation and local temporal modeling to enhance visual quality and coherence. The framework also includes a temporal knot module to ensure smooth transitions between video chunks and a 'running ahead' mechanism to maintain long-term consistency. Overall, Knot Forcing achieves high fidelity and responsiveness, making it suitable for interactive environments on standard consumer hardware."
                },
                "zh": {
                    "title": "å®æ—¶è‚–åƒåŠ¨ç”»çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºKnot Forcingçš„å®æ—¶è‚–åƒåŠ¨ç”»æµåª’ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é«˜è§†è§‰ä¿çœŸåº¦å’Œæ—¶é—´ä¸€è‡´æ€§çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰é¡¹å…³é”®è®¾è®¡å®ç°äº†é«˜æ•ˆçš„é€å¸§ç”Ÿæˆï¼Œé¿å…äº†é”™è¯¯ç´¯ç§¯å’Œè¿åŠ¨ä¸è¿ç»­æ€§ã€‚é¦–å…ˆï¼Œé‡‡ç”¨äº†åŸºäºå—çš„ç”Ÿæˆç­–ç•¥ï¼Œåˆ©ç”¨ç¼“å­˜çš„KVçŠ¶æ€ä¿æŒå…¨å±€èº«ä»½ã€‚å…¶æ¬¡ï¼Œæ—¶é—´ç»“æ¨¡å—é€šè¿‡å›¾åƒåˆ°è§†é¢‘çš„æ¡ä»¶åŒ–å¹³æ»‘ç›¸é‚»å—ä¹‹é—´çš„è¿åŠ¨è¿‡æ¸¡ï¼Œæœ€åï¼ŒåŠ¨æ€æ›´æ–°å‚è€ƒå¸§çš„æ—¶é—´åæ ‡ä»¥æ”¯æŒé•¿æœŸä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22100",
            "title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis",
            "url": "https://huggingface.co/papers/2512.22100",
            "abstract": "Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.",
            "score": 2,
            "issue_id": 318,
            "pub_date": "2025-12-26",
            "pub_date_card": {
                "ru": "26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 26",
                "zh": "12æœˆ26æ—¥"
            },
            "hash": "4ec4a8f8fa64277b",
            "authors": [
                "Duygu Altinok"
            ],
            "affiliations": [
                "Independent Researcher, Berlin, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22100.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#low_resource",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ‡¹ğŸ‡·",
                "ru": {
                    "title": "Ğ¢ÑƒÑ€ĞµÑ†ĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ» ÑĞ²Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TrGLUE â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ NLU Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ SentiTurca Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¸Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑÑ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ³Ğ»Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° GLUE, Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞ»Ğ¾ÑÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ»ÑƒĞ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ LLM, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ fine-tuning Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Turkish NLP with TrGLUE and SentiTurca",
                    "desc": "This paper introduces TrGLUE, a new benchmark designed specifically for evaluating natural language understanding (NLU) in the Turkish language. It aims to fill the gap in existing benchmarks by providing a comprehensive set of NLU tasks similar to the GLUE benchmark for English. The authors also present SentiTurca, a specialized benchmark for sentiment analysis in Turkish. To support researchers, they offer fine-tuning and evaluation code for transformer models, ensuring that the benchmarks are accessible and useful for advancing Turkish NLP research."
                },
                "zh": {
                    "title": "å»ºç«‹åœŸè€³å…¶è¯­è‡ªç„¶è¯­è¨€ç†è§£è¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†TrGLUEï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åœŸè€³å…¶è¯­çš„è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¡«è¡¥ç°æœ‰è¯„ä¼°å·¥å…·çš„ç©ºç™½ã€‚TrGLUEåŒ…å«å¤šç§NLUä»»åŠ¡ï¼Œå¹¶æä¾›äº†æƒ…æ„Ÿåˆ†æçš„ä¸“é—¨åŸºå‡†SentiTurcaã€‚ä¸ºäº†æ”¯æŒç ”ç©¶äººå‘˜ï¼Œæ–‡ç« è¿˜æä¾›äº†åŸºäºå˜æ¢å™¨æ¨¡å‹çš„å¾®è°ƒå’Œè¯„ä¼°ä»£ç ï¼Œæ–¹ä¾¿æœ‰æ•ˆä½¿ç”¨è¿™äº›åŸºå‡†ã€‚é€šè¿‡TrGLUEï¼Œæˆ‘ä»¬å¸Œæœ›å»ºç«‹ä¸€ä¸ªå¼ºå¤§çš„åœŸè€³å…¶è¯­NLUè¯„ä¼°æ¡†æ¶ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜è·å–æœ‰ä»·å€¼çš„èµ„æºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.22984",
            "title": "Reverse Personalization",
            "url": "https://huggingface.co/papers/2512.22984",
            "abstract": "A reverse personalization framework using conditional diffusion inversion enables attribute-controllable face anonymization, balancing identity removal and image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identity-specific features rely either on the subject being well-represented in the pre-trained model or require model fine-tuning for specific identities. In this work, we analyze the identity generation process and introduce a reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the model's training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attribute-controllable anonymization. We demonstrate that our method achieves a state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/hanweikung/reverse-personalization .",
            "score": 0,
            "issue_id": 315,
            "pub_date": "2025-12-28",
            "pub_date_card": {
                "ru": "28 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 28",
                "zh": "12æœˆ28æ—¥"
            },
            "hash": "b26018be9b20b1fc",
            "authors": [
                "Han-Wei Kung",
                "Tuomas Varanka",
                "Nicu Sebe"
            ],
            "affiliations": [
                "University of Oulu",
                "University of Trento"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.22984.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¸Ñ† Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ğ»Ğ¸Ñ†, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½ĞµÑ‚ Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ²ĞµÑ‚Ğ²Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ»Ğ¸Ñ†Ğ°, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ğ¾Ğ» Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Face Anonymization with Conditional Diffusion Inversion",
                    "desc": "This paper presents a novel reverse personalization framework that utilizes conditional diffusion inversion for face anonymization. The method allows for the removal of identity-specific features while maintaining high image quality and control over facial attributes. Unlike previous techniques that depend on the model's training data or require fine-tuning, this approach enables direct manipulation of images without text prompts. The authors demonstrate that their framework achieves superior results in balancing identity removal and attribute preservation compared to existing methods."
                },
                "zh": {
                    "title": "åå‘ä¸ªæ€§åŒ–ï¼šå¯æ§çš„äººè„¸åŒ¿ååŒ–æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åå‘ä¸ªæ€§åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨æ¡ä»¶æ‰©æ•£åæ¼”å®ç°å¯æ§å±æ€§çš„äººè„¸åŒ¿ååŒ–ï¼Œå¹³è¡¡èº«ä»½å»é™¤ä¸å›¾åƒè´¨é‡ã€‚ç°æœ‰çš„æ–¹æ³•åœ¨å»é™¤æˆ–ä¿®æ”¹èº«ä»½ç‰¹å¾æ—¶ï¼Œå¾€å¾€ä¾èµ–äºé¢„è®­ç»ƒæ¨¡å‹å¯¹ç‰¹å®šä¸ªä½“çš„è‰¯å¥½è¡¨ç¤ºï¼Œæˆ–éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡æ¡ä»¶æ‰©æ•£åæ¼”ï¼Œå…è®¸ç›´æ¥æ“æ§å›¾åƒï¼Œè€Œæ— éœ€æ–‡æœ¬æç¤ºï¼Œå¹¶å¼•å…¥èº«ä»½å¼•å¯¼çš„æ¡ä»¶åˆ†æ”¯ï¼Œä»¥è¶…è¶Šè®­ç»ƒæ•°æ®ä¸­çš„ä¸ªä½“ã€‚ä¸ä»¥å¾€ç¼ºä¹é¢éƒ¨å±æ€§æ§åˆ¶çš„åŒ¿ååŒ–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒå¯æ§å±æ€§çš„åŒ¿ååŒ–ï¼Œå¹¶åœ¨èº«ä»½å»é™¤ã€å±æ€§ä¿ç•™å’Œå›¾åƒè´¨é‡ä¹‹é—´å®ç°äº†æœ€ä½³å¹³è¡¡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-29.html",
    "link_next": "2025-12-31.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "29.12",
        "en": "12/29",
        "zh": "12æœˆ29æ—¥"
    },
    "short_date_next": {
        "ru": "31.12",
        "en": "12/31",
        "zh": "12æœˆ31æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 0,
        "#benchmark": 9,
        "#agents": 7,
        "#cv": 5,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 5,
        "#3d": 1,
        "#audio": 1,
        "#video": 8,
        "#multimodal": 12,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 11,
        "#healthcare": 1,
        "#training": 11,
        "#robotics": 6,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 9,
        "#survey": 0,
        "#diffusion": 8,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 11,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    }
}