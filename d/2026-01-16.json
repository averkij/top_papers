{
    "date": {
        "ru": "16 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 16",
        "zh": "1æœˆ16æ—¥"
    },
    "time_utc": "2026-01-16 05:28",
    "weekday": 4,
    "issue_id": 612,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.09668",
            "title": "STEP3-VL-10B Technical Report",
            "url": "https://huggingface.co/papers/2601.09668",
            "abstract": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.",
            "score": 50,
            "issue_id": 611,
            "pub_date": "2026-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "40d93c6c302d8c56",
            "authors": [
                "Ailin Huang",
                "Chengyuan Yao",
                "Chunrui Han",
                "Fanqi Wan",
                "Hangyu Guo",
                "Haoran Lv",
                "Hongyu Zhou",
                "Jia Wang",
                "Jian Zhou",
                "Jianjian Sun",
                "Jingcheng Hu",
                "Kangheng Lin",
                "Liang Zhao",
                "Mitt Huang",
                "Song Yuan",
                "Wenwen Qu",
                "Xiangfeng Wang",
                "Yanlin Lai",
                "Yingxiu Zhao",
                "Yinmin Zhang",
                "Yukang Shi",
                "Yuyang Chen",
                "Zejia Weng",
                "Ziyang Meng",
                "Ang Li",
                "Aobo Kong",
                "Bo Dong",
                "Changyi Wan",
                "David Wang",
                "Di Qi",
                "Dingming Li",
                "En Yu",
                "Guopeng Li",
                "Haiquan Yin",
                "Han Zhou",
                "Hanshan Zhang",
                "Haolong Yan",
                "Hebin Zhou",
                "Hongbo Peng",
                "Jiaran Zhang",
                "Jiashu Lv",
                "Jiayi Fu",
                "Jie Cheng",
                "Jie Zhou",
                "Jisheng Yin",
                "Jingjing Xie",
                "Jingwei Wu",
                "Jun Zhang",
                "Junfeng Liu",
                "Kaijun Tan",
                "Kaiwen Yan",
                "Liangyu Chen",
                "Lina Chen",
                "Mingliang Li",
                "Qian Zhao",
                "Quan Sun",
                "Shaoliang Pang",
                "Shengjie Fan",
                "Shijie Shang",
                "Siyuan Zhang",
                "Tianhao You",
                "Wei Ji",
                "Wuxun Xie",
                "Xiaobo Yang",
                "Xiaojie Hou",
                "Xiaoran Jiao",
                "Xiaoxiao Ren",
                "Xiangwen Kong",
                "Xin Huang",
                "Xin Wu",
                "Xing Chen",
                "Xinran Wang",
                "Xuelin Zhang",
                "Yana Wei",
                "Yang Li",
                "Yanming Xu",
                "Yeqing Shen",
                "Yuang Peng",
                "Yue Peng",
                "Yu Zhou",
                "Yusheng Li",
                "Yuxiang Yang",
                "Yuyang Zhang",
                "Zhe Xie",
                "Zhewei Huang",
                "Zhenyi Lu",
                "Zhimin Fan",
                "Zihui Cheng",
                "Daxin Jiang",
                "Qi Han",
                "Xiangyu Zhang",
                "Yibo Zhu",
                "Zheng Ge"
            ],
            "affiliations": [
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.09668.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#architecture",
                    "#open_source",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞœĞ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ğ¿Ğ°ĞºĞµÑ‚Ğµ",
                    "desc": "STEP3-VL-10B â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² 10-20 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 1.2 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Perception Encoder Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Qwen3-8B Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1000 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Parallel Coordinated Reasoning Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…: 92.2% Ğ½Ğ° MMBench, 80.11% Ğ½Ğ° MMMU Ğ¸ 94.43% Ğ½Ğ° AIME2025, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Gemini 2.5 Pro."
                },
                "en": {
                    "title": "Compact Powerhouse: STEP3-VL-10B Redefines Multimodal Intelligence",
                    "desc": "STEP3-VL-10B is a cutting-edge multimodal model that combines a language-aligned Perception Encoder with a Qwen3-8B decoder for enhanced visual and language understanding. It utilizes a unified pre-training approach on a massive dataset of 1.2 trillion multimodal tokens, allowing it to learn intricate relationships between text and images. The model also incorporates a scaled post-training process with reinforcement learning and Parallel Coordinated Reasoning (PaCoRe) to optimize its reasoning capabilities during inference. As a result, STEP3-VL-10B achieves performance levels comparable to much larger models while maintaining a compact size, making it a significant advancement in the field of multimodal AI."
                },
                "zh": {
                    "title": "STEP3-VL-10Bï¼šç´§å‡‘é«˜æ•ˆçš„å¤šæ¨¡æ€æ™ºèƒ½æ–°æ ‡æ†",
                    "desc": "STEP3-VL-10B æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¼€æºåŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨é‡æ–°å®šä¹‰ç´§å‡‘æ•ˆç‡ä¸å‰æ²¿å¤šæ¨¡æ€æ™ºèƒ½ä¹‹é—´çš„æƒè¡¡ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œç»“åˆè¯­è¨€å¯¹é½çš„æ„ŸçŸ¥ç¼–ç å™¨å’Œ Qwen3-8B è§£ç å™¨ï¼Œå»ºç«‹äº†å†…åœ¨çš„è§†è§‰-è¯­è¨€ååŒã€‚å®ƒè¿˜é‡‡ç”¨äº†å¤§è§„æ¨¡çš„åè®­ç»ƒæµç¨‹å’Œå¹¶è¡Œåè°ƒæ¨ç†ï¼ˆPaCoReï¼‰ï¼Œä»¥æé«˜è§†è§‰æ¨ç†çš„æ•ˆç‡ã€‚å°½ç®¡æ¨¡å‹ä½“ç§¯ä»…ä¸º 10Bï¼Œä½†å…¶æ€§èƒ½å¯ä¸ 10 åˆ° 20 å€æ›´å¤§çš„æ¨¡å‹ç›¸åª²ç¾ï¼Œç”šè‡³è¶…è¶Šä¸€äº›é¡¶çº§ä¸“æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.08763",
            "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
            "url": "https://huggingface.co/papers/2601.08763",
            "abstract": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.",
            "score": 23,
            "issue_id": 610,
            "pub_date": "2026-01-13",
            "pub_date_card": {
                "ru": "13 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 13",
                "zh": "1æœˆ13æ—¥"
            },
            "hash": "9504e6a54b716754",
            "authors": [
                "Zhiyuan Hu",
                "Yucheng Wang",
                "Yufei He",
                "Jiaying Wu",
                "Yilun Zhao",
                "See-Kiong Ng",
                "Cynthia Breazeal",
                "Anh Tuan Luu",
                "Hae Won Park",
                "Bryan Hooi"
            ],
            "affiliations": [
                "MIT",
                "NTU",
                "NUS",
                "Yale"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.08763.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² RL Ğ´Ğ»Ñ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ rollout, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑĞµÑ‚ Ñ€ĞµĞ´ĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ exploration collapse, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Uniqueness-Aware Reinforcement Learning Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM-ÑÑƒĞ´ÑŒÑ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº pass@k Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸, Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ pass@1."
                },
                "en": {
                    "title": "Unlocking Diverse Solutions with Unique Strategies in RL",
                    "desc": "This paper introduces Uniqueness-Aware Reinforcement Learning (UARL) to enhance the performance of large language models (LLMs) in complex reasoning tasks. Traditional reinforcement learning often leads to exploration collapse, where models focus too much on common reasoning patterns, limiting their ability to discover diverse solutions. UARL addresses this by rewarding rare high-level reasoning strategies, encouraging the model to explore a wider range of solutions. The method clusters similar solutions and adjusts rewards to favor unique strategies, resulting in improved performance across various reasoning benchmarks without compromising initial accuracy."
                },
                "zh": {
                    "title": "ç‹¬ç‰¹æ€§æ„è¯†å¼ºåŒ–å­¦ä¹ ï¼šæå‡æ¨ç†å¤šæ ·æ€§ä¸æ€§èƒ½",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºç‹¬ç‰¹æ€§æ„è¯†å¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡å¥–åŠ±ç¨€æœ‰çš„é«˜å±‚æ¬¡æ¨ç†ç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢å¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸æ˜¯ä»…ä»…é›†ä¸­äºå°‘æ•°ä¸»å¯¼çš„æ¨ç†æ¨¡å¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åŸºäºè¯­è¨€æ¨¡å‹çš„è¯„åˆ¤è€…å¯¹ç›¸åŒé—®é¢˜çš„è§£å†³æ–¹æ¡ˆè¿›è¡Œèšç±»ï¼Œä»è€Œæé«˜äº†æ­£ç¡®ä½†æ–°é¢–ç­–ç•¥çš„å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦ã€ç‰©ç†å’ŒåŒ»å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ˜¾è‘—æé«˜äº†å¤šæ ·æ€§å’Œæ•´ä½“æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.09667",
            "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
            "url": "https://huggingface.co/papers/2601.09667",
            "abstract": "Multi-Agent Test-Time Reinforcement Learning (MATTRL) enhances multi-agent reasoning through structured textual experience injection and consensus-based decision making at inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.",
            "score": 22,
            "issue_id": 610,
            "pub_date": "2026-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "135666a5989aae03",
            "authors": [
                "Zhiyuan Hu",
                "Yunhai Hu",
                "Juncheng Liu",
                "Shuyue Stella Li",
                "Yucheng Wang",
                "Zhen Xu",
                "See-Kiong Ng",
                "Anh Tuan Luu",
                "Xinxing Xu",
                "Bryan Hooi",
                "Cynthia Breazeal",
                "Hae Won Park"
            ],
            "affiliations": [
                "Columbia",
                "MIT",
                "Microsoft",
                "NTU",
                "NUS",
                "NYU",
                "UW"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.09667.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#reasoning",
                    "#inference",
                    "#rl",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¸ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MATTRL â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²-ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¿ÑƒĞ»Ğ° Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ-Ğ±Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸Ğ· Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ MATTRL Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 3,67% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ½Ğ° 8,67% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ´Ğ½Ğ°Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multi-Agent Reasoning with MATTRL",
                    "desc": "Multi-Agent Test-Time Reinforcement Learning (MATTRL) is a novel framework that enhances multi-agent reasoning by incorporating structured textual experiences during inference. It addresses the challenges of multi-agent reinforcement learning (MARL), such as instability and non-stationarity, by enabling agents to collaborate and reach consensus based on shared knowledge. MATTRL forms a team of specialists that engage in multi-turn discussions, improving decision-making accuracy through the integration of test-time experiences. The framework demonstrates significant performance improvements across various benchmarks, showcasing its effectiveness in robust multi-agent reasoning without the need for extensive tuning."
                },
                "zh": {
                    "title": "å¤šæ™ºèƒ½ä½“æ¨ç†çš„æ–°è·¯å¾„",
                    "desc": "å¤šæ™ºèƒ½ä½“æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆMATTRLï¼‰é€šè¿‡åœ¨æ¨ç†æ—¶æ³¨å…¥ç»“æ„åŒ–æ–‡æœ¬ç»éªŒå’ŒåŸºäºå…±è¯†çš„å†³ç­–åˆ¶å®šï¼Œå¢å¼ºäº†å¤šæ™ºèƒ½ä½“çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å½¢æˆäº†ä¸€ä¸ªå¤šä¸“å®¶å›¢é˜Ÿï¼Œä¸“æ³¨äºå¤šè½®è®¨è®ºï¼Œå¹¶åœ¨æ¨ç†æ—¶æ£€ç´¢å’Œæ•´åˆç»éªŒï¼Œä»¥è¾¾æˆæœ€ç»ˆå†³ç­–ã€‚MATTRLåœ¨åŒ»å­¦ã€æ•°å­¦å’Œæ•™è‚²ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡æé«˜äº†3.67%çš„å‡†ç¡®ç‡ï¼Œç›¸æ¯”äºå¤šæ™ºèƒ½ä½“åŸºçº¿å’Œ8.67%ç›¸æ¯”äºå•æ™ºèƒ½ä½“åŸºçº¿ã€‚è¯¥æ–¹æ³•æä¾›äº†ä¸€æ¡ç¨³å®šã€é«˜æ•ˆçš„è·¯å¾„ï¼Œä»¥å®ç°å¯¹åˆ†å¸ƒå˜åŒ–çš„é²æ£’æ€§ï¼Œè€Œæ— éœ€è°ƒä¼˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10305",
            "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
            "url": "https://huggingface.co/papers/2601.10305",
            "abstract": "A large-scale Chinese image-text dataset called DanQing is introduced to advance vision-language pretraining, demonstrating superior performance in various downstream tasks through continual pretraining of the SigLIP2 model.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.",
            "score": 21,
            "issue_id": 610,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "35356d22511867a5",
            "authors": [
                "Hengyu Shen",
                "Tiancheng Gu",
                "Bin Qin",
                "Lan Wu",
                "Yuling Wu",
                "Shuo Tan",
                "Zelong Sun",
                "Jun Wang",
                "Nan Wu",
                "Xiang An",
                "Weidong Cai",
                "Ziyong Feng",
                "Kaicheng Yang"
            ],
            "affiliations": [
                "Glint Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10305.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ‡¨ğŸ‡³",
                "ru": {
                    "title": "DanQing: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ vision-language pretraining",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DanQing, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ 2024-2025 Ğ³Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ SigLIP2, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° DanQing, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ…. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½ Ğ¿Ğ¾Ğ´ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹ CC-BY 4.0 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ vision-language pretraining."
                },
                "en": {
                    "title": "DanQing: Elevating Chinese Vision-Language Pretraining",
                    "desc": "The paper introduces DanQing, a large-scale Chinese image-text dataset designed to enhance vision-language pretraining. It addresses the lack of high-quality Chinese data by providing 100 million curated image-text pairs, collected from recent web data. The dataset supports the continual pretraining of the SigLIP2 model, which shows improved performance in various Chinese downstream tasks like zero-shot classification and cross-modal retrieval. By making DanQing available for research, the authors aim to advance the field of Chinese vision-language models."
                },
                "zh": {
                    "title": "DanQingï¼šæ¨åŠ¨ä¸­æ–‡è§†è§‰-è¯­è¨€é¢„è®­ç»ƒçš„æœªæ¥",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºDanQingçš„å¤§è§„æ¨¡ä¸­æ–‡å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ï¼Œæ—¨åœ¨æ¨åŠ¨è§†è§‰-è¯­è¨€é¢„è®­ç»ƒçš„å‘å±•ã€‚é€šè¿‡å¯¹SigLIP2æ¨¡å‹çš„æŒç»­é¢„è®­ç»ƒï¼ŒDanQingåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥æ•°æ®é›†åŒ…å«1äº¿å¯¹å›¾åƒå’Œæ–‡æœ¬ï¼Œç»è¿‡ä¸¥æ ¼ç­›é€‰ï¼Œç¡®ä¿äº†æ•°æ®è´¨é‡çš„ä¼˜è¶Šæ€§ã€‚DanQingä¸»è¦åŸºäº2024-2025å¹´çš„ç½‘ç»œæ•°æ®ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¯­ä¹‰è¶‹åŠ¿ï¼Œä¸ºå®é™…åº”ç”¨æä¾›æ›´å¤§çš„ä»·å€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10332",
            "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
            "url": "https://huggingface.co/papers/2601.10332",
            "abstract": "Text-to-image diffusion models enhanced with language model reasoning capabilities achieve improved factual consistency and semantic alignment through a think-then-generate paradigm with dual-gradient reinforcement optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.",
            "score": 16,
            "issue_id": 610,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "3795f149cc03f38f",
            "authors": [
                "Siqi Kou",
                "Jiachun Jin",
                "Zetong Zhou",
                "Ye Ma",
                "Yugang Wang",
                "Quan Chen",
                "Peng Jiang",
                "Xiao Yang",
                "Jun Zhu",
                "Kai Yu",
                "Zhijie Deng"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10332.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#rlhf",
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœÑ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸ĞµĞ¼: ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ³Ğ´Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚, Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ¿ĞµÑ€ĞµĞ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ \"think-then-generate\", Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ LLM Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Dual-GRPO, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Image Generation with Reasoning Power",
                    "desc": "This paper introduces a new approach to text-to-image diffusion models that enhances their ability to generate images by incorporating reasoning capabilities from large language models. The proposed think-then-generate (T2G) paradigm allows the model to first analyze and rewrite user prompts before generating images, improving the alignment between text and visuals. By using dual-gradient reinforcement optimization, the model is trained to produce images that are not only visually coherent but also semantically accurate. The results demonstrate significant advancements in factual consistency and realism, suggesting a move towards more integrated models that combine reasoning and visual generation."
                },
                "zh": {
                    "title": "æ¨ç†ä¸ç”Ÿæˆçš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒçš„äº‹å®ä¸€è‡´æ€§å’Œè¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬å¼•å…¥äº†â€œå…ˆæ€è€ƒåç”Ÿæˆâ€çš„èŒƒå¼ï¼Œé¼“åŠ±è¯­è¨€æ¨¡å‹å¯¹ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œæ¨ç†å’Œé‡å†™ï¼Œä»è€Œæ”¹å–„ç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚é€šè¿‡åŒæ¢¯åº¦å¼ºåŒ–ä¼˜åŒ–ï¼Œæˆ‘ä»¬ç¡®ä¿äº†è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„å…±åŒä¼˜åŒ–ï¼Œä½¿å¾—ç”Ÿæˆçš„å›¾åƒåœ¨è¯­ä¹‰ä¸Šæ›´åŠ ä¸€è‡´ä¸”è§†è§‰ä¸Šæ›´å…·è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†ç”Ÿæˆå›¾åƒçš„çœŸå®æ„Ÿå’Œä¸€è‡´æ€§ï¼Œå±•ç¤ºäº†ä¸‹ä¸€ä»£ç»Ÿä¸€æ¨¡å‹çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10061",
            "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2601.10061",
            "abstract": "Chain-of-Frame reasoning is integrated into text-to-image generation through progressive visual refinement with explicit intermediate steps, achieving superior performance on benchmark datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.",
            "score": 16,
            "issue_id": 610,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "4386e88394aa8c6c",
            "authors": [
                "Chengzhuo Tong",
                "Mingkun Chang",
                "Shenglong Zhang",
                "Yuran Wang",
                "Cheng Liang",
                "Zhizheng Zhao",
                "Ruichuan An",
                "Bohan Zeng",
                "Yang Shi",
                "Yifan Dai",
                "Ziming Zhao",
                "Guanbin Li",
                "Pengfei Wan",
                "Yuanxing Zhang",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "Nanjing University",
                "Peking University",
                "Sun Yat-sen University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10061.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸĞ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Chain-of-Frame Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CoF-T2I, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ°Ğ´Ñ€ ĞºĞ°Ğº Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CoF-Evol-Instruct Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğº ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ."
                },
                "en": {
                    "title": "Enhancing Text-to-Image Generation with Chain-of-Frame Reasoning",
                    "desc": "This paper introduces CoF-T2I, a novel model that incorporates Chain-of-Frame (CoF) reasoning into text-to-image (T2I) generation. By using progressive visual refinement, the model generates images through a series of intermediate frames that serve as explicit reasoning steps. The authors also present a new dataset, CoF-Evol-Instruct, which captures the evolution of visual generation from semantic concepts to aesthetic outputs. Experimental results demonstrate that CoF-T2I outperforms existing video models and achieves strong performance on benchmark tests, highlighting the potential of video reasoning techniques in enhancing T2I tasks."
                },
                "zh": {
                    "title": "é“¾å¸§æ¨ç†åŠ©åŠ›æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCoF-T2Içš„æ¨¡å‹ï¼Œå°†é“¾å¸§æ¨ç†ï¼ˆChain-of-Frame reasoningï¼‰æ•´åˆåˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆtext-to-image generationï¼‰ä¸­ã€‚è¯¥æ¨¡å‹é€šè¿‡é€æ­¥çš„è§†è§‰ç»†åŒ–è¿‡ç¨‹ï¼Œåˆ©ç”¨ä¸­é—´å¸§ä½œä¸ºæ˜ç¡®çš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†CoF-Evol-Instructæ•°æ®é›†ï¼Œä»¥æ”¯æŒä»è¯­ä¹‰åˆ°ç¾å­¦çš„ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoF-T2Iåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºè§†é¢‘æ¨¡å‹åœ¨é«˜è´¨é‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å·¨å¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10527",
            "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
            "url": "https://huggingface.co/papers/2601.10527",
            "abstract": "Frontier language and vision models show varied safety performance across different evaluation criteria, highlighting the need for comprehensive, standardized safety assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
            "score": 14,
            "issue_id": 610,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "fc0d323ab183caeb",
            "authors": [
                "Xingjun Ma",
                "Yixu Wang",
                "Hengyuan Xu",
                "Yutao Wu",
                "Yifan Ding",
                "Yunhan Zhao",
                "Zilong Wang",
                "Jiabin Hua",
                "Ming Wen",
                "Jianan Liu",
                "Ranjie Duan",
                "Yifeng Gao",
                "Yingshui Tan",
                "Yunhao Chen",
                "Hui Xue",
                "Xin Wang",
                "Wei Cheng",
                "Jingjing Chen",
                "Zuxuan Wu",
                "Bo Li",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "Deakin University",
                "Fudan University",
                "Shanghai Innovation institute",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10527.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#ethics",
                    "#multimodal",
                    "#multilingual",
                    "#survey"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ°Ñ…: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ½ÑƒĞ¶Ğ½Ñ‹ ĞµĞ´Ğ¸Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞµĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸, ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°ĞºĞ°Ñ… Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Safety in AI Models: A Multidimensional Challenge",
                    "desc": "This paper evaluates the safety performance of advanced language and vision models, revealing that their safety varies significantly across different evaluation criteria. The authors assess seven leading models using a unified protocol that includes various evaluation methods, highlighting the inconsistent safety profiles among them. While some models, like GPT-5.2, perform well across multiple safety dimensions, others show weaknesses, particularly under adversarial conditions. The findings emphasize the complexity of safety in these models and the necessity for standardized assessments to better understand and mitigate real-world risks."
                },
                "zh": {
                    "title": "å‰æ²¿æ¨¡å‹å®‰å…¨æ€§è¯„ä¼°çš„å¿…è¦æ€§",
                    "desc": "å‰æ²¿è¯­è¨€å’Œè§†è§‰æ¨¡å‹åœ¨ä¸åŒè¯„ä¼°æ ‡å‡†ä¸‹çš„å®‰å…¨æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œæ˜¾ç¤ºå‡ºå…¨é¢ã€æ ‡å‡†åŒ–å®‰å…¨è¯„ä¼°çš„å¿…è¦æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•åœ¨æ¨ç†ã€æ„ŸçŸ¥å’Œç”Ÿæˆèƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™äº›è¿›å±•æ˜¯å¦èƒ½å¸¦æ¥ç›¸åº”çš„å®‰å…¨æ€§æå‡ä»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬å¯¹ä¸ƒä¸ªå‰æ²¿æ¨¡å‹è¿›è¡Œäº†ç»¼åˆå®‰å…¨è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå®‰å…¨æ€§è¡¨ç°å­˜åœ¨æ˜æ˜¾å·®å¼‚ï¼Œå°¤å…¶åœ¨å¯¹æŠ—æ€§è¯„ä¼°ä¸­ï¼Œæ‰€æœ‰æ¨¡å‹çš„è¡¨ç°éƒ½æ˜¾è‘—ä¸‹é™ã€‚æ•´ä½“è€Œè¨€ï¼Œè¿™äº›ç»“æœè¡¨æ˜ï¼Œå‰æ²¿æ¨¡å‹çš„å®‰å…¨æ€§æ˜¯å¤šç»´çš„ï¼Œå—åˆ°æ¨¡æ€ã€è¯­è¨€å’Œè¯„ä¼°æ–¹æ¡ˆçš„å½±å“ï¼Œå¼ºè°ƒäº†æ ‡å‡†åŒ–å®‰å…¨è¯„ä¼°çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10712",
            "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
            "url": "https://huggingface.co/papers/2601.10712",
            "abstract": "MatchTIR enhances LLM reasoning by introducing fine-grained credit assignment through bipartite matching and dual-level advantage estimation for tool-integrated tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.",
            "score": 12,
            "issue_id": 610,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "9a506aa7bea3ff82",
            "authors": [
                "Changle Qu",
                "Sunhao Dai",
                "Hengyi Cai",
                "Jun Xu",
                "Shuaiqiang Wang",
                "Dawei Yin"
            ],
            "affiliations": [
                "Baidu Inc.",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10712.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#training",
                    "#small_models"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°ÑĞ»ÑƒĞ³ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ¸partite matching Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² LLM",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ MatchTIR Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ»ĞºĞ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ±Ğ¸partite matching Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ…ĞµĞ¼Ñƒ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ ÑƒÑĞ¿ĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ 4B Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MatchTIR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Refining LLM Reasoning with Fine-Grained Credit Assignment",
                    "desc": "MatchTIR is a novel framework designed to improve the reasoning capabilities of large language models (LLMs) when interacting with external tools. It addresses the limitations of traditional reinforcement learning methods that provide uniform rewards across all steps, which can obscure the effectiveness of specific tool interactions. By employing bipartite matching for fine-grained credit assignment, MatchTIR differentiates between successful and unsuccessful tool calls, enhancing the model's learning process. Additionally, the dual-level advantage estimation balances immediate step performance with overall task success, leading to improved outcomes in complex, multi-turn scenarios."
                },
                "zh": {
                    "title": "MatchTIRï¼šç»†ç²’åº¦ä¿¡ç”¨åˆ†é…æå‡LLMæ¨ç†èƒ½åŠ›",
                    "desc": "MatchTIR æ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç»†ç²’åº¦çš„ä¿¡ç”¨åˆ†é…æ¥å¤„ç†å·¥å…·é›†æˆä»»åŠ¡ã€‚å®ƒé‡‡ç”¨äºŒåˆ†åŒ¹é…å’ŒåŒå±‚ä¼˜åŠ¿ä¼°è®¡ï¼Œè§£å†³äº†ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é•¿æ—¶é—´å¤šè½®åœºæ™¯ä¸­æ— æ³•æœ‰æ•ˆåŒºåˆ†æœ‰æ•ˆå·¥å…·è°ƒç”¨ä¸å†—ä½™æˆ–é”™è¯¯è°ƒç”¨çš„é—®é¢˜ã€‚MatchTIR å°†ä¿¡ç”¨åˆ†é…è§†ä¸ºä¸€ä¸ªäºŒåˆ†åŒ¹é…é—®é¢˜ï¼Œåˆ©ç”¨é¢„æµ‹å’ŒçœŸå®è½¨è¿¹ä¹‹é—´çš„åŒ¹é…æ¥ç”Ÿæˆå¯†é›†çš„è½®æ¬¡çº§å¥–åŠ±ã€‚é€šè¿‡ç»“åˆå±€éƒ¨æ­¥éª¤ç²¾åº¦å’Œå…¨å±€ä»»åŠ¡æˆåŠŸï¼ŒMatchTIR æä¾›äº†æ›´ç²¾ç¡®çš„å¥–åŠ±åˆ†é…ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10156",
            "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
            "url": "https://huggingface.co/papers/2601.10156",
            "abstract": "A guardrail model and reasoning framework are developed to detect and prevent unsafe tool invocations in LLM agents, improving both safety and task performance under adversarial conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.",
            "score": 10,
            "issue_id": 611,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "23ba9d146be279ff",
            "authors": [
                "Yutao Mou",
                "Zhangchi Xue",
                "Lijun Li",
                "Peiyang Liu",
                "Shikun Zhang",
                "Wei Ye",
                "Jing Shao"
            ],
            "affiliations": [
                "National Engineering Research Center for Software Engineering, Peking University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10156.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#rl",
                    "#agents",
                    "#security",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ TS-Guard Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ñ… Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TS-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº TS-Flow, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ guardrail-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼Ğ¸. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 65% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 10% Ğ¿Ñ€Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ°Ñ… Ñ‡ĞµÑ€ĞµĞ· prompt injection."
                },
                "en": {
                    "title": "Enhancing Safety in LLM Agents with Proactive Guardrails",
                    "desc": "This paper presents a guardrail model and reasoning framework designed to enhance the safety of large language model (LLM) agents when they invoke external tools. The authors introduce TS-Bench, a benchmark for evaluating the safety of tool invocations at each step of the agent's actions. The TS-Guard model utilizes multi-task reinforcement learning to identify and prevent unsafe tool invocations by analyzing the agent's interaction history. Additionally, the TS-Flow framework significantly reduces harmful actions while improving the overall task performance of LLM agents under adversarial conditions."
                },
                "zh": {
                    "title": "æå‡LLMä»£ç†å®‰å…¨æ€§çš„æŠ¤æ æ¨¡å‹ä¸æ¨ç†æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¿æŠ¤æ¨¡å‹å’Œæ¨ç†æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹å’Œé˜²æ­¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ä¸­çš„ä¸å®‰å…¨å·¥å…·è°ƒç”¨ï¼Œä»è€Œåœ¨å¯¹æŠ—æ€§æ¡ä»¶ä¸‹æé«˜å®‰å…¨æ€§å’Œä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†TS-Benchï¼Œç”¨äºé€æ­¥å·¥å…·è°ƒç”¨å®‰å…¨æ£€æµ‹ã€‚æ¥ç€ï¼Œå¼€å‘äº†TS-Guardæ¨¡å‹ï¼Œåˆ©ç”¨å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼Œèƒ½å¤Ÿåœ¨æ‰§è¡Œå‰ä¸»åŠ¨æ£€æµ‹ä¸å®‰å…¨çš„å·¥å…·è°ƒç”¨è¡Œä¸ºã€‚æœ€åï¼Œä»‹ç»äº†TS-Flowæ¨ç†æ¡†æ¶ï¼Œæ˜¾è‘—å‡å°‘äº†ReActé£æ ¼ä»£ç†çš„æœ‰å®³å·¥å…·è°ƒç”¨ï¼Œå¹¶åœ¨æç¤ºæ³¨å…¥æ”»å‡»ä¸‹æé«˜äº†è‰¯æ€§ä»»åŠ¡çš„å®Œæˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10103",
            "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
            "url": "https://huggingface.co/papers/2601.10103",
            "abstract": "FlowAct-R1 enables real-time interactive humanoid video generation with high-fidelity synthesis and low-latency responsiveness through MMDiT architecture and chunkwise diffusion forcing strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.",
            "score": 9,
            "issue_id": 610,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "c14351d1e6e66f86",
            "authors": [
                "Lizhen Wang",
                "Yongming Zhu",
                "Zhipeng Ge",
                "Youwei Zheng",
                "Longhao Zhang",
                "Tianshu Hu",
                "Shiyang Qin",
                "Mingshuang Luo",
                "Jiaxu Zhang",
                "Xin Chen",
                "Yulong Wang",
                "Zerong Zheng",
                "Jianwen Jiang",
                "Chao Liang",
                "Weifeng Chen",
                "Xing Wang",
                "Yuan Zhang",
                "Mingyuan Gao"
            ],
            "affiliations": [
                "Bytedance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10103.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#inference",
                    "#video",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "FlowAct-R1 â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ñ€Ğ° (MMDiT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ» Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ (chunkwise diffusion forcing), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… 25 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ 480p Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾ĞºĞ¾Ğ»Ğ¾ 1,5 ÑĞµĞºÑƒĞ½Ğ´ Ğ´Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ñ‚ĞµĞ»Ğ¾Ğ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Real-Time Interactive Humanoid Video Generation Made Easy!",
                    "desc": "FlowAct-R1 is a framework designed for generating interactive humanoid videos in real-time with high visual quality. It uses a MMDiT architecture to allow for video synthesis of any length while keeping the response time low. The framework incorporates a chunkwise diffusion forcing strategy to minimize errors and maintain consistency during long interactions. With optimizations, it can produce stable video at 25 frames per second and quickly start displaying the first frame, enabling lifelike interactions with users."
                },
                "zh": {
                    "title": "å®æ—¶äº’åŠ¨ç±»äººè§†é¢‘ç”Ÿæˆçš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "FlowAct-R1 æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå®æ—¶äº’åŠ¨ç±»äººè§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°é«˜ä¿çœŸåˆæˆå’Œä½å»¶è¿Ÿå“åº”ã€‚è¯¥æ¡†æ¶åŸºäº MMDiT æ¶æ„ï¼Œæ”¯æŒä»»æ„æ—¶é•¿çš„è§†é¢‘æµåˆæˆï¼ŒåŒæ—¶ä¿æŒä½å»¶è¿Ÿã€‚é€šè¿‡å¼•å…¥åˆ†å—æ‰©æ•£å¼ºåˆ¶ç­–ç•¥å’Œæ–°é¢–çš„è‡ªæˆ‘å¼ºåˆ¶å˜ä½“ï¼ŒFlowAct-R1 èƒ½æœ‰æ•ˆå‡å°‘é”™è¯¯ç´¯ç§¯ï¼Œç¡®ä¿åœ¨æŒç»­äº’åŠ¨ä¸­çš„é•¿æœŸæ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–è§’è‰²é£æ ¼ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå®ç°ç”ŸåŠ¨çš„è¡Œä¸ºè¡¨ç°å’Œæ„ŸçŸ¥çœŸå®æ„Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10611",
            "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
            "url": "https://huggingface.co/papers/2601.10611",
            "abstract": "Molmo2 is a new open-source video-language model family that achieves state-of-the-art performance through novel datasets and training methods, particularly excelling in video grounding tasks without relying on proprietary models.  \t\t\t\t\tAI-generated summary \t\t\t\t Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).",
            "score": 6,
            "issue_id": 610,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "cc00c73dd80a271e",
            "authors": [
                "Christopher Clark",
                "Jieyu Zhang",
                "Zixian Ma",
                "Jae Sung Park",
                "Mohammadreza Salehi",
                "Rohun Tripathi",
                "Sangho Lee",
                "Zhongzheng Ren",
                "Chris Dongjoo Kim",
                "Yinuo Yang",
                "Vincent Shao",
                "Yue Yang",
                "Weikai Huang",
                "Ziqi Gao",
                "Taira Anderson",
                "Jianrui Zhang",
                "Jitesh Jain",
                "George Stoica",
                "Winson Han",
                "Ali Farhadi",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10611.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#benchmark",
                    "#synthetic",
                    "#multimodal",
                    "#open_source",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Molmo2 â€” ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ 7 Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ 2 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ â€” ÑÑ‚Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° video grounding, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ½Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ¸Ñ… Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. 8B Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Molmo2 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Gemini 3 Pro Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… video pointing Ğ¸ video tracking."
                },
                "en": {
                    "title": "Molmo2: Redefining Open-Source Video-Language Models",
                    "desc": "Molmo2 is a new family of open-source video-language models that achieves top performance in video grounding tasks. It introduces innovative datasets and training methods, allowing it to excel without relying on proprietary models. The model includes seven new video datasets and two multi-image datasets, enhancing its capabilities in tasks like object tracking and video Q&A. With a unique training approach, Molmo2 outperforms existing open-weight models and even competes with some proprietary models in various video tasks."
                },
                "zh": {
                    "title": "Molmo2ï¼šå¼€æºè§†é¢‘è¯­è¨€æ¨¡å‹çš„æ–°çªç ´",
                    "desc": "Molmo2æ˜¯ä¸€ç§æ–°çš„å¼€æºè§†é¢‘è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œé€šè¿‡åˆ›æ–°çš„æ•°æ®é›†å’Œè®­ç»ƒæ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè€Œä¸ä¾èµ–äºä¸“æœ‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹æä¾›äº†7ä¸ªæ–°çš„è§†é¢‘æ•°æ®é›†å’Œ2ä¸ªå¤šå›¾åƒæ•°æ®é›†ï¼Œæ”¯æŒé«˜è´¨é‡çš„è§†é¢‘ç†è§£å’Œå®šä½èƒ½åŠ›ã€‚Molmo2åœ¨çŸ­è§†é¢‘è®¡æ•°å’Œå­—å¹•ç”Ÿæˆç­‰ä»»åŠ¡ä¸­è¶…è¶Šäº†å…¶ä»–å¼€æºæ¨¡å‹ï¼Œå¹¶åœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¿‡äº†ä¸“æœ‰æ¨¡å‹ã€‚é€šè¿‡é«˜æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆå’ŒåŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼ŒMolmo2å±•ç¤ºäº†åœ¨è§†é¢‘è¯­è¨€å¤„ç†é¢†åŸŸçš„å¼ºå¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.09881",
            "title": "Transition Matching Distillation for Fast Video Generation",
            "url": "https://huggingface.co/papers/2601.09881",
            "abstract": "Transition Matching Distillation enables efficient video generation by distilling diffusion models into few-step predictors using conditional flows and semantic representation decomposition.  \t\t\t\t\tAI-generated summary \t\t\t\t Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd",
            "score": 4,
            "issue_id": 610,
            "pub_date": "2026-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "f7b4d9c997525867",
            "authors": [
                "Weili Nie",
                "Julius Berner",
                "Nanye Ma",
                "Chao Liu",
                "Saining Xie",
                "Arash Vahdat"
            ],
            "affiliations": [
                "NVIDIA",
                "NYU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.09881.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#diffusion",
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Transition Matching Distillation (TMD) Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸: Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ±Ğ°ĞºĞ±Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñƒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TMD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Efficient Video Generation with Transition Matching Distillation",
                    "desc": "Transition Matching Distillation (TMD) is a new method that improves video generation by simplifying the process of using diffusion models. It does this by transforming complex multi-step sampling into a more efficient few-step prediction using conditional flows. The approach involves breaking down the diffusion model into two parts: a backbone for extracting important features and a flow head for refining these features. TMD has been shown to generate high-quality videos faster while maintaining visual fidelity and adherence to prompts, outperforming other models in similar conditions."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•ï¼šè¿‡æ¸¡åŒ¹é…è’¸é¦",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè¿‡æ¸¡åŒ¹é…è’¸é¦ï¼ˆTMDï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å°†è§†é¢‘æ‰©æ•£æ¨¡å‹è’¸é¦ä¸ºé«˜æ•ˆçš„å°‘æ­¥ç”Ÿæˆå™¨ã€‚TMDçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ‰©æ•£æ¨¡å‹çš„å¤šæ­¥å»å™ªè½¨è¿¹ä¸å°‘æ­¥æ¦‚ç‡è¿‡æ¸¡è¿‡ç¨‹è¿›è¡ŒåŒ¹é…ã€‚é€šè¿‡å°†åŸå§‹æ‰©æ•£æ¨¡å‹åˆ†è§£ä¸ºä¸»è¦éª¨å¹²å’Œæµå¤´ä¸¤ä¸ªéƒ¨åˆ†ï¼ŒTMDèƒ½å¤Ÿæœ‰æ•ˆæå–è¯­ä¹‰è¡¨ç¤ºå¹¶è¿›è¡Œå¤šæ¬¡å†…éƒ¨æµæ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTMDåœ¨ç”Ÿæˆé€Ÿåº¦å’Œè§†è§‰è´¨é‡ä¹‹é—´æä¾›äº†çµæ´»ä¸”å¼ºå¤§çš„å¹³è¡¡ï¼Œä¼˜äºç°æœ‰çš„è’¸é¦æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.09142",
            "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
            "url": "https://huggingface.co/papers/2601.09142",
            "abstract": "EvasionBench introduces a large-scale benchmark for detecting evasive responses in earnings calls using a multi-model annotation framework that leverages disagreement between advanced language models to identify challenging examples, resulting in a highly accurate model with significantly reduced inference costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 human-annotated test samples (Cohen's Kappa 0.835) across three evasion levels. Our key contribution is a multi-model annotation framework leveraging a core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using a judge to resolve labels. This approach outperforms single-model distillation by 2.4 percent, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3 percent accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at a fraction of inference cost.",
            "score": 4,
            "issue_id": 610,
            "pub_date": "2026-01-14",
            "pub_date_card": {
                "ru": "14 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 14",
                "zh": "1æœˆ14æ—¥"
            },
            "hash": "8e4e9d937a122b6f",
            "authors": [
                "Shijian Ma",
                "Yan Lin",
                "Yi Yang"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.09142.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#small_models",
                    "#dataset"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ñ‹ â€” ÑƒÑ‡Ğ¸Ğ¼ÑÑ Ğ½Ğ° ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EvasionBench â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑƒĞºĞ»Ğ¾Ğ½Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ñ‹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ†ĞµĞ½Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ½ĞµÑĞ²Ğ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ° 2.4 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ°. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Eva-4B Ñ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 81.3 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ½Ğ° 25 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "EvasionBench: Enhancing Evasive Response Detection in Earnings Calls",
                    "desc": "EvasionBench is a new benchmark designed to improve the detection of evasive responses in earnings calls, which is important for maintaining financial transparency. It includes a large dataset of 30,000 training samples and 1,000 human-annotated test samples, ensuring high-quality annotations with a Cohen's Kappa score of 0.835. The innovative multi-model annotation framework utilizes the disagreement between advanced language models to identify challenging examples, which enhances the training process. The resulting model, Eva-4B, achieves an impressive accuracy of 81.3%, significantly outperforming previous models while reducing inference costs."
                },
                "zh": {
                    "title": "EvasionBenchï¼šæå‡è´¢æŠ¥é€æ˜åº¦çš„å›é¿æ€§å›ç­”æ£€æµ‹åŸºå‡†",
                    "desc": "EvasionBenchæ˜¯ä¸€ä¸ªç”¨äºæ£€æµ‹è´¢æŠ¥ç”µè¯ä¼šè®®ä¸­å›é¿æ€§å›ç­”çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«30,000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ1,000ä¸ªç»è¿‡äººå·¥æ ‡æ³¨çš„æµ‹è¯•æ ·æœ¬ï¼Œæ¶µç›–ä¸‰ç§å›é¿çº§åˆ«ã€‚è¯¥ç ”ç©¶çš„å…³é”®è´¡çŒ®æ˜¯åˆ©ç”¨å¤šæ¨¡å‹æ ‡æ³¨æ¡†æ¶ï¼Œé€šè¿‡å…ˆè¿›è¯­è¨€æ¨¡å‹ä¹‹é—´çš„åˆ†æ­§æ¥è¯†åˆ«éš¾ä¾‹ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æœ€ç»ˆï¼Œè®­ç»ƒå‡ºçš„Eva-4Bæ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šè¾¾åˆ°äº†81.3%ï¼Œå¹¶ä¸”åœ¨æ¨ç†æˆæœ¬ä¸Šæ˜¾è‘—é™ä½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10592",
            "title": "Action100M: A Large-scale Video Action Dataset",
            "url": "https://huggingface.co/papers/2601.10592",
            "abstract": "Action100M is a large-scale video action dataset constructed from internet instructional videos using automated pipelines with V-JEPA embeddings and GPT-based reasoning for structured annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.",
            "score": 3,
            "issue_id": 610,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "f279a476fac31098",
            "authors": [
                "Delong Chen",
                "Tejaswi Kasarla",
                "Yejin Bang",
                "Mustafa Shukor",
                "Willy Chung",
                "Jade Yu",
                "Allen Bolourchi",
                "Theo Moutakanni",
                "Pascale Fung"
            ],
            "affiliations": [
                "HKUST",
                "Meta FAIR",
                "Sorbonne UniversitÃ©",
                "University of Amsterdam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10592.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ¾Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Action100M â€” ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ· 1.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ² Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ V-JEPA 2 ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ GPT-OSS-120B Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ°ĞºÑ‚ĞµÑ€Ğµ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´Ñ€ĞµĞ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ VL-JEPA Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ zero-shot Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Action100M ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "Unlocking Action Recognition with Action100M",
                    "desc": "Action100M is a comprehensive video action dataset created from 1.2 million instructional videos sourced from the internet, totaling 14.6 years of content. It features over 100 million segments that are temporally localized and annotated with open-vocabulary action labels and detailed captions. The dataset is generated through an automated process that utilizes V-JEPA embeddings for hierarchical segmentation and a GPT-based model for structured annotation. Training on Action100M has shown significant improvements in action recognition tasks, making it a valuable resource for advancing video understanding and machine learning research."
                },
                "zh": {
                    "title": "Action100Mï¼šè§†é¢‘ç†è§£çš„æ–°åŸºç¡€",
                    "desc": "Action100Mæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„è§†é¢‘åŠ¨ä½œæ•°æ®é›†ï¼Œæ¥æºäºäº’è”ç½‘çš„æ•™å­¦è§†é¢‘ï¼Œä½¿ç”¨è‡ªåŠ¨åŒ–æµç¨‹æ„å»ºï¼Œç»“åˆäº†V-JEPAåµŒå…¥å’ŒåŸºäºGPTçš„æ¨ç†è¿›è¡Œç»“æ„åŒ–æ³¨é‡Šã€‚è¯¥æ•°æ®é›†åŒ…å«120ä¸‡æ®µè§†é¢‘ï¼Œæ€»æ—¶é•¿è¾¾åˆ°14.6å¹´ï¼Œç”Ÿæˆäº†è¶…è¿‡1äº¿ä¸ªæ—¶é—´å®šä½çš„åŠ¨ä½œç‰‡æ®µï¼Œå¹¶æä¾›å¼€æ”¾è¯æ±‡çš„åŠ¨ä½œç›‘ç£å’Œä¸°å¯Œçš„æè¿°ã€‚æ•°æ®é›†çš„ç”Ÿæˆè¿‡ç¨‹åŒ…æ‹¬ä½¿ç”¨V-JEPA 2åµŒå…¥è¿›è¡Œåˆ†å±‚æ—¶é—´åˆ†å‰²ï¼Œåˆ›å»ºå¤šå±‚æ¬¡çš„å¸§å’Œç‰‡æ®µæ ‡é¢˜ï¼Œå¹¶é€šè¿‡å¤šè½®è‡ªæˆ‘ç²¾ç‚¼ç¨‹åºèšåˆè¯æ®ä»¥è¾“å‡ºç»“æ„åŒ–æ³¨é‡Šã€‚é€šè¿‡åœ¨Action100Mä¸Šè®­ç»ƒVL-JEPAï¼Œå±•ç¤ºäº†æ•°æ®è§„æ¨¡çš„æŒç»­æ”¹è¿›å’Œåœ¨å¤šæ ·åŒ–åŠ¨ä½œè¯†åˆ«åŸºå‡†ä¸Šçš„å¼ºå¤§é›¶æ ·æœ¬æ€§èƒ½ï¼Œç¡®ç«‹äº†Action100Mä½œä¸ºè§†é¢‘ç†è§£å’Œä¸–ç•Œå»ºæ¨¡ç ”ç©¶çš„æ–°åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10553",
            "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
            "url": "https://huggingface.co/papers/2601.10553",
            "abstract": "Latent world models enhance video generation physics plausibility through inference-time alignment and trajectory steering, achieving superior performance in challenging benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.",
            "score": 2,
            "issue_id": 610,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "e499c674410a0a40",
            "authors": [
                "Jianhao Yuan",
                "Xiaofeng Zhang",
                "Felix Friedrich",
                "Nicolas Beltran-Velez",
                "Melissa Hall",
                "Reyhane Askari-Hemmat",
                "Xiaochuang Han",
                "Nicolas Ballas",
                "Michal Drozdzal",
                "Adriana Romero-Soriano"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Columbia University",
                "FAIR, Meta Superintelligence Labs",
                "McGill University",
                "Mila - QuÃ©bec AI Institute",
                "University of Oxford",
                "UniversitÃ© de MontrÃ©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10553.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#inference"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸ĞºĞ° Ğ² Ñ„Ğ¾ĞºÑƒÑĞµ: Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ WMReward â€” Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ prior Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ VJEPA-2 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ğ¾ Ğ²ÑĞµÑ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼ Ğ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑÑ‚Ğ° Ğ² ĞºĞ¾Ğ½ĞºÑƒÑ€ÑĞµ ICCV 2025 Perception Test PhysicsIQ Challenge Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 7.42% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ best-of-class Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing Video Generation with Physics-Plausible Latent Models",
                    "desc": "This paper presents a method to enhance the physics realism of AI-generated videos by using latent world models. The authors identify that the lack of adherence to physical laws in video generation is not only due to poor pre-training but also due to ineffective inference strategies. They introduce a new approach called WMReward, which aligns the generation process with physics principles during inference by steering candidate trajectories based on a latent world model's physics knowledge. Their method shows significant improvements in generating videos that are more physically plausible, achieving top results in a competitive benchmark."
                },
                "zh": {
                    "title": "åˆ©ç”¨æ½œåœ¨ä¸–ç•Œæ¨¡å‹æå‡è§†é¢‘ç”Ÿæˆçš„ç‰©ç†åˆç†æ€§",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡æ½œåœ¨ä¸–ç•Œæ¨¡å‹æ¥å¢å¼ºè§†é¢‘ç”Ÿæˆçš„ç‰©ç†åˆç†æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨æ¨ç†æ—¶çš„ç­–ç•¥ä¸è¶³æ˜¯å¯¼è‡´ç‰©ç†ä¸åˆç†çš„ä¸»è¦åŸå› ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†WMRewardï¼Œå°†æé«˜è§†é¢‘ç”Ÿæˆçš„ç‰©ç†åˆç†æ€§è§†ä¸ºä¸€ä¸ªæ¨ç†æ—¶å¯¹é½é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªç”Ÿæˆè®¾ç½®ä¸­æ˜¾è‘—æ”¹å–„äº†ç‰©ç†åˆç†æ€§ï¼Œå¹¶åœ¨ICC 2025 Perception Test PhysicsIQ Challengeä¸­è·å¾—ç¬¬ä¸€åã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10129",
            "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
            "url": "https://huggingface.co/papers/2601.10129",
            "abstract": "LaViT addresses the perception gap in multimodal reasoning by aligning latent visual thoughts through autoregressive reconstruction of visual semantics and attention trajectories, improving visual grounding and model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.",
            "score": 2,
            "issue_id": 611,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "116dda5e12789200",
            "authors": [
                "Linquan Wu",
                "Tianxiang Jiang",
                "Yifei Dong",
                "Haoyu Yang",
                "Fengji Zhang",
                "Shichaang Meng",
                "Ai Xuan",
                "Linqi Song",
                "Jacky Keung"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "University of Electronic Science and Technology of China",
                "University of Science and Technology of China",
                "Utrecht University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10129.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#small_models",
                    "#training"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ LaViT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ¡Ñ‚ÑƒĞ´ĞµĞ½Ñ‚-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ curriculum learning Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ´Ğ¾ 16.9% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ 3-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4o."
                },
                "en": {
                    "title": "Bridging the Perception Gap in Multimodal Reasoning",
                    "desc": "LaViT is a framework designed to improve multimodal reasoning by addressing the perception gap between visual and textual information. It focuses on aligning latent visual thoughts through autoregressive reconstruction, which helps the model better understand visual semantics and attention dynamics. By ensuring that student models learn to reconstruct the teacher's visual attention before generating text, LaViT reduces reliance on language priors and enhances visual grounding. The results show significant performance improvements on complex reasoning tasks, allowing smaller models to outperform larger ones."
                },
                "zh": {
                    "title": "LaViTï¼šç¼©å°å¤šæ¨¡æ€æ¨ç†çš„æ„ŸçŸ¥å·®è·",
                    "desc": "LaViT æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„æ„ŸçŸ¥å·®è·é—®é¢˜ã€‚å®ƒé€šè¿‡è‡ªå›å½’é‡å»ºè§†è§‰è¯­ä¹‰å’Œæ³¨æ„åŠ›è½¨è¿¹æ¥å¯¹é½æ½œåœ¨çš„è§†è§‰æ€ç»´ï¼Œä»è€Œæé«˜è§†è§‰åŸºç¡€å’Œæ¨¡å‹æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒLaViT å¼ºè°ƒå­¦ç”Ÿæ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬ä¹‹å‰ï¼Œå¿…é¡»é‡å»ºæ•™å¸ˆæ¨¡å‹çš„è§†è§‰ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–è¯­è¨€å…ˆéªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaViT åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œç”šè‡³ä½¿å¾—ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹è¶…è¶Šäº†æ›´å¤§çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10201",
            "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
            "url": "https://huggingface.co/papers/2601.10201",
            "abstract": "Process Reward Learning decomposes reinforcement learning objectives into intermediate steps to provide fine-grained supervision for improving large language model reasoning abilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.",
            "score": 1,
            "issue_id": 611,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "ab07b6201b05fb06",
            "authors": [
                "Jiarui Yao",
                "Ruida Wang",
                "Tong Zhang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10201.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞŸĞ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ‚ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğº Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Process Reward Learning (PRL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PRL ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚ĞµĞ½ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ¾Ğ¼ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ ĞšÑƒĞ»ÑŒĞ±Ğ°ĞºĞ°-Ğ›ĞµĞ¹Ğ±Ğ»ĞµÑ€Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Decomposing Rewards for Enhanced Reasoning in LLMs",
                    "desc": "Process Reward Learning (PRL) enhances the reasoning capabilities of Large Language Models (LLMs) by breaking down reinforcement learning objectives into smaller, manageable steps. This approach provides fine-grained supervision, addressing the limitations of traditional methods that rely solely on outcome rewards. By introducing rigorous process rewards, PRL optimizes the training efficiency without the need for complex additional steps like Monte Carlo Tree Search. Experimental results indicate that PRL not only boosts average performance in reasoning tasks but also expands the models' reasoning capabilities significantly."
                },
                "zh": {
                    "title": "è¿‡ç¨‹å¥–åŠ±å­¦ä¹ ï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºè¿‡ç¨‹å¥–åŠ±å­¦ä¹ ï¼ˆPRLï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚PRLé€šè¿‡å°†å¼ºåŒ–å­¦ä¹ ç›®æ ‡åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤ï¼Œæä¾›äº†ç»†ç²’åº¦çš„ç›‘ç£ä¿¡å·ï¼Œä»è€Œæ”¹å–„äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒPRLèƒ½å¤Ÿå°†ç»“æœå¥–åŠ±è½¬åŒ–ä¸ºè¿‡ç¨‹ç›‘ç£ä¿¡å·ï¼Œæ›´å¥½åœ°æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRLä¸ä»…æé«˜äº†LLMsçš„å¹³å‡æ¨ç†æ€§èƒ½ï¼Œè¿˜æ‰©å±•äº†æ¨ç†çš„è¾¹ç•Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.10080",
            "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
            "url": "https://huggingface.co/papers/2601.10080",
            "abstract": "Executable and interpretable decision trees are induced from narrative data to create robust behavioral profiles for role-playing agents, outperforming traditional methods in consistency and reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on 85 characters across 16 artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.",
            "score": 1,
            "issue_id": 611,
            "pub_date": "2026-01-15",
            "pub_date_card": {
                "ru": "15 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 15",
                "zh": "1æœˆ15æ—¥"
            },
            "hash": "4f2e5fe6f7a8b20a",
            "authors": [
                "Letian Peng",
                "Kun Zhou",
                "Longfei Yun",
                "Yupeng Hou",
                "Jingbo Shang"
            ],
            "affiliations": [
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.10080.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´ĞµÑ€ĞµĞ²ÑŒÑ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Codified Decision Trees (CDT) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ğ´ÑƒÑ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³Ğ´Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑƒĞ·Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ ÑÑ†ĞµĞ½, Ğ° Ğ»Ğ¸ÑÑ‚ÑŒÑ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… CDT ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ñ€ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Building Reliable Agents with Codified Decision Trees",
                    "desc": "This paper introduces Codified Decision Trees (CDT), a novel framework for creating executable and interpretable decision trees from narrative data. These decision trees help role-playing agents develop robust behavioral profiles that are consistent and reliable across various contexts. By using a data-driven approach, CDT structures behavioral profiles as trees of conditional rules, allowing for clear and deterministic decision-making during execution. The results show that CDT significantly outperforms traditional methods and human-written profiles, leading to better agent performance in diverse scenarios."
                },
                "zh": {
                    "title": "ç¼–ç å†³ç­–æ ‘ï¼šæå‡è§’è‰²æ‰®æ¼”ä»£ç†çš„è¡Œä¸ºä¸€è‡´æ€§ä¸å¯é æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç¼–ç å†³ç­–æ ‘ï¼ˆCDTï¼‰çš„æ¡†æ¶ï¼Œç”¨äºä»å™äº‹æ•°æ®ä¸­ç”Ÿæˆå¯æ‰§è¡Œå’Œå¯è§£é‡Šçš„å†³ç­–æ ‘ï¼Œä»¥åˆ›å»ºç¨³å¥çš„è§’è‰²æ‰®æ¼”ä»£ç†çš„è¡Œä¸ºæ¡£æ¡ˆã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒCDTèƒ½å¤Ÿæä¾›æ›´ä¸€è‡´å’Œå¯é çš„è¡Œä¸ºè¡¨ç°ï¼Œè§£å†³äº†ç°æœ‰æ¡£æ¡ˆç»“æ„ä¸æ¸…æ™°ã€ä¸å¯æ‰§è¡Œå’ŒéªŒè¯ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£ç”Ÿæˆå€™é€‰åœºæ™¯-åŠ¨ä½œè§„åˆ™ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ•°æ®éªŒè¯å’Œå±‚æ¬¡åŒ–ç»†åŒ–ï¼Œå½¢æˆé€æ˜ä¸”å¯æ›´æ–°çš„è¡Œä¸ºæ¡£æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCDTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºäººå·¥ç¼–å†™çš„æ¡£æ¡ˆå’Œä¹‹å‰çš„æ¡£æ¡ˆç”Ÿæˆæ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-15.html",
    "link_next": "2026-01-19.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "15.01",
        "en": "01/15",
        "zh": "1æœˆ15æ—¥"
    },
    "short_date_next": {
        "ru": "19.01",
        "en": "01/19",
        "zh": "1æœˆ19æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 8,
        "#agents": 4,
        "#cv": 0,
        "#rl": 5,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 0,
        "#audio": 0,
        "#video": 6,
        "#multimodal": 9,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 10,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 3,
        "#science": 0,
        "#low_resource": 0
    }
}