{
    "date": {
        "ru": "9 июня",
        "en": "June 9",
        "zh": "6月9日"
    },
    "time_utc": "2025-06-09 06:18",
    "weekday": 0,
    "issue_id": 4189,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.01111",
            "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
            "url": "https://huggingface.co/papers/2506.01111",
            "abstract": "A novel two-stage pipeline using specialized pretrained models and a large language model enhances audio caption quality by integrating diverse multimodal cues and contextual information.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality, large-scale audio captioning is crucial for advancing audio understanding, yet current automated methods often generate captions that lack fine-grained detail and contextual accuracy, primarily due to their reliance on limited unimodal or superficial multimodal information. Drawing inspiration from human auditory perception, which adeptly integrates cross-modal cues and performs sophisticated auditory scene analysis, we introduce a novel two-stage automated pipeline. This pipeline first employs specialized pretrained models to extract diverse contextual cues (e.g., speech, music, general sounds, and visual information from associated video). A large language model (LLM) then synthesizes these rich, multimodal inputs to generate detailed and context-aware audio captions. Key contributions of this work include: (1) the proposed scalable method for fine-grained audio caption generation; (2) FusionAudio, a new large-scale dataset comprising 1.2 million such detailed captions, combined with 6 million QA pairs; and (3) enhanced audio models developed using FusionAudio, specifically a CLAP-based audio encoder with superior audio-text alignment and instruction following. This paper paves the way for more nuanced and accurate automated understanding of complex audio environments. Code and data can be found in https://github.com/satsuki2486441738/FusionAudio.",
            "score": 16,
            "issue_id": 4186,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 июня",
                "en": "June 1",
                "zh": "6月1日"
            },
            "hash": "a649684de588a812",
            "authors": [
                "Shunian Chen",
                "Xinyuan Xie",
                "Zheshu Chen",
                "Liyan Zhao",
                "Owen Lee",
                "Zhan Su",
                "Qilin Sun",
                "Benyou Wang"
            ],
            "affiliations": [
                "South China University of Technology",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01111.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#audio",
                    "#multimodal",
                    "#optimization",
                    "#data",
                    "#games"
                ],
                "emoji": "🎧",
                "ru": {
                    "title": "Революция в аудио-подписях: мультимодальный ИИ для точного описания звуков",
                    "desc": "Статья представляет новый двухэтапный конвейер для улучшения качества аудио-подписей, используя специализированные предобученные модели и большую языковую модель (LLM). Этот метод интегрирует разнообразные мультимодальные сигналы и контекстную информацию для создания более детальных и точных описаний аудио. Авторы также представляют FusionAudio - новый крупномасштабный датасет, содержащий 1,2 миллиона подробных аудио-подписей и 6 миллионов пар вопросов-ответов. Исследование демонстрирует улучшенные аудио-модели, разработанные с использованием FusionAudio, включая аудио-энкодер на основе CLAP с улучшенным выравниванием аудио и текста."
                },
                "en": {
                    "title": "Enhancing Audio Captions with Multimodal Insights",
                    "desc": "This paper presents a two-stage pipeline that improves the quality of audio captions by using specialized pretrained models alongside a large language model (LLM). The first stage extracts various contextual cues from audio, such as speech and music, as well as visual information from related videos. In the second stage, the LLM synthesizes these multimodal inputs to create detailed and contextually accurate captions. The work introduces a new dataset, FusionAudio, which contains 1.2 million detailed captions and enhances audio models for better audio-text alignment."
                },
                "zh": {
                    "title": "提升音频字幕质量的创新方法",
                    "desc": "本文提出了一种新颖的两阶段管道，利用专门的预训练模型和大型语言模型来提高音频字幕的质量。该方法通过提取多样的上下文线索，如语音、音乐和视觉信息，来增强音频理解。然后，使用大型语言模型综合这些多模态输入，生成详细且具有上下文意识的音频字幕。此研究的关键贡献包括可扩展的细粒度音频字幕生成方法和一个新的大规模数据集FusionAudio，包含120万条详细字幕和600万对问答。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05984",
            "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
            "url": "https://huggingface.co/papers/2506.05984",
            "abstract": "Audio-aware large language models can assess speaking styles in audio inputs, demonstrating performance comparable to human judges in evaluating synthesized speech along dimensions like emotion, volume, and pitch.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-aware large language models (ALLMs) can understand the textual and non-textual information in the audio input. In this paper, we explore using ALLMs as an automatic judge to assess the speaking styles of speeches. We use ALLM judges to evaluate the speeches generated by SLMs on two tasks: voice style instruction following and role-playing. The speaking style we consider includes emotion, volume, speaking pace, word emphasis, pitch control, and non-verbal elements. We use four spoken language models (SLMs) to complete the two tasks and use humans and ALLMs to judge the SLMs' responses. We compare two ALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and show that the agreement between Gemini and human judges is comparable to the agreement between human evaluators. These promising results show that ALLMs can be used as a judge to evaluate SLMs. Our results also reveal that current SLMs, even GPT-4o-audio, still have room for improvement in controlling the speaking style and generating natural dialogues.",
            "score": 6,
            "issue_id": 4185,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 июня",
                "en": "June 6",
                "zh": "6月6日"
            },
            "hash": "10dcc4567ff634c1",
            "authors": [
                "Cheng-Han Chiang",
                "Xiaofei Wang",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Linjie Li",
                "Radu Kopetz",
                "Yao Qian",
                "Zhendong Wang",
                "Zhengyuan Yang",
                "Hung-yi Lee",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05984.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#audio",
                    "#interpretability",
                    "#games"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "АОБЛМ как объективные судьи стиля речи",
                    "desc": "Аудио-осведомленные большие языковые модели (АОБЛМ) способны оценивать стили речи в аудиовходах, демонстрируя производительность, сравнимую с оценками людей-судей. В исследовании АОБЛМ использовались для оценки речей, сгенерированных разговорными языковыми моделями (РЯМ) в задачах следования инструкциям по стилю голоса и ролевой игры. Оценивались такие аспекты, как эмоции, громкость, темп речи, выделение слов, контроль высоты тона и невербальные элементы. Результаты показали, что согласованность между оценками Gemini и человеческих судей сопоставима с согласованностью между оценками разных людей."
                },
                "en": {
                    "title": "Evaluating Speech Styles with AI: ALLMs vs. Human Judges",
                    "desc": "This paper discusses the capabilities of audio-aware large language models (ALLMs) in evaluating speaking styles from audio inputs. The authors demonstrate that ALLMs can assess synthesized speech similarly to human judges, focusing on aspects like emotion, volume, and pitch. They compare the performance of two ALLMs, GPT-4o-audio and Gemini-2.5-pro, against human evaluations in tasks involving voice style instruction and role-playing. The findings indicate that while ALLMs can effectively judge speaking styles, there is still potential for improvement in the speaking style control of current spoken language models (SLMs)."
                },
                "zh": {
                    "title": "音频感知模型：评估说话风格的新工具",
                    "desc": "音频感知的大型语言模型（ALLMs）能够评估音频输入中的说话风格，其表现与人类评审在情感、音量和音调等维度上的评估相当。本文探讨了使用ALLMs作为自动评审者来评估演讲的说话风格。我们使用四个口语语言模型（SLMs）完成两个任务，并通过人类和ALLMs对SLMs的响应进行评估。研究结果表明，ALLMs可以作为评审工具来评估SLMs，但当前的SLMs在控制说话风格和生成自然对话方面仍有改进空间。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05629",
            "title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
            "url": "https://huggingface.co/papers/2506.05629",
            "abstract": "A new method using input-dependent soft prompting with a self-attention mechanism improves parameter-efficient fine-tuning for large language models, enhancing zero-shot domain transfer.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of large language models in domain-specific tasks necessitates fine-tuning, which is computationally expensive and technically challenging. This paper focuses on parameter-efficient fine-tuning using soft prompting, a promising approach that adapts pre-trained models to downstream tasks by learning a small set of parameters. We propose a novel Input Dependent Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) that generates soft prompts based on the input tokens and attends different tokens with varying importance. Our method is simple and efficient, keeping the number of trainable parameters small. We show the merits of the proposed approach compared to state-of-the-art techniques on various tasks and show the improved zero shot domain transfer capability.",
            "score": 4,
            "issue_id": 4186,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "c88ec16aee1c43d5",
            "authors": [
                "Ananth Muppidi",
                "Abhilash Nandy",
                "Sambaran Bandyopadhyay"
            ],
            "affiliations": [
                "Adobe Research, India",
                "IIIT Hyderabad, India",
                "IIT Kharagpur, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05629.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#transfer_learning",
                    "#small_models"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективная донастройка языковых моделей с помощью динамических промптов",
                    "desc": "Статья представляет новый метод параметрически-эффективной донастройки больших языковых моделей. Предложенная техника Input Dependent Soft Prompting with self-Attention Mechanism (ID-SPAM) генерирует мягкие промпты на основе входных токенов. Метод использует механизм самовнимания для присвоения различной важности разным токенам. ID-SPAM показывает улучшенные результаты по сравнению с современными подходами на различных задачах, особенно в zero-shot переносе на новые домены."
                },
                "en": {
                    "title": "Efficient Fine-Tuning with Input-Dependent Soft Prompts",
                    "desc": "This paper introduces a new technique called Input Dependent Soft Prompting with a Self-Attention Mechanism (ID-SPAM) to enhance fine-tuning of large language models. It focuses on making the fine-tuning process more efficient by using a small set of parameters that adapt the model to specific tasks. The self-attention mechanism allows the model to weigh the importance of different input tokens when generating soft prompts. The results demonstrate that ID-SPAM outperforms existing methods, particularly in zero-shot domain transfer scenarios."
                },
                "zh": {
                    "title": "输入依赖的软提示，提升微调效率",
                    "desc": "本文提出了一种新的方法，利用输入依赖的软提示和自注意力机制，来提高大语言模型的参数高效微调能力。这种方法通过学习一小组参数，适应预训练模型到下游任务，减少了计算成本。我们的方法生成基于输入标记的软提示，并对不同的重要性标记进行关注，从而实现了高效的微调。实验结果表明，该方法在多个任务上优于现有技术，并提升了零-shot领域迁移能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06276",
            "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis",
            "url": "https://huggingface.co/papers/2506.06276",
            "abstract": "STARFlow, a generative model combining normalizing flows with autoregressive Transformers, achieves competitive image synthesis performance with innovations in architecture and latent space modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution.",
            "score": 2,
            "issue_id": 4189,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 июня",
                "en": "June 6",
                "zh": "6月6日"
            },
            "hash": "14f98c6826d7c6bb",
            "authors": [
                "Jiatao Gu",
                "Tianrong Chen",
                "David Berthelot",
                "Huangjie Zheng",
                "Yuyang Wang",
                "Ruixiang Zhang",
                "Laurent Dinh",
                "Miguel Angel Bautista",
                "Josh Susskind",
                "Shuangfei Zhai"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06276.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Нормализующие потоки покоряют высоты генерации изображений",
                    "desc": "STARFlow - это генеративная модель, объединяющая нормализующие потоки и авторегрессивные трансформеры для синтеза изображений высокого разрешения. Модель использует глубоко-мелкую архитектуру и моделирование в латентном пространстве предобученных автоэнкодеров. Ключевые инновации включают новый алгоритм управления для повышения качества сэмплов и обучение методом максимального правдоподобия без дискретизации. STARFlow достигает конкурентоспособных результатов в задачах генерации изображений по классу и тексту, приближаясь к современным диффузионным моделям."
                },
                "en": {
                    "title": "STARFlow: Merging Flows and Transformers for High-Quality Image Generation",
                    "desc": "STARFlow is a generative model that merges normalizing flows with autoregressive Transformers to create high-quality images. It introduces the Transformer Autoregressive Flow (TARFlow), which effectively models continuous distributions while maintaining scalability. Key innovations include a deep-shallow architecture for efficient computation, latent space modeling using pretrained autoencoders, and a novel guidance algorithm to enhance sample quality. This model achieves competitive results in both class-conditional and text-conditional image generation, marking a significant advancement in the use of normalizing flows for high-resolution image synthesis."
                },
                "zh": {
                    "title": "STARFlow：高效图像合成的新纪元",
                    "desc": "STARFlow是一种结合了归一化流和自回归变换器的生成模型，能够在高分辨率图像合成中实现强大的性能。其核心是变换器自回归流（TARFlow），将归一化流的表达能力与自回归变换器的结构建模能力相结合。通过深浅设计、在预训练自编码器的潜在空间建模以及新颖的引导算法，STARFlow显著提高了可扩展性和样本质量。该模型能够在连续空间中进行精确的最大似然训练，且在类条件和文本条件的图像生成任务中表现出色，接近最先进的扩散模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06199",
            "title": "3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model",
            "url": "https://huggingface.co/papers/2506.06199",
            "abstract": "A 3D flow world model learned from human and robot manipulation data, using video diffusion and GPT-4o, enables robots to perform diverse manipulation tasks with strong generalization and cross-embodiment adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t Manipulation has long been a challenging task for robots, while humans can effortlessly perform complex interactions with objects, such as hanging a cup on the mug rack. A key reason is the lack of a large and uniform dataset for teaching robots manipulation skills. Current robot datasets often record robot action in different action spaces within a simple scene. This hinders the robot to learn a unified and robust action representation for different robots within diverse scenes. Observing how humans understand a manipulation task, we find that understanding how the objects should move in the 3D space is a critical clue for guiding actions. This clue is embodiment-agnostic and suitable for both humans and different robots. Motivated by this, we aim to learn a 3D flow world model from both human and robot manipulation data. This model predicts the future movement of the interacting objects in 3D space, guiding action planning for manipulation. Specifically, we synthesize a large-scale 3D optical flow dataset, named ManiFlow-110k, through a moving object auto-detect pipeline. A video diffusion-based world model then learns manipulation physics from these data, generating 3D optical flow trajectories conditioned on language instructions. With the generated 3D object optical flow, we propose a flow-guided rendering mechanism, which renders the predicted final state and leverages GPT-4o to assess whether the predicted flow aligns with the task description. This equips the robot with a closed-loop planning ability. Finally, we consider the predicted 3D optical flow as constraints for an optimization policy to determine a chunk of robot actions for manipulation. Extensive experiments demonstrate strong generalization across diverse robotic manipulation tasks and reliable cross-embodiment adaptation without hardware-specific training.",
            "score": 2,
            "issue_id": 4186,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 июня",
                "en": "June 6",
                "zh": "6月6日"
            },
            "hash": "c1b9d0a9c29bdf3b",
            "authors": [
                "Hongyan Zhi",
                "Peihao Chen",
                "Siyuan Zhou",
                "Yubo Dong",
                "Quanxi Wu",
                "Lei Han",
                "Mingkui Tan"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Pazhou Laboratory",
                "South China University of Technology",
                "Tencent Robotics"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06199.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#robotics",
                    "#optimization",
                    "#3d",
                    "#games"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Универсальная модель 3D-потока для роботизированных манипуляций",
                    "desc": "Статья представляет модель 3D-потока для манипуляций роботов, обученную на данных человеческих и роботизированных манипуляций. Модель использует видеодиффузию и GPT-4o для прогнозирования движения объектов в 3D-пространстве. Авторы создали датасет ManiFlow-110k и разработали механизм рендеринга на основе потока для оценки соответствия предсказанных действий поставленной задаче. Эксперименты показали сильную обобщающую способность модели и возможность адаптации к различным роботизированным системам без специфического обучения."
                },
                "en": {
                    "title": "Empowering Robots with 3D Flow for Versatile Manipulation",
                    "desc": "This paper presents a novel 3D flow world model that enables robots to learn manipulation tasks by leveraging both human and robot data. The model predicts how objects move in 3D space, which helps robots plan their actions more effectively. By creating a large dataset called ManiFlow-110k and using a video diffusion approach, the researchers teach robots to understand manipulation physics and generate action plans based on language instructions. The results show that this method allows robots to generalize well across various tasks and adapt to different robotic embodiments without needing specific training for each hardware type."
                },
                "zh": {
                    "title": "学习3D流动模型，提升机器人操作能力",
                    "desc": "本论文提出了一种从人类和机器人操作数据中学习的3D流动世界模型，旨在帮助机器人执行多样化的操作任务。通过合成一个名为ManiFlow-110k的大规模3D光流数据集，模型能够预测交互对象在3D空间中的未来运动。利用视频扩散技术和GPT-4o，模型生成的3D光流轨迹可以指导机器人的操作规划。实验结果表明，该模型在不同的机器人操作任务中具有强大的泛化能力和跨实体适应性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.05523",
            "title": "MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2506.05523",
            "abstract": "MORSE-500, a video benchmark with 500 scripted clips, evaluates multimodal reasoning across six categories, highlighting performance gaps in abstract and planning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in vision-language models (VLMs), current benchmarks for multimodal reasoning fall short in three key dimensions. First, they overwhelmingly rely on static images, failing to capture the temporal complexity of real-world environments. Second, they narrowly focus on mathematical problem-solving, neglecting the broader spectrum of reasoning skills -- including abstract, physical, planning, spatial, and temporal capabilities -- required for robust multimodal intelligence. Third, many benchmarks quickly saturate, offering limited headroom for diagnosing failure modes or measuring continued progress. We introduce MORSE-500 (Multimodal Reasoning Stress-test Environment), a video benchmark composed of 500 fully scripted clips with embedded questions spanning six complementary reasoning categories. Each instance is programmatically generated using deterministic Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and curated real footage. This script-driven design allows fine-grained control over visual complexity, distractor density, and temporal dynamics -- enabling difficulty to be scaled systematically as models improve. Unlike static benchmarks that become obsolete once saturated, MORSE-500 is built to evolve: its controllable generation pipeline supports the creation of arbitrarily challenging new instances, making it ideally suited for stress-testing next-generation models. Initial experiments with state-of-the-art systems -- including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest available at the time, alongside strong open-source models -- reveal substantial performance gaps across all categories, with particularly large deficits in abstract and planning tasks. We release the full dataset, generation scripts, and evaluation harness to support transparent, reproducible, and forward-looking multimodal reasoning research.",
            "score": 2,
            "issue_id": 4188,
            "pub_date": "2025-06-05",
            "pub_date_card": {
                "ru": "5 июня",
                "en": "June 5",
                "zh": "6月5日"
            },
            "hash": "524394ef06ba3ba1",
            "authors": [
                "Zikui Cai",
                "Andrew Wang",
                "Anirudh Satheesh",
                "Ankit Nakhawa",
                "Hyunwoo Jae",
                "Keenan Powell",
                "Minghui Liu",
                "Neel Jay",
                "Sungbin Oh",
                "Xiyao Wang",
                "Yongyuan Liang",
                "Tom Goldstein",
                "Furong Huang"
            ],
            "affiliations": [
                "Capital One",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.05523.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#survey",
                    "#video",
                    "#open_source",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "MORSE-500: Новый рубеж в оценке мультимодального ИИ",
                    "desc": "MORSE-500 - это новый видеобенчмарк для оценки мультимодального рассуждения моделей искусственного интеллекта. Он включает 500 специально созданных видеоклипов с вопросами по шести категориям рассуждений, включая абстрактное мышление и планирование. Бенчмарк позволяет систематически увеличивать сложность тестов по мере улучшения моделей. Эксперименты показали значительные пробелы в производительности современных систем, особенно в абстрактных задачах и планировании."
                },
                "en": {
                    "title": "MORSE-500: Evolving Benchmark for Multimodal Reasoning",
                    "desc": "MORSE-500 is a new video benchmark designed to evaluate multimodal reasoning in AI across six different categories. It addresses limitations in existing benchmarks by incorporating dynamic video clips instead of static images, allowing for a more realistic assessment of reasoning skills. The benchmark includes a variety of reasoning tasks, such as abstract thinking and planning, which are essential for advanced multimodal intelligence. By providing a scalable and evolving dataset, MORSE-500 aims to facilitate ongoing research and development in multimodal reasoning capabilities."
                },
                "zh": {
                    "title": "MORSE-500：多模态推理的新基准",
                    "desc": "MORSE-500是一个包含500个脚本化视频片段的基准测试，旨在评估多模态推理能力。该基准涵盖六个互补的推理类别，强调了在抽象和规划任务中的性能差距。与静态图像基准不同，MORSE-500能够捕捉现实环境的时间复杂性，并支持生成具有不同难度的新实例。通过提供完整的数据集和生成脚本，MORSE-500为多模态推理研究提供了透明和可重复的支持。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04255",
            "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization",
            "url": "https://huggingface.co/papers/2506.04255",
            "abstract": "HASHIRU, a novel MAS framework, enhances flexibility, resource efficiency, and adaptability by dynamically managing specialized agents and using a hybrid intelligence approach with smaller, local LLMs and external APIs.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid Large Language Model (LLM) advancements are fueling autonomous Multi-Agent System (MAS) development. However, current frameworks often lack flexibility, resource awareness, model diversity, and autonomous tool creation. This paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent Resource Utilization), a novel MAS framework enhancing flexibility, resource efficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically managing specialized \"employee\" agents, instantiated based on task needs and resource constraints (cost, memory). Its hybrid intelligence prioritizes smaller, local LLMs (via Ollama) while flexibly using external APIs and larger models when necessary. An economic model with hiring/firing costs promotes team stability and efficient resource allocation. The system also includes autonomous API tool creation and a memory function. Evaluations on tasks like academic paper review (58% success), safety assessments (100% on a JailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash on GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate HASHIRU's capabilities. Case studies illustrate its self-improvement via autonomous cost model generation, tool integration, and budget management. HASHIRU offers a promising approach for more robust, efficient, and adaptable MAS through dynamic hierarchical control, resource-aware hybrid intelligence, and autonomous functional extension. Source code and benchmarks are available at https://github.com/HASHIRU-AI/HASHIRU and https://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is available at https://hashiruagentx-hashiruai.hf.space upon request.",
            "score": 1,
            "issue_id": 4189,
            "pub_date": "2025-06-01",
            "pub_date_card": {
                "ru": "1 июня",
                "en": "June 1",
                "zh": "6月1日"
            },
            "hash": "d3d7d73af3533148",
            "authors": [
                "Kunal Pai",
                "Parth Shah",
                "Harshil Patel"
            ],
            "affiliations": [
                "Independent Researcher",
                "UC Davis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04255.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agi",
                    "#optimization",
                    "#benchmark",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Гибкая и эффективная мультиагентная система нового поколения",
                    "desc": "HASHIRU - это новая система мультиагентного взаимодействия, которая повышает гибкость, эффективность использования ресурсов и адаптивность. Она использует динамическое управление специализированными агентами и гибридный подход к интеллекту, сочетая небольшие локальные языковые модели и внешние API. HASHIRU включает экономическую модель с затратами на найм/увольнение агентов для стабильности команды и эффективного распределения ресурсов. Система продемонстрировала высокую эффективность в различных задачах, превзойдя некоторые существующие модели."
                },
                "en": {
                    "title": "HASHIRU: Dynamic Intelligence for Efficient Multi-Agent Systems",
                    "desc": "HASHIRU is a new framework for Multi-Agent Systems (MAS) that improves flexibility and resource efficiency by managing specialized agents dynamically. It uses a hybrid intelligence approach, combining smaller local Large Language Models (LLMs) with external APIs to adapt to different tasks. The framework includes a 'CEO' agent that oversees 'employee' agents based on the specific needs and available resources, promoting efficient team management. Evaluations show HASHIRU's strong performance in various tasks, highlighting its ability to autonomously create tools and manage resources effectively."
                },
                "zh": {
                    "title": "HASHIRU：灵活高效的多智能体系统新框架",
                    "desc": "HASHIRU是一个新颖的多智能体系统（MAS）框架，旨在提高灵活性、资源效率和适应性。它通过动态管理专门的代理（如“CEO”代理和“员工”代理）来满足任务需求和资源限制。该框架采用混合智能，优先使用较小的本地大语言模型（LLM），并在必要时灵活调用外部API和更大的模型。HASHIRU的评估结果显示其在多个任务上表现出色，展示了其在动态控制和资源感知方面的优势。"
                }
            }
        }
    ],
    "link_prev": "2025-06-06.html",
    "link_next": "2025-06-10.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "06.06",
        "en": "06/06",
        "zh": "6月6日"
    },
    "short_date_next": {
        "ru": "10.06",
        "en": "06/10",
        "zh": "6月10日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 2,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了ComfyUI-Copilot，一个使用大型语言模型和多代理系统的插件。它旨在提高AI驱动的艺术创作平台ComfyUI的易用性和效率。ComfyUI虽然灵活，但对新手来说有一定难度。ComfyUI-Copilot通过智能推荐和自动化工作流构建来解决这些问题。测试和用户反馈显示它能准确推荐节点并加速工作流开发。",
        "title": "ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development",
        "pinyin": "这篇文章介绍了ComfyUI-Copilot，一个使用大型语言模型和多代理系统的插件。它旨在提高AI驱动的艺术创作平台ComfyUI的易用性和效率。ComfyUI虽然灵活，但对新手来说有一定难度。ComfyUI-Copilot通过智能推荐和自动化工作流构建来解决这些问题。测试和用户反馈显示它能准确推荐节点并加速工作流开发。\n\nzhè piān wén zhāng jiè shào le ComfyUI-Copilot, yī gè shǐ yòng dà xíng yǔ yán mó xíng hé duō dài lǐ xì tǒng de chā jiàn. tā zhǐ yú tí gāo AI qū dòng de yì shù chuàng zuò píng tái ComfyUI de yì yòng xìng hé xiào lǜ. ComfyUI suī rán líng huó, dàn duì xīn shǒu lái shuō yǒu yī dìng nán dù. ComfyUI-Copilot tōng guò zhì néng tuī jiàn hé zì dòng huà gōng zuò liú gòu jiàn lái jiě jué zhè xiē wèn tí. cè shì hé yòng hù fǎn kuì xiǎn shì tā néng zhǔn què tuī jiàn jié diǎn bìng jiā sù gōng zuò liú kāi fā.",
        "vocab": "[{'word': '介绍', 'pinyin': 'jiè shào', 'trans': 'introduce'},\n{'word': '插件', 'pinyin': 'chā jiàn', 'trans': 'plugin'},\n{'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'},\n{'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'},\n{'word': '易用性', 'pinyin': 'yì yòng xìng', 'trans': 'usability'},\n{'word': '效率', 'pinyin': 'xiào lǜ', 'trans': 'efficiency'},\n{'word': '灵活', 'pinyin': 'líng huó', 'trans': 'flexible'},\n{'word': '新手', 'pinyin': 'xīn shǒu', 'trans': 'beginner'},\n{'word': '难度', 'pinyin': 'nán dù', 'trans': 'difficulty'},\n{'word': '智能', 'pinyin': 'zhì néng', 'trans': 'intelligent'},\n{'word': '推荐', 'pinyin': 'tuī jiàn', 'trans': 'recommend'},\n{'word': '自动化', 'pinyin': 'zì dòng huà', 'trans': 'automation'},\n{'word': '工作流', 'pinyin': 'gōng zuò liú', 'trans': 'workflow'},\n{'word': '构建', 'pinyin': 'gòu jiàn', 'trans': 'build'},\n{'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'},\n{'word': '准确', 'pinyin': 'zhǔn què', 'trans': 'accurate'},\n{'word': '节点', 'pinyin': 'jié diǎn', 'trans': 'node'},\n{'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'},\n{'word': '开发', 'pinyin': 'kāi fā', 'trans': 'develop'},\n{'word': '反馈', 'pinyin': 'fǎn kuì', 'trans': 'feedback'}]",
        "trans": "This article introduces ComfyUI-Copilot, a plugin that utilizes large language models and multi-agent systems. It aims to enhance the usability and efficiency of the AI-driven art creation platform ComfyUI. While ComfyUI is flexible, it can be challenging for beginners. ComfyUI-Copilot addresses these issues by providing intelligent recommendations and automated workflow construction. Testing and user feedback indicate that it accurately recommends nodes and accelerates workflow development.",
        "update_ts": "2025-06-08 12:46"
    }
}