{
    "date": {
        "ru": "16 сентября",
        "en": "September 16",
        "zh": "9月16日"
    },
    "time_utc": "2024-09-16 09:00",
    "weekday": 0,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-16",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.08857",
            "title": "InstantDrag: Improving Interactivity in Drag-based Image Editing",
            "url": "https://huggingface.co/papers/2409.08857",
            "abstract": "Drag-based image editing has recently gained popularity for its interactivity and precision. However, despite the ability of text-to-image models to generate samples within a second, drag editing still lags behind due to the challenge of accurately reflecting user interaction while maintaining image content. Some existing approaches rely on computationally intensive per-image optimization or intricate guidance-based methods, requiring additional inputs such as masks for movable regions and text prompts, thereby compromising the interactivity of the editing process. We introduce InstantDrag, an optimization-free pipeline that enhances interactivity and speed, requiring only an image and a drag instruction as input. InstantDrag consists of two carefully designed networks: a drag-conditioned optical flow generator (FlowGen) and an optical flow-conditioned diffusion model (FlowDiffusion). InstantDrag learns motion dynamics for drag-based image editing in real-world video datasets by decomposing the task into motion generation and motion-conditioned image generation. We demonstrate InstantDrag's capability to perform fast, photo-realistic edits without masks or text prompts through experiments on facial video datasets and general scenes. These results highlight the efficiency of our approach in handling drag-based image editing, making it a promising solution for interactive, real-time applications.",
            "score": 30,
            "issue_id": 1,
            "pub_date": "2024-09-13",
            "pub_date_card": {
                "ru": "13 сентября",
                "en": "September 13",
                "zh": "9月13日"
            },
            "hash": "f1d54686dd0f3e15",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#cv",
                    "#optimization",
                    "#games",
                    "#diffusion",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "🖱️",
                "ru": {
                    "title": "Мгновенное редактирование изображений с помощью перетаскивания",
                    "desc": "InstantDrag - это новый подход к редактированию изображений на основе перетаскивания, который не требует оптимизации и использует только изображение и инструкцию по перетаскиванию в качестве входных данных. Система состоит из двух сетей: генератора оптического потока с учетом перетаскивания (FlowGen) и диффузионной модели, обусловленной оптическим потоком (FlowDiffusion). InstantDrag обучается динамике движения на наборах данных видео реального мира, разделяя задачу на генерацию движения и генерацию изображений с учетом движения. Этот метод позволяет выполнять быстрое фотореалистичное редактирование без масок или текстовых подсказок, что делает его перспективным решением для интерактивных приложений реального времени."
                },
                "en": {
                    "title": "InstantDrag: Fast and Interactive Image Editing Made Simple",
                    "desc": "This paper presents InstantDrag, a new method for drag-based image editing that improves speed and interactivity without the need for complex optimizations. Unlike traditional methods that require masks or text prompts, InstantDrag only needs an image and a simple drag instruction. It utilizes two specialized networks: FlowGen, which generates optical flow based on drag conditions, and FlowDiffusion, which creates images conditioned on that flow. The results show that InstantDrag can produce fast and realistic edits, making it suitable for real-time applications in various contexts."
                },
                "zh": {
                    "title": "InstantDrag：快速、真实感的拖拽图像编辑",
                    "desc": "本文介绍了一种名为InstantDrag的图像编辑方法，旨在提高拖拽编辑的交互性和速度。与传统方法不同，InstantDrag不需要复杂的优化过程，只需输入一张图像和拖拽指令。该方法由两个网络组成：拖拽条件光流生成器和光流条件扩散模型，能够有效学习拖拽编辑中的运动动态。实验结果表明，InstantDrag能够快速实现真实感编辑，适用于实时交互应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08615",
            "title": "DrawingSpinUp: 3D Animation from Single Character Drawings",
            "url": "https://huggingface.co/papers/2409.08615",
            "abstract": "Animating various character drawings is an engaging visual content creation task. Given a single character drawing, existing animation methods are limited to flat 2D motions and thus lack 3D effects. An alternative solution is to reconstruct a 3D model from a character drawing as a proxy and then retarget 3D motion data onto it. However, the existing image-to-3D methods could not work well for amateur character drawings in terms of appearance and geometry. We observe the contour lines, commonly existing in character drawings, would introduce significant ambiguity in texture synthesis due to their view-dependence. Additionally, thin regions represented by single-line contours are difficult to reconstruct (e.g., slim limbs of a stick figure) due to their delicate structures. To address these issues, we propose a novel system, DrawingSpinUp, to produce plausible 3D animations and breathe life into character drawings, allowing them to freely spin up, leap, and even perform a hip-hop dance. For appearance improvement, we adopt a removal-then-restoration strategy to first remove the view-dependent contour lines and then render them back after retargeting the reconstructed character. For geometry refinement, we develop a skeleton-based thinning deformation algorithm to refine the slim structures represented by the single-line contours. The experimental evaluations and a perceptual user study show that our proposed method outperforms the existing 2D and 3D animation methods and generates high-quality 3D animations from a single character drawing. Please refer to our project page (https://lordliang.github.io/DrawingSpinUp) for the code and generated animations.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-09-13",
            "pub_date_card": {
                "ru": "13 сентября",
                "en": "September 13",
                "zh": "9月13日"
            },
            "hash": "a61d3cf7a11ab0c1",
            "data": {
                "categories": [
                    "#cv",
                    "#games",
                    "#open_source",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Оживляем рисунки: от 2D к 3D анимации",
                    "desc": "Статья представляет новую систему DrawingSpinUp для создания правдоподобных 3D-анимаций из одиночных рисунков персонажей. Авторы решают проблемы, связанные с контурными линиями и тонкими структурами, используя стратегию удаления и восстановления контуров, а также алгоритм утончения на основе скелета. Система позволяет персонажам свободно вращаться, прыгать и даже танцевать хип-хоп. Экспериментальные оценки и исследование восприятия пользователей показывают, что предложенный метод превосходит существующие методы 2D и 3D анимации."
                },
                "en": {
                    "title": "Breathe Life into Drawings with 3D Animation!",
                    "desc": "This paper presents DrawingSpinUp, a novel system designed to create high-quality 3D animations from single character drawings. Traditional methods struggle with 2D animations and often fail to accurately represent the geometry and appearance of amateur drawings. The proposed approach addresses these challenges by removing view-dependent contour lines before retargeting 3D motion data, and then restoring these contours for improved visual fidelity. Additionally, a skeleton-based thinning deformation algorithm is introduced to enhance the representation of delicate structures, resulting in more realistic animations that outperform existing techniques."
                },
                "zh": {
                    "title": "让角色绘图动起来的3D动画新方法",
                    "desc": "本文提出了一种新系统DrawingSpinUp，用于将单个角色绘图转化为逼真的3D动画。现有的图像到3D的方法在处理业余角色绘图时效果不佳，主要是因为轮廓线的存在导致纹理合成的模糊。我们采用了一种去除再恢复的策略，先去除视角依赖的轮廓线，再在重定向后将其恢复，以改善外观。通过骨架基础的细化变形算法，我们能够更好地重建细长结构，从而生成高质量的3D动画。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08947",
            "title": "A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis",
            "url": "https://huggingface.co/papers/2409.08947",
            "abstract": "Relighting radiance fields is severely underconstrained for multi-view data, which is most often captured under a single illumination condition; It is especially hard for full scenes containing multiple objects. We introduce a method to create relightable radiance fields using such single-illumination data by exploiting priors extracted from 2D image diffusion models. We first fine-tune a 2D diffusion model on a multi-illumination dataset conditioned by light direction, allowing us to augment a single-illumination capture into a realistic -- but possibly inconsistent -- multi-illumination dataset from directly defined light directions. We use this augmented data to create a relightable radiance field represented by 3D Gaussian splats. To allow direct control of light direction for low-frequency lighting, we represent appearance with a multi-layer perceptron parameterized on light direction. To enforce multi-view consistency and overcome inaccuracies we optimize a per-image auxiliary feature vector. We show results on synthetic and real multi-view data under single illumination, demonstrating that our method successfully exploits 2D diffusion model priors to allow realistic 3D relighting for complete scenes. Project site https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-13",
            "pub_date_card": {
                "ru": "13 сентября",
                "en": "September 13",
                "zh": "9月13日"
            },
            "hash": "53dd7086261a9945",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#graphs",
                    "#data",
                    "#diffusion",
                    "#architecture",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "Реалистичное переосвещение 3D сцен с помощью 2D диффузионных моделей",
                    "desc": "Статья представляет метод создания переосвещаемых полей излучения с использованием данных с одним освещением путем применения приоров из 2D диффузионных моделей изображений. Авторы дообучают 2D диффузионную модель на наборе данных с множественным освещением, что позволяет дополнить захват с единичным освещением реалистичным набором данных с множественным освещением. Этот дополненный набор данных используется для создания переосвещаемого поля излучения, представленного 3D гауссовыми сплатами. Метод успешно применяет приоры 2D диффузионных моделей для реалистичного 3D переосвещения полных сцен."
                },
                "en": {
                    "title": "Transforming Single Illumination into Realistic 3D Relighting",
                    "desc": "This paper addresses the challenge of relighting radiance fields using multi-view data captured under a single illumination condition. The authors propose a novel method that leverages priors from 2D image diffusion models to generate a more realistic multi-illumination dataset from limited single-illumination captures. By fine-tuning a diffusion model and using 3D Gaussian splats, they create a relightable radiance field that allows for direct control of light direction. The approach also incorporates a multi-layer perceptron to represent appearance based on light direction and optimizes auxiliary feature vectors to ensure consistency across multiple views."
                },
                "zh": {
                    "title": "利用二维扩散模型实现三维重光照的创新方法",
                    "desc": "本论文提出了一种利用单一光照条件下的数据生成可重光照的辐射场的方法。我们通过对二维扩散模型进行微调，利用多光照数据集的先验知识，增强了单一光照捕获的数据。该方法使用三维高斯点云表示可重光照的辐射场，并通过多层感知机对光照方向进行参数化，以实现对低频光照的直接控制。实验结果表明，我们的方法能够在合成和真实的多视角数据上实现逼真的三维重光照。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08513",
            "title": "Mamba-YOLO-World: Marrying YOLO-World with Mamba for Open-Vocabulary Detection",
            "url": "https://huggingface.co/papers/2409.08513",
            "abstract": "Open-vocabulary detection (OVD) aims to detect objects beyond a predefined set of categories. As a pioneering model incorporating the YOLO series into OVD, YOLO-World is well-suited for scenarios prioritizing speed and efficiency.However, its performance is hindered by its neck feature fusion mechanism, which causes the quadratic complexity and the limited guided receptive fields.To address these limitations, we present Mamba-YOLO-World, a novel YOLO-based OVD model employing the proposed MambaFusion Path Aggregation Network (MambaFusion-PAN) as its neck architecture. Specifically, we introduce an innovative State Space Model-based feature fusion mechanism consisting of a Parallel-Guided Selective Scan algorithm and a Serial-Guided Selective Scan algorithm with linear complexity and globally guided receptive fields. It leverages multi-modal input sequences and mamba hidden states to guide the selective scanning process.Experiments demonstrate that our model outperforms the original YOLO-World on the COCO and LVIS benchmarks in both zero-shot and fine-tuning settings while maintaining comparable parameters and FLOPs. Additionally, it surpasses existing state-of-the-art OVD methods with fewer parameters and FLOPs.",
            "score": 10,
            "issue_id": 1,
            "pub_date": "2024-09-13",
            "pub_date_card": {
                "ru": "13 сентября",
                "en": "September 13",
                "zh": "9月13日"
            },
            "hash": "8dff1ca21cb53332",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#benchmark",
                    "#games",
                    "#open_source",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "🐍",
                "ru": {
                    "title": "Mamba-YOLO-World: Эффективное обнаружение объектов с открытым словарем",
                    "desc": "Mamba-YOLO-World - это новая модель для обнаружения объектов с открытым словарем (OVD), основанная на архитектуре YOLO. Она использует инновационный механизм слияния признаков MambaFusion-PAN, который решает проблемы квадратичной сложности и ограниченных рецептивных полей предыдущей модели YOLO-World. Mamba-YOLO-World применяет алгоритмы выборочного сканирования на основе модели пространства состояний для эффективной обработки мультимодальных входных последовательностей. Эксперименты показывают, что модель превосходит оригинальный YOLO-World и другие современные методы OVD при меньшем количестве параметров и вычислительных операций."
                },
                "en": {
                    "title": "Mamba-YOLO-World: Efficient Open-Vocabulary Detection Redefined",
                    "desc": "This paper introduces Mamba-YOLO-World, an advanced model for open-vocabulary detection (OVD) that enhances the YOLO framework. It addresses the limitations of the previous YOLO-World model, particularly its inefficient neck feature fusion mechanism, which leads to high computational complexity. The new MambaFusion Path Aggregation Network (MambaFusion-PAN) utilizes a State Space Model-based feature fusion approach, allowing for efficient parallel and serial guided scanning of features. Experimental results show that Mamba-YOLO-World outperforms its predecessor and other leading OVD methods while maintaining a low computational footprint."
                },
                "zh": {
                    "title": "Mamba-YOLO-World：高效的开放词汇检测新模型",
                    "desc": "开放词汇检测（OVD）旨在识别超出预定义类别的物体。本文提出的Mamba-YOLO-World模型，采用了新颖的MambaFusion路径聚合网络作为其颈部架构，解决了YOLO-World在特征融合机制上的局限性。该模型引入了基于状态空间模型的特征融合机制，具有线性复杂度和全局引导感受野，能够有效处理多模态输入序列。实验结果表明，Mamba-YOLO-World在COCO和LVIS基准测试中超越了原始YOLO-World，并在参数和FLOPs上保持相似，且在现有的OVD方法中表现优越。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08353",
            "title": "Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos",
            "url": "https://huggingface.co/papers/2409.08353",
            "abstract": "Volumetric video represents a transformative advancement in visual media, enabling users to freely navigate immersive virtual experiences and narrowing the gap between digital and real worlds. However, the need for extensive manual intervention to stabilize mesh sequences and the generation of excessively large assets in existing workflows impedes broader adoption. In this paper, we present a novel Gaussian-based approach, dubbed DualGS, for real-time and high-fidelity playback of complex human performance with excellent compression ratios. Our key idea in DualGS is to separately represent motion and appearance using the corresponding skin and joint Gaussians. Such an explicit disentanglement can significantly reduce motion redundancy and enhance temporal coherence. We begin by initializing the DualGS and anchoring skin Gaussians to joint Gaussians at the first frame. Subsequently, we employ a coarse-to-fine training strategy for frame-by-frame human performance modeling. It includes a coarse alignment phase for overall motion prediction as well as a fine-grained optimization for robust tracking and high-fidelity rendering. To integrate volumetric video seamlessly into VR environments, we efficiently compress motion using entropy encoding and appearance using codec compression coupled with a persistent codebook. Our approach achieves a compression ratio of up to 120 times, only requiring approximately 350KB of storage per frame. We demonstrate the efficacy of our representation through photo-realistic, free-view experiences on VR headsets, enabling users to immersively watch musicians in performance and feel the rhythm of the notes at the performers' fingertips.",
            "score": 10,
            "issue_id": 1,
            "pub_date": "2024-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "c6b38fb50d06c2f1",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#training",
                    "#optimization",
                    "#compression",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "🕺",
                "ru": {
                    "title": "Революция в объемном видео: реалистичное погружение при минимальных затратах",
                    "desc": "Эта статья представляет новый подход DualGS для создания объемного видео с использованием гауссовых распределений. Метод раздельно моделирует движение и внешний вид с помощью гауссовых распределений для суставов и кожи. DualGS применяет стратегию обучения от грубого к точному для покадрового моделирования человеческих движений. Подход обеспечивает высокое сжатие данных - до 120 раз, требуя всего около 350 КБ на кадр."
                },
                "en": {
                    "title": "Revolutionizing Volumetric Video with DualGS: Immersive Experiences Made Efficient!",
                    "desc": "This paper introduces DualGS, a novel Gaussian-based method for enhancing volumetric video playback, particularly for complex human performances. By separately modeling motion and appearance with skin and joint Gaussians, the approach reduces redundancy and improves the temporal coherence of the video. The method employs a coarse-to-fine training strategy for accurate motion prediction and high-fidelity rendering, achieving impressive compression ratios of up to 120 times. Ultimately, DualGS enables immersive experiences in virtual reality with minimal storage requirements, allowing users to engage with performances in a more interactive way."
                },
                "zh": {
                    "title": "DualGS：高效压缩与沉浸式体验的结合",
                    "desc": "这篇论文介绍了一种新的高斯基础方法，称为DualGS，用于实时播放复杂的人类表演。该方法通过分别表示运动和外观，显著减少了运动冗余并增强了时间一致性。DualGS采用粗到细的训练策略，能够在虚拟现实环境中高效压缩视频数据，压缩比高达120倍。最终，用户可以在VR头戴设备上沉浸式观看表演，感受音乐的节奏。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08514",
            "title": "Apollo: Band-sequence Modeling for High-Quality Audio Restoration",
            "url": "https://huggingface.co/papers/2409.08514",
            "abstract": "Audio restoration has become increasingly significant in modern society, not only due to the demand for high-quality auditory experiences enabled by advanced playback devices, but also because the growing capabilities of generative audio models necessitate high-fidelity audio. Typically, audio restoration is defined as a task of predicting undistorted audio from damaged input, often trained using a GAN framework to balance perception and distortion. Since audio degradation is primarily concentrated in mid- and high-frequency ranges, especially due to codecs, a key challenge lies in designing a generator capable of preserving low-frequency information while accurately reconstructing high-quality mid- and high-frequency content. Inspired by recent advancements in high-sample-rate music separation, speech enhancement, and audio codec models, we propose Apollo, a generative model designed for high-sample-rate audio restoration. Apollo employs an explicit frequency band split module to model the relationships between different frequency bands, allowing for more coherent and higher-quality restored audio. Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently outperforms existing SR-GAN models across various bit rates and music genres, particularly excelling in complex scenarios involving mixtures of multiple instruments and vocals. Apollo significantly improves music restoration quality while maintaining computational efficiency. The source code for Apollo is publicly available at https://github.com/JusperLee/Apollo.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-13",
            "pub_date_card": {
                "ru": "13 сентября",
                "en": "September 13",
                "zh": "9月13日"
            },
            "hash": "1d46169e43987c03",
            "data": {
                "categories": [
                    "#audio",
                    "#dataset",
                    "#games",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "Apollo: новый уровень восстановления аудио с помощью ИИ",
                    "desc": "Статья представляет Apollo - генеративную модель для восстановления аудио высокого качества. Модель использует явное разделение частотных диапазонов для более точного моделирования взаимосвязей между ними. Apollo превосходит существующие SR-GAN модели на различных битрейтах и музыкальных жанрах, особенно в сложных сценариях с микшированием нескольких инструментов и вокала. Модель значительно улучшает качество восстановления музыки при сохранении вычислительной эффективности."
                },
                "en": {
                    "title": "Apollo: Revolutionizing High-Quality Audio Restoration",
                    "desc": "This paper presents Apollo, a generative model specifically designed for high-sample-rate audio restoration. It addresses the challenge of reconstructing undistorted audio from degraded inputs, particularly focusing on preserving low-frequency information while enhancing mid- and high-frequency content. Apollo utilizes a frequency band split module to effectively model the relationships between different frequency bands, leading to improved audio quality. Evaluations show that Apollo outperforms existing models, especially in complex audio scenarios, while also being computationally efficient."
                },
                "zh": {
                    "title": "Apollo：高质量音频修复的新纪元",
                    "desc": "音频修复在现代社会中变得越来越重要，尤其是随着高质量播放设备的普及和生成音频模型的进步。音频修复的任务是从受损的输入中预测出无失真的音频，通常使用生成对抗网络（GAN）框架进行训练，以平衡感知和失真。由于音频降解主要集中在中高频范围，设计一个能够保留低频信息并准确重建高质量中高频内容的生成器是一个关键挑战。我们提出的Apollo模型通过显式的频带分割模块建模不同频带之间的关系，从而实现更连贯和高质量的音频修复。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08272",
            "title": "Click2Mask: Local Editing with Dynamic Mask Generation",
            "url": "https://huggingface.co/papers/2409.08272",
            "abstract": "Recent advancements in generative models have revolutionized image generation and editing, making these tasks accessible to non-experts. This paper focuses on local image editing, particularly the task of adding new content to a loosely specified area. Existing methods often require a precise mask or a detailed description of the location, which can be cumbersome and prone to errors. We propose Click2Mask, a novel approach that simplifies the local editing process by requiring only a single point of reference (in addition to the content description). A mask is dynamically grown around this point during a Blended Latent Diffusion (BLD) process, guided by a masked CLIP-based semantic loss. Click2Mask surpasses the limitations of segmentation-based and fine-tuning dependent methods, offering a more user-friendly and contextually accurate solution. Our experiments demonstrate that Click2Mask not only minimizes user effort but also delivers competitive or superior local image manipulation results compared to SoTA methods, according to both human judgement and automatic metrics. Key contributions include the simplification of user input, the ability to freely add objects unconstrained by existing segments, and the integration potential of our dynamic mask approach within other editing methods.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2024-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "42e1f24dce33e73b",
            "data": {
                "categories": [
                    "#diffusion",
                    "#interpretability",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "🖱️",
                "ru": {
                    "title": "Одним кликом: революция в локальном редактировании изображений",
                    "desc": "Статья представляет новый метод локального редактирования изображений под названием Click2Mask. Этот подход позволяет добавлять новый контент в изображение, используя только одну точку и описание содержимого, вместо точной маски или детального описания местоположения. Click2Mask динамически создает маску вокруг указанной точки во время процесса Blended Latent Diffusion, ориентируясь на семантическую потерю на основе CLIP. Эксперименты показывают, что Click2Mask минимизирует усилия пользователя и обеспечивает конкурентоспособные или превосходящие результаты по сравнению с современными методами."
                },
                "en": {
                    "title": "Effortless Image Editing with Click2Mask",
                    "desc": "This paper introduces Click2Mask, a new method for local image editing that allows users to add content with minimal input. Instead of needing detailed masks or descriptions, users only need to click once to specify a point, and the system automatically generates a mask around it. The method uses a Blended Latent Diffusion process and a masked CLIP-based semantic loss to ensure the added content blends well with the existing image. Click2Mask outperforms traditional methods in both user-friendliness and image quality, making it easier for non-experts to edit images effectively."
                },
                "zh": {
                    "title": "简化局部图像编辑，轻松添加新内容",
                    "desc": "最近生成模型的进展使得图像生成和编辑变得更加简单，普通用户也能轻松使用。本文重点研究局部图像编辑，特别是向模糊指定区域添加新内容的任务。我们提出了一种名为Click2Mask的新方法，只需一个参考点和内容描述即可简化编辑过程。实验表明，Click2Mask在用户努力最小化的同时，提供了与最先进方法相当或更优的局部图像处理结果。"
                }
            }
        }
    ],
    "link_prev": "2024-09-13.html",
    "link_next": "2024-09-17.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "13.09",
        "en": "09/13",
        "zh": "9月13日"
    },
    "short_date_next": {
        "ru": "17.09",
        "en": "09/17",
        "zh": "9月17日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 6,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 4,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0,
        "#compression": 0
    }
}