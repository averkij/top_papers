{
    "date": {
        "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 16",
        "zh": "9æœˆ16æ—¥"
    },
    "time_utc": "2024-09-16 09:00",
    "weekday": 0,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-16",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.08857",
            "title": "InstantDrag: Improving Interactivity in Drag-based Image Editing",
            "url": "https://huggingface.co/papers/2409.08857",
            "abstract": "Drag-based image editing has recently gained popularity for its interactivity and precision. However, despite the ability of text-to-image models to generate samples within a second, drag editing still lags behind due to the challenge of accurately reflecting user interaction while maintaining image content. Some existing approaches rely on computationally intensive per-image optimization or intricate guidance-based methods, requiring additional inputs such as masks for movable regions and text prompts, thereby compromising the interactivity of the editing process. We introduce InstantDrag, an optimization-free pipeline that enhances interactivity and speed, requiring only an image and a drag instruction as input. InstantDrag consists of two carefully designed networks: a drag-conditioned optical flow generator (FlowGen) and an optical flow-conditioned diffusion model (FlowDiffusion). InstantDrag learns motion dynamics for drag-based image editing in real-world video datasets by decomposing the task into motion generation and motion-conditioned image generation. We demonstrate InstantDrag's capability to perform fast, photo-realistic edits without masks or text prompts through experiments on facial video datasets and general scenes. These results highlight the efficiency of our approach in handling drag-based image editing, making it a promising solution for interactive, real-time applications.",
            "score": 30,
            "issue_id": 1,
            "pub_date": "2024-09-13",
            "pub_date_card": {
                "ru": "13 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 13",
                "zh": "9æœˆ13æ—¥"
            },
            "hash": "f1d54686dd0f3e15",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#cv",
                    "#optimization",
                    "#games",
                    "#diffusion",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "ğŸ–±ï¸",
                "ru": {
                    "title": "ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "InstantDrag - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑĞµÑ‚ĞµĞ¹: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ (FlowGen) Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ (FlowDiffusion). InstantDrag Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¼Ğ°ÑĞ¾Ğº Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "InstantDrag: Fast and Interactive Image Editing Made Simple",
                    "desc": "This paper presents InstantDrag, a new method for drag-based image editing that improves speed and interactivity without the need for complex optimizations. Unlike traditional methods that require masks or text prompts, InstantDrag only needs an image and a simple drag instruction. It utilizes two specialized networks: FlowGen, which generates optical flow based on drag conditions, and FlowDiffusion, which creates images conditioned on that flow. The results show that InstantDrag can produce fast and realistic edits, making it suitable for real-time applications in various contexts."
                },
                "zh": {
                    "title": "InstantDragï¼šå¿«é€Ÿã€çœŸå®æ„Ÿçš„æ‹–æ‹½å›¾åƒç¼–è¾‘",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºInstantDragçš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ‹–æ‹½ç¼–è¾‘çš„äº¤äº’æ€§å’Œé€Ÿåº¦ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒInstantDragä¸éœ€è¦å¤æ‚çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œåªéœ€è¾“å…¥ä¸€å¼ å›¾åƒå’Œæ‹–æ‹½æŒ‡ä»¤ã€‚è¯¥æ–¹æ³•ç”±ä¸¤ä¸ªç½‘ç»œç»„æˆï¼šæ‹–æ‹½æ¡ä»¶å…‰æµç”Ÿæˆå™¨å’Œå…‰æµæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ æ‹–æ‹½ç¼–è¾‘ä¸­çš„è¿åŠ¨åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInstantDragèƒ½å¤Ÿå¿«é€Ÿå®ç°çœŸå®æ„Ÿç¼–è¾‘ï¼Œé€‚ç”¨äºå®æ—¶äº¤äº’åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08615",
            "title": "DrawingSpinUp: 3D Animation from Single Character Drawings",
            "url": "https://huggingface.co/papers/2409.08615",
            "abstract": "Animating various character drawings is an engaging visual content creation task. Given a single character drawing, existing animation methods are limited to flat 2D motions and thus lack 3D effects. An alternative solution is to reconstruct a 3D model from a character drawing as a proxy and then retarget 3D motion data onto it. However, the existing image-to-3D methods could not work well for amateur character drawings in terms of appearance and geometry. We observe the contour lines, commonly existing in character drawings, would introduce significant ambiguity in texture synthesis due to their view-dependence. Additionally, thin regions represented by single-line contours are difficult to reconstruct (e.g., slim limbs of a stick figure) due to their delicate structures. To address these issues, we propose a novel system, DrawingSpinUp, to produce plausible 3D animations and breathe life into character drawings, allowing them to freely spin up, leap, and even perform a hip-hop dance. For appearance improvement, we adopt a removal-then-restoration strategy to first remove the view-dependent contour lines and then render them back after retargeting the reconstructed character. For geometry refinement, we develop a skeleton-based thinning deformation algorithm to refine the slim structures represented by the single-line contours. The experimental evaluations and a perceptual user study show that our proposed method outperforms the existing 2D and 3D animation methods and generates high-quality 3D animations from a single character drawing. Please refer to our project page (https://lordliang.github.io/DrawingSpinUp) for the code and generated animations.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-09-13",
            "pub_date_card": {
                "ru": "13 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 13",
                "zh": "9æœˆ13æ—¥"
            },
            "hash": "a61d3cf7a11ab0c1",
            "data": {
                "categories": [
                    "#cv",
                    "#games",
                    "#open_source",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞĞ¶Ğ¸Ğ²Ğ»ÑĞµĞ¼ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¸: Ğ¾Ñ‚ 2D Ğº 3D Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ DrawingSpinUp Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… 3D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ĞºĞ¾Ğ½Ñ‚ÑƒÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ÑƒÑ€Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑƒÑ‚Ğ¾Ğ½Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºĞµĞ»ĞµÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾ Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ, Ğ¿Ñ€Ñ‹Ğ³Ğ°Ñ‚ÑŒ Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ñ‚Ğ°Ğ½Ñ†ĞµĞ²Ğ°Ñ‚ÑŒ Ñ…Ğ¸Ğ¿-Ñ…Ğ¾Ğ¿. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ 2D Ğ¸ 3D Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Breathe Life into Drawings with 3D Animation!",
                    "desc": "This paper presents DrawingSpinUp, a novel system designed to create high-quality 3D animations from single character drawings. Traditional methods struggle with 2D animations and often fail to accurately represent the geometry and appearance of amateur drawings. The proposed approach addresses these challenges by removing view-dependent contour lines before retargeting 3D motion data, and then restoring these contours for improved visual fidelity. Additionally, a skeleton-based thinning deformation algorithm is introduced to enhance the representation of delicate structures, resulting in more realistic animations that outperform existing techniques."
                },
                "zh": {
                    "title": "è®©è§’è‰²ç»˜å›¾åŠ¨èµ·æ¥çš„3DåŠ¨ç”»æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°ç³»ç»ŸDrawingSpinUpï¼Œç”¨äºå°†å•ä¸ªè§’è‰²ç»˜å›¾è½¬åŒ–ä¸ºé€¼çœŸçš„3DåŠ¨ç”»ã€‚ç°æœ‰çš„å›¾åƒåˆ°3Dçš„æ–¹æ³•åœ¨å¤„ç†ä¸šä½™è§’è‰²ç»˜å›¾æ—¶æ•ˆæœä¸ä½³ï¼Œä¸»è¦æ˜¯å› ä¸ºè½®å»“çº¿çš„å­˜åœ¨å¯¼è‡´çº¹ç†åˆæˆçš„æ¨¡ç³Šã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å»é™¤å†æ¢å¤çš„ç­–ç•¥ï¼Œå…ˆå»é™¤è§†è§’ä¾èµ–çš„è½®å»“çº¿ï¼Œå†åœ¨é‡å®šå‘åå°†å…¶æ¢å¤ï¼Œä»¥æ”¹å–„å¤–è§‚ã€‚é€šè¿‡éª¨æ¶åŸºç¡€çš„ç»†åŒ–å˜å½¢ç®—æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°é‡å»ºç»†é•¿ç»“æ„ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„3DåŠ¨ç”»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08947",
            "title": "A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis",
            "url": "https://huggingface.co/papers/2409.08947",
            "abstract": "Relighting radiance fields is severely underconstrained for multi-view data, which is most often captured under a single illumination condition; It is especially hard for full scenes containing multiple objects. We introduce a method to create relightable radiance fields using such single-illumination data by exploiting priors extracted from 2D image diffusion models. We first fine-tune a 2D diffusion model on a multi-illumination dataset conditioned by light direction, allowing us to augment a single-illumination capture into a realistic -- but possibly inconsistent -- multi-illumination dataset from directly defined light directions. We use this augmented data to create a relightable radiance field represented by 3D Gaussian splats. To allow direct control of light direction for low-frequency lighting, we represent appearance with a multi-layer perceptron parameterized on light direction. To enforce multi-view consistency and overcome inaccuracies we optimize a per-image auxiliary feature vector. We show results on synthetic and real multi-view data under single illumination, demonstrating that our method successfully exploits 2D diffusion model priors to allow realistic 3D relighting for complete scenes. Project site https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-13",
            "pub_date_card": {
                "ru": "13 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 13",
                "zh": "9æœˆ13æ—¥"
            },
            "hash": "53dd7086261a9945",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#graphs",
                    "#data",
                    "#diffusion",
                    "#architecture",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ 3D ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 2D Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ· 2D Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ 2D Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ñ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ Ğ¸Ğ·Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ»Ğ°Ñ‚Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ 2D Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ 3D Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Transforming Single Illumination into Realistic 3D Relighting",
                    "desc": "This paper addresses the challenge of relighting radiance fields using multi-view data captured under a single illumination condition. The authors propose a novel method that leverages priors from 2D image diffusion models to generate a more realistic multi-illumination dataset from limited single-illumination captures. By fine-tuning a diffusion model and using 3D Gaussian splats, they create a relightable radiance field that allows for direct control of light direction. The approach also incorporates a multi-layer perceptron to represent appearance based on light direction and optimizes auxiliary feature vectors to ensure consistency across multiple views."
                },
                "zh": {
                    "title": "åˆ©ç”¨äºŒç»´æ‰©æ•£æ¨¡å‹å®ç°ä¸‰ç»´é‡å…‰ç…§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å•ä¸€å…‰ç…§æ¡ä»¶ä¸‹çš„æ•°æ®ç”Ÿæˆå¯é‡å…‰ç…§çš„è¾å°„åœºçš„æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡å¯¹äºŒç»´æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåˆ©ç”¨å¤šå…‰ç…§æ•°æ®é›†çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¢å¼ºäº†å•ä¸€å…‰ç…§æ•è·çš„æ•°æ®ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ä¸‰ç»´é«˜æ–¯ç‚¹äº‘è¡¨ç¤ºå¯é‡å…‰ç…§çš„è¾å°„åœºï¼Œå¹¶é€šè¿‡å¤šå±‚æ„ŸçŸ¥æœºå¯¹å…‰ç…§æ–¹å‘è¿›è¡Œå‚æ•°åŒ–ï¼Œä»¥å®ç°å¯¹ä½é¢‘å…‰ç…§çš„ç›´æ¥æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨åˆæˆå’ŒçœŸå®çš„å¤šè§†è§’æ•°æ®ä¸Šå®ç°é€¼çœŸçš„ä¸‰ç»´é‡å…‰ç…§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08513",
            "title": "Mamba-YOLO-World: Marrying YOLO-World with Mamba for Open-Vocabulary Detection",
            "url": "https://huggingface.co/papers/2409.08513",
            "abstract": "Open-vocabulary detection (OVD) aims to detect objects beyond a predefined set of categories. As a pioneering model incorporating the YOLO series into OVD, YOLO-World is well-suited for scenarios prioritizing speed and efficiency.However, its performance is hindered by its neck feature fusion mechanism, which causes the quadratic complexity and the limited guided receptive fields.To address these limitations, we present Mamba-YOLO-World, a novel YOLO-based OVD model employing the proposed MambaFusion Path Aggregation Network (MambaFusion-PAN) as its neck architecture. Specifically, we introduce an innovative State Space Model-based feature fusion mechanism consisting of a Parallel-Guided Selective Scan algorithm and a Serial-Guided Selective Scan algorithm with linear complexity and globally guided receptive fields. It leverages multi-modal input sequences and mamba hidden states to guide the selective scanning process.Experiments demonstrate that our model outperforms the original YOLO-World on the COCO and LVIS benchmarks in both zero-shot and fine-tuning settings while maintaining comparable parameters and FLOPs. Additionally, it surpasses existing state-of-the-art OVD methods with fewer parameters and FLOPs.",
            "score": 10,
            "issue_id": 1,
            "pub_date": "2024-09-13",
            "pub_date_card": {
                "ru": "13 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 13",
                "zh": "9æœˆ13æ—¥"
            },
            "hash": "8dff1ca21cb53332",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#benchmark",
                    "#games",
                    "#open_source",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ",
                "ru": {
                    "title": "Mamba-YOLO-World: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼",
                    "desc": "Mamba-YOLO-World - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼ (OVD), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ YOLO. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² MambaFusion-PAN, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ YOLO-World. Mamba-YOLO-World Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ YOLO-World Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ OVD Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Mamba-YOLO-World: Efficient Open-Vocabulary Detection Redefined",
                    "desc": "This paper introduces Mamba-YOLO-World, an advanced model for open-vocabulary detection (OVD) that enhances the YOLO framework. It addresses the limitations of the previous YOLO-World model, particularly its inefficient neck feature fusion mechanism, which leads to high computational complexity. The new MambaFusion Path Aggregation Network (MambaFusion-PAN) utilizes a State Space Model-based feature fusion approach, allowing for efficient parallel and serial guided scanning of features. Experimental results show that Mamba-YOLO-World outperforms its predecessor and other leading OVD methods while maintaining a low computational footprint."
                },
                "zh": {
                    "title": "Mamba-YOLO-Worldï¼šé«˜æ•ˆçš„å¼€æ”¾è¯æ±‡æ£€æµ‹æ–°æ¨¡å‹",
                    "desc": "å¼€æ”¾è¯æ±‡æ£€æµ‹ï¼ˆOVDï¼‰æ—¨åœ¨è¯†åˆ«è¶…å‡ºé¢„å®šä¹‰ç±»åˆ«çš„ç‰©ä½“ã€‚æœ¬æ–‡æå‡ºçš„Mamba-YOLO-Worldæ¨¡å‹ï¼Œé‡‡ç”¨äº†æ–°é¢–çš„MambaFusionè·¯å¾„èšåˆç½‘ç»œä½œä¸ºå…¶é¢ˆéƒ¨æ¶æ„ï¼Œè§£å†³äº†YOLO-Worldåœ¨ç‰¹å¾èåˆæœºåˆ¶ä¸Šçš„å±€é™æ€§ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„ç‰¹å¾èåˆæœºåˆ¶ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦å’Œå…¨å±€å¼•å¯¼æ„Ÿå—é‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€è¾“å…¥åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMamba-YOLO-Worldåœ¨COCOå’ŒLVISåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†åŸå§‹YOLO-Worldï¼Œå¹¶åœ¨å‚æ•°å’ŒFLOPsä¸Šä¿æŒç›¸ä¼¼ï¼Œä¸”åœ¨ç°æœ‰çš„OVDæ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08353",
            "title": "Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos",
            "url": "https://huggingface.co/papers/2409.08353",
            "abstract": "Volumetric video represents a transformative advancement in visual media, enabling users to freely navigate immersive virtual experiences and narrowing the gap between digital and real worlds. However, the need for extensive manual intervention to stabilize mesh sequences and the generation of excessively large assets in existing workflows impedes broader adoption. In this paper, we present a novel Gaussian-based approach, dubbed DualGS, for real-time and high-fidelity playback of complex human performance with excellent compression ratios. Our key idea in DualGS is to separately represent motion and appearance using the corresponding skin and joint Gaussians. Such an explicit disentanglement can significantly reduce motion redundancy and enhance temporal coherence. We begin by initializing the DualGS and anchoring skin Gaussians to joint Gaussians at the first frame. Subsequently, we employ a coarse-to-fine training strategy for frame-by-frame human performance modeling. It includes a coarse alignment phase for overall motion prediction as well as a fine-grained optimization for robust tracking and high-fidelity rendering. To integrate volumetric video seamlessly into VR environments, we efficiently compress motion using entropy encoding and appearance using codec compression coupled with a persistent codebook. Our approach achieves a compression ratio of up to 120 times, only requiring approximately 350KB of storage per frame. We demonstrate the efficacy of our representation through photo-realistic, free-view experiences on VR headsets, enabling users to immersively watch musicians in performance and feel the rhythm of the notes at the performers' fingertips.",
            "score": 10,
            "issue_id": 1,
            "pub_date": "2024-09-12",
            "pub_date_card": {
                "ru": "12 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 12",
                "zh": "9æœˆ12æ—¥"
            },
            "hash": "c6b38fb50d06c2f1",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#training",
                    "#optimization",
                    "#compression",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ•º",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾: Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ DualGS Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ¶Ğ¸. DualGS Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - Ğ´Ğ¾ 120 Ñ€Ğ°Ğ·, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ Ğ¾ĞºĞ¾Ğ»Ğ¾ 350 ĞšĞ‘ Ğ½Ğ° ĞºĞ°Ğ´Ñ€."
                },
                "en": {
                    "title": "Revolutionizing Volumetric Video with DualGS: Immersive Experiences Made Efficient!",
                    "desc": "This paper introduces DualGS, a novel Gaussian-based method for enhancing volumetric video playback, particularly for complex human performances. By separately modeling motion and appearance with skin and joint Gaussians, the approach reduces redundancy and improves the temporal coherence of the video. The method employs a coarse-to-fine training strategy for accurate motion prediction and high-fidelity rendering, achieving impressive compression ratios of up to 120 times. Ultimately, DualGS enables immersive experiences in virtual reality with minimal storage requirements, allowing users to engage with performances in a more interactive way."
                },
                "zh": {
                    "title": "DualGSï¼šé«˜æ•ˆå‹ç¼©ä¸æ²‰æµ¸å¼ä½“éªŒçš„ç»“åˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„é«˜æ–¯åŸºç¡€æ–¹æ³•ï¼Œç§°ä¸ºDualGSï¼Œç”¨äºå®æ—¶æ’­æ”¾å¤æ‚çš„äººç±»è¡¨æ¼”ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†åˆ«è¡¨ç¤ºè¿åŠ¨å’Œå¤–è§‚ï¼Œæ˜¾è‘—å‡å°‘äº†è¿åŠ¨å†—ä½™å¹¶å¢å¼ºäº†æ—¶é—´ä¸€è‡´æ€§ã€‚DualGSé‡‡ç”¨ç²—åˆ°ç»†çš„è®­ç»ƒç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨è™šæ‹Ÿç°å®ç¯å¢ƒä¸­é«˜æ•ˆå‹ç¼©è§†é¢‘æ•°æ®ï¼Œå‹ç¼©æ¯”é«˜è¾¾120å€ã€‚æœ€ç»ˆï¼Œç”¨æˆ·å¯ä»¥åœ¨VRå¤´æˆ´è®¾å¤‡ä¸Šæ²‰æµ¸å¼è§‚çœ‹è¡¨æ¼”ï¼Œæ„Ÿå—éŸ³ä¹çš„èŠ‚å¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08514",
            "title": "Apollo: Band-sequence Modeling for High-Quality Audio Restoration",
            "url": "https://huggingface.co/papers/2409.08514",
            "abstract": "Audio restoration has become increasingly significant in modern society, not only due to the demand for high-quality auditory experiences enabled by advanced playback devices, but also because the growing capabilities of generative audio models necessitate high-fidelity audio. Typically, audio restoration is defined as a task of predicting undistorted audio from damaged input, often trained using a GAN framework to balance perception and distortion. Since audio degradation is primarily concentrated in mid- and high-frequency ranges, especially due to codecs, a key challenge lies in designing a generator capable of preserving low-frequency information while accurately reconstructing high-quality mid- and high-frequency content. Inspired by recent advancements in high-sample-rate music separation, speech enhancement, and audio codec models, we propose Apollo, a generative model designed for high-sample-rate audio restoration. Apollo employs an explicit frequency band split module to model the relationships between different frequency bands, allowing for more coherent and higher-quality restored audio. Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently outperforms existing SR-GAN models across various bit rates and music genres, particularly excelling in complex scenarios involving mixtures of multiple instruments and vocals. Apollo significantly improves music restoration quality while maintaining computational efficiency. The source code for Apollo is publicly available at https://github.com/JusperLee/Apollo.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-13",
            "pub_date_card": {
                "ru": "13 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 13",
                "zh": "9æœˆ13æ—¥"
            },
            "hash": "1d46169e43987c03",
            "data": {
                "categories": [
                    "#audio",
                    "#dataset",
                    "#games",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Apollo: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Apollo - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸. Apollo Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ SR-GAN Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ°Ñ… Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¶Ğ°Ğ½Ñ€Ğ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ¼Ğ¸ĞºÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¾ĞºĞ°Ğ»Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Apollo: Revolutionizing High-Quality Audio Restoration",
                    "desc": "This paper presents Apollo, a generative model specifically designed for high-sample-rate audio restoration. It addresses the challenge of reconstructing undistorted audio from degraded inputs, particularly focusing on preserving low-frequency information while enhancing mid- and high-frequency content. Apollo utilizes a frequency band split module to effectively model the relationships between different frequency bands, leading to improved audio quality. Evaluations show that Apollo outperforms existing models, especially in complex audio scenarios, while also being computationally efficient."
                },
                "zh": {
                    "title": "Apolloï¼šé«˜è´¨é‡éŸ³é¢‘ä¿®å¤çš„æ–°çºªå…ƒ",
                    "desc": "éŸ³é¢‘ä¿®å¤åœ¨ç°ä»£ç¤¾ä¼šä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œå°¤å…¶æ˜¯éšç€é«˜è´¨é‡æ’­æ”¾è®¾å¤‡çš„æ™®åŠå’Œç”ŸæˆéŸ³é¢‘æ¨¡å‹çš„è¿›æ­¥ã€‚éŸ³é¢‘ä¿®å¤çš„ä»»åŠ¡æ˜¯ä»å—æŸçš„è¾“å…¥ä¸­é¢„æµ‹å‡ºæ— å¤±çœŸçš„éŸ³é¢‘ï¼Œé€šå¸¸ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œä»¥å¹³è¡¡æ„ŸçŸ¥å’Œå¤±çœŸã€‚ç”±äºéŸ³é¢‘é™è§£ä¸»è¦é›†ä¸­åœ¨ä¸­é«˜é¢‘èŒƒå›´ï¼Œè®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿä¿ç•™ä½é¢‘ä¿¡æ¯å¹¶å‡†ç¡®é‡å»ºé«˜è´¨é‡ä¸­é«˜é¢‘å†…å®¹çš„ç”Ÿæˆå™¨æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºçš„Apolloæ¨¡å‹é€šè¿‡æ˜¾å¼çš„é¢‘å¸¦åˆ†å‰²æ¨¡å—å»ºæ¨¡ä¸åŒé¢‘å¸¦ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œå®ç°æ›´è¿è´¯å’Œé«˜è´¨é‡çš„éŸ³é¢‘ä¿®å¤ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.08272",
            "title": "Click2Mask: Local Editing with Dynamic Mask Generation",
            "url": "https://huggingface.co/papers/2409.08272",
            "abstract": "Recent advancements in generative models have revolutionized image generation and editing, making these tasks accessible to non-experts. This paper focuses on local image editing, particularly the task of adding new content to a loosely specified area. Existing methods often require a precise mask or a detailed description of the location, which can be cumbersome and prone to errors. We propose Click2Mask, a novel approach that simplifies the local editing process by requiring only a single point of reference (in addition to the content description). A mask is dynamically grown around this point during a Blended Latent Diffusion (BLD) process, guided by a masked CLIP-based semantic loss. Click2Mask surpasses the limitations of segmentation-based and fine-tuning dependent methods, offering a more user-friendly and contextually accurate solution. Our experiments demonstrate that Click2Mask not only minimizes user effort but also delivers competitive or superior local image manipulation results compared to SoTA methods, according to both human judgement and automatic metrics. Key contributions include the simplification of user input, the ability to freely add objects unconstrained by existing segments, and the integration potential of our dynamic mask approach within other editing methods.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2024-09-12",
            "pub_date_card": {
                "ru": "12 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 12",
                "zh": "9æœˆ12æ—¥"
            },
            "hash": "42e1f24dce33e73b",
            "data": {
                "categories": [
                    "#diffusion",
                    "#interpretability",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ–±ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ½Ğ¸Ğ¼ ĞºĞ»Ğ¸ĞºĞ¾Ğ¼: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Click2Mask. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ñƒ Ñ‚Ğ¾Ñ‡ĞºÑƒ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. Click2Mask Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ°ÑĞºÑƒ Ğ²Ğ¾ĞºÑ€ÑƒĞ³ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Blended Latent Diffusion, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CLIP. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Click2Mask Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ¸Ğ»Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Effortless Image Editing with Click2Mask",
                    "desc": "This paper introduces Click2Mask, a new method for local image editing that allows users to add content with minimal input. Instead of needing detailed masks or descriptions, users only need to click once to specify a point, and the system automatically generates a mask around it. The method uses a Blended Latent Diffusion process and a masked CLIP-based semantic loss to ensure the added content blends well with the existing image. Click2Mask outperforms traditional methods in both user-friendliness and image quality, making it easier for non-experts to edit images effectively."
                },
                "zh": {
                    "title": "ç®€åŒ–å±€éƒ¨å›¾åƒç¼–è¾‘ï¼Œè½»æ¾æ·»åŠ æ–°å†…å®¹",
                    "desc": "æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•ä½¿å¾—å›¾åƒç”Ÿæˆå’Œç¼–è¾‘å˜å¾—æ›´åŠ ç®€å•ï¼Œæ™®é€šç”¨æˆ·ä¹Ÿèƒ½è½»æ¾ä½¿ç”¨ã€‚æœ¬æ–‡é‡ç‚¹ç ”ç©¶å±€éƒ¨å›¾åƒç¼–è¾‘ï¼Œç‰¹åˆ«æ˜¯å‘æ¨¡ç³ŠæŒ‡å®šåŒºåŸŸæ·»åŠ æ–°å†…å®¹çš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºClick2Maskçš„æ–°æ–¹æ³•ï¼Œåªéœ€ä¸€ä¸ªå‚è€ƒç‚¹å’Œå†…å®¹æè¿°å³å¯ç®€åŒ–ç¼–è¾‘è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒClick2Maskåœ¨ç”¨æˆ·åŠªåŠ›æœ€å°åŒ–çš„åŒæ—¶ï¼Œæä¾›äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜çš„å±€éƒ¨å›¾åƒå¤„ç†ç»“æœã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-09-13.html",
    "link_next": "2024-09-17.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "13.09",
        "en": "09/13",
        "zh": "9æœˆ13æ—¥"
    },
    "short_date_next": {
        "ru": "17.09",
        "en": "09/17",
        "zh": "9æœˆ17æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 6,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 4,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0,
        "#compression": 0
    }
}