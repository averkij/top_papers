{
    "date": {
        "ru": "25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 25",
        "zh": "4æœˆ25æ—¥"
    },
    "time_utc": "2025-04-25 11:09",
    "weekday": 4,
    "issue_id": 3435,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.17761",
            "title": "Step1X-Edit: A Practical Framework for General Image Editing",
            "url": "https://huggingface.co/papers/2504.17761",
            "abstract": "In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. However, there is still a large gap between the open-source algorithm with these closed-source models. Thus, in this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against the closed-source models like GPT-4o and Gemini2 Flash. More specifically, we adopt the Multimodal LLM to process the reference image and the user's editing instruction. A latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. To train the model, we build a data generation pipeline to produce a high-quality dataset. For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions. Experimental results on GEdit-Bench demonstrate that Step1X-Edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.",
            "score": 41,
            "issue_id": 3427,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "2896842e49a93757",
            "authors": [
                "Shiyu Liu",
                "Yucheng Han",
                "Peng Xing",
                "Fukun Yin",
                "Rui Wang",
                "Wei Cheng",
                "Jiaqi Liao",
                "Yingming Wang",
                "Honghao Fu",
                "Chunrui Han",
                "Guopeng Li",
                "Yuang Peng",
                "Quan Sun",
                "Jingwei Wu",
                "Yan Cai",
                "Zheng Ge",
                "Ranchen Ming",
                "Lei Xia",
                "Xianfang Zeng",
                "Yibo Zhu",
                "Binxing Jiao",
                "Xiangyu Zhang",
                "Gang Yu",
                "Daxin Jiang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.17761.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#diffusion",
                    "#training",
                    "#multimodal",
                    "#cv",
                    "#dataset",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Step1X-Edit: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Step1X-Edit, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ GPT-4o Ğ¸ Gemini2 Flash. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ LLM Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GEdit-Bench. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Step1X-Edit Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Bridging the Gap in Image Editing with Step1X-Edit",
                    "desc": "This paper presents Step1X-Edit, a new image editing model that aims to match the performance of advanced closed-source models like GPT-4o and Gemini2 Flash. It utilizes a Multimodal LLM to effectively process both the reference image and user editing instructions, integrating a latent embedding with a diffusion image decoder to generate the final edited image. The authors also introduce GEdit-Bench, a benchmark designed to evaluate the model based on real-world user instructions. Experimental results show that Step1X-Edit significantly outperforms existing open-source models and approaches the capabilities of leading proprietary systems, marking a notable advancement in image editing technology."
                },
                "zh": {
                    "title": "å¼€æºå›¾åƒç¼–è¾‘çš„æœªæ¥ï¼šStep1X-Edit",
                    "desc": "è¿‘å¹´æ¥ï¼Œå›¾åƒç¼–è¾‘æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„å‘å±•ã€‚æ–°å‘å¸ƒçš„å¤šæ¨¡æ€æ¨¡å‹å¦‚GPT-4oå’ŒGemini2 Flashå±•ç°äº†å¼ºå¤§çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ»¡è¶³å¤§å¤šæ•°ç”¨æˆ·çš„ç¼–è¾‘éœ€æ±‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºStep1X-Editçš„å…ˆè¿›å›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œæ—¨åœ¨ä¸è¿™äº›é—­æºæ¨¡å‹çš„æ€§èƒ½ç›¸åª²ç¾ã€‚é€šè¿‡é‡‡ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¤„ç†å‚è€ƒå›¾åƒå’Œç”¨æˆ·ç¼–è¾‘æŒ‡ä»¤ï¼Œå¹¶ç»“åˆæ‰©æ•£å›¾åƒè§£ç å™¨ï¼ŒStep1X-Editåœ¨çœŸå®ç”¨æˆ·æŒ‡ä»¤çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰çš„å¼€æºåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17502",
            "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image\n  Generation",
            "url": "https://huggingface.co/papers/2504.17502",
            "abstract": "Subject-driven text-to-image (T2I) generation aims to produce images that align with a given textual description, while preserving the visual identity from a referenced subject image. Despite its broad downstream applicability -- ranging from enhanced personalization in image generation to consistent character representation in video rendering -- progress in this field is limited by the lack of reliable automatic evaluation. Existing methods either assess only one aspect of the task (i.e., textual alignment or subject preservation), misalign with human judgments, or rely on costly API-based evaluation. To address this, we introduce RefVNLI, a cost-effective metric that evaluates both textual alignment and subject preservation in a single prediction. Trained on a large-scale dataset derived from video-reasoning benchmarks and image perturbations, RefVNLI outperforms or matches existing baselines across multiple benchmarks and subject categories (e.g., Animal, Object), achieving up to 6.4-point gains in textual alignment and 8.5-point gains in subject consistency. It also excels with lesser-known concepts, aligning with human preferences at over 87\\% accuracy.",
            "score": 37,
            "issue_id": 3433,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "5f4407bb2bd57352",
            "authors": [
                "Aviv Slobodkin",
                "Hagai Taitelbaum",
                "Yonatan Bitton",
                "Brian Gordon",
                "Michal Sokolik",
                "Nitzan Bitton Guetta",
                "Almog Gueta",
                "Royi Rassin",
                "Itay Laish",
                "Dani Lischinski",
                "Idan Szpektor"
            ],
            "affiliations": [
                "Ben Gurion University",
                "Google Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17502.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "RefVNLI: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ RefVNLI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. RefVNLI Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 6.4 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ 8.5 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "RefVNLI: A New Standard for Evaluating Text-to-Image Generation",
                    "desc": "This paper presents RefVNLI, a new metric for evaluating subject-driven text-to-image (T2I) generation. It effectively measures both textual alignment with descriptions and preservation of visual identity from reference images. By training on a large dataset, RefVNLI shows significant improvements over existing evaluation methods, achieving higher accuracy in aligning generated images with human preferences. This advancement allows for better personalization and consistency in image generation tasks, making it a valuable tool for future developments in the field."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬ä¸å›¾åƒä¸€è‡´æ€§çš„è¯„ä¼°æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æè¿°ç”Ÿæˆä¸å‚è€ƒå›¾åƒä¸€è‡´çš„å›¾åƒã€‚å°½ç®¡è¯¥é¢†åŸŸæœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œä½†ç”±äºç¼ºä¹å¯é çš„è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ï¼Œè¿›å±•å—åˆ°é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†RefVNLIï¼Œè¿™æ˜¯ä¸€ç§ç»æµé«˜æ•ˆçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¯ä»¥åŒæ—¶è¯„ä¼°æ–‡æœ¬å¯¹é½å’Œä¸»é¢˜ä¿ç•™ã€‚ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†çš„è®­ç»ƒï¼ŒRefVNLIåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ–‡æœ¬å¯¹é½å’Œä¸»é¢˜ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17192",
            "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
            "url": "https://huggingface.co/papers/2504.17192",
            "abstract": "Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins.",
            "score": 32,
            "issue_id": 3430,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "01c5ca3f5c3906ce",
            "authors": [
                "Minju Seo",
                "Jinheon Baek",
                "Seongyun Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17192.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#architecture",
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#science"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğº ĞºĞ¾Ğ´Ñƒ: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "PaperCoder - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ¿Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ„Ğ°Ğ·Ñ‹. PaperCoder Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ PaperBench."
                },
                "en": {
                    "title": "Transforming Research into Code with PaperCoder",
                    "desc": "This paper presents PaperCoder, a framework that uses Large Language Models (LLMs) to convert machine learning research papers into functional code repositories. The process involves three main stages: planning, analysis, and generation, each handled by specialized agents that work together. In the planning stage, a roadmap and system architecture are created, while the analysis stage focuses on understanding implementation details. Finally, the generation stage produces modular code that respects dependencies, and the framework has been shown to outperform existing methods in generating high-quality implementations."
                },
                "zh": {
                    "title": "PaperCoderï¼šå°†è®ºæ–‡è½¬åŒ–ä¸ºä»£ç çš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "å°½ç®¡æœºå™¨å­¦ä¹ ç ”ç©¶è¿…é€Ÿå‘å±•ï¼Œä½†ç›¸åº”çš„ä»£ç å®ç°å¾€å¾€ç¼ºä¹ï¼Œå¯¼è‡´ç ”ç©¶äººå‘˜åœ¨é‡ç°ç»“æœå’ŒåŸºäºå…ˆå‰å·¥ä½œè¿›è¡Œæ„å»ºæ—¶è€—æ—¶è´¹åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PaperCoderï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œå¯ä»¥å°†æœºå™¨å­¦ä¹ è®ºæ–‡è½¬åŒ–ä¸ºåŠŸèƒ½æ€§ä»£ç åº“ã€‚PaperCoderåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šè§„åˆ’ã€åˆ†æå’Œç”Ÿæˆï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰ä¸“é—¨çš„æ™ºèƒ½ä½“åä½œå®Œæˆä»»åŠ¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒPaperCoderåœ¨ç”Ÿæˆé«˜è´¨é‡ã€å¿ å®çš„å®ç°æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨æœ€æ–°çš„PaperBenchåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å¼ºæœ‰åŠ›çš„åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17432",
            "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
            "url": "https://huggingface.co/papers/2504.17432",
            "abstract": "The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-text encoding, and (3) deficient compositionality due to bag-of-words behavior. While recent Multimodal Large Language Models (MLLMs) have demonstrated significant advances in generalized vision-language understanding, their potential for learning transferable multimodal representations remains underexplored.In this work, we present UniME (Universal Multimodal Embedding), a novel two-stage framework that leverages MLLMs to learn discriminative representations for diverse downstream tasks. In the first stage, we perform textual discriminative knowledge distillation from a powerful LLM-based teacher model to enhance the embedding capability of the MLLM\\'s language component. In the second stage, we introduce hard negative enhanced instruction tuning to further advance discriminative representation learning. Specifically, we initially mitigate false negative contamination and then sample multiple hard negatives per instance within each batch, forcing the model to focus on challenging samples. This approach not only improves discriminative power but also enhances instruction-following ability in downstream tasks. We conduct extensive experiments on the MMEB benchmark and multiple retrieval tasks, including short and long caption retrieval and compositional retrieval. Results demonstrate that UniME achieves consistent performance improvement across all tasks, exhibiting superior discriminative and compositional capabilities.",
            "score": 21,
            "issue_id": 3427,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "3a77377667aeb98f",
            "authors": [
                "Tiancheng Gu",
                "Kaicheng Yang",
                "Ziyong Feng",
                "Xingjun Wang",
                "Yanzhao Zhang",
                "Dingkun Long",
                "Yingda Chen",
                "Weidong Cai",
                "Jiankang Deng"
            ],
            "affiliations": [
                "DeepGlint Tongyi Lab, Alibaba Group",
                "Imperial College London",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17432.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#transfer_learning",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "UniME: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ",
                    "desc": "UniME - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ LLM-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° MLLM. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMEB Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ UniME Ğ² Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Multimodal Learning with UniME",
                    "desc": "The paper introduces UniME, a new framework designed to improve multimodal representation learning by addressing limitations in existing models like CLIP. It utilizes a two-stage process where the first stage enhances the language component of a Multimodal Large Language Model (MLLM) through knowledge distillation from a teacher model. The second stage employs hard negative enhanced instruction tuning to refine the model's ability to distinguish between challenging samples. Experimental results show that UniME significantly boosts performance in various image-text retrieval tasks, demonstrating better discriminative and compositional skills."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€å­¦ä¹ çš„åŒºåˆ†èƒ½åŠ›ä¸ç»„åˆèƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€åµŒå…¥æ¡†æ¶UniMEï¼ˆé€šç”¨å¤šæ¨¡æ€åµŒå…¥ï¼‰ï¼Œæ—¨åœ¨å…‹æœç°æœ‰CLIPæ¡†æ¶çš„å±€é™æ€§ã€‚UniMEé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„å­¦ä¹ ï¼Œé¦–å…ˆä»å¼ºå¤§çš„è¯­è¨€æ¨¡å‹æ•™å¸ˆæ¨¡å‹ä¸­è¿›è¡Œæ–‡æœ¬çŸ¥è¯†è’¸é¦ï¼Œä»¥å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯­è¨€ç»„ä»¶åµŒå…¥èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å¼•å…¥å›°éš¾è´Ÿæ ·æœ¬å¢å¼ºçš„æŒ‡ä»¤è°ƒä¼˜ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„åŒºåˆ†èƒ½åŠ›å’ŒæŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniMEåœ¨å¤šä¸ªæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰æ›´å¼ºçš„åŒºåˆ†æ€§å’Œç»„åˆèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17207",
            "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation",
            "url": "https://huggingface.co/papers/2504.17207",
            "abstract": "We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. Perspective-taking, the ability to perceive an environment or situation from an alternative viewpoint, is a key benchmark for human-level visual understanding, essential for environmental interaction and collaboration with autonomous agents. Despite advancements in spatial reasoning within VLMs, recent research has shown that modern VLMs significantly lack perspective-aware reasoning capabilities and exhibit a strong bias toward egocentric interpretations. To bridge the gap between VLMs and human perception, we focus on the role of mental imagery, where humans perceive the world through abstracted representations that facilitate perspective shifts. Motivated by this, we propose a framework for perspective-aware reasoning, named Abstract Perspective Change (APC), that effectively leverages vision foundation models, such as object detection, segmentation, and orientation estimation, to construct scene abstractions and enable perspective transformations. Our experiments on synthetic and real-image benchmarks, compared with various VLMs, demonstrate significant improvements in perspective-aware reasoning with our framework, further outperforming fine-tuned spatial reasoning models and novel-view-synthesis-based approaches.",
            "score": 15,
            "issue_id": 3427,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "a45aa292431b93b2",
            "authors": [
                "Phillip Y. Lee",
                "Jihyeon Je",
                "Chanho Park",
                "Mikaela Angelina Uy",
                "Leonidas Guibas",
                "Minhyuk Sung"
            ],
            "affiliations": [
                "KAIST",
                "NVIDIA",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17207.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (VLM) Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Abstract Perspective Change (APC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ ÑÑ†ĞµĞ½ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ VLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing VLMs with Perspective-Aware Reasoning through Mental Imagery",
                    "desc": "This paper introduces a new framework called Abstract Perspective Change (APC) aimed at enhancing perspective-aware reasoning in vision-language models (VLMs). The authors highlight that current VLMs struggle with understanding different viewpoints, often defaulting to egocentric perspectives. By simulating mental imagery, the APC framework allows VLMs to create abstract representations of scenes, facilitating perspective shifts. Experimental results show that APC significantly improves perspective reasoning compared to existing models, including those specifically fine-tuned for spatial reasoning."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§’æ„ŸçŸ¥èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¿ƒç†å›¾åƒæ¨¡æ‹Ÿå®ç°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­è§†è§’æ„ŸçŸ¥æ¨ç†çš„æ¡†æ¶ã€‚è§†è§’è½¬æ¢æ˜¯æŒ‡ä»ä¸åŒçš„è§†è§’æ„ŸçŸ¥ç¯å¢ƒæˆ–æƒ…å¢ƒï¼Œè¿™å¯¹äºäººç±»çš„è§†è§‰ç†è§£è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨ä¸è‡ªä¸»ä»£ç†çš„äº’åŠ¨ä¸­ã€‚å°½ç®¡VLMsåœ¨ç©ºé—´æ¨ç†æ–¹é¢å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†ç ”ç©¶è¡¨æ˜ç°ä»£VLMsåœ¨è§†è§’æ„ŸçŸ¥æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œä¸”åå‘äºè‡ªæˆ‘ä¸­å¿ƒçš„è§£é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºæŠ½è±¡è§†è§’å˜åŒ–ï¼ˆAPCï¼‰çš„æ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æ„å»ºåœºæ™¯æŠ½è±¡å¹¶å®ç°è§†è§’è½¬æ¢ï¼Œä»è€Œæ˜¾è‘—æå‡äº†è§†è§’æ„ŸçŸ¥æ¨ç†çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16511",
            "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining",
            "url": "https://huggingface.co/papers/2504.16511",
            "abstract": "Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approaches overlook the inherent trade-off between quality and diversity, necessitating their joint consideration. Given a fixed training quota, it is essential to evaluate both the quality of each data point and its complementary effect on the overall dataset. In this paper, we introduce a unified data selection framework called QuaDMix, which automatically optimizes the data distribution for LLM pretraining while balancing both quality and diversity. Specifically, we first propose multiple criteria to measure data quality and employ domain classification to distinguish data points, thereby measuring overall diversity. QuaDMix then employs a unified parameterized data sampling function that determines the sampling probability of each data point based on these quality and diversity related labels. To accelerate the search for the optimal parameters involved in the QuaDMix framework, we conduct simulated experiments on smaller models and use LightGBM for parameters searching, inspired by the RegMix method. Our experiments across diverse models and datasets demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks. These results outperform the independent strategies for quality and diversity, highlighting the necessity and ability to balance data quality and diversity.",
            "score": 13,
            "issue_id": 3431,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 23",
                "zh": "4æœˆ23æ—¥"
            },
            "hash": "5f8a634a52f613d1",
            "authors": [
                "Fengze Liu",
                "Weidong Zhou",
                "Binbin Liu",
                "Zhimiao Yu",
                "Yifan Zhang",
                "Haobin Lin",
                "Yifeng Yu",
                "Xiaohuan Zhou",
                "Taifeng Wang",
                "Yong Cao"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16511.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#data",
                    "#optimization"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ QuaDMix - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). QuaDMix ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ QuaDMix ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 7.2% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ."
                },
                "en": {
                    "title": "Balancing Quality and Diversity in LLM Training with QuaDMix",
                    "desc": "This paper presents QuaDMix, a new framework for selecting training data for large language models (LLMs) that optimally balances quality and diversity. Traditional methods often treat these two metrics separately, which can lead to suboptimal performance. QuaDMix introduces a unified approach that evaluates both the quality of individual data points and their contribution to the overall diversity of the dataset. The framework uses a parameterized sampling function and employs machine learning techniques to enhance the selection process, resulting in a significant performance boost in LLM training."
                },
                "zh": {
                    "title": "å¹³è¡¡è´¨é‡ä¸å¤šæ ·æ€§ï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºQuaDMixçš„ç»Ÿä¸€æ•°æ®é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸åˆ†åˆ«ä¼˜åŒ–è¿™ä¸¤ä¸ªæŒ‡æ ‡ï¼Œå¿½è§†äº†å®ƒä»¬ä¹‹é—´çš„å†…åœ¨æƒè¡¡ã€‚QuaDMixé€šè¿‡å¤šä¸ªæ ‡å‡†æ¥è¯„ä¼°æ•°æ®è´¨é‡ï¼Œå¹¶åˆ©ç”¨é¢†åŸŸåˆ†ç±»æ¥åŒºåˆ†æ•°æ®ç‚¹ï¼Œä»è€Œè¡¡é‡æ•´ä½“å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQuaDMixåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†7.2%çš„æ€§èƒ½ï¼Œè¯æ˜äº†åŒæ—¶å¹³è¡¡æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§çš„å¿…è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17789",
            "title": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models",
            "url": "https://huggingface.co/papers/2504.17789",
            "abstract": "Autoregressive (AR) models, long dominant in language generation, are increasingly applied to image synthesis but are often considered less competitive than Diffusion-based models. A primary limitation is the substantial number of image tokens required for AR models, which constrains both training and inference efficiency, as well as image resolution. To address this, we present Token-Shuffle, a novel yet simple method that reduces the number of image tokens in Transformer. Our key insight is the dimensional redundancy of visual vocabularies in Multimodal Large Language Models (MLLMs), where low-dimensional visual codes from visual encoder are directly mapped to high-dimensional language vocabularies. Leveraging this, we consider two key operations: token-shuffle, which merges spatially local tokens along channel dimension to decrease the input token number, and token-unshuffle, which untangles the inferred tokens after Transformer blocks to restore the spatial arrangement for output. Jointly training with textual prompts, our strategy requires no additional pretrained text-encoder and enables MLLMs to support extremely high-resolution image synthesis in a unified next-token prediction way while maintaining efficient training and inference. For the first time, we push the boundary of AR text-to-image generation to a resolution of 2048x2048 with gratifying generation performance. In GenAI-benchmark, our 2.7B model achieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen by 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human evaluations also demonstrate our prominent image generation ability in terms of text-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle can serve as a foundational design for efficient high-resolution image generation within MLLMs.",
            "score": 4,
            "issue_id": 3431,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "1c3c71248cdb19ca",
            "authors": [
                "Xu Ma",
                "Peize Sun",
                "Haoyu Ma",
                "Hao Tang",
                "Chih-Yao Ma",
                "Jialiang Wang",
                "Kunpeng Li",
                "Xiaoliang Dai",
                "Yujun Shi",
                "Xuan Ju",
                "Yushi Hu",
                "Artsiom Sanakoyeu",
                "Felix Juefei-Xu",
                "Ji Hou",
                "Junjiao Tian",
                "Tao Xu",
                "Tingbo Hou",
                "Yen-Cheng Liu",
                "Zecheng He",
                "Zijian He",
                "Matt Feiszli",
                "Peizhao Zhang",
                "Peter Vajda",
                "Sam Tsai",
                "Yun Fu"
            ],
            "affiliations": [
                "Meta FAIR",
                "Meta GenAI",
                "National University of Singapore",
                "Northeastern University",
                "The Chinese University of Hong Kong",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17789.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#diffusion",
                    "#cv",
                    "#benchmark",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Token-Shuffle: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Token-Shuffle Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Transformer, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Token-Shuffle Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (Ğ´Ğ¾ 2048x2048) Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 2,7 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Token-Shuffle: Revolutionizing High-Resolution Image Generation with Efficiency",
                    "desc": "This paper introduces Token-Shuffle, a new method that enhances autoregressive (AR) models for image synthesis by reducing the number of image tokens needed. The authors leverage the redundancy in visual vocabularies of Multimodal Large Language Models (MLLMs) to improve efficiency in both training and inference. By implementing token-shuffle and token-unshuffle operations, they can maintain high-resolution outputs while simplifying the input requirements. Their approach achieves impressive results, generating images at a resolution of 2048x2048 and outperforming existing AR and diffusion models in various benchmarks."
                },
                "zh": {
                    "title": "Token-Shuffleï¼šé«˜æ•ˆé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºToken-Shuffleçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹åœ¨å›¾åƒåˆæˆä¸­çš„æ•ˆç‡ã€‚é€šè¿‡å‡å°‘è¾“å…¥å›¾åƒæ ‡è®°çš„æ•°é‡ï¼ŒToken-Shuffleèƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢å¤–é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨çš„æƒ…å†µä¸‹ï¼Œå®ç°é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰è¯æ±‡çš„ç»´åº¦å†—ä½™ï¼Œç»“åˆäº†æ ‡è®°æ··æ´—å’Œæ ‡è®°è§£æ··æ´—æ“ä½œã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„2.7Bæ¨¡å‹åœ¨GenAIåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ¨åŠ¨äº†ARæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„åˆ†è¾¨ç‡è¾¾åˆ°2048x2048ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17069",
            "title": "Distilling semantically aware orders for autoregressive image generation",
            "url": "https://huggingface.co/papers/2504.17069",
            "abstract": "Autoregressive patch-based image generation has recently shown competitive results in terms of image quality and scalability. It can also be easily integrated and scaled within Vision-Language models. Nevertheless, autoregressive models require a defined order for patch generation. While a natural order based on the dictation of the words makes sense for text generation, there is no inherent generation order that exists for image generation. Traditionally, a raster-scan order (from top-left to bottom-right) guides autoregressive image generation models. In this paper, we argue that this order is suboptimal, as it fails to respect the causality of the image content: for instance, when conditioned on a visual description of a sunset, an autoregressive model may generate clouds before the sun, even though the color of clouds should depend on the color of the sun and not the inverse. In this work, we show that first by training a model to generate patches in any-given-order, we can infer both the content and the location (order) of each patch during generation. Secondly, we use these extracted orders to finetune the any-given-order model to produce better-quality images. Through our experiments, we show on two datasets that this new generation method produces better images than the traditional raster-scan approach, with similar training costs and no extra annotations.",
            "score": 4,
            "issue_id": 3430,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 23",
                "zh": "4æœˆ23æ—¥"
            },
            "hash": "8311350122eafda9",
            "authors": [
                "Rishav Pramanik",
                "Antoine Poupon",
                "Juan A. Rodriguez",
                "Masih Aminbeidokhti",
                "David Vazquez",
                "Christopher Pal",
                "Zhaozheng Yin",
                "Marco Pedersoli"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Ecole de technologie superieure, QC, Canada",
                "International Laboratory on Learning Systems (ILLS)",
                "Mila-Quebec AI Institute",
                "Polytechnique Montreal",
                "ServiceNow Research",
                "Stony Brook University, NY, USA",
                "Universite Paris-Saclay, CentraleSupelec, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17069.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¸Ğ¼ĞµĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ğ°Ñ‚Ñ‡Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ĞºĞ°Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ‚Ñ‡Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ² ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Causal Patch Ordering",
                    "desc": "This paper discusses a new approach to autoregressive image generation that improves upon the traditional raster-scan method. The authors argue that the raster-scan order does not respect the natural dependencies in image content, leading to suboptimal results. They propose a model that can generate image patches in any order, allowing for better alignment with the causal relationships in the image. Their experiments demonstrate that this method yields higher quality images while maintaining similar training costs and requiring no additional annotations."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å›¾åƒç”Ÿæˆé¡ºåºï¼Œæå‡è´¨é‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è‡ªå›å½’åŸºäºè¡¥ä¸çš„å›¾åƒç”Ÿæˆæ–¹æ³•ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„å…‰æ …æ‰«æé¡ºåºåœ¨ç”Ÿæˆå›¾åƒæ—¶å¹¶ä¸ç†æƒ³ï¼Œå› ä¸ºå®ƒæœªèƒ½è€ƒè™‘å›¾åƒå†…å®¹çš„å› æœå…³ç³»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒæ¨¡å‹ä»¥ä»»æ„é¡ºåºç”Ÿæˆè¡¥ä¸ï¼Œä»è€Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ¨æ–­æ¯ä¸ªè¡¥ä¸çš„å†…å®¹å’Œä½ç½®ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åˆ©ç”¨æå–çš„é¡ºåºå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–°æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šç”Ÿæˆçš„å›¾åƒè´¨é‡ä¼˜äºä¼ ç»Ÿçš„å…‰æ …æ‰«ææ–¹æ³•ï¼Œä¸”è®­ç»ƒæˆæœ¬ç›¸ä¼¼ï¼Œæ— éœ€é¢å¤–çš„æ ‡æ³¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17040",
            "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
            "url": "https://huggingface.co/papers/2504.17040",
            "abstract": "We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embeddings by merging similar tokens based on image complexity, addressing the inherent inefficiency of fixed-length outputs in vision transformers. Second, Virtual Token Unmerging (VTU) simulates the expected token sequence for large language models (LLMs) by efficiently reconstructing the attention dynamics of a full sequence, thus preserving the downstream performance without additional fine-tuning. Unlike previous approaches, our method dynamically adapts token compression to the content of the image and operates completely training-free, making it readily applicable to most state-of-the-art VLM architectures. Extensive experiments on image and video understanding tasks demonstrate that DyMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models across diverse VLM architectures, including the recently popularized AnyRes-based visual encoders. Furthermore, through qualitative analyses, we demonstrate that DToMe effectively adapts token reduction based on image complexity and, unlike existing systems, provides users more control over computational costs. Project page: https://mikewangwzhl.github.io/dymu/.",
            "score": 3,
            "issue_id": 3432,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 23",
                "zh": "4æœˆ23æ—¥"
            },
            "hash": "38841d87701816a0",
            "authors": [
                "Zhenhailong Wang",
                "Senthil Purushwalkam",
                "Caiming Xiong",
                "Silvio Savarese",
                "Heng Ji",
                "Ran Xu"
            ],
            "affiliations": [
                "Salesforce Research",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17040.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… VLM Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "DyMU - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Dynamic Token Merging (DToMe) Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Virtual Token Unmerging (VTU) Ğ´Ğ»Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². DyMU Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ VLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DyMU Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ÑÑ€ĞµĞ´Ğ½ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 32%-85% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Dynamic Efficiency in Vision-Language Models",
                    "desc": "DyMU is a novel framework designed to optimize vision-language models (VLMs) by reducing their computational load without sacrificing performance. It features Dynamic Token Merging (DToMe), which intelligently merges similar visual tokens based on the complexity of the image, thus addressing inefficiencies in traditional fixed-length outputs. Additionally, Virtual Token Unmerging (VTU) reconstructs the attention dynamics of large language models, allowing for effective token sequence simulation without the need for fine-tuning. This training-free approach enables significant reductions in visual token counts, achieving up to 85% less while maintaining performance across various VLM architectures."
                },
                "zh": {
                    "title": "DyMUï¼šåŠ¨æ€å‡å°‘è§†è§‰è¯­è¨€æ¨¡å‹è®¡ç®—è´Ÿæ‹…çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æˆ‘ä»¬æå‡ºäº†DyMUï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ã€æ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œèƒ½å¤ŸåŠ¨æ€å‡å°‘è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è®¡ç®—è´Ÿæ‹…ï¼ŒåŒæ—¶ä¿æŒé«˜ä»»åŠ¡æ€§èƒ½ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šåŠ¨æ€æ ‡è®°åˆå¹¶ï¼ˆDToMeï¼‰é€šè¿‡æ ¹æ®å›¾åƒå¤æ‚æ€§åˆå¹¶ç›¸ä¼¼çš„è§†è§‰æ ‡è®°åµŒå…¥ï¼Œè§£å†³äº†è§†è§‰å˜æ¢å™¨ä¸­å›ºå®šé•¿åº¦è¾“å‡ºçš„ä½æ•ˆé—®é¢˜ã€‚è™šæ‹Ÿæ ‡è®°è§£åˆå¹¶ï¼ˆVTUï¼‰åˆ™é€šè¿‡é«˜æ•ˆé‡å»ºå®Œæ•´åºåˆ—çš„æ³¨æ„åŠ›åŠ¨æ€ï¼Œæ¨¡æ‹Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é¢„æœŸæ ‡è®°åºåˆ—ï¼Œä»è€Œåœ¨ä¸è¿›è¡Œé¢å¤–å¾®è°ƒçš„æƒ…å†µä¸‹ä¿æŒä¸‹æ¸¸æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDyMUèƒ½å¤Ÿå°†å¹³å‡è§†è§‰æ ‡è®°æ•°é‡å‡å°‘32%-85%ï¼ŒåŒæ—¶åœ¨å¤šç§VLMæ¶æ„ä¸­å®ç°ä¸å…¨é•¿æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16921",
            "title": "IberBench: LLM Evaluation on Iberian Languages",
            "url": "https://huggingface.co/papers/2504.16921",
            "abstract": "Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fall short in several key areas: they overlook the diversity of language varieties, prioritize fundamental Natural Language Processing (NLP) capabilities over tasks of industrial relevance, and are static. With these aspects in mind, we present IberBench, a comprehensive and extensible benchmark designed to assess LLM performance on both fundamental and industry-relevant NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America. IberBench integrates 101 datasets from evaluation campaigns and recent benchmarks, covering 22 task categories such as sentiment and emotion analysis, toxicity detection, and summarization. The benchmark addresses key limitations in current evaluation practices, such as the lack of linguistic diversity and static evaluation setups by enabling continual updates and community-driven model and dataset submissions moderated by a committee of experts. We evaluate 23 LLMs ranging from 100 million to 14 billion parameters and provide empirical insights into their strengths and limitations. Our findings indicate that (i) LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii) performance is on average lower for Galician and Basque, (iii) some tasks show results close to random, and (iv) in other tasks LLMs perform above random but below shared task systems. IberBench offers open-source implementations for the entire evaluation pipeline, including dataset normalization and hosting, incremental evaluation of LLMs, and a publicly accessible leaderboard.",
            "score": 3,
            "issue_id": 3432,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 23",
                "zh": "4æœˆ23æ—¥"
            },
            "hash": "05f62a3747dea135",
            "authors": [
                "JosÃ© Ãngel GonzÃ¡lez",
                "Ian Borrego Obrador",
                "Ãlvaro Romo Herrero",
                "Areg Mikael Sarvazyan",
                "Mara Chinea-RÃ­os",
                "Angelo Basile",
                "Marc Franco-Salvador"
            ],
            "affiliations": [
                "Keepler Data Tech, Madrid, 28014, Spain",
                "Symanto Research, Valencia, 46011, Spain",
                "United Nations International Computing Centre (UNICC), Valencia, 46930, Spain",
                "Universitat Polit`ecnica de Val`encia, Valencia, 46022, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16921.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multilingual",
                    "#dataset",
                    "#low_resource",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "IberBench: Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° LLM Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾",
                    "desc": "IberBench - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾-Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (NLP) Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ˜Ğ±ĞµÑ€Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑƒĞ¾ÑÑ‚Ñ€Ğ¾Ğ²Ğ° Ğ¸ Ğ˜Ğ±ĞµÑ€Ğ¾Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 101 Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ 22 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. IberBench Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾-Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ñ‡ĞµĞ¼ Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸, Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ³Ğ°Ğ»Ğ¸ÑĞ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ±Ğ°ÑĞºÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "IberBench: A New Standard for Evaluating Language Models Beyond English",
                    "desc": "This paper introduces IberBench, a new benchmark for evaluating Large Language Models (LLMs) on various Natural Language Processing (NLP) tasks in languages from the Iberian Peninsula and Ibero-America. It addresses the limitations of existing benchmarks that are primarily focused on English and lack diversity in language varieties and industry-relevant tasks. IberBench includes 101 datasets across 22 task categories, allowing for a more comprehensive assessment of LLM performance. The benchmark also supports continuous updates and community contributions, providing insights into the strengths and weaknesses of 23 evaluated LLMs, particularly highlighting their challenges with industry-relevant tasks and specific languages like Galician and Basque."
                },
                "zh": {
                    "title": "IberBenchï¼šå¤šè¯­è¨€LLMè¯„ä¼°çš„æ–°åŸºå‡†",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¯„ä¼°ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è‹±è¯­ä»¥å¤–çš„è¯­è¨€ä¸­ï¼Œé«˜è´¨é‡çš„æ•°æ®å¾€å¾€æœ‰é™ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ï¼Œç¼ºä¹å¯¹å…¶ä»–è¯­è¨€çš„å…¨é¢è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†IberBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢ä¸”å¯æ‰©å±•çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨ä¼Šæ¯”åˆ©äºšåŠå²›å’Œä¼Šæ¯”åˆ©äºšç¾æ´²è¯­è¨€ä¸­çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡è¡¨ç°ã€‚IberBenchæ•´åˆäº†101ä¸ªæ•°æ®é›†ï¼Œæ¶µç›–22ä¸ªä»»åŠ¡ç±»åˆ«ï¼Œå¹¶å…è®¸ç¤¾åŒºé©±åŠ¨çš„æ¨¡å‹å’Œæ•°æ®é›†æäº¤ï¼Œä»¥åº”å¯¹å½“å‰è¯„ä¼°å®è·µä¸­çš„å…³é”®å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16064",
            "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
            "url": "https://huggingface.co/papers/2504.16064",
            "abstract": "Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-level image latents (from a variational autoencoder) and high-level semantic features (from a pretrained self-supervised encoder like DINO). Our latent-semantic diffusion approach learns to generate coherent image-feature pairs from pure noise, significantly enhancing both generative quality and training efficiency, all while requiring only minimal modifications to standard Diffusion Transformer architectures. By eliminating the need for complex distillation objectives, our unified design simplifies training and unlocks a powerful new inference strategy: Representation Guidance, which leverages learned semantics to steer and refine image generation. Evaluated in both conditional and unconditional settings, our method delivers substantial improvements in image quality and training convergence speed, establishing a new direction for representation-aware generative modeling.",
            "score": 3,
            "issue_id": 3433,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "a72d3bf45681bce8",
            "authors": [
                "Theodoros Kouzelis",
                "Efstathios Karypidis",
                "Ioannis Kakogeorgiou",
                "Spyros Gidaris",
                "Nikos Komodakis"
            ],
            "affiliations": [
                "Archimedes, Athena RC IIT, NCSR 'Demokritos'",
                "Archimedes, Athena RC National Technical University of Athens",
                "Archimedes, Athena RC University of Crete IACM-Forth",
                "valeĞ¾.ai"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16064.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#inference",
                    "#optimization",
                    "#training",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾-ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° - Representation Guidance, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Bridging Latent and Semantic Features for Superior Image Generation",
                    "desc": "This paper presents a new framework for generating high-quality images using latent diffusion models (LDMs). It combines low-level image features from a variational autoencoder with high-level semantic information from a self-supervised encoder. The proposed latent-semantic diffusion method generates image-feature pairs from noise, improving both the quality of generated images and the efficiency of training. Additionally, it introduces a novel inference strategy called Representation Guidance, which uses learned semantics to enhance the image generation process."
                },
                "zh": {
                    "title": "æ½œåœ¨-è¯­ä¹‰æ‰©æ•£ï¼šæå‡å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨é«˜è´¨é‡å›¾åƒç”Ÿæˆä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†å°†è¡¨ç¤ºå­¦ä¹ ä¸ç”Ÿæˆå»ºæ¨¡ç»“åˆä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”Ÿæˆå›¾åƒå»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œè”åˆå»ºæ¨¡ä½çº§å›¾åƒæ½œå˜é‡ï¼ˆæ¥è‡ªå˜åˆ†è‡ªç¼–ç å™¨ï¼‰å’Œé«˜çº§è¯­ä¹‰ç‰¹å¾ï¼ˆæ¥è‡ªé¢„è®­ç»ƒçš„è‡ªç›‘ç£ç¼–ç å™¨ï¼Œå¦‚DINOï¼‰ã€‚æˆ‘ä»¬çš„æ½œåœ¨-è¯­ä¹‰æ‰©æ•£æ–¹æ³•èƒ½å¤Ÿä»çº¯å™ªå£°ä¸­ç”Ÿæˆä¸€è‡´çš„å›¾åƒç‰¹å¾å¯¹ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡å’Œè®­ç»ƒæ•ˆç‡ï¼ŒåŒæ—¶å¯¹æ ‡å‡†æ‰©æ•£å˜æ¢å™¨æ¶æ„çš„ä¿®æ”¹æœ€å°åŒ–ã€‚é€šè¿‡æ¶ˆé™¤å¤æ‚çš„è’¸é¦ç›®æ ‡ï¼Œæˆ‘ä»¬çš„ç»Ÿä¸€è®¾è®¡ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å¼€å¯äº†ä¸€ç§å¼ºå¤§çš„æ–°æ¨ç†ç­–ç•¥ï¼šè¡¨ç¤ºå¼•å¯¼ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„è¯­ä¹‰æ¥å¼•å¯¼å’Œä¼˜åŒ–å›¾åƒç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15921",
            "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting",
            "url": "https://huggingface.co/papers/2504.15921",
            "abstract": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a system to summarise hour long videos with no-supervision. Most existing video understanding models work well on short videos of pre-segmented events, yet they struggle to summarise longer videos where relevant events are sparsely distributed and not pre-segmented. Moreover, long-form video understanding often relies on supervised hierarchical training that needs extensive annotations which are costly, slow and prone to inconsistency. With ViSMaP we bridge the gap between short videos (where annotated data is plentiful) and long ones (where it's not). We rely on LLMs to create optimised pseudo-summaries of long videos using segment descriptions from short ones. These pseudo-summaries are used as training data for a model that generates long-form video summaries, bypassing the need for expensive annotations of long videos. Specifically, we adopt a meta-prompting strategy to iteratively generate and refine creating pseudo-summaries of long videos. The strategy leverages short clip descriptions obtained from a supervised short video model to guide the summary. Each iteration uses three LLMs working in sequence: one to generate the pseudo-summary from clip descriptions, another to evaluate it, and a third to optimise the prompt of the generator. This iteration is necessary because the quality of the pseudo-summaries is highly dependent on the generator prompt, and varies widely among videos. We evaluate our summaries extensively on multiple datasets; our results show that ViSMaP achieves performance comparable to fully supervised state-of-the-art models while generalising across domains without sacrificing performance. Code will be released upon publication.",
            "score": 3,
            "issue_id": 3432,
            "pub_date": "2025-04-22",
            "pub_date_card": {
                "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 22",
                "zh": "4æœˆ22æ—¥"
            },
            "hash": "ed8621b93c4f4cbe",
            "authors": [
                "Jian Hu",
                "Dimitrios Korkinof",
                "Shaogang Gong",
                "Mariano Beguerisse-Diaz"
            ],
            "affiliations": [
                "Queen Mary University of London",
                "Spotify"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15921.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#dataset",
                    "#optimization",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ViSMap: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ",
                    "desc": "ViSMap - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ½ĞµÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ĞµÑ‚Ğ°Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´Ğ»Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ĞµÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ´Ğ»Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¸Ñ… Ğ½ĞµÑ‚. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑĞµĞ²Ğ´Ğ¾-ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ…. Ğ­Ñ‚Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ°Ğ¼Ğ¼Ğ°Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸."
                },
                "en": {
                    "title": "Unsupervised Video Summarization Made Easy with ViSMap!",
                    "desc": "ViSMap is an innovative system designed for unsupervised video summarization, particularly effective for long videos where relevant events are not clearly defined. Unlike traditional models that require extensive annotations and struggle with longer content, ViSMap utilizes large language models (LLMs) to create optimized pseudo-summaries from short video segment descriptions. This approach allows the model to generate long-form video summaries without the need for costly and time-consuming annotations. By employing a meta-prompting strategy, ViSMap iteratively refines these pseudo-summaries, achieving performance that rivals fully supervised models while maintaining versatility across different video domains."
                },
                "zh": {
                    "title": "æ— ç›‘ç£è§†é¢‘æ‘˜è¦çš„æ–°çªç ´",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†ViSMapï¼šä¸€ç§é€šè¿‡å…ƒæç¤ºè¿›è¡Œæ— ç›‘ç£è§†é¢‘æ‘˜è¦çš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿå¯¹é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘è¿›è¡Œæ€»ç»“ã€‚ç°æœ‰çš„è§†é¢‘ç†è§£æ¨¡å‹åœ¨çŸ­è§†é¢‘ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é•¿è§†é¢‘ä¸­ï¼Œç”±äºç›¸å…³äº‹ä»¶åˆ†å¸ƒç¨€ç–ä¸”æœªé¢„å…ˆåˆ†æ®µï¼Œæ•ˆæœè¾ƒå·®ã€‚ViSMapé€šè¿‡åˆ©ç”¨çŸ­è§†é¢‘çš„ç‰‡æ®µæè¿°ï¼Œç”Ÿæˆä¼˜åŒ–çš„ä¼ªæ‘˜è¦ï¼Œä½œä¸ºè®­ç»ƒæ•°æ®æ¥ç”Ÿæˆé•¿è§†é¢‘æ‘˜è¦ï¼Œä»è€Œé¿å…äº†æ˜‚è´µçš„é•¿è§†é¢‘æ³¨é‡Šéœ€æ±‚ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å…ƒæç¤ºç­–ç•¥ï¼Œè¿­ä»£ç”Ÿæˆå’Œä¼˜åŒ–é•¿è§†é¢‘çš„ä¼ªæ‘˜è¦ï¼Œæœ€ç»ˆå®ç°äº†ä¸å®Œå…¨ç›‘ç£çš„æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17414",
            "title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models",
            "url": "https://huggingface.co/papers/2504.17414",
            "abstract": "Video try-on replaces clothing in videos with target garments. Existing methods struggle to generate high-quality and temporally consistent results when handling complex clothing patterns and diverse body poses. We present 3DV-TON, a novel diffusion-based framework for generating high-fidelity and temporally consistent video try-on results. Our approach employs generated animatable textured 3D meshes as explicit frame-level guidance, alleviating the issue of models over-focusing on appearance fidelity at the expanse of motion coherence. This is achieved by enabling direct reference to consistent garment texture movements throughout video sequences. The proposed method features an adaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe for initial 2D image try-on, followed by (2) reconstructing and animating a textured 3D mesh synchronized with original video poses. We further introduce a robust rectangular masking strategy that successfully mitigates artifact propagation caused by leaking clothing information during dynamic human and garment movements. To advance video try-on research, we introduce HR-VVT, a high-resolution benchmark dataset containing 130 videos with diverse clothing types and scenarios. Quantitative and qualitative results demonstrate our superior performance over existing methods. The project page is at this link https://2y7c3.github.io/3DV-TON/",
            "score": 2,
            "issue_id": 3432,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "e3156af621b6aa6c",
            "authors": [
                "Min Wei",
                "Chaohui Yu",
                "Jingkai Zhou",
                "Fan Wang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17414.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#leakage",
                    "#diffusion",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ‘š",
                "ru": {
                    "title": "3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "3DV-TON - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞµ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ±Ğ¸Ñ‚ÑŒÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ğ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… HR-VVT Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Video Try-On with 3D Guidance",
                    "desc": "The paper introduces 3DV-TON, a new framework for video try-on that enhances the quality and consistency of clothing replacement in videos. It utilizes a diffusion-based approach that generates detailed 3D meshes to guide the video frames, ensuring that the clothing movements remain coherent with the body poses. By focusing on both appearance and motion, the method addresses common challenges in handling complex clothing patterns. Additionally, the authors present a new dataset, HR-VVT, to support further research in this area, showcasing their method's superior performance through extensive evaluations."
                },
                "zh": {
                    "title": "é«˜ä¿çœŸè§†é¢‘è¯•ç©¿çš„æ–°æ–¹æ³•",
                    "desc": "è§†é¢‘è¯•ç©¿æŠ€æœ¯å¯ä»¥åœ¨è§†é¢‘ä¸­æ›¿æ¢ç›®æ ‡æœè£…ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æœè£…å›¾æ¡ˆå’Œå¤šæ ·åŒ–èº«ä½“å§¿åŠ¿æ—¶ï¼Œç”Ÿæˆçš„ç»“æœè´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§è¾ƒå·®ã€‚æˆ‘ä»¬æå‡ºäº†3DV-TONï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸå’Œæ—¶é—´ä¸€è‡´çš„è§†é¢‘è¯•ç©¿ç»“æœã€‚è¯¥æ–¹æ³•ä½¿ç”¨ç”Ÿæˆçš„å¯åŠ¨ç”»çº¹ç†3Dç½‘æ ¼ä½œä¸ºæ˜ç¡®çš„å¸§çº§æŒ‡å¯¼ï¼Œè§£å†³äº†æ¨¡å‹åœ¨å¤–è§‚ä¿çœŸåº¦ä¸è¿åŠ¨ä¸€è‡´æ€§ä¹‹é—´çš„è¿‡åº¦å…³æ³¨é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å¼ºå¤§çš„çŸ©å½¢é®ç½©ç­–ç•¥ï¼Œæœ‰æ•ˆå‡è½»äº†åŠ¨æ€äººç±»å’Œæœè£…è¿åŠ¨ä¸­ä¿¡æ¯æ³„æ¼å¯¼è‡´çš„ä¼ªå½±ä¼ æ’­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17343",
            "title": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming\n  Videos",
            "url": "https://huggingface.co/papers/2504.17343",
            "abstract": "The rapid growth of online video platforms, particularly live streaming services, has created an urgent need for real-time video understanding systems. These systems must process continuous video streams and respond to user queries instantaneously, presenting unique challenges for current Video Large Language Models (VideoLLMs). While existing VideoLLMs excel at processing complete videos, they face significant limitations in streaming scenarios due to their inability to handle dense, redundant frames efficiently. We introduce TimeChat-Online, a novel online VideoLLM that revolutionizes real-time video interaction. At its core lies our innovative Differential Token Drop (DTD) module, which addresses the fundamental challenge of visual redundancy in streaming videos. Drawing inspiration from human visual perception's Change Blindness phenomenon, DTD preserves meaningful temporal changes while filtering out static, redundant content between frames. Remarkably, our experiments demonstrate that DTD achieves an 82.8% reduction in video tokens while maintaining 98% performance on StreamingBench, revealing that over 80% of visual content in streaming videos is naturally redundant without requiring language guidance. To enable seamless real-time interaction, we present TimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse interaction patterns including backward-tracing, current-perception, and future-responding scenarios. TimeChat-Online's unique Proactive Response capability, naturally achieved through continuous monitoring of video scene transitions via DTD, sets it apart from conventional approaches. Our extensive evaluation demonstrates TimeChat-Online's superior performance on streaming benchmarks (StreamingBench and OvOBench) and maintaining competitive results on long-form video tasks such as Video-MME and MLVU.",
            "score": 2,
            "issue_id": 3432,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "4d81b69e4ad78f8a",
            "authors": [
                "Linli Yao",
                "Yicheng Li",
                "Yuancheng Wei",
                "Lei Li",
                "Shuhuai Ren",
                "Yuanxin Liu",
                "Kun Ouyang",
                "Lean Wang",
                "Shicheng Li",
                "Sida Li",
                "Lingpeng Kong",
                "Qi Liu",
                "Yuanxing Zhang",
                "Xu Sun"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Peking University",
                "South China University of Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17343.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#long_context",
                    "#dataset",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "TimeChat-Online: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "TimeChat-Online - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VideoLLM Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Differential Token Drop (DTD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ½Ğ° 82.8%, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ 98% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ TimeChat-Online-139K Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Real-Time Video Interaction with TimeChat-Online",
                    "desc": "This paper presents TimeChat-Online, a new Video Large Language Model (VideoLLM) designed for real-time video understanding, particularly in live streaming contexts. It introduces the Differential Token Drop (DTD) module, which effectively reduces visual redundancy by filtering out static frames while preserving important changes, inspired by the human perception phenomenon known as Change Blindness. The model significantly decreases the number of video tokens processed, achieving an 82.8% reduction while maintaining high performance on streaming benchmarks. Additionally, TimeChat-Online features a unique Proactive Response capability that enhances real-time interaction by continuously monitoring video transitions."
                },
                "zh": {
                    "title": "å®æ—¶è§†é¢‘ç†è§£çš„é©å‘½æ€§è¿›å±•",
                    "desc": "éšç€åœ¨çº¿ç›´æ’­å¹³å°çš„å¿«é€Ÿå‘å±•ï¼Œå®æ—¶è§†é¢‘ç†è§£ç³»ç»Ÿçš„éœ€æ±‚å˜å¾—è¿«åˆ‡ã€‚è¿™äº›ç³»ç»Ÿéœ€è¦å¤„ç†è¿ç»­çš„è§†é¢‘æµå¹¶å³æ—¶å“åº”ç”¨æˆ·æŸ¥è¯¢ï¼Œç»™ç°æœ‰çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†TimeChat-Onlineï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åœ¨çº¿VideoLLMï¼Œé‡‡ç”¨äº†åˆ›æ–°çš„å·®åˆ†ä»¤ç‰Œä¸¢å¼ƒï¼ˆDTDï¼‰æ¨¡å—ï¼Œæœ‰æ•ˆè§£å†³äº†æµåª’ä½“è§†é¢‘ä¸­çš„è§†è§‰å†—ä½™é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒDTDåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘äº†82.8%çš„è§†é¢‘ä»¤ç‰Œï¼Œæ˜¾ç¤ºå‡ºæµåª’ä½“è§†é¢‘ä¸­è¶…è¿‡80%çš„è§†è§‰å†…å®¹æ˜¯å†—ä½™çš„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17601",
            "title": "Interpretable non-linear dimensionality reduction using gaussian\n  weighted linear transformation",
            "url": "https://huggingface.co/papers/2504.17601",
            "abstract": "Dimensionality reduction techniques are fundamental for analyzing and visualizing high-dimensional data. With established methods like t-SNE and PCA presenting a trade-off between representational power and interpretability. This paper introduces a novel approach that bridges this gap by combining the interpretability of linear methods with the expressiveness of non-linear transformations. The proposed algorithm constructs a non-linear mapping between high-dimensional and low-dimensional spaces through a combination of linear transformations, each weighted by Gaussian functions. This architecture enables complex non-linear transformations while preserving the interpretability advantages of linear methods, as each transformation can be analyzed independently. The resulting model provides both powerful dimensionality reduction and transparent insights into the transformed space. Techniques for interpreting the learned transformations are presented, including methods for identifying suppressed dimensions and how space is expanded and contracted. These tools enable practitioners to understand how the algorithm preserves and modifies geometric relationships during dimensionality reduction. To ensure the practical utility of this algorithm, the creation of user-friendly software packages is emphasized, facilitating its adoption in both academia and industry.",
            "score": 1,
            "issue_id": 3431,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "d796e92b08ec0b7e",
            "authors": [
                "Erik Bergh"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.17601.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Bridging Interpretability and Expressiveness in Dimensionality Reduction",
                    "desc": "This paper presents a new method for dimensionality reduction that combines the strengths of linear and non-linear techniques. It uses a series of linear transformations weighted by Gaussian functions to create a non-linear mapping from high-dimensional to low-dimensional spaces. This approach maintains the interpretability of linear methods while allowing for complex transformations, making it easier to analyze the results. Additionally, the paper offers tools for understanding how the algorithm modifies geometric relationships, ensuring that users can gain insights into the data effectively."
                },
                "zh": {
                    "title": "ç»“åˆå¯è§£é‡Šæ€§ä¸è¡¨ç°åŠ›çš„é™ç»´æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„é™ç»´æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ï¼ˆå¦‚t-SNEå’ŒPCAï¼‰åœ¨è¡¨ç°åŠ›å’Œå¯è§£é‡Šæ€§ä¹‹é—´çš„æƒè¡¡ã€‚è¯¥ç®—æ³•é€šè¿‡çº¿æ€§å˜æ¢çš„ç»„åˆï¼Œç»“åˆé«˜æ–¯å‡½æ•°ï¼Œæ„å»ºé«˜ç»´ä¸ä½ç»´ç©ºé—´ä¹‹é—´çš„éçº¿æ€§æ˜ å°„ã€‚è¿™æ ·å¯ä»¥å®ç°å¤æ‚çš„éçº¿æ€§å˜æ¢ï¼ŒåŒæ—¶ä¿æŒçº¿æ€§æ–¹æ³•çš„å¯è§£é‡Šæ€§ï¼Œä½¿æ¯ä¸ªå˜æ¢éƒ½å¯ä»¥ç‹¬ç«‹åˆ†æã€‚æ–‡ç« è¿˜æä¾›äº†ç†è§£å­¦ä¹ åˆ°çš„å˜æ¢çš„å·¥å…·ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£ç®—æ³•åœ¨é™ç»´è¿‡ç¨‹ä¸­å¦‚ä½•ä¿æŒå’Œä¿®æ”¹å‡ ä½•å…³ç³»ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-24.html",
    "link_next": "2025-04-28.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "24.04",
        "en": "04/24",
        "zh": "4æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "28.04",
        "en": "04/28",
        "zh": "4æœˆ28æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 3,
        "#benchmark": 10,
        "#agents": 2,
        "#cv": 7,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "æœ€è¿‘å‡ å¹´ï¼Œå›¾åƒç¼–è¾‘æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ€æ–°çš„å¤šæ¨¡æ€æ¨¡å‹å¦‚GPT-4oå’ŒGemini2 Flashå±•ç¤ºäº†å¼ºå¤§çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¼€æºç®—æ³•ä¸è¿™äº›é—­æºæ¨¡å‹ä¹‹é—´ä»æœ‰å·®è·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªåä¸ºStep1X-Editçš„å›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œæ—¨åœ¨æä¾›ä¸é—­æºæ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒStep1X-Editåœ¨å›¾åƒç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚",
        "title": "Step1X-Edit: A Practical Framework for General Image Editing",
        "pinyin": "æœ€è¿‘å‡ å¹´ï¼Œå›¾åƒç¼–è¾‘æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ€æ–°çš„å¤šæ¨¡æ€æ¨¡å‹å¦‚GPT-4oå’ŒGemini2 Flashå±•ç¤ºäº†å¼ºå¤§çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¼€æºç®—æ³•ä¸è¿™äº›é—­æºæ¨¡å‹ä¹‹é—´ä»æœ‰å·®è·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªåä¸ºStep1X-Editçš„å›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œæ—¨åœ¨æä¾›ä¸é—­æºæ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒStep1X-Editåœ¨å›¾åƒç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚\n\nZuÃ¬jÃ¬n jÇ niÃ¡n, tÃºxiÃ ng biÄnjÃ­ mÃ³xÃ­ng qÇ”dÃ©le xiÇnzhÃ¹ jÃ¬nzhÄn. ZuÃ¬xÄ«n de duÅ mÃ³shÃ¬ mÃ³xÃ­ng rÃº GPT-4o hÃ© Gemini2 Flash zhÇnshÃ¬le qiÃ¡ngdÃ  de tÃºxiÃ ng biÄnjÃ­ nÃ©nglÃ¬. RÃ¡n'Ã©r, kÄiyuÃ¡n suÃ nfÇ yÇ” zhÃ¨xiÄ“ bÃ¬yuÃ¡n mÃ³xÃ­ng zhÄ«jiÄn rÃ©ngyÇ’u chÄjÃ¹. YÄ«ncÇ, wÇ’men fÄbÃ¹le yÄ«gÃ¨ mÃ­ngwÃ¨i Step1X-Edit de tÃºxiÃ ng biÄnjÃ­ mÃ³xÃ­ng, zhÇyÇn tÃ­gÅng yÇ” bÃ¬yuÃ¡n mÃ³xÃ­ng xiÄng bÇmÄ›i de xÃ­ngnÃ©ng. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, Step1X-Edit zÃ i tÃºxiÃ ng biÄnjÃ­ fÄngmiÃ n biÇoxiÃ n chÅ«sÃ¨.",
        "vocab": "[\n    {\"word\": \"å›¾åƒ\", \"pinyin\": \"tÃº xiÃ ng\", \"trans\": \"image\"},\n    {\"word\": \"ç¼–è¾‘\", \"pinyin\": \"biÄn jÃ­\", \"trans\": \"edit\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å–å¾—\", \"pinyin\": \"qÇ” dÃ©\", \"trans\": \"achieve\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"remarkable\"},\n    {\"word\": \"è¿›å±•\", \"pinyin\": \"jÃ¬n zhÇn\", \"trans\": \"progress\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"å±•ç¤º\", \"pinyin\": \"zhÇn shÃ¬\", \"trans\": \"demonstrate\"},\n    {\"word\": \"å¼ºå¤§\", \"pinyin\": \"qiÃ¡ng dÃ \", \"trans\": \"powerful\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"capability\"},\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄi yuÃ¡n\", \"trans\": \"open-source\"},\n    {\"word\": \"ç®—æ³•\", \"pinyin\": \"suÃ n fÇ\", \"trans\": \"algorithm\"},\n    {\"word\": \"é—­æº\", \"pinyin\": \"bÃ¬ yuÃ¡n\", \"trans\": \"closed-source\"},\n    {\"word\": \"å·®è·\", \"pinyin\": \"chÄ jÃ¹\", \"trans\": \"gap\"},\n    {\"word\": \"å‘å¸ƒ\", \"pinyin\": \"fÄ bÃ¹\", \"trans\": \"release\"},\n    {\"word\": \"åä¸º\", \"pinyin\": \"mÃ­ng wÃ©i\", \"trans\": \"named\"},\n    {\"word\": \"æ—¨åœ¨\", \"pinyin\": \"zhÇ zÃ i\", \"trans\": \"aim to\"},\n    {\"word\": \"æä¾›\", \"pinyin\": \"tÃ­ gÅng\", \"trans\": \"provide\"},\n    {\"word\": \"ç›¸åª²ç¾\", \"pinyin\": \"xiÄng pÃ¬ mÄ›i\", \"trans\": \"comparable\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ© guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"}\n]",
        "trans": "In recent years, image editing models have made significant progress. The latest multimodal models, such as GPT-4o and Gemini2 Flash, have demonstrated powerful image editing capabilities. However, there is still a gap between open-source algorithms and these closed-source models. Therefore, we have released an image editing model called Step1X-Edit, aiming to provide performance comparable to closed-source models. Experimental results show that Step1X-Edit performs excellently in image editing.",
        "update_ts": "2025-04-25 09:12"
    }
}