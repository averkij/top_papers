{
    "date": {
        "ru": "25 апреля",
        "en": "April 25",
        "zh": "4月25日"
    },
    "time_utc": "2025-04-25 07:11",
    "weekday": 4,
    "issue_id": 3431,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.17761",
            "title": "Step1X-Edit: A Practical Framework for General Image Editing",
            "url": "https://huggingface.co/papers/2504.17761",
            "abstract": "In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. However, there is still a large gap between the open-source algorithm with these closed-source models. Thus, in this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against the closed-source models like GPT-4o and Gemini2 Flash. More specifically, we adopt the Multimodal LLM to process the reference image and the user's editing instruction. A latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. To train the model, we build a data generation pipeline to produce a high-quality dataset. For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions. Experimental results on GEdit-Bench demonstrate that Step1X-Edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.",
            "score": 34,
            "issue_id": 3427,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 апреля",
                "en": "April 24",
                "zh": "4月24日"
            },
            "hash": "2896842e49a93757",
            "authors": [
                "Shiyu Liu",
                "Yucheng Han",
                "Peng Xing",
                "Fukun Yin",
                "Rui Wang",
                "Wei Cheng",
                "Jiaqi Liao",
                "Yingming Wang",
                "Honghao Fu",
                "Chunrui Han",
                "Guopeng Li",
                "Yuang Peng",
                "Quan Sun",
                "Jingwei Wu",
                "Yan Cai",
                "Zheng Ge",
                "Ranchen Ming",
                "Lei Xia",
                "Xianfang Zeng",
                "Yibo Zhu",
                "Binxing Jiao",
                "Xiangyu Zhang",
                "Gang Yu",
                "Daxin Jiang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.17761.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#diffusion",
                    "#training",
                    "#multimodal",
                    "#cv",
                    "#dataset",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Step1X-Edit: Открытая модель редактирования изображений на уровне лучших закрытых решений",
                    "desc": "Статья представляет новую модель редактирования изображений Step1X-Edit, сопоставимую по производительности с закрытыми моделями GPT-4o и Gemini2 Flash. Модель использует мультимодальную LLM для обработки изображения и инструкций пользователя, а также диффузионный декодер для генерации целевого изображения. Для обучения модели был создан высококачественный датасет, а для оценки разработан новый бенчмарк GEdit-Bench. Эксперименты показывают, что Step1X-Edit значительно превосходит существующие открытые модели и приближается к производительности ведущих проприетарных решений."
                },
                "en": {
                    "title": "Bridging the Gap in Image Editing with Step1X-Edit",
                    "desc": "This paper presents Step1X-Edit, a new image editing model that aims to match the performance of advanced closed-source models like GPT-4o and Gemini2 Flash. It utilizes a Multimodal LLM to effectively process both the reference image and user editing instructions, integrating a latent embedding with a diffusion image decoder to generate the final edited image. The authors also introduce GEdit-Bench, a benchmark designed to evaluate the model based on real-world user instructions. Experimental results show that Step1X-Edit significantly outperforms existing open-source models and approaches the capabilities of leading proprietary systems, marking a notable advancement in image editing technology."
                },
                "zh": {
                    "title": "开源图像编辑的未来：Step1X-Edit",
                    "desc": "近年来，图像编辑模型取得了显著的发展。新发布的多模态模型如GPT-4o和Gemini2 Flash展现了强大的图像编辑能力，能够满足大多数用户的编辑需求。本文提出了一种名为Step1X-Edit的先进图像编辑模型，旨在与这些闭源模型的性能相媲美。通过采用多模态大语言模型处理参考图像和用户编辑指令，并结合扩散图像解码器，Step1X-Edit在真实用户指令的基准测试中表现优异，显著超越现有的开源基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17192",
            "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
            "url": "https://huggingface.co/papers/2504.17192",
            "abstract": "Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins.",
            "score": 21,
            "issue_id": 3430,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 апреля",
                "en": "April 24",
                "zh": "4月24日"
            },
            "hash": "01c5ca3f5c3906ce",
            "authors": [
                "Minju Seo",
                "Jinheon Baek",
                "Seongyun Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17192.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#architecture",
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#science"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "От статьи к коду: автоматизация реализации алгоритмов машинного обучения",
                    "desc": "PaperCoder - это мультиагентная система на основе больших языковых моделей (LLM), которая преобразует научные статьи по машинному обучению в функциональные репозитории кода. Система работает в три этапа: планирование, анализ и генерация, используя специализированных агентов для каждой фазы. PaperCoder демонстрирует эффективность в создании высококачественных и точных реализаций алгоритмов из статей. Система превосходит сильные базовые модели в недавно выпущенном бенчмарке PaperBench."
                },
                "en": {
                    "title": "Transforming Research into Code with PaperCoder",
                    "desc": "This paper presents PaperCoder, a framework that uses Large Language Models (LLMs) to convert machine learning research papers into functional code repositories. The process involves three main stages: planning, analysis, and generation, each handled by specialized agents that work together. In the planning stage, a roadmap and system architecture are created, while the analysis stage focuses on understanding implementation details. Finally, the generation stage produces modular code that respects dependencies, and the framework has been shown to outperform existing methods in generating high-quality implementations."
                },
                "zh": {
                    "title": "PaperCoder：将论文转化为代码的智能助手",
                    "desc": "尽管机器学习研究迅速发展，但相应的代码实现往往缺乏，导致研究人员在重现结果和基于先前工作进行构建时耗时费力。为了解决这个问题，我们提出了PaperCoder，这是一个多智能体的大型语言模型框架，可以将机器学习论文转化为功能性代码库。PaperCoder分为三个阶段：规划、分析和生成，每个阶段都有专门的智能体协作完成任务。我们的评估结果表明，PaperCoder在生成高质量、忠实的实现方面表现出色，且在最新的PaperBench基准测试中超越了强有力的基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17432",
            "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
            "url": "https://huggingface.co/papers/2504.17432",
            "abstract": "The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-text encoding, and (3) deficient compositionality due to bag-of-words behavior. While recent Multimodal Large Language Models (MLLMs) have demonstrated significant advances in generalized vision-language understanding, their potential for learning transferable multimodal representations remains underexplored.In this work, we present UniME (Universal Multimodal Embedding), a novel two-stage framework that leverages MLLMs to learn discriminative representations for diverse downstream tasks. In the first stage, we perform textual discriminative knowledge distillation from a powerful LLM-based teacher model to enhance the embedding capability of the MLLM\\'s language component. In the second stage, we introduce hard negative enhanced instruction tuning to further advance discriminative representation learning. Specifically, we initially mitigate false negative contamination and then sample multiple hard negatives per instance within each batch, forcing the model to focus on challenging samples. This approach not only improves discriminative power but also enhances instruction-following ability in downstream tasks. We conduct extensive experiments on the MMEB benchmark and multiple retrieval tasks, including short and long caption retrieval and compositional retrieval. Results demonstrate that UniME achieves consistent performance improvement across all tasks, exhibiting superior discriminative and compositional capabilities.",
            "score": 19,
            "issue_id": 3427,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 апреля",
                "en": "April 24",
                "zh": "4月24日"
            },
            "hash": "3a77377667aeb98f",
            "authors": [
                "Tiancheng Gu",
                "Kaicheng Yang",
                "Ziyong Feng",
                "Xingjun Wang",
                "Yanzhao Zhang",
                "Dingkun Long",
                "Yingda Chen",
                "Weidong Cai",
                "Jiankang Deng"
            ],
            "affiliations": [
                "DeepGlint Tongyi Lab, Alibaba Group",
                "Imperial College London",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17432.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#transfer_learning",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "UniME: Улучшение мультимодальных представлений через дистилляцию знаний и инструктивную настройку",
                    "desc": "UniME - это новый двухэтапный фреймворк для обучения дискриминативным мультимодальным представлениям с использованием мультимодальных больших языковых моделей (MLLM). На первом этапе происходит дистилляция знаний от мощной LLM-модели для улучшения языкового компонента MLLM. Второй этап включает инструктивную настройку с усиленными сложными негативными примерами для повышения дискриминативной способности. Эксперименты на бенчмарке MMEB и задачах поиска показывают превосходство UniME в дискриминативных и композиционных возможностях."
                },
                "en": {
                    "title": "Enhancing Multimodal Learning with UniME",
                    "desc": "The paper introduces UniME, a new framework designed to improve multimodal representation learning by addressing limitations in existing models like CLIP. It utilizes a two-stage process where the first stage enhances the language component of a Multimodal Large Language Model (MLLM) through knowledge distillation from a teacher model. The second stage employs hard negative enhanced instruction tuning to refine the model's ability to distinguish between challenging samples. Experimental results show that UniME significantly boosts performance in various image-text retrieval tasks, demonstrating better discriminative and compositional skills."
                },
                "zh": {
                    "title": "提升多模态学习的区分能力与组合能力",
                    "desc": "本文介绍了一种新的多模态嵌入框架UniME（通用多模态嵌入），旨在克服现有CLIP框架的局限性。UniME通过两个阶段的学习，首先从强大的语言模型教师模型中进行文本知识蒸馏，以增强多模态大语言模型的语言组件嵌入能力。其次，通过引入困难负样本增强的指令调优，进一步提升模型的区分能力和指令跟随能力。实验结果表明，UniME在多个检索任务中表现出色，具有更强的区分性和组合能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17207",
            "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation",
            "url": "https://huggingface.co/papers/2504.17207",
            "abstract": "We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. Perspective-taking, the ability to perceive an environment or situation from an alternative viewpoint, is a key benchmark for human-level visual understanding, essential for environmental interaction and collaboration with autonomous agents. Despite advancements in spatial reasoning within VLMs, recent research has shown that modern VLMs significantly lack perspective-aware reasoning capabilities and exhibit a strong bias toward egocentric interpretations. To bridge the gap between VLMs and human perception, we focus on the role of mental imagery, where humans perceive the world through abstracted representations that facilitate perspective shifts. Motivated by this, we propose a framework for perspective-aware reasoning, named Abstract Perspective Change (APC), that effectively leverages vision foundation models, such as object detection, segmentation, and orientation estimation, to construct scene abstractions and enable perspective transformations. Our experiments on synthetic and real-image benchmarks, compared with various VLMs, demonstrate significant improvements in perspective-aware reasoning with our framework, further outperforming fine-tuned spatial reasoning models and novel-view-synthesis-based approaches.",
            "score": 12,
            "issue_id": 3427,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 апреля",
                "en": "April 24",
                "zh": "4月24日"
            },
            "hash": "a45aa292431b93b2",
            "authors": [
                "Phillip Y. Lee",
                "Jihyeon Je",
                "Chanho Park",
                "Mikaela Angelina Uy",
                "Leonidas Guibas",
                "Minhyuk Sung"
            ],
            "affiliations": [
                "KAIST",
                "NVIDIA",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17207.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Улучшение пространственного мышления ИИ через абстрактное изменение перспективы",
                    "desc": "Статья представляет фреймворк для перспективно-ориентированных рассуждений в визуально-языковых моделях (VLM) через симуляцию ментальных образов. Авторы предлагают подход под названием Abstract Perspective Change (APC), который использует модели компьютерного зрения для создания абстракций сцен и трансформации перспектив. Эксперименты на синтетических и реальных изображениях показывают значительное улучшение способности VLM к рассуждениям с учетом перспективы. Предложенный метод превосходит как дообученные модели пространственных рассуждений, так и подходы на основе синтеза новых ракурсов."
                },
                "en": {
                    "title": "Enhancing VLMs with Perspective-Aware Reasoning through Mental Imagery",
                    "desc": "This paper introduces a new framework called Abstract Perspective Change (APC) aimed at enhancing perspective-aware reasoning in vision-language models (VLMs). The authors highlight that current VLMs struggle with understanding different viewpoints, often defaulting to egocentric perspectives. By simulating mental imagery, the APC framework allows VLMs to create abstract representations of scenes, facilitating perspective shifts. Experimental results show that APC significantly improves perspective reasoning compared to existing models, including those specifically fine-tuned for spatial reasoning."
                },
                "zh": {
                    "title": "提升视觉语言模型的视角感知能力",
                    "desc": "本文提出了一种通过心理图像模拟实现视觉语言模型（VLMs）中视角感知推理的框架。视角转换是指从不同的视角感知环境或情境，这对于人类的视觉理解至关重要，尤其是在与自主代理的互动中。尽管VLMs在空间推理方面取得了一定进展，但研究表明现代VLMs在视角感知推理能力上存在显著不足，且偏向于自我中心的解释。为了解决这一问题，我们提出了名为抽象视角变化（APC）的框架，利用视觉基础模型构建场景抽象并实现视角转换，从而显著提升了视角感知推理的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.16511",
            "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining",
            "url": "https://huggingface.co/papers/2504.16511",
            "abstract": "Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approaches overlook the inherent trade-off between quality and diversity, necessitating their joint consideration. Given a fixed training quota, it is essential to evaluate both the quality of each data point and its complementary effect on the overall dataset. In this paper, we introduce a unified data selection framework called QuaDMix, which automatically optimizes the data distribution for LLM pretraining while balancing both quality and diversity. Specifically, we first propose multiple criteria to measure data quality and employ domain classification to distinguish data points, thereby measuring overall diversity. QuaDMix then employs a unified parameterized data sampling function that determines the sampling probability of each data point based on these quality and diversity related labels. To accelerate the search for the optimal parameters involved in the QuaDMix framework, we conduct simulated experiments on smaller models and use LightGBM for parameters searching, inspired by the RegMix method. Our experiments across diverse models and datasets demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks. These results outperform the independent strategies for quality and diversity, highlighting the necessity and ability to balance data quality and diversity.",
            "score": 10,
            "issue_id": 3431,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 апреля",
                "en": "April 23",
                "zh": "4月23日"
            },
            "hash": "5f8a634a52f613d1",
            "authors": [
                "Fengze Liu",
                "Weidong Zhou",
                "Binbin Liu",
                "Zhimiao Yu",
                "Yifan Zhang",
                "Haobin Lin",
                "Yifeng Yu",
                "Xiaohuan Zhou",
                "Taifeng Wang",
                "Yong Cao"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.16511.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#data",
                    "#optimization"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Баланс качества и разнообразия данных для улучшения языковых моделей",
                    "desc": "Эта статья представляет QuaDMix - новый подход к оптимизации данных для обучения больших языковых моделей (LLM). QuaDMix учитывает как качество, так и разнообразие данных, используя параметризованную функцию выборки. Метод применяет различные критерии для оценки качества данных и классификацию по доменам для измерения разнообразия. Эксперименты показали, что QuaDMix улучшает производительность моделей в среднем на 7.2% по сравнению с независимыми стратегиями оптимизации качества и разнообразия."
                },
                "en": {
                    "title": "Balancing Quality and Diversity in LLM Training with QuaDMix",
                    "desc": "This paper presents QuaDMix, a new framework for selecting training data for large language models (LLMs) that optimally balances quality and diversity. Traditional methods often treat these two metrics separately, which can lead to suboptimal performance. QuaDMix introduces a unified approach that evaluates both the quality of individual data points and their contribution to the overall diversity of the dataset. The framework uses a parameterized sampling function and employs machine learning techniques to enhance the selection process, resulting in a significant performance boost in LLM training."
                },
                "zh": {
                    "title": "平衡质量与多样性，提升模型性能",
                    "desc": "本文提出了一种名为QuaDMix的统一数据选择框架，旨在同时优化大语言模型（LLM）训练数据的质量和多样性。传统方法通常分别优化这两个指标，忽视了它们之间的内在权衡。QuaDMix通过多个标准来评估数据质量，并利用领域分类来区分数据点，从而衡量整体多样性。实验结果表明，QuaDMix在多个基准测试中平均提高了7.2%的性能，证明了同时平衡数据质量和多样性的必要性和有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17069",
            "title": "Distilling semantically aware orders for autoregressive image generation",
            "url": "https://huggingface.co/papers/2504.17069",
            "abstract": "Autoregressive patch-based image generation has recently shown competitive results in terms of image quality and scalability. It can also be easily integrated and scaled within Vision-Language models. Nevertheless, autoregressive models require a defined order for patch generation. While a natural order based on the dictation of the words makes sense for text generation, there is no inherent generation order that exists for image generation. Traditionally, a raster-scan order (from top-left to bottom-right) guides autoregressive image generation models. In this paper, we argue that this order is suboptimal, as it fails to respect the causality of the image content: for instance, when conditioned on a visual description of a sunset, an autoregressive model may generate clouds before the sun, even though the color of clouds should depend on the color of the sun and not the inverse. In this work, we show that first by training a model to generate patches in any-given-order, we can infer both the content and the location (order) of each patch during generation. Secondly, we use these extracted orders to finetune the any-given-order model to produce better-quality images. Through our experiments, we show on two datasets that this new generation method produces better images than the traditional raster-scan approach, with similar training costs and no extra annotations.",
            "score": 2,
            "issue_id": 3430,
            "pub_date": "2025-04-23",
            "pub_date_card": {
                "ru": "23 апреля",
                "en": "April 23",
                "zh": "4月23日"
            },
            "hash": "8311350122eafda9",
            "authors": [
                "Rishav Pramanik",
                "Antoine Poupon",
                "Juan A. Rodriguez",
                "Masih Aminbeidokhti",
                "David Vazquez",
                "Christopher Pal",
                "Zhaozheng Yin",
                "Marco Pedersoli"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Ecole de technologie superieure, QC, Canada",
                "International Laboratory on Learning Systems (ILLS)",
                "Mila-Quebec AI Institute",
                "Polytechnique Montreal",
                "ServiceNow Research",
                "Stony Brook University, NY, USA",
                "Universite Paris-Saclay, CentraleSupelec, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17069.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#training"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Умная генерация изображений: правильный порядок имеет значение",
                    "desc": "Статья представляет новый подход к авторегрессивной генерации изображений по патчам. Авторы предлагают обучать модель генерировать патчи в произвольном порядке, а затем во время генерации определять как содержимое, так и расположение каждого патча. Этот метод позволяет учитывать причинно-следственные связи в содержании изображения, в отличие от традиционного подхода с фиксированным порядком генерации. Эксперименты показывают, что предложенный метод позволяет получать изображения лучшего качества при аналогичных затратах на обучение."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Causal Patch Ordering",
                    "desc": "This paper discusses a new approach to autoregressive image generation that improves upon the traditional raster-scan method. The authors argue that the raster-scan order does not respect the natural dependencies in image content, leading to suboptimal results. They propose a model that can generate image patches in any order, allowing for better alignment with the causal relationships in the image. Their experiments demonstrate that this method yields higher quality images while maintaining similar training costs and requiring no additional annotations."
                },
                "zh": {
                    "title": "优化图像生成顺序，提升质量",
                    "desc": "本文探讨了自回归基于补丁的图像生成方法，指出传统的光栅扫描顺序在生成图像时并不理想，因为它未能考虑图像内容的因果关系。我们提出了一种新方法，通过训练模型以任意顺序生成补丁，从而在生成过程中推断每个补丁的内容和位置。接着，我们利用提取的顺序对模型进行微调，以提高生成图像的质量。实验结果表明，这种新方法在两个数据集上生成的图像质量优于传统的光栅扫描方法，且训练成本相似，无需额外的标注。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17601",
            "title": "Interpretable non-linear dimensionality reduction using gaussian\n  weighted linear transformation",
            "url": "https://huggingface.co/papers/2504.17601",
            "abstract": "Dimensionality reduction techniques are fundamental for analyzing and visualizing high-dimensional data. With established methods like t-SNE and PCA presenting a trade-off between representational power and interpretability. This paper introduces a novel approach that bridges this gap by combining the interpretability of linear methods with the expressiveness of non-linear transformations. The proposed algorithm constructs a non-linear mapping between high-dimensional and low-dimensional spaces through a combination of linear transformations, each weighted by Gaussian functions. This architecture enables complex non-linear transformations while preserving the interpretability advantages of linear methods, as each transformation can be analyzed independently. The resulting model provides both powerful dimensionality reduction and transparent insights into the transformed space. Techniques for interpreting the learned transformations are presented, including methods for identifying suppressed dimensions and how space is expanded and contracted. These tools enable practitioners to understand how the algorithm preserves and modifies geometric relationships during dimensionality reduction. To ensure the practical utility of this algorithm, the creation of user-friendly software packages is emphasized, facilitating its adoption in both academia and industry.",
            "score": 1,
            "issue_id": 3431,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 апреля",
                "en": "April 24",
                "zh": "4月24日"
            },
            "hash": "d796e92b08ec0b7e",
            "authors": [
                "Erik Bergh"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.17601.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Интерпретируемое нелинейное снижение размерности: мощность и прозрачность",
                    "desc": "Эта статья представляет новый метод снижения размерности данных, сочетающий интерпретируемость линейных методов с выразительностью нелинейных преобразований. Алгоритм создает нелинейное отображение между пространствами высокой и низкой размерности, используя комбинацию линейных преобразований, взвешенных гауссовыми функциями. Предложенный подход обеспечивает мощное снижение размерности и прозрачное понимание преобразованного пространства. Авторы также представляют методы интерпретации полученных преобразований, включая идентификацию подавленных измерений и анализ расширения и сжатия пространства."
                },
                "en": {
                    "title": "Bridging Interpretability and Expressiveness in Dimensionality Reduction",
                    "desc": "This paper presents a new method for dimensionality reduction that combines the strengths of linear and non-linear techniques. It uses a series of linear transformations weighted by Gaussian functions to create a non-linear mapping from high-dimensional to low-dimensional spaces. This approach maintains the interpretability of linear methods while allowing for complex transformations, making it easier to analyze the results. Additionally, the paper offers tools for understanding how the algorithm modifies geometric relationships, ensuring that users can gain insights into the data effectively."
                },
                "zh": {
                    "title": "结合可解释性与表现力的降维新方法",
                    "desc": "本文介绍了一种新的降维方法，旨在解决现有方法（如t-SNE和PCA）在表现力和可解释性之间的权衡。该算法通过线性变换的组合，结合高斯函数，构建高维与低维空间之间的非线性映射。这样可以实现复杂的非线性变换，同时保持线性方法的可解释性，使每个变换都可以独立分析。文章还提供了理解学习到的变换的工具，帮助用户理解算法在降维过程中如何保持和修改几何关系。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17789",
            "title": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models",
            "url": "https://huggingface.co/papers/2504.17789",
            "abstract": "Autoregressive (AR) models, long dominant in language generation, are increasingly applied to image synthesis but are often considered less competitive than Diffusion-based models. A primary limitation is the substantial number of image tokens required for AR models, which constrains both training and inference efficiency, as well as image resolution. To address this, we present Token-Shuffle, a novel yet simple method that reduces the number of image tokens in Transformer. Our key insight is the dimensional redundancy of visual vocabularies in Multimodal Large Language Models (MLLMs), where low-dimensional visual codes from visual encoder are directly mapped to high-dimensional language vocabularies. Leveraging this, we consider two key operations: token-shuffle, which merges spatially local tokens along channel dimension to decrease the input token number, and token-unshuffle, which untangles the inferred tokens after Transformer blocks to restore the spatial arrangement for output. Jointly training with textual prompts, our strategy requires no additional pretrained text-encoder and enables MLLMs to support extremely high-resolution image synthesis in a unified next-token prediction way while maintaining efficient training and inference. For the first time, we push the boundary of AR text-to-image generation to a resolution of 2048x2048 with gratifying generation performance. In GenAI-benchmark, our 2.7B model achieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen by 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human evaluations also demonstrate our prominent image generation ability in terms of text-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle can serve as a foundational design for efficient high-resolution image generation within MLLMs.",
            "score": 0,
            "issue_id": 3431,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 апреля",
                "en": "April 24",
                "zh": "4月24日"
            },
            "hash": "1c3c71248cdb19ca",
            "authors": [
                "Xu Ma",
                "Peize Sun",
                "Haoyu Ma",
                "Hao Tang",
                "Chih-Yao Ma",
                "Jialiang Wang",
                "Kunpeng Li",
                "Xiaoliang Dai",
                "Yujun Shi",
                "Xuan Ju",
                "Yushi Hu",
                "Artsiom Sanakoyeu",
                "Felix Juefei-Xu",
                "Ji Hou",
                "Junjiao Tian",
                "Tao Xu",
                "Tingbo Hou",
                "Yen-Cheng Liu",
                "Zecheng He",
                "Zijian He",
                "Matt Feiszli",
                "Peizhao Zhang",
                "Peter Vajda",
                "Sam Tsai",
                "Yun Fu"
            ],
            "affiliations": [
                "Meta FAIR",
                "Meta GenAI",
                "National University of Singapore",
                "Northeastern University",
                "The Chinese University of Hong Kong",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17789.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#diffusion",
                    "#cv",
                    "#benchmark",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Token-Shuffle: прорыв в высокоэффективной генерации изображений высокого разрешения",
                    "desc": "Статья представляет метод Token-Shuffle для улучшения авторегрессионных моделей в задаче генерации изображений. Этот метод уменьшает количество токенов изображения в Transformer, используя размерную избыточность визуальных словарей в мультимодальных больших языковых моделях. Token-Shuffle позволяет генерировать изображения высокого разрешения (до 2048x2048) при сохранении эффективности обучения и вывода. Результаты показывают, что модель с 2,7 млрд параметров превосходит другие авторегрессионные и диффузионные модели по различным метрикам."
                },
                "en": {
                    "title": "Token-Shuffle: Revolutionizing High-Resolution Image Generation with Efficiency",
                    "desc": "This paper introduces Token-Shuffle, a new method that enhances autoregressive (AR) models for image synthesis by reducing the number of image tokens needed. The authors leverage the redundancy in visual vocabularies of Multimodal Large Language Models (MLLMs) to improve efficiency in both training and inference. By implementing token-shuffle and token-unshuffle operations, they can maintain high-resolution outputs while simplifying the input requirements. Their approach achieves impressive results, generating images at a resolution of 2048x2048 and outperforming existing AR and diffusion models in various benchmarks."
                },
                "zh": {
                    "title": "Token-Shuffle：高效高分辨率图像生成的新方法",
                    "desc": "本文提出了一种名为Token-Shuffle的新方法，旨在提高自回归（AR）模型在图像合成中的效率。通过减少输入图像标记的数量，Token-Shuffle能够在不需要额外预训练文本编码器的情况下，实现高分辨率图像生成。该方法利用多模态大语言模型中的视觉词汇的维度冗余，结合了标记混洗和标记解混洗操作。最终，我们的2.7B模型在GenAI基准测试中表现优异，推动了AR文本到图像生成的分辨率达到2048x2048。"
                }
            }
        }
    ],
    "link_prev": "2025-04-24.html",
    "link_next": "2025-04-28.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "24.04",
        "en": "04/24",
        "zh": "4月24日"
    },
    "short_date_next": {
        "ru": "28.04",
        "en": "04/28",
        "zh": "4月28日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 3,
        "#benchmark": 6,
        "#agents": 2,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "视觉推理是人类智能的核心组成部分，也是先进多模态模型的关键能力。然而，当前对多模态大语言模型（MLLMs）的推理评估往往依赖文本描述，允许基于语言的推理捷径，无法衡量真正的视觉中心推理。为解决这一问题，我们引入了VisuLogic：一个包含1,000个人类验证问题的基准，涵盖六个类别（如量化转换、空间关系、属性比较）。这些不同类型的问题可以从多个角度评估MLLMs的视觉推理能力。我们对该基准评估了领先的MLLMs，并分析其结果，以识别常见的失败模式。大多数模型的准确率低于30%，仅略高于25%的随机基线，远低于人类的51.4%，显示出视觉推理方面的显著差距。此外，我们提供了一个补充训练数据集和一个强化学习基线，以支持进一步的进展。",
        "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
        "pinyin": "视觉推理是人类智能的核心组成部分，也是先进多模态模型的关键能力。然而，当前对多模态大语言模型（MLLMs）的推理评估往往依赖文本描述，允许基于语言的推理捷径，无法衡量真正的视觉中心推理。为解决这一问题，我们引入了VisuLogic：一个包含1,000个人类验证问题的基准，涵盖六个类别（如量化转换、空间关系、属性比较）。这些不同类型的问题可以从多个角度评估MLLMs的视觉推理能力。我们对该基准评估了领先的MLLMs，并分析其结果，以识别常见的失败模式。大多数模型的准确率低于30%，仅略高于25%的随机基线，远低于人类的51.4%，显示出视觉推理方面的显著差距。此外，我们提供了一个补充训练数据集和一个强化学习基线，以支持进一步的进展。\n\nShìjué tuīlǐ shì rénlèi zhìnéng de héxīn zǔchéng bùfēn, yě shì xiānjìn duō móshì duō móshì móxíng de guǎnjiàn nénglì. Rán'ér, dāngqián duì duō móshì dà yǔyán móxíng (MLLMs) de tuīlǐ píngjià wǎngwǎng yīlài wénběn miáoshù, yǔn xǔ jīyú yǔyán de tuīlǐ jiéjìng, wúfǎ héngliáng zhēnzhèng de shìjué zhōngxīn tuīlǐ. Wèi jiějué zhè yī wèntí, wǒmen yǐn rùle VisuLogic: yīgè bāohán 1,000 gè rénlèi yànzhèng wèntí de jīzhǔn, hánjiē liù gè lèibié (rú liàng huà zhuǎnhuàn, kōngjiān guānxì, shǔxìng bǐjiào). Zhèxiē bùtóng lèixíng de wèntí kěyǐ cóng duō gè jiǎodù píngjià MLLMs de shìjué tuīlǐ nénglì. Wǒmen duì gāi jīzhǔn píngjià le lǐngxiān de MLLMs, bìng fēnxi qí jiéguǒ, yǐ shíbié chángjiàn de shībài móshì. Dàduōshù móxíng de zhǔnquèlǜ dīyú 30%, jǐn lüè gāoyú 25% de suíjī jīzhǔn, yuǎn dīyú rénlèi de 51.4%, xiǎnshì chū shìjué tuīlǐ fāngmiàn de xiǎnzhù chājù. Cǐwài, wǒmen tígōngle yīgè bǔchōng xùnliàn shùjùjí hé yīgè qiángzhù xuéxí jīzhǔn, yǐ zhīchí jìnfā de jìnbù.",
        "vocab": "[\n    {\"word\": \"视觉\", \"pinyin\": \"shìjué\", \"trans\": \"vision\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"核心\", \"pinyin\": \"héxīn\", \"trans\": \"core\"},\n    {\"word\": \"组成部分\", \"pinyin\": \"zǔchéng bùfēn\", \"trans\": \"component\"},\n    {\"word\": \"先进\", \"pinyin\": \"xiānjìn\", \"trans\": \"advanced\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō móshì\", \"trans\": \"multimodal\"},\n    {\"word\": \"模型\", \"pinyin\": \"móxíng\", \"trans\": \"model\"},\n    {\"word\": \"关键\", \"pinyin\": \"guǎnjiàn\", \"trans\": \"key\"},\n    {\"word\": \"能力\", \"pinyin\": \"nénglì\", \"trans\": \"ability\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yīlài\", \"trans\": \"rely on\"},\n    {\"word\": \"描述\", \"pinyin\": \"miáoshù\", \"trans\": \"description\"},\n    {\"word\": \"允许\", \"pinyin\": \"yǔnxǔ\", \"trans\": \"allow\"},\n    {\"word\": \"基于\", \"pinyin\": \"jīyú\", \"trans\": \"based on\"},\n    {\"word\": \"捷径\", \"pinyin\": \"jiéjìng\", \"trans\": \"shortcut\"},\n    {\"word\": \"衡量\", \"pinyin\": \"héngliáng\", \"trans\": \"measure\"},\n    {\"word\": \"真正\", \"pinyin\": \"zhēnzhèng\", \"trans\": \"genuine\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐnrù\", \"trans\": \"introduce\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"验证\", \"pinyin\": \"yànzhèng\", \"trans\": \"verification\"},\n    {\"word\": \"问题\", \"pinyin\": \"wèntí\", \"trans\": \"question\"},\n    {\"word\": \"涵盖\", \"pinyin\": \"hángài\", \"trans\": \"cover\"},\n    {\"word\": \"类别\", \"pinyin\": \"lèibié\", \"trans\": \"category\"},\n    {\"word\": \"量化\", \"pinyin\": \"liànghuà\", \"trans\": \"quantification\"},\n    {\"word\": \"转换\", \"pinyin\": \"zhuǎnhuàn\", \"trans\": \"conversion\"},\n    {\"word\": \"空间\", \"pinyin\": \"kōngjiān\", \"trans\": \"space\"},\n    {\"word\": \"关系\", \"pinyin\": \"guānxì\", \"trans\": \"relationship\"},\n    {\"word\": \"属性\", \"pinyin\": \"shǔxìng\", \"trans\": \"attribute\"},\n    {\"word\": \"比较\", \"pinyin\": \"bǐjiào\", \"trans\": \"comparison\"},\n    {\"word\": \"角度\", \"pinyin\": \"jiǎodù\", \"trans\": \"angle\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluate\"},\n    {\"word\": \"领先\", \"pinyin\": \"lǐngxiān\", \"trans\": \"leading\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēnxī\", \"trans\": \"analyze\"},\n    {\"word\": \"结果\", \"pinyin\": \"jiéguǒ\", \"trans\": \"result\"},\n    {\"word\": \"识别\", \"pinyin\": \"shíbié\", \"trans\": \"identify\"},\n    {\"word\": \"模式\", \"pinyin\": \"móshì\", \"trans\": \"pattern\"},\n    {\"word\": \"准确率\", \"pinyin\": \"zhǔnquèlǜ\", \"trans\": \"accuracy\"},\n    {\"word\": \"随机\", \"pinyin\": \"suíjī\", \"trans\": \"random\"},\n    {\"word\": \"基线\", \"pinyin\": \"jīxiàn\", \"trans\": \"baseline\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎnzhù\", \"trans\": \"significant\"},\n    {\"word\": \"差距\", \"pinyin\": \"chājù\", \"trans\": \"gap\"},\n    {\"word\": \"补充\", \"pinyin\": \"bǔchōng\", \"trans\": \"supplement\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"},\n    {\"word\": \"强化\", \"pinyin\": \"qiánghuà\", \"trans\": \"reinforce\"},\n    {\"word\": \"学习\", \"pinyin\": \"xuéxí\", \"trans\": \"learning\"},\n    {\"word\": \"支持\", \"pinyin\": \"zhīchí\", \"trans\": \"support\"},\n    {\"word\": \"进展\", \"pinyin\": \"jìnzhǎn\", \"trans\": \"progress\"}\n]",
        "trans": "Visual reasoning is a core component of human intelligence and a key capability of advanced multimodal models. However, current evaluations of reasoning in multimodal large language models (MLLMs) often rely on textual descriptions, allowing for language-based reasoning shortcuts that fail to measure true vision-centric reasoning. To address this issue, we introduce VisuLogic: a benchmark containing 1,000 human-verified questions covering six categories (such as quantitative transformations, spatial relationships, and attribute comparisons). These different types of questions can evaluate the visual reasoning capabilities of MLLMs from multiple angles. We evaluated leading MLLMs on this benchmark and analyzed the results to identify common failure modes. Most models achieved an accuracy of less than 30%, only slightly above the 25% random baseline and significantly lower than the human accuracy of 51.4%, indicating a notable gap in visual reasoning. Additionally, we provide a supplementary training dataset and a reinforcement learning baseline to support further advancements.",
        "update_ts": "2025-04-24 09:12"
    }
}