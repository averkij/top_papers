{
    "date": {
        "ru": "25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 25",
        "zh": "4æœˆ25æ—¥"
    },
    "time_utc": "2025-04-25 05:11",
    "weekday": 4,
    "issue_id": 3429,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.17761",
            "title": "Step1X-Edit: A Practical Framework for General Image Editing",
            "url": "https://huggingface.co/papers/2504.17761",
            "abstract": "In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. However, there is still a large gap between the open-source algorithm with these closed-source models. Thus, in this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against the closed-source models like GPT-4o and Gemini2 Flash. More specifically, we adopt the Multimodal LLM to process the reference image and the user's editing instruction. A latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. To train the model, we build a data generation pipeline to produce a high-quality dataset. For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions. Experimental results on GEdit-Bench demonstrate that Step1X-Edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.",
            "score": 29,
            "issue_id": 3427,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "2896842e49a93757",
            "authors": [
                "Shiyu Liu",
                "Yucheng Han",
                "Peng Xing",
                "Fukun Yin",
                "Rui Wang",
                "Wei Cheng",
                "Jiaqi Liao",
                "Yingming Wang",
                "Honghao Fu",
                "Chunrui Han",
                "Guopeng Li",
                "Yuang Peng",
                "Quan Sun",
                "Jingwei Wu",
                "Yan Cai",
                "Zheng Ge",
                "Ranchen Ming",
                "Lei Xia",
                "Xianfang Zeng",
                "Yibo Zhu",
                "Binxing Jiao",
                "Xiangyu Zhang",
                "Gang Yu",
                "Daxin Jiang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.17761.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#diffusion",
                    "#training",
                    "#multimodal",
                    "#cv",
                    "#dataset",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Step1X-Edit: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Step1X-Edit, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ GPT-4o Ğ¸ Gemini2 Flash. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ LLM Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GEdit-Bench. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Step1X-Edit Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Bridging the Gap in Image Editing with Step1X-Edit",
                    "desc": "This paper presents Step1X-Edit, a new image editing model that aims to match the performance of advanced closed-source models like GPT-4o and Gemini2 Flash. It utilizes a Multimodal LLM to effectively process both the reference image and user editing instructions, integrating a latent embedding with a diffusion image decoder to generate the final edited image. The authors also introduce GEdit-Bench, a benchmark designed to evaluate the model based on real-world user instructions. Experimental results show that Step1X-Edit significantly outperforms existing open-source models and approaches the capabilities of leading proprietary systems, marking a notable advancement in image editing technology."
                },
                "zh": {
                    "title": "å¼€æºå›¾åƒç¼–è¾‘çš„æœªæ¥ï¼šStep1X-Edit",
                    "desc": "è¿‘å¹´æ¥ï¼Œå›¾åƒç¼–è¾‘æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„å‘å±•ã€‚æ–°å‘å¸ƒçš„å¤šæ¨¡æ€æ¨¡å‹å¦‚GPT-4oå’ŒGemini2 Flashå±•ç°äº†å¼ºå¤§çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ»¡è¶³å¤§å¤šæ•°ç”¨æˆ·çš„ç¼–è¾‘éœ€æ±‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºStep1X-Editçš„å…ˆè¿›å›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œæ—¨åœ¨ä¸è¿™äº›é—­æºæ¨¡å‹çš„æ€§èƒ½ç›¸åª²ç¾ã€‚é€šè¿‡é‡‡ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¤„ç†å‚è€ƒå›¾åƒå’Œç”¨æˆ·ç¼–è¾‘æŒ‡ä»¤ï¼Œå¹¶ç»“åˆæ‰©æ•£å›¾åƒè§£ç å™¨ï¼ŒStep1X-Editåœ¨çœŸå®ç”¨æˆ·æŒ‡ä»¤çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰çš„å¼€æºåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17432",
            "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
            "url": "https://huggingface.co/papers/2504.17432",
            "abstract": "The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-text encoding, and (3) deficient compositionality due to bag-of-words behavior. While recent Multimodal Large Language Models (MLLMs) have demonstrated significant advances in generalized vision-language understanding, their potential for learning transferable multimodal representations remains underexplored.In this work, we present UniME (Universal Multimodal Embedding), a novel two-stage framework that leverages MLLMs to learn discriminative representations for diverse downstream tasks. In the first stage, we perform textual discriminative knowledge distillation from a powerful LLM-based teacher model to enhance the embedding capability of the MLLM\\'s language component. In the second stage, we introduce hard negative enhanced instruction tuning to further advance discriminative representation learning. Specifically, we initially mitigate false negative contamination and then sample multiple hard negatives per instance within each batch, forcing the model to focus on challenging samples. This approach not only improves discriminative power but also enhances instruction-following ability in downstream tasks. We conduct extensive experiments on the MMEB benchmark and multiple retrieval tasks, including short and long caption retrieval and compositional retrieval. Results demonstrate that UniME achieves consistent performance improvement across all tasks, exhibiting superior discriminative and compositional capabilities.",
            "score": 14,
            "issue_id": 3427,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "3a77377667aeb98f",
            "authors": [
                "Tiancheng Gu",
                "Kaicheng Yang",
                "Ziyong Feng",
                "Xingjun Wang",
                "Yanzhao Zhang",
                "Dingkun Long",
                "Yingda Chen",
                "Weidong Cai",
                "Jiankang Deng"
            ],
            "affiliations": [
                "DeepGlint Tongyi Lab, Alibaba Group",
                "Imperial College London",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17432.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#transfer_learning",
                    "#optimization",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "UniME: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ",
                    "desc": "UniME - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ LLM-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° MLLM. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMEB Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ UniME Ğ² Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Multimodal Learning with UniME",
                    "desc": "The paper introduces UniME, a new framework designed to improve multimodal representation learning by addressing limitations in existing models like CLIP. It utilizes a two-stage process where the first stage enhances the language component of a Multimodal Large Language Model (MLLM) through knowledge distillation from a teacher model. The second stage employs hard negative enhanced instruction tuning to refine the model's ability to distinguish between challenging samples. Experimental results show that UniME significantly boosts performance in various image-text retrieval tasks, demonstrating better discriminative and compositional skills."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€å­¦ä¹ çš„åŒºåˆ†èƒ½åŠ›ä¸ç»„åˆèƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€åµŒå…¥æ¡†æ¶UniMEï¼ˆé€šç”¨å¤šæ¨¡æ€åµŒå…¥ï¼‰ï¼Œæ—¨åœ¨å…‹æœç°æœ‰CLIPæ¡†æ¶çš„å±€é™æ€§ã€‚UniMEé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„å­¦ä¹ ï¼Œé¦–å…ˆä»å¼ºå¤§çš„è¯­è¨€æ¨¡å‹æ•™å¸ˆæ¨¡å‹ä¸­è¿›è¡Œæ–‡æœ¬çŸ¥è¯†è’¸é¦ï¼Œä»¥å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯­è¨€ç»„ä»¶åµŒå…¥èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å¼•å…¥å›°éš¾è´Ÿæ ·æœ¬å¢å¼ºçš„æŒ‡ä»¤è°ƒä¼˜ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„åŒºåˆ†èƒ½åŠ›å’ŒæŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniMEåœ¨å¤šä¸ªæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰æ›´å¼ºçš„åŒºåˆ†æ€§å’Œç»„åˆèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.17207",
            "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation",
            "url": "https://huggingface.co/papers/2504.17207",
            "abstract": "We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. Perspective-taking, the ability to perceive an environment or situation from an alternative viewpoint, is a key benchmark for human-level visual understanding, essential for environmental interaction and collaboration with autonomous agents. Despite advancements in spatial reasoning within VLMs, recent research has shown that modern VLMs significantly lack perspective-aware reasoning capabilities and exhibit a strong bias toward egocentric interpretations. To bridge the gap between VLMs and human perception, we focus on the role of mental imagery, where humans perceive the world through abstracted representations that facilitate perspective shifts. Motivated by this, we propose a framework for perspective-aware reasoning, named Abstract Perspective Change (APC), that effectively leverages vision foundation models, such as object detection, segmentation, and orientation estimation, to construct scene abstractions and enable perspective transformations. Our experiments on synthetic and real-image benchmarks, compared with various VLMs, demonstrate significant improvements in perspective-aware reasoning with our framework, further outperforming fine-tuned spatial reasoning models and novel-view-synthesis-based approaches.",
            "score": 11,
            "issue_id": 3427,
            "pub_date": "2025-04-24",
            "pub_date_card": {
                "ru": "24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 24",
                "zh": "4æœˆ24æ—¥"
            },
            "hash": "a45aa292431b93b2",
            "authors": [
                "Phillip Y. Lee",
                "Jihyeon Je",
                "Chanho Park",
                "Mikaela Angelina Uy",
                "Leonidas Guibas",
                "Minhyuk Sung"
            ],
            "affiliations": [
                "KAIST",
                "NVIDIA",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.17207.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (VLM) Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Abstract Perspective Change (APC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ ÑÑ†ĞµĞ½ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ VLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing VLMs with Perspective-Aware Reasoning through Mental Imagery",
                    "desc": "This paper introduces a new framework called Abstract Perspective Change (APC) aimed at enhancing perspective-aware reasoning in vision-language models (VLMs). The authors highlight that current VLMs struggle with understanding different viewpoints, often defaulting to egocentric perspectives. By simulating mental imagery, the APC framework allows VLMs to create abstract representations of scenes, facilitating perspective shifts. Experimental results show that APC significantly improves perspective reasoning compared to existing models, including those specifically fine-tuned for spatial reasoning."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§’æ„ŸçŸ¥èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¿ƒç†å›¾åƒæ¨¡æ‹Ÿå®ç°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­è§†è§’æ„ŸçŸ¥æ¨ç†çš„æ¡†æ¶ã€‚è§†è§’è½¬æ¢æ˜¯æŒ‡ä»ä¸åŒçš„è§†è§’æ„ŸçŸ¥ç¯å¢ƒæˆ–æƒ…å¢ƒï¼Œè¿™å¯¹äºäººç±»çš„è§†è§‰ç†è§£è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨ä¸è‡ªä¸»ä»£ç†çš„äº’åŠ¨ä¸­ã€‚å°½ç®¡VLMsåœ¨ç©ºé—´æ¨ç†æ–¹é¢å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†ç ”ç©¶è¡¨æ˜ç°ä»£VLMsåœ¨è§†è§’æ„ŸçŸ¥æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œä¸”åå‘äºè‡ªæˆ‘ä¸­å¿ƒçš„è§£é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºæŠ½è±¡è§†è§’å˜åŒ–ï¼ˆAPCï¼‰çš„æ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æ„å»ºåœºæ™¯æŠ½è±¡å¹¶å®ç°è§†è§’è½¬æ¢ï¼Œä»è€Œæ˜¾è‘—æå‡äº†è§†è§’æ„ŸçŸ¥æ¨ç†çš„èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-24.html",
    "link_next": "2025-04-28.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "24.04",
        "en": "04/24",
        "zh": "4æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "28.04",
        "en": "04/28",
        "zh": "4æœˆ28æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è§†è§‰æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œä¹Ÿæ˜¯å…ˆè¿›å¤šæ¨¡æ€æ¨¡å‹çš„å…³é”®èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†è¯„ä¼°å¾€å¾€ä¾èµ–æ–‡æœ¬æè¿°ï¼Œå…è®¸åŸºäºè¯­è¨€çš„æ¨ç†æ·å¾„ï¼Œæ— æ³•è¡¡é‡çœŸæ­£çš„è§†è§‰ä¸­å¿ƒæ¨ç†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisuLogicï¼šä¸€ä¸ªåŒ…å«1,000ä¸ªäººç±»éªŒè¯é—®é¢˜çš„åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªç±»åˆ«ï¼ˆå¦‚é‡åŒ–è½¬æ¢ã€ç©ºé—´å…³ç³»ã€å±æ€§æ¯”è¾ƒï¼‰ã€‚è¿™äº›ä¸åŒç±»å‹çš„é—®é¢˜å¯ä»¥ä»å¤šä¸ªè§’åº¦è¯„ä¼°MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹è¯¥åŸºå‡†è¯„ä¼°äº†é¢†å…ˆçš„MLLMsï¼Œå¹¶åˆ†æå…¶ç»“æœï¼Œä»¥è¯†åˆ«å¸¸è§çš„å¤±è´¥æ¨¡å¼ã€‚å¤§å¤šæ•°æ¨¡å‹çš„å‡†ç¡®ç‡ä½äº30%ï¼Œä»…ç•¥é«˜äº25%çš„éšæœºåŸºçº¿ï¼Œè¿œä½äºäººç±»çš„51.4%ï¼Œæ˜¾ç¤ºå‡ºè§†è§‰æ¨ç†æ–¹é¢çš„æ˜¾è‘—å·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªè¡¥å……è®­ç»ƒæ•°æ®é›†å’Œä¸€ä¸ªå¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„è¿›å±•ã€‚",
        "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
        "pinyin": "è§†è§‰æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œä¹Ÿæ˜¯å…ˆè¿›å¤šæ¨¡æ€æ¨¡å‹çš„å…³é”®èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†è¯„ä¼°å¾€å¾€ä¾èµ–æ–‡æœ¬æè¿°ï¼Œå…è®¸åŸºäºè¯­è¨€çš„æ¨ç†æ·å¾„ï¼Œæ— æ³•è¡¡é‡çœŸæ­£çš„è§†è§‰ä¸­å¿ƒæ¨ç†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisuLogicï¼šä¸€ä¸ªåŒ…å«1,000ä¸ªäººç±»éªŒè¯é—®é¢˜çš„åŸºå‡†ï¼Œæ¶µç›–å…­ä¸ªç±»åˆ«ï¼ˆå¦‚é‡åŒ–è½¬æ¢ã€ç©ºé—´å…³ç³»ã€å±æ€§æ¯”è¾ƒï¼‰ã€‚è¿™äº›ä¸åŒç±»å‹çš„é—®é¢˜å¯ä»¥ä»å¤šä¸ªè§’åº¦è¯„ä¼°MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹è¯¥åŸºå‡†è¯„ä¼°äº†é¢†å…ˆçš„MLLMsï¼Œå¹¶åˆ†æå…¶ç»“æœï¼Œä»¥è¯†åˆ«å¸¸è§çš„å¤±è´¥æ¨¡å¼ã€‚å¤§å¤šæ•°æ¨¡å‹çš„å‡†ç¡®ç‡ä½äº30%ï¼Œä»…ç•¥é«˜äº25%çš„éšæœºåŸºçº¿ï¼Œè¿œä½äºäººç±»çš„51.4%ï¼Œæ˜¾ç¤ºå‡ºè§†è§‰æ¨ç†æ–¹é¢çš„æ˜¾è‘—å·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªè¡¥å……è®­ç»ƒæ•°æ®é›†å’Œä¸€ä¸ªå¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„è¿›å±•ã€‚\n\nShÃ¬juÃ© tuÄ«lÇ shÃ¬ rÃ©nlÃ¨i zhÃ¬nÃ©ng de hÃ©xÄ«n zÇ”chÃ©ng bÃ¹fÄ“n, yÄ› shÃ¬ xiÄnjÃ¬n duÅ mÃ³shÃ¬ duÅ mÃ³shÃ¬ mÃ³xÃ­ng de guÇnjiÃ n nÃ©nglÃ¬. RÃ¡n'Ã©r, dÄngqiÃ¡n duÃ¬ duÅ mÃ³shÃ¬ dÃ  yÇ”yÃ¡n mÃ³xÃ­ng (MLLMs) de tuÄ«lÇ pÃ­ngjiÃ  wÇngwÇng yÄ«lÃ i wÃ©nbÄ›n miÃ¡oshÃ¹, yÇ”n xÇ” jÄ«yÃº yÇ”yÃ¡n de tuÄ«lÇ jiÃ©jÃ¬ng, wÃºfÇ hÃ©ngliÃ¡ng zhÄ“nzhÃ¨ng de shÃ¬juÃ© zhÅngxÄ«n tuÄ«lÇ. WÃ¨i jiÄ›juÃ© zhÃ¨ yÄ« wÃ¨ntÃ­, wÇ’men yÇn rÃ¹le VisuLogic: yÄ«gÃ¨ bÄohÃ¡n 1,000 gÃ¨ rÃ©nlÃ¨i yÃ nzhÃ¨ng wÃ¨ntÃ­ de jÄ«zhÇ”n, hÃ¡njiÄ“ liÃ¹ gÃ¨ lÃ¨ibiÃ© (rÃº liÃ ng huÃ  zhuÇnhuÃ n, kÅngjiÄn guÄnxÃ¬, shÇ”xÃ¬ng bÇjiÃ o). ZhÃ¨xiÄ“ bÃ¹tÃ³ng lÃ¨ixÃ­ng de wÃ¨ntÃ­ kÄ›yÇ cÃ³ng duÅ gÃ¨ jiÇodÃ¹ pÃ­ngjiÃ  MLLMs de shÃ¬juÃ© tuÄ«lÇ nÃ©nglÃ¬. WÇ’men duÃ¬ gÄi jÄ«zhÇ”n pÃ­ngjiÃ  le lÇngxiÄn de MLLMs, bÃ¬ng fÄ“nxi qÃ­ jiÃ©guÇ’, yÇ shÃ­biÃ© chÃ¡ngjiÃ n de shÄ«bÃ i mÃ³shÃ¬. DÃ duÅshÃ¹ mÃ³xÃ­ng de zhÇ”nquÃ¨lÇœ dÄ«yÃº 30%, jÇn lÃ¼Ã¨ gÄoyÃº 25% de suÃ­jÄ« jÄ«zhÇ”n, yuÇn dÄ«yÃº rÃ©nlÃ¨i de 51.4%, xiÇnshÃ¬ chÅ« shÃ¬juÃ© tuÄ«lÇ fÄngmiÃ n de xiÇnzhÃ¹ chÄjÃ¹. CÇwÃ i, wÇ’men tÃ­gÅngle yÄ«gÃ¨ bÇ”chÅng xÃ¹nliÃ n shÃ¹jÃ¹jÃ­ hÃ© yÄ«gÃ¨ qiÃ¡ngzhÃ¹ xuÃ©xÃ­ jÄ«zhÇ”n, yÇ zhÄ«chÃ­ jÃ¬nfÄ de jÃ¬nbÃ¹.",
        "vocab": "[\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬juÃ©\", \"trans\": \"vision\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ«lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"æ ¸å¿ƒ\", \"pinyin\": \"hÃ©xÄ«n\", \"trans\": \"core\"},\n    {\"word\": \"ç»„æˆéƒ¨åˆ†\", \"pinyin\": \"zÇ”chÃ©ng bÃ¹fÄ“n\", \"trans\": \"component\"},\n    {\"word\": \"å…ˆè¿›\", \"pinyin\": \"xiÄnjÃ¬n\", \"trans\": \"advanced\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³shÃ¬\", \"trans\": \"multimodal\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÇnjiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©nglÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"ä¾èµ–\", \"pinyin\": \"yÄ«lÃ i\", \"trans\": \"rely on\"},\n    {\"word\": \"æè¿°\", \"pinyin\": \"miÃ¡oshÃ¹\", \"trans\": \"description\"},\n    {\"word\": \"å…è®¸\", \"pinyin\": \"yÇ”nxÇ”\", \"trans\": \"allow\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ«yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"æ·å¾„\", \"pinyin\": \"jiÃ©jÃ¬ng\", \"trans\": \"shortcut\"},\n    {\"word\": \"è¡¡é‡\", \"pinyin\": \"hÃ©ngliÃ¡ng\", \"trans\": \"measure\"},\n    {\"word\": \"çœŸæ­£\", \"pinyin\": \"zhÄ“nzhÃ¨ng\", \"trans\": \"genuine\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇnrÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ«zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"éªŒè¯\", \"pinyin\": \"yÃ nzhÃ¨ng\", \"trans\": \"verification\"},\n    {\"word\": \"é—®é¢˜\", \"pinyin\": \"wÃ¨ntÃ­\", \"trans\": \"question\"},\n    {\"word\": \"æ¶µç›–\", \"pinyin\": \"hÃ¡ngÃ i\", \"trans\": \"cover\"},\n    {\"word\": \"ç±»åˆ«\", \"pinyin\": \"lÃ¨ibiÃ©\", \"trans\": \"category\"},\n    {\"word\": \"é‡åŒ–\", \"pinyin\": \"liÃ nghuÃ \", \"trans\": \"quantification\"},\n    {\"word\": \"è½¬æ¢\", \"pinyin\": \"zhuÇnhuÃ n\", \"trans\": \"conversion\"},\n    {\"word\": \"ç©ºé—´\", \"pinyin\": \"kÅngjiÄn\", \"trans\": \"space\"},\n    {\"word\": \"å…³ç³»\", \"pinyin\": \"guÄnxÃ¬\", \"trans\": \"relationship\"},\n    {\"word\": \"å±æ€§\", \"pinyin\": \"shÇ”xÃ¬ng\", \"trans\": \"attribute\"},\n    {\"word\": \"æ¯”è¾ƒ\", \"pinyin\": \"bÇjiÃ o\", \"trans\": \"comparison\"},\n    {\"word\": \"è§’åº¦\", \"pinyin\": \"jiÇodÃ¹\", \"trans\": \"angle\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­nggÅ«\", \"trans\": \"evaluate\"},\n    {\"word\": \"é¢†å…ˆ\", \"pinyin\": \"lÇngxiÄn\", \"trans\": \"leading\"},\n    {\"word\": \"åˆ†æ\", \"pinyin\": \"fÄ“nxÄ«\", \"trans\": \"analyze\"},\n    {\"word\": \"ç»“æœ\", \"pinyin\": \"jiÃ©guÇ’\", \"trans\": \"result\"},\n    {\"word\": \"è¯†åˆ«\", \"pinyin\": \"shÃ­biÃ©\", \"trans\": \"identify\"},\n    {\"word\": \"æ¨¡å¼\", \"pinyin\": \"mÃ³shÃ¬\", \"trans\": \"pattern\"},\n    {\"word\": \"å‡†ç¡®ç‡\", \"pinyin\": \"zhÇ”nquÃ¨lÇœ\", \"trans\": \"accuracy\"},\n    {\"word\": \"éšæœº\", \"pinyin\": \"suÃ­jÄ«\", \"trans\": \"random\"},\n    {\"word\": \"åŸºçº¿\", \"pinyin\": \"jÄ«xiÃ n\", \"trans\": \"baseline\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇnzhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"å·®è·\", \"pinyin\": \"chÄjÃ¹\", \"trans\": \"gap\"},\n    {\"word\": \"è¡¥å……\", \"pinyin\": \"bÇ”chÅng\", \"trans\": \"supplement\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹jÃ¹jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"å¼ºåŒ–\", \"pinyin\": \"qiÃ¡nghuÃ \", \"trans\": \"reinforce\"},\n    {\"word\": \"å­¦ä¹ \", \"pinyin\": \"xuÃ©xÃ­\", \"trans\": \"learning\"},\n    {\"word\": \"æ”¯æŒ\", \"pinyin\": \"zhÄ«chÃ­\", \"trans\": \"support\"},\n    {\"word\": \"è¿›å±•\", \"pinyin\": \"jÃ¬nzhÇn\", \"trans\": \"progress\"}\n]",
        "trans": "Visual reasoning is a core component of human intelligence and a key capability of advanced multimodal models. However, current evaluations of reasoning in multimodal large language models (MLLMs) often rely on textual descriptions, allowing for language-based reasoning shortcuts that fail to measure true vision-centric reasoning. To address this issue, we introduce VisuLogic: a benchmark containing 1,000 human-verified questions covering six categories (such as quantitative transformations, spatial relationships, and attribute comparisons). These different types of questions can evaluate the visual reasoning capabilities of MLLMs from multiple angles. We evaluated leading MLLMs on this benchmark and analyzed the results to identify common failure modes. Most models achieved an accuracy of less than 30%, only slightly above the 25% random baseline and significantly lower than the human accuracy of 51.4%, indicating a notable gap in visual reasoning. Additionally, we provide a supplementary training dataset and a reinforcement learning baseline to support further advancements.",
        "update_ts": "2025-04-24 09:12"
    }
}