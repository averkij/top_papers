{
    "date": {
        "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 18",
        "zh": "9æœˆ18æ—¥"
    },
    "time_utc": "2024-09-18 09:00",
    "weekday": 2,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-18",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.11340",
            "title": "OmniGen: Unified Image Generation",
            "url": "https://huggingface.co/papers/2409.11340",
            "abstract": "In this work, we introduce OmniGen, a new diffusion model for unified image generation. Unlike popular diffusion models (e.g., Stable Diffusion), OmniGen no longer requires additional modules such as ControlNet or IP-Adapter to process diverse control conditions. OmniGenis characterized by the following features: 1) Unification: OmniGen not only demonstrates text-to-image generation capabilities but also inherently supports other downstream tasks, such as image editing, subject-driven generation, and visual-conditional generation. Additionally, OmniGen can handle classical computer vision tasks by transforming them into image generation tasks, such as edge detection and human pose recognition. 2) Simplicity: The architecture of OmniGen is highly simplified, eliminating the need for additional text encoders. Moreover, it is more user-friendly compared to existing diffusion models, enabling complex tasks to be accomplished through instructions without the need for extra preprocessing steps (e.g., human pose estimation), thereby significantly simplifying the workflow of image generation. 3) Knowledge Transfer: Through learning in a unified format, OmniGen effectively transfers knowledge across different tasks, manages unseen tasks and domains, and exhibits novel capabilities. We also explore the model's reasoning capabilities and potential applications of chain-of-thought mechanism. This work represents the first attempt at a general-purpose image generation model, and there remain several unresolved issues. We will open-source the related resources at https://github.com/VectorSpaceLab/OmniGen to foster advancements in this field.",
            "score": 106,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "f00584cb183c5a7a",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#agents",
                    "#reasoning",
                    "#transfer_learning",
                    "#architecture"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "OmniGen: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "OmniGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° OmniGen ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ° Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "OmniGen: Simplifying Unified Image Generation for All Tasks",
                    "desc": "OmniGen is a novel diffusion model designed for unified image generation, capable of handling various tasks without needing extra modules. It simplifies the process by integrating text-to-image generation with other functionalities like image editing and classical computer vision tasks. The model's architecture is streamlined, allowing users to perform complex tasks through simple instructions, thus enhancing usability. Additionally, OmniGen facilitates knowledge transfer across different tasks, showcasing its potential for reasoning and adaptability in unseen domains."
                },
                "zh": {
                    "title": "OmniGenï¼šç»Ÿä¸€å›¾åƒç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹OmniGenï¼Œç”¨äºç»Ÿä¸€çš„å›¾åƒç”Ÿæˆã€‚ä¸æµè¡Œçš„æ‰©æ•£æ¨¡å‹ä¸åŒï¼ŒOmniGenä¸å†éœ€è¦é¢å¤–çš„æ¨¡å—æ¥å¤„ç†å¤šæ ·çš„æ§åˆ¶æ¡ä»¶ã€‚OmniGenå…·æœ‰ç»Ÿä¸€æ€§ã€ç®€åŒ–æ€§å’ŒçŸ¥è¯†è½¬ç§»ç­‰ç‰¹ç‚¹ï¼Œèƒ½å¤Ÿæ”¯æŒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘ç­‰å¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶ä¸”ç®€åŒ–äº†ç”¨æˆ·æ“ä½œæµç¨‹ã€‚è¯¥æ¨¡å‹çš„é¦–æ¬¡å°è¯•ä¸ºé€šç”¨å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæœªæ¥å°†å¼€æºç›¸å…³èµ„æºä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11402",
            "title": "NVLM: Open Frontier-Class Multimodal LLMs",
            "url": "https://huggingface.co/papers/2409.11402",
            "abstract": "We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we are releasing the model weights and will open-source the code for the community: https://nvlm-project.github.io/.",
            "score": 71,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "48599eb4efe8218f",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "NVLM 1.0: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜",
                    "desc": "NVLM 1.0 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ²ÑĞ·Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ 1D Ñ‚Ğ°Ğ¹Ğ»-Ñ‚ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. NVLM 1.0 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ LLM Ğ¿Ğ¾ÑĞ»Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "NVLM 1.0: Redefining Multimodal Language Models for Superior Performance",
                    "desc": "The NVLM 1.0 is a new family of multimodal large language models that excel in tasks involving both vision and language, achieving top results compared to existing models. It demonstrates enhanced performance in text-only tasks after being trained with multimodal data. The paper compares different model architectures and proposes a new design that improves training efficiency and reasoning abilities. Additionally, it emphasizes the importance of high-quality datasets over sheer scale, and the authors plan to share their model and code with the research community."
                },
                "zh": {
                    "title": "NVLM 1.0ï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ–°çªç ´",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†NVLM 1.0ï¼Œè¿™æ˜¯ä¸€ç§å‰æ²¿çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—æœ€å…ˆè¿›çš„æˆæœã€‚NVLM 1.0åœ¨å¤šæ¨¡æ€è®­ç»ƒåï¼Œæ–‡æœ¬æ€§èƒ½æ˜¾è‘—æå‡ï¼Œè¶…è¶Šäº†å…¶åŸºç¡€çš„è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä»…è§£ç å™¨çš„å¤šæ¨¡æ€æ¨¡å‹å’ŒåŸºäºäº¤å‰æ³¨æ„åŠ›çš„æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§æ–°æ¶æ„ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡å’Œå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç²¾å¿ƒç­–åˆ’äº†å¤šæ¨¡æ€é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒæ•°æ®é›†ï¼Œå‘ç°æ•°æ®é›†è´¨é‡å’Œä»»åŠ¡å¤šæ ·æ€§æ¯”è§„æ¨¡æ›´ä¸ºé‡è¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11355",
            "title": "Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think",
            "url": "https://huggingface.co/papers/2409.11355",
            "abstract": "Recent work showed that large diffusion models can be reused as highly precise monocular depth estimators by casting depth estimation as an image-conditional image generation task. While the proposed model achieved state-of-the-art results, high computational demands due to multi-step inference limited its use in many scenarios. In this paper, we show that the perceived inefficiency was caused by a flaw in the inference pipeline that has so far gone unnoticed. The fixed model performs comparably to the best previously reported configuration while being more than 200times faster. To optimize for downstream task performance, we perform end-to-end fine-tuning on top of the single-step model with task-specific losses and get a deterministic model that outperforms all other diffusion-based depth and normal estimation models on common zero-shot benchmarks. We surprisingly find that this fine-tuning protocol also works directly on Stable Diffusion and achieves comparable performance to current state-of-the-art diffusion-based depth and normal estimation models, calling into question some of the conclusions drawn from prior works.",
            "score": 28,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "4d17730da7f1e732",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 200 Ñ€Ğ°Ğ· Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ² Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹. ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ¾, Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ Ğ´Ğ»Ñ Stable Diffusion, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚."
                },
                "en": {
                    "title": "Revolutionizing Depth Estimation: Fast and Efficient Diffusion Models",
                    "desc": "This paper addresses the inefficiencies in using large diffusion models for monocular depth estimation, which were previously thought to require multi-step inference. The authors identify a flaw in the inference pipeline that, when corrected, allows for a single-step model that is over 200 times faster while maintaining competitive performance. They also introduce an end-to-end fine-tuning approach that enhances the model's performance on specific tasks, surpassing existing diffusion-based models in zero-shot benchmarks. Additionally, the findings challenge previous assumptions about the capabilities of diffusion models in depth and normal estimation tasks."
                },
                "zh": {
                    "title": "æå‡æ·±åº¦ä¼°è®¡æ•ˆç‡çš„çªç ´æ€§è¿›å±•",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹æ‰©æ•£æ¨¡å‹å¯ä»¥ä½œä¸ºé«˜ç²¾åº¦çš„å•ç›®æ·±åº¦ä¼°è®¡å™¨ï¼Œé€šè¿‡å°†æ·±åº¦ä¼°è®¡è§†ä¸ºå›¾åƒæ¡ä»¶çš„å›¾åƒç”Ÿæˆä»»åŠ¡æ¥å®ç°ã€‚å°½ç®¡æ‰€æå‡ºçš„æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œä½†ç”±äºå¤šæ­¥æ¨ç†çš„é«˜è®¡ç®—éœ€æ±‚ï¼Œé™åˆ¶äº†å…¶åœ¨è®¸å¤šåœºæ™¯ä¸­çš„ä½¿ç”¨ã€‚æœ¬æ–‡æ­ç¤ºäº†æ¨ç†æµç¨‹ä¸­çš„ä¸€ä¸ªæœªè¢«æ³¨æ„çš„ç¼ºé™·ï¼Œå¯¼è‡´äº†æ„ŸçŸ¥ä¸Šçš„ä½æ•ˆç‡ã€‚ç»è¿‡ä¿®æ­£çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸ä¹‹å‰æŠ¥å‘Šçš„æœ€ä½³é…ç½®ç›¸å½“ï¼Œä½†é€Ÿåº¦æé«˜äº†200å€ä»¥ä¸Šï¼Œå¹¶ä¸”é€šè¿‡ç«¯åˆ°ç«¯çš„å¾®è°ƒï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11406",
            "title": "Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion",
            "url": "https://huggingface.co/papers/2409.11406",
            "abstract": "In 3D modeling, designers often use an existing 3D model as a reference to create new ones. This practice has inspired the development of Phidias, a novel generative model that uses diffusion for reference-augmented 3D generation. Given an image, our method leverages a retrieved or user-provided 3D reference model to guide the generation process, thereby enhancing the generation quality, generalization ability, and controllability. Our model integrates three key components: 1) meta-ControlNet that dynamically modulates the conditioning strength, 2) dynamic reference routing that mitigates misalignment between the input image and 3D reference, and 3) self-reference augmentations that enable self-supervised training with a progressive curriculum. Collectively, these designs result in a clear improvement over existing methods. Phidias establishes a unified framework for 3D generation using text, image, and 3D conditions with versatile applications.",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "67298e05fdb45375",
            "data": {
                "categories": [
                    "#3d",
                    "#rag",
                    "#cv"
                ],
                "emoji": "ğŸ—¿",
                "ru": {
                    "title": "Phidias: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ²",
                    "desc": "Phidias - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ 3D-Ñ€ĞµÑ„ĞµÑ€ĞµĞ½Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ°-ControlNet, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ² Ğ¸ ÑĞ°Ğ¼Ğ¾Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Phidias Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Phidias: Enhancing 3D Generation with Smart References",
                    "desc": "Phidias is a new generative model designed for creating 3D models by using existing 3D references. It employs a diffusion process to enhance the quality and control of the generated models based on input images. The model features three main components: a meta-ControlNet for adjusting conditioning strength, dynamic reference routing to align images with 3D references, and self-reference augmentations for self-supervised training. Overall, Phidias improves upon previous methods and offers a flexible framework for 3D generation using various input types."
                },
                "zh": {
                    "title": "Phidiasï¼šå¢å¼ºä¸‰ç»´ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "åœ¨ä¸‰ç»´å»ºæ¨¡ä¸­ï¼Œè®¾è®¡å¸ˆå¸¸å¸¸ä½¿ç”¨ç°æœ‰çš„ä¸‰ç»´æ¨¡å‹ä½œä¸ºå‚è€ƒæ¥åˆ›å»ºæ–°çš„æ¨¡å‹ã€‚Phidiasæ˜¯ä¸€ç§æ–°é¢–çš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æŠ€æœ¯è¿›è¡Œå‚è€ƒå¢å¼ºçš„ä¸‰ç»´ç”Ÿæˆã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆæ£€ç´¢åˆ°çš„æˆ–ç”¨æˆ·æä¾›çš„ä¸‰ç»´å‚è€ƒæ¨¡å‹ï¼Œæ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€æ³›åŒ–èƒ½åŠ›å’Œå¯æ§æ€§ã€‚Phidiaså»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¯ä»¥ä½¿ç”¨æ–‡æœ¬ã€å›¾åƒå’Œä¸‰ç»´æ¡ä»¶è¿›è¡Œä¸‰ç»´ç”Ÿæˆï¼Œå…·æœ‰å¤šç§åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11136",
            "title": "Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models",
            "url": "https://huggingface.co/papers/2409.11136",
            "abstract": "Instruction-tuned language models (LM) are able to respond to imperative commands, providing a more natural user interface compared to their base counterparts. In this work, we present Promptriever, the first retrieval model able to be prompted like an LM. To train Promptriever, we curate and release a new instance-level instruction training set from MS MARCO, spanning nearly 500k instances. Promptriever not only achieves strong performance on standard retrieval tasks, but also follows instructions. We observe: (1) large gains (reaching SoTA) on following detailed relevance instructions (+14.3 p-MRR / +3.1 nDCG on FollowIR), (2) significantly increased robustness to lexical choices/phrasing in the query+instruction (+12.9 Robustness@10 on InstructIR), and (3) the ability to perform hyperparameter search via prompting to reliably improve retrieval performance (+1.4 average increase on BEIR). Promptriever demonstrates that retrieval models can be controlled with prompts on a per-query basis, setting the stage for future work aligning LM prompting techniques with information retrieval.",
            "score": 21,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "e070d3d767ca4cff",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#rag",
                    "#alignment"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Promptriever: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Promptriever - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°Ğ¼Ğ¸, ĞºĞ°Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Promptriever Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· MS MARCO, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¾ĞºĞ¾Ğ»Ğ¾ 500 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ…. Promptriever Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°."
                },
                "en": {
                    "title": "Prompting the Future of Information Retrieval with Promptriever",
                    "desc": "This paper introduces Promptriever, a novel retrieval model that can be prompted similarly to language models (LMs). It is trained on a new dataset derived from MS MARCO, which includes nearly 500,000 instances of instruction-based queries. Promptriever shows significant improvements in retrieval tasks, achieving state-of-the-art results when following detailed relevance instructions and demonstrating enhanced robustness to variations in query phrasing. Additionally, it can optimize its performance through hyperparameter tuning via prompts, paving the way for integrating LM prompting methods into information retrieval systems."
                },
                "zh": {
                    "title": "Promptrieverï¼šæŒ‡ä»¤é©±åŠ¨çš„æ£€ç´¢æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPromptrieverçš„æ£€ç´¢æ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿåƒè¯­è¨€æ¨¡å‹ä¸€æ ·å“åº”æŒ‡ä»¤ã€‚æˆ‘ä»¬ä»MS MARCOä¸­æ•´ç†å¹¶å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„å®ä¾‹çº§æŒ‡ä»¤è®­ç»ƒé›†ï¼ŒåŒ…å«è¿‘50ä¸‡ä¸ªå®ä¾‹ã€‚Promptrieveråœ¨æ ‡å‡†æ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°éµå¾ªè¯¦ç»†çš„ç›¸å…³æ€§æŒ‡ä»¤ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒPromptrieveråœ¨æ£€ç´¢æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿé€šè¿‡æç¤ºè¿›è¡Œè¶…å‚æ•°æœç´¢ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜æ£€ç´¢æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.10819",
            "title": "EzAudio: Enhancing Text-to-Audio Generation with Efficient Diffusion Transformer",
            "url": "https://huggingface.co/papers/2409.10819",
            "abstract": "Latent diffusion models have shown promising results in text-to-audio (T2A) generation tasks, yet previous models have encountered difficulties in generation quality, computational cost, diffusion sampling, and data preparation. In this paper, we introduce EzAudio, a transformer-based T2A diffusion model, to handle these challenges. Our approach includes several key innovations: (1) We build the T2A model on the latent space of a 1D waveform Variational Autoencoder (VAE), avoiding the complexities of handling 2D spectrogram representations and using an additional neural vocoder. (2) We design an optimized diffusion transformer architecture specifically tailored for audio latent representations and diffusion modeling, which enhances convergence speed, training stability, and memory usage, making the training process easier and more efficient. (3) To tackle data scarcity, we adopt a data-efficient training strategy that leverages unlabeled data for learning acoustic dependencies, audio caption data annotated by audio-language models for text-to-audio alignment learning, and human-labeled data for fine-tuning. (4) We introduce a classifier-free guidance (CFG) rescaling method that simplifies EzAudio by achieving strong prompt alignment while preserving great audio quality when using larger CFG scores, eliminating the need to struggle with finding the optimal CFG score to balance this trade-off. EzAudio surpasses existing open-source models in both objective metrics and subjective evaluations, delivering realistic listening experiences while maintaining a streamlined model structure, low training costs, and an easy-to-follow training pipeline. Code, data, and pre-trained models are released at: https://haidog-yaqub.github.io/EzAudio-Page/.",
            "score": 17,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "dc5844f3b0b10c50",
            "data": {
                "categories": [
                    "#audio",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "EzAudio: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "EzAudio - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ 1D VAE, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. EzAudio Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "EzAudio: Simplifying Text-to-Audio Generation with Efficiency and Quality",
                    "desc": "This paper presents EzAudio, a transformer-based model designed for text-to-audio (T2A) generation, addressing issues like generation quality and computational efficiency. By utilizing a latent space from a 1D waveform Variational Autoencoder (VAE), EzAudio simplifies the process of audio generation without the need for complex spectrograms. The model features an optimized diffusion transformer architecture that improves training stability and reduces memory usage, making it more efficient. Additionally, it employs a data-efficient training strategy and a classifier-free guidance method to enhance audio quality and prompt alignment, outperforming existing models in both objective and subjective evaluations."
                },
                "zh": {
                    "title": "EzAudioï¼šé«˜æ•ˆçš„æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆæ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆæ¨¡å‹EzAudioï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ã€è®¡ç®—æˆæœ¬å’Œæ•°æ®å‡†å¤‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç»´æ³¢å½¢å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„æ½œåœ¨ç©ºé—´æ¥æ„å»ºæ¨¡å‹ï¼Œé¿å…äº†å¤„ç†äºŒç»´å£°è°±å›¾çš„å¤æ‚æ€§ã€‚é€šè¿‡ä¼˜åŒ–çš„æ‰©æ•£å˜æ¢å™¨æ¶æ„ï¼Œæˆ‘ä»¬æé«˜äº†æ”¶æ•›é€Ÿåº¦å’Œè®­ç»ƒç¨³å®šæ€§ï¼ŒåŒæ—¶é™ä½äº†å†…å­˜ä½¿ç”¨ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€ç§æ•°æ®é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®å’Œäººç±»æ ‡æ³¨æ•°æ®æ¥å¢å¼ºæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11055",
            "title": "A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B",
            "url": "https://huggingface.co/papers/2409.11055",
            "abstract": "Prior research works have evaluated quantized LLMs using limited metrics such as perplexity or a few basic knowledge tasks and old datasets. Additionally, recent large-scale models such as Llama 3.1 with up to 405B have not been thoroughly examined. This paper evaluates the performance of instruction-tuned LLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on models ranging from 7B to 405B. Using 13 benchmarks, we assess performance across six task types: commonsense Q\\&A, knowledge and language understanding, instruction following, hallucination detection, mathematics, and dialogue. Our key findings reveal that (1) quantizing a larger LLM to a similar size as a smaller FP16 LLM generally performs better across most benchmarks, except for hallucination detection and instruction following; (2) performance varies significantly with different quantization methods, model size, and bit-width, with weight-only methods often yielding better results in larger models; (3) task difficulty does not significantly impact accuracy degradation due to quantization; and (4) the MT-Bench evaluation method has limited discriminatory power among recent high-performing LLMs.",
            "score": 16,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "bb004f38b9982a21",
            "data": {
                "categories": [
                    "#inference",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ - Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ»ÑƒÑ‡ÑˆĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 7B Ğ´Ğ¾ 405B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 13 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑˆĞµÑÑ‚Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ MT-Bench Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Unlocking the Power of Quantized Large Language Models",
                    "desc": "This paper investigates the performance of instruction-tuned large language models (LLMs) when subjected to various quantization techniques, including GPTQ, AWQ, SmoothQuant, and FP8. It evaluates models ranging from 7 billion to 405 billion parameters across 13 diverse benchmarks, covering tasks like commonsense Q&A and dialogue. The findings indicate that larger quantized models often outperform smaller FP16 models, although performance varies with quantization methods and model sizes. Additionally, the study highlights that task difficulty does not significantly affect accuracy loss due to quantization, and the MT-Bench evaluation method may not effectively differentiate between high-performing LLMs."
                },
                "zh": {
                    "title": "é‡åŒ–æ–¹æ³•å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å…¨é¢è¯„ä¼°",
                    "desc": "æœ¬è®ºæ–‡è¯„ä¼°äº†ä¸åŒé‡åŒ–æ–¹æ³•å¯¹æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬GPTQã€AWQã€SmoothQuantå’ŒFP8ã€‚æˆ‘ä»¬ä½¿ç”¨13ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å¸¸è¯†é—®ç­”ã€çŸ¥è¯†å’Œè¯­è¨€ç†è§£ã€æŒ‡ä»¤è·Ÿéšã€å¹»è§‰æ£€æµ‹ã€æ•°å­¦å’Œå¯¹è¯ç­‰å…­ç§ä»»åŠ¡ç±»å‹ã€‚ç ”ç©¶å‘ç°ï¼Œé‡åŒ–è¾ƒå¤§çš„LLMé€šå¸¸åœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç›¸åŒå¤§å°çš„è¾ƒå°FP16 LLMï¼Œå°½ç®¡åœ¨å¹»è§‰æ£€æµ‹å’ŒæŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­è¡¨ç°æœ‰æ‰€ä¸åŒã€‚æ­¤å¤–ï¼Œä¸åŒçš„é‡åŒ–æ–¹æ³•ã€æ¨¡å‹å¤§å°å’Œä½å®½å¯¹æ€§èƒ½çš„å½±å“æ˜¾è‘—ï¼Œè€Œä»»åŠ¡éš¾åº¦å¯¹é‡åŒ–å¼•èµ·çš„å‡†ç¡®æ€§ä¸‹é™å½±å“ä¸å¤§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11367",
            "title": "OSV: One Step is Enough for High-Quality Image to Video Generation",
            "url": "https://huggingface.co/papers/2409.11367",
            "abstract": "Video diffusion models have shown great potential in generating high-quality videos, making them an increasingly popular focus. However, their inherent iterative nature leads to substantial computational and time costs. While efforts have been made to accelerate video diffusion by reducing inference steps (through techniques like consistency distillation) and GAN training (these approaches often fall short in either performance or training stability). In this work, we introduce a two-stage training framework that effectively combines consistency distillation with GAN training to address these challenges. Additionally, we propose a novel video discriminator design, which eliminates the need for decoding the video latents and improves the final performance. Our model is capable of producing high-quality videos in merely one-step, with the flexibility to perform multi-step refinement for further performance enhancement. Our quantitative evaluation on the OpenWebVid-1M benchmark shows that our model significantly outperforms existing methods. Notably, our 1-step performance(FVD 171.15) exceeds the 8-step performance of the consistency distillation based method, AnimateLCM (FVD 184.79), and approaches the 25-step performance of advanced Stable Video Diffusion (FVD 156.94).",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "b0d563ca10cd945c",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ GAN-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³, Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ. ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OpenWebVid-1M Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Video Generation: One-Step High-Quality Output!",
                    "desc": "This paper presents a new approach to video generation using diffusion models, which are known for their ability to create high-quality videos. The authors introduce a two-stage training framework that merges consistency distillation with Generative Adversarial Network (GAN) training, aiming to reduce the computational costs associated with video generation. A key innovation is the design of a video discriminator that avoids the need to decode video latents, enhancing performance. The proposed model achieves impressive results, generating high-quality videos in just one step while also allowing for optional multi-step refinement, outperforming existing methods on benchmark evaluations."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ–¹é¢å±•ç°äº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†å…¶è¿­ä»£ç‰¹æ€§å¯¼è‡´äº†è¾ƒé«˜çš„è®¡ç®—å’Œæ—¶é—´æˆæœ¬ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæœ‰æ•ˆç»“åˆäº†ä¸€è‡´æ€§è’¸é¦å’ŒGANè®­ç»ƒï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§æ–°çš„è§†é¢‘é‰´åˆ«å™¨ï¼Œçœå»äº†å¯¹è§†é¢‘æ½œå˜é‡çš„è§£ç ï¼Œæå‡äº†æœ€ç»ˆæ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä»…ä¸€æ­¥ä¸­ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œå¹¶å…·å¤‡å¤šæ­¥ç²¾ç»†åŒ–çš„çµæ´»æ€§ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.10568",
            "title": "On the limits of agency in agent-based models",
            "url": "https://huggingface.co/papers/2409.10568",
            "abstract": "Agent-based modeling (ABM) seeks to understand the behavior of complex systems by simulating a collection of agents that act and interact within an environment. Their practical utility requires capturing realistic environment dynamics and adaptive agent behavior while efficiently simulating million-size populations. Recent advancements in large language models (LLMs) present an opportunity to enhance ABMs by using LLMs as agents with further potential to capture adaptive behavior. However, the computational infeasibility of using LLMs for large populations has hindered their widespread adoption. In this paper, we introduce AgentTorch -- a framework that scales ABMs to millions of agents while capturing high-resolution agent behavior using LLMs. We benchmark the utility of LLMs as ABM agents, exploring the trade-off between simulation scale and individual agency. Using the COVID-19 pandemic as a case study, we demonstrate how AgentTorch can simulate 8.4 million agents representing New York City, capturing the impact of isolation and employment behavior on health and economic outcomes. We compare the performance of different agent architectures based on heuristic and LLM agents in predicting disease waves and unemployment rates. Furthermore, we showcase AgentTorch's capabilities for retrospective, counterfactual, and prospective analyses, highlighting how adaptive agent behavior can help overcome the limitations of historical data in policy design. AgentTorch is an open-source project actively being used for policy-making and scientific discovery around the world. The framework is available here: github.com/AgentTorch/AgentTorch.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2024-09-14",
            "pub_date_card": {
                "ru": "14 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 14",
                "zh": "9æœˆ14æ—¥"
            },
            "hash": "14749474dc3c2b04",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#rl",
                    "#medicine"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "AgentTorch: ĞœĞ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "AgentTorch - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (ĞĞœ) Ğ´Ğ¾ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ¿Ğ°Ğ½Ğ´ĞµĞ¼Ğ¸Ğ¸ COVID-19 Ğ² ĞÑŒÑ-Ğ™Ğ¾Ñ€ĞºĞµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ AgentTorch Ğ´Ğ»Ñ Ñ€ĞµÑ‚Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾, ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ LLM."
                },
                "en": {
                    "title": "Scaling Agent-Based Models with Language Intelligence",
                    "desc": "This paper presents AgentTorch, a framework designed to enhance agent-based modeling (ABM) by integrating large language models (LLMs) as agents. It addresses the challenge of simulating millions of agents while maintaining realistic behaviors and interactions in complex environments. The framework is demonstrated through a case study on the COVID-19 pandemic, simulating 8.4 million agents to analyze health and economic outcomes. AgentTorch not only benchmarks the performance of LLMs against traditional agent architectures but also supports various analytical approaches for effective policy-making."
                },
                "zh": {
                    "title": "AgentTorchï¼šå¤§è§„æ¨¡ä»£ç†å»ºæ¨¡çš„æ–°çªç ´",
                    "desc": "ä»£ç†åŸºç¡€å»ºæ¨¡ï¼ˆABMï¼‰é€šè¿‡æ¨¡æ‹Ÿä¸€ç»„åœ¨ç¯å¢ƒä¸­è¡ŒåŠ¨å’Œäº’åŠ¨çš„ä»£ç†ï¼Œæ¥ç†è§£å¤æ‚ç³»ç»Ÿçš„è¡Œä¸ºã€‚æœ¬æ–‡æå‡ºäº†AgentTorchæ¡†æ¶ï¼Œèƒ½å¤Ÿå°†ABMæ‰©å±•åˆ°æ•°ç™¾ä¸‡ä¸ªä»£ç†ï¼ŒåŒæ—¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ•æ‰é«˜åˆ†è¾¨ç‡çš„ä»£ç†è¡Œä¸ºã€‚æˆ‘ä»¬ä»¥COVID-19ç–«æƒ…ä¸ºæ¡ˆä¾‹ï¼Œå±•ç¤ºäº†AgentTorchå¦‚ä½•æ¨¡æ‹Ÿ840ä¸‡åä»£è¡¨çº½çº¦å¸‚çš„ä»£ç†ï¼Œåˆ†æéš”ç¦»å’Œå°±ä¸šè¡Œä¸ºå¯¹å¥åº·å’Œç»æµç»“æœçš„å½±å“ã€‚AgentTorchæ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ­£åœ¨å…¨çƒèŒƒå›´å†…ç”¨äºæ”¿ç­–åˆ¶å®šå’Œç§‘å­¦å‘ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.10923",
            "title": "Agile Continuous Jumping in Discontinuous Terrains",
            "url": "https://huggingface.co/papers/2409.10923",
            "abstract": "We focus on agile, continuous, and terrain-adaptive jumping of quadrupedal robots in discontinuous terrains such as stairs and stepping stones. Unlike single-step jumping, continuous jumping requires accurately executing highly dynamic motions over long horizons, which is challenging for existing approaches. To accomplish this task, we design a hierarchical learning and control framework, which consists of a learned heightmap predictor for robust terrain perception, a reinforcement-learning-based centroidal-level motion policy for versatile and terrain-adaptive planning, and a low-level model-based leg controller for accurate motion tracking. In addition, we minimize the sim-to-real gap by accurately modeling the hardware characteristics. Our framework enables a Unitree Go1 robot to perform agile and continuous jumps on human-sized stairs and sparse stepping stones, for the first time to the best of our knowledge. In particular, the robot can cross two stair steps in each jump and completes a 3.5m long, 2.8m high, 14-step staircase in 4.5 seconds. Moreover, the same policy outperforms baselines in various other parkour tasks, such as jumping over single horizontal or vertical discontinuities. Experiment videos can be found at https://yxyang.github.io/jumping\\_cod/.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "ae280e28c063dda1",
            "data": {
                "categories": [
                    "#robotics",
                    "#rl"
                ],
                "emoji": "ğŸ¦¿",
                "ru": {
                    "title": "ĞŸÑ€Ñ‹Ğ¶ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: Ğ¾Ñ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ñ‹Ğ¶ĞºĞ¾Ğ² Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ğ¾Ğ½Ğ¾Ğ³Ğ¸Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ»ÑŒĞµÑ„Ğ° Ğ¼ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ĞµĞ¹ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ Unitree Go1 Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ñ‹Ğ¶ĞºĞ¸ Ğ¿Ğ¾ Ğ»ĞµÑÑ‚Ğ½Ğ¸Ñ†Ğµ Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ğ¼ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ĞºÑƒÑ€-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Agile Jumping: Quadrupedal Robots Conquering Complex Terrains!",
                    "desc": "This paper presents a novel approach for enabling quadrupedal robots to perform agile and continuous jumps over complex terrains like stairs and stepping stones. The authors introduce a hierarchical learning and control framework that includes a heightmap predictor for terrain perception, a reinforcement learning policy for adaptive motion planning, and a model-based leg controller for precise movement execution. By effectively bridging the simulation-to-reality gap, the framework allows the Unitree Go1 robot to achieve impressive jumping capabilities, such as crossing multiple stair steps in a single jump. The results demonstrate significant improvements over existing methods in various parkour tasks, showcasing the potential for advanced robotic agility in challenging environments."
                },
                "zh": {
                    "title": "å››è¶³æœºå™¨äººï¼šæ•æ·è·³è·ƒçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å››è¶³æœºå™¨äººåœ¨ä¸è¿ç»­åœ°å½¢ï¼ˆå¦‚æ¥¼æ¢¯å’Œè·³çŸ³ï¼‰ä¸Šçš„æ•æ·ã€è¿ç»­å’Œé€‚åº”æ€§è·³è·ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†å±‚å­¦ä¹ å’Œæ§åˆ¶æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸€ä¸ªç”¨äºç¨³å¥åœ°å½¢æ„ŸçŸ¥çš„é«˜åº¦å›¾é¢„æµ‹å™¨ã€åŸºäºå¼ºåŒ–å­¦ä¹ çš„è´¨å¿ƒçº§è¿åŠ¨ç­–ç•¥ä»¥åŠä¸€ä¸ªä½çº§æ¨¡å‹é©±åŠ¨çš„è…¿éƒ¨æ§åˆ¶å™¨ã€‚é€šè¿‡å‡†ç¡®å»ºæ¨¡ç¡¬ä»¶ç‰¹æ€§ï¼Œæˆ‘ä»¬å‡å°‘äº†æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä½¿Unitree Go1æœºå™¨äººé¦–æ¬¡èƒ½å¤Ÿåœ¨æ¥¼æ¢¯å’Œè·³çŸ³ä¸Šè¿›è¡Œæ•æ·çš„è¿ç»­è·³è·ƒï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šç§éšœç¢ç‰©è·³è·ƒä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11211",
            "title": "SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction",
            "url": "https://huggingface.co/papers/2409.11211",
            "abstract": "Digitizing 3D static scenes and 4D dynamic events from multi-view images has long been a challenge in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a practical and scalable reconstruction method, gaining popularity due to its impressive reconstruction quality, real-time rendering capabilities, and compatibility with widely used visualization tools. However, the method requires a substantial number of input views to achieve high-quality scene reconstruction, introducing a significant practical bottleneck. This challenge is especially severe in capturing dynamic scenes, where deploying an extensive camera array can be prohibitively costly. In this work, we identify the lack of spatial autocorrelation of splat features as one of the factors contributing to the suboptimal performance of the 3DGS technique in sparse reconstruction settings. To address the issue, we propose an optimization strategy that effectively regularizes splat features by modeling them as the outputs of a corresponding implicit neural field. This results in a consistent enhancement of reconstruction quality across various scenarios. Our approach effectively handles static and dynamic cases, as demonstrated by extensive testing across different setups and scene complexities.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "cacfe3e60ddde42f",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "ğŸŒŸ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° 3D Gaussian Splatting (3DGS) Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ÑĞ¿Ğ»Ğ°Ñ‚Ğ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… ĞºĞ°Ğº Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing 3D Reconstruction with Implicit Neural Fields",
                    "desc": "This paper addresses the challenges of reconstructing 3D static scenes and 4D dynamic events from multiple images. It highlights the limitations of the 3D Gaussian Splatting (3DGS) method, particularly its need for many input views to produce high-quality results. The authors propose a new optimization strategy that improves the performance of 3DGS by regularizing splat features through an implicit neural field model. Their approach shows significant improvements in reconstruction quality for both static and dynamic scenes, making it more effective in various scenarios."
                },
                "zh": {
                    "title": "æå‡3Dé‡å»ºè´¨é‡çš„æ–°ç­–ç•¥",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä»å¤šè§†è§’å›¾åƒä¸­æ•°å­—åŒ–ä¸‰ç»´é™æ€åœºæ™¯å’Œå››ç»´åŠ¨æ€äº‹ä»¶çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡å°†3Dé«˜æ–¯ç‚¹ç‰¹å¾å»ºæ¨¡ä¸ºéšå¼ç¥ç»åœºçš„è¾“å‡ºï¼Œæ¥æ”¹å–„ç¨€ç–é‡å»ºä¸­çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°æ­£åˆ™åŒ–äº†ç‰¹å¾ï¼Œæå‡äº†é‡å»ºè´¨é‡ï¼Œé€‚ç”¨äºé™æ€å’ŒåŠ¨æ€åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒè®¾ç½®å’Œåœºæ™¯å¤æ‚æ€§ä¸‹å‡è¡¨ç°å‡ºä¸€è‡´çš„è´¨é‡æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11733",
            "title": "Human-like Affective Cognition in Foundation Models",
            "url": "https://huggingface.co/papers/2409.11733",
            "abstract": "Understanding emotions is fundamental to human interaction and experience. Humans easily infer emotions from situations or facial expressions, situations from emotions, and do a variety of other affective cognition. How adept is modern AI at these inferences? We introduce an evaluation framework for testing affective cognition in foundation models. Starting from psychological theory, we generate 1,280 diverse scenarios exploring relationships between appraisals, emotions, expressions, and outcomes. We evaluate the abilities of foundation models (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefully selected conditions. Our results show foundation models tend to agree with human intuitions, matching or exceeding interparticipant agreement. In some conditions, models are ``superhuman'' -- they better predict modal human judgements than the average human. All models benefit from chain-of-thought reasoning. This suggests foundation models have acquired a human-like understanding of emotions and their influence on beliefs and behavior.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2024-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "c0d82f8c3405bbb7",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ 1280 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞ¾Ñ€Ğ¸ÑÑ…, Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4, Claude-3 Ğ¸ Gemini-1.5-Pro, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼, Ğ° Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ ĞµĞ³Ğ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ˜Ğ˜ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµĞ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ° ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "AI's Emotional Intelligence: Surpassing Human Intuition",
                    "desc": "This paper explores how well modern AI can understand and infer emotions, which is crucial for human interactions. The authors create a framework to evaluate the affective cognition of foundation models like GPT-4 and Claude-3 by generating 1,280 scenarios based on psychological theories. The study finds that these models often align with human emotional intuitions and, in some cases, outperform average human judgments. The results indicate that foundation models utilize chain-of-thought reasoning, suggesting they have developed a sophisticated understanding of emotions and their impact on human beliefs and behaviors."
                },
                "zh": {
                    "title": "åŸºç¡€æ¨¡å‹çš„æƒ…æ„Ÿç†è§£èƒ½åŠ›",
                    "desc": "ç†è§£æƒ…æ„Ÿå¯¹äººç±»äº’åŠ¨å’Œä½“éªŒè‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæµ‹è¯•åŸºç¡€æ¨¡å‹åœ¨æƒ…æ„Ÿè®¤çŸ¥æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ç”Ÿæˆäº†1280ä¸ªå¤šæ ·åŒ–çš„åœºæ™¯ï¼Œæ¢è®¨è¯„ä¼°ã€æƒ…æ„Ÿã€è¡¨æƒ…å’Œç»“æœä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºç¡€æ¨¡å‹åœ¨æŸäº›æ¡ä»¶ä¸‹çš„è¡¨ç°è¶…è¶Šäº†æ™®é€šäººç±»ï¼Œæ˜¾ç¤ºå‡ºå®ƒä»¬åœ¨æƒ…æ„Ÿç†è§£æ–¹é¢å·²æ¥è¿‘äººç±»æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11242",
            "title": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse",
            "url": "https://huggingface.co/papers/2409.11242",
            "abstract": "LLMs are an integral part of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the quality of end-to-end RAG systems, there is a lack of research on understanding the appropriateness of an LLM for the RAG task. Thus, we introduce a new metric, Trust-Score, that provides a holistic evaluation of the trustworthiness of LLMs in an RAG framework. We show that various prompting methods, such as in-context learning, fail to adapt LLMs effectively to the RAG task. Thus, we propose Trust-Align, a framework to align LLMs for higher Trust-Score. LLaMA-3-8b, aligned with our method, significantly outperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up 29.2) and ELI5 (up 14.9). We release our code at: https://github.com/declare-lab/trust-align.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "b54be5fdb3e2ac7d",
            "data": {
                "categories": [
                    "#rag",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… RAG",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Trust-Score Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ¯Ğœ) Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¯Ğœ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ RAG. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Trust-Align Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¯Ğœ Ğº Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaMA-3-8b Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Trust-Align."
                },
                "en": {
                    "title": "Enhancing LLM Trustworthiness in RAG Systems",
                    "desc": "This paper addresses the role of large language models (LLMs) in retrieval-augmented generation (RAG) systems, highlighting a gap in understanding how well these models perform in this context. The authors introduce a new evaluation metric called Trust-Score, which assesses the reliability of LLMs when integrated into RAG frameworks. They find that common prompting techniques, like in-context learning, do not effectively prepare LLMs for RAG tasks. To improve performance, they propose Trust-Align, a method that aligns LLMs to achieve higher Trust-Scores, demonstrating significant improvements in benchmark tasks with their aligned model, LLaMA-3-8b."
                },
                "zh": {
                    "title": "æå‡LLMsåœ¨RAGç³»ç»Ÿä¸­çš„ä¿¡ä»»åº¦",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç§°ä¸ºä¿¡ä»»åˆ†æ•°ï¼ˆTrust-Scoreï¼‰ï¼Œç”¨äºå…¨é¢è¯„ä¼°LLMsåœ¨RAGæ¡†æ¶ä¸­çš„å¯ä¿¡åº¦ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„æç¤ºæ–¹æ³•ï¼ˆå¦‚ä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰æœªèƒ½æœ‰æ•ˆè°ƒæ•´LLMsä»¥é€‚åº”RAGä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†Trust-Alignæ¡†æ¶ï¼Œä»¥æé«˜LLMsçš„ä¿¡ä»»åˆ†æ•°ï¼Œå¹¶å±•ç¤ºäº†ä¸æˆ‘ä»¬æ–¹æ³•å¯¹é½çš„LLaMA-3-8båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºåŒç±»å¼€æºLLMsã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.09323",
            "title": "Implicit Neural Representations with Fourier Kolmogorov-Arnold Networks",
            "url": "https://huggingface.co/papers/2409.09323",
            "abstract": "Implicit neural representations (INRs) use neural networks to provide continuous and resolution-independent representations of complex signals with a small number of parameters. However, existing INR models often fail to capture important frequency components specific to each task. To address this issue, in this paper, we propose a Fourier Kolmogorov Arnold network (FKAN) for INRs. The proposed FKAN utilizes learnable activation functions modeled as Fourier series in the first layer to effectively control and learn the task-specific frequency components. In addition, the activation functions with learnable Fourier coefficients improve the ability of the network to capture complex patterns and details, which is beneficial for high-resolution and high-dimensional data. Experimental results show that our proposed FKAN model outperforms three state-of-the-art baseline schemes, and improves the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) for the image representation task and intersection over union (IoU) for the 3D occupancy volume representation task, respectively.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2024-09-14",
            "pub_date_card": {
                "ru": "14 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 14",
                "zh": "9æœˆ14æ—¥"
            },
            "hash": "44a56990a6b294f3",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#3d"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "FKAN: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ğ¤ÑƒÑ€ÑŒĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ñ‹Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ (INR) - ÑĞµÑ‚ÑŒ Ğ¤ÑƒÑ€ÑŒĞµ-ĞšĞ¾Ğ»Ğ¼Ğ¾Ğ³Ğ¾Ñ€Ğ¾Ğ²Ğ°-ĞÑ€Ğ½Ğ¾Ğ»ÑŒĞ´Ğ° (FKAN). FKAN Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ĞºĞ°Ğº Ñ€ÑĞ´Ñ‹ Ğ¤ÑƒÑ€ÑŒĞµ Ğ² Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑĞ»Ğ¾Ğµ, Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞµÑ‚Ğ¸ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ FKAN Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑÑ…ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Implicit Neural Representations with Fourier Kolmogorov Arnold Networks",
                    "desc": "This paper introduces the Fourier Kolmogorov Arnold network (FKAN), a novel approach to implicit neural representations (INRs) that enhances the ability to capture task-specific frequency components. By employing learnable activation functions modeled as Fourier series, FKAN effectively controls the frequency characteristics needed for different tasks. This method allows the network to better learn complex patterns and details, making it particularly useful for high-resolution and high-dimensional data. Experimental results demonstrate that FKAN significantly outperforms existing models in terms of image representation and 3D occupancy volume tasks, as indicated by improved PSNR, SSIM, and IoU metrics."
                },
                "zh": {
                    "title": "å‚…é‡Œå¶ç½‘ç»œï¼šæå‡éšå¼ç¥ç»è¡¨ç¤ºçš„é¢‘ç‡æ•æ‰èƒ½åŠ›",
                    "desc": "éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRsï¼‰ä½¿ç”¨ç¥ç»ç½‘ç»œä»¥å°‘é‡å‚æ•°æä¾›è¿ç»­ä¸”ä¸åˆ†è¾¨ç‡æ— å…³çš„å¤æ‚ä¿¡å·è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„INRæ¨¡å‹å¾€å¾€æ— æ³•æ•æ‰åˆ°ç‰¹å®šä»»åŠ¡çš„é‡è¦é¢‘ç‡æˆåˆ†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å‚…é‡Œå¶ç§‘å°”è«æˆˆç½—å¤«é˜¿è¯ºå¾·ç½‘ç»œï¼ˆFKANï¼‰ã€‚è¯¥ç½‘ç»œé€šè¿‡åœ¨ç¬¬ä¸€å±‚ä½¿ç”¨å¯å­¦ä¹ çš„å‚…é‡Œå¶çº§æ•°æ¿€æ´»å‡½æ•°ï¼Œæœ‰æ•ˆåœ°æ§åˆ¶å’Œå­¦ä¹ ä»»åŠ¡ç‰¹å®šçš„é¢‘ç‡æˆåˆ†ï¼Œä»è€Œæé«˜äº†ç½‘ç»œæ•æ‰å¤æ‚æ¨¡å¼å’Œç»†èŠ‚çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.10836",
            "title": "Single-Layer Learnable Activation for Implicit Neural Representation (SL$^{2}$A-INR)",
            "url": "https://huggingface.co/papers/2409.10836",
            "abstract": "Implicit Neural Representation (INR), leveraging a neural network to transform coordinate input into corresponding attributes, has recently driven significant advances in several vision-related domains. However, the performance of INR is heavily influenced by the choice of the nonlinear activation function used in its multilayer perceptron (MLP) architecture. Multiple nonlinearities have been investigated; yet, current INRs face limitations in capturing high-frequency components, diverse signal types, and handling inverse problems. We have identified that these problems can be greatly alleviated by introducing a paradigm shift in INRs. We find that an architecture with learnable activations in initial layers can represent fine details in the underlying signals. Specifically, we propose SL^{2}A-INR, a hybrid network for INR with a single-layer learnable activation function, prompting the effectiveness of traditional ReLU-based MLPs. Our method performs superior across diverse tasks, including image representation, 3D shape reconstructions, inpainting, single image super-resolution, CT reconstruction, and novel view synthesis. Through comprehensive experiments, SL^{2}A-INR sets new benchmarks in accuracy, quality, and convergence rates for INR.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "84d7067757cce461",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#3d",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¼Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ (INR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ SL^{2}A-INR, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ğ¼ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ReLU. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ, Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³ Ğ¸ ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ SL^{2}A-INR ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ INR."
                },
                "en": {
                    "title": "Revolutionizing Implicit Neural Representation with Learnable Activations",
                    "desc": "This paper introduces a new approach to Implicit Neural Representation (INR) by proposing a hybrid network called SL^{2}A-INR. The key innovation is the use of learnable activation functions in the initial layers of the multilayer perceptron (MLP), which enhances the network's ability to capture fine details in various signals. The authors demonstrate that this architecture significantly improves performance in tasks such as image representation, 3D shape reconstruction, and super-resolution. Overall, SL^{2}A-INR achieves state-of-the-art results in accuracy and convergence for INR applications."
                },
                "zh": {
                    "title": "å¼•å…¥å¯å­¦ä¹ æ¿€æ´»å‡½æ•°ï¼Œæå‡éšå¼ç¥ç»è¡¨ç¤ºçš„æ€§èƒ½",
                    "desc": "éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰åˆ©ç”¨ç¥ç»ç½‘ç»œå°†åæ ‡è¾“å…¥è½¬æ¢ä¸ºç›¸åº”çš„å±æ€§ï¼Œæœ€è¿‘åœ¨å¤šä¸ªè§†è§‰ç›¸å…³é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒINRçš„æ€§èƒ½å—åˆ°å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ¶æ„ä¸­éçº¿æ€§æ¿€æ´»å‡½æ•°é€‰æ‹©çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡åœ¨INRä¸­å¼•å…¥å¯å­¦ä¹ çš„æ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥æœ‰æ•ˆæ•æ‰ä¿¡å·ä¸­çš„ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯æˆ‘ä»¬æå‡ºçš„SL^{2}A-INRç½‘ç»œåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬å›¾åƒè¡¨ç¤ºã€3Då½¢çŠ¶é‡å»ºå’Œå•å›¾åƒè¶…åˆ†è¾¨ç‡ç­‰ã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼ŒSL^{2}A-INRåœ¨å‡†ç¡®æ€§ã€è´¨é‡å’Œæ”¶æ•›é€Ÿåº¦ä¸Šè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.10831",
            "title": "PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing",
            "url": "https://huggingface.co/papers/2409.10831",
            "abstract": "The recent explosion of generative AI-Music systems has raised numerous concerns over data copyright, licensing music from musicians, and the conflict between open-source AI and large prestige companies. Such issues highlight the need for publicly available, copyright-free musical data, in which there is a large shortage, particularly for symbolic music data. To alleviate this issue, we present PDMX: a large-scale open-source dataset of over 250K public domain MusicXML scores collected from the score-sharing forum MuseScore, making it the largest available copyright-free symbolic music dataset to our knowledge. PDMX additionally includes a wealth of both tag and user interaction metadata, allowing us to efficiently analyze the dataset and filter for high quality user-generated scores. Given the additional metadata afforded by our data collection process, we conduct multitrack music generation experiments evaluating how different representative subsets of PDMX lead to different behaviors in downstream models, and how user-rating statistics can be used as an effective measure of data quality. Examples can be found at https://pnlong.github.io/PDMX.demo/.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "d51dd5272b8dfc7e",
            "data": {
                "categories": [
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ¼",
                "ru": {
                    "title": "PDMX: Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ² Ğ¼ÑƒĞ·Ñ‹ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PDMX - ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 250 Ñ‚Ñ‹ÑÑÑ‡ Ğ½Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ñ‚ÑƒÑ€, Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ…ÑÑ Ğ² Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ² Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. PDMX Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ñ‚ĞµĞ³Ğ°Ñ… Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ñ‚ÑƒÑ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾Ñ€Ğ¾Ğ¶ĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Music Creativity with PDMX: A Treasure Trove of Copyright-Free Scores",
                    "desc": "This paper introduces PDMX, a large-scale open-source dataset containing over 250,000 public domain MusicXML scores, addressing the shortage of copyright-free symbolic music data. The dataset is collected from the MuseScore platform and includes valuable metadata such as tags and user interactions, which enhances the analysis and quality filtering of the scores. The authors conduct experiments on multitrack music generation to explore how different subsets of PDMX affect the performance of machine learning models. Additionally, they demonstrate that user-rating statistics can serve as a reliable indicator of data quality in music generation tasks."
                },
                "zh": {
                    "title": "PDMXï¼šå¼€æºéŸ³ä¹æ•°æ®é›†ï¼ŒåŠ©åŠ›ç‰ˆæƒé—®é¢˜è§£å†³",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†PDMXï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å‹å¼€æºæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡25ä¸‡ä»½å…¬å…±é¢†åŸŸçš„MusicXMLä¹è°±ï¼Œæ—¨åœ¨è§£å†³ç¬¦å·éŸ³ä¹æ•°æ®çš„ç‰ˆæƒé—®é¢˜ã€‚è¯¥æ•°æ®é›†æ¥æºäºä¹è°±åˆ†äº«è®ºå›MuseScoreï¼Œæ˜¯ç›®å‰å·²çŸ¥çš„æœ€å¤§ç‰ˆæƒå…è´¹ç¬¦å·éŸ³ä¹æ•°æ®é›†ã€‚PDMXè¿˜åŒ…å«ä¸°å¯Œçš„æ ‡ç­¾å’Œç”¨æˆ·äº¤äº’å…ƒæ•°æ®ï¼Œä¾¿äºé«˜æ•ˆåˆ†æå’Œç­›é€‰é«˜è´¨é‡çš„ç”¨æˆ·ç”Ÿæˆä¹è°±ã€‚é€šè¿‡å¯¹ä¸åŒå­é›†çš„å¤šè½¨éŸ³ä¹ç”Ÿæˆå®éªŒï¼Œæœ¬æ–‡æ¢è®¨äº†ç”¨æˆ·è¯„åˆ†ç»Ÿè®¡å¦‚ä½•ä½œä¸ºæ•°æ®è´¨é‡çš„æœ‰æ•ˆè¡¡é‡æ ‡å‡†ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-09-17.html",
    "link_next": "2024-09-19.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "17.09",
        "en": "09/17",
        "zh": "9æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "19.09",
        "en": "09/19",
        "zh": "9æœˆ19æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 8,
        "#agents": 2,
        "#cv": 5,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 3,
        "#plp": 0,
        "#inference": 2,
        "#3d": 4,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#medicine": 1,
        "#training": 5,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0
    }
}