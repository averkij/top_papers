{
    "date": {
        "ru": "25 Ğ¸ÑĞ»Ñ",
        "en": "July 25",
        "zh": "7æœˆ25æ—¥"
    },
    "time_utc": "2025-07-25 03:57",
    "weekday": 4,
    "issue_id": 5006,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.15758",
            "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy\n  Optimization",
            "url": "https://huggingface.co/papers/2507.15758",
            "abstract": "Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\% while improving accuracy by 2.3\\%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.",
            "score": 12,
            "issue_id": 5006,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "ce0e5d782e94f5ee",
            "authors": [
                "Xingyu Wu",
                "Yuchen Yan",
                "Shangke Lyu",
                "Linjuan Wu",
                "Yiwen Qiu",
                "Yongliang Shen",
                "Weiming Lu",
                "Jian Shao",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15758.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#math",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑĞ»Ğ¾Ğ², Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑĞ¼Ñ‹ÑĞ»Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LAPO (Length-Adaptive Policy Optimization). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. LAPO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 40.9%, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 2.3% Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LAPO, Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Smart Reasoning: Less is More with LAPO",
                    "desc": "This paper introduces Length-Adaptive Policy Optimization (LAPO), a new method for improving reasoning models in machine learning. LAPO allows models to learn how much reasoning is needed for different problems, rather than setting strict limits on reasoning length. It uses a two-stage reinforcement learning process where models first learn successful reasoning patterns and then apply this knowledge during inference. The results show that LAPO can significantly reduce the number of tokens used while also increasing the accuracy of the models."
                },
                "zh": {
                    "title": "æ™ºèƒ½æ¨ç†ï¼Œçµæ´»æ§åˆ¶ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºé•¿åº¦è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ˆLAPOï¼‰ï¼Œæ—¨åœ¨æ”¹å–„æ¨ç†æ¨¡å‹çš„æ•ˆç‡ã€‚LAPOé€šè¿‡å¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå†…åœ¨ç†è§£é€‚å½“çš„æ¨ç†æ·±åº¦ï¼Œä»è€Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ æˆåŠŸè§£å†³æ–¹æ¡ˆé•¿åº¦çš„ç»Ÿè®¡åˆ†å¸ƒï¼›ç¬¬äºŒé˜¶æ®µï¼Œæ¨¡å‹å°†è¿™äº›æ¨¡å¼ä½œä¸ºå…ƒè®¤çŸ¥æŒ‡å¯¼ï¼ŒåµŒå…¥æ¨ç†ä¸Šä¸‹æ–‡ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLAPOåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡å°‘äº†å¤šè¾¾40.9%çš„æ ‡è®°ä½¿ç”¨ï¼ŒåŒæ—¶æé«˜äº†2.3%çš„å‡†ç¡®ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.18537",
            "title": "TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive\n  Generation",
            "url": "https://huggingface.co/papers/2507.18537",
            "abstract": "TTS-VAR, a test-time scaling framework for visual auto-regressive models, improves generation quality by dynamically adjusting batch sizes and using clustering and resampling techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses. Alternatively, test-time scaling has garnered growing attention due to resource efficiency and promising performance. In this work, we present TTS-VAR, the first general test-time scaling framework for visual auto-regressive (VAR) models, modeling the generation process as a path searching problem. To dynamically balance computational efficiency with exploration capacity, we first introduce an adaptive descending batch size schedule throughout the causal generation process. Besides, inspired by VAR's hierarchical coarse-to-fine multi-scale generation, our framework integrates two key components: (i) At coarse scales, we observe that generated tokens are hard for evaluation, possibly leading to erroneous acceptance of inferior samples or rejection of superior samples. Noticing that the coarse scales contain sufficient structural information, we propose clustering-based diversity search. It preserves structural variety through semantic feature clustering, enabling later selection on samples with higher potential. (ii) In fine scales, resampling-based potential selection prioritizes promising candidates using potential scores, which are defined as reward functions incorporating multi-scale generation history. Experiments on the powerful VAR model Infinity show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights reveal that early-stage structural features effectively influence final quality, and resampling efficacy varies across generation scales. Code is available at https://github.com/ali-vilab/TTS-VAR.",
            "score": 7,
            "issue_id": 5005,
            "pub_date": "2025-07-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ»Ñ",
                "en": "July 24",
                "zh": "7æœˆ24æ—¥"
            },
            "hash": "acf3b32f27e7342b",
            "authors": [
                "Zhekai Chen",
                "Ruihang Chu",
                "Yukang Chen",
                "Shiwei Zhang",
                "Yujie Wei",
                "Yingya Zhang",
                "Xihui Liu"
            ],
            "affiliations": [
                "CUHK",
                "HKU MMLab",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.18537.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "TTS-VAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ğ±Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ°. TTS-VAR Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿ÑƒÑ‚Ğ¸, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ GenEval Ğ½Ğ° 8.7% Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Infinity."
                },
                "en": {
                    "title": "Dynamic Scaling for Enhanced Visual Generation Quality",
                    "desc": "TTS-VAR is a novel framework designed to enhance the quality of visual auto-regressive (VAR) models during the generation process. It introduces a dynamic batch size adjustment strategy that balances computational efficiency with the ability to explore diverse outputs. The framework employs clustering techniques at coarse scales to maintain structural diversity and resampling methods at fine scales to prioritize high-potential candidates based on their generation history. Experiments demonstrate a significant improvement in generation quality, highlighting the importance of early-stage features in the overall output."
                },
                "zh": {
                    "title": "åŠ¨æ€è°ƒæ•´ï¼Œæå‡ç”Ÿæˆè´¨é‡çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "TTS-VARæ˜¯ä¸€ä¸ªç”¨äºè§†è§‰è‡ªå›å½’æ¨¡å‹çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å°å’Œä½¿ç”¨èšç±»ä¸é‡é‡‡æ ·æŠ€æœ¯æ¥æé«˜ç”Ÿæˆè´¨é‡ã€‚è¯¥æ¡†æ¶å°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºè·¯å¾„æœç´¢é—®é¢˜ï¼Œæ—¨åœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡ä¸æ¢ç´¢èƒ½åŠ›ã€‚å®ƒåœ¨ç²—å°ºåº¦ä¸Šé‡‡ç”¨åŸºäºèšç±»çš„å¤šæ ·æ€§æœç´¢ï¼Œä»¥ä¿ç•™ç»“æ„å¤šæ ·æ€§ï¼Œå¹¶åœ¨ç»†å°ºåº¦ä¸Šé€šè¿‡é‡é‡‡æ ·ä¼˜å…ˆé€‰æ‹©æ½œåœ¨çš„ä¼˜è´¨æ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTTS-VARåœ¨å¼ºå¤§çš„VARæ¨¡å‹Infinityä¸Šå®ç°äº†8.7%çš„GenEvalåˆ†æ•°æå‡ï¼Œæ˜¾ç¤ºå‡ºæ—©æœŸç»“æ„ç‰¹å¾å¯¹æœ€ç»ˆè´¨é‡çš„æœ‰æ•ˆå½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15844",
            "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning",
            "url": "https://huggingface.co/papers/2507.15844",
            "abstract": "Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. HBPO addresses the fundamental challenge of exploration space collapse in efficiency-oriented training, where penalties on long output length systematically bias models away from necessary long reasoning paths. Through hierarchical budget exploration, our approach partitions rollout samples into multiple subgroups with distinct token budgets, aiming to enable efficient resource allocation while preventing degradation of capability. We introduce differentiated reward mechanisms that create budget-aware incentives aligned with the complexity of the problem, allowing models to discover natural correspondences between task requirements and computational effort. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Unlike existing methods that impose external constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity.",
            "score": 3,
            "issue_id": 5006,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "c5c45ecb7b33f3cb",
            "authors": [
                "Shangke Lyu",
                "Linjuan Wu",
                "Yuchen Yan",
                "Xingyu Wu",
                "Hao Li",
                "Yongliang Shen",
                "Peisheng Jiang",
                "Weiming Lu",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "SF Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15844.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Hierarchical Budget Policy Optimization (HBPO) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. HBPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HBPO ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 60.6% Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 3.14% Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Optimizing Reasoning Efficiency with Adaptive Depth",
                    "desc": "This paper introduces Hierarchical Budget Policy Optimization (HBPO), a new reinforcement learning framework designed to improve the efficiency of reasoning models. HBPO allows models to adapt their reasoning depth based on the complexity of the problem, rather than using a one-size-fits-all approach. By implementing hierarchical budget exploration and differentiated reward mechanisms, the framework encourages models to allocate resources effectively while maintaining their performance. The results show that HBPO can significantly reduce token usage and enhance accuracy, demonstrating that efficiency and capability can be optimized together."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ï¼Œèƒ½åŠ›ä¸æ•ˆç‡å¹¶å­˜",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå±‚æ¬¡é¢„ç®—ç­–ç•¥ä¼˜åŒ–ï¼ˆHBPOï¼‰çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ¨ç†æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ã€‚HBPOé€šè¿‡åˆ†å±‚é¢„ç®—æ¢ç´¢ï¼Œå°†æ ·æœ¬åˆ†æˆå¤šä¸ªå…·æœ‰ä¸åŒä»¤ç‰Œé¢„ç®—çš„å­ç»„ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„èµ„æºåˆ†é…ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å·®å¼‚åŒ–å¥–åŠ±æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®é—®é¢˜å¤æ‚æ€§è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œé¿å…äº†é•¿è¾“å‡ºé•¿åº¦çš„æƒ©ç½šå¯¹æ¨¡å‹çš„è´Ÿé¢å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHBPOåœ¨å››ä¸ªæ¨ç†åŸºå‡†ä¸Šå¹³å‡å‡å°‘äº†60.6%çš„ä»¤ç‰Œä½¿ç”¨ï¼ŒåŒæ—¶æé«˜äº†3.14%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†æ¨ç†æ•ˆç‡ä¸èƒ½åŠ›å¯ä»¥åŒæ—¶ä¼˜åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.16535",
            "title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent\n  Diffusion",
            "url": "https://huggingface.co/papers/2507.16535",
            "abstract": "Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m x 600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, a tailored framework for large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely large-scale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D. Our project page is available at https://whiteinblue.github.io/earthcrafter/",
            "score": 2,
            "issue_id": 5005,
            "pub_date": "2025-07-22",
            "pub_date_card": {
                "ru": "22 Ğ¸ÑĞ»Ñ",
                "en": "July 22",
                "zh": "7æœˆ22æ—¥"
            },
            "hash": "7ee137161bbe047e",
            "authors": [
                "Shang Liu",
                "Chenjie Cao",
                "Chaohui Yu",
                "Wen Qian",
                "Jing Wang",
                "Fan Wang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Fudan University",
                "Hupan Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.16535.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#architecture",
                    "#synthetic",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "EarthCrafter: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ—ĞµĞ¼Ğ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Aerial-Earth3D - ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 3D Ğ°ÑÑ€Ğ¾ÑÑŠĞµĞ¼ĞºĞ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 50 Ñ‚Ñ‹ÑÑÑ‡ ÑÑ†ĞµĞ½ Ğ¿Ğ¾ Ğ²ÑĞµĞ¹ Ñ‚ĞµÑ€Ñ€Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¡Ğ¨Ğ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº EarthCrafter Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° EarthCrafter Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Revolutionizing Large-Scale 3D Earth Generation",
                    "desc": "This paper presents a solution to the challenge of generating large-scale 3D models of Earth's surface by introducing a new dataset and a novel model architecture. The Aerial-Earth3D dataset is the largest of its kind, containing 50,000 scenes with detailed annotations that support diverse terrain representation. The EarthCrafter framework utilizes a dual approach with sparse-decoupled latent diffusion, allowing for efficient generation of 3D structures and textures while managing computational costs. The results show significant improvements in generating realistic and plausible large-scale 3D environments, with applications in urban planning and terrain synthesis."
                },
                "zh": {
                    "title": "å¤§è§„æ¨¡3Dåœ°çƒç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "å°½ç®¡æœ€è¿‘çš„3Dç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°å¤§è§„æ¨¡åœ°ç†èŒƒå›´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡æ•°æ®åŸºç¡€è®¾æ–½å’Œæ¨¡å‹æ¶æ„çš„åŒé‡åˆ›æ–°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬ä»‹ç»äº†Aerial-Earth3Dï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„3Dèˆªç©ºæ•°æ®é›†ï¼ŒåŒ…å«50,000ä¸ªåœºæ™¯ï¼Œæ”¯æŒå¤§è§„æ¨¡çš„3Dåœ°çƒç”Ÿæˆã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†EarthCrafteræ¡†æ¶ï¼Œé€šè¿‡ç¨€ç–è§£è€¦çš„æ½œåœ¨æ‰©æ•£æŠ€æœ¯ï¼Œå®ç°äº†é«˜æ•ˆçš„åœ°å½¢ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.18634",
            "title": "Captain Cinema: Towards Short Movie Generation",
            "url": "https://huggingface.co/papers/2507.18634",
            "abstract": "Captain Cinema generates high-quality short movies from textual descriptions using top-down keyframe planning and bottom-up video synthesis with interleaved training of Multimodal Diffusion Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: https://thecinema.ai",
            "score": 1,
            "issue_id": 5006,
            "pub_date": "2025-07-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ»Ñ",
                "en": "July 24",
                "zh": "7æœˆ24æ—¥"
            },
            "hash": "a52dc2cc9f063733",
            "authors": [
                "Junfei Xiao",
                "Ceyuan Yang",
                "Lvmin Zhang",
                "Shengqu Cai",
                "Yang Zhao",
                "Yuwei Guo",
                "Gordon Wetzstein",
                "Maneesh Agrawala",
                "Alan Yuille",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "CUHK Project Lead",
                "Johns Hopkins University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.18634.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#long_context",
                    "#story_generation",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº ĞºĞ¸Ğ½Ğ¾: Ğ˜Ğ˜-Ñ€ĞµĞ¶Ğ¸ÑÑĞµÑ€ Captain Cinema",
                    "desc": "Captain Cinema - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² ÑĞ²ĞµÑ€Ñ…Ñƒ Ğ²Ğ½Ğ¸Ğ·, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ½Ğ¸Ğ·Ñƒ Ğ²Ğ²ĞµÑ€Ñ…. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Transforming Text to Cinematic Reality with Captain Cinema",
                    "desc": "Captain Cinema is a framework designed to create short movies from text descriptions. It uses a two-step process: first, it generates keyframes that outline the movie's storyline, ensuring that the visuals and narrative are coherent. Next, it synthesizes video by connecting these keyframes, allowing for smooth transitions and dynamic scenes. The model employs Multimodal Diffusion Transformers with a unique training strategy to enhance the quality and efficiency of the movie generation process."
                },
                "zh": {
                    "title": "ç”¨æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡çŸ­ç‰‡çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "Captain Cinema æ˜¯ä¸€ä¸ªçŸ­ç‰‡ç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡çš„çŸ­ç”µå½±ã€‚é¦–å…ˆï¼Œå®ƒé€šè¿‡è‡ªä¸Šè€Œä¸‹çš„å…³é”®å¸§è§„åˆ’ç”Ÿæˆä¸€ç³»åˆ—å…³é”®å¸§ï¼Œä»¥ç¡®ä¿æ•…äº‹æƒ…èŠ‚å’Œè§†è§‰å¤–è§‚çš„ä¸€è‡´æ€§ã€‚æ¥ç€ï¼Œè¿™äº›å…³é”®å¸§ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œä¾›è§†é¢‘åˆæˆæ¨¡å‹ä½¿ç”¨ï¼Œä»è€Œç”Ÿæˆåœºæ™¯ä¹‹é—´çš„æ—¶ç©ºåŠ¨æ€ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§äº¤é”™è®­ç»ƒç­–ç•¥ï¼Œä¸“é—¨ä¸ºé•¿ä¸Šä¸‹æ–‡è§†é¢‘æ•°æ®è®¾è®¡çš„å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ï¼Œä»¥æ”¯æŒå¤šåœºæ™¯é•¿å™äº‹ç”µå½±çš„ç¨³å®šé«˜æ•ˆç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.18192",
            "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance",
            "url": "https://huggingface.co/papers/2507.18192",
            "abstract": "TeEFusion enhances text-to-image synthesis by efficiently incorporating classifier-free guidance into text embeddings, reducing inference costs without sacrificing image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-image synthesis largely benefit from sophisticated sampling strategies and classifier-free guidance (CFG) to ensure high-quality generation. However, CFG's reliance on two forward passes, especially when combined with intricate sampling algorithms, results in prohibitively high inference costs. To address this, we introduce TeEFusion (Text Embeddings Fusion), a novel and efficient distillation method that directly incorporates the guidance magnitude into the text embeddings and distills the teacher model's complex sampling strategy. By simply fusing conditional and unconditional text embeddings using linear operations, TeEFusion reconstructs the desired guidance without adding extra parameters, simultaneously enabling the student model to learn from the teacher's output produced via its sophisticated sampling approach. Extensive experiments on state-of-the-art models such as SD3 demonstrate that our method allows the student to closely mimic the teacher's performance with a far simpler and more efficient sampling strategy. Consequently, the student model achieves inference speeds up to 6times faster than the teacher model, while maintaining image quality at levels comparable to those obtained through the teacher's complex sampling approach. The code is publicly available at https://github.com/AIDC-AI/TeEFusion{github.com/AIDC-AI/TeEFusion}.",
            "score": 1,
            "issue_id": 5006,
            "pub_date": "2025-07-24",
            "pub_date_card": {
                "ru": "24 Ğ¸ÑĞ»Ñ",
                "en": "July 24",
                "zh": "7æœˆ24æ—¥"
            },
            "hash": "c5028e9aad7b0964",
            "authors": [
                "Minghao Fu",
                "Guo-Hua Wang",
                "Xiaohao Chen",
                "Qing-Guo Chen",
                "Zhao Xu",
                "Weihua Luo",
                "Kaifu Zhang"
            ],
            "affiliations": [
                "Alibaba International Digital Commerce Group",
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "School of Artificial Intelligence, Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.18192.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#inference",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "TeEFusion: Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "TeEFusion - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ classifier-free guidance Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡ĞµĞ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ñ€Ğ¾Ğ´Ğµ SD3 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 6 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "TeEFusion: Fast and Efficient Text-to-Image Synthesis",
                    "desc": "TeEFusion is a new method that improves text-to-image synthesis by integrating classifier-free guidance directly into text embeddings. This approach reduces the need for multiple forward passes, which are costly in terms of computation, while still producing high-quality images. By fusing conditional and unconditional text embeddings through simple linear operations, TeEFusion allows a student model to learn from a more complex teacher model without increasing the number of parameters. As a result, the student model can generate images up to six times faster than the teacher model, while maintaining similar image quality."
                },
                "zh": {
                    "title": "TeEFusionï¼šé«˜æ•ˆçš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ–¹æ³•",
                    "desc": "TeEFusionæ˜¯ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åµŒå…¥èåˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„æ•ˆç‡ã€‚å®ƒé€šè¿‡å°†æ— åˆ†ç±»å™¨å¼•å¯¼ç›´æ¥èå…¥æ–‡æœ¬åµŒå…¥ä¸­ï¼Œå‡å°‘äº†æ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒè´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡çº¿æ€§æ“ä½œç®€å•åœ°èåˆæ¡ä»¶å’Œæ— æ¡ä»¶æ–‡æœ¬åµŒå…¥ï¼Œé¿å…äº†é¢å¤–å‚æ•°çš„å¢åŠ ã€‚å®éªŒè¡¨æ˜ï¼ŒTeEFusionä½¿å¾—å­¦ç”Ÿæ¨¡å‹åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ¯”æ•™å¸ˆæ¨¡å‹å¿«6å€ï¼ŒåŒæ—¶å›¾åƒè´¨é‡ä¸æ•™å¸ˆæ¨¡å‹ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.14988",
            "title": "DMOSpeech 2: Reinforcement Learning for Duration Prediction in\n  Metric-Optimized Speech Synthesis",
            "url": "https://huggingface.co/papers/2507.14988",
            "abstract": "DMOSpeech 2 optimizes duration prediction and introduces teacher-guided sampling to enhance speech synthesis performance and diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based text-to-speech (TTS) systems have made remarkable progress in zero-shot speech synthesis, yet optimizing all components for perceptual metrics remains challenging. Prior work with DMOSpeech demonstrated direct metric optimization for speech generation components, but duration prediction remained unoptimized. This paper presents DMOSpeech 2, which extends metric optimization to the duration predictor through a reinforcement learning approach. The proposed system implements a novel duration policy framework using group relative preference optimization (GRPO) with speaker similarity and word error rate as reward signals. By optimizing this previously unoptimized component, DMOSpeech 2 creates a more complete metric-optimized synthesis pipeline. Additionally, this paper introduces teacher-guided sampling, a hybrid approach leveraging a teacher model for initial denoising steps before transitioning to the student model, significantly improving output diversity while maintaining efficiency. Comprehensive evaluations demonstrate superior performance across all metrics compared to previous systems, while reducing sampling steps by half without quality degradation. These advances represent a significant step toward speech synthesis systems with metric optimization across multiple components. The audio samples, code and pre-trained models are available at https://dmospeech2.github.io/.",
            "score": 0,
            "issue_id": 5005,
            "pub_date": "2025-07-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ»Ñ",
                "en": "July 20",
                "zh": "7æœˆ20æ—¥"
            },
            "hash": "27df664b6cc093b4",
            "authors": [
                "Yinghao Aaron Li",
                "Xilin Jiang",
                "Fei Tao",
                "Cheng Niu",
                "Kaifeng Xu",
                "Juntong Song",
                "Nima Mesgarani"
            ],
            "affiliations": [
                "Columbia University",
                "NewsBreak"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.14988.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#rl",
                    "#audio",
                    "#optimization"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…",
                    "desc": "DMOSpeech 2 - ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (GRPO) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ° Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. DMOSpeech 2 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Optimizing Duration for Diverse Speech Synthesis",
                    "desc": "DMOSpeech 2 enhances speech synthesis by optimizing duration prediction, which was previously unaddressed. It employs a reinforcement learning strategy with group relative preference optimization (GRPO) to improve the duration predictor using metrics like speaker similarity and word error rate. Additionally, the paper introduces teacher-guided sampling, which combines a teacher model for initial processing with a student model for efficiency, leading to greater output diversity. Overall, DMOSpeech 2 achieves superior performance in speech synthesis while reducing the number of sampling steps needed, marking a significant advancement in metric-optimized TTS systems."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è¯­éŸ³åˆæˆï¼Œæå‡å¤šæ ·æ€§ä¸æ•ˆç‡",
                    "desc": "DMOSpeech 2 æ˜¯ä¸€ç§ä¼˜åŒ–è¯­éŸ³åˆæˆä¸­æŒç»­æ—¶é—´é¢„æµ‹çš„æ–°æ–¹æ³•ï¼Œé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥æ¥æå‡åˆæˆæ•ˆæœã€‚è¯¥ç³»ç»Ÿå¼•å…¥äº†åŸºäºç»„ç›¸å¯¹åå¥½çš„ä¼˜åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨è¯´è¯è€…ç›¸ä¼¼æ€§å’Œè¯é”™è¯¯ç‡ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä»è€Œä¼˜åŒ–äº†ä¹‹å‰æœªä¼˜åŒ–çš„æŒç»­æ—¶é—´é¢„æµ‹ç»„ä»¶ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒDMOSpeech 2 è¿˜é‡‡ç”¨äº†æ•™å¸ˆå¼•å¯¼é‡‡æ ·çš„æ–¹æ³•ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹è¿›è¡Œåˆæ­¥å»å™ªï¼Œå†è½¬å‘å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†è¾“å‡ºçš„å¤šæ ·æ€§ã€‚ç»¼åˆè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¹‹å‰çš„ç³»ç»Ÿï¼ŒåŒæ—¶å°†é‡‡æ ·æ­¥éª¤å‡å°‘äº†ä¸€åŠï¼Œä¸”æ²¡æœ‰é™ä½è´¨é‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-24.html",
    "link_next": "2025-07-28.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "24.07",
        "en": "07/24",
        "zh": "7æœˆ24æ—¥"
    },
    "short_date_next": {
        "ru": "28.07",
        "en": "07/28",
        "zh": "7æœˆ28æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 2,
        "#rl": 3,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}