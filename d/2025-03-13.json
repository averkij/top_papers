{
    "date": {
        "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 13",
        "zh": "3æœˆ13æ—¥"
    },
    "time_utc": "2025-03-13 03:22",
    "weekday": 3,
    "issue_id": 2677,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.09427",
            "title": "Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation",
            "url": "https://huggingface.co/papers/2503.09427",
            "abstract": "Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-the-art cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells -- the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative improvement of textual discrepancy for cell description generation, 20.5\\% higher accuracy for cell type annotation, and 4\\% improvement in k-NN accuracy for text-conditioned pseudo-cell generation, outperforming baselines.",
            "score": 1,
            "issue_id": 2677,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "491beb48064068d2",
            "authors": [
                "Yaorui Shi",
                "Jiaqi Yang",
                "Sihang Li",
                "Junfeng Fang",
                "Xiang Wang",
                "Zhiyuan Liu",
                "Yang Zhang"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09427.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#transfer_learning",
                    "#science",
                    "#multimodal",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ»ĞµÑ‚Ğ¾Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "scMMGPT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ»ĞµÑ‚Ğ¾Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸Ğ· ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. scMMGPT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 27 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… ĞºĞ»ĞµÑ‚Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞµĞ³Ğ¾Ğ´Ğ½ÑÑˆĞ½Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ."
                },
                "en": {
                    "title": "Bridging Cells and Text: The Power of scMMGPT",
                    "desc": "This paper introduces the Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a novel model designed to integrate single-cell RNA sequencing data with textual information. Traditional pre-trained language models struggle with this integration due to their inability to process both modalities effectively. scMMGPT addresses these limitations by utilizing cross-modal projectors and extensive pre-training on a large dataset of 27 million cells, enhancing its performance in joint tasks. The results demonstrate significant improvements in cell description generation, cell type annotation accuracy, and text-conditioned pseudo-cell generation compared to existing models."
                },
                "zh": {
                    "title": "å•ç»†èƒå¤šæ¨¡æ€ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨çš„åˆ›æ–°åº”ç”¨",
                    "desc": "é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨ç§‘å­¦ç ”ç©¶ä¸­å¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†åœ¨å•ç»†èƒåˆ†æä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚ç°æœ‰çš„æ–‡æœ¬PLMsæ— æ³•å¤„ç†å•ç»†èƒRNAæµ‹åºæ•°æ®ï¼Œè€Œç»†èƒPLMsåˆæ— æ³•å¤„ç†è‡ªç”±æ–‡æœ¬ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„ä½¿ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å•ç»†èƒå¤šæ¨¡æ€ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆscMMGPTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç»†èƒå’Œæ–‡æœ¬è”åˆå»ºæ¨¡çš„ç»Ÿä¸€PLMã€‚scMMGPTé€šè¿‡ä¸“é—¨çš„è·¨æ¨¡æ€æŠ•å½±å™¨å’Œåœ¨2700ä¸‡ä¸ªç»†èƒä¸Šè¿›è¡Œçš„å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†ç»†èƒæè¿°ç”Ÿæˆå’Œç»†èƒç±»å‹æ³¨é‡Šçš„å‡†ç¡®æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-12.html",
    "link_next": "2025-03-14.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "12.03",
        "en": "03/12",
        "zh": "3æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "14.03",
        "en": "03/14",
        "zh": "3æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†YuEï¼Œä¸€ç§åŸºäºLLaMA2æ¶æ„çš„å¼€æ”¾åŸºç¡€æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé•¿è¾¾äº”åˆ†é’Ÿçš„éŸ³ä¹ï¼Œä¿æŒæ­Œè¯å¯¹é½ã€è¿è´¯çš„éŸ³ä¹ç»“æ„å’Œå¼•äººå…¥èƒœçš„æ­Œå”±æ—‹å¾‹ã€‚YuEé€šè¿‡å¤šä»»åŠ¡ã€å¤šé˜¶æ®µçš„é¢„è®­ç»ƒæ–¹æ³•å®ç°è¿™ä¸€ç‚¹ã€‚å®ƒè¿˜èƒ½è¿›è¡Œé£æ ¼è½¬æ¢å’ŒåŒå‘ç”Ÿæˆã€‚å®éªŒæ˜¾ç¤ºï¼ŒYuEåœ¨éŸ³ä¹æ€§å’Œå£°ä¹çµæ´»æ€§ä¸ŠåŒ¹æ•Œæˆ–è¶…è¶Šä¸€äº›ä¸“æœ‰ç³»ç»Ÿã€‚æ­¤å¤–ï¼ŒYuEåœ¨éŸ³ä¹ç†è§£ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚",
        "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†YuEï¼Œä¸€ç§åŸºäºLLaMA2æ¶æ„çš„å¼€æ”¾åŸºç¡€æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé•¿è¾¾äº”åˆ†é’Ÿçš„éŸ³ä¹ï¼Œä¿æŒæ­Œè¯å¯¹é½ã€è¿è´¯çš„éŸ³ä¹ç»“æ„å’Œå¼•äººå…¥èƒœçš„æ­Œå”±æ—‹å¾‹ã€‚YuEé€šè¿‡å¤šä»»åŠ¡ã€å¤šé˜¶æ®µçš„é¢„è®­ç»ƒæ–¹æ³•å®ç°è¿™ä¸€ç‚¹ã€‚å®ƒè¿˜èƒ½è¿›è¡Œé£æ ¼è½¬æ¢å’ŒåŒå‘ç”Ÿæˆã€‚å®éªŒæ˜¾ç¤ºï¼ŒYuEåœ¨éŸ³ä¹æ€§å’Œå£°ä¹çµæ´»æ€§ä¸ŠåŒ¹æ•Œæˆ–è¶…è¶Šä¸€äº›ä¸“æœ‰ç³»ç»Ÿã€‚æ­¤å¤–ï¼ŒYuEåœ¨éŸ³ä¹ç†è§£ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le YuE, yÄ« zhÇ’ng jÄ«yÃº LLaMA2 jiÃ gÃ²u de kÄifÃ ng jÄ«chÇ” mÃ³xÃ­ng. TÄ nÃ©ng shÄ“ngchÃ©ng chÃ¡ng dÃ¡ wÇ” fÄ“nzhÅng de yÄ«nyuÃ¨, bÇochÃ­ gÄ“cÃ­ duÃ¬qÃ­, liÃ¡nhÃ© de yÄ«nyuÃ¨ jiÃ©gÃ²u hÃ© yÇnrÃ©nrÃ¹shÃ¨ng de gÄ“chÃ ng xuÃ¡nlÇœ. YuE tÅngguÃ² duÅ rÃ¨nwÃ¹, duÅ jiÄ“duÃ n de yÃ¹xÃ¹nliÃ n fÄngfÇ shÃ­xiÃ n zhÃ¨ yÄ«diÇn. TÄ hÃ¡i nÃ©ng jÃ¬nxÃ­ng fÄ“nggÃ© zhuÇnhuÃ n hÃ© shuÄngxiÃ ng shÄ“ngchÃ©ng. ShÃ­yÃ n xiÇnshÃ¬, YuE zÃ i yÄ«nyuÃ¨xÃ¬ng hÃ© shÄ“ngyuÃ¨ lÃ­nghuÃ³xÃ¬ng shÃ ng pÇdÃ­ huÃ² chÄoyuÃ¨ yÄ«xiÄ“ zhuÄnyÇ’u xÃ¬tÇ’ng. CÇwÃ i,YuE zÃ i yÄ«nyuÃ¨ lÇjiÄ› rÃ¨nwÃ¹ zhÅng yÄ› biÇoxiÃ n chÅ«sÃ¨.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ« yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"æ¶æ„\", \"pinyin\": \"jiÃ  gÃ²u\", \"trans\": \"architecture\"},\n    {\"word\": \"å¼€æ”¾\", \"pinyin\": \"kÄi fÃ ng\", \"trans\": \"open\"},\n    {\"word\": \"åŸºç¡€\", \"pinyin\": \"jÄ« chÇ”\", \"trans\": \"foundation\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"ä¿æŒ\", \"pinyin\": \"bÇo chÃ­\", \"trans\": \"maintain\"},\n    {\"word\": \"å¯¹é½\", \"pinyin\": \"duÃ¬ qÃ­\", \"trans\": \"alignment\"},\n    {\"word\": \"è¿è´¯\", \"pinyin\": \"liÃ¡n guÃ n\", \"trans\": \"coherent\"},\n    {\"word\": \"ç»“æ„\", \"pinyin\": \"jiÃ© gÃ²u\", \"trans\": \"structure\"},\n    {\"word\": \"å¼•äººå…¥èƒœ\", \"pinyin\": \"yÇn rÃ©n rÃ¹ shÃ¨ng\", \"trans\": \"fascinating\"},\n    {\"word\": \"æ—‹å¾‹\", \"pinyin\": \"xuÃ¡n lÇœ\", \"trans\": \"melody\"},\n    {\"word\": \"å¤šä»»åŠ¡\", \"pinyin\": \"duÅ rÃ¨n wÃ¹\", \"trans\": \"multi-task\"},\n    {\"word\": \"å¤šé˜¶æ®µ\", \"pinyin\": \"duÅ jiÄ“ duÃ n\", \"trans\": \"multi-stage\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pre-training\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"é£æ ¼\", \"pinyin\": \"fÄ“ng gÃ©\", \"trans\": \"style\"},\n    {\"word\": \"è½¬æ¢\", \"pinyin\": \"zhuÇn huÃ n\", \"trans\": \"conversion\"},\n    {\"word\": \"åŒå‘\", \"pinyin\": \"shuÄng xiÃ ng\", \"trans\": \"bidirectional\"},\n    {\"word\": \"åŒ¹æ•Œ\", \"pinyin\": \"pÇ dÃ­\", \"trans\": \"match\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄo yuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"ä¸“æœ‰\", \"pinyin\": \"zhuÄn yÇ’u\", \"trans\": \"proprietary\"},\n    {\"word\": \"ç³»ç»Ÿ\", \"pinyin\": \"xÃ¬ tÇ’ng\", \"trans\": \"system\"},\n    {\"word\": \"çµæ´»æ€§\", \"pinyin\": \"lÃ­ng huÃ³ xÃ¬ng\", \"trans\": \"flexibility\"},\n    {\"word\": \"æ­¤å¤–\", \"pinyin\": \"cÇ wÃ i\", \"trans\": \"moreover\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understanding\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"}\n]",
        "trans": "This article introduces YuE, an open-source foundational model based on the LLaMA2 architecture. It can generate music up to five minutes long, maintaining lyric alignment, coherent musical structure, and captivating vocal melodies. YuE achieves this through a multi-task, multi-stage pre-training method. It is also capable of style transfer and bidirectional generation. Experiments show that YuE matches or surpasses some proprietary systems in terms of musicality and vocal flexibility. Additionally, YuE performs excellently in music understanding tasks.",
        "update_ts": "2025-03-12 09:11"
    }
}