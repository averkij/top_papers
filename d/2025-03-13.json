{
    "date": {
        "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 13",
        "zh": "3æœˆ13æ—¥"
    },
    "time_utc": "2025-03-13 05:11",
    "weekday": 3,
    "issue_id": 2679,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.09573",
            "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
            "url": "https://huggingface.co/papers/2503.09573",
            "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/",
            "score": 7,
            "issue_id": 2678,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "32f097e93cbf5f3a",
            "authors": [
                "Marianne Arriola",
                "Aaron Gokaslan",
                "Justin T Chiu",
                "Zhihan Yang",
                "Zhixuan Qi",
                "Jiaqi Han",
                "Subham Sekhar Sahoo",
                "Volodymyr Kuleshov"
            ],
            "affiliations": [
                "Cohere, NY, USA",
                "Cornell Tech, NY, USA",
                "Stanford University, CA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09573.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ‘Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸. Ğ‘Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Block Diffusion: The Future of Flexible Language Generation",
                    "desc": "This paper presents block diffusion language models, which combine the strengths of diffusion models and autoregressive models. These models allow for flexible-length text generation and improve efficiency during inference by using techniques like KV caching and parallel token sampling. The authors introduce a comprehensive approach for training these models, including methods to reduce gradient variance and optimize noise schedules. As a result, block diffusion models achieve state-of-the-art performance in language modeling tasks and can generate sequences of varying lengths."
                },
                "zh": {
                    "title": "å—æ‰©æ•£æ¨¡å‹ï¼šçµæ´»ç”Ÿæˆä¸é«˜æ•ˆæ¨ç†çš„ç»“åˆ",
                    "desc": "æ‰©æ•£è¯­è¨€æ¨¡å‹ç›¸æ¯”è‡ªå›å½’æ¨¡å‹å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå¦‚å¹¶è¡Œç”Ÿæˆå’Œå¯æ§æ€§ï¼Œä½†åœ¨ä¼¼ç„¶å»ºæ¨¡æ–¹é¢è¡¨ç°è¾ƒå·®ï¼Œå¹¶ä¸”ç”Ÿæˆé•¿åº¦å›ºå®šã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç±»å—æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†ç¦»æ•£å»å™ªæ‰©æ•£å’Œè‡ªå›å½’æ¨¡å‹çš„ä¼˜ç‚¹ã€‚å—æ‰©æ•£å…‹æœäº†è¿™ä¸¤ç§æ–¹æ³•çš„å…³é”®é™åˆ¶ï¼Œæ”¯æŒçµæ´»é•¿åº¦çš„ç”Ÿæˆï¼Œå¹¶é€šè¿‡KVç¼“å­˜å’Œå¹¶è¡Œä»¤ç‰Œé‡‡æ ·æé«˜æ¨ç†æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ„å»ºæœ‰æ•ˆå—æ‰©æ•£æ¨¡å‹çš„æ–¹æ¡ˆï¼ŒåŒ…æ‹¬é«˜æ•ˆçš„è®­ç»ƒç®—æ³•ã€æ¢¯åº¦æ–¹å·®ä¼°è®¡å™¨å’Œæ•°æ®é©±åŠ¨çš„å™ªå£°è°ƒåº¦ï¼Œä»¥æœ€å°åŒ–æ–¹å·®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09427",
            "title": "Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation",
            "url": "https://huggingface.co/papers/2503.09427",
            "abstract": "Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-the-art cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells -- the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative improvement of textual discrepancy for cell description generation, 20.5\\% higher accuracy for cell type annotation, and 4\\% improvement in k-NN accuracy for text-conditioned pseudo-cell generation, outperforming baselines.",
            "score": 1,
            "issue_id": 2677,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 12",
                "zh": "3æœˆ12æ—¥"
            },
            "hash": "491beb48064068d2",
            "authors": [
                "Yaorui Shi",
                "Jiaqi Yang",
                "Sihang Li",
                "Junfeng Fang",
                "Xiang Wang",
                "Zhiyuan Liu",
                "Yang Zhang"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09427.jpg",
            "data": {
                "categories": [
                    "#plp",
                    "#transfer_learning",
                    "#science",
                    "#multimodal",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ»ĞµÑ‚Ğ¾Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "scMMGPT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ»ĞµÑ‚Ğ¾Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸Ğ· ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. scMMGPT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 27 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… ĞºĞ»ĞµÑ‚Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞµĞ³Ğ¾Ğ´Ğ½ÑÑˆĞ½Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ."
                },
                "en": {
                    "title": "Bridging Cells and Text: The Power of scMMGPT",
                    "desc": "This paper introduces the Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a novel model designed to integrate single-cell RNA sequencing data with textual information. Traditional pre-trained language models struggle with this integration due to their inability to process both modalities effectively. scMMGPT addresses these limitations by utilizing cross-modal projectors and extensive pre-training on a large dataset of 27 million cells, enhancing its performance in joint tasks. The results demonstrate significant improvements in cell description generation, cell type annotation accuracy, and text-conditioned pseudo-cell generation compared to existing models."
                },
                "zh": {
                    "title": "å•ç»†èƒå¤šæ¨¡æ€ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨çš„åˆ›æ–°åº”ç”¨",
                    "desc": "é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨ç§‘å­¦ç ”ç©¶ä¸­å¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†åœ¨å•ç»†èƒåˆ†æä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚ç°æœ‰çš„æ–‡æœ¬PLMsæ— æ³•å¤„ç†å•ç»†èƒRNAæµ‹åºæ•°æ®ï¼Œè€Œç»†èƒPLMsåˆæ— æ³•å¤„ç†è‡ªç”±æ–‡æœ¬ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„ä½¿ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å•ç»†èƒå¤šæ¨¡æ€ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆscMMGPTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç»†èƒå’Œæ–‡æœ¬è”åˆå»ºæ¨¡çš„ç»Ÿä¸€PLMã€‚scMMGPTé€šè¿‡ä¸“é—¨çš„è·¨æ¨¡æ€æŠ•å½±å™¨å’Œåœ¨2700ä¸‡ä¸ªç»†èƒä¸Šè¿›è¡Œçš„å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†ç»†èƒæè¿°ç”Ÿæˆå’Œç»†èƒç±»å‹æ³¨é‡Šçš„å‡†ç¡®æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-12.html",
    "link_next": "2025-03-14.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "12.03",
        "en": "03/12",
        "zh": "3æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "14.03",
        "en": "03/14",
        "zh": "3æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†YuEï¼Œä¸€ç§åŸºäºLLaMA2æ¶æ„çš„å¼€æ”¾åŸºç¡€æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé•¿è¾¾äº”åˆ†é’Ÿçš„éŸ³ä¹ï¼Œä¿æŒæ­Œè¯å¯¹é½ã€è¿è´¯çš„éŸ³ä¹ç»“æ„å’Œå¼•äººå…¥èƒœçš„æ­Œå”±æ—‹å¾‹ã€‚YuEé€šè¿‡å¤šä»»åŠ¡ã€å¤šé˜¶æ®µçš„é¢„è®­ç»ƒæ–¹æ³•å®ç°è¿™ä¸€ç‚¹ã€‚å®ƒè¿˜èƒ½è¿›è¡Œé£æ ¼è½¬æ¢å’ŒåŒå‘ç”Ÿæˆã€‚å®éªŒæ˜¾ç¤ºï¼ŒYuEåœ¨éŸ³ä¹æ€§å’Œå£°ä¹çµæ´»æ€§ä¸ŠåŒ¹æ•Œæˆ–è¶…è¶Šä¸€äº›ä¸“æœ‰ç³»ç»Ÿã€‚æ­¤å¤–ï¼ŒYuEåœ¨éŸ³ä¹ç†è§£ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚",
        "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†YuEï¼Œä¸€ç§åŸºäºLLaMA2æ¶æ„çš„å¼€æ”¾åŸºç¡€æ¨¡å‹ã€‚å®ƒèƒ½ç”Ÿæˆé•¿è¾¾äº”åˆ†é’Ÿçš„éŸ³ä¹ï¼Œä¿æŒæ­Œè¯å¯¹é½ã€è¿è´¯çš„éŸ³ä¹ç»“æ„å’Œå¼•äººå…¥èƒœçš„æ­Œå”±æ—‹å¾‹ã€‚YuEé€šè¿‡å¤šä»»åŠ¡ã€å¤šé˜¶æ®µçš„é¢„è®­ç»ƒæ–¹æ³•å®ç°è¿™ä¸€ç‚¹ã€‚å®ƒè¿˜èƒ½è¿›è¡Œé£æ ¼è½¬æ¢å’ŒåŒå‘ç”Ÿæˆã€‚å®éªŒæ˜¾ç¤ºï¼ŒYuEåœ¨éŸ³ä¹æ€§å’Œå£°ä¹çµæ´»æ€§ä¸ŠåŒ¹æ•Œæˆ–è¶…è¶Šä¸€äº›ä¸“æœ‰ç³»ç»Ÿã€‚æ­¤å¤–ï¼ŒYuEåœ¨éŸ³ä¹ç†è§£ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le YuE, yÄ« zhÇ’ng jÄ«yÃº LLaMA2 jiÃ gÃ²u de kÄifÃ ng jÄ«chÇ” mÃ³xÃ­ng. TÄ nÃ©ng shÄ“ngchÃ©ng chÃ¡ng dÃ¡ wÇ” fÄ“nzhÅng de yÄ«nyuÃ¨, bÇochÃ­ gÄ“cÃ­ duÃ¬qÃ­, liÃ¡nhÃ© de yÄ«nyuÃ¨ jiÃ©gÃ²u hÃ© yÇnrÃ©nrÃ¹shÃ¨ng de gÄ“chÃ ng xuÃ¡nlÇœ. YuE tÅngguÃ² duÅ rÃ¨nwÃ¹, duÅ jiÄ“duÃ n de yÃ¹xÃ¹nliÃ n fÄngfÇ shÃ­xiÃ n zhÃ¨ yÄ«diÇn. TÄ hÃ¡i nÃ©ng jÃ¬nxÃ­ng fÄ“nggÃ© zhuÇnhuÃ n hÃ© shuÄngxiÃ ng shÄ“ngchÃ©ng. ShÃ­yÃ n xiÇnshÃ¬, YuE zÃ i yÄ«nyuÃ¨xÃ¬ng hÃ© shÄ“ngyuÃ¨ lÃ­nghuÃ³xÃ¬ng shÃ ng pÇdÃ­ huÃ² chÄoyuÃ¨ yÄ«xiÄ“ zhuÄnyÇ’u xÃ¬tÇ’ng. CÇwÃ i,YuE zÃ i yÄ«nyuÃ¨ lÇjiÄ› rÃ¨nwÃ¹ zhÅng yÄ› biÇoxiÃ n chÅ«sÃ¨.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ« yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"æ¶æ„\", \"pinyin\": \"jiÃ  gÃ²u\", \"trans\": \"architecture\"},\n    {\"word\": \"å¼€æ”¾\", \"pinyin\": \"kÄi fÃ ng\", \"trans\": \"open\"},\n    {\"word\": \"åŸºç¡€\", \"pinyin\": \"jÄ« chÇ”\", \"trans\": \"foundation\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"ä¿æŒ\", \"pinyin\": \"bÇo chÃ­\", \"trans\": \"maintain\"},\n    {\"word\": \"å¯¹é½\", \"pinyin\": \"duÃ¬ qÃ­\", \"trans\": \"alignment\"},\n    {\"word\": \"è¿è´¯\", \"pinyin\": \"liÃ¡n guÃ n\", \"trans\": \"coherent\"},\n    {\"word\": \"ç»“æ„\", \"pinyin\": \"jiÃ© gÃ²u\", \"trans\": \"structure\"},\n    {\"word\": \"å¼•äººå…¥èƒœ\", \"pinyin\": \"yÇn rÃ©n rÃ¹ shÃ¨ng\", \"trans\": \"fascinating\"},\n    {\"word\": \"æ—‹å¾‹\", \"pinyin\": \"xuÃ¡n lÇœ\", \"trans\": \"melody\"},\n    {\"word\": \"å¤šä»»åŠ¡\", \"pinyin\": \"duÅ rÃ¨n wÃ¹\", \"trans\": \"multi-task\"},\n    {\"word\": \"å¤šé˜¶æ®µ\", \"pinyin\": \"duÅ jiÄ“ duÃ n\", \"trans\": \"multi-stage\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹n liÃ n\", \"trans\": \"pre-training\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"é£æ ¼\", \"pinyin\": \"fÄ“ng gÃ©\", \"trans\": \"style\"},\n    {\"word\": \"è½¬æ¢\", \"pinyin\": \"zhuÇn huÃ n\", \"trans\": \"conversion\"},\n    {\"word\": \"åŒå‘\", \"pinyin\": \"shuÄng xiÃ ng\", \"trans\": \"bidirectional\"},\n    {\"word\": \"åŒ¹æ•Œ\", \"pinyin\": \"pÇ dÃ­\", \"trans\": \"match\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄo yuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"ä¸“æœ‰\", \"pinyin\": \"zhuÄn yÇ’u\", \"trans\": \"proprietary\"},\n    {\"word\": \"ç³»ç»Ÿ\", \"pinyin\": \"xÃ¬ tÇ’ng\", \"trans\": \"system\"},\n    {\"word\": \"çµæ´»æ€§\", \"pinyin\": \"lÃ­ng huÃ³ xÃ¬ng\", \"trans\": \"flexibility\"},\n    {\"word\": \"æ­¤å¤–\", \"pinyin\": \"cÇ wÃ i\", \"trans\": \"moreover\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understanding\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"}\n]",
        "trans": "This article introduces YuE, an open-source foundational model based on the LLaMA2 architecture. It can generate music up to five minutes long, maintaining lyric alignment, coherent musical structure, and captivating vocal melodies. YuE achieves this through a multi-task, multi-stage pre-training method. It is also capable of style transfer and bidirectional generation. Experiments show that YuE matches or surpasses some proprietary systems in terms of musicality and vocal flexibility. Additionally, YuE performs excellently in music understanding tasks.",
        "update_ts": "2025-03-12 09:11"
    }
}