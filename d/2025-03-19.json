{
    "date": {
        "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 19",
        "zh": "3æœˆ19æ—¥"
    },
    "time_utc": "2025-03-19 02:19",
    "weekday": 2,
    "issue_id": 2776,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.13424",
            "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
            "url": "https://huggingface.co/papers/2503.13424",
            "abstract": "Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and heavy labour of the simulation. In this paper, we propose Infinite Mobility, a novel method for synthesizing high-fidelity articulated objects through procedural generation. User study and quantitative evaluation demonstrate that our method can produce results that excel current state-of-the-art methods and are comparable to human-annotated datasets in both physics property and mesh quality. Furthermore, we show that our synthetic data can be used as training data for generative models, enabling next-step scaling up. Code is available at https://github.com/Intern-Nexus/Infinite-Mobility",
            "score": 5,
            "issue_id": 2776,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "20793013b58dba36",
            "authors": [
                "Xinyu Lian",
                "Zichao Yu",
                "Ruiming Liang",
                "Yitong Wang",
                "Li Ray Luo",
                "Kaixu Chen",
                "Yuanzhen Zhou",
                "Qihong Tang",
                "Xudong Xu",
                "Zhaoyang Lyu",
                "Bo Dai",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Fudan University",
                "Harbin Institute of Technology, Shenzhen",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Shanghai Artificial Intelligence Laboratory",
                "South China University of Technology",
                "The University of Hong Kong",
                "Tongji University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13424.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ñ‡Ğ»ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Infinite Mobility Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾Ñ‡Ğ»ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ, Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¿Ğ¾Ğ»Ğ¸Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Infinite Mobility, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Articulated Object Creation with Infinite Mobility",
                    "desc": "This paper introduces Infinite Mobility, a new approach for creating high-quality articulated objects using procedural generation techniques. Unlike traditional methods that rely on limited data or labor-intensive simulations, Infinite Mobility generates objects that maintain high fidelity in both physical properties and mesh quality. The authors conducted user studies and quantitative evaluations, showing that their method outperforms existing state-of-the-art techniques and rivals human-annotated datasets. Additionally, the synthetic data produced can be utilized for training generative models, facilitating further advancements in the field of embodied AI."
                },
                "zh": {
                    "title": "æ— é™ç§»åŠ¨ï¼šåˆæˆé«˜ä¿çœŸå…³èŠ‚ç‰©ä½“çš„æ–°æ–¹æ³•",
                    "desc": "åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ— é™ç§»åŠ¨ï¼ˆInfinite Mobilityï¼‰çš„æ–¹æ³•ï¼Œç”¨äºé€šè¿‡ç¨‹åºç”Ÿæˆåˆæˆé«˜ä¿çœŸåº¦çš„å…³èŠ‚ç‰©ä½“ã€‚è¿™ç§æ–¹æ³•å…‹æœäº†ç°æœ‰æ•°æ®é©±åŠ¨æˆ–æ¨¡æ‹Ÿæ–¹æ³•åœ¨è§„æ¨¡å’Œè´¨é‡ä¸Šçš„é™åˆ¶ã€‚ç”¨æˆ·ç ”ç©¶å’Œå®šé‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç‰©ç†å±æ€§å’Œç½‘æ ¼è´¨é‡ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¸äººå·¥æ ‡æ³¨çš„æ•°æ®é›†ç›¸å½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆæˆæ•°æ®å¯ä»¥ä½œä¸ºç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ•°æ®ï¼Œæ”¯æŒåç»­çš„æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14125",
            "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
            "url": "https://huggingface.co/papers/2503.14125",
            "abstract": "Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the seesaw effect between gradient vanishing and representation collapse. However, Hyper-Connections increase memory access costs by expanding the width of hidden states. In this paper, we propose Frac-Connections, a novel approach that divides hidden states into multiple parts rather than expanding their width. Frac-Connections retain partial benefits of Hyper-Connections while reducing memory consumption. To validate their effectiveness, we conduct large-scale experiments on language tasks, with the largest being a 7B MoE model trained on up to 3T tokens, demonstrating that Frac-Connections significantly outperform residual connections.",
            "score": 3,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "936ea0aa4972c382",
            "authors": [
                "Defa Zhu",
                "Hongzhi Huang",
                "Jundong Zhou",
                "Zihao Huang",
                "Yutao Zeng",
                "Banggu Wu",
                "Qiyang Min",
                "Xun Zhou"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14125.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Frac-Connections: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Frac-Connections. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ´ĞµÑ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ (residual connections), Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹. Frac-Connections ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Hyper-Connections, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MoE Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° 3 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Frac-Connections: Efficient Memory for Deep Learning",
                    "desc": "This paper introduces Frac-Connections, a new method that improves upon Hyper-Connections in deep learning. While Hyper-Connections allow for multiple connection strengths to combat issues like gradient vanishing, they also increase memory usage due to wider hidden states. Frac-Connections address this by splitting hidden states into smaller parts, maintaining some advantages of Hyper-Connections while reducing memory costs. The authors demonstrate the effectiveness of Frac-Connections through extensive experiments on language tasks, showing superior performance compared to traditional residual connections."
                },
                "zh": {
                    "title": "Frac-Connectionsï¼šä¼˜åŒ–æ·±åº¦å­¦ä¹ çš„å†…å­˜ä½¿ç”¨",
                    "desc": "æ®‹å·®è¿æ¥æ˜¯ç°ä»£æ·±åº¦å­¦ä¹ æ¶æ„çš„æ ¸å¿ƒï¼Œèƒ½å¤Ÿé€šè¿‡å‡è½»æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ¥è®­ç»ƒéå¸¸æ·±çš„ç½‘ç»œã€‚è¶…è¿æ¥æœ€è¿‘é€šè¿‡åœ¨ä¸åŒæ·±åº¦å¼•å…¥å¤šä¸ªè¿æ¥å¼ºåº¦æ¥æ¨å¹¿æ®‹å·®è¿æ¥ï¼Œä»è€Œè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±ä¸è¡¨ç¤ºå´©æºƒä¹‹é—´çš„æ‘‡æ‘†æ•ˆåº”ã€‚ç„¶è€Œï¼Œè¶…è¿æ¥é€šè¿‡æ‰©å±•éšè—çŠ¶æ€çš„å®½åº¦å¢åŠ äº†å†…å­˜è®¿é—®æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†Frac-Connectionsï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å°†éšè—çŠ¶æ€åˆ’åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†è€Œä¸æ˜¯æ‰©å±•å…¶å®½åº¦ï¼Œä¿ç•™äº†è¶…è¿æ¥çš„éƒ¨åˆ†ä¼˜åŠ¿ï¼ŒåŒæ—¶å‡å°‘äº†å†…å­˜æ¶ˆè€—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14456",
            "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
            "url": "https://huggingface.co/papers/2503.14456",
            "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC^0. To demonstrate RWKV-7's language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset.   To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "0cd796cef6fa6475",
            "authors": [
                "Bo Peng",
                "Ruichong Zhang",
                "Daniel Goldstein",
                "Eric Alcaide",
                "Haowen Hou",
                "Janna Lu",
                "William Merrill",
                "Guangyu Song",
                "Kaifeng Tan",
                "Saiteja Utpala",
                "Nathan Wilce",
                "Johan S. Wind",
                "Tianyi Wu",
                "Daniel Wuttke",
                "Christian Zhou-Zheng"
            ],
            "affiliations": [
                "Beijing Normal University",
                "Dalle Molle Institute for Artificial Intelligence USI-SUPSI",
                "Denigma",
                "EleutherAI",
                "George Mason University",
                "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
                "New York University",
                "RWKV Project (under Linux Foundation AI & Data)",
                "Recursal AI",
                "Shenzhen University",
                "Tano Labs",
                "Tsinghua University",
                "University of Oslo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14456.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#multilingual"
                ],
                "emoji": "ğŸ¦¢",
                "ru": {
                    "title": "RWKV-7: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "RWKV-7 'Goose' - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½, Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ´ĞµĞ»ÑŒÑ‚Ñ‹ Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. RWKV-7 ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¸Ğ· 3,1 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¼ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ RWKV-7 Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 0,19 Ğ´Ğ¾ 2,9 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "RWKV-7: Efficient Multilingual Mastery with Fewer Parameters",
                    "desc": "RWKV-7 \"Goose\" is a novel sequence modeling architecture that achieves state-of-the-art performance on multilingual tasks with only 3 billion parameters. It utilizes a unique formulation of the delta rule and vector-valued gating, allowing for efficient memory usage and constant inference time per token. The model demonstrates advanced capabilities such as state tracking and recognition of regular languages, surpassing traditional Transformers in complexity. Additionally, RWKV-7 is trained on a large multilingual corpus and is made openly available for further research and development."
                },
                "zh": {
                    "title": "RWKV-7ï¼šå¤šè¯­è¨€ä»»åŠ¡çš„æ–°çªç ´",
                    "desc": "RWKV-7 \"Goose\" æ˜¯ä¸€ç§æ–°çš„åºåˆ—å»ºæ¨¡æ¶æ„ï¼Œå…·æœ‰3äº¿å‚æ•°çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œåœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚ä¸å…¶ä»–é¡¶çº§3Bæ¨¡å‹ç›¸æ¯”ï¼ŒRWKV-7åœ¨è®­ç»ƒæ—¶ä½¿ç”¨çš„æ ‡è®°æ•°é‡æ˜¾è‘—å‡å°‘ï¼Œä½†ä»èƒ½ä¸å½“å‰çš„è‹±è¯­è¯­è¨€æ€§èƒ½ç›¸åŒ¹é…ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†æ–°çš„å¹¿ä¹‰å¢é‡è§„åˆ™ï¼Œç»“åˆäº†å‘é‡å€¼é—¨æ§å’Œä¸Šä¸‹æ–‡å­¦ä¹ ç‡ï¼ŒåŒæ—¶ä¿æŒäº†è®­ç»ƒçš„å¹¶è¡Œæ€§ã€‚RWKV-7èƒ½å¤Ÿè¿›è¡ŒçŠ¶æ€è·Ÿè¸ªå¹¶è¯†åˆ«æ‰€æœ‰æ­£è§„è¯­è¨€ï¼Œè¶…è¶Šäº†æ ‡å‡†å¤æ‚æ€§çŒœæƒ³ä¸‹çš„Transformerçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14378",
            "title": "Impossible Videos",
            "url": "https://huggingface.co/papers/2503.14378",
            "abstract": "Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can today's video generation models effectively follow prompts to create impossible video content? 2) Are today's video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models.",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "3334aa74b743ac8d",
            "authors": [
                "Zechen Bai",
                "Hai Ci",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14378.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "IPV-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ IPV-Bench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¹ 4 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸ 14 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ½Ğ°Ñ€ÑƒÑˆĞ°ÑÑ‰Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ»Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Video-LLM Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ´ĞµĞ¸ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Exploring the Impossible: Advancing Video Generation and Understanding",
                    "desc": "This paper addresses the limitations of current synthetic video datasets, which mainly focus on replicating real-world scenarios. It introduces IPV-Bench, a new benchmark designed to evaluate video generation and understanding models on their ability to create and comprehend impossible video content. The benchmark includes a detailed taxonomy with various categories that challenge models to generate videos that defy physical and social laws. The findings highlight the current capabilities and shortcomings of video models, providing insights for future advancements in the field."
                },
                "zh": {
                    "title": "æ¢ç´¢ä¸å¯èƒ½è§†é¢‘çš„ç”Ÿæˆä¸ç†è§£",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åˆæˆè§†é¢‘åœ¨æ•°æ®ç¨€ç¼ºå’Œå¤šæ ·æ€§æ–¹é¢çš„åº”ç”¨ã€‚å½“å‰çš„åˆæˆæ•°æ®é›†ä¸»è¦å¤åˆ¶ç°å®åœºæ™¯ï¼Œè€Œå¯¹ä¸å¯èƒ½ã€åäº‹å®å’Œåç°å®çš„è§†é¢‘æ¦‚å¿µç ”ç©¶ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†IPV-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œä¿ƒè¿›è§†é¢‘ç†è§£ä¸ç”Ÿæˆçš„è¿›å±•ã€‚è¯¥åŸºå‡†æ¶µç›–äº†å¤šç§è¿åç‰©ç†ã€ç”Ÿç‰©ã€åœ°ç†æˆ–ç¤¾ä¼šæ³•åˆ™çš„åœºæ™¯ï¼Œå¹¶é€šè¿‡æ„å»ºæç¤ºå¥—ä»¶æ¥æŒ‘æˆ˜è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„åˆ›é€ åŠ›å’Œæç¤ºè·Ÿéšèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12545",
            "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2503.12545",
            "abstract": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant concerns about privacy and security. To address these issues, machine unlearning (MU) has emerged as a promising solution, enabling the removal of specific knowledge from an already trained model without requiring retraining from scratch. Although MU for MLLMs has gained attention, current evaluations of its efficacy remain incomplete, and the underlying problem is often poorly defined, which hinders the development of strategies for creating more secure and trustworthy systems. To bridge this gap, we introduce a benchmark, named PEBench, which includes a dataset of personal entities and corresponding general event scenes, designed to comprehensively assess the performance of MU for MLLMs. Through PEBench, we aim to provide a standardized and robust framework to advance research in secure and privacy-preserving multimodal models. We benchmarked 6 MU methods, revealing their strengths and limitations, and shedding light on key challenges and opportunities for MU in MLLMs.",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "8a908fbc8ce24853",
            "authors": [
                "Zhaopan Xu",
                "Pengfei Zhou",
                "Weidong Tang",
                "Jiaxin Ai",
                "Wangbo Zhao",
                "Xiaojiang Peng",
                "Kai Wang",
                "Yang You",
                "Wenqi Shao",
                "Hongxun Yao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HIT",
                "NUS",
                "SZTU",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "XDU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12545.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#ethics",
                    "#security",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ”’",
                "ru": {
                    "title": "PEBench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PEBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (MU) Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). PEBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 6 Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² MU, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Privacy in Multimodal Models with Machine Unlearning",
                    "desc": "This paper discusses the advancements of Multimodal Large Language Models (MLLMs) in tasks like visual question answering and reasoning, while highlighting the privacy concerns associated with their data usage. It introduces machine unlearning (MU) as a method to selectively remove knowledge from these models without complete retraining. The authors present PEBench, a benchmark designed to evaluate MU methods specifically for MLLMs, using a dataset of personal entities and general event scenes. By benchmarking six MU techniques, the study identifies their strengths and weaknesses, aiming to enhance the security and trustworthiness of multimodal systems."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¤šæ¨¡æ€æ¨¡å‹çš„å®‰å…¨ä¸éšç§ä¿æŠ¤",
                    "desc": "è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰é—®ç­”ã€è§†è§‰ç†è§£å’Œæ¨ç†ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›è¿›å±•ä¾èµ–äºä»äº’è”ç½‘æ”¶é›†çš„å¤§é‡æ•°æ®ï¼Œè¿™å¼•å‘äº†éšç§å’Œå®‰å…¨æ–¹é¢çš„é‡å¤§æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœºå™¨é—å¿˜ï¼ˆMUï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»å·²è®­ç»ƒçš„æ¨¡å‹ä¸­åˆ é™¤ç‰¹å®šçŸ¥è¯†ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•PEBenchï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°MUåœ¨MLLMsä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨å®‰å…¨å’Œéšç§ä¿æŠ¤çš„å¤šæ¨¡æ€æ¨¡å‹ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12505",
            "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification",
            "url": "https://huggingface.co/papers/2503.12505",
            "abstract": "Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs.",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "0ac4fdd3411855ac",
            "authors": [
                "Zhaopan Xu",
                "Pengfei Zhou",
                "Jiaxin Ai",
                "Wangbo Zhao",
                "Kai Wang",
                "Xiaojiang Peng",
                "Wenqi Shao",
                "Hongxun Yao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HIT",
                "NUS",
                "SZTU",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12505.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MPBench: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MPBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° (PRM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. MPBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑˆĞ°Ğ³Ğ¾Ğ², Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ PRM Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs with MPBench",
                    "desc": "This paper discusses the importance of reasoning in large language models (LLMs) and introduces process-level reward models (PRMs) to enhance their reasoning capabilities. PRMs provide step-wise rewards that help LLMs learn from their mistakes during training and improve their decision-making during inference. The authors identify a gap in existing benchmarks, which primarily focus on error detection, and propose MPBench, a new benchmark that evaluates PRMs across various reasoning scenarios. MPBench includes three evaluation paradigms: assessing step correctness, aggregating answers, and guiding the reasoning process search, offering a comprehensive framework for understanding PRMs' effectiveness."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„å¤šæ¨¡æ€åŸºå‡†",
                    "desc": "æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†å¤æ‚ä»»åŠ¡çš„é‡è¦èƒ½åŠ›ï¼Œè€Œè¯†åˆ«è¿‡ç¨‹é”™è¯¯å¯¹äºæå‡è¿™ä¸€èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ€è¿‘æå‡ºçš„è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰é€šè¿‡æä¾›é€æ­¥å¥–åŠ±ï¼Œä¿ƒè¿›äº†å¼ºåŒ–å­¦ä¹ å’Œæ•°æ®ç”Ÿæˆï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å¯¼LLMsèµ°å‘æ­£ç¡®çš„æ­¥éª¤ï¼Œæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PRMsåŸºå‡†ä¸»è¦åŸºäºæ–‡æœ¬ï¼Œä¸“æ³¨äºé”™è¯¯æ£€æµ‹ï¼Œå¿½è§†äº†æ¨ç†æœç´¢ç­‰å…¶ä»–åœºæ™¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MPBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šä»»åŠ¡å¤šæ¨¡æ€åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°PRMsåœ¨ä¸åŒåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10410",
            "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation",
            "url": "https://huggingface.co/papers/2503.10410",
            "abstract": "Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration errors, sparse information, and multi-view consistency, leading to poor performance on recent published datasets. To significantly enhance roadside collaborative perception and address critical data issues, we present the first simulation framework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable of generating diverse, multi-view consistent simulated roadside data through dynamic foreground editing and full-scene style transfer of a single image. RoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures accurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View Occlusion-Aware Sampler (MOAS) determines the placement of diverse digital assets within 3D space; (3) DepthSAM innovatively models foreground-background relationships from single-frame fixed-view images, ensuring multi-view consistency of foreground; and (4) Scalable Post-Processing Toolkit generates more realistic and enriched scenes through style transfer and other enhancements. RoCo-Sim significantly improves roadside 3D object detection, outperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on TUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception simulation. Code and pre-trained models will be released soon: https://github.com/duyuwen-duen/RoCo-Sim",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "44150f611040e79d",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#data",
                    "#dataset",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "RoCo-Sim: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ",
                    "desc": "RoCo-Sim - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ğ±ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸, Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. RoCo-Sim Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñ‰Ğ¸Ğº Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¹, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½-Ñ„Ğ¾Ğ½ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ 3D-Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Roadside Awareness with Collaborative Perception",
                    "desc": "This paper introduces Roadside Collaborative Perception, a system where multiple roadside units work together to improve vehicle awareness of their surroundings. It highlights the limitations of current methods that focus mainly on model design while neglecting important data issues such as calibration errors and sparse information. To address these challenges, the authors present RoCo-Sim, a simulation framework that generates diverse and consistent roadside data using advanced techniques like dynamic foreground editing. RoCo-Sim significantly enhances roadside 3D object detection performance, surpassing state-of-the-art methods on benchmark datasets."
                },
                "zh": {
                    "title": "æå‡è·¯è¾¹æ„ŸçŸ¥çš„ååŒåŠ›é‡",
                    "desc": "è·¯è¾¹ååŒæ„ŸçŸ¥æ˜¯ä¸€ç§ç³»ç»Ÿï¼Œå¤šä¸ªè·¯è¾¹å•å…ƒåä½œæ±‡èšæ„ŸçŸ¥æ•°æ®ï¼Œå¸®åŠ©è½¦è¾†æé«˜ç¯å¢ƒæ„è¯†ã€‚ç°æœ‰çš„è·¯è¾¹æ„ŸçŸ¥æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡å‹è®¾è®¡ï¼Œä½†å¿½è§†äº†æ•°æ®é—®é¢˜ï¼Œå¦‚æ ¡å‡†è¯¯å·®ã€ä¿¡æ¯ç¨€ç–å’Œå¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå¯¼è‡´åœ¨æœ€æ–°æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ä¸ºæ˜¾è‘—æå‡è·¯è¾¹ååŒæ„ŸçŸ¥å¹¶è§£å†³å…³é”®æ•°æ®é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªè·¯è¾¹ååŒæ„ŸçŸ¥æ¨¡æ‹Ÿæ¡†æ¶RoCo-Simã€‚RoCo-Simèƒ½å¤Ÿé€šè¿‡åŠ¨æ€å‰æ™¯ç¼–è¾‘å’Œå•å›¾åƒçš„å…¨åœºæ™¯é£æ ¼è¿ç§»ç”Ÿæˆå¤šæ ·åŒ–çš„ã€å¤šè§†å›¾ä¸€è‡´çš„æ¨¡æ‹Ÿè·¯è¾¹æ•°æ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10546",
            "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
            "url": "https://huggingface.co/papers/2503.10546",
            "abstract": "With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at http://kuda-dynamics.github.io.",
            "score": 0,
            "issue_id": 2776,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "f856affbe8bcf064",
            "authors": [
                "Zixian Liu",
                "Mingtong Zhang",
                "Yunzhu Li"
            ],
            "affiliations": [
                "Columbia University",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10546.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#robotics",
                    "#open_source",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "KUDA: Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼",
                    "desc": "KUDA - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLM) Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. KUDA ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ½Ğ° RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµÑ‚ VLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. Ğ—Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "KUDA: Bridging Language and Dynamics for Robotic Manipulation",
                    "desc": "This paper presents KUDA, an innovative open-vocabulary robotic manipulation system that incorporates dynamics learning and visual prompting. Unlike previous methods, KUDA utilizes keypoints to create target specifications that are understandable by vision-language models (VLMs) and can be transformed into cost functions for planning. The system processes language instructions and visual data to assign keypoints to images, which are then optimized using a learned dynamics model to generate robotic movements. The effectiveness of KUDA is validated through various manipulation tasks, showcasing its ability to handle complex interactions with different object types."
                },
                "zh": {
                    "title": "KUDAï¼šåŠ¨æ€å­¦ä¹ ä¸è§†è§‰æç¤ºçš„å¼€æ”¾è¯æ±‡æ“ä½œç³»ç»Ÿ",
                    "desc": "éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¼€æ”¾è¯æ±‡çš„æœºå™¨äººæ“ä½œç³»ç»Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰æ–¹æ³•å¿½è§†äº†ç‰©ä½“åŠ¨æ€çš„é‡è¦æ€§ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ›´å¤æ‚åŠ¨æ€ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†KUDAï¼Œä¸€ä¸ªé›†æˆäº†åŠ¨æ€å­¦ä¹ å’Œé€šè¿‡å…³é”®ç‚¹è¿›è¡Œè§†è§‰æç¤ºçš„å¼€æ”¾è¯æ±‡æ“ä½œç³»ç»Ÿï¼Œåˆ©ç”¨äº†VLMså’ŒåŸºäºå­¦ä¹ çš„ç¥ç»åŠ¨æ€æ¨¡å‹ã€‚KUDAé€šè¿‡å°†å…³é”®ç‚¹åˆ†é…ç»™RGBå›¾åƒï¼Œå¹¶æŸ¥è¯¢VLMç”Ÿæˆç›®æ ‡è§„èŒƒï¼Œå°†æŠ½è±¡çš„å…³é”®ç‚¹è¡¨ç¤ºè½¬æ¢ä¸ºæˆæœ¬å‡½æ•°ï¼Œä»è€Œä¼˜åŒ–æœºå™¨äººè½¨è¿¹ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-18.html",
    "link_next": "2025-03-20.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "18.03",
        "en": "03/18",
        "zh": "3æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.03",
        "en": "03/20",
        "zh": "3æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚å®ƒæŒ‡å‡ºï¼Œåˆæ ¼çš„ç”Ÿæˆè§†é¢‘å¿…é¡»ç¡®ä¿æƒ…èŠ‚åˆç†æ€§å’Œè¿è´¯æ€§ï¼ŒåŒæ—¶ä¿æŒç‰©ä½“å’Œåœºæ™¯åœ¨ä¸åŒè§†è§’ä¸‹çš„è§†è§‰ä¸€è‡´æ€§ã€‚è¿‡å»çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ—¶é—´æˆ–ç©ºé—´ä¸€è‡´æ€§ä¸Šï¼Œæˆ–è€…å®ƒä»¬çš„åŸºæœ¬ç»“åˆã€‚ç„¶è€Œï¼Œæ‘„åƒæœºç§»åŠ¨å¯èƒ½å¼•å…¥æ–°ç‰©ä½“æˆ–æ¶ˆé™¤ç°æœ‰ç‰©ä½“ï¼Œä»è€Œå½±å“å‰é¢çš„å™è¿°ã€‚æœ¬æ–‡ä»‹ç»å¹¶ç ”ç©¶äº†æ•´ä½“æ—¶ç©ºä¸€è‡´æ€§ï¼Œè€ƒè™‘äº†æƒ…èŠ‚è¿›å±•å’Œæ‘„åƒæŠ€æœ¯ä¹‹é—´çš„ååŒä½œç”¨ï¼Œä»¥åŠå…ˆå‰å†…å®¹å¯¹åç»­ç”Ÿæˆçš„é•¿æœŸå½±å“ã€‚\n\nç ”ç©¶åŒ…æ‹¬æ•°æ®é›†æ„å»ºå’Œæ¨¡å‹å¼€å‘ã€‚é¦–å…ˆï¼Œæ„å»ºäº†åŒ…å«1000ä¸‡ä¸ªè§†é¢‘çš„DropletVideo-10Mæ•°æ®é›†ï¼Œæ¯ä¸ªè§†é¢‘å¹³å‡æœ‰206ä¸ªå­—çš„æè¿°ã€‚ç„¶åï¼Œå¼€å‘å¹¶è®­ç»ƒäº†DropletVideoæ¨¡å‹ï¼Œæ“…é•¿åœ¨è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒæ—¶ç©ºè¿è´¯æ€§ã€‚æ•°æ®é›†å’Œæ¨¡å‹å¯ä»¥åœ¨https://dropletx.github.ioè®¿é—®ã€‚",
        "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le shÃ¬pÇn shÄ“ngchÃ©ng zhÅng de shÃ­kÅng yÄ«zhÃ¬xÃ¬ng. TÄ zhÇchÅ«, hÃ©gÃ© de shÄ“ngchÃ©ng shÃ¬pÇn bÃ¬xÅ« quÃ¨bÇo qÃ­ngjiÃ¨ hÃ©lÇxÃ¬ng hÃ© liÃ¡nhÃ©ngxÃ¬ng, tÃ³ngshÃ­ bÇochÃ­ wÃ¹tÇ hÃ© chÇngjÇng zÃ i bÃ¹tÃ³ng shÃ¬jiÇo xiÃ  de shÃ¬juÃ© yÄ«zhÃ¬xÃ¬ng. GuÃ²qÃ¹ de yÃ¡njiÅ« zhÇ”yÃ o jÃ­zhÃ²ng zÃ i shÃ­jiÄn huÃ² kÅngjiÄn yÄ«zhÃ¬xÃ¬ng shÃ ng, huÃ²zhÄ› tÄmen de jÄ«bÄ›n jiÃ©hÃ©. RÃ¡n'Ã©r, shÃ¨xiÃ ngjÄ« yÃ­dÃ²ng kÄ›nÃ©ng yÇnrÃ¹ xÄ«n wÃ¹tÇ huÃ² xiÄochÃº xiÃ nyÇ’u wÃ¹tÇ, zÃ²ng'Ã©r yÇngxiÇng qiÃ¡nmiÃ n de xÃ¹shÃ¹. BÄ›nwÃ©n jiÃ¨shÃ o bÃ¬ng yÃ¡njiÅ« le zhÄ›ngtÇ shÃ­kÅng yÄ«zhÃ¬xÃ¬ng, kÇolÇœ le qÃ­ngjiÃ¨ jÃ¬nzhÇn hÃ© shÃ¨xiÃ ng jÃ¬shÃ¹ zhÄ«jiÄn de xiÃ©tÃ³ng zuÃ²yÃ²ng, yÇjiÇn qiÃ¡nqiÃ¡n nÃ¨irÃ³ng duÃ¬ hÃ²uxÃ¹ shÄ“ngchÃ©ng de chÃ¡ngqÄ« yÇngxiÇng.\n\nYÃ¡njiÅ« bÄokuÃ² shÃ¹jÃ¹jÃ­ gÃ²uchÃ©ng hÃ© mÃ³xÃ­ng kÄifÄ. ShÇ’uxiÄn, gÃ²uchÃ©ng le bÄohÃ¡n 1000 wÃ n gÃ¨ shÃ¬pÇn de DropletVideo-10M shÃ¹jÃ¹jÃ­, mÄ›i gÃ¨ shÃ¬pÇn pÃ­ngjÅ«n yÇ’u 206 gÃ¨ zÃ¬ de miÃ¡oshÃ¹. RÃ¡nhÃ²u, kÄifÄ hÃ© xÃ¹nliÃ n le DropletVideo mÃ³xÃ­ng, shÃ nchÃ¡ng zÃ i shÃ¬pÇn shÄ“ngchÃ©ng guÃ²chÃ©ng zhÅng bÇochÃ­ shÃ­kÅng liÃ¡nhÃ©ngxÃ¬ng. ShÃ¹jÃ¹jÃ­ hÃ© mÃ³xÃ­ng kÄ›yÇ zÃ i https://dropletx.github.io fÇngwÃ¨n.",
        "vocab": "[{'word': 'è§†é¢‘ç”Ÿæˆ', 'pinyin': 'shÃ¬pÃ­n shÄ“ngchÃ©ng', 'trans': 'video generation'},\n{'word': 'æ—¶ç©ºä¸€è‡´æ€§', 'pinyin': 'shÃ­kÅng yÄ«zhÃ¬xÃ¬ng', 'trans': 'spatiotemporal consistency'},\n{'word': 'åˆæ ¼', 'pinyin': 'hÃ©gÃ©', 'trans': 'qualified'},\n{'word': 'æƒ…èŠ‚åˆç†æ€§', 'pinyin': 'qÃ­ngjiÃ© hÃ©lÇxÃ¬ng', 'trans': 'plot rationality'},\n{'word': 'è¿è´¯æ€§', 'pinyin': 'liÃ¡nhÃ©ngxÃ¬ng', 'trans': 'coherence'},\n{'word': 'è§†è§‰ä¸€è‡´æ€§', 'pinyin': 'shÃ¬juÃ© yÄ«zhÃ¬xÃ¬ng', 'trans': 'visual consistency'},\n{'word': 'æ‘„åƒæœº', 'pinyin': 'shÃ¨xiÃ ngjÄ«', 'trans': 'camera'},\n{'word': 'ååŒä½œç”¨', 'pinyin': 'xiÃ©tÃ³ng zuÃ²yÃ²ng', 'trans': 'synergistic effect'},\n{'word': 'é•¿æœŸå½±å“', 'pinyin': 'chÃ¡ngqÄ« yÇngxiÇng', 'trans': 'long-term impact'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹jÃ¹jÃ­', 'trans': 'dataset'},\n{'word': 'æ¨¡å‹', 'pinyin': 'mÃ³xÃ­ng', 'trans': 'model'},\n{'word': 'æ„å»º', 'pinyin': 'gÃ²ujiÃ n', 'trans': 'construct'},\n{'word': 'è®­ç»ƒ', 'pinyin': 'xÃ¹nliÃ n', 'trans': 'train'},\n{'word': 'è®¿é—®', 'pinyin': 'fÇngwÃ¨n', 'trans': 'access'}]",
        "trans": "This article discusses the spatiotemporal consistency in video generation. It points out that a qualified generated video must ensure the reasonableness and coherence of the plot while maintaining the visual consistency of objects and scenes from different angles. Previous research has mainly focused on temporal or spatial consistency, or their basic combination. However, camera movements may introduce new objects or remove existing ones, affecting the previous narrative. This paper introduces and studies overall spatiotemporal consistency, considering the synergy between plot development and cinematographic techniques, as well as the long-term impact of previous content on subsequent generation.\n\nThe research includes the construction of a dataset and model development. First, the DropletVideo-10M dataset was constructed, containing 10 million videos, each with an average description of 206 words. Then, the DropletVideo model was developed and trained, excelling in maintaining spatiotemporal coherence during the video generation process. The dataset and model can be accessed at https://dropletx.github.io.",
        "update_ts": "2025-03-18 09:12"
    }
}