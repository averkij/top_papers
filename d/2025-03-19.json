{
    "date": {
        "ru": "19 марта",
        "en": "March 19",
        "zh": "3月19日"
    },
    "time_utc": "2025-03-19 02:19",
    "weekday": 2,
    "issue_id": 2776,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.13424",
            "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
            "url": "https://huggingface.co/papers/2503.13424",
            "abstract": "Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and heavy labour of the simulation. In this paper, we propose Infinite Mobility, a novel method for synthesizing high-fidelity articulated objects through procedural generation. User study and quantitative evaluation demonstrate that our method can produce results that excel current state-of-the-art methods and are comparable to human-annotated datasets in both physics property and mesh quality. Furthermore, we show that our synthetic data can be used as training data for generative models, enabling next-step scaling up. Code is available at https://github.com/Intern-Nexus/Infinite-Mobility",
            "score": 5,
            "issue_id": 2776,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 марта",
                "en": "March 17",
                "zh": "3月17日"
            },
            "hash": "20793013b58dba36",
            "authors": [
                "Xinyu Lian",
                "Zichao Yu",
                "Ruiming Liang",
                "Yitong Wang",
                "Li Ray Luo",
                "Kaixu Chen",
                "Yuanzhen Zhou",
                "Qihong Tang",
                "Xudong Xu",
                "Zhaoyang Lyu",
                "Bo Dai",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Fudan University",
                "Harbin Institute of Technology, Shenzhen",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Shanghai Artificial Intelligence Laboratory",
                "South China University of Technology",
                "The University of Hong Kong",
                "Tongji University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13424.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Процедурная генерация сочлененных объектов для воплощенного ИИ",
                    "desc": "Статья представляет новый метод под названием Infinite Mobility для синтеза высококачественных сочлененных объектов с помощью процедурной генерации. Авторы утверждают, что их подход превосходит современные методы и сравним с датасетами, размеченными вручную, по физическим свойствам и качеству полигональных сеток. Метод решает проблему ограниченности существующих подходов, основанных на данных или симуляции. Синтетические данные, полученные с помощью Infinite Mobility, могут использоваться для обучения генеративных моделей, что открывает возможности для масштабирования."
                },
                "en": {
                    "title": "Revolutionizing Articulated Object Creation with Infinite Mobility",
                    "desc": "This paper introduces Infinite Mobility, a new approach for creating high-quality articulated objects using procedural generation techniques. Unlike traditional methods that rely on limited data or labor-intensive simulations, Infinite Mobility generates objects that maintain high fidelity in both physical properties and mesh quality. The authors conducted user studies and quantitative evaluations, showing that their method outperforms existing state-of-the-art techniques and rivals human-annotated datasets. Additionally, the synthetic data produced can be utilized for training generative models, facilitating further advancements in the field of embodied AI."
                },
                "zh": {
                    "title": "无限移动：合成高保真关节物体的新方法",
                    "desc": "在这篇论文中，我们提出了一种名为无限移动（Infinite Mobility）的方法，用于通过程序生成合成高保真度的关节物体。这种方法克服了现有数据驱动或模拟方法在规模和质量上的限制。用户研究和定量评估表明，我们的方法在物理属性和网格质量上超越了当前最先进的方法，并且与人工标注的数据集相当。此外，我们的合成数据可以作为生成模型的训练数据，支持后续的扩展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14125",
            "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
            "url": "https://huggingface.co/papers/2503.14125",
            "abstract": "Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the seesaw effect between gradient vanishing and representation collapse. However, Hyper-Connections increase memory access costs by expanding the width of hidden states. In this paper, we propose Frac-Connections, a novel approach that divides hidden states into multiple parts rather than expanding their width. Frac-Connections retain partial benefits of Hyper-Connections while reducing memory consumption. To validate their effectiveness, we conduct large-scale experiments on language tasks, with the largest being a 7B MoE model trained on up to 3T tokens, demonstrating that Frac-Connections significantly outperform residual connections.",
            "score": 3,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 марта",
                "en": "March 18",
                "zh": "3月18日"
            },
            "hash": "936ea0aa4972c382",
            "authors": [
                "Defa Zhu",
                "Hongzhi Huang",
                "Jundong Zhou",
                "Zihao Huang",
                "Yutao Zeng",
                "Banggu Wu",
                "Qiyang Min",
                "Xun Zhou"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14125.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Frac-Connections: оптимизация глубоких нейросетей без расширения скрытых состояний",
                    "desc": "Статья представляет новый подход к архитектуре глубоких нейронных сетей под названием Frac-Connections. Этот метод развивает идею остаточных соединений (residual connections), разделяя скрытые состояния на несколько частей вместо расширения их ширины. Frac-Connections сохраняют преимущества Hyper-Connections, при этом снижая потребление памяти. Эффективность подхода подтверждена масштабными экспериментами на языковых задачах, включая обучение модели MoE размером 7 миллиардов параметров на 3 триллионах токенов."
                },
                "en": {
                    "title": "Frac-Connections: Efficient Memory for Deep Learning",
                    "desc": "This paper introduces Frac-Connections, a new method that improves upon Hyper-Connections in deep learning. While Hyper-Connections allow for multiple connection strengths to combat issues like gradient vanishing, they also increase memory usage due to wider hidden states. Frac-Connections address this by splitting hidden states into smaller parts, maintaining some advantages of Hyper-Connections while reducing memory costs. The authors demonstrate the effectiveness of Frac-Connections through extensive experiments on language tasks, showing superior performance compared to traditional residual connections."
                },
                "zh": {
                    "title": "Frac-Connections：优化深度学习的内存使用",
                    "desc": "残差连接是现代深度学习架构的核心，能够通过减轻梯度消失问题来训练非常深的网络。超连接最近通过在不同深度引入多个连接强度来推广残差连接，从而解决了梯度消失与表示崩溃之间的摇摆效应。然而，超连接通过扩展隐藏状态的宽度增加了内存访问成本。我们提出了Frac-Connections，这是一种新方法，通过将隐藏状态划分为多个部分而不是扩展其宽度，保留了超连接的部分优势，同时减少了内存消耗。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14456",
            "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
            "url": "https://huggingface.co/papers/2503.14456",
            "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC^0. To demonstrate RWKV-7's language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset.   To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 марта",
                "en": "March 18",
                "zh": "3月18日"
            },
            "hash": "0cd796cef6fa6475",
            "authors": [
                "Bo Peng",
                "Ruichong Zhang",
                "Daniel Goldstein",
                "Eric Alcaide",
                "Haowen Hou",
                "Janna Lu",
                "William Merrill",
                "Guangyu Song",
                "Kaifeng Tan",
                "Saiteja Utpala",
                "Nathan Wilce",
                "Johan S. Wind",
                "Tianyi Wu",
                "Daniel Wuttke",
                "Christian Zhou-Zheng"
            ],
            "affiliations": [
                "Beijing Normal University",
                "Dalle Molle Institute for Artificial Intelligence USI-SUPSI",
                "Denigma",
                "EleutherAI",
                "George Mason University",
                "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
                "New York University",
                "RWKV Project (under Linux Foundation AI & Data)",
                "Recursal AI",
                "Shenzhen University",
                "Tano Labs",
                "Tsinghua University",
                "University of Oslo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14456.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#multilingual"
                ],
                "emoji": "🦢",
                "ru": {
                    "title": "RWKV-7: Эффективная архитектура для многоязычного моделирования последовательностей",
                    "desc": "RWKV-7 'Goose' - это новая архитектура моделирования последовательностей, которая устанавливает новый state-of-the-art в производительности при 3 миллиардах параметров на многоязычных задачах. Модель требует постоянного использования памяти и времени вывода на токен, вводит обобщенную формулировку правила дельты с векторным гейтингом и обучением в контексте. RWKV-7 способна отслеживать состояния и распознавать все регулярные языки, превосходя возможности трансформеров. Авторы также представляют многоязычный корпус из 3,1 триллиона токенов и обучают на нем четыре модели RWKV-7 размером от 0,19 до 2,9 миллиардов параметров."
                },
                "en": {
                    "title": "RWKV-7: Efficient Multilingual Mastery with Fewer Parameters",
                    "desc": "RWKV-7 \"Goose\" is a novel sequence modeling architecture that achieves state-of-the-art performance on multilingual tasks with only 3 billion parameters. It utilizes a unique formulation of the delta rule and vector-valued gating, allowing for efficient memory usage and constant inference time per token. The model demonstrates advanced capabilities such as state tracking and recognition of regular languages, surpassing traditional Transformers in complexity. Additionally, RWKV-7 is trained on a large multilingual corpus and is made openly available for further research and development."
                },
                "zh": {
                    "title": "RWKV-7：多语言任务的新突破",
                    "desc": "RWKV-7 \"Goose\" 是一种新的序列建模架构，具有3亿参数的预训练语言模型，在多语言任务中达到了新的最先进水平。与其他顶级3B模型相比，RWKV-7在训练时使用的标记数量显著减少，但仍能与当前的英语语言性能相匹配。该模型采用了新的广义增量规则，结合了向量值门控和上下文学习率，同时保持了训练的并行性。RWKV-7能够进行状态跟踪并识别所有正规语言，超越了标准复杂性猜想下的Transformer的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14378",
            "title": "Impossible Videos",
            "url": "https://huggingface.co/papers/2503.14378",
            "abstract": "Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can today's video generation models effectively follow prompts to create impossible video content? 2) Are today's video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models.",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 марта",
                "en": "March 18",
                "zh": "3月18日"
            },
            "hash": "3334aa74b743ac8d",
            "authors": [
                "Zechen Bai",
                "Hai Ci",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14378.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "IPV-Bench: Новый рубеж в понимании и генерации невозможных видео",
                    "desc": "Эта статья представляет IPV-Bench - новый бенчмарк для оценки и развития понимания и генерации видео. Он основан на таксономии, охватывающей 4 домена и 14 категорий, и включает разнообразные сцены, нарушающие физические, биологические, географические или социальные законы. Бенчмарк содержит набор промптов для оценки моделей генерации видео и видеоданные для оценки способности Video-LLM понимать невозможные видео. Комплексная оценка выявляет ограничения и предоставляет идеи для будущих направлений развития видеомоделей."
                },
                "en": {
                    "title": "Exploring the Impossible: Advancing Video Generation and Understanding",
                    "desc": "This paper addresses the limitations of current synthetic video datasets, which mainly focus on replicating real-world scenarios. It introduces IPV-Bench, a new benchmark designed to evaluate video generation and understanding models on their ability to create and comprehend impossible video content. The benchmark includes a detailed taxonomy with various categories that challenge models to generate videos that defy physical and social laws. The findings highlight the current capabilities and shortcomings of video models, providing insights for future advancements in the field."
                },
                "zh": {
                    "title": "探索不可能视频的生成与理解",
                    "desc": "本论文探讨了合成视频在数据稀缺和多样性方面的应用。当前的合成数据集主要复制现实场景，而对不可能、反事实和反现实的视频概念研究不足。我们提出了IPV-Bench，这是一个新颖的基准，旨在评估和促进视频理解与生成的进展。该基准涵盖了多种违反物理、生物、地理或社会法则的场景，并通过构建提示套件来挑战视频生成模型的创造力和提示跟随能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12545",
            "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2503.12545",
            "abstract": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant concerns about privacy and security. To address these issues, machine unlearning (MU) has emerged as a promising solution, enabling the removal of specific knowledge from an already trained model without requiring retraining from scratch. Although MU for MLLMs has gained attention, current evaluations of its efficacy remain incomplete, and the underlying problem is often poorly defined, which hinders the development of strategies for creating more secure and trustworthy systems. To bridge this gap, we introduce a benchmark, named PEBench, which includes a dataset of personal entities and corresponding general event scenes, designed to comprehensively assess the performance of MU for MLLMs. Through PEBench, we aim to provide a standardized and robust framework to advance research in secure and privacy-preserving multimodal models. We benchmarked 6 MU methods, revealing their strengths and limitations, and shedding light on key challenges and opportunities for MU in MLLMs.",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 марта",
                "en": "March 16",
                "zh": "3月16日"
            },
            "hash": "8a908fbc8ce24853",
            "authors": [
                "Zhaopan Xu",
                "Pengfei Zhou",
                "Weidong Tang",
                "Jiaxin Ai",
                "Wangbo Zhao",
                "Xiaojiang Peng",
                "Kai Wang",
                "Yang You",
                "Wenqi Shao",
                "Hongxun Yao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HIT",
                "NUS",
                "SZTU",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "XDU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12545.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#ethics",
                    "#security",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🔒",
                "ru": {
                    "title": "PEBench: Новый стандарт для оценки безопасности мультимодальных ИИ-моделей",
                    "desc": "Статья представляет новый бенчмарк PEBench для оценки эффективности методов машинного разобучения (MU) в мультимодальных больших языковых моделях (MLLM). PEBench включает набор данных с личными сущностями и соответствующими общими сценами событий. Авторы протестировали 6 методов MU, выявив их сильные и слабые стороны. Исследование направлено на продвижение разработки безопасных и конфиденциальных мультимодальных моделей."
                },
                "en": {
                    "title": "Enhancing Privacy in Multimodal Models with Machine Unlearning",
                    "desc": "This paper discusses the advancements of Multimodal Large Language Models (MLLMs) in tasks like visual question answering and reasoning, while highlighting the privacy concerns associated with their data usage. It introduces machine unlearning (MU) as a method to selectively remove knowledge from these models without complete retraining. The authors present PEBench, a benchmark designed to evaluate MU methods specifically for MLLMs, using a dataset of personal entities and general event scenes. By benchmarking six MU techniques, the study identifies their strengths and weaknesses, aiming to enhance the security and trustworthiness of multimodal systems."
                },
                "zh": {
                    "title": "推动多模态模型的安全与隐私保护",
                    "desc": "近年来，多模态大型语言模型（MLLMs）在视觉问答、视觉理解和推理等任务上取得了显著进展。然而，这些进展依赖于从互联网收集的大量数据，这引发了隐私和安全方面的重大担忧。为了解决这些问题，机器遗忘（MU）作为一种有前景的解决方案应运而生，能够在不需要从头开始重新训练的情况下，从已训练的模型中删除特定知识。我们引入了一个基准测试PEBench，旨在全面评估MU在MLLMs中的表现，推动安全和隐私保护的多模态模型研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12505",
            "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification",
            "url": "https://huggingface.co/papers/2503.12505",
            "abstract": "Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs.",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 марта",
                "en": "March 16",
                "zh": "3月16日"
            },
            "hash": "0ac4fdd3411855ac",
            "authors": [
                "Zhaopan Xu",
                "Pengfei Zhou",
                "Jiaxin Ai",
                "Wangbo Zhao",
                "Kai Wang",
                "Xiaojiang Peng",
                "Wenqi Shao",
                "Hongxun Yao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HIT",
                "NUS",
                "SZTU",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12505.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MPBench: Комплексная оценка рассуждений языковых моделей",
                    "desc": "Статья представляет новый бенчмарк MPBench для оценки эффективности моделей вознаграждения на уровне процесса (PRM) в различных сценариях рассуждений. MPBench включает три парадигмы оценки: корректность шагов, агрегацию ответов и поиск процесса рассуждений. Бенчмарк охватывает мультимодальные задачи и позволяет всесторонне оценить PRM в контексте рассуждений языковых моделей. Это помогает улучшить способности больших языковых моделей (LLM) к рассуждениям и выявлению ошибок в процессе."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs with MPBench",
                    "desc": "This paper discusses the importance of reasoning in large language models (LLMs) and introduces process-level reward models (PRMs) to enhance their reasoning capabilities. PRMs provide step-wise rewards that help LLMs learn from their mistakes during training and improve their decision-making during inference. The authors identify a gap in existing benchmarks, which primarily focus on error detection, and propose MPBench, a new benchmark that evaluates PRMs across various reasoning scenarios. MPBench includes three evaluation paradigms: assessing step correctness, aggregating answers, and guiding the reasoning process search, offering a comprehensive framework for understanding PRMs' effectiveness."
                },
                "zh": {
                    "title": "全面评估推理过程的多模态基准",
                    "desc": "推理是大型语言模型（LLMs）处理复杂任务的重要能力，而识别过程错误对于提升这一能力至关重要。最近提出的过程级奖励模型（PRMs）通过提供逐步奖励，促进了强化学习和数据生成，从而在推理过程中引导LLMs走向正确的步骤，提高推理准确性。然而，现有的PRMs基准主要基于文本，专注于错误检测，忽视了推理搜索等其他场景。为了解决这一问题，我们引入了MPBench，这是一个全面的多任务多模态基准，旨在系统评估PRMs在不同场景中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10410",
            "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation",
            "url": "https://huggingface.co/papers/2503.10410",
            "abstract": "Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration errors, sparse information, and multi-view consistency, leading to poor performance on recent published datasets. To significantly enhance roadside collaborative perception and address critical data issues, we present the first simulation framework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable of generating diverse, multi-view consistent simulated roadside data through dynamic foreground editing and full-scene style transfer of a single image. RoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures accurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View Occlusion-Aware Sampler (MOAS) determines the placement of diverse digital assets within 3D space; (3) DepthSAM innovatively models foreground-background relationships from single-frame fixed-view images, ensuring multi-view consistency of foreground; and (4) Scalable Post-Processing Toolkit generates more realistic and enriched scenes through style transfer and other enhancements. RoCo-Sim significantly improves roadside 3D object detection, outperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on TUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception simulation. Code and pre-trained models will be released soon: https://github.com/duyuwen-duen/RoCo-Sim",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "44150f611040e79d",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#data",
                    "#dataset",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "RoCo-Sim: прорыв в симуляции дорожного восприятия",
                    "desc": "RoCo-Sim - это первая система симуляции для совместного восприятия дорожной обстановки. Она решает проблемы калибровки, разреженности данных и мультиракурсной согласованности путем генерации синтетических данных. RoCo-Sim включает оптимизацию внешних параметров камер, многоракурсный выборщик с учетом окклюзий, моделирование отношений передний план-фон и инструменты постобработки. Система значительно улучшает 3D-детектирование объектов, превосходя современные методы на популярных наборах данных."
                },
                "en": {
                    "title": "Enhancing Roadside Awareness with Collaborative Perception",
                    "desc": "This paper introduces Roadside Collaborative Perception, a system where multiple roadside units work together to improve vehicle awareness of their surroundings. It highlights the limitations of current methods that focus mainly on model design while neglecting important data issues such as calibration errors and sparse information. To address these challenges, the authors present RoCo-Sim, a simulation framework that generates diverse and consistent roadside data using advanced techniques like dynamic foreground editing. RoCo-Sim significantly enhances roadside 3D object detection performance, surpassing state-of-the-art methods on benchmark datasets."
                },
                "zh": {
                    "title": "提升路边感知的协同力量",
                    "desc": "路边协同感知是一种系统，多个路边单元协作汇聚感知数据，帮助车辆提高环境意识。现有的路边感知方法主要关注模型设计，但忽视了数据问题，如校准误差、信息稀疏和多视图一致性，导致在最新数据集上的表现不佳。为显著提升路边协同感知并解决关键数据问题，我们提出了首个路边协同感知模拟框架RoCo-Sim。RoCo-Sim能够通过动态前景编辑和单图像的全场景风格迁移生成多样化的、多视图一致的模拟路边数据。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10546",
            "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
            "url": "https://huggingface.co/papers/2503.10546",
            "abstract": "With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at http://kuda-dynamics.github.io.",
            "score": 0,
            "issue_id": 2776,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 марта",
                "en": "March 13",
                "zh": "3月13日"
            },
            "hash": "f856affbe8bcf064",
            "authors": [
                "Zixian Liu",
                "Mingtong Zhang",
                "Yunzhu Li"
            ],
            "affiliations": [
                "Columbia University",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10546.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#robotics",
                    "#open_source",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "KUDA: Динамическое планирование для роботов с открытым словарем",
                    "desc": "KUDA - это система манипуляции с открытым словарем, которая объединяет обучение динамике и визуальное управление через ключевые точки. Она использует модели видео-языкового взаимодействия (VLM) и нейронные модели динамики для планирования траекторий робота. KUDA сначала назначает ключевые точки на RGB-изображении и запрашивает VLM для генерации целевых спецификаций. Затем эти абстрактные представления на основе ключевых точек преобразуются в функции стоимости, которые оптимизируются с помощью обученной модели динамики."
                },
                "en": {
                    "title": "KUDA: Bridging Language and Dynamics for Robotic Manipulation",
                    "desc": "This paper presents KUDA, an innovative open-vocabulary robotic manipulation system that incorporates dynamics learning and visual prompting. Unlike previous methods, KUDA utilizes keypoints to create target specifications that are understandable by vision-language models (VLMs) and can be transformed into cost functions for planning. The system processes language instructions and visual data to assign keypoints to images, which are then optimized using a learned dynamics model to generate robotic movements. The effectiveness of KUDA is validated through various manipulation tasks, showcasing its ability to handle complex interactions with different object types."
                },
                "zh": {
                    "title": "KUDA：动态学习与视觉提示的开放词汇操作系统",
                    "desc": "随着大语言模型（LLMs）和视觉语言模型（VLMs）的快速发展，开放词汇的机器人操作系统取得了显著进展。然而，许多现有方法忽视了物体动态的重要性，限制了它们在更复杂动态任务中的适用性。我们提出了KUDA，一个集成了动态学习和通过关键点进行视觉提示的开放词汇操作系统，利用了VLMs和基于学习的神经动态模型。KUDA通过将关键点分配给RGB图像，并查询VLM生成目标规范，将抽象的关键点表示转换为成本函数，从而优化机器人轨迹。"
                }
            }
        }
    ],
    "link_prev": "2025-03-18.html",
    "link_next": "2025-03-20.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "18.03",
        "en": "03/18",
        "zh": "3月18日"
    },
    "short_date_next": {
        "ru": "20.03",
        "en": "03/20",
        "zh": "3月20日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了视频生成中的时空一致性。它指出，合格的生成视频必须确保情节合理性和连贯性，同时保持物体和场景在不同视角下的视觉一致性。过去的研究主要集中在时间或空间一致性上，或者它们的基本结合。然而，摄像机移动可能引入新物体或消除现有物体，从而影响前面的叙述。本文介绍并研究了整体时空一致性，考虑了情节进展和摄像技术之间的协同作用，以及先前内容对后续生成的长期影响。\n\n研究包括数据集构建和模型开发。首先，构建了包含1000万个视频的DropletVideo-10M数据集，每个视频平均有206个字的描述。然后，开发并训练了DropletVideo模型，擅长在视频生成过程中保持时空连贯性。数据集和模型可以在https://dropletx.github.io访问。",
        "title": "DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation",
        "pinyin": "Zhè piān wénzhāng tǎolùn le shìpǐn shēngchéng zhōng de shíkōng yīzhìxìng. Tā zhǐchū, hégé de shēngchéng shìpǐn bìxū quèbǎo qíngjiè hélǐxìng hé liánhéngxìng, tóngshí bǎochí wùtǐ hé chǎngjǐng zài bùtóng shìjiǎo xià de shìjué yīzhìxìng. Guòqù de yánjiū zhǔyào jízhòng zài shíjiān huò kōngjiān yīzhìxìng shàng, huòzhě tāmen de jīběn jiéhé. Rán'ér, shèxiàngjī yídòng kěnéng yǐnrù xīn wùtǐ huò xiāochú xiànyǒu wùtǐ, zòng'ér yǐngxiǎng qiánmiàn de xùshù. Běnwén jièshào bìng yánjiū le zhěngtǐ shíkōng yīzhìxìng, kǎolǜ le qíngjiè jìnzhǎn hé shèxiàng jìshù zhījiān de xiétóng zuòyòng, yǐjiǎn qiánqián nèiróng duì hòuxù shēngchéng de chángqī yǐngxiǎng.\n\nYánjiū bāokuò shùjùjí gòuchéng hé móxíng kāifā. Shǒuxiān, gòuchéng le bāohán 1000 wàn gè shìpǐn de DropletVideo-10M shùjùjí, měi gè shìpǐn píngjūn yǒu 206 gè zì de miáoshù. Ránhòu, kāifā hé xùnliàn le DropletVideo móxíng, shàncháng zài shìpǐn shēngchéng guòchéng zhōng bǎochí shíkōng liánhéngxìng. Shùjùjí hé móxíng kěyǐ zài https://dropletx.github.io fǎngwèn.",
        "vocab": "[{'word': '视频生成', 'pinyin': 'shìpín shēngchéng', 'trans': 'video generation'},\n{'word': '时空一致性', 'pinyin': 'shíkōng yīzhìxìng', 'trans': 'spatiotemporal consistency'},\n{'word': '合格', 'pinyin': 'hégé', 'trans': 'qualified'},\n{'word': '情节合理性', 'pinyin': 'qíngjié hélǐxìng', 'trans': 'plot rationality'},\n{'word': '连贯性', 'pinyin': 'liánhéngxìng', 'trans': 'coherence'},\n{'word': '视觉一致性', 'pinyin': 'shìjué yīzhìxìng', 'trans': 'visual consistency'},\n{'word': '摄像机', 'pinyin': 'shèxiàngjī', 'trans': 'camera'},\n{'word': '协同作用', 'pinyin': 'xiétóng zuòyòng', 'trans': 'synergistic effect'},\n{'word': '长期影响', 'pinyin': 'chángqī yǐngxiǎng', 'trans': 'long-term impact'},\n{'word': '数据集', 'pinyin': 'shùjùjí', 'trans': 'dataset'},\n{'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'},\n{'word': '构建', 'pinyin': 'gòujiàn', 'trans': 'construct'},\n{'word': '训练', 'pinyin': 'xùnliàn', 'trans': 'train'},\n{'word': '访问', 'pinyin': 'fǎngwèn', 'trans': 'access'}]",
        "trans": "This article discusses the spatiotemporal consistency in video generation. It points out that a qualified generated video must ensure the reasonableness and coherence of the plot while maintaining the visual consistency of objects and scenes from different angles. Previous research has mainly focused on temporal or spatial consistency, or their basic combination. However, camera movements may introduce new objects or remove existing ones, affecting the previous narrative. This paper introduces and studies overall spatiotemporal consistency, considering the synergy between plot development and cinematographic techniques, as well as the long-term impact of previous content on subsequent generation.\n\nThe research includes the construction of a dataset and model development. First, the DropletVideo-10M dataset was constructed, containing 10 million videos, each with an average description of 206 words. Then, the DropletVideo model was developed and trained, excelling in maintaining spatiotemporal coherence during the video generation process. The dataset and model can be accessed at https://dropletx.github.io.",
        "update_ts": "2025-03-18 09:12"
    }
}