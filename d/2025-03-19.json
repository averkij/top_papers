{
    "date": {
        "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 19",
        "zh": "3æœˆ19æ—¥"
    },
    "time_utc": "2025-03-19 09:12",
    "weekday": 2,
    "issue_id": 2783,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.14456",
            "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
            "url": "https://huggingface.co/papers/2503.14456",
            "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC^0. To demonstrate RWKV-7's language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset.   To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.",
            "score": 59,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "0cd796cef6fa6475",
            "authors": [
                "Bo Peng",
                "Ruichong Zhang",
                "Daniel Goldstein",
                "Eric Alcaide",
                "Haowen Hou",
                "Janna Lu",
                "William Merrill",
                "Guangyu Song",
                "Kaifeng Tan",
                "Saiteja Utpala",
                "Nathan Wilce",
                "Johan S. Wind",
                "Tianyi Wu",
                "Daniel Wuttke",
                "Christian Zhou-Zheng"
            ],
            "affiliations": [
                "Beijing Normal University",
                "Dalle Molle Institute for Artificial Intelligence USI-SUPSI",
                "Denigma",
                "EleutherAI",
                "George Mason University",
                "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
                "New York University",
                "RWKV Project (under Linux Foundation AI & Data)",
                "Recursal AI",
                "Shenzhen University",
                "Tano Labs",
                "Tsinghua University",
                "University of Oslo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14456.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#multilingual"
                ],
                "emoji": "ğŸ¦¢",
                "ru": {
                    "title": "RWKV-7: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "RWKV-7 'Goose' - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½, Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ´ĞµĞ»ÑŒÑ‚Ñ‹ Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. RWKV-7 ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¸Ğ· 3,1 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¼ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ RWKV-7 Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 0,19 Ğ´Ğ¾ 2,9 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "RWKV-7: Efficient Multilingual Mastery with Fewer Parameters",
                    "desc": "RWKV-7 \"Goose\" is a novel sequence modeling architecture that achieves state-of-the-art performance on multilingual tasks with only 3 billion parameters. It utilizes a unique formulation of the delta rule and vector-valued gating, allowing for efficient memory usage and constant inference time per token. The model demonstrates advanced capabilities such as state tracking and recognition of regular languages, surpassing traditional Transformers in complexity. Additionally, RWKV-7 is trained on a large multilingual corpus and is made openly available for further research and development."
                },
                "zh": {
                    "title": "RWKV-7ï¼šå¤šè¯­è¨€ä»»åŠ¡çš„æ–°çªç ´",
                    "desc": "RWKV-7 \"Goose\" æ˜¯ä¸€ç§æ–°çš„åºåˆ—å»ºæ¨¡æ¶æ„ï¼Œå…·æœ‰3äº¿å‚æ•°çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œåœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚ä¸å…¶ä»–é¡¶çº§3Bæ¨¡å‹ç›¸æ¯”ï¼ŒRWKV-7åœ¨è®­ç»ƒæ—¶ä½¿ç”¨çš„æ ‡è®°æ•°é‡æ˜¾è‘—å‡å°‘ï¼Œä½†ä»èƒ½ä¸å½“å‰çš„è‹±è¯­è¯­è¨€æ€§èƒ½ç›¸åŒ¹é…ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†æ–°çš„å¹¿ä¹‰å¢é‡è§„åˆ™ï¼Œç»“åˆäº†å‘é‡å€¼é—¨æ§å’Œä¸Šä¸‹æ–‡å­¦ä¹ ç‡ï¼ŒåŒæ—¶ä¿æŒäº†è®­ç»ƒçš„å¹¶è¡Œæ€§ã€‚RWKV-7èƒ½å¤Ÿè¿›è¡ŒçŠ¶æ€è·Ÿè¸ªå¹¶è¯†åˆ«æ‰€æœ‰æ­£è§„è¯­è¨€ï¼Œè¶…è¶Šäº†æ ‡å‡†å¤æ‚æ€§çŒœæƒ³ä¸‹çš„Transformerçš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14378",
            "title": "Impossible Videos",
            "url": "https://huggingface.co/papers/2503.14378",
            "abstract": "Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can today's video generation models effectively follow prompts to create impossible video content? 2) Are today's video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models.",
            "score": 36,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "3334aa74b743ac8d",
            "authors": [
                "Zechen Bai",
                "Hai Ci",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14378.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "IPV-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ IPV-Bench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¹ 4 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸ 14 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹, Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ½Ğ°Ñ€ÑƒÑˆĞ°ÑÑ‰Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ»Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Video-LLM Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ´ĞµĞ¸ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Exploring the Impossible: Advancing Video Generation and Understanding",
                    "desc": "This paper addresses the limitations of current synthetic video datasets, which mainly focus on replicating real-world scenarios. It introduces IPV-Bench, a new benchmark designed to evaluate video generation and understanding models on their ability to create and comprehend impossible video content. The benchmark includes a detailed taxonomy with various categories that challenge models to generate videos that defy physical and social laws. The findings highlight the current capabilities and shortcomings of video models, providing insights for future advancements in the field."
                },
                "zh": {
                    "title": "æ¢ç´¢ä¸å¯èƒ½è§†é¢‘çš„ç”Ÿæˆä¸ç†è§£",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åˆæˆè§†é¢‘åœ¨æ•°æ®ç¨€ç¼ºå’Œå¤šæ ·æ€§æ–¹é¢çš„åº”ç”¨ã€‚å½“å‰çš„åˆæˆæ•°æ®é›†ä¸»è¦å¤åˆ¶ç°å®åœºæ™¯ï¼Œè€Œå¯¹ä¸å¯èƒ½ã€åäº‹å®å’Œåç°å®çš„è§†é¢‘æ¦‚å¿µç ”ç©¶ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†IPV-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œä¿ƒè¿›è§†é¢‘ç†è§£ä¸ç”Ÿæˆçš„è¿›å±•ã€‚è¯¥åŸºå‡†æ¶µç›–äº†å¤šç§è¿åç‰©ç†ã€ç”Ÿç‰©ã€åœ°ç†æˆ–ç¤¾ä¼šæ³•åˆ™çš„åœºæ™¯ï¼Œå¹¶é€šè¿‡æ„å»ºæç¤ºå¥—ä»¶æ¥æŒ‘æˆ˜è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„åˆ›é€ åŠ›å’Œæç¤ºè·Ÿéšèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14478",
            "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
            "url": "https://huggingface.co/papers/2503.14478",
            "abstract": "Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLM's creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on https://github.com/open-compass/Creation-MMBench.",
            "score": 32,
            "issue_id": 2777,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "120f1d8ec2eb88a8",
            "authors": [
                "Xinyu Fang",
                "Zhijian Chen",
                "Kai Lan",
                "Shengyuan Ding",
                "Yingji Liang",
                "Xiangyu Zhao",
                "Farong Wen",
                "Zicheng Zhang",
                "Guofeng Zhang",
                "Haodong Duan",
                "Kai Chen",
                "Dahua Lin"
            ],
            "affiliations": [
                "East China Normal University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "The Chinese University of Hong Kong",
                "Tongji University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14478.jpg",
            "data": {
                "categories": [
                    "#creativity",
                    "#open_source",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ÑÑ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Creation-MMBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 765 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 51 Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ñ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ MLLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ² Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unlocking Creativity in Multimodal AI with Creation-MMBench",
                    "desc": "This paper introduces Creation-MMBench, a new benchmark designed to evaluate the creative capabilities of Multimodal Large Language Models (MLLMs) in tasks that involve both text and images. The benchmark includes 765 test cases across 51 specific tasks, each with tailored evaluation criteria to assess the quality of responses and their consistency with visual inputs. Experimental findings indicate that current open-source MLLMs perform poorly in creative tasks compared to proprietary models, and that visual fine-tuning may hinder the creative performance of base LLMs. Creation-MMBench aims to enhance understanding and development of MLLM creativity, providing a resource for future research in multimodal generative intelligence."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ›é€ åŠ›",
                    "desc": "åˆ›é€ åŠ›æ˜¯æ™ºèƒ½çš„ä¸€ä¸ªåŸºæœ¬æ–¹é¢ï¼Œæ¶‰åŠåœ¨ä¸åŒæƒ…å¢ƒä¸­ç”Ÿæˆæ–°é¢–ä¸”é€‚å½“çš„è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ›é€ èƒ½åŠ›æ–¹é¢å¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ï¼Œä½†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¯„ä¼°ä»ç„¶ç›¸å¯¹ç¼ºä¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Creation-MMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsåœ¨åŸºäºå›¾åƒçš„å®é™…ä»»åŠ¡ä¸­åˆ›é€ èƒ½åŠ›çš„å¤šæ¨¡æ€åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å¼€æºMLLMsåœ¨åˆ›é€ æ€§ä»»åŠ¡ä¸­æ˜¾è‘—ä½äºä¸“æœ‰æ¨¡å‹ï¼Œè€Œè§†è§‰å¾®è°ƒå¯èƒ½ä¼šå¯¹åŸºç¡€LLMçš„åˆ›é€ èƒ½åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14476",
            "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
            "url": "https://huggingface.co/papers/2503.14476",
            "abstract": "Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.",
            "score": 20,
            "issue_id": 2777,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "5b4841d2845817e8",
            "authors": [
                "Qiying Yu",
                "Zheng Zhang",
                "Ruofei Zhu",
                "Yufeng Yuan",
                "Xiaochen Zuo",
                "Yu Yue",
                "Tiantian Fan",
                "Gaohong Liu",
                "Lingjun Liu",
                "Xin Liu",
                "Haibin Lin",
                "Zhiqi Lin",
                "Bole Ma",
                "Guangming Sheng",
                "Yuxuan Tong",
                "Chi Zhang",
                "Mofan Zhang",
                "Wang Zhang",
                "Hang Zhu",
                "Jinhua Zhu",
                "Jiaze Chen",
                "Jiangjie Chen",
                "Chengyi Wang",
                "Hongli Yu",
                "Weinan Dai",
                "Yuxuan Song",
                "Xiangpeng Wei",
                "Hao Zhou",
                "Jingjing Liu",
                "Wei-Ying Ma",
                "Ya-Qin Zhang",
                "Lin Yan",
                "Mu Qiao",
                "Yonghui Wu",
                "Mingxuan Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Institute for AI Industry Research (AIR), Tsinghua University",
                "SIA-Lab of Tsinghua AIR and ByteDance",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14476.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° RL Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ DAPO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰ÑƒÑ 50 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ² Ğ½Ğ° AIME 2024 Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-32B. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ´ĞµĞ»Ğ°ÑÑ‰Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğ¼ RL Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¯Ğœ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Open-Source Reinforcement Learning",
                    "desc": "This paper introduces the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, which enhances the reasoning capabilities of large language models (LLMs) through reinforcement learning (RL). The authors address the lack of transparency in existing state-of-the-art LLMs by providing detailed insights into their training processes and techniques. They report achieving a significant performance score of 50 points on the AIME 2024 benchmark using the Qwen2.5-32B model. By open-sourcing their training code and dataset, they aim to improve reproducibility and foster further advancements in large-scale LLM reinforcement learning research."
                },
                "zh": {
                    "title": "è§£é”å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ½œåŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œç§°ä¸ºè§£è€¦å‰ªè¾‘å’ŒåŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼ŒæˆåŠŸå®ç°äº†åœ¨AIME 2024ä¸Šè·å¾—50åˆ†çš„æˆç»©ï¼Œä½¿ç”¨çš„æ˜¯Qwen2.5-32BåŸºç¡€æ¨¡å‹ã€‚ä¸ä»¥å¾€ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬å…¬å¼€äº†å››ä¸ªå…³é”®æŠ€æœ¯ç»†èŠ‚ï¼Œå¸®åŠ©ç¤¾åŒºæ›´å¥½åœ°ç†è§£å’Œå¤ç°æˆ‘ä»¬çš„è®­ç»ƒç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€æºäº†è®­ç»ƒä»£ç å’Œç»è¿‡ç²¾å¿ƒå¤„ç†çš„æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›å¤§å‹LLMå¼ºåŒ–å­¦ä¹ çš„å¯é‡å¤æ€§å’Œæœªæ¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12797",
            "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding",
            "url": "https://huggingface.co/papers/2503.12797",
            "abstract": "Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on KVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception.",
            "score": 19,
            "issue_id": 2777,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "c682a086aaac0fa1",
            "authors": [
                "Xinyu Ma",
                "Ziyang Ding",
                "Zhicong Luo",
                "Chi Chen",
                "Zonghao Guo",
                "Derek F. Wong",
                "Xiaoyi Feng",
                "Maosong Sun"
            ],
            "affiliations": [
                "Northwestern Polytechnical University",
                "Shandong University",
                "Tsinghua University",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12797.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#data",
                    "#dataset",
                    "#reasoning",
                    "#synthetic",
                    "#benchmark",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "DeepPerception: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ knowledge-intensive visual grounding (KVG), Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ÑƒÑ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DeepPerception, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ MLLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ KVG-Bench Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Visual Perception in MLLMs with DeepPerception",
                    "desc": "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in visual perception and reasoning. It introduces a new task called knowledge-intensive visual grounding (KVG), which combines fine-grained visual discrimination with domain-specific knowledge. The authors propose DeepPerception, an enhanced MLLM that utilizes a data synthesis pipeline and a two-stage training framework to improve cognitive reasoning and perception integration. Experimental results show that DeepPerception outperforms existing methods, demonstrating the potential for MLLMs to achieve human-like visual understanding."
                },
                "zh": {
                    "title": "æå‡è§†è§‰æ„ŸçŸ¥çš„è®¤çŸ¥æ•´åˆèƒ½åŠ›",
                    "desc": "äººç±»ä¸“å®¶åœ¨ç»†ç²’åº¦è§†è§‰è¾¨åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåˆ©ç”¨é¢†åŸŸçŸ¥è¯†æ¥ä¼˜åŒ–æ„ŸçŸ¥ç‰¹å¾ï¼Œè€Œå½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¿™æ–¹é¢ä»æ˜¾ä¸è¶³ã€‚å°½ç®¡æ‹¥æœ‰ä¸°å¯Œçš„ä¸“å®¶çº§çŸ¥è¯†ï¼ŒMLLMsåœ¨è§†è§‰æ„ŸçŸ¥ä¸­æ•´åˆæ¨ç†çš„èƒ½åŠ›è¾ƒå¼±ï¼Œå¸¸å¸¸ç›´æ¥ç”Ÿæˆå“åº”è€Œç¼ºä¹æ·±å…¥åˆ†æã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†å¯†é›†å‹è§†è§‰å®šä½ï¼ˆKVGï¼‰ï¼Œè¿™æ˜¯ä¸€é¡¹æ–°é¢–çš„è§†è§‰å®šä½ä»»åŠ¡ï¼Œè¦æ±‚åŒæ—¶å…·å¤‡ç»†ç²’åº¦æ„ŸçŸ¥å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†çš„æ•´åˆã€‚æˆ‘ä»¬æå‡ºçš„DeepPerceptionæ¨¡å‹å¢å¼ºäº†è®¤çŸ¥è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®åˆæˆå’Œä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ˜¾è‘—æé«˜äº†åœ¨KVG-Benchæ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12329",
            "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era",
            "url": "https://huggingface.co/papers/2503.12329",
            "abstract": "Image captioning has been a longstanding challenge in vision-language research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed and comprehensive image descriptions. However, benchmarking the quality of such captions remains unresolved. This paper addresses two key questions: (1) How well do current VLMs actually perform on image captioning, particularly compared to humans? We built CapArena, a platform with over 6000 pairwise caption battles and high-quality human preference votes. Our arena-style evaluation marks a milestone, showing that leading models like GPT-4o achieve or even surpass human performance, while most open-source models lag behind. (2) Can automated metrics reliably assess detailed caption quality? Using human annotations from CapArena, we evaluate traditional and recent captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while some metrics (e.g., METEOR) show decent caption-level agreement with humans, their systematic biases lead to inconsistencies in model ranking. In contrast, VLM-as-a-Judge demonstrates robust discernment at both the caption and model levels. Building on these insights, we release CapArena-Auto, an accurate and efficient automated benchmark for detailed captioning, achieving 94.3% correlation with human rankings at just $4 per test. Data and resources will be open-sourced at https://caparena.github.io.",
            "score": 17,
            "issue_id": 2778,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "c97a8c730bfcbfa8",
            "authors": [
                "Kanzhi Cheng",
                "Wenpo Song",
                "Jiaxin Fan",
                "Zheng Ma",
                "Qiushi Sun",
                "Fangzhi Xu",
                "Chenyang Yan",
                "Nuo Chen",
                "Jianbing Zhang",
                "Jiajun Chen"
            ],
            "affiliations": [
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "Shanghai Artificial Intelligence Laboratory",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12329.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#games",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "ğŸ“¸",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼: Ğ˜Ğ˜ Ğ´Ğ¾Ğ³Ğ¾Ğ½ÑĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ CapArena Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ², Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, GPT-4o) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ CapArena-Auto - Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Elevating Image Captioning: Human-Level Performance and Reliable Metrics",
                    "desc": "This paper tackles the challenge of evaluating image captioning performance in Vision-Language Models (VLMs). It introduces CapArena, a platform that conducts over 6000 pairwise caption battles to compare VLM outputs with human-generated captions. The findings indicate that advanced models like GPT-4o can match or exceed human performance, while many open-source models do not. Additionally, the study assesses various automated metrics for caption quality, revealing that while some metrics align with human preferences, VLM-as-a-Judge provides a more reliable evaluation method, leading to the development of CapArena-Auto for efficient benchmarking."
                },
                "zh": {
                    "title": "å›¾åƒæè¿°çš„æ–°æ ‡å‡†ï¼šVLMçš„å´›èµ·ä¸è¯„ä¼°",
                    "desc": "å›¾åƒæè¿°ä¸€ç›´æ˜¯è§†è§‰ä¸è¯­è¨€ç ”ç©¶ä¸­çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œç°ä»£è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èƒ½å¤Ÿç”Ÿæˆè¯¦ç»†ä¸”å…¨é¢çš„å›¾åƒæè¿°ã€‚æœ¬æ–‡é€šè¿‡å»ºç«‹CapArenaå¹³å°ï¼Œè¯„ä¼°å½“å‰VLMåœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°é¢†å…ˆæ¨¡å‹å¦‚GPT-4oçš„è¡¨ç°ç”šè‡³è¶…è¿‡äº†äººç±»ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„å¯é æ€§ï¼Œç»“æœè¡¨æ˜VLMä½œä¸ºè¯„åˆ¤è€…åœ¨æè¿°è´¨é‡è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†ä¸€ç§æ–°çš„é«˜æ•ˆåŸºå‡†æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13424",
            "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
            "url": "https://huggingface.co/papers/2503.13424",
            "abstract": "Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and heavy labour of the simulation. In this paper, we propose Infinite Mobility, a novel method for synthesizing high-fidelity articulated objects through procedural generation. User study and quantitative evaluation demonstrate that our method can produce results that excel current state-of-the-art methods and are comparable to human-annotated datasets in both physics property and mesh quality. Furthermore, we show that our synthetic data can be used as training data for generative models, enabling next-step scaling up. Code is available at https://github.com/Intern-Nexus/Infinite-Mobility",
            "score": 13,
            "issue_id": 2776,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "20793013b58dba36",
            "authors": [
                "Xinyu Lian",
                "Zichao Yu",
                "Ruiming Liang",
                "Yitong Wang",
                "Li Ray Luo",
                "Kaixu Chen",
                "Yuanzhen Zhou",
                "Qihong Tang",
                "Xudong Xu",
                "Zhaoyang Lyu",
                "Bo Dai",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Fudan University",
                "Harbin Institute of Technology, Shenzhen",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Shanghai Artificial Intelligence Laboratory",
                "South China University of Technology",
                "The University of Hong Kong",
                "Tongji University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13424.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ñ‡Ğ»ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Infinite Mobility Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¾Ñ‡Ğ»ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ, Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¿Ğ¾Ğ»Ğ¸Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Infinite Mobility, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Articulated Object Creation with Infinite Mobility",
                    "desc": "This paper introduces Infinite Mobility, a new approach for creating high-quality articulated objects using procedural generation techniques. Unlike traditional methods that rely on limited data or labor-intensive simulations, Infinite Mobility generates objects that maintain high fidelity in both physical properties and mesh quality. The authors conducted user studies and quantitative evaluations, showing that their method outperforms existing state-of-the-art techniques and rivals human-annotated datasets. Additionally, the synthetic data produced can be utilized for training generative models, facilitating further advancements in the field of embodied AI."
                },
                "zh": {
                    "title": "æ— é™ç§»åŠ¨ï¼šåˆæˆé«˜ä¿çœŸå…³èŠ‚ç‰©ä½“çš„æ–°æ–¹æ³•",
                    "desc": "åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ— é™ç§»åŠ¨ï¼ˆInfinite Mobilityï¼‰çš„æ–¹æ³•ï¼Œç”¨äºé€šè¿‡ç¨‹åºç”Ÿæˆåˆæˆé«˜ä¿çœŸåº¦çš„å…³èŠ‚ç‰©ä½“ã€‚è¿™ç§æ–¹æ³•å…‹æœäº†ç°æœ‰æ•°æ®é©±åŠ¨æˆ–æ¨¡æ‹Ÿæ–¹æ³•åœ¨è§„æ¨¡å’Œè´¨é‡ä¸Šçš„é™åˆ¶ã€‚ç”¨æˆ·ç ”ç©¶å’Œå®šé‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç‰©ç†å±æ€§å’Œç½‘æ ¼è´¨é‡ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¸äººå·¥æ ‡æ³¨çš„æ•°æ®é›†ç›¸å½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆæˆæ•°æ®å¯ä»¥ä½œä¸ºç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ•°æ®ï¼Œæ”¯æŒåç»­çš„æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14125",
            "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
            "url": "https://huggingface.co/papers/2503.14125",
            "abstract": "Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the seesaw effect between gradient vanishing and representation collapse. However, Hyper-Connections increase memory access costs by expanding the width of hidden states. In this paper, we propose Frac-Connections, a novel approach that divides hidden states into multiple parts rather than expanding their width. Frac-Connections retain partial benefits of Hyper-Connections while reducing memory consumption. To validate their effectiveness, we conduct large-scale experiments on language tasks, with the largest being a 7B MoE model trained on up to 3T tokens, demonstrating that Frac-Connections significantly outperform residual connections.",
            "score": 10,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "936ea0aa4972c382",
            "authors": [
                "Defa Zhu",
                "Hongzhi Huang",
                "Jundong Zhou",
                "Zihao Huang",
                "Yutao Zeng",
                "Banggu Wu",
                "Qiyang Min",
                "Xun Zhou"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14125.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Frac-Connections: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Frac-Connections. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ´ĞµÑ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ (residual connections), Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹. Frac-Connections ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Hyper-Connections, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MoE Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° 3 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Frac-Connections: Efficient Memory for Deep Learning",
                    "desc": "This paper introduces Frac-Connections, a new method that improves upon Hyper-Connections in deep learning. While Hyper-Connections allow for multiple connection strengths to combat issues like gradient vanishing, they also increase memory usage due to wider hidden states. Frac-Connections address this by splitting hidden states into smaller parts, maintaining some advantages of Hyper-Connections while reducing memory costs. The authors demonstrate the effectiveness of Frac-Connections through extensive experiments on language tasks, showing superior performance compared to traditional residual connections."
                },
                "zh": {
                    "title": "Frac-Connectionsï¼šä¼˜åŒ–æ·±åº¦å­¦ä¹ çš„å†…å­˜ä½¿ç”¨",
                    "desc": "æ®‹å·®è¿æ¥æ˜¯ç°ä»£æ·±åº¦å­¦ä¹ æ¶æ„çš„æ ¸å¿ƒï¼Œèƒ½å¤Ÿé€šè¿‡å‡è½»æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ¥è®­ç»ƒéå¸¸æ·±çš„ç½‘ç»œã€‚è¶…è¿æ¥æœ€è¿‘é€šè¿‡åœ¨ä¸åŒæ·±åº¦å¼•å…¥å¤šä¸ªè¿æ¥å¼ºåº¦æ¥æ¨å¹¿æ®‹å·®è¿æ¥ï¼Œä»è€Œè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±ä¸è¡¨ç¤ºå´©æºƒä¹‹é—´çš„æ‘‡æ‘†æ•ˆåº”ã€‚ç„¶è€Œï¼Œè¶…è¿æ¥é€šè¿‡æ‰©å±•éšè—çŠ¶æ€çš„å®½åº¦å¢åŠ äº†å†…å­˜è®¿é—®æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†Frac-Connectionsï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å°†éšè—çŠ¶æ€åˆ’åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†è€Œä¸æ˜¯æ‰©å±•å…¶å®½åº¦ï¼Œä¿ç•™äº†è¶…è¿æ¥çš„éƒ¨åˆ†ä¼˜åŠ¿ï¼ŒåŒæ—¶å‡å°‘äº†å†…å­˜æ¶ˆè€—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12505",
            "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification",
            "url": "https://huggingface.co/papers/2503.12505",
            "abstract": "Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs.",
            "score": 6,
            "issue_id": 2776,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "0ac4fdd3411855ac",
            "authors": [
                "Zhaopan Xu",
                "Pengfei Zhou",
                "Jiaxin Ai",
                "Wangbo Zhao",
                "Kai Wang",
                "Xiaojiang Peng",
                "Wenqi Shao",
                "Hongxun Yao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HIT",
                "NUS",
                "SZTU",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12505.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MPBench: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MPBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° (PRM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. MPBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑˆĞ°Ğ³Ğ¾Ğ², Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ PRM Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs with MPBench",
                    "desc": "This paper discusses the importance of reasoning in large language models (LLMs) and introduces process-level reward models (PRMs) to enhance their reasoning capabilities. PRMs provide step-wise rewards that help LLMs learn from their mistakes during training and improve their decision-making during inference. The authors identify a gap in existing benchmarks, which primarily focus on error detection, and propose MPBench, a new benchmark that evaluates PRMs across various reasoning scenarios. MPBench includes three evaluation paradigms: assessing step correctness, aggregating answers, and guiding the reasoning process search, offering a comprehensive framework for understanding PRMs' effectiveness."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„å¤šæ¨¡æ€åŸºå‡†",
                    "desc": "æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†å¤æ‚ä»»åŠ¡çš„é‡è¦èƒ½åŠ›ï¼Œè€Œè¯†åˆ«è¿‡ç¨‹é”™è¯¯å¯¹äºæå‡è¿™ä¸€èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ€è¿‘æå‡ºçš„è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰é€šè¿‡æä¾›é€æ­¥å¥–åŠ±ï¼Œä¿ƒè¿›äº†å¼ºåŒ–å­¦ä¹ å’Œæ•°æ®ç”Ÿæˆï¼Œä»è€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å¯¼LLMsèµ°å‘æ­£ç¡®çš„æ­¥éª¤ï¼Œæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PRMsåŸºå‡†ä¸»è¦åŸºäºæ–‡æœ¬ï¼Œä¸“æ³¨äºé”™è¯¯æ£€æµ‹ï¼Œå¿½è§†äº†æ¨ç†æœç´¢ç­‰å…¶ä»–åœºæ™¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MPBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šä»»åŠ¡å¤šæ¨¡æ€åŸºå‡†ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°PRMsåœ¨ä¸åŒåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14504",
            "title": "Aligning Multimodal LLM with Human Preference: A Survey",
            "url": "https://huggingface.co/papers/2503.14504",
            "abstract": "Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
            "score": 5,
            "issue_id": 2778,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "b33bcba515cfa942",
            "authors": [
                "Tao Yu",
                "Yi-Fan Zhang",
                "Chaoyou Fu",
                "Junkang Wu",
                "Jinda Lu",
                "Kun Wang",
                "Xingyu Lu",
                "Yunhang Shen",
                "Guibin Zhang",
                "Dingjie Song",
                "Yibo Yan",
                "Tianlong Xu",
                "Qingsong Wen",
                "Zhang Zhang",
                "Yan Huang",
                "Liang Wang",
                "Tieniu Tan"
            ],
            "affiliations": [
                "Institute of automation, Chinese academy of science",
                "Lehigh University",
                "Nanjing University",
                "Nanyang Technological University",
                "National University of Singapore",
                "Shenzhen International Graduate School, Tsinghua University",
                "Squirrel Ai Learning",
                "Tencent Youtu Lab",
                "The Hong Kong University of Science and Technology",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14504.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#alignment"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ². Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Aligning MLLMs: Bridging Gaps for Better Understanding",
                    "desc": "This paper reviews alignment algorithms for Multimodal Large Language Models (MLLMs), which integrate visual, auditory, and textual data. It highlights the challenges of truthfulness, safety, and reasoning in MLLMs, emphasizing the need for effective alignment strategies. The authors categorize alignment algorithms based on application scenarios, dataset construction, and evaluation benchmarks. The goal is to provide a structured overview that aids researchers in advancing alignment techniques for MLLMs."
                },
                "zh": {
                    "title": "å¯¹é½ç®—æ³•åŠ©åŠ›å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æœªæ¥",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿé€šè¿‡ç®€å•çš„æç¤ºå¤„ç†å„ç§é€šç”¨ä»»åŠ¡ï¼Œè€Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†æ¶‰åŠè§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬æ•°æ®çš„å¤æ‚ä»»åŠ¡æ–¹é¢å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…³äºçœŸå®æ€§ã€å®‰å…¨æ€§ã€ç±»o1æ¨ç†å’Œä¸äººç±»åå¥½çš„å¯¹é½ç­‰å…³é”®é—®é¢˜ä»æœªå¾—åˆ°å……åˆ†è§£å†³ã€‚æœ¬æ–‡æ—¨åœ¨ç³»ç»Ÿæ€§åœ°å›é¡¾MLLMsçš„å¯¹é½ç®—æ³•ï¼Œæ¢è®¨å…¶åº”ç”¨åœºæ™¯ã€æ•°æ®é›†æ„å»ºæ ¸å¿ƒå› ç´ ã€è¯„ä¼°åŸºå‡†ä»¥åŠæœªæ¥å‘å±•æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14499",
            "title": "Measuring AI Ability to Complete Long Tasks",
            "url": "https://huggingface.co/papers/2503.14499",
            "abstract": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.",
            "score": 4,
            "issue_id": 2777,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "c31aeeed7f6139af",
            "authors": [
                "Thomas Kwa",
                "Ben West",
                "Joel Becker",
                "Amy Deng",
                "Katharyn Garcia",
                "Max Hasin",
                "Sami Jawhar",
                "Megan Kinniment",
                "Nate Rush",
                "Sydney Von Arx",
                "Ryan Bloom",
                "Thomas Broadley",
                "Haoxing Du",
                "Brian Goodrich",
                "Nikola Jurkovic",
                "Luke Harold Miles",
                "Seraphina Nix",
                "Tao Lin",
                "Neev Parikh",
                "David Rein",
                "Lucas Jun Koba Sato",
                "Hjalmar Wijk",
                "Daniel M. Ziegler",
                "Elizabeth Barnes",
                "Lawrence Chan"
            ],
            "affiliations": [
                "Anthropic",
                "Model Evaluation & Threat Research (METR)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14499.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "â³",
                "ru": {
                    "title": "Ğ’Ñ€ĞµĞ¼Ñ Ğ½Ğ° Ğ²Ğ°ÑˆĞµĞ¹ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğµ: Ğ˜Ğ˜ Ğ´Ğ¾Ğ³Ğ¾Ğ½ÑĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ - Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ 50%-Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ»ÑĞ´ÑĞ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ˜Ğ˜ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ñ 50%-Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑƒÑĞ¿ĞµÑ…Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ˜Ğ˜, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Claude 3.7 Sonnet, Ğ¸Ğ¼ĞµÑÑ‚ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 50 Ğ¼Ğ¸Ğ½ÑƒÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ˜Ğ˜ ÑƒĞ´Ğ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ ÑĞµĞ¼ÑŒ Ğ¼ĞµÑÑÑ†ĞµĞ² Ñ 2019 Ğ³Ğ¾Ğ´Ğ°, Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Measuring AI Progress: The 50%-Task-Completion Time Horizon",
                    "desc": "This paper introduces a new metric called the 50%-task-completion time horizon to evaluate AI systems based on human performance. It measures the time it takes for humans to complete tasks that AI can accomplish with a 50% success rate. The study found that leading AI models, like Claude 3.7 Sonnet, have a time horizon of about 50 minutes, and this capability has been improving rapidly, doubling approximately every seven months. The authors highlight the implications of this trend, suggesting that AI could soon automate complex software tasks that currently require significant human effort."
                },
                "zh": {
                    "title": "AIèƒ½åŠ›çš„æ–°åº¦é‡ï¼š50%ä»»åŠ¡å®Œæˆæ—¶é—´",
                    "desc": "å°½ç®¡äººå·¥æ™ºèƒ½åœ¨åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†å…¶å®é™…è¡¨ç°çš„æ„ä¹‰ä»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼š50%ä»»åŠ¡å®Œæˆæ—¶é—´èŒƒå›´ï¼Œæ—¨åœ¨é‡åŒ–AIç³»ç»Ÿä¸äººç±»èƒ½åŠ›çš„å¯¹æ¯”ã€‚é€šè¿‡å¯¹äººç±»ä¸“å®¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„å®Œæˆæ—¶é—´è¿›è¡Œæµ‹é‡ï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„å‰æ²¿AIæ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„50%æ—¶é—´èŒƒå›´çº¦ä¸º50åˆ†é’Ÿã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒAIæ¨¡å‹çš„æ—¶é—´èŒƒå›´è‡ª2019å¹´ä»¥æ¥å¤§çº¦æ¯ä¸ƒä¸ªæœˆç¿»å€ï¼Œæœªæ¥äº”å¹´å†…ï¼ŒAIç³»ç»Ÿå¯èƒ½èƒ½å¤Ÿè‡ªåŠ¨åŒ–è®¸å¤šç›®å‰éœ€è¦äººç±»ä¸€ä¸ªæœˆæ‰èƒ½å®Œæˆçš„è½¯ä»¶ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14495",
            "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
            "url": "https://huggingface.co/papers/2503.14495",
            "abstract": "Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency",
            "score": 4,
            "issue_id": 2779,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "590d8cdf2ae26151",
            "authors": [
                "Jiacheng Guo",
                "Yue Wu",
                "Jiahao Qiu",
                "Kaixuan Huang",
                "Xinzhe Juan",
                "Ling Yang",
                "Mengdi Wang"
            ],
            "affiliations": [
                "AI Lab, Princeton University",
                "Department of Computer Science & Engineering, University of Michigan",
                "Department of Electrical & Computer Engineering, Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14495.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#benchmark",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞŸÑ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğº Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ DeepSeek R1 Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ 7B/8B Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ 70B/72B Ğ¸ GPT-4o Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ ProcessBench."
                },
                "en": {
                    "title": "Enhancing Verification Accuracy through Temporal Consistency",
                    "desc": "This paper introduces a novel temporal consistency method for enhancing verification in mathematical reasoning. The approach allows verifiers to iteratively refine their judgments based on previous assessments, improving accuracy over traditional one-round verification methods. By utilizing a sequence of self-reflection actions, the method shows significant performance gains on various benchmarks for identifying mathematical process errors. Notably, it enables smaller distilled models to outperform larger models, demonstrating its effectiveness in practical applications."
                },
                "zh": {
                    "title": "æå‡æ•°å­¦éªŒè¯çš„æ—¶é—´ä¸€è‡´æ€§æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´ä¸€è‡´æ€§éªŒè¯æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ•°å­¦æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£åœ°æ ¹æ®ä¹‹å‰çš„è¯„ä¼°æ¥ç»†åŒ–åˆ¤æ–­ï¼Œå…‹æœäº†å•è½®éªŒè¯å’Œå¤šæ¨¡å‹è¾©è®ºçš„å±€é™æ€§ã€‚é€šè¿‡åœ¨å¤šä¸ªæ•°å­¦è¿‡ç¨‹é”™è¯¯è¯†åˆ«åŸºå‡†ï¼ˆå¦‚Mathcheckã€ProcessBenchå’ŒPRM800Kï¼‰ä¸Šçš„å®è¯è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºç›¸è¾ƒäºåŸºçº¿æ–¹æ³•çš„ä¸€è‡´æ€§æ€§èƒ½æå‡ã€‚åº”ç”¨äºæœ€æ–°çš„DeepSeek R1è’¸é¦æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿å¾—7B/8Bè’¸é¦æ¨¡å‹åœ¨ProcessBenchä¸Šè¶…è¶Šäº†æ‰€æœ‰70B/72Bæ¨¡å‹å’ŒGPT-4oã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14492",
            "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control",
            "url": "https://huggingface.co/papers/2503.14492",
            "abstract": "We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1.",
            "score": 4,
            "issue_id": 2777,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "5098c043161888fa",
            "authors": [
                "NVIDIA",
                ":",
                "Hassan Abu Alhaija",
                "Jose Alvarez",
                "Maciej Bala",
                "Tiffany Cai",
                "Tianshi Cao",
                "Liz Cha",
                "Joshua Chen",
                "Mike Chen",
                "Francesco Ferroni",
                "Sanja Fidler",
                "Dieter Fox",
                "Yunhao Ge",
                "Jinwei Gu",
                "Ali Hassani",
                "Michael Isaev",
                "Pooya Jannaty",
                "Shiyi Lan",
                "Tobias Lasser",
                "Huan Ling",
                "Ming-Yu Liu",
                "Xian Liu",
                "Yifan Lu",
                "Alice Luo",
                "Qianli Ma",
                "Hanzi Mao",
                "Fabio Ramos",
                "Xuanchi Ren",
                "Tianchang Shen",
                "Shitao Tang",
                "Ting-Chun Wang",
                "Jay Wu",
                "Jiashu Xu",
                "Stella Xu",
                "Kevin Xie",
                "Yuchong Ye",
                "Xiaodong Yang",
                "Xiaohui Zeng",
                "Yu Zeng"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14492.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#robotics",
                    "#transfer_learning",
                    "#open_source",
                    "#inference",
                    "#3d"
                ],
                "emoji": "ğŸŒŒ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼",
                    "desc": "Cosmos-Transfer - ÑÑ‚Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸ÑÑ…. Ğ­Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¸Ñ€Ğ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Sim2Real. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞµĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜, Ğ² Ñ‚Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Sim2Real Ğ¸ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹."
                },
                "en": {
                    "title": "Empowering World Generation with Adaptive Control",
                    "desc": "Cosmos-Transfer is a novel model designed for generating world simulations using various spatial control inputs like segmentation, depth, and edge maps. It features an adaptive and customizable spatial conditional scheme that allows for different weights to be assigned to inputs based on their spatial locations. This flexibility enhances the controllability of world generation, making it suitable for applications such as Sim2Real in robotics and autonomous vehicles. The model has been extensively evaluated, and its code is open-sourced to foster further research in the field."
                },
                "zh": {
                    "title": "å¯æ§çš„ä¸–ç•Œç”Ÿæˆï¼ŒåŠ©åŠ›ç‰©ç†äººå·¥æ™ºèƒ½",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†Cosmos-Transferï¼Œè¿™æ˜¯ä¸€ç§æ¡ä»¶ä¸–ç•Œç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®å¤šç§ç©ºé—´æ§åˆ¶è¾“å…¥ï¼ˆå¦‚åˆ†å‰²ã€æ·±åº¦å’Œè¾¹ç¼˜ï¼‰ç”Ÿæˆä¸–ç•Œæ¨¡æ‹Ÿã€‚åœ¨è®¾è®¡ä¸Šï¼Œç©ºé—´æ¡ä»¶æ–¹æ¡ˆæ˜¯è‡ªé€‚åº”å’Œå¯å®šåˆ¶çš„ï¼Œå…è®¸åœ¨ä¸åŒç©ºé—´ä½ç½®å¯¹ä¸åŒæ¡ä»¶è¾“å…¥è¿›è¡ŒåŠ æƒã€‚è¿™ä½¿å¾—ä¸–ç•Œç”Ÿæˆå…·æœ‰é«˜åº¦å¯æ§æ€§ï¼Œå¹¶åœ¨å¤šç§ä¸–ç•Œåˆ°ä¸–ç•Œçš„è½¬ç§»åº”ç”¨ä¸­æ‰¾åˆ°ç”¨é€”ï¼ŒåŒ…æ‹¬Sim2Realã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œåˆ†æäº†æ‰€æå‡ºçš„æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ç‰©ç†äººå·¥æ™ºèƒ½ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬æœºå™¨äººSim2Realå’Œè‡ªåŠ¨é©¾é©¶è½¦è¾†æ•°æ®å¢å¼ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12545",
            "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2503.12545",
            "abstract": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant concerns about privacy and security. To address these issues, machine unlearning (MU) has emerged as a promising solution, enabling the removal of specific knowledge from an already trained model without requiring retraining from scratch. Although MU for MLLMs has gained attention, current evaluations of its efficacy remain incomplete, and the underlying problem is often poorly defined, which hinders the development of strategies for creating more secure and trustworthy systems. To bridge this gap, we introduce a benchmark, named PEBench, which includes a dataset of personal entities and corresponding general event scenes, designed to comprehensively assess the performance of MU for MLLMs. Through PEBench, we aim to provide a standardized and robust framework to advance research in secure and privacy-preserving multimodal models. We benchmarked 6 MU methods, revealing their strengths and limitations, and shedding light on key challenges and opportunities for MU in MLLMs.",
            "score": 4,
            "issue_id": 2776,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "8a908fbc8ce24853",
            "authors": [
                "Zhaopan Xu",
                "Pengfei Zhou",
                "Weidong Tang",
                "Jiaxin Ai",
                "Wangbo Zhao",
                "Xiaojiang Peng",
                "Kai Wang",
                "Yang You",
                "Wenqi Shao",
                "Hongxun Yao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HIT",
                "NUS",
                "SZTU",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "XDU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12545.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#ethics",
                    "#security",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ”’",
                "ru": {
                    "title": "PEBench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PEBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (MU) Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). PEBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 6 Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² MU, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Privacy in Multimodal Models with Machine Unlearning",
                    "desc": "This paper discusses the advancements of Multimodal Large Language Models (MLLMs) in tasks like visual question answering and reasoning, while highlighting the privacy concerns associated with their data usage. It introduces machine unlearning (MU) as a method to selectively remove knowledge from these models without complete retraining. The authors present PEBench, a benchmark designed to evaluate MU methods specifically for MLLMs, using a dataset of personal entities and general event scenes. By benchmarking six MU techniques, the study identifies their strengths and weaknesses, aiming to enhance the security and trustworthiness of multimodal systems."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¤šæ¨¡æ€æ¨¡å‹çš„å®‰å…¨ä¸éšç§ä¿æŠ¤",
                    "desc": "è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰é—®ç­”ã€è§†è§‰ç†è§£å’Œæ¨ç†ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›è¿›å±•ä¾èµ–äºä»äº’è”ç½‘æ”¶é›†çš„å¤§é‡æ•°æ®ï¼Œè¿™å¼•å‘äº†éšç§å’Œå®‰å…¨æ–¹é¢çš„é‡å¤§æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœºå™¨é—å¿˜ï¼ˆMUï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»å·²è®­ç»ƒçš„æ¨¡å‹ä¸­åˆ é™¤ç‰¹å®šçŸ¥è¯†ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•PEBenchï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°MUåœ¨MLLMsä¸­çš„è¡¨ç°ï¼Œæ¨åŠ¨å®‰å…¨å’Œéšç§ä¿æŠ¤çš„å¤šæ¨¡æ€æ¨¡å‹ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14151",
            "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
            "url": "https://huggingface.co/papers/2503.14151",
            "abstract": "We present Concat-ID, a unified framework for identity-preserving video generation. Concat-ID employs Variational Autoencoders to extract image features, which are concatenated with video latents along the sequence dimension, leveraging solely 3D self-attention mechanisms without the need for additional modules. A novel cross-video pairing strategy and a multi-stage training regimen are introduced to balance identity consistency and facial editability while enhancing video naturalness. Extensive experiments demonstrate Concat-ID's superiority over existing methods in both single and multi-identity generation, as well as its seamless scalability to multi-subject scenarios, including virtual try-on and background-controllable generation. Concat-ID establishes a new benchmark for identity-preserving video synthesis, providing a versatile and scalable solution for a wide range of applications.",
            "score": 1,
            "issue_id": 2783,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "ba38580b0d116018",
            "authors": [
                "Yong Zhong",
                "Zhuoyi Yang",
                "Jiayan Teng",
                "Xiaotao Gu",
                "Chongxuan Li"
            ],
            "affiliations": [
                "Gaoling School of AI, Renmin University of China, Beijing, China",
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14151.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Concat-ID: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Concat-ID - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ 3D Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ»Ğ¸Ñ†. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Concat-ID Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Identity-Preserving Video Generation Made Easy with Concat-ID",
                    "desc": "Concat-ID is a new framework designed for generating videos that maintain the identity of subjects. It uses Variational Autoencoders to capture important image features and combines them with video data using 3D self-attention, simplifying the process without extra components. The framework introduces a unique method for pairing videos and a multi-stage training approach to ensure that the generated videos are both consistent in identity and editable, while also looking natural. Through extensive testing, Concat-ID has shown to outperform existing techniques in generating videos with single and multiple identities, making it adaptable for various applications like virtual try-ons and customizable backgrounds."
                },
                "zh": {
                    "title": "Concat-IDï¼šèº«ä»½ä¸€è‡´æ€§è§†é¢‘ç”Ÿæˆçš„æ–°æ ‡æ†",
                    "desc": "Concat-ID æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä¿æŒèº«ä»½ä¸€è‡´æ€§çš„è§†é¢‘ã€‚å®ƒä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨æå–å›¾åƒç‰¹å¾ï¼Œå¹¶å°†è¿™äº›ç‰¹å¾ä¸è§†é¢‘æ½œåœ¨å˜é‡åœ¨åºåˆ—ç»´åº¦ä¸Šè¿›è¡Œè¿æ¥ï¼Œå®Œå…¨ä¾èµ–äº 3D è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè€Œæ— éœ€é¢å¤–æ¨¡å—ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è·¨è§†é¢‘é…å¯¹ç­–ç•¥å’Œå¤šé˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼Œä»¥å¹³è¡¡èº«ä»½ä¸€è‡´æ€§å’Œé¢éƒ¨å¯ç¼–è¾‘æ€§ï¼ŒåŒæ—¶æé«˜è§†é¢‘çš„è‡ªç„¶æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConcat-ID åœ¨å•ä¸€å’Œå¤šèº«ä»½ç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ— ç¼æ‰©å±•åˆ°å¤šä¸»ä½“åœºæ™¯ï¼ŒåŒ…æ‹¬è™šæ‹Ÿè¯•ç©¿å’ŒèƒŒæ™¯å¯æ§ç”Ÿæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12271",
            "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection",
            "url": "https://huggingface.co/papers/2503.12271",
            "abstract": "The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach.",
            "score": 1,
            "issue_id": 2778,
            "pub_date": "2025-03-15",
            "pub_date_card": {
                "ru": "15 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 15",
                "zh": "3æœˆ15æ—¥"
            },
            "hash": "2f560e2ec0839955",
            "authors": [
                "Shufan Li",
                "Konstantinos Kallidromitis",
                "Akash Gokul",
                "Arsh Koneru",
                "Yusuke Kato",
                "Kazuki Kozuka",
                "Aditya Grover"
            ],
            "affiliations": [
                "Panasonic AI Research",
                "Salesforce AI Research",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12271.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#benchmark",
                    "#inference",
                    "#diffusion",
                    "#reasoning"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Reflect-DiT: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Reflect-DiT Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° best-of-N, Reflect-DiT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Diffusion Transformer Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ½ĞµĞµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ GenEval, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° 0.81. Reflect-DiT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° inference-time scaling Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Refining Image Generation with Reflective Learning",
                    "desc": "This paper presents Reflect-DiT, a novel method for enhancing text-to-image generation by incorporating in-context reflection capabilities into Diffusion Transformers. Unlike traditional best-of-N sampling, which generates multiple images and selects the best, Reflect-DiT refines its outputs based on previous generations and textual feedback. This approach allows the model to focus on specific areas for improvement, leading to more tailored and effective image generation. Experimental results show that Reflect-DiT achieves a new state-of-the-art performance on the GenEval benchmark with fewer samples, demonstrating its efficiency compared to larger models."
                },
                "zh": {
                    "title": "åæ€ç”Ÿæˆï¼Œæå‡å›¾åƒè´¨é‡ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºReflect-DiTã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥ä¸Šä¸‹æ–‡åæ€èƒ½åŠ›ï¼Œå¸®åŠ©Diffusion Transformersæ ¹æ®ä¹‹å‰ç”Ÿæˆçš„å›¾åƒå’Œæ–‡æœ¬åé¦ˆè¿›è¡Œæ”¹è¿›ã€‚ä¸ä¼ ç»Ÿçš„æœ€ä½³Né‡‡æ ·æ–¹æ³•ä¸åŒï¼ŒReflect-DiTèƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šçš„æ”¹è¿›éœ€æ±‚è¿›è¡Œè°ƒæ•´ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReflect-DiTåœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›åˆ†æ•°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10546",
            "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
            "url": "https://huggingface.co/papers/2503.10546",
            "abstract": "With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at http://kuda-dynamics.github.io.",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "f856affbe8bcf064",
            "authors": [
                "Zixian Liu",
                "Mingtong Zhang",
                "Yunzhu Li"
            ],
            "affiliations": [
                "Columbia University",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10546.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#robotics",
                    "#open_source",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "KUDA: Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼",
                    "desc": "KUDA - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLM) Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. KUDA ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ½Ğ° RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµÑ‚ VLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹. Ğ—Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "KUDA: Bridging Language and Dynamics for Robotic Manipulation",
                    "desc": "This paper presents KUDA, an innovative open-vocabulary robotic manipulation system that incorporates dynamics learning and visual prompting. Unlike previous methods, KUDA utilizes keypoints to create target specifications that are understandable by vision-language models (VLMs) and can be transformed into cost functions for planning. The system processes language instructions and visual data to assign keypoints to images, which are then optimized using a learned dynamics model to generate robotic movements. The effectiveness of KUDA is validated through various manipulation tasks, showcasing its ability to handle complex interactions with different object types."
                },
                "zh": {
                    "title": "KUDAï¼šåŠ¨æ€å­¦ä¹ ä¸è§†è§‰æç¤ºçš„å¼€æ”¾è¯æ±‡æ“ä½œç³»ç»Ÿ",
                    "desc": "éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¼€æ”¾è¯æ±‡çš„æœºå™¨äººæ“ä½œç³»ç»Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰æ–¹æ³•å¿½è§†äº†ç‰©ä½“åŠ¨æ€çš„é‡è¦æ€§ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ›´å¤æ‚åŠ¨æ€ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†KUDAï¼Œä¸€ä¸ªé›†æˆäº†åŠ¨æ€å­¦ä¹ å’Œé€šè¿‡å…³é”®ç‚¹è¿›è¡Œè§†è§‰æç¤ºçš„å¼€æ”¾è¯æ±‡æ“ä½œç³»ç»Ÿï¼Œåˆ©ç”¨äº†VLMså’ŒåŸºäºå­¦ä¹ çš„ç¥ç»åŠ¨æ€æ¨¡å‹ã€‚KUDAé€šè¿‡å°†å…³é”®ç‚¹åˆ†é…ç»™RGBå›¾åƒï¼Œå¹¶æŸ¥è¯¢VLMç”Ÿæˆç›®æ ‡è§„èŒƒï¼Œå°†æŠ½è±¡çš„å…³é”®ç‚¹è¡¨ç¤ºè½¬æ¢ä¸ºæˆæœ¬å‡½æ•°ï¼Œä»è€Œä¼˜åŒ–æœºå™¨äººè½¨è¿¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10410",
            "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation",
            "url": "https://huggingface.co/papers/2503.10410",
            "abstract": "Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration errors, sparse information, and multi-view consistency, leading to poor performance on recent published datasets. To significantly enhance roadside collaborative perception and address critical data issues, we present the first simulation framework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable of generating diverse, multi-view consistent simulated roadside data through dynamic foreground editing and full-scene style transfer of a single image. RoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures accurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View Occlusion-Aware Sampler (MOAS) determines the placement of diverse digital assets within 3D space; (3) DepthSAM innovatively models foreground-background relationships from single-frame fixed-view images, ensuring multi-view consistency of foreground; and (4) Scalable Post-Processing Toolkit generates more realistic and enriched scenes through style transfer and other enhancements. RoCo-Sim significantly improves roadside 3D object detection, outperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on TUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception simulation. Code and pre-trained models will be released soon: https://github.com/duyuwen-duen/RoCo-Sim",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "44150f611040e79d",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#data",
                    "#dataset",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "RoCo-Sim: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ",
                    "desc": "RoCo-Sim - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ğ±ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸, Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. RoCo-Sim Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñ‰Ğ¸Ğº Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¹, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½-Ñ„Ğ¾Ğ½ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ 3D-Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Roadside Awareness with Collaborative Perception",
                    "desc": "This paper introduces Roadside Collaborative Perception, a system where multiple roadside units work together to improve vehicle awareness of their surroundings. It highlights the limitations of current methods that focus mainly on model design while neglecting important data issues such as calibration errors and sparse information. To address these challenges, the authors present RoCo-Sim, a simulation framework that generates diverse and consistent roadside data using advanced techniques like dynamic foreground editing. RoCo-Sim significantly enhances roadside 3D object detection performance, surpassing state-of-the-art methods on benchmark datasets."
                },
                "zh": {
                    "title": "æå‡è·¯è¾¹æ„ŸçŸ¥çš„ååŒåŠ›é‡",
                    "desc": "è·¯è¾¹ååŒæ„ŸçŸ¥æ˜¯ä¸€ç§ç³»ç»Ÿï¼Œå¤šä¸ªè·¯è¾¹å•å…ƒåä½œæ±‡èšæ„ŸçŸ¥æ•°æ®ï¼Œå¸®åŠ©è½¦è¾†æé«˜ç¯å¢ƒæ„è¯†ã€‚ç°æœ‰çš„è·¯è¾¹æ„ŸçŸ¥æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡å‹è®¾è®¡ï¼Œä½†å¿½è§†äº†æ•°æ®é—®é¢˜ï¼Œå¦‚æ ¡å‡†è¯¯å·®ã€ä¿¡æ¯ç¨€ç–å’Œå¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå¯¼è‡´åœ¨æœ€æ–°æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ä¸ºæ˜¾è‘—æå‡è·¯è¾¹ååŒæ„ŸçŸ¥å¹¶è§£å†³å…³é”®æ•°æ®é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªè·¯è¾¹ååŒæ„ŸçŸ¥æ¨¡æ‹Ÿæ¡†æ¶RoCo-Simã€‚RoCo-Simèƒ½å¤Ÿé€šè¿‡åŠ¨æ€å‰æ™¯ç¼–è¾‘å’Œå•å›¾åƒçš„å…¨åœºæ™¯é£æ ¼è¿ç§»ç”Ÿæˆå¤šæ ·åŒ–çš„ã€å¤šè§†å›¾ä¸€è‡´çš„æ¨¡æ‹Ÿè·¯è¾¹æ•°æ®ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-18.html",
    "link_next": "2025-03-20.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "18.03",
        "en": "03/18",
        "zh": "3æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.03",
        "en": "03/20",
        "zh": "3æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 11,
        "#agents": 2,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 4,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 2,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 7,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 1,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0,
        "#creativity": 1
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº† RWKV-7 \"Goose\"ï¼Œä¸€ç§æ–°çš„åºåˆ—å»ºæ¨¡æ¶æ„ã€‚å®ƒåœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼Œå°½ç®¡è®­ç»ƒçš„ä»¤ç‰Œæ•°é‡å°‘ã€‚RWKV-7 æ¨¡å‹æ¯ä¸ªä»¤ç‰Œçš„å†…å­˜å’Œæ¨ç†æ—¶é—´æ˜¯å¸¸æ•°ã€‚å®ƒå¼•å…¥äº†æ–°çš„ delta è§„åˆ™å’Œæ”¾æ¾çš„å€¼æ›¿æ¢è§„åˆ™ã€‚æˆ‘ä»¬å±•ç¤ºäº† RWKV-7 çš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼Œå¹¶å‘å¸ƒäº†æ¨¡å‹å’Œæ•°æ®é›†ã€‚",
        "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
        "pinyin": "WÇ’men jiÃ¨shÃ o le RWKV-7 \"Goose\", yÄ« zhÇ’ng xÄ«n de xÃ¹liÃ¨ jiÃ nmÃ³ jiÃ gÃ²u. TÄ zÃ i duÅyÇ”yÃ¡n rÃ¨nwÃ¹ shÃ ng dÃ¡dÃ o le xÄ«n de zuÃ¬jiÄ xÃ¬ngnÃ©ng, jÇnguÇn xÃ¹nliÃ n de lÃ¬ngpÃ¡i shÃ¹liÃ ng shÇo. RWKV-7 mÃ³xÃ­ng mÄ›i ge lÃ¬ngpÃ¡i de nÃ¨icÃºn hÃ© tuÄ«lÇ shÃ­jiÄn shÃ¬ chÃ¡ngshÃ¹. TÄ yÇnrÃ¹ le xÄ«n de delta guÄ«zÃ© hÃ© fÃ ngsÅng de zhÃ­ tÃ¬huÃ n guÄ«zÃ©. WÇ’men zhÇnshÃ¬ le RWKV-7 de yÇ”yÃ¡n jiÃ nmÃ³ nÃ©nglÃ¬, bÃ¬ng fÄbÃ¹ le mÃ³xÃ­ng hÃ© shÃ¹jÃ¹jÃ­.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"åºåˆ—\", \"pinyin\": \"xÃ¹ liÃ¨\", \"trans\": \"sequence\"},\n    {\"word\": \"å»ºæ¨¡\", \"pinyin\": \"jiÃ n mÃ³\", \"trans\": \"modeling\"},\n    {\"word\": \"æ¶æ„\", \"pinyin\": \"jiÃ  gÃ²u\", \"trans\": \"architecture\"},\n    {\"word\": \"å¤šè¯­è¨€\", \"pinyin\": \"duÅ yÇ” yÃ¡n\", \"trans\": \"multilingual\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wu\", \"trans\": \"task\"},\n    {\"word\": \"è¾¾åˆ°\", \"pinyin\": \"dÃ¡ dÃ o\", \"trans\": \"achieve\"},\n    {\"word\": \"æœ€ä½³\", \"pinyin\": \"zuÃ¬ jiÄ\", \"trans\": \"best\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"å°½ç®¡\", \"pinyin\": \"jÃ¬n guÇn\", \"trans\": \"although\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"training\"},\n    {\"word\": \"ä»¤ç‰Œ\", \"pinyin\": \"lÃ¬ng pÃ i\", \"trans\": \"token\"},\n    {\"word\": \"æ•°é‡\", \"pinyin\": \"shÃ¹ liÃ ng\", \"trans\": \"quantity\"},\n    {\"word\": \"å°‘\", \"pinyin\": \"shÇo\", \"trans\": \"few\"},\n    {\"word\": \"å†…å­˜\", \"pinyin\": \"nÃ¨i cÃºn\", \"trans\": \"memory\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"æ—¶é—´\", \"pinyin\": \"shÃ­ jiÄn\", \"trans\": \"time\"},\n    {\"word\": \"å¸¸æ•°\", \"pinyin\": \"chÃ¡ng shÃ¹\", \"trans\": \"constant\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇn rÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"è§„åˆ™\", \"pinyin\": \"guÄ« zÃ©\", \"trans\": \"rule\"},\n    {\"word\": \"æ”¾æ¾\", \"pinyin\": \"fÃ ng sÅng\", \"trans\": \"relax\"},\n    {\"word\": \"å€¼\", \"pinyin\": \"zhÃ­\", \"trans\": \"value\"},\n    {\"word\": \"æ›¿æ¢\", \"pinyin\": \"tÃ¬ huÃ n\", \"trans\": \"replace\"},\n    {\"word\": \"å±•ç¤º\", \"pinyin\": \"zhÇn shÃ¬\", \"trans\": \"demonstrate\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"å‘å¸ƒ\", \"pinyin\": \"fÄ bÃ¹\", \"trans\": \"release\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"}\n]",
        "trans": "We introduced RWKV-7 \"Goose,\" a new sequence modeling architecture. It achieved new best performance on multilingual tasks despite being trained with fewer tokens. The RWKV-7 model has constant memory and inference time per token. It introduces new delta rules and relaxed value substitution rules. We demonstrated the language modeling capabilities of RWKV-7 and released the model and dataset.",
        "update_ts": "2025-03-19 09:12"
    }
}