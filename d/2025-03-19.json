{
    "date": {
        "ru": "19 –º–∞—Ä—Ç–∞",
        "en": "March 19",
        "zh": "3Êúà19Êó•"
    },
    "time_utc": "2025-03-19 16:13",
    "weekday": 2,
    "issue_id": 2790,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.14456",
            "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
            "url": "https://huggingface.co/papers/2503.14456",
            "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC^0. To demonstrate RWKV-7's language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset.   To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.",
            "score": 80,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "0cd796cef6fa6475",
            "authors": [
                "Bo Peng",
                "Ruichong Zhang",
                "Daniel Goldstein",
                "Eric Alcaide",
                "Haowen Hou",
                "Janna Lu",
                "William Merrill",
                "Guangyu Song",
                "Kaifeng Tan",
                "Saiteja Utpala",
                "Nathan Wilce",
                "Johan S. Wind",
                "Tianyi Wu",
                "Daniel Wuttke",
                "Christian Zhou-Zheng"
            ],
            "affiliations": [
                "Beijing Normal University",
                "Dalle Molle Institute for Artificial Intelligence USI-SUPSI",
                "Denigma",
                "EleutherAI",
                "George Mason University",
                "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
                "New York University",
                "RWKV Project (under Linux Foundation AI & Data)",
                "Recursal AI",
                "Shenzhen University",
                "Tano Labs",
                "Tsinghua University",
                "University of Oslo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14456.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#dataset",
                    "#multilingual"
                ],
                "emoji": "ü¶¢",
                "ru": {
                    "title": "RWKV-7: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π",
                    "desc": "RWKV-7 'Goose' - —ç—Ç–æ –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä–∞—è —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–π state-of-the-art –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ 3 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–≤–æ–¥–∞ –Ω–∞ —Ç–æ–∫–µ–Ω, –≤–≤–æ–¥–∏—Ç –æ–±–æ–±—â–µ–Ω–Ω—É—é —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –ø—Ä–∞–≤–∏–ª–∞ –¥–µ–ª—å—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –≥–µ–π—Ç–∏–Ω–≥–æ–º –∏ –æ–±—É—á–µ–Ω–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. RWKV-7 —Å–ø–æ—Å–æ–±–Ω–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –≤—Å–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ —è–∑—ã–∫–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –∫–æ—Ä–ø—É—Å –∏–∑ 3,1 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ–±—É—á–∞—é—Ç –Ω–∞ –Ω–µ–º —á–µ—Ç—ã—Ä–µ –º–æ–¥–µ–ª–∏ RWKV-7 —Ä–∞–∑–º–µ—Ä–æ–º –æ—Ç 0,19 –¥–æ 2,9 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."
                },
                "en": {
                    "title": "RWKV-7: Efficient Multilingual Mastery with Fewer Parameters",
                    "desc": "RWKV-7 \"Goose\" is a novel sequence modeling architecture that achieves state-of-the-art performance on multilingual tasks with only 3 billion parameters. It utilizes a unique formulation of the delta rule and vector-valued gating, allowing for efficient memory usage and constant inference time per token. The model demonstrates advanced capabilities such as state tracking and recognition of regular languages, surpassing traditional Transformers in complexity. Additionally, RWKV-7 is trained on a large multilingual corpus and is made openly available for further research and development."
                },
                "zh": {
                    "title": "RWKV-7ÔºöÂ§öËØ≠Ë®Ä‰ªªÂä°ÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "RWKV-7 \"Goose\" ÊòØ‰∏ÄÁßçÊñ∞ÁöÑÂ∫èÂàóÂª∫Ê®°Êû∂ÊûÑÔºåÂÖ∑Êúâ3‰∫øÂèÇÊï∞ÁöÑÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÔºåÂú®Â§öËØ≠Ë®Ä‰ªªÂä°‰∏≠ËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÊ∞¥Âπ≥„ÄÇ‰∏éÂÖ∂‰ªñÈ°∂Á∫ß3BÊ®°ÂûãÁõ∏ÊØîÔºåRWKV-7Âú®ËÆ≠ÁªÉÊó∂‰ΩøÁî®ÁöÑÊ†áËÆ∞Êï∞ÈáèÊòæËëóÂáèÂ∞ëÔºå‰ΩÜ‰ªçËÉΩ‰∏éÂΩìÂâçÁöÑËã±ËØ≠ËØ≠Ë®ÄÊÄßËÉΩÁõ∏ÂåπÈÖç„ÄÇËØ•Ê®°ÂûãÈááÁî®‰∫ÜÊñ∞ÁöÑÂπø‰πâÂ¢ûÈáèËßÑÂàôÔºåÁªìÂêà‰∫ÜÂêëÈáèÂÄºÈó®ÊéßÂíå‰∏ä‰∏ãÊñáÂ≠¶‰π†ÁéáÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËÆ≠ÁªÉÁöÑÂπ∂Ë°åÊÄß„ÄÇRWKV-7ËÉΩÂ§üËøõË°åÁä∂ÊÄÅË∑üË∏™Âπ∂ËØÜÂà´ÊâÄÊúâÊ≠£ËßÑËØ≠Ë®ÄÔºåË∂ÖË∂ä‰∫ÜÊ†áÂáÜÂ§çÊùÇÊÄßÁåúÊÉ≥‰∏ãÁöÑTransformerÁöÑËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14378",
            "title": "Impossible Videos",
            "url": "https://huggingface.co/papers/2503.14378",
            "abstract": "Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can today's video generation models effectively follow prompts to create impossible video content? 2) Are today's video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models.",
            "score": 43,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "3334aa74b743ac8d",
            "authors": [
                "Zechen Bai",
                "Hai Ci",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14378.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "IPV-Bench: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã—Ö –≤–∏–¥–µ–æ",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç IPV-Bench - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Ä–∞–∑–≤–∏—Ç–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ. –û–Ω –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–µ–π 4 –¥–æ–º–µ–Ω–∞ –∏ 14 –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –∏ –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å—Ü–µ–Ω—ã, –Ω–∞—Ä—É—à–∞—é—â–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ, –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ, –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏–ª–∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–∫–æ–Ω—ã. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∞–±–æ—Ä –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ Video-LLM –ø–æ–Ω–∏–º–∞—Ç—å –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∏–¥–µ–æ. –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—ã—è–≤–ª—è–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–¥–µ–∏ –¥–ª—è –±—É–¥—É—â–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Ä–∞–∑–≤–∏—Ç–∏—è –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Exploring the Impossible: Advancing Video Generation and Understanding",
                    "desc": "This paper addresses the limitations of current synthetic video datasets, which mainly focus on replicating real-world scenarios. It introduces IPV-Bench, a new benchmark designed to evaluate video generation and understanding models on their ability to create and comprehend impossible video content. The benchmark includes a detailed taxonomy with various categories that challenge models to generate videos that defy physical and social laws. The findings highlight the current capabilities and shortcomings of video models, providing insights for future advancements in the field."
                },
                "zh": {
                    "title": "Êé¢Á¥¢‰∏çÂèØËÉΩËßÜÈ¢ëÁöÑÁîüÊàê‰∏éÁêÜËß£",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂêàÊàêËßÜÈ¢ëÂú®Êï∞ÊçÆÁ®ÄÁº∫ÂíåÂ§öÊ†∑ÊÄßÊñπÈù¢ÁöÑÂ∫îÁî®„ÄÇÂΩìÂâçÁöÑÂêàÊàêÊï∞ÊçÆÈõÜ‰∏ªË¶ÅÂ§çÂà∂Áé∞ÂÆûÂú∫ÊôØÔºåËÄåÂØπ‰∏çÂèØËÉΩ„ÄÅÂèç‰∫ãÂÆûÂíåÂèçÁé∞ÂÆûÁöÑËßÜÈ¢ëÊ¶ÇÂøµÁ†îÁ©∂‰∏çË∂≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜIPV-BenchÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞Âíå‰øÉËøõËßÜÈ¢ëÁêÜËß£‰∏éÁîüÊàêÁöÑËøõÂ±ï„ÄÇËØ•Âü∫ÂáÜÊ∂µÁõñ‰∫ÜÂ§öÁßçËøùÂèçÁâ©ÁêÜ„ÄÅÁîüÁâ©„ÄÅÂú∞ÁêÜÊàñÁ§æ‰ºöÊ≥ïÂàôÁöÑÂú∫ÊôØÔºåÂπ∂ÈÄöËøáÊûÑÂª∫ÊèêÁ§∫Â•ó‰ª∂Êù•ÊåëÊàòËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÁöÑÂàõÈÄ†ÂäõÂíåÊèêÁ§∫Ë∑üÈöèËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14478",
            "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
            "url": "https://huggingface.co/papers/2503.14478",
            "abstract": "Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLM's creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on https://github.com/open-compass/Creation-MMBench.",
            "score": 36,
            "issue_id": 2777,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "120f1d8ec2eb88a8",
            "authors": [
                "Xinyu Fang",
                "Zhijian Chen",
                "Kai Lan",
                "Shengyuan Ding",
                "Yingji Liang",
                "Xiangyu Zhao",
                "Farong Wen",
                "Zicheng Zhang",
                "Guofeng Zhang",
                "Haodong Duan",
                "Kai Chen",
                "Dahua Lin"
            ],
            "affiliations": [
                "East China Normal University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "The Chinese University of Hong Kong",
                "Tongji University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14478.jpg",
            "data": {
                "categories": [
                    "#creativity",
                    "#open_source",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "–ò–∑–º–µ—Ä—è—è —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Creation-MMBench - –Ω–æ–≤—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –∑–∞–¥–∞—á–∞—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç 765 —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 51 –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∑–∞–¥–∞—á—É, —Å –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª—É—á–∞—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ MLLM –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º –º–æ–¥–µ–ª—è–º –≤ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –≤–ª–∏—è—Ç—å –Ω–∞ —Ç–≤–æ—Ä—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–∞–∑–æ–≤–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Unlocking Creativity in Multimodal AI with Creation-MMBench",
                    "desc": "This paper introduces Creation-MMBench, a new benchmark designed to evaluate the creative capabilities of Multimodal Large Language Models (MLLMs) in tasks that involve both text and images. The benchmark includes 765 test cases across 51 specific tasks, each with tailored evaluation criteria to assess the quality of responses and their consistency with visual inputs. Experimental findings indicate that current open-source MLLMs perform poorly in creative tasks compared to proprietary models, and that visual fine-tuning may hinder the creative performance of base LLMs. Creation-MMBench aims to enhance understanding and development of MLLM creativity, providing a resource for future research in multimodal generative intelligence."
                },
                "zh": {
                    "title": "ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂàõÈÄ†Âäõ",
                    "desc": "ÂàõÈÄ†ÂäõÊòØÊô∫ËÉΩÁöÑ‰∏Ä‰∏™Âü∫Êú¨ÊñπÈù¢ÔºåÊ∂âÂèäÂú®‰∏çÂêåÊÉÖÂ¢É‰∏≠ÁîüÊàêÊñ∞È¢ñ‰∏îÈÄÇÂΩìÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇËôΩÁÑ∂Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂàõÈÄ†ËÉΩÂäõÊñπÈù¢ÂæóÂà∞‰∫ÜÂπøÊ≥õËØÑ‰º∞Ôºå‰ΩÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÁöÑËØÑ‰º∞‰ªçÁÑ∂Áõ∏ÂØπÁº∫‰πè„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜCreation-MMBenchÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®ËÆæËÆ°Áî®‰∫éËØÑ‰º∞MLLMsÂú®Âü∫‰∫éÂõæÂÉèÁöÑÂÆûÈôÖ‰ªªÂä°‰∏≠ÂàõÈÄ†ËÉΩÂäõÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂΩìÂâçÁöÑÂºÄÊ∫êMLLMsÂú®ÂàõÈÄ†ÊÄß‰ªªÂä°‰∏≠ÊòæËëó‰Ωé‰∫é‰∏ìÊúâÊ®°ÂûãÔºåËÄåËßÜËßâÂæÆË∞ÉÂèØËÉΩ‰ºöÂØπÂü∫Á°ÄLLMÁöÑÂàõÈÄ†ËÉΩÂäõ‰∫ßÁîüË¥üÈù¢ÂΩ±Âìç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14476",
            "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
            "url": "https://huggingface.co/papers/2503.14476",
            "abstract": "Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.",
            "score": 34,
            "issue_id": 2777,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "5b4841d2845817e8",
            "authors": [
                "Qiying Yu",
                "Zheng Zhang",
                "Ruofei Zhu",
                "Yufeng Yuan",
                "Xiaochen Zuo",
                "Yu Yue",
                "Tiantian Fan",
                "Gaohong Liu",
                "Lingjun Liu",
                "Xin Liu",
                "Haibin Lin",
                "Zhiqi Lin",
                "Bole Ma",
                "Guangming Sheng",
                "Yuxuan Tong",
                "Chi Zhang",
                "Mofan Zhang",
                "Wang Zhang",
                "Hang Zhu",
                "Jinhua Zhu",
                "Jiaze Chen",
                "Jiangjie Chen",
                "Chengyi Wang",
                "Hongli Yu",
                "Weinan Dai",
                "Yuxuan Song",
                "Xiangpeng Wei",
                "Hao Zhou",
                "Jingjing Liu",
                "Wei-Ying Ma",
                "Ya-Qin Zhang",
                "Lin Yan",
                "Mu Qiao",
                "Yonghui Wu",
                "Mingxuan Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Institute for AI Industry Research (AIR), Tsinghua University",
                "SIA-Lab of Tsinghua AIR and ByteDance",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14476.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#open_source"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–û—Ç–∫—Ä—ã—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ RL –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º DAPO –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ—Ç–∫—Ä—ã—Ç–æ –ø—É–±–ª–∏–∫—É—é—Ç —Å–∏—Å—Ç–µ–º—É, –¥–æ—Å—Ç–∏–≥–∞—é—â—É—é 50 –±–∞–ª–ª–æ–≤ –Ω–∞ AIME 2024 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Qwen2.5-32B. –í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç—Å—è —á–µ—Ç—ã—Ä–µ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏, –¥–µ–ª–∞—é—â–∏–µ —É—Å–ø–µ—à–Ω—ã–º RL –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ø–ú. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –∫–æ–¥ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."
                },
                "en": {
                    "title": "Unlocking LLM Potential with Open-Source Reinforcement Learning",
                    "desc": "This paper introduces the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, which enhances the reasoning capabilities of large language models (LLMs) through reinforcement learning (RL). The authors address the lack of transparency in existing state-of-the-art LLMs by providing detailed insights into their training processes and techniques. They report achieving a significant performance score of 50 points on the AIME 2024 benchmark using the Qwen2.5-32B model. By open-sourcing their training code and dataset, they aim to improve reproducibility and foster further advancements in large-scale LLM reinforcement learning research."
                },
                "zh": {
                    "title": "Ëß£ÈîÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÊΩúÂäõ",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁÆóÊ≥ïÔºåÁß∞‰∏∫Ëß£ËÄ¶Ââ™ËæëÂíåÂä®ÊÄÅÈááÊ†∑Á≠ñÁï•‰ºòÂåñÔºàDAPOÔºâÔºåÊó®Âú®ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊäÄÊúØÔºåÊàêÂäüÂÆûÁé∞‰∫ÜÂú®AIME 2024‰∏äËé∑Âæó50ÂàÜÁöÑÊàêÁª©Ôºå‰ΩøÁî®ÁöÑÊòØQwen2.5-32BÂü∫Á°ÄÊ®°Âûã„ÄÇ‰∏é‰ª•ÂæÄÁ†îÁ©∂‰∏çÂêåÔºåÊàë‰ª¨ÂÖ¨ÂºÄ‰∫ÜÂõõ‰∏™ÂÖ≥ÈîÆÊäÄÊúØÁªÜËäÇÔºåÂ∏ÆÂä©Á§æÂå∫Êõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÂ§çÁé∞Êàë‰ª¨ÁöÑËÆ≠ÁªÉÁªìÊûú„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂºÄÊ∫ê‰∫ÜËÆ≠ÁªÉ‰ª£Á†ÅÂíåÁªèËøáÁ≤æÂøÉÂ§ÑÁêÜÁöÑÊï∞ÊçÆÈõÜÔºå‰ª•‰øÉËøõÂ§ßÂûãLLMÂº∫ÂåñÂ≠¶‰π†ÁöÑÂèØÈáçÂ§çÊÄßÂíåÊú™Êù•Á†îÁ©∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12797",
            "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding",
            "url": "https://huggingface.co/papers/2503.12797",
            "abstract": "Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on KVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception.",
            "score": 22,
            "issue_id": 2777,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 –º–∞—Ä—Ç–∞",
                "en": "March 17",
                "zh": "3Êúà17Êó•"
            },
            "hash": "c682a086aaac0fa1",
            "authors": [
                "Xinyu Ma",
                "Ziyang Ding",
                "Zhicong Luo",
                "Chi Chen",
                "Zonghao Guo",
                "Derek F. Wong",
                "Xiaoyi Feng",
                "Maosong Sun"
            ],
            "affiliations": [
                "Northwestern Polytechnical University",
                "Shandong University",
                "Tsinghua University",
                "University of Macau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12797.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#data",
                    "#dataset",
                    "#reasoning",
                    "#synthetic",
                    "#benchmark",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "DeepPerception: –£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –ò–ò —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –∑–Ω–∞–Ω–∏–π",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –∑–∞–¥–∞—á—É knowledge-intensive visual grounding (KVG), —Ç—Ä–µ–±—É—é—â—É—é —Ç–æ–Ω–∫–æ–≥–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å DeepPerception, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π MLLM. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å–æ–∑–¥–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ KVG-Bench –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Enhancing Visual Perception in MLLMs with DeepPerception",
                    "desc": "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in visual perception and reasoning. It introduces a new task called knowledge-intensive visual grounding (KVG), which combines fine-grained visual discrimination with domain-specific knowledge. The authors propose DeepPerception, an enhanced MLLM that utilizes a data synthesis pipeline and a two-stage training framework to improve cognitive reasoning and perception integration. Experimental results show that DeepPerception outperforms existing methods, demonstrating the potential for MLLMs to achieve human-like visual understanding."
                },
                "zh": {
                    "title": "ÊèêÂçáËßÜËßâÊÑüÁü•ÁöÑËÆ§Áü•Êï¥ÂêàËÉΩÂäõ",
                    "desc": "‰∫∫Á±ª‰∏ìÂÆ∂Âú®ÁªÜÁ≤íÂ∫¶ËßÜËßâËæ®Âà´ÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÂà©Áî®È¢ÜÂüüÁü•ËØÜÊù•‰ºòÂåñÊÑüÁü•ÁâπÂæÅÔºåËÄåÂΩìÂâçÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËøôÊñπÈù¢‰ªçÊòæ‰∏çË∂≥„ÄÇÂ∞ΩÁÆ°Êã•Êúâ‰∏∞ÂØåÁöÑ‰∏ìÂÆ∂Á∫ßÁü•ËØÜÔºåMLLMsÂú®ËßÜËßâÊÑüÁü•‰∏≠Êï¥ÂêàÊé®ÁêÜÁöÑËÉΩÂäõËæÉÂº±ÔºåÂ∏∏Â∏∏Áõ¥Êé•ÁîüÊàêÂìçÂ∫îËÄåÁº∫‰πèÊ∑±ÂÖ•ÂàÜÊûê„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÁü•ËØÜÂØÜÈõÜÂûãËßÜËßâÂÆö‰ΩçÔºàKVGÔºâÔºåËøôÊòØ‰∏ÄÈ°πÊñ∞È¢ñÁöÑËßÜËßâÂÆö‰Ωç‰ªªÂä°ÔºåË¶ÅÊ±ÇÂêåÊó∂ÂÖ∑Â§áÁªÜÁ≤íÂ∫¶ÊÑüÁü•ÂíåÈ¢ÜÂüüÁâπÂÆöÁü•ËØÜÁöÑÊï¥Âêà„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑDeepPerceptionÊ®°ÂûãÂ¢ûÂº∫‰∫ÜËÆ§Áü•ËßÜËßâÊÑüÁü•ËÉΩÂäõÔºåÈÄöËøáËá™Âä®ÂåñÊï∞ÊçÆÂêàÊàêÂíå‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂú®KVG-BenchÊï∞ÊçÆÈõÜ‰∏äÁöÑÂáÜÁ°ÆÊÄßÂíåË∑®È¢ÜÂüüÊ≥õÂåñËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12329",
            "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era",
            "url": "https://huggingface.co/papers/2503.12329",
            "abstract": "Image captioning has been a longstanding challenge in vision-language research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed and comprehensive image descriptions. However, benchmarking the quality of such captions remains unresolved. This paper addresses two key questions: (1) How well do current VLMs actually perform on image captioning, particularly compared to humans? We built CapArena, a platform with over 6000 pairwise caption battles and high-quality human preference votes. Our arena-style evaluation marks a milestone, showing that leading models like GPT-4o achieve or even surpass human performance, while most open-source models lag behind. (2) Can automated metrics reliably assess detailed caption quality? Using human annotations from CapArena, we evaluate traditional and recent captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while some metrics (e.g., METEOR) show decent caption-level agreement with humans, their systematic biases lead to inconsistencies in model ranking. In contrast, VLM-as-a-Judge demonstrates robust discernment at both the caption and model levels. Building on these insights, we release CapArena-Auto, an accurate and efficient automated benchmark for detailed captioning, achieving 94.3% correlation with human rankings at just $4 per test. Data and resources will be open-sourced at https://caparena.github.io.",
            "score": 18,
            "issue_id": 2778,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 –º–∞—Ä—Ç–∞",
                "en": "March 16",
                "zh": "3Êúà16Êó•"
            },
            "hash": "c97a8c730bfcbfa8",
            "authors": [
                "Kanzhi Cheng",
                "Wenpo Song",
                "Jiaxin Fan",
                "Zheng Ma",
                "Qiushi Sun",
                "Fangzhi Xu",
                "Chenyang Yan",
                "Nuo Chen",
                "Jianbing Zhang",
                "Jiajun Chen"
            ],
            "affiliations": [
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "Shanghai Artificial Intelligence Laboratory",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12329.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#games",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "üì∏",
                "ru": {
                    "title": "–ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º: –ò–ò –¥–æ–≥–æ–Ω—è–µ—Ç —á–µ–ª–æ–≤–µ–∫–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å –ø–æ–º–æ—â—å—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM). –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—É CapArena –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–æ–¥–ø–∏—Å–µ–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è–º–∏ –∏ –ª—é–¥—å–º–∏, –ø–æ–∫–∞–∑–∞–≤, —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, GPT-4o) –¥–æ—Å—Ç–∏–≥–∞—é—Ç –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–¥–ø–∏—Å–µ–π, –≤—ã—è–≤–ª—è—è –∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ CapArena-Auto - –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º."
                },
                "en": {
                    "title": "Elevating Image Captioning: Human-Level Performance and Reliable Metrics",
                    "desc": "This paper tackles the challenge of evaluating image captioning performance in Vision-Language Models (VLMs). It introduces CapArena, a platform that conducts over 6000 pairwise caption battles to compare VLM outputs with human-generated captions. The findings indicate that advanced models like GPT-4o can match or exceed human performance, while many open-source models do not. Additionally, the study assesses various automated metrics for caption quality, revealing that while some metrics align with human preferences, VLM-as-a-Judge provides a more reliable evaluation method, leading to the development of CapArena-Auto for efficient benchmarking."
                },
                "zh": {
                    "title": "ÂõæÂÉèÊèèËø∞ÁöÑÊñ∞Ê†áÂáÜÔºöVLMÁöÑÂ¥õËµ∑‰∏éËØÑ‰º∞",
                    "desc": "ÂõæÂÉèÊèèËø∞‰∏ÄÁõ¥ÊòØËßÜËßâ‰∏éËØ≠Ë®ÄÁ†îÁ©∂‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊåëÊàò„ÄÇÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂèëÂ±ïÔºåÁé∞‰ª£ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâËÉΩÂ§üÁîüÊàêËØ¶ÁªÜ‰∏îÂÖ®Èù¢ÁöÑÂõæÂÉèÊèèËø∞„ÄÇÊú¨ÊñáÈÄöËøáÂª∫Á´ãCapArenaÂπ≥Âè∞ÔºåËØÑ‰º∞ÂΩìÂâçVLMÂú®ÂõæÂÉèÊèèËø∞‰ªªÂä°‰∏äÁöÑË°®Áé∞ÔºåÂèëÁé∞È¢ÜÂÖàÊ®°ÂûãÂ¶ÇGPT-4oÁöÑË°®Áé∞ÁîöËá≥Ë∂ÖËøá‰∫Ü‰∫∫Á±ª„ÄÇÊàë‰ª¨ËøòÂàÜÊûê‰∫ÜËá™Âä®ËØÑ‰º∞ÊåáÊ†áÁöÑÂèØÈù†ÊÄßÔºåÁªìÊûúË°®ÊòéVLM‰Ωú‰∏∫ËØÑÂà§ËÄÖÂú®ÊèèËø∞Ë¥®ÈáèËØÑ‰º∞‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈ´òÊïàÂü∫ÂáÜÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13424",
            "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
            "url": "https://huggingface.co/papers/2503.13424",
            "abstract": "Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and heavy labour of the simulation. In this paper, we propose Infinite Mobility, a novel method for synthesizing high-fidelity articulated objects through procedural generation. User study and quantitative evaluation demonstrate that our method can produce results that excel current state-of-the-art methods and are comparable to human-annotated datasets in both physics property and mesh quality. Furthermore, we show that our synthetic data can be used as training data for generative models, enabling next-step scaling up. Code is available at https://github.com/Intern-Nexus/Infinite-Mobility",
            "score": 17,
            "issue_id": 2776,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 –º–∞—Ä—Ç–∞",
                "en": "March 17",
                "zh": "3Êúà17Êó•"
            },
            "hash": "20793013b58dba36",
            "authors": [
                "Xinyu Lian",
                "Zichao Yu",
                "Ruiming Liang",
                "Yitong Wang",
                "Li Ray Luo",
                "Kaixu Chen",
                "Yuanzhen Zhou",
                "Qihong Tang",
                "Xudong Xu",
                "Zhaoyang Lyu",
                "Bo Dai",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Fudan University",
                "Harbin Institute of Technology, Shenzhen",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Shanghai Artificial Intelligence Laboratory",
                "South China University of Technology",
                "The University of Hong Kong",
                "Tongji University",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13424.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ü—Ä–æ—Ü–µ–¥—É—Ä–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ—á–ª–µ–Ω–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –≤–æ–ø–ª–æ—â–µ–Ω–Ω–æ–≥–æ –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Infinite Mobility –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–æ—á–ª–µ–Ω–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –∏—Ö –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏ —Å—Ä–∞–≤–Ω–∏–º —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏, —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ –≤—Ä—É—á–Ω—É—é, –ø–æ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º —Å–≤–æ–π—Å—Ç–≤–∞–º –∏ –∫–∞—á–µ—Å—Ç–≤—É –ø–æ–ª–∏–≥–æ–Ω–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–æ–∫. –ú–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ —Å–∏–º—É–ª—è—Ü–∏–∏. –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é Infinite Mobility, –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è."
                },
                "en": {
                    "title": "Revolutionizing Articulated Object Creation with Infinite Mobility",
                    "desc": "This paper introduces Infinite Mobility, a new approach for creating high-quality articulated objects using procedural generation techniques. Unlike traditional methods that rely on limited data or labor-intensive simulations, Infinite Mobility generates objects that maintain high fidelity in both physical properties and mesh quality. The authors conducted user studies and quantitative evaluations, showing that their method outperforms existing state-of-the-art techniques and rivals human-annotated datasets. Additionally, the synthetic data produced can be utilized for training generative models, facilitating further advancements in the field of embodied AI."
                },
                "zh": {
                    "title": "Êó†ÈôêÁßªÂä®ÔºöÂêàÊàêÈ´ò‰øùÁúüÂÖ≥ËäÇÁâ©‰ΩìÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Âú®ËøôÁØáËÆ∫Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Êó†ÈôêÁßªÂä®ÔºàInfinite MobilityÔºâÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÈÄöËøáÁ®ãÂ∫èÁîüÊàêÂêàÊàêÈ´ò‰øùÁúüÂ∫¶ÁöÑÂÖ≥ËäÇÁâ©‰Ωì„ÄÇËøôÁßçÊñπÊ≥ïÂÖãÊúç‰∫ÜÁé∞ÊúâÊï∞ÊçÆÈ©±Âä®ÊàñÊ®°ÊãüÊñπÊ≥ïÂú®ËßÑÊ®°ÂíåË¥®Èáè‰∏äÁöÑÈôêÂà∂„ÄÇÁî®Êà∑Á†îÁ©∂ÂíåÂÆöÈáèËØÑ‰º∞Ë°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Áâ©ÁêÜÂ±ûÊÄßÂíåÁΩëÊ†ºË¥®Èáè‰∏äË∂ÖË∂ä‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºåÂπ∂‰∏î‰∏é‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊï∞ÊçÆÈõÜÁõ∏ÂΩì„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÂêàÊàêÊï∞ÊçÆÂèØ‰ª•‰Ωú‰∏∫ÁîüÊàêÊ®°ÂûãÁöÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÊîØÊåÅÂêéÁª≠ÁöÑÊâ©Â±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14125",
            "title": "Frac-Connections: Fractional Extension of Hyper-Connections",
            "url": "https://huggingface.co/papers/2503.14125",
            "abstract": "Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the seesaw effect between gradient vanishing and representation collapse. However, Hyper-Connections increase memory access costs by expanding the width of hidden states. In this paper, we propose Frac-Connections, a novel approach that divides hidden states into multiple parts rather than expanding their width. Frac-Connections retain partial benefits of Hyper-Connections while reducing memory consumption. To validate their effectiveness, we conduct large-scale experiments on language tasks, with the largest being a 7B MoE model trained on up to 3T tokens, demonstrating that Frac-Connections significantly outperform residual connections.",
            "score": 12,
            "issue_id": 2776,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "936ea0aa4972c382",
            "authors": [
                "Defa Zhu",
                "Hongzhi Huang",
                "Jundong Zhou",
                "Zihao Huang",
                "Yutao Zeng",
                "Banggu Wu",
                "Qiyang Min",
                "Xun Zhou"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14125.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "üß©",
                "ru": {
                    "title": "Frac-Connections: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Frac-Connections. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç –∏–¥–µ—é –æ—Å—Ç–∞—Ç–æ—á–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π (residual connections), —Ä–∞–∑–¥–µ–ª—è—è —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–µ–π –≤–º–µ—Å—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏—Ö —à–∏—Ä–∏–Ω—ã. Frac-Connections —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Hyper-Connections, –ø—Ä–∏ —ç—Ç–æ–º —Å–Ω–∏–∂–∞—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ –º–∞—Å—à—Ç–∞–±–Ω—ã–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏ –Ω–∞ —è–∑—ã–∫–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ MoE —Ä–∞–∑–º–µ—Ä–æ–º 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ 3 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤."
                },
                "en": {
                    "title": "Frac-Connections: Efficient Memory for Deep Learning",
                    "desc": "This paper introduces Frac-Connections, a new method that improves upon Hyper-Connections in deep learning. While Hyper-Connections allow for multiple connection strengths to combat issues like gradient vanishing, they also increase memory usage due to wider hidden states. Frac-Connections address this by splitting hidden states into smaller parts, maintaining some advantages of Hyper-Connections while reducing memory costs. The authors demonstrate the effectiveness of Frac-Connections through extensive experiments on language tasks, showing superior performance compared to traditional residual connections."
                },
                "zh": {
                    "title": "Frac-ConnectionsÔºö‰ºòÂåñÊ∑±Â∫¶Â≠¶‰π†ÁöÑÂÜÖÂ≠ò‰ΩøÁî®",
                    "desc": "ÊÆãÂ∑ÆËøûÊé•ÊòØÁé∞‰ª£Ê∑±Â∫¶Â≠¶‰π†Êû∂ÊûÑÁöÑÊ†∏ÂøÉÔºåËÉΩÂ§üÈÄöËøáÂáèËΩªÊ¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢òÊù•ËÆ≠ÁªÉÈùûÂ∏∏Ê∑±ÁöÑÁΩëÁªú„ÄÇË∂ÖËøûÊé•ÊúÄËøëÈÄöËøáÂú®‰∏çÂêåÊ∑±Â∫¶ÂºïÂÖ•Â§ö‰∏™ËøûÊé•Âº∫Â∫¶Êù•Êé®ÂπøÊÆãÂ∑ÆËøûÊé•Ôºå‰ªéËÄåËß£ÂÜ≥‰∫ÜÊ¢ØÂ∫¶Ê∂àÂ§±‰∏éË°®Á§∫Â¥©Ê∫É‰πãÈó¥ÁöÑÊëáÊëÜÊïàÂ∫î„ÄÇÁÑ∂ËÄåÔºåË∂ÖËøûÊé•ÈÄöËøáÊâ©Â±ïÈöêËóèÁä∂ÊÄÅÁöÑÂÆΩÂ∫¶Â¢ûÂä†‰∫ÜÂÜÖÂ≠òËÆøÈóÆÊàêÊú¨„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜFrac-ConnectionsÔºåËøôÊòØ‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÈÄöËøáÂ∞ÜÈöêËóèÁä∂ÊÄÅÂàíÂàÜ‰∏∫Â§ö‰∏™ÈÉ®ÂàÜËÄå‰∏çÊòØÊâ©Â±ïÂÖ∂ÂÆΩÂ∫¶Ôºå‰øùÁïô‰∫ÜË∂ÖËøûÊé•ÁöÑÈÉ®ÂàÜ‰ºòÂäøÔºåÂêåÊó∂ÂáèÂ∞ë‰∫ÜÂÜÖÂ≠òÊ∂àËÄó„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14504",
            "title": "Aligning Multimodal LLM with Human Preference: A Survey",
            "url": "https://huggingface.co/papers/2503.14504",
            "abstract": "Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.",
            "score": 8,
            "issue_id": 2778,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "b33bcba515cfa942",
            "authors": [
                "Tao Yu",
                "Yi-Fan Zhang",
                "Chaoyou Fu",
                "Junkang Wu",
                "Jinda Lu",
                "Kun Wang",
                "Xingyu Lu",
                "Yunhang Shen",
                "Guibin Zhang",
                "Dingjie Song",
                "Yibo Yan",
                "Tianlong Xu",
                "Qingsong Wen",
                "Zhang Zhang",
                "Yan Huang",
                "Liang Wang",
                "Tieniu Tan"
            ],
            "affiliations": [
                "Institute of automation, Chinese academy of science",
                "Lehigh University",
                "Nanjing University",
                "Nanyang Technological University",
                "National University of Singapore",
                "Shenzhen International Graduate School, Tsinghua University",
                "Squirrel Ai Learning",
                "Tencent Youtu Lab",
                "The Hong Kong University of Science and Technology",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14504.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#alignment"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò: –ø—É—Ç—å –∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –æ–±–∑–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, –≤–∫–ª—é—á–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ. –û–Ω–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –∏ –æ–±—Å—É–∂–¥–∞—é—Ç –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤. –°—Ç–∞—Ç—å—è —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏."
                },
                "en": {
                    "title": "Aligning MLLMs: Bridging Gaps for Better Understanding",
                    "desc": "This paper reviews alignment algorithms for Multimodal Large Language Models (MLLMs), which integrate visual, auditory, and textual data. It highlights the challenges of truthfulness, safety, and reasoning in MLLMs, emphasizing the need for effective alignment strategies. The authors categorize alignment algorithms based on application scenarios, dataset construction, and evaluation benchmarks. The goal is to provide a structured overview that aids researchers in advancing alignment techniques for MLLMs."
                },
                "zh": {
                    "title": "ÂØπÈΩêÁÆóÊ≥ïÂä©ÂäõÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÁöÑÊú™Êù•",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËÉΩÂ§üÈÄöËøáÁÆÄÂçïÁöÑÊèêÁ§∫Â§ÑÁêÜÂêÑÁßçÈÄöÁî®‰ªªÂä°ÔºåËÄåÊó†ÈúÄÁâπÂÆö‰ªªÂä°ÁöÑËÆ≠ÁªÉ„ÄÇÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§ÑÁêÜÊ∂âÂèäËßÜËßâ„ÄÅÂê¨ËßâÂíåÊñáÊú¨Êï∞ÊçÆÁöÑÂ§çÊùÇ‰ªªÂä°ÊñπÈù¢Â±ïÁé∞‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÖ≥‰∫éÁúüÂÆûÊÄß„ÄÅÂÆâÂÖ®ÊÄß„ÄÅÁ±ªo1Êé®ÁêÜÂíå‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩêÁ≠âÂÖ≥ÈîÆÈóÆÈ¢ò‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜËß£ÂÜ≥„ÄÇÊú¨ÊñáÊó®Âú®Á≥ªÁªüÊÄßÂú∞ÂõûÈ°æMLLMsÁöÑÂØπÈΩêÁÆóÊ≥ïÔºåÊé¢ËÆ®ÂÖ∂Â∫îÁî®Âú∫ÊôØ„ÄÅÊï∞ÊçÆÈõÜÊûÑÂª∫Ê†∏ÂøÉÂõ†Á¥†„ÄÅËØÑ‰º∞Âü∫ÂáÜ‰ª•ÂèäÊú™Êù•ÂèëÂ±ïÊñπÂêë„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14492",
            "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control",
            "url": "https://huggingface.co/papers/2503.14492",
            "abstract": "We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1.",
            "score": 8,
            "issue_id": 2777,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "5098c043161888fa",
            "authors": [
                "NVIDIA",
                ":",
                "Hassan Abu Alhaija",
                "Jose Alvarez",
                "Maciej Bala",
                "Tiffany Cai",
                "Tianshi Cao",
                "Liz Cha",
                "Joshua Chen",
                "Mike Chen",
                "Francesco Ferroni",
                "Sanja Fidler",
                "Dieter Fox",
                "Yunhao Ge",
                "Jinwei Gu",
                "Ali Hassani",
                "Michael Isaev",
                "Pooya Jannaty",
                "Shiyi Lan",
                "Tobias Lasser",
                "Huan Ling",
                "Ming-Yu Liu",
                "Xian Liu",
                "Yifan Lu",
                "Alice Luo",
                "Qianli Ma",
                "Hanzi Mao",
                "Fabio Ramos",
                "Xuanchi Ren",
                "Tianchang Shen",
                "Shitao Tang",
                "Ting-Chun Wang",
                "Jay Wu",
                "Jiashu Xu",
                "Stella Xu",
                "Kevin Xie",
                "Yuchong Ye",
                "Xiaodong Yang",
                "Xiaohui Zeng",
                "Yu Zeng"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14492.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#robotics",
                    "#transfer_learning",
                    "#open_source",
                    "#inference",
                    "#3d"
                ],
                "emoji": "üåå",
                "ru": {
                    "title": "–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∏—Ä–æ–≤ —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º",
                    "desc": "Cosmos-Transfer - —ç—Ç–æ —É—Å–ª–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–∏—Ä–æ–≤, —Å–ø–æ—Å–æ–±–Ω–∞—è —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–∏–º—É–ª—è—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π. –ú–æ–¥–µ–ª—å –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–∏–±–∫–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –≤–µ—Å–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Å–ª–æ–≤–Ω—ã—Ö –≤—Ö–æ–¥–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ª–æ–∫–∞—Ü–∏—è—Ö. –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –º–∏—Ä–æ–≤ –∏ –Ω–∞—Ö–æ–¥–∏—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –ø–µ—Ä–µ–Ω–æ—Å–∞ –º–µ–∂–¥—É –º–∏—Ä–∞–º–∏, –≤–∫–ª—é—á–∞—è Sim2Real. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ –µ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–ª—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ò–ò, –≤ —Ç–æ–º —á–∏—Å–ª–µ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏ Sim2Real –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π."
                },
                "en": {
                    "title": "Empowering World Generation with Adaptive Control",
                    "desc": "Cosmos-Transfer is a novel model designed for generating world simulations using various spatial control inputs like segmentation, depth, and edge maps. It features an adaptive and customizable spatial conditional scheme that allows for different weights to be assigned to inputs based on their spatial locations. This flexibility enhances the controllability of world generation, making it suitable for applications such as Sim2Real in robotics and autonomous vehicles. The model has been extensively evaluated, and its code is open-sourced to foster further research in the field."
                },
                "zh": {
                    "title": "ÂèØÊéßÁöÑ‰∏ñÁïåÁîüÊàêÔºåÂä©ÂäõÁâ©ÁêÜ‰∫∫Â∑•Êô∫ËÉΩ",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜCosmos-TransferÔºåËøôÊòØ‰∏ÄÁßçÊù°‰ª∂‰∏ñÁïåÁîüÊàêÊ®°ÂûãÔºåÂèØ‰ª•Ê†πÊçÆÂ§öÁßçÁ©∫Èó¥ÊéßÂà∂ËæìÂÖ•ÔºàÂ¶ÇÂàÜÂâ≤„ÄÅÊ∑±Â∫¶ÂíåËæπÁºòÔºâÁîüÊàê‰∏ñÁïåÊ®°Êãü„ÄÇÂú®ËÆæËÆ°‰∏äÔºåÁ©∫Èó¥Êù°‰ª∂ÊñπÊ°àÊòØËá™ÈÄÇÂ∫îÂíåÂèØÂÆöÂà∂ÁöÑÔºåÂÖÅËÆ∏Âú®‰∏çÂêåÁ©∫Èó¥‰ΩçÁΩÆÂØπ‰∏çÂêåÊù°‰ª∂ËæìÂÖ•ËøõË°åÂä†ÊùÉ„ÄÇËøô‰ΩøÂæó‰∏ñÁïåÁîüÊàêÂÖ∑ÊúâÈ´òÂ∫¶ÂèØÊéßÊÄßÔºåÂπ∂Âú®Â§öÁßç‰∏ñÁïåÂà∞‰∏ñÁïåÁöÑËΩ¨ÁßªÂ∫îÁî®‰∏≠ÊâæÂà∞Áî®ÈÄîÔºåÂåÖÊã¨Sim2Real„ÄÇÊàë‰ª¨ËøõË°å‰∫ÜÂπøÊ≥õÁöÑËØÑ‰º∞ÔºåÂàÜÊûê‰∫ÜÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÔºåÂπ∂Â±ïÁ§∫‰∫ÜÂÖ∂Âú®Áâ©ÁêÜ‰∫∫Â∑•Êô∫ËÉΩ‰∏≠ÁöÑÂ∫îÁî®ÔºåÂåÖÊã¨Êú∫Âô®‰∫∫Sim2RealÂíåËá™Âä®È©æÈ©∂ËΩ¶ËæÜÊï∞ÊçÆÂ¢ûÂº∫„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12505",
            "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification",
            "url": "https://huggingface.co/papers/2503.12505",
            "abstract": "Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs.",
            "score": 7,
            "issue_id": 2776,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 –º–∞—Ä—Ç–∞",
                "en": "March 16",
                "zh": "3Êúà16Êó•"
            },
            "hash": "0ac4fdd3411855ac",
            "authors": [
                "Zhaopan Xu",
                "Pengfei Zhou",
                "Jiaxin Ai",
                "Wangbo Zhao",
                "Kai Wang",
                "Xiaojiang Peng",
                "Wenqi Shao",
                "Hongxun Yao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HIT",
                "NUS",
                "SZTU",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12505.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "MPBench: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ MPBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ (PRM) –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. MPBench –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ –ø–∞—Ä–∞–¥–∏–≥–º—ã –æ—Ü–µ–Ω–∫–∏: –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤, –∞–≥—Ä–µ–≥–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–æ–≤ –∏ –ø–æ–∏—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ –æ—Ü–µ–Ω–∏—Ç—å PRM –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç —É–ª—É—á—à–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –∏ –≤—ã—è–≤–ª–µ–Ω–∏—é –æ—à–∏–±–æ–∫ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs with MPBench",
                    "desc": "This paper discusses the importance of reasoning in large language models (LLMs) and introduces process-level reward models (PRMs) to enhance their reasoning capabilities. PRMs provide step-wise rewards that help LLMs learn from their mistakes during training and improve their decision-making during inference. The authors identify a gap in existing benchmarks, which primarily focus on error detection, and propose MPBench, a new benchmark that evaluates PRMs across various reasoning scenarios. MPBench includes three evaluation paradigms: assessing step correctness, aggregating answers, and guiding the reasoning process search, offering a comprehensive framework for understanding PRMs' effectiveness."
                },
                "zh": {
                    "title": "ÂÖ®Èù¢ËØÑ‰º∞Êé®ÁêÜËøáÁ®ãÁöÑÂ§öÊ®°ÊÄÅÂü∫ÂáÜ",
                    "desc": "Êé®ÁêÜÊòØÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ§ÑÁêÜÂ§çÊùÇ‰ªªÂä°ÁöÑÈáçË¶ÅËÉΩÂäõÔºåËÄåËØÜÂà´ËøáÁ®ãÈîôËØØÂØπ‰∫éÊèêÂçáËøô‰∏ÄËÉΩÂäõËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊúÄËøëÊèêÂá∫ÁöÑËøáÁ®ãÁ∫ßÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâÈÄöËøáÊèê‰æõÈÄêÊ≠•Â•ñÂä±Ôºå‰øÉËøõ‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÂíåÊï∞ÊçÆÁîüÊàêÔºå‰ªéËÄåÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÂºïÂØºLLMsËµ∞ÂêëÊ≠£Á°ÆÁöÑÊ≠•È™§ÔºåÊèêÈ´òÊé®ÁêÜÂáÜÁ°ÆÊÄß„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑPRMsÂü∫ÂáÜ‰∏ªË¶ÅÂü∫‰∫éÊñáÊú¨Ôºå‰∏ìÊ≥®‰∫éÈîôËØØÊ£ÄÊµãÔºåÂøΩËßÜ‰∫ÜÊé®ÁêÜÊêúÁ¥¢Á≠âÂÖ∂‰ªñÂú∫ÊôØ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜMPBenchÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂ§ö‰ªªÂä°Â§öÊ®°ÊÄÅÂü∫ÂáÜÔºåÊó®Âú®Á≥ªÁªüËØÑ‰º∞PRMsÂú®‰∏çÂêåÂú∫ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14499",
            "title": "Measuring AI Ability to Complete Long Tasks",
            "url": "https://huggingface.co/papers/2503.14499",
            "abstract": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.",
            "score": 6,
            "issue_id": 2777,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "c31aeeed7f6139af",
            "authors": [
                "Thomas Kwa",
                "Ben West",
                "Joel Becker",
                "Amy Deng",
                "Katharyn Garcia",
                "Max Hasin",
                "Sami Jawhar",
                "Megan Kinniment",
                "Nate Rush",
                "Sydney Von Arx",
                "Ryan Bloom",
                "Thomas Broadley",
                "Haoxing Du",
                "Brian Goodrich",
                "Nikola Jurkovic",
                "Luke Harold Miles",
                "Seraphina Nix",
                "Tao Lin",
                "Neev Parikh",
                "David Rein",
                "Lucas Jun Koba Sato",
                "Hjalmar Wijk",
                "Daniel M. Ziegler",
                "Elizabeth Barnes",
                "Lawrence Chan"
            ],
            "affiliations": [
                "Anthropic",
                "Model Evaluation & Threat Research (METR)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14499.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "‚è≥",
                "ru": {
                    "title": "–í—Ä–µ–º—è –Ω–∞ –≤–∞—à–µ–π —Å—Ç–æ—Ä–æ–Ω–µ: –ò–ò –¥–æ–≥–æ–Ω—è–µ—Ç —á–µ–ª–æ–≤–µ–∫–∞ –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ò–ò-—Å–∏—Å—Ç–µ–º - –≥–æ—Ä–∏–∑–æ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ 50%-–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á. –≠—Ç–æ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –∏–∑–º–µ—Ä—è–µ—Ç –≤—Ä–µ–º—è, –∫–æ—Ç–æ—Ä–æ–µ –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ª—é–¥—è–º –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ –ò–ò –º–æ–≥—É—Ç –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å 50%-–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é —É—Å–ø–µ—Ö–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ò–ò, —Ç–∞–∫–∏–µ –∫–∞–∫ Claude 3.7 Sonnet, –∏–º–µ—é—Ç –≥–æ—Ä–∏–∑–æ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ –æ–∫–æ–ª–æ 50 –º–∏–Ω—É—Ç. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ –ò–ò —É–¥–≤–∞–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏–º–µ—Ä–Ω–æ –∫–∞–∂–¥—ã–µ —Å–µ–º—å –º–µ—Å—è—Ü–µ–≤ —Å 2019 –≥–æ–¥–∞, –∏ –æ–±—Å—É–∂–¥–∞—é—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏ –ò–ò-—Å–∏—Å—Ç–µ–º."
                },
                "en": {
                    "title": "Measuring AI Progress: The 50%-Task-Completion Time Horizon",
                    "desc": "This paper introduces a new metric called the 50%-task-completion time horizon to evaluate AI systems based on human performance. It measures the time it takes for humans to complete tasks that AI can accomplish with a 50% success rate. The study found that leading AI models, like Claude 3.7 Sonnet, have a time horizon of about 50 minutes, and this capability has been improving rapidly, doubling approximately every seven months. The authors highlight the implications of this trend, suggesting that AI could soon automate complex software tasks that currently require significant human effort."
                },
                "zh": {
                    "title": "AIËÉΩÂäõÁöÑÊñ∞Â∫¶ÈáèÔºö50%‰ªªÂä°ÂÆåÊàêÊó∂Èó¥",
                    "desc": "Â∞ΩÁÆ°‰∫∫Â∑•Êô∫ËÉΩÂú®Âü∫ÂáÜÊµãËØï‰∏äÂèñÂæó‰∫ÜÂø´ÈÄüËøõÂ±ïÔºå‰ΩÜÂÖ∂ÂÆûÈôÖË°®Áé∞ÁöÑÊÑè‰πâ‰ªç‰∏çÊòéÁ°Æ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ∫¶ÈáèÊ†áÂáÜÔºö50%‰ªªÂä°ÂÆåÊàêÊó∂Èó¥ËåÉÂõ¥ÔºåÊó®Âú®ÈáèÂåñAIÁ≥ªÁªü‰∏é‰∫∫Á±ªËÉΩÂäõÁöÑÂØπÊØî„ÄÇÈÄöËøáÂØπ‰∫∫Á±ª‰∏ìÂÆ∂Âú®Â§ö‰∏™‰ªªÂä°‰∏äÁöÑÂÆåÊàêÊó∂Èó¥ËøõË°åÊµãÈáèÔºåÊàë‰ª¨ÂèëÁé∞ÂΩìÂâçÁöÑÂâçÊ≤øAIÊ®°ÂûãÂú®Ëøô‰∫õ‰ªªÂä°‰∏äÁöÑ50%Êó∂Èó¥ËåÉÂõ¥Á∫¶‰∏∫50ÂàÜÈíü„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåAIÊ®°ÂûãÁöÑÊó∂Èó¥ËåÉÂõ¥Ëá™2019Âπ¥‰ª•Êù•Â§ßÁ∫¶ÊØè‰∏É‰∏™ÊúàÁøªÂÄçÔºåÊú™Êù•‰∫îÂπ¥ÂÜÖÔºåAIÁ≥ªÁªüÂèØËÉΩËÉΩÂ§üËá™Âä®ÂåñËÆ∏Â§öÁõÆÂâçÈúÄË¶Å‰∫∫Á±ª‰∏Ä‰∏™ÊúàÊâçËÉΩÂÆåÊàêÁöÑËΩØ‰ª∂‰ªªÂä°„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13265",
            "title": "FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View\n  Synthesis",
            "url": "https://huggingface.co/papers/2503.13265",
            "abstract": "Generating flexible-view 3D scenes, including 360{\\deg} rotation and zooming, from single images is challenging due to a lack of 3D data. To this end, we introduce FlexWorld, a novel framework consisting of two key components: (1) a strong video-to-video (V2V) diffusion model to generate high-quality novel view images from incomplete input rendered from a coarse scene, and (2) a progressive expansion process to construct a complete 3D scene. In particular, leveraging an advanced pre-trained video model and accurate depth-estimated training pairs, our V2V model can generate novel views under large camera pose variations. Building upon it, FlexWorld progressively generates new 3D content and integrates it into the global scene through geometry-aware scene fusion. Extensive experiments demonstrate the effectiveness of FlexWorld in generating high-quality novel view videos and flexible-view 3D scenes from single images, achieving superior visual quality under multiple popular metrics and datasets compared to existing state-of-the-art methods. Qualitatively, we highlight that FlexWorld can generate high-fidelity scenes with flexible views like 360{\\deg} rotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld.",
            "score": 6,
            "issue_id": 2785,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 –º–∞—Ä—Ç–∞",
                "en": "March 17",
                "zh": "3Êúà17Êó•"
            },
            "hash": "8b68e5c333f5fe62",
            "authors": [
                "Luxi Chen",
                "Zihan Zhou",
                "Min Zhao",
                "Yikai Wang",
                "Ge Zhang",
                "Wenhao Huang",
                "Hao Sun",
                "Ji-Rong Wen",
                "Chongxuan Li"
            ],
            "affiliations": [
                "Beijing Key Laboratory of Big Data Management and Analysis Methods",
                "ByteDance",
                "Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch MLCenter, Tsinghua University",
                "Gaoling School of AI, Renmin University of China",
                "School of Artificial Intelligence, Beijing Normal University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13265.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#video"
                ],
                "emoji": "üåê",
                "ru": {
                    "title": "–°–æ–∑–¥–∞–Ω–∏–µ –≥–∏–±–∫–∏—Ö 3D-–º–∏—Ä–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
                    "desc": "FlexWorld - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥–∏–±–∫–∏—Ö 3D-—Å—Ü–µ–Ω –∏–∑ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—è –≤—Ä–∞—â–µ–Ω–∏–µ –Ω–∞ 360¬∞ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ. –û–Ω–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –º–æ—â–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –≤–∏–¥–µ–æ-–≤-–≤–∏–¥–µ–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ–ª–Ω–æ–π 3D-—Å—Ü–µ–Ω—ã. FlexWorld –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—å –∏ —Ç–æ—á–Ω—ã–µ –ø–∞—Ä—ã –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –æ—Ü–µ–Ω–∫–æ–π –≥–ª—É–±–∏–Ω—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–æ–≤ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö –ø–æ–ª–æ–∂–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ FlexWorld –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –Ω–æ–≤—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤ –∏ –≥–∏–±–∫–∏—Ö 3D-—Å—Ü–µ–Ω –∏–∑ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Transforming Single Images into Dynamic 3D Worlds",
                    "desc": "FlexWorld is a new framework designed to create flexible 3D scenes from single images, addressing the challenge of limited 3D data. It features a video-to-video (V2V) diffusion model that generates high-quality images from incomplete scenes, allowing for significant camera pose variations. Additionally, it employs a progressive expansion process to build a complete 3D environment, integrating new content through geometry-aware scene fusion. Experiments show that FlexWorld outperforms existing methods in generating high-fidelity scenes with flexible viewing options like 360-degree rotations and zooming."
                },
                "zh": {
                    "title": "FlexWorldÔºö‰ªéÂçïÂõæÂÉèÁîüÊàêÁÅµÊ¥ªËßÜËßí3DÂú∫ÊôØÁöÑÂàõÊñ∞Ê°ÜÊû∂",
                    "desc": "FlexWorldÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®‰ªéÂçïÂº†ÂõæÂÉèÁîüÊàêÁÅµÊ¥ªËßÜËßíÁöÑ3DÂú∫ÊôØÔºåÂåÖÊã¨360Â∫¶ÊóãËΩ¨ÂíåÁº©Êîæ„ÄÇËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏§‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂Ôºö‰∏Ä‰∏™Âº∫Â§ßÁöÑËßÜÈ¢ëÂà∞ËßÜÈ¢ëÔºàV2VÔºâÊâ©Êï£Ê®°ÂûãÔºåÁî®‰∫é‰ªéÁ≤óÁ≥ôÂú∫ÊôØÊ∏≤ÊüìÁöÑÁº∫Â§±ËæìÂÖ•‰∏≠ÁîüÊàêÈ´òË¥®ÈáèÁöÑÊñ∞ËßÜÂõæÂõæÂÉèÔºå‰ª•Âèä‰∏Ä‰∏™Ê∏êËøõÊâ©Â±ïËøáÁ®ãÔºåÁî®‰∫éÊûÑÂª∫ÂÆåÊï¥ÁöÑ3DÂú∫ÊôØ„ÄÇÈÄöËøáÂà©Áî®ÂÖàËøõÁöÑÈ¢ÑËÆ≠ÁªÉËßÜÈ¢ëÊ®°ÂûãÂíåÂáÜÁ°ÆÁöÑÊ∑±Â∫¶‰º∞ËÆ°ËÆ≠ÁªÉÂØπÔºåV2VÊ®°ÂûãËÉΩÂ§üÂú®Â§ßÁõ∏Êú∫ÂßøÊÄÅÂèòÂåñ‰∏ãÁîüÊàêÊñ∞ËßÜÂõæ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFlexWorldÂú®ÁîüÊàêÈ´òË¥®ÈáèÊñ∞ËßÜÂõæËßÜÈ¢ëÂíåÁÅµÊ¥ªËßÜËßí3DÂú∫ÊôØÊñπÈù¢Ë°®Áé∞‰ºòË∂äÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14495",
            "title": "Temporal Consistency for LLM Reasoning Process Error Identification",
            "url": "https://huggingface.co/papers/2503.14495",
            "abstract": "Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency",
            "score": 5,
            "issue_id": 2779,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "590d8cdf2ae26151",
            "authors": [
                "Jiacheng Guo",
                "Yue Wu",
                "Jiahao Qiu",
                "Kaixuan Huang",
                "Xinzhe Juan",
                "Ling Yang",
                "Mengdi Wang"
            ],
            "affiliations": [
                "AI Lab, Princeton University",
                "Department of Computer Science & Engineering, University of Michigan",
                "Department of Electrical & Computer Engineering, Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14495.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#benchmark",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#architecture"
                ],
                "emoji": "üßÆ",
                "ru": {
                    "title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –æ–¥–Ω–æ—Ä–∞–∑–æ–≤–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–ª–∏ –ø–æ–¥—Ö–æ–¥–æ–≤ —Å —É—á–∞—Å—Ç–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ –≤—ã—è–≤–ª–µ–Ω–∏—é –æ—à–∏–±–æ–∫ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –ü—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –∫ –Ω–µ–¥–∞–≤–Ω–æ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –º–æ–¥–µ–ª—è–º DeepSeek R1 –º–µ—Ç–æ–¥ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø–æ–∑–≤–æ–ª–∏–≤ –º–æ–¥–µ–ª—è–º 7B/8B –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ 70B/72B –∏ GPT-4o –Ω–∞ —Ç–µ—Å—Ç–µ ProcessBench."
                },
                "en": {
                    "title": "Enhancing Verification Accuracy through Temporal Consistency",
                    "desc": "This paper introduces a novel temporal consistency method for enhancing verification in mathematical reasoning. The approach allows verifiers to iteratively refine their judgments based on previous assessments, improving accuracy over traditional one-round verification methods. By utilizing a sequence of self-reflection actions, the method shows significant performance gains on various benchmarks for identifying mathematical process errors. Notably, it enables smaller distilled models to outperform larger models, demonstrating its effectiveness in practical applications."
                },
                "zh": {
                    "title": "ÊèêÂçáÊï∞Â≠¶È™åËØÅÁöÑÊó∂Èó¥‰∏ÄËá¥ÊÄßÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊó∂Èó¥‰∏ÄËá¥ÊÄßÈ™åËØÅÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÊï∞Â≠¶Êé®ÁêÜÁöÑÊúâÊïàÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËø≠‰ª£Âú∞Ê†πÊçÆ‰πãÂâçÁöÑËØÑ‰º∞Êù•ÁªÜÂåñÂà§Êñ≠ÔºåÂÖãÊúç‰∫ÜÂçïËΩÆÈ™åËØÅÂíåÂ§öÊ®°ÂûãËæ©ËÆ∫ÁöÑÂ±ÄÈôêÊÄß„ÄÇÈÄöËøáÂú®Â§ö‰∏™Êï∞Â≠¶ËøáÁ®ãÈîôËØØËØÜÂà´Âü∫ÂáÜÔºàÂ¶ÇMathcheck„ÄÅProcessBenchÂíåPRM800KÔºâ‰∏äÁöÑÂÆûËØÅËØÑ‰º∞ÔºåÊòæÁ§∫Âá∫Áõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÁöÑ‰∏ÄËá¥ÊÄßÊÄßËÉΩÊèêÂçá„ÄÇÂ∫îÁî®‰∫éÊúÄÊñ∞ÁöÑDeepSeek R1Ëí∏È¶èÊ®°ÂûãÊó∂ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ΩøÂæó7B/8BËí∏È¶èÊ®°ÂûãÂú®ProcessBench‰∏äË∂ÖË∂ä‰∫ÜÊâÄÊúâ70B/72BÊ®°ÂûãÂíåGPT-4o„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14151",
            "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
            "url": "https://huggingface.co/papers/2503.14151",
            "abstract": "We present Concat-ID, a unified framework for identity-preserving video generation. Concat-ID employs Variational Autoencoders to extract image features, which are concatenated with video latents along the sequence dimension, leveraging solely 3D self-attention mechanisms without the need for additional modules. A novel cross-video pairing strategy and a multi-stage training regimen are introduced to balance identity consistency and facial editability while enhancing video naturalness. Extensive experiments demonstrate Concat-ID's superiority over existing methods in both single and multi-identity generation, as well as its seamless scalability to multi-subject scenarios, including virtual try-on and background-controllable generation. Concat-ID establishes a new benchmark for identity-preserving video synthesis, providing a versatile and scalable solution for a wide range of applications.",
            "score": 5,
            "issue_id": 2783,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "ba38580b0d116018",
            "authors": [
                "Yong Zhong",
                "Zhuoyi Yang",
                "Jiayan Teng",
                "Xiaotao Gu",
                "Chongxuan Li"
            ],
            "affiliations": [
                "Gaoling School of AI, Renmin University of China, Beijing, China",
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14151.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#3d",
                    "#video"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "Concat-ID: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏",
                    "desc": "Concat-ID - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ 3D –º–µ—Ö–∞–Ω–∏–∑–º—ã —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤–∏–¥–µ–æ –∏ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å—é –ª–∏—Ü. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ Concat-ID –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –æ–¥–Ω–∏–º –∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏."
                },
                "en": {
                    "title": "Identity-Preserving Video Generation Made Easy with Concat-ID",
                    "desc": "Concat-ID is a new framework designed for generating videos that maintain the identity of subjects. It uses Variational Autoencoders to capture important image features and combines them with video data using 3D self-attention, simplifying the process without extra components. The framework introduces a unique method for pairing videos and a multi-stage training approach to ensure that the generated videos are both consistent in identity and editable, while also looking natural. Through extensive testing, Concat-ID has shown to outperform existing techniques in generating videos with single and multiple identities, making it adaptable for various applications like virtual try-ons and customizable backgrounds."
                },
                "zh": {
                    "title": "Concat-IDÔºöË∫´‰ªΩ‰∏ÄËá¥ÊÄßËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Ê†áÊùÜ",
                    "desc": "Concat-ID ÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÁîüÊàê‰øùÊåÅË∫´‰ªΩ‰∏ÄËá¥ÊÄßÁöÑËßÜÈ¢ë„ÄÇÂÆÉ‰ΩøÁî®ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÊèêÂèñÂõæÂÉèÁâπÂæÅÔºåÂπ∂Â∞ÜËøô‰∫õÁâπÂæÅ‰∏éËßÜÈ¢ëÊΩúÂú®ÂèòÈáèÂú®Â∫èÂàóÁª¥Â∫¶‰∏äËøõË°åËøûÊé•ÔºåÂÆåÂÖ®‰æùËµñ‰∫é 3D Ëá™Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåËÄåÊó†ÈúÄÈ¢ùÂ§ñÊ®°Âùó„ÄÇËØ•ÊñπÊ≥ïÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑË∑®ËßÜÈ¢ëÈÖçÂØπÁ≠ñÁï•ÂíåÂ§öÈò∂ÊÆµËÆ≠ÁªÉÊñπÊ°àÔºå‰ª•Âπ≥Ë°°Ë∫´‰ªΩ‰∏ÄËá¥ÊÄßÂíåÈù¢ÈÉ®ÂèØÁºñËæëÊÄßÔºåÂêåÊó∂ÊèêÈ´òËßÜÈ¢ëÁöÑËá™ÁÑ∂ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåConcat-ID Âú®Âçï‰∏ÄÂíåÂ§öË∫´‰ªΩÁîüÊàêÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂‰∏îËÉΩÂ§üÊó†ÁºùÊâ©Â±ïÂà∞Â§ö‰∏ª‰ΩìÂú∫ÊôØÔºåÂåÖÊã¨ËôöÊãüËØïÁ©øÂíåËÉåÊôØÂèØÊéßÁîüÊàê„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13111",
            "title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs",
            "url": "https://huggingface.co/papers/2503.13111",
            "abstract": "Multimodal large language models (MLLMs) excel at 2D visual understanding but remain limited in their ability to reason about 3D space. In this work, we leverage large-scale high-quality 3D scene data with open-set annotations to introduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data covers diverse spatial tasks including spatial relationship prediction, metric size and distance estimation, and 3D grounding. We show that CA-VQA enables us to train MM-Spatial, a strong generalist MLLM that also achieves state-of-the-art performance on 3D spatial understanding benchmarks, including our own. We show how incorporating metric depth and multi-view inputs (provided in CA-VQA) can further improve 3D understanding, and demonstrate that data alone allows our model to achieve depth perception capabilities comparable to dedicated monocular depth estimation models. We will publish our SFT dataset and benchmark.",
            "score": 5,
            "issue_id": 2785,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 –º–∞—Ä—Ç–∞",
                "en": "March 17",
                "zh": "3Êúà17Êó•"
            },
            "hash": "ce045e2d94eafd19",
            "authors": [
                "Erik Daxberger",
                "Nina Wenzel",
                "David Griffiths",
                "Haiming Gang",
                "Justin Lazarow",
                "Gefen Kohavi",
                "Kai Kang",
                "Marcin Eichner",
                "Yinfei Yang",
                "Afshin Dehghan",
                "Peter Grasch"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.13111.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#3d",
                    "#multimodal",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "üè†",
                "ru": {
                    "title": "–ù–æ–≤—ã–π —à–∞–≥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é 3D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ç—Ä–µ—Ö–º–µ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö CA-VQA –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤ –∏–Ω—Ç–µ—Ä—å–µ—Ä–∞—Ö. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –º–æ–¥–µ–ª—å MM-Spatial, –ø–æ–∫–∞–∑–∞–≤—à–∞—è –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ 3D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –≤–∫–ª—é—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã –∏ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Enhancing 3D Understanding in MLLMs with CA-VQA",
                    "desc": "This paper addresses the limitations of multimodal large language models (MLLMs) in understanding 3D spaces. The authors introduce a new supervised fine-tuning dataset called Cubify Anything VQA (CA-VQA), which includes high-quality 3D scene data with open-set annotations. CA-VQA focuses on various spatial tasks such as predicting spatial relationships and estimating sizes and distances in indoor environments. The study demonstrates that their model, MM-Spatial, achieves state-of-the-art performance in 3D spatial understanding by leveraging the rich data from CA-VQA, enhancing depth perception capabilities."
                },
                "zh": {
                    "title": "ÊèêÂçá‰∏âÁª¥Á©∫Èó¥ÁêÜËß£ÁöÑÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°Âûã",
                    "desc": "ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÔºåÂÆÉÂú®ÁêÜËß£‰∫åÁª¥ËßÜËßâÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®‰∏âÁª¥Á©∫Èó¥Êé®ÁêÜ‰∏ä‰ªçÁÑ∂ÊúâÈôê„ÄÇÁ†îÁ©∂ËÄÖÂà©Áî®Â§ßËßÑÊ®°È´òË¥®ÈáèÁöÑ‰∏âÁª¥Âú∫ÊôØÊï∞ÊçÆÔºåÂàõÂª∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÁõëÁù£ÂæÆË∞ÉÊï∞ÊçÆÈõÜÂíåËØÑ‰º∞Âü∫ÂáÜÔºå‰∏ìÊ≥®‰∫éÂÆ§ÂÜÖÂú∫ÊôØ„ÄÇÊñ∞Êï∞ÊçÆÈõÜCubify Anything VQAÔºàCA-VQAÔºâÊ∂µÁõñ‰∫ÜÂ§öÁßçÁ©∫Èó¥‰ªªÂä°ÔºåÂåÖÊã¨Á©∫Èó¥ÂÖ≥Á≥ªÈ¢ÑÊµã„ÄÅÂ∫¶ÈáèÂ§ßÂ∞èÂíåË∑ùÁ¶ª‰º∞ËÆ°‰ª•Âèä‰∏âÁª¥ÂÆö‰Ωç„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈÄöËøáÁªìÂêàÂ∫¶ÈáèÊ∑±Â∫¶ÂíåÂ§öËßÜËßíËæìÂÖ•ÔºåÊ®°ÂûãÂú®‰∏âÁª¥ÁêÜËß£ÊñπÈù¢ÁöÑË°®Áé∞ÂæóÂà∞‰∫ÜÊòæËëóÊèêÂçáÔºåÁîöËá≥ËææÂà∞‰∫Ü‰∏ìÁî®ÂçïÁõÆÊ∑±Â∫¶‰º∞ËÆ°Ê®°ÂûãÁöÑËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12545",
            "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2503.12545",
            "abstract": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant concerns about privacy and security. To address these issues, machine unlearning (MU) has emerged as a promising solution, enabling the removal of specific knowledge from an already trained model without requiring retraining from scratch. Although MU for MLLMs has gained attention, current evaluations of its efficacy remain incomplete, and the underlying problem is often poorly defined, which hinders the development of strategies for creating more secure and trustworthy systems. To bridge this gap, we introduce a benchmark, named PEBench, which includes a dataset of personal entities and corresponding general event scenes, designed to comprehensively assess the performance of MU for MLLMs. Through PEBench, we aim to provide a standardized and robust framework to advance research in secure and privacy-preserving multimodal models. We benchmarked 6 MU methods, revealing their strengths and limitations, and shedding light on key challenges and opportunities for MU in MLLMs.",
            "score": 4,
            "issue_id": 2776,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 –º–∞—Ä—Ç–∞",
                "en": "March 16",
                "zh": "3Êúà16Êó•"
            },
            "hash": "8a908fbc8ce24853",
            "authors": [
                "Zhaopan Xu",
                "Pengfei Zhou",
                "Weidong Tang",
                "Jiaxin Ai",
                "Wangbo Zhao",
                "Xiaojiang Peng",
                "Kai Wang",
                "Yang You",
                "Wenqi Shao",
                "Hongxun Yao",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "HIT",
                "NUS",
                "SZTU",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "XDU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12545.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#ethics",
                    "#security",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "üîí",
                "ru": {
                    "title": "PEBench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò-–º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ PEBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ —Ä–∞–∑–æ–±—É—á–µ–Ω–∏—è (MU) –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLLM). PEBench –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –ª–∏—á–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –æ–±—â–∏–º–∏ —Å—Ü–µ–Ω–∞–º–∏ —Å–æ–±—ã—Ç–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ 6 –º–µ—Ç–æ–¥–æ–≤ MU, –≤—ã—è–≤–∏–≤ –∏—Ö —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –∏ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Enhancing Privacy in Multimodal Models with Machine Unlearning",
                    "desc": "This paper discusses the advancements of Multimodal Large Language Models (MLLMs) in tasks like visual question answering and reasoning, while highlighting the privacy concerns associated with their data usage. It introduces machine unlearning (MU) as a method to selectively remove knowledge from these models without complete retraining. The authors present PEBench, a benchmark designed to evaluate MU methods specifically for MLLMs, using a dataset of personal entities and general event scenes. By benchmarking six MU techniques, the study identifies their strengths and weaknesses, aiming to enhance the security and trustworthiness of multimodal systems."
                },
                "zh": {
                    "title": "Êé®Âä®Â§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÂÆâÂÖ®‰∏éÈöêÁßÅ‰øùÊä§",
                    "desc": "ËøëÂπ¥Êù•ÔºåÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËßÜËßâÈóÆÁ≠î„ÄÅËßÜËßâÁêÜËß£ÂíåÊé®ÁêÜÁ≠â‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õËøõÂ±ï‰æùËµñ‰∫é‰ªé‰∫íËÅîÁΩëÊî∂ÈõÜÁöÑÂ§ßÈáèÊï∞ÊçÆÔºåËøôÂºïÂèë‰∫ÜÈöêÁßÅÂíåÂÆâÂÖ®ÊñπÈù¢ÁöÑÈáçÂ§ßÊãÖÂøß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú∫Âô®ÈÅóÂøòÔºàMUÔºâ‰Ωú‰∏∫‰∏ÄÁßçÊúâÂâçÊôØÁöÑËß£ÂÜ≥ÊñπÊ°àÂ∫îËøêËÄåÁîüÔºåËÉΩÂ§üÂú®‰∏çÈúÄË¶Å‰ªéÂ§¥ÂºÄÂßãÈáçÊñ∞ËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ªéÂ∑≤ËÆ≠ÁªÉÁöÑÊ®°Âûã‰∏≠Âà†Èô§ÁâπÂÆöÁü•ËØÜ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Âü∫ÂáÜÊµãËØïPEBenchÔºåÊó®Âú®ÂÖ®Èù¢ËØÑ‰º∞MUÂú®MLLMs‰∏≠ÁöÑË°®Áé∞ÔºåÊé®Âä®ÂÆâÂÖ®ÂíåÈöêÁßÅ‰øùÊä§ÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÁ†îÁ©∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09443",
            "title": "Florenz: Scaling Laws for Systematic Generalization in Vision-Language\n  Models",
            "url": "https://huggingface.co/papers/2503.09443",
            "abstract": "Cross-lingual transfer enables vision-language models (VLMs) to perform vision tasks in various languages with training data only in one language. Current approaches rely on large pre-trained multilingual language models. However, they face the curse of multilinguality, sacrificing downstream task performance for multilingual capabilities, struggling with lexical ambiguities, and falling behind recent advances. In this work, we study the scaling laws of systematic generalization with monolingual VLMs for multilingual tasks, focusing on the impact of model size and seen training samples. We propose Florenz, a monolingual encoder-decoder VLM with 0.4B to 11.2B parameters combining the pre-trained VLM Florence-2 and the large language model Gemma-2. Florenz is trained with varying compute budgets on a synthetic dataset that features intentionally incomplete language coverage for image captioning, thus, testing generalization from the fully covered translation task. We show that not only does indirectly learning unseen task-language pairs adhere to a scaling law, but also that with our data generation pipeline and the proposed Florenz model family, image captioning abilities can emerge in a specific language even when only data for the translation task is available. Fine-tuning on a mix of downstream datasets yields competitive performance and demonstrates promising scaling trends in multimodal machine translation (Multi30K, CoMMuTE), lexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO Karpathy).",
            "score": 4,
            "issue_id": 2784,
            "pub_date": "2025-03-12",
            "pub_date_card": {
                "ru": "12 –º–∞—Ä—Ç–∞",
                "en": "March 12",
                "zh": "3Êúà12Êó•"
            },
            "hash": "c895c390f49ed5c1",
            "authors": [
                "Julian Spravil",
                "Sebastian Houben",
                "Sven Behnke"
            ],
            "affiliations": [
                "Fraunhofer IAIS, Germany",
                "Lamarr Institute for Machine Learning and Artificial Intelligence, Germany",
                "University of Applied Sciences Bonn-Rhein-Sieg, Germany",
                "University of Bonn, Computer Science Institute VI, Center for Robotics, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09443.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multilingual",
                    "#multimodal",
                    "#training",
                    "#synthetic",
                    "#machine_translation",
                    "#long_context",
                    "#transfer_learning"
                ],
                "emoji": "üåê",
                "ru": {
                    "title": "–ú–æ–Ω–æ–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—é—Ç —è–∑—ã–∫–æ–≤—ã–µ –±–∞—Ä—å–µ—Ä—ã –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–Ω–æ–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–æ–¥–µ–ª—å Florenz, –æ–±—ä–µ–¥–∏–Ω—è—é—â—É—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å Florence-2 –∏ —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å Gemma-2. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —Å –Ω–µ–ø–æ–ª–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–ø–∏—Å–∞–Ω–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —è–∑—ã–∫–µ –º–æ–∂–µ—Ç –ø–æ—è–≤–∏—Ç—å—Å—è –¥–∞–∂–µ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–¥–∞—á–∏ –ø–µ—Ä–µ–≤–æ–¥–∞."
                },
                "en": {
                    "title": "Florenz: Bridging Languages with Monolingual Vision-Language Models",
                    "desc": "This paper introduces Florenz, a monolingual vision-language model (VLM) designed to enhance cross-lingual transfer for vision tasks. Unlike existing multilingual models that struggle with performance due to the complexities of multiple languages, Florenz leverages a single language to achieve systematic generalization across various languages. The model is built on a combination of the pre-trained VLM Florence-2 and the large language model Gemma-2, with a focus on scaling laws related to model size and training data. Results show that Florenz can effectively learn to perform image captioning in new languages, even when trained only on translation tasks, demonstrating strong performance in several downstream applications."
                },
                "zh": {
                    "title": "ÂçïËØ≠Ê®°ÂûãÁöÑË∑®ËØ≠Ë®ÄËÉΩÂäõÊñ∞Á™ÅÁ†¥",
                    "desc": "ËøôÁØáËÆ∫ÊñáÁ†îÁ©∂‰∫ÜË∑®ËØ≠Ë®ÄËøÅÁßªÂ¶Ç‰Ωï‰ΩøËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®‰ªÖÁî®‰∏ÄÁßçËØ≠Ë®ÄÁöÑËÆ≠ÁªÉÊï∞ÊçÆ‰∏ãÊâßË°åËßÜËßâ‰ªªÂä°„ÄÇÂΩìÂâçÁöÑÊñπÊ≥ï‰æùËµñ‰∫éÂ§ßÂûãÈ¢ÑËÆ≠ÁªÉÁöÑÂ§öËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩÜÂú®Â§öËØ≠Ë®ÄËÉΩÂäõ‰∏é‰∏ãÊ∏∏‰ªªÂä°ÊÄßËÉΩ‰πãÈó¥Â≠òÂú®ÊùÉË°°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜFlorenzÔºå‰∏Ä‰∏™ÂçïËØ≠ÁºñÁ†ÅÂô®-Ëß£Á†ÅÂô®VLMÔºåÁªìÂêà‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑFlorence-2ÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãGemma-2ÔºåÂÖ∑Êúâ0.4BÂà∞11.2BÁöÑÂèÇÊï∞„ÄÇÈÄöËøáÂú®ÂêàÊàêÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÔºåFlorenzÂ±ïÁ§∫‰∫ÜÂç≥‰ΩøÂè™ÊúâÁøªËØë‰ªªÂä°ÁöÑÊï∞ÊçÆÔºå‰πüËÉΩÂú®ÁâπÂÆöËØ≠Ë®Ä‰∏≠‰∫ßÁîüÂõæÂÉèÊèèËø∞ËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12303",
            "title": "Towards Self-Improving Systematic Cognition for Next-Generation\n  Foundation MLLMs",
            "url": "https://huggingface.co/papers/2503.12303",
            "abstract": "Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) face challenges with fine-grained perception and complex reasoning. Prevalent multimodal pre-training approaches focus on enhancing perception by training on high-quality image captions due to the extremely high cost of collecting chain-of-thought (CoT) reasoning data for improving reasoning. While leveraging advanced MLLMs for caption generation enhances scalability, the outputs often lack comprehensiveness and accuracy. In this paper, we introduce Self-Improving cognition (SIcog), a self-learning framework designed to construct next-generation foundation MLLMs by enhancing their systematic cognitive capabilities through multimodal pre-training with self-generated data. Specifically, we propose Chain-of-Description, an approach that improves an MLLM's systematic perception by enabling step-by-step visual understanding, ensuring greater comprehensiveness and accuracy. Additionally, we adopt a structured CoT reasoning technique to enable MLLMs to integrate in-depth multimodal reasoning. To construct a next-generation foundation MLLM with self-improved cognition, SIcog first equips an MLLM with systematic perception and reasoning abilities using minimal external annotations. The enhanced models then generate detailed captions and CoT reasoning data, which are further curated through self-consistency. This curated data is ultimately used for multimodal pre-training to develop next-generation foundation models. Extensive experiments on both low- and high-resolution MLLMs across diverse benchmarks demonstrate that, with merely 213K self-generated pre-training samples, SIcog produces next-generation foundation MLLMs with significantly improved cognition, achieving benchmark-leading performance compared to prevalent pre-training approaches.",
            "score": 3,
            "issue_id": 2787,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 –º–∞—Ä—Ç–∞",
                "en": "March 16",
                "zh": "3Êúà16Êó•"
            },
            "hash": "d0f6251740b9c15c",
            "authors": [
                "Xiaoying Zhang",
                "Da Peng",
                "Yipeng Zhang",
                "Zonghao Guo",
                "Chengyue Wu",
                "Chi Chen",
                "Wei Ke",
                "Helen Meng",
                "Maosong Sun"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "The University of Hong Kong",
                "Tsinghua University",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12303.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–°–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—â–∏–π—Å—è –ò–ò: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SIcog - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM). SIcog –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ Chain-of-Description –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –ø–æ —Ü–µ–ø–æ—á–∫–µ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –§—Ä–µ–π–º–≤–æ—Ä–∫ —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∞–µ—Ç MLLM —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –≤–Ω–µ—à–Ω–∏–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏, –∑–∞—Ç–µ–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏ –∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SIcog –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ MLLM, –¥–æ—Å—Ç–∏–≥–∞—è –≤–µ–¥—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö."
                },
                "en": {
                    "title": "Enhancing MLLMs with Self-Improving Cognition for Better Perception and Reasoning",
                    "desc": "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in fine-grained perception and complex reasoning. It introduces Self-Improving cognition (SIcog), a framework that enhances MLLMs by using self-generated data for multimodal pre-training. The proposed Chain-of-Description method allows MLLMs to develop systematic visual understanding, improving the quality of generated captions and reasoning. Through minimal external annotations, SIcog enables MLLMs to achieve superior performance on various benchmarks with significantly fewer training samples."
                },
                "zh": {
                    "title": "Ëá™ÊàëÊîπËøõËÆ§Áü•ÔºåÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊô∫ËÉΩ",
                    "desc": "Â∞ΩÁÆ°Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂÖ∑ÊúâÂº∫Â§ßÁöÑËÉΩÂäõÔºå‰ΩÜÂú®ÁªÜÁ≤íÂ∫¶ÊÑüÁü•ÂíåÂ§çÊùÇÊé®ÁêÜÊñπÈù¢‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇÁé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÈ¢ÑËÆ≠ÁªÉÊñπÊ≥ï‰∏ªË¶ÅÈÄöËøáÈ´òË¥®ÈáèÁöÑÂõæÂÉèÊèèËø∞Êù•Â¢ûÂº∫ÊÑüÁü•Ôºå‰ΩÜÊî∂ÈõÜÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÊé®ÁêÜÊï∞ÊçÆÁöÑÊàêÊú¨ÊûÅÈ´ò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™ÊàëÊîπËøõËÆ§Áü•Ê°ÜÊû∂ÔºàSIcogÔºâÔºåÊó®Âú®ÈÄöËøáËá™ÁîüÊàêÊï∞ÊçÆÁöÑÂ§öÊ®°ÊÄÅÈ¢ÑËÆ≠ÁªÉÊù•ÊèêÂçáMLLMÁöÑÁ≥ªÁªüËÆ§Áü•ËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÊèèËø∞ÈìæÊñπÊ≥ïËÉΩÂ§üÈÄêÊ≠•Â¢ûÂº∫ËßÜËßâÁêÜËß£ÔºåÁ°Æ‰øùÁîüÊàêÁöÑÊèèËø∞Êõ¥ÂÖ®Èù¢„ÄÅÊõ¥ÂáÜÁ°ÆÔºåÂêåÊó∂ÁªìÂêàÁªìÊûÑÂåñÁöÑCoTÊé®ÁêÜÊäÄÊúØÔºåÂÆûÁé∞Ê∑±ÂÖ•ÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12271",
            "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection",
            "url": "https://huggingface.co/papers/2503.12271",
            "abstract": "The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach.",
            "score": 3,
            "issue_id": 2778,
            "pub_date": "2025-03-15",
            "pub_date_card": {
                "ru": "15 –º–∞—Ä—Ç–∞",
                "en": "March 15",
                "zh": "3Êúà15Êó•"
            },
            "hash": "2f560e2ec0839955",
            "authors": [
                "Shufan Li",
                "Konstantinos Kallidromitis",
                "Akash Gokul",
                "Arsh Koneru",
                "Yusuke Kato",
                "Kazuki Kozuka",
                "Aditya Grover"
            ],
            "affiliations": [
                "Panasonic AI Research",
                "Salesforce AI Research",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12271.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#benchmark",
                    "#inference",
                    "#diffusion",
                    "#reasoning"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "Reflect-DiT: –£–º–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Reflect-DiT –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ best-of-N, Reflect-DiT –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º Diffusion Transformer –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–Ω–µ–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç–æ–≤—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ GenEval, –¥–æ—Å—Ç–∏–≥–∞—è –Ω–æ–≤–æ–≥–æ —Ä–µ–∫–æ—Ä–¥–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 0.81. Reflect-DiT –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ inference-time scaling –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Refining Image Generation with Reflective Learning",
                    "desc": "This paper presents Reflect-DiT, a novel method for enhancing text-to-image generation by incorporating in-context reflection capabilities into Diffusion Transformers. Unlike traditional best-of-N sampling, which generates multiple images and selects the best, Reflect-DiT refines its outputs based on previous generations and textual feedback. This approach allows the model to focus on specific areas for improvement, leading to more tailored and effective image generation. Experimental results show that Reflect-DiT achieves a new state-of-the-art performance on the GenEval benchmark with fewer samples, demonstrating its efficiency compared to larger models."
                },
                "zh": {
                    "title": "ÂèçÊÄùÁîüÊàêÔºåÊèêÂçáÂõæÂÉèË¥®ÈáèÔºÅ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊñπÊ≥ïÔºåÁß∞‰∏∫Reflect-DiT„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®ÁîüÊàêËøáÁ®ã‰∏≠ÂºïÂÖ•‰∏ä‰∏ãÊñáÂèçÊÄùËÉΩÂäõÔºåÂ∏ÆÂä©Diffusion TransformersÊ†πÊçÆ‰πãÂâçÁîüÊàêÁöÑÂõæÂÉèÂíåÊñáÊú¨ÂèçÈ¶àËøõË°åÊîπËøõ„ÄÇ‰∏é‰º†ÁªüÁöÑÊúÄ‰Ω≥NÈááÊ†∑ÊñπÊ≥ï‰∏çÂêåÔºåReflect-DiTËÉΩÂ§üÈíàÂØπÁâπÂÆöÁöÑÊîπËøõÈúÄÊ±ÇËøõË°åË∞ÉÊï¥Ôºå‰ªéËÄåÊèêÈ´òÁîüÊàêË¥®Èáè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåReflect-DiTÂú®GenEvalÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÂàÜÊï∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10410",
            "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation",
            "url": "https://huggingface.co/papers/2503.10410",
            "abstract": "Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration errors, sparse information, and multi-view consistency, leading to poor performance on recent published datasets. To significantly enhance roadside collaborative perception and address critical data issues, we present the first simulation framework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable of generating diverse, multi-view consistent simulated roadside data through dynamic foreground editing and full-scene style transfer of a single image. RoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures accurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View Occlusion-Aware Sampler (MOAS) determines the placement of diverse digital assets within 3D space; (3) DepthSAM innovatively models foreground-background relationships from single-frame fixed-view images, ensuring multi-view consistency of foreground; and (4) Scalable Post-Processing Toolkit generates more realistic and enriched scenes through style transfer and other enhancements. RoCo-Sim significantly improves roadside 3D object detection, outperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on TUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception simulation. Code and pre-trained models will be released soon: https://github.com/duyuwen-duen/RoCo-Sim",
            "score": 2,
            "issue_id": 2776,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 –º–∞—Ä—Ç–∞",
                "en": "March 13",
                "zh": "3Êúà13Êó•"
            },
            "hash": "44150f611040e79d",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#data",
                    "#dataset",
                    "#cv",
                    "#open_source"
                ],
                "emoji": "üöó",
                "ru": {
                    "title": "RoCo-Sim: –ø—Ä–æ—Ä—ã–≤ –≤ —Å–∏–º—É–ª—è—Ü–∏–∏ –¥–æ—Ä–æ–∂–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è",
                    "desc": "RoCo-Sim - —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∏–º—É–ª—è—Ü–∏–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –¥–æ—Ä–æ–∂–Ω–æ–π –æ–±—Å—Ç–∞–Ω–æ–≤–∫–∏. –û–Ω–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏, —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—É—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. RoCo-Sim –≤–∫–ª—é—á–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≤–Ω–µ—à–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–∞–º–µ—Ä, –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã–π –≤—ã–±–æ—Ä—â–∏–∫ —Å —É—á–µ—Ç–æ–º –æ–∫–∫–ª—é–∑–∏–π, –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –ø–µ—Ä–µ–¥–Ω–∏–π –ø–ª–∞–Ω-—Ñ–æ–Ω –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏. –°–∏—Å—Ç–µ–º–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç 3D-–¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "Enhancing Roadside Awareness with Collaborative Perception",
                    "desc": "This paper introduces Roadside Collaborative Perception, a system where multiple roadside units work together to improve vehicle awareness of their surroundings. It highlights the limitations of current methods that focus mainly on model design while neglecting important data issues such as calibration errors and sparse information. To address these challenges, the authors present RoCo-Sim, a simulation framework that generates diverse and consistent roadside data using advanced techniques like dynamic foreground editing. RoCo-Sim significantly enhances roadside 3D object detection performance, surpassing state-of-the-art methods on benchmark datasets."
                },
                "zh": {
                    "title": "ÊèêÂçáË∑ØËæπÊÑüÁü•ÁöÑÂçèÂêåÂäõÈáè",
                    "desc": "Ë∑ØËæπÂçèÂêåÊÑüÁü•ÊòØ‰∏ÄÁßçÁ≥ªÁªüÔºåÂ§ö‰∏™Ë∑ØËæπÂçïÂÖÉÂçè‰ΩúÊ±áËÅöÊÑüÁü•Êï∞ÊçÆÔºåÂ∏ÆÂä©ËΩ¶ËæÜÊèêÈ´òÁéØÂ¢ÉÊÑèËØÜ„ÄÇÁé∞ÊúâÁöÑË∑ØËæπÊÑüÁü•ÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®Ê®°ÂûãËÆæËÆ°Ôºå‰ΩÜÂøΩËßÜ‰∫ÜÊï∞ÊçÆÈóÆÈ¢òÔºåÂ¶ÇÊ†°ÂáÜËØØÂ∑Æ„ÄÅ‰ø°ÊÅØÁ®ÄÁñèÂíåÂ§öËßÜÂõæ‰∏ÄËá¥ÊÄßÔºåÂØºËá¥Âú®ÊúÄÊñ∞Êï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞‰∏ç‰Ω≥„ÄÇ‰∏∫ÊòæËëóÊèêÂçáË∑ØËæπÂçèÂêåÊÑüÁü•Âπ∂Ëß£ÂÜ≥ÂÖ≥ÈîÆÊï∞ÊçÆÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÈ¶ñ‰∏™Ë∑ØËæπÂçèÂêåÊÑüÁü•Ê®°ÊãüÊ°ÜÊû∂RoCo-Sim„ÄÇRoCo-SimËÉΩÂ§üÈÄöËøáÂä®ÊÄÅÂâçÊôØÁºñËæëÂíåÂçïÂõæÂÉèÁöÑÂÖ®Âú∫ÊôØÈ£éÊ†ºËøÅÁßªÁîüÊàêÂ§öÊ†∑ÂåñÁöÑ„ÄÅÂ§öËßÜÂõæ‰∏ÄËá¥ÁöÑÊ®°ÊãüË∑ØËæπÊï∞ÊçÆ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14002",
            "title": "MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific\n  Generative Modeling",
            "url": "https://huggingface.co/papers/2503.14002",
            "abstract": "Generative models have recently made remarkable progress in the field of 3D objects. However, their practical application in fields like engineering remains limited since they fail to deliver the accuracy, quality, and controllability needed for domain-specific tasks. Fine-tuning large generative models is a promising perspective for making these models available in these fields. Creating high-quality, domain-specific 3D datasets is crucial for fine-tuning large generative models, yet the data filtering and annotation process remains a significant bottleneck. We present MeshFleet, a filtered and annotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive publicly available collection of 3D objects. Our approach proposes a pipeline for automated data filtering based on a quality classifier. This classifier is trained on a manually labeled subset of Objaverse, incorporating DINOv2 and SigLIP embeddings, refined through caption-based analysis and uncertainty estimation. We demonstrate the efficacy of our filtering method through a comparative analysis against caption and image aesthetic score-based techniques and fine-tuning experiments with SV3D, highlighting the importance of targeted data selection for domain-specific 3D generative modeling.",
            "score": 1,
            "issue_id": 2789,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 –º–∞—Ä—Ç–∞",
                "en": "March 18",
                "zh": "3Êúà18Êó•"
            },
            "hash": "8affb04cd78c70c9",
            "authors": [
                "Damian Boborzi",
                "Phillip Mueller",
                "Jonas Emrich",
                "Dominik Schmid",
                "Sebastian Mueller",
                "Lars Mikelsons"
            ],
            "affiliations": [
                "BMW Group",
                "University of Augsburg"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14002.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#optimization",
                    "#data",
                    "#3d"
                ],
                "emoji": "üöó",
                "ru": {
                    "title": "MeshFleet: –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è 3D –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MeshFleet - –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö 3D –º–æ–¥–µ–ª–µ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∏–∑ Objaverse-XL. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∫–∞—á–µ—Å—Ç–≤–∞, –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –Ω–∞ –≤—Ä—É—á–Ω—É—é —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ Objaverse —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ DINOv2 –∏ SigLIP. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –ø—É—Ç–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –º–µ—Ç–æ–¥–∞–º–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞ –ø–æ–¥–ø–∏—Å—è—Ö –∏ –æ—Ü–µ–Ω–∫–∞—Ö —ç—Å—Ç–µ—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–º–µ—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ 3D –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è."
                },
                "en": {
                    "title": "Enhancing 3D Generative Models with Targeted Data Selection",
                    "desc": "This paper discusses advancements in generative models for creating 3D objects, particularly focusing on their application in engineering. It highlights the challenges of achieving the necessary accuracy and quality for specific tasks, which can be addressed by fine-tuning these models with high-quality datasets. The authors introduce MeshFleet, a curated 3D vehicle dataset derived from Objaverse-XL, aimed at improving the training process of generative models. They also present a novel automated data filtering method using a quality classifier, which enhances the dataset's relevance and effectiveness for domain-specific applications."
                },
                "zh": {
                    "title": "MeshFleetÔºöÊèêÂçá3DÁîüÊàêÊ®°ÂûãÁöÑÂÖ≥ÈîÆÊï∞ÊçÆÈõÜ",
                    "desc": "ÁîüÊàêÊ®°ÂûãÂú®3DÁâ©‰ΩìÈ¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®Â∑•Á®ãÁ≠âÈ¢ÜÂüüÁöÑÂÆûÈôÖÂ∫îÁî®‰ªçÁÑ∂ÊúâÈôêÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Êó†Ê≥ïÊèê‰æõÊâÄÈúÄÁöÑÂáÜÁ°ÆÊÄß„ÄÅË¥®ÈáèÂíåÂèØÊéßÊÄß„ÄÇÂØπÂ§ßÂûãÁîüÊàêÊ®°ÂûãËøõË°åÂæÆË∞ÉÊòØ‰ΩøËøô‰∫õÊ®°ÂûãÂú®ÁâπÂÆöÈ¢ÜÂüüÂèØÁî®ÁöÑÊúâÂâçÊôØÁöÑÊñπÊ≥ï„ÄÇÂàõÂª∫È´òË¥®Èáè„ÄÅÁâπÂÆöÈ¢ÜÂüüÁöÑ3DÊï∞ÊçÆÈõÜÂØπ‰∫éÂæÆË∞ÉÂ§ßÂûãÁîüÊàêÊ®°ÂûãËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÊï∞ÊçÆËøáÊª§ÂíåÊ†áÊ≥®ËøáÁ®ã‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÈáçÂ§ßÁì∂È¢à„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜMeshFleetÔºåËøôÊòØ‰∏Ä‰∏™‰ªéObjaverse-XLÊèêÂèñÁöÑÁªèËøáËøáÊª§ÂíåÊ†áÊ≥®ÁöÑ3DËΩ¶ËæÜÊï∞ÊçÆÈõÜÔºåÂ±ïÁ§∫‰∫ÜÂü∫‰∫éË¥®ÈáèÂàÜÁ±ªÂô®ÁöÑËá™Âä®ÂåñÊï∞ÊçÆËøáÊª§ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13661",
            "title": "Pensez: Less Data, Better Reasoning -- Rethinking French LLM",
            "url": "https://huggingface.co/papers/2503.13661",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, achieving strong performance in specialized domains like mathematical reasoning and non-English languages often requires extensive training on massive datasets. This paper investigates a contrasting approach: strategic fine-tuning on a small, high-quality, bilingual (English-French) dataset to enhance both the reasoning capabilities and French language proficiency of a large language model. Rather than relying on scale, we explore the hypothesis that targeted data curation and optimized training can achieve competitive, or even superior, performance. We demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000 carefully selected samples, significant improvements in mathematical reasoning. Specifically, Pensez 7B exhibits an increase in accuracy of the base model up to 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark. These results challenge the prevailing assumption that massive datasets are aprerequisite for strong reasoning performance in LLMs, highlighting the potential of strategic data curation and optimized fine-tuning for enhancing both specialized skills and multilingual capabilities. Our findings have implications for the efficient development of high-performing, multilingual LLMs, especially in resource-constrained scenarios.",
            "score": 1,
            "issue_id": 2784,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 –º–∞—Ä—Ç–∞",
                "en": "March 17",
                "zh": "3Êúà17Êó•"
            },
            "hash": "248eff76119a7839",
            "authors": [
                "Huy Hoang Ha"
            ],
            "affiliations": [
                "Menlo Research",
                "Universit√© Grenoble Alpes"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13661.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#multilingual",
                    "#data",
                    "#training",
                    "#transfer_learning",
                    "#low_resource"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ú–∞–ª—ã–µ –¥–∞–Ω–Ω—ã–µ, –±–æ–ª—å—à–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è",
                    "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—É—Ç–µ–º —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –¥–≤—É—è–∑—ã—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –∏ –≤–ª–∞–¥–µ–Ω–∏—é —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–º —è–∑—ã–∫–æ–º –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤—Å–µ–≥–æ 2000 —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö. –ú–æ–¥–µ–ª—å Pensez 7B –ø–æ–∫–∞–∑–∞–ª–∞ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–æ 20% –Ω–∞ —Ç–µ—Å—Ç–µ AIME25 –∏ –Ω–∞ 12% –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–º —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–µ —Ç–µ—Å—Ç–∞ MATH —É—Ä–æ–≤–Ω—è 5. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å—Ç–∞–≤—è—Ç –ø–æ–¥ —Å–æ–º–Ω–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–≥—Ä–æ–º–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM –≤ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö."
                },
                "en": {
                    "title": "Strategic Fine-Tuning: Small Data, Big Gains!",
                    "desc": "This paper explores how to improve large language models (LLMs) in specialized areas like math and French without needing huge datasets. It focuses on strategic fine-tuning using a small, high-quality bilingual dataset of only 2,000 samples. The results show that this targeted approach can significantly boost reasoning accuracy, achieving up to a 20% improvement in mathematical tasks. This challenges the idea that only large datasets can lead to strong performance, suggesting that careful data selection and training can be more effective."
                },
                "zh": {
                    "title": "Â∞èÊï∞ÊçÆÈõÜ‰πüËÉΩÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®Êï∞Â≠¶Êé®ÁêÜÂíåÈùûËã±ËØ≠ËØ≠Ë®ÄÁ≠â‰∏ì‰∏öÈ¢ÜÂüüÁöÑË°®Áé∞ÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÊï∞ÊçÆÁöÑËÆ≠ÁªÉ„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßç‰∏çÂêåÁöÑÊñπÊ≥ïÔºöÈÄöËøáÂú®Â∞èËßÑÊ®°È´òË¥®ÈáèÁöÑÂèåËØ≠ÔºàËã±ËØ≠-Ê≥ïËØ≠ÔºâÊï∞ÊçÆÈõÜ‰∏äËøõË°åÊàòÁï•ÊÄßÂæÆË∞ÉÔºå‰ª•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÊ≥ïËØ≠Ê∞¥Âπ≥„ÄÇÊàë‰ª¨ÈÄöËøáÂØπ‰ªÖ2000‰∏™Á≤æÂøÉÊåëÈÄâÁöÑÊ†∑Êú¨ËøõË°åÊúâÈíàÂØπÊÄßÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÔºåÂú®Êï∞Â≠¶Êé®ÁêÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊèêÂçáÔºåPensez 7BÂú®AIME25‰∏äÁöÑÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü20%ÔºåÂú®Ê≥ïËØ≠MATH 5Á∫ßÂü∫ÂáÜ‰∏äÊèêÈ´ò‰∫Ü12%„ÄÇËøô‰∫õÁªìÊûúÊåëÊàò‰∫ÜÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÊòØLLMsÂº∫Â§ßÊé®ÁêÜÊÄßËÉΩÁöÑÂÖàÂÜ≥Êù°‰ª∂ÁöÑÊôÆÈÅçÂÅáËÆæÔºåÁ™ÅÊòæ‰∫ÜÊàòÁï•Êï∞ÊçÆÁ≠ñÂàíÂíå‰ºòÂåñÂæÆË∞ÉÂú®ÊèêÂçá‰∏ì‰∏öÊäÄËÉΩÂíåÂ§öËØ≠Ë®ÄËÉΩÂäõÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12127",
            "title": "Hyperbolic Safety-Aware Vision-Language Models",
            "url": "https://huggingface.co/papers/2503.12127",
            "abstract": "Addressing the retrieval of unsafe content from vision-language models such as CLIP is an important step towards real-world integration. Current efforts have relied on unlearning techniques that try to erase the model's knowledge of unsafe concepts. While effective in reducing unwanted outputs, unlearning limits the model's capacity to discern between safe and unsafe content. In this work, we introduce a novel approach that shifts from unlearning to an awareness paradigm by leveraging the inherent hierarchical properties of the hyperbolic space. We propose to encode safe and unsafe content as an entailment hierarchy, where both are placed in different regions of hyperbolic space. Our HySAC, Hyperbolic Safety-Aware CLIP, employs entailment loss functions to model the hierarchical and asymmetrical relations between safe and unsafe image-text pairs. This modelling, ineffective in standard vision-language models due to their reliance on Euclidean embeddings, endows the model with awareness of unsafe content, enabling it to serve as both a multimodal unsafe classifier and a flexible content retriever, with the option to dynamically redirect unsafe queries toward safer alternatives or retain the original output. Extensive experiments show that our approach not only enhances safety recognition but also establishes a more adaptable and interpretable framework for content moderation in vision-language models. Our source code is available at https://github.com/aimagelab/HySAC.",
            "score": 1,
            "issue_id": 2784,
            "pub_date": "2025-03-15",
            "pub_date_card": {
                "ru": "15 –º–∞—Ä—Ç–∞",
                "en": "March 15",
                "zh": "3Êúà15Êó•"
            },
            "hash": "fe42b5bcd1d5d4be",
            "authors": [
                "Tobia Poppi",
                "Tejaswi Kasarla",
                "Pascal Mettes",
                "Lorenzo Baraldi",
                "Rita Cucchiara"
            ],
            "affiliations": [
                "IIT-CNR, Italy",
                "University of Amsterdam, Netherlands",
                "University of Modena and Reggio Emilia, Italy",
                "University of Pisa, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12127.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#open_source",
                    "#cv",
                    "#security"
                ],
                "emoji": "üõ°Ô∏è",
                "ru": {
                    "title": "–ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—è –¥–ª—è –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ CLIP. –í–º–µ—Å—Ç–æ —É–¥–∞–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –æ –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏—è—Ö, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–µ—Ä–∞—Ä—Ö–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∏ –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –ú–æ–¥–µ–ª—å HySAC (Hyperbolic Safety-Aware CLIP) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –º–µ–∂–¥—É –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –∏ –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É–ª—É—á—à–∞–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –±–æ–ª–µ–µ –≥–∏–±–∫—É—é —Å–∏—Å—Ç–µ–º—É –º–æ–¥–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."
                },
                "en": {
                    "title": "Enhancing Safety Awareness in Vision-Language Models with Hyperbolic Space",
                    "desc": "This paper addresses the challenge of retrieving unsafe content from vision-language models like CLIP by introducing a new approach called Hyperbolic Safety-Aware CLIP (HySAC). Instead of using unlearning techniques that erase knowledge of unsafe concepts, HySAC utilizes the hierarchical properties of hyperbolic space to encode safe and unsafe content as an entailment hierarchy. By employing entailment loss functions, the model can better understand the relationships between safe and unsafe image-text pairs, enhancing its ability to classify and moderate content. The results demonstrate that HySAC improves safety recognition and offers a more flexible framework for content moderation in multimodal applications."
                },
                "zh": {
                    "title": "ÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÊÑèËØÜ",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰Ωï‰ªéËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàÂ¶ÇCLIPÔºâ‰∏≠Ê£ÄÁ¥¢‰∏çÂÆâÂÖ®ÂÜÖÂÆπ„ÄÇÂΩìÂâçÁöÑÊñπÊ≥ï‰∏ªË¶Å‰æùËµñ‰∫éÊ∂àÈô§Â≠¶‰π†ÊäÄÊúØÔºå‰ΩÜËøôÁßçÊñπÊ≥ïÈôêÂà∂‰∫ÜÊ®°ÂûãÂå∫ÂàÜÂÆâÂÖ®Âíå‰∏çÂÆâÂÖ®ÂÜÖÂÆπÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÂà©Áî®ÂèåÊõ≤Á©∫Èó¥ÁöÑÂ±ÇÊ¨°ÁâπÊÄßÔºåÂ∞ÜÂÆâÂÖ®Âíå‰∏çÂÆâÂÖ®ÂÜÖÂÆπÁºñÁ†Å‰∏∫Ëï¥Âê´Â±ÇÊ¨°ÁªìÊûÑ„ÄÇÊàë‰ª¨ÁöÑHySACÊ®°ÂûãÈÄöËøáËï¥Âê´ÊçüÂ§±ÂáΩÊï∞Âª∫Ê®°ÂÆâÂÖ®‰∏é‰∏çÂÆâÂÖ®ÂõæÂÉè-ÊñáÊú¨ÂØπ‰πãÈó¥ÁöÑÂ±ÇÊ¨°Âíå‰∏çÂØπÁß∞ÂÖ≥Á≥ªÔºå‰ªéËÄåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÂØπ‰∏çÂÆâÂÖ®ÂÜÖÂÆπÁöÑÊÑèËØÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10546",
            "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation",
            "url": "https://huggingface.co/papers/2503.10546",
            "abstract": "With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at http://kuda-dynamics.github.io.",
            "score": 1,
            "issue_id": 2776,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 –º–∞—Ä—Ç–∞",
                "en": "March 13",
                "zh": "3Êúà13Êó•"
            },
            "hash": "f856affbe8bcf064",
            "authors": [
                "Zixian Liu",
                "Mingtong Zhang",
                "Yunzhu Li"
            ],
            "affiliations": [
                "Columbia University",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10546.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#robotics",
                    "#open_source",
                    "#agents"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "KUDA: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º",
                    "desc": "KUDA - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –¥–∏–Ω–∞–º–∏–∫–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª–∏ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è (VLM) –∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–∏–Ω–∞–º–∏–∫–∏ –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–æ–±–æ—Ç–∞. KUDA —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–∑–Ω–∞—á–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ –Ω–∞ RGB-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç VLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ü–µ–ª–µ–≤—ã—Ö —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π. –ó–∞—Ç–µ–º —ç—Ç–∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ —Ñ—É–Ω–∫—Ü–∏–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–∏–Ω–∞–º–∏–∫–∏."
                },
                "en": {
                    "title": "KUDA: Bridging Language and Dynamics for Robotic Manipulation",
                    "desc": "This paper presents KUDA, an innovative open-vocabulary robotic manipulation system that incorporates dynamics learning and visual prompting. Unlike previous methods, KUDA utilizes keypoints to create target specifications that are understandable by vision-language models (VLMs) and can be transformed into cost functions for planning. The system processes language instructions and visual data to assign keypoints to images, which are then optimized using a learned dynamics model to generate robotic movements. The effectiveness of KUDA is validated through various manipulation tasks, showcasing its ability to handle complex interactions with different object types."
                },
                "zh": {
                    "title": "KUDAÔºöÂä®ÊÄÅÂ≠¶‰π†‰∏éËßÜËßâÊèêÁ§∫ÁöÑÂºÄÊîæËØçÊ±áÊìç‰ΩúÁ≥ªÁªü",
                    "desc": "ÈöèÁùÄÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÔºåÂºÄÊîæËØçÊ±áÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÁ≥ªÁªüÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ï„ÄÇÁÑ∂ËÄåÔºåËÆ∏Â§öÁé∞ÊúâÊñπÊ≥ïÂøΩËßÜ‰∫ÜÁâ©‰ΩìÂä®ÊÄÅÁöÑÈáçË¶ÅÊÄßÔºåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®Êõ¥Â§çÊùÇÂä®ÊÄÅ‰ªªÂä°‰∏≠ÁöÑÈÄÇÁî®ÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜKUDAÔºå‰∏Ä‰∏™ÈõÜÊàê‰∫ÜÂä®ÊÄÅÂ≠¶‰π†ÂíåÈÄöËøáÂÖ≥ÈîÆÁÇπËøõË°åËßÜËßâÊèêÁ§∫ÁöÑÂºÄÊîæËØçÊ±áÊìç‰ΩúÁ≥ªÁªüÔºåÂà©Áî®‰∫ÜVLMsÂíåÂü∫‰∫éÂ≠¶‰π†ÁöÑÁ•ûÁªèÂä®ÊÄÅÊ®°Âûã„ÄÇKUDAÈÄöËøáÂ∞ÜÂÖ≥ÈîÆÁÇπÂàÜÈÖçÁªôRGBÂõæÂÉèÔºåÂπ∂Êü•ËØ¢VLMÁîüÊàêÁõÆÊ†áËßÑËåÉÔºåÂ∞ÜÊäΩË±°ÁöÑÂÖ≥ÈîÆÁÇπË°®Á§∫ËΩ¨Êç¢‰∏∫ÊàêÊú¨ÂáΩÊï∞Ôºå‰ªéËÄå‰ºòÂåñÊú∫Âô®‰∫∫ËΩ®Ëøπ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08893",
            "title": "EvalTree: Profiling Language Model Weaknesses via Hierarchical\n  Capability Trees",
            "url": "https://huggingface.co/papers/2503.08893",
            "abstract": "An ideal model evaluation should achieve two goals: identifying where the model fails and providing actionable improvement guidance. Toward these goals for Language Model (LM) evaluations, we formulate the problem of generating a weakness profile, a set of weaknesses expressed in natural language, given an LM's performance on every individual instance in a benchmark. We introduce a suite of quantitative assessments to compare different weakness profiling methods. We also propose a weakness profiling method EvalTree. It constructs a capability tree where each node represents a capability described in natural language and is linked to a subset of benchmark instances that specifically evaluate this capability; it then extracts nodes where the LM performs poorly to generate a weakness profile. On the MATH and WildChat benchmarks, we show that EvalTree outperforms baseline weakness profiling methods by identifying weaknesses more precisely and comprehensively. Weakness profiling further enables weakness-guided data collection, and training data collection guided by EvalTree-identified weaknesses improves LM performance more than other data collection strategies. We also show how EvalTree exposes flaws in Chatbot Arena's human-voter-based evaluation practice. To facilitate future work, we release our code and an interface that allows practitioners to interactively explore the capability trees built by EvalTree.",
            "score": 1,
            "issue_id": 2790,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 –º–∞—Ä—Ç–∞",
                "en": "March 11",
                "zh": "3Êúà11Êó•"
            },
            "hash": "8645a737473c8209",
            "authors": [
                "Zhiyuan Zeng",
                "Yizhong Wang",
                "Hannaneh Hajishirzi",
                "Pang Wei Koh"
            ],
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "Paul G. Allen School of Computer Science & Engineering, University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08893.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#data",
                    "#interpretability",
                    "#training",
                    "#open_source"
                ],
                "emoji": "üå≥",
                "ru": {
                    "title": "EvalTree: —Ç–æ—á–Ω–æ–µ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–∞–±–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ EvalTree –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è —Å–ª–∞–±–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. EvalTree —Å—Ç—Ä–æ–∏—Ç –¥–µ—Ä–µ–≤–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π, –≥–¥–µ –∫–∞–∂–¥—ã–π —É–∑–µ–ª —Å–≤—è–∑–∞–Ω —Å –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏—Ö –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç–µ –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–ª–∞–±–æ—Å—Ç–µ–π –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö MATH –∏ WildChat. –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–∞–±–æ—Å—Ç–µ–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ —Å–æ–±–∏—Ä–∞—Ç—å –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π."
                },
                "en": {
                    "title": "Uncovering Weaknesses for Stronger Language Models",
                    "desc": "This paper focuses on improving the evaluation of Language Models (LMs) by creating a weakness profile that identifies specific areas where the model underperforms. The authors introduce EvalTree, a method that organizes model capabilities into a tree structure, linking each capability to benchmark instances that test it. By analyzing these capabilities, EvalTree generates a detailed profile of weaknesses, allowing for targeted improvements in model training. The results demonstrate that using EvalTree leads to better performance in LMs by guiding data collection based on identified weaknesses, surpassing traditional evaluation methods."
                },
                "zh": {
                    "title": "ËØÜÂà´Ê®°ÂûãÂº±ÁÇπÔºåÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØ≠Ë®ÄÊ®°ÂûãËØÑ‰º∞ÊñπÊ≥ïÔºåÊó®Âú®ËØÜÂà´Ê®°ÂûãÁöÑÂº±ÁÇπÂπ∂Êèê‰æõÊîπËøõÂª∫ËÆÆ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂº±ÁÇπÂàÜÊûêÁöÑÊ¶ÇÂøµÔºåÈÄöËøáÁîüÊàêÂº±ÁÇπÊ°£Ê°àÊù•ÊèèËø∞Ê®°ÂûãÂú®Âü∫ÂáÜÊµãËØï‰∏≠ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑEvalTreeÊñπÊ≥ïÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ËÉΩÂäõÊ†ëÔºåËÉΩÂ§üÊõ¥ÂáÜÁ°ÆÂú∞ËØÜÂà´Ê®°ÂûãÁöÑ‰∏çË∂≥‰πãÂ§Ñ„ÄÇÈÄöËøáÂú®MATHÂíåWildChatÂü∫ÂáÜÊµãËØï‰∏äÁöÑÂÆûÈ™åÔºåEvalTreeÊòæÁ§∫Âá∫ÊØî‰º†ÁªüÊñπÊ≥ïÊõ¥‰ºòË∂äÁöÑÊÄßËÉΩÔºåÂπ∂‰∏îËÉΩÂ§üÊåáÂØºÊï∞ÊçÆÊî∂ÈõÜ‰ª•ÊèêÂçáÊ®°ÂûãË°®Áé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08683",
            "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous\n  Driving",
            "url": "https://huggingface.co/papers/2503.08683",
            "abstract": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise for improving safety by addressing the perception and prediction uncertainties inherent in single-agent systems. However, traditional cooperative methods are constrained by rigid collaboration protocols and limited generalization to unseen interactive scenarios. While LLM-based approaches offer generalized reasoning capabilities, their challenges in spatial planning and unstable inference latency hinder their direct application in cooperative driving. To address these limitations, we propose CoLMDriver, the first full-pipeline LLM-based cooperative driving system, enabling effective language-based negotiation and real-time driving control. CoLMDriver features a parallel driving pipeline with two key components: (i) an LLM-based negotiation module under an actor-critic paradigm, which continuously refines cooperation policies through feedback from previous decisions of all vehicles; and (ii) an intention-guided waypoint generator, which translates negotiation outcomes into executable waypoints. Additionally, we introduce InterDrive, a CARLA-based simulation benchmark comprising 10 challenging interactive driving scenarios for evaluating V2V cooperation. Experimental results demonstrate that CoLMDriver significantly outperforms existing approaches, achieving an 11% higher success rate across diverse highly interactive V2V driving scenarios. Code will be released on https://github.com/cxliu0314/CoLMDriver.",
            "score": 0,
            "issue_id": 2790,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 –º–∞—Ä—Ç–∞",
                "en": "March 11",
                "zh": "3Êúà11Êó•"
            },
            "hash": "ef46cd2c17c64880",
            "authors": [
                "Changxing Liu",
                "Genjia Liu",
                "Zijun Wang",
                "Jinchang Yang",
                "Siheng Chen"
            ],
            "affiliations": [
                "Multi-Agent Governance & Intelligence Crew (MAGIC)",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08683.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#rl",
                    "#agents"
                ],
                "emoji": "üöó",
                "ru": {
                    "title": "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç –Ω–æ–≤—É—é —ç—Ä—É –≤ –∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–º –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º –≤–æ–∂–¥–µ–Ω–∏–∏",
                    "desc": "CoLMDriver - —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –≤–æ–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥—É–ª—å –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø—É—Ç–µ–≤—ã—Ö —Ç–æ—á–µ–∫, —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π –Ω–∞–º–µ—Ä–µ–Ω–∏—è–º–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ InterDrive - —Å–∏–º—É–ª—è—Ü–∏–æ–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –Ω–∞ –±–∞–∑–µ CARLA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–æ–ø–µ—Ä–∞—Ü–∏–∏ –º–µ–∂–¥—É —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã–º–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ CoLMDriver –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã, –¥–æ—Å—Ç–∏–≥–∞—è –Ω–∞ 11% –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π."
                },
                "en": {
                    "title": "Revolutionizing V2V Cooperation with CoLMDriver!",
                    "desc": "This paper introduces CoLMDriver, a novel system for vehicle-to-vehicle (V2V) cooperative autonomous driving that leverages large language models (LLMs) for improved safety and decision-making. CoLMDriver addresses the limitations of traditional methods by implementing a full-pipeline approach that includes a negotiation module and a waypoint generator, allowing vehicles to communicate and plan collaboratively in real-time. The negotiation module uses an actor-critic framework to adaptively refine cooperation strategies based on feedback from previous interactions. Experimental results show that CoLMDriver outperforms existing systems, achieving a higher success rate in complex driving scenarios, demonstrating its effectiveness in enhancing V2V cooperation."
                },
                "zh": {
                    "title": "CoLMDriverÔºöÊô∫ËÉΩÂçè‰ΩúÈ©æÈ©∂ÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫CoLMDriverÁöÑÂÖ®ÊµÅÁ®ãÂü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂêà‰ΩúÈ©æÈ©∂Á≥ªÁªüÔºåÊó®Âú®ÊèêÈ´òËΩ¶ËæÜÈó¥ÁöÑÂÆâÂÖ®ÊÄß„ÄÇËØ•Á≥ªÁªüÈÄöËøáËØ≠Ë®ÄÂü∫Á°ÄÁöÑÂçèÂïÜÂíåÂÆûÊó∂È©æÈ©∂ÊéßÂà∂ÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÂêà‰ΩúÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇCoLMDriverÂåÖÂê´‰∏§‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºöÂü∫‰∫éÊºîÂëò-ËØÑËÆ∫ÂÆ∂ËåÉÂºèÁöÑÂçèÂïÜÊ®°ÂùóÂíåÊÑèÂõæÂºïÂØºÁöÑË∑ØÂæÑÁÇπÁîüÊàêÂô®ÔºåËÉΩÂ§üÊúâÊïàÂú∞Â∞ÜÂçèÂïÜÁªìÊûúËΩ¨Âåñ‰∏∫ÂèØÊâßË°åÁöÑË∑ØÂæÑÁÇπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCoLMDriverÂú®Â§öÁßçÈ´òÂ∫¶‰∫íÂä®ÁöÑËΩ¶ËæÜÈó¥È©æÈ©∂Âú∫ÊôØ‰∏≠ÔºåÊàêÂäüÁéáÊØîÁé∞ÊúâÊñπÊ≥ïÊèêÈ´ò‰∫Ü11%„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-03-18.html",
    "link_next": "2025-03-20.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "18.03",
        "en": "03/18",
        "zh": "3Êúà18Êó•"
    },
    "short_date_next": {
        "ru": "20.03",
        "en": "03/20",
        "zh": "3Êúà20Êó•"
    },
    "categories": {
        "#dataset": 11,
        "#data": 5,
        "#benchmark": 15,
        "#agents": 3,
        "#cv": 4,
        "#rl": 3,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 7,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 10,
        "#math": 1,
        "#multilingual": 3,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 2,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 11,
        "#transfer_learning": 5,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 2,
        "#optimization": 10,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 5,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 10,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1,
        "#creativity": 1
    },
    "zh": {
        "text": "Êàë‰ª¨‰ªãÁªç‰∫Ü RWKV-7 \"Goose\"Ôºå‰∏ÄÁßçÊñ∞ÁöÑÂ∫èÂàóÂª∫Ê®°Êû∂ÊûÑ„ÄÇÂÆÉÂú®Â§öËØ≠Ë®Ä‰ªªÂä°‰∏äËææÂà∞‰∫ÜÊñ∞ÁöÑÊúÄ‰Ω≥ÊÄßËÉΩÔºåÂ∞ΩÁÆ°ËÆ≠ÁªÉÁöÑ‰ª§ÁâåÊï∞ÈáèÂ∞ë„ÄÇRWKV-7 Ê®°ÂûãÊØè‰∏™‰ª§ÁâåÁöÑÂÜÖÂ≠òÂíåÊé®ÁêÜÊó∂Èó¥ÊòØÂ∏∏Êï∞„ÄÇÂÆÉÂºïÂÖ•‰∫ÜÊñ∞ÁöÑ delta ËßÑÂàôÂíåÊîæÊùæÁöÑÂÄºÊõøÊç¢ËßÑÂàô„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫Ü RWKV-7 ÁöÑËØ≠Ë®ÄÂª∫Ê®°ËÉΩÂäõÔºåÂπ∂ÂèëÂ∏É‰∫ÜÊ®°ÂûãÂíåÊï∞ÊçÆÈõÜ„ÄÇ",
        "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
        "pinyin": "W«ímen ji√®sh√†o le RWKV-7 \"Goose\", yƒ´ zh«íng xƒ´n de x√πli√® ji√†nm√≥ ji√†g√≤u. TƒÅ z√†i du≈çy«îy√°n r√®nw√π sh√†ng d√°d√†o le xƒ´n de zu√¨jiƒÅ x√¨ngn√©ng, j«êngu«én x√πnli√†n de l√¨ngp√°i sh√πli√†ng sh«éo. RWKV-7 m√≥x√≠ng mƒõi ge l√¨ngp√°i de n√®ic√∫n h√© tuƒ´l«ê sh√≠jiƒÅn sh√¨ ch√°ngsh√π. TƒÅ y«ênr√π le xƒ´n de delta guƒ´z√© h√© f√†ngs≈çng de zh√≠ t√¨hu√†n guƒ´z√©. W«ímen zh«énsh√¨ le RWKV-7 de y«îy√°n ji√†nm√≥ n√©ngl√¨, b√¨ng fƒÅb√π le m√≥x√≠ng h√© sh√πj√πj√≠.",
        "vocab": "[\n    {\"word\": \"‰ªãÁªç\", \"pinyin\": \"ji√® sh√†o\", \"trans\": \"introduce\"},\n    {\"word\": \"Â∫èÂàó\", \"pinyin\": \"x√π li√®\", \"trans\": \"sequence\"},\n    {\"word\": \"Âª∫Ê®°\", \"pinyin\": \"ji√†n m√≥\", \"trans\": \"modeling\"},\n    {\"word\": \"Êû∂ÊûÑ\", \"pinyin\": \"ji√† g√≤u\", \"trans\": \"architecture\"},\n    {\"word\": \"Â§öËØ≠Ë®Ä\", \"pinyin\": \"du≈ç y«î y√°n\", \"trans\": \"multilingual\"},\n    {\"word\": \"‰ªªÂä°\", \"pinyin\": \"r√®n wu\", \"trans\": \"task\"},\n    {\"word\": \"ËææÂà∞\", \"pinyin\": \"d√° d√†o\", \"trans\": \"achieve\"},\n    {\"word\": \"ÊúÄ‰Ω≥\", \"pinyin\": \"zu√¨ jiƒÅ\", \"trans\": \"best\"},\n    {\"word\": \"ÊÄßËÉΩ\", \"pinyin\": \"x√¨ng n√©ng\", \"trans\": \"performance\"},\n    {\"word\": \"Â∞ΩÁÆ°\", \"pinyin\": \"j√¨n gu«én\", \"trans\": \"although\"},\n    {\"word\": \"ËÆ≠ÁªÉ\", \"pinyin\": \"x√πn li√†n\", \"trans\": \"training\"},\n    {\"word\": \"‰ª§Áâå\", \"pinyin\": \"l√¨ng p√†i\", \"trans\": \"token\"},\n    {\"word\": \"Êï∞Èáè\", \"pinyin\": \"sh√π li√†ng\", \"trans\": \"quantity\"},\n    {\"word\": \"Â∞ë\", \"pinyin\": \"sh«éo\", \"trans\": \"few\"},\n    {\"word\": \"ÂÜÖÂ≠ò\", \"pinyin\": \"n√®i c√∫n\", \"trans\": \"memory\"},\n    {\"word\": \"Êé®ÁêÜ\", \"pinyin\": \"tuƒ´ l«ê\", \"trans\": \"inference\"},\n    {\"word\": \"Êó∂Èó¥\", \"pinyin\": \"sh√≠ jiƒÅn\", \"trans\": \"time\"},\n    {\"word\": \"Â∏∏Êï∞\", \"pinyin\": \"ch√°ng sh√π\", \"trans\": \"constant\"},\n    {\"word\": \"ÂºïÂÖ•\", \"pinyin\": \"y«ên r√π\", \"trans\": \"introduce\"},\n    {\"word\": \"ËßÑÂàô\", \"pinyin\": \"guƒ´ z√©\", \"trans\": \"rule\"},\n    {\"word\": \"ÊîæÊùæ\", \"pinyin\": \"f√†ng s≈çng\", \"trans\": \"relax\"},\n    {\"word\": \"ÂÄº\", \"pinyin\": \"zh√≠\", \"trans\": \"value\"},\n    {\"word\": \"ÊõøÊç¢\", \"pinyin\": \"t√¨ hu√†n\", \"trans\": \"replace\"},\n    {\"word\": \"Â±ïÁ§∫\", \"pinyin\": \"zh«én sh√¨\", \"trans\": \"demonstrate\"},\n    {\"word\": \"ËÉΩÂäõ\", \"pinyin\": \"n√©ng l√¨\", \"trans\": \"ability\"},\n    {\"word\": \"ÂèëÂ∏É\", \"pinyin\": \"fƒÅ b√π\", \"trans\": \"release\"},\n    {\"word\": \"Êï∞ÊçÆÈõÜ\", \"pinyin\": \"sh√π j√π j√≠\", \"trans\": \"dataset\"}\n]",
        "trans": "We introduced RWKV-7 \"Goose,\" a new sequence modeling architecture. It achieved new best performance on multilingual tasks despite being trained with fewer tokens. The RWKV-7 model has constant memory and inference time per token. It introduces new delta rules and relaxed value substitution rules. We demonstrated the language modeling capabilities of RWKV-7 and released the model and dataset.",
        "update_ts": "2025-03-19 09:12"
    }
}