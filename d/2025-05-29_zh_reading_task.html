
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
            <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){dataLayer.push(arguments);}
                gtag('js', new Date());
                gtag('config', 'G-C1CRWDNJ1J');
            </script>
            <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
            <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@100..900&display=swap" rel="stylesheet">
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Chinese reading task about ML</title>
            <style>
                body {
                    font-family: Arial, sans-serif;
                    background-color: #f4f4f9;
                    color: #333;
                    margin: 0;
                    padding: 20px;
                }
                .container {
                    max-width: 800px;
                    margin: 0 auto;
                    background-color: #fff;
                    padding: 20px;
                    border-radius: 8px;
                    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
                }
                h1 {
                    color: #0056b3;
                    text-align: center;
                }
                p {
                    line-height: 1.6;
                }
                .zh-text {
                    font-size: 1.3em;
                    font-family: 'Noto Sans SC';
                    font-weight: 300;
                    margin: 0 0 5px 0;
                }
                .pinyin {
                    padding-top: 5px;
                    padding-bottom: 5px;
                    font-style: italic;
                    color: #888;
                }
                table {
                    width: 100%;
                    border-collapse: collapse;
                    margin-top: 20px;
                }
                th, td {
                    padding: 12px;
                    border: 1px solid #ddd;
                    text-align: left;
                }
                th {
                    background-color: #0056b3;
                    color: #fff;
                }
                td {
                    background-color: #f9f9f9;
                }
                td.zh {
                    font-family: 'Noto Sans SC';
                    font-size: 1.2em;
                    font-weight: 400;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in
  Learning to Reason</h1>
                <div><p class='zh-text'>1. 这篇文章研究了大语言模型（LLMs）在强化学习中的表现。</p>
<p class='zh-text'>2. 研究发现，LLMs在奖励噪音（reward noise）存在的情况下仍能表现出强大的鲁棒性。</p>
<p class='zh-text'>3. 即使在数学任务中手动翻转40%的奖励函数输出，模型仍能快速收敛，提高其数学任务的表现。</p>
<p class='zh-text'>4. 通过奖励关键推理短语（RPR）的出现，模型在开放式任务中的表现也得到了提升。</p>
<p class='zh-text'>5. 这些发现强调了改进模型在预训练阶段的基础能力的重要性。</p></div>
                <div class="pinyin">
                    <p>1. 这篇文章研究了大语言模型（LLMs）在强化学习中的表现。研究发现，LLMs在奖励噪音（reward noise）存在的情况下仍能表现出强大的鲁棒性。即使在数学任务中手动翻转40%的奖励函数输出，模型仍能快速收敛，提高其数学任务的表现。通过奖励关键推理短语（RPR）的出现，模型在开放式任务中的表现也得到了提升。这些发现强调了改进模型在预训练阶段的基础能力的重要性。

Zhè piān wénzhāng yánjiū le dà yǔyán móxíng (LLMs) zài qiángzhù xuéxí zhōng de biǎoxiàn</p>
<p>2.  Yánjiū fāxiàn, LLMs zài jiǎnglì zàoyīn (reward noise) cúnzài de qíngkuàng xià réng néng biǎoxiàn chū qiángdà de lǔcǔxìng</p>
<p>3.  Jíshǐ zài shùxué rènwù zhōng shǒudòng fānzhuǎn 40% de jiǎnglì hánshù shūchū, móxíng réng néng kuàisù shōuliǎn, tígāo qí shùxué rènwù de biǎoxiàn</p>
<p>4.  Tōngguò jiǎnglì guǎnjiàn tuīlǐ duǎnyǔ (RPR) de chūxiàn, móxíng zài kāifàngshì rènwù zhōng de biǎoxiàn yě dédào le tíshēng</p>
<p>5.  Zhèxiē fāxiàn qiángdiào le gǎijìn móxíng zài yùxùn jiēduàn de jīchǔ nénglì de zhòngyàoxìng</p>
                </div>
                <div><p>1. This article investigates the performance of large language models (LLMs) in reinforcement learning.</p>
<p>2.  The study found that LLMs exhibit strong robustness even in the presence of reward noise.</p>
<p>3.  Even when manually flipping 40% of the reward function outputs in mathematical tasks, the model can still converge quickly and improve its performance on these tasks.</p>
<p>4.  By rewarding the appearance of key reasoning phrases (RPR), the model's performance on open-ended tasks is also enhanced.</p>
<p>5.  These findings underscore the importance of improving the model's foundational capabilities during the pre-training phase.</p></div>
                <h2>Vocabulary</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Word</th>
                            <th>Pinyin</th>
                            <th>Translation</th>
                        </tr>
                    </thead>
                    <tbody>
        
                        <tr>
                            <td class="zh">大语言模型</td>
                            <td>dà yǔyán móxíng</td>
                            <td>large language model</td>
                        </tr>
            
                        <tr>
                            <td class="zh">强化学习</td>
                            <td>qiáng huà xuéxí</td>
                            <td>reinforcement learning</td>
                        </tr>
            
                        <tr>
                            <td class="zh">表现</td>
                            <td>biǎo xiàn</td>
                            <td>performance</td>
                        </tr>
            
                        <tr>
                            <td class="zh">奖励噪音</td>
                            <td>jiǎng lì zào yīn</td>
                            <td>reward noise</td>
                        </tr>
            
                        <tr>
                            <td class="zh">鲁棒性</td>
                            <td>lǔ bāng xìng</td>
                            <td>robustness</td>
                        </tr>
            
                        <tr>
                            <td class="zh">手动翻转</td>
                            <td>shǒu dòng fān zhuǎn</td>
                            <td>manually flip</td>
                        </tr>
            
                        <tr>
                            <td class="zh">奖励函数</td>
                            <td>jiǎng lì hán shù</td>
                            <td>reward function</td>
                        </tr>
            
                        <tr>
                            <td class="zh">输出</td>
                            <td>shū chū</td>
                            <td>output</td>
                        </tr>
            
                        <tr>
                            <td class="zh">快速收敛</td>
                            <td>kuài sù shōu liǎn</td>
                            <td>rapid convergence</td>
                        </tr>
            
                        <tr>
                            <td class="zh">提高</td>
                            <td>tí gāo</td>
                            <td>improve</td>
                        </tr>
            
                        <tr>
                            <td class="zh">奖励关键推理短语</td>
                            <td>jiǎng lì guǎn jiàn tuī lǐ duǎn yǔ</td>
                            <td>reward key reasoning phrases</td>
                        </tr>
            
                        <tr>
                            <td class="zh">开放式任务</td>
                            <td>kāi fàng shì rèn wù</td>
                            <td>open-ended tasks</td>
                        </tr>
            
                        <tr>
                            <td class="zh">发现</td>
                            <td>fā xiàn</td>
                            <td>findings</td>
                        </tr>
            
                        <tr>
                            <td class="zh">强调</td>
                            <td>qiáng diào</td>
                            <td>emphasize</td>
                        </tr>
            
                        <tr>
                            <td class="zh">改进</td>
                            <td>gǎi jìn</td>
                            <td>improve</td>
                        </tr>
            
                        <tr>
                            <td class="zh">预训练阶段</td>
                            <td>yù xùn liàn jiē duàn</td>
                            <td>pre-training stage</td>
                        </tr>
            
                        <tr>
                            <td class="zh">基础能力</td>
                            <td>jī chǔ néng lì</td>
                            <td>foundational capabilities</td>
                        </tr>
            
                    </tbody>
                </table>
            </div>
        </body>
        </html>
        