{
    "date": {
        "ru": "23 Ğ¸ÑĞ»Ñ",
        "en": "July 23",
        "zh": "7æœˆ23æ—¥"
    },
    "time_utc": "2025-07-23 01:00",
    "weekday": 2,
    "issue_id": 4958,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.15846",
            "title": "GUI-G^2: Gaussian Reward Modeling for GUI Grounding",
            "url": "https://huggingface.co/papers/2507.15846",
            "abstract": "Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G^2), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G^2 incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G^2, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks.",
            "score": 98,
            "issue_id": 4936,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "d36bacfa3f66add9",
            "authors": [
                "Fei Tang",
                "Zhangxuan Gu",
                "Zhengxi Lu",
                "Xuyang Liu",
                "Shuheng Shen",
                "Changhua Meng",
                "Wen Wang",
                "Wenqi Zhang",
                "Yongliang Shen",
                "Weiming Lu",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15846.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ–±ï¸",
                "ru": {
                    "title": "Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ GUI",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ (GUI). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ GUI Gaussian Grounding Rewards (GUI-G^2), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° ĞºĞ°Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GUI-G^2 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 24.7% Ğ½Ğ° ScreenSpot-Pro."
                },
                "en": {
                    "title": "Revolutionizing GUI Interaction with Continuous Gaussian Rewards",
                    "desc": "This paper presents a new method called GUI Gaussian Grounding Rewards (GUI-G^2) for improving how machines interact with graphical user interfaces (GUIs) using natural language instructions. Unlike traditional reinforcement learning methods that use simple binary rewards, GUI-G^2 models GUI elements as continuous Gaussian distributions, allowing for more nuanced and effective learning. The framework includes mechanisms for precise localization and spatial alignment, which help the model understand where to click based on human-like behavior. Experiments show that GUI-G^2 significantly outperforms existing methods, demonstrating better adaptability to different interface designs and improved overall performance in GUI tasks."
                },
                "zh": {
                    "title": "é«˜æ–¯å¥–åŠ±æ¡†æ¶æå‡GUIäº¤äº’ç²¾åº¦",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¡†æ¶ï¼Œç§°ä¸ºGUI Gaussian Grounding Rewardsï¼ˆGUI-G^2ï¼‰ï¼Œç”¨äºå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„ç²¾ç¡®ä½ç½®ã€‚ä¸ä¼ ç»Ÿçš„äºŒå…ƒå¥–åŠ±æ–¹æ³•ä¸åŒï¼ŒGUI-G^2é€šè¿‡å°†GUIå…ƒç´ å»ºæ¨¡ä¸ºè¿ç»­çš„é«˜æ–¯åˆ†å¸ƒï¼Œæä¾›äº†æ›´ä¸°å¯Œçš„æ¢¯åº¦ä¿¡å·ï¼Œä¿ƒè¿›äº†æ¨¡å‹çš„ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é«˜æ–¯ç‚¹å¥–åŠ±å’Œè¦†ç›–å¥–åŠ±ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†ä¸åŒå…ƒç´ çš„å°ºåº¦ï¼Œå¹¶æé«˜äº†æ¨¡å‹åœ¨ç•Œé¢å˜åŒ–ä¸­çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGUI-G^2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨GUIäº¤äº’ä»»åŠ¡ä¸­çš„æ–°èŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.14683",
            "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via\n  Context-Aware Multi-Stage Policy Optimization",
            "url": "https://huggingface.co/papers/2507.14683",
            "abstract": "The MiroMind-M1 series of open-source reasoning language models achieves state-of-the-art performance on mathematical reasoning benchmarks through a two-stage training process and Context-Aware Multi-Stage Policy Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have recently evolved from fluent text generation to advanced reasoning across diverse domains, giving rise to reasoning language models. Among these domains, mathematical reasoning serves as a representative benchmark as it requires precise multi-step logic and abstract reasoning, which can be generalized to other tasks. While closed-source RLMs such as GPT-o3 demonstrate impressive reasoning capabilities, their proprietary nature limits transparency and reproducibility. Although many open-source projects aim to close this gap, most of them lack sufficient openness by omitting critical resources such as datasets and detailed training configurations, which hinders reproducibility. To contribute toward greater transparency in RLM development, we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on the Qwen-2.5 backbone that match or exceed the performance of existing open-source RLMs. Specifically, our models are trained in two stages: SFT on a carefully curated corpus of 719K math-reasoning problems with verified CoT trajectories, followed by RLVR on 62K challenging and verifiable problems. To enhance the robustness and efficiency of the RLVR process, we introduce Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates length-progressive training with an adaptive repetition penalty to encourage context-aware RL training. Our model achieves state-of-the-art or competitive performance and superior token efficiency among Qwen-2.5-based open-source 7B and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B, MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K, MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope these resources will support further research and foster community advancement.",
            "score": 91,
            "issue_id": 4937,
            "pub_date": "2025-07-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ»Ñ",
                "en": "July 19",
                "zh": "7æœˆ19æ—¥"
            },
            "hash": "47799c3d5002f685",
            "authors": [
                "Xingxuan Li",
                "Yao Xiao",
                "Dianwen Ng",
                "Hai Ye",
                "Yue Deng",
                "Xiang Lin",
                "Bin Wang",
                "Zhanfeng Mo",
                "Chong Zhang",
                "Yueyi Zhang",
                "Zonglin Yang",
                "Ruilin Li",
                "Lei Lei",
                "Shihao Xu",
                "Han Zhao",
                "Weiling Chen",
                "Feng Ji",
                "Lidong Bing"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.14683.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ",
                    "desc": "MiroMind-M1 - ÑÑ‚Ğ¾ ÑĞµÑ€Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 719 Ñ‚Ñ‹ÑÑÑ‡ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 62 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Context-Aware Multi-Stage Policy Optimization Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ÑĞµ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Open-Source Models for Superior Mathematical Reasoning",
                    "desc": "The MiroMind-M1 series introduces open-source reasoning language models that excel in mathematical reasoning tasks through a two-stage training approach. The first stage involves supervised fine-tuning (SFT) on a large dataset of math problems, while the second stage employs reinforcement learning with verified responses (RLVR) to refine the model's reasoning capabilities. To improve training efficiency, the authors propose a novel Context-Aware Multi-Stage Policy Optimization algorithm that adapts training based on context and problem complexity. By providing complete access to models, datasets, and training configurations, this work aims to enhance transparency and reproducibility in the development of reasoning language models."
                },
                "zh": {
                    "title": "å¼€æºæ¨ç†æ¨¡å‹çš„é€æ˜æ€§ä¸å…ˆè¿›æ€§",
                    "desc": "MiroMind-M1ç³»åˆ—æ˜¯ä¸€ä¸ªå¼€æºæ¨ç†è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šé˜¶æ®µç­–ç•¥ä¼˜åŒ–ï¼Œåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚è¿™äº›æ¨¡å‹é¦–å…ˆåœ¨ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„719Kæ•°å­¦æ¨ç†é—®é¢˜ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œç„¶ååœ¨62Kå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ éªŒè¯ã€‚ä¸ºäº†æé«˜å¼ºåŒ–å­¦ä¹ éªŒè¯è¿‡ç¨‹çš„é²æ£’æ€§å’Œæ•ˆç‡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œç»“åˆäº†é•¿åº¦æ¸è¿›è®­ç»ƒå’Œè‡ªé€‚åº”é‡å¤æƒ©ç½šã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡å‘å¸ƒå®Œæ•´çš„æ¨¡å‹ã€æ•°æ®é›†å’Œè®­ç»ƒé…ç½®ï¼Œä¿ƒè¿›ç ”ç©¶çš„å¯é‡å¤æ€§å’Œç¤¾åŒºçš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.14843",
            "title": "The Invisible Leash: Why RLVR May Not Escape Its Origin",
            "url": "https://huggingface.co/papers/2507.14843",
            "abstract": "Theoretical and empirical analysis reveals that Reinforcement Learning with Verifiable Rewards (RLVR) enhances precision but narrows exploration, limiting its ability to discover novel solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large reasoning models highlight Reinforcement Learning with Verifiable Rewards (RLVR) as a promising method for enhancing AI's capabilities, particularly in solving complex logical tasks. However, it remains unclear whether RLVR truly expands a model's reasoning boundary or merely amplifies high-reward outputs that the base model already knows for improved precision. This study presents a theoretical and empirical investigation that provides fresh insights into the potential limits of RLVR. First, we offer a new theoretical perspective that RLVR is constrained by the base model's support-unable to sample solutions with zero initial probability-and operates as a conservative reweighting mechanism that may restrict the discovery of entirely original solutions. We also identify an entropy-reward tradeoff: while RLVR reliably enhances precision, it may progressively narrow exploration and potentially overlook correct yet underrepresented solutions. Extensive empirical experiments validate that while RLVR consistently improves pass@1, the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets, failing to recover correct answers that were previously accessible to the base model. Interestingly, we also observe that while RLVR sometimes increases token-level entropy, resulting in greater uncertainty at each generation step, answer-level entropy declines, indicating that these seemingly more uncertain paths ultimately converge onto a smaller set of distinct answers. Taken together, these findings reveal potential limits of RLVR in extending reasoning horizons. Breaking this invisible leash may require future algorithmic innovations such as explicit exploration mechanisms or hybrid strategies that seed probability mass into underrepresented solution regions.",
            "score": 60,
            "issue_id": 4940,
            "pub_date": "2025-07-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ»Ñ",
                "en": "July 20",
                "zh": "7æœˆ20æ—¥"
            },
            "hash": "bb8fd850ce625ea5",
            "authors": [
                "Fang Wu",
                "Weihao Xuan",
                "Ximing Lu",
                "Zaid Harchaoui",
                "Yejin Choi"
            ],
            "affiliations": [
                "RIKEN AIP",
                "Stanford University",
                "University of Tokyo",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.14843.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "RLVR: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†ĞµĞ½Ğ¾Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ RLVR Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ RLVR Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ¾Ğ½ÑĞµÑ€Ğ²Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RLVR ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ pass@1, Ğ½Ğ¾ ÑÑƒĞ¶Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑƒĞ¿ÑƒÑĞºĞ°Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ½ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Balancing Precision and Exploration in RLVR",
                    "desc": "This paper investigates Reinforcement Learning with Verifiable Rewards (RLVR) and its impact on AI's problem-solving abilities. It finds that while RLVR improves precision in generating high-reward outputs, it limits exploration, which can hinder the discovery of novel solutions. The study introduces a theoretical framework showing that RLVR acts as a conservative mechanism, unable to sample solutions with zero initial probability. Empirical results indicate that although RLVR enhances performance metrics like pass@1, it often reduces the diversity of solutions, suggesting a need for new strategies to encourage exploration."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±çš„æ¢ç´¢é™åˆ¶",
                    "desc": "å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æé«˜ç²¾åº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å´é™åˆ¶äº†æ¢ç´¢èƒ½åŠ›ï¼Œå¯èƒ½å¯¼è‡´æ— æ³•å‘ç°æ–°é¢–çš„è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLVRçš„æ•ˆæœå—åˆ°åŸºç¡€æ¨¡å‹çš„æ”¯æŒé™åˆ¶ï¼Œæ— æ³•é‡‡æ ·åˆå§‹æ¦‚ç‡ä¸ºé›¶çš„è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶RLVRåœ¨æé«˜ç²¾åº¦æ–¹é¢è¡¨ç°ç¨³å®šï¼Œä½†å…¶å¯¹æ¢ç´¢çš„å‹ç¼©å¯èƒ½ä¼šå¿½è§†ä¸€äº›æ­£ç¡®ä½†ä»£è¡¨æ€§ä¸è¶³çš„è§£å†³æ–¹æ¡ˆã€‚æœªæ¥çš„ç®—æ³•åˆ›æ–°å¯èƒ½éœ€è¦å¼•å…¥æ˜¾å¼æ¢ç´¢æœºåˆ¶æˆ–æ··åˆç­–ç•¥ï¼Œä»¥ä¾¿åœ¨æœªè¢«å……åˆ†ä»£è¡¨çš„è§£å†³æ–¹æ¡ˆåŒºåŸŸä¸­æ³¨å…¥æ¦‚ç‡è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.14119",
            "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
            "url": "https://huggingface.co/papers/2507.14119",
            "abstract": "An automated pipeline mines high-fidelity image editing triplets using generative models and a task-tuned validator, enabling large-scale training without human labeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets: original image, instruction, edited image. Yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated edit-quality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines high-fidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by approximately 2.2x, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit: an open dataset of 358k high-quality triplets. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, an open-source fine-tuned Bagel model, which achieves state-of-the-art metrics in our experiments.",
            "score": 36,
            "issue_id": 4944,
            "pub_date": "2025-07-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ»Ñ",
                "en": "July 18",
                "zh": "7æœˆ18æ—¥"
            },
            "hash": "4c0e1974ad169d32",
            "authors": [
                "Maksim Kuprashevich",
                "Grigorii Alekseenko",
                "Irina Tolstykh",
                "Georgii Fedorov",
                "Bulat Suleimanov",
                "Vladimir Dokholyan",
                "Aleksandr Gordeev"
            ],
            "affiliations": [
                "Layer Team, SALUTEDEV"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.14119.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#diffusion",
                    "#open_source",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ˜Ğ˜-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ñ€ Gemini Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞ¸ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… NHR-Edit Ğ¸Ğ· 358 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Bagel-NHR-Edit, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰ÑƒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Automating Image Editing: High-Fidelity Triplet Generation Without Human Input",
                    "desc": "This paper presents an automated pipeline that generates high-quality image editing triplets using generative models, which consist of an original image, an instruction, and the edited image. The system employs a task-tuned validator to ensure that the edits adhere to the specified instructions while maintaining visual appeal and coherence. By automating the mining process, the pipeline significantly increases the volume of training data available for image editing models without requiring human labeling. The authors also introduce the NHR-Edit dataset, containing 358,000 high-quality triplets, and a fine-tuned model that achieves state-of-the-art performance in image editing tasks."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–æŒ–æ˜é«˜ä¿çœŸå›¾åƒç¼–è¾‘ä¸‰å…ƒç»„çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨åŒ–çš„ç®¡é“ï¼Œç”¨äºæŒ–æ˜é«˜ä¿çœŸå›¾åƒç¼–è¾‘ä¸‰å…ƒç»„ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹å’Œä»»åŠ¡è°ƒä¼˜çš„éªŒè¯å™¨ï¼Œå®ç°å¤§è§„æ¨¡è®­ç»ƒè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨ä¸åŒé¢†åŸŸã€åˆ†è¾¨ç‡å’Œé£æ ¼ä¸­è‡ªåŠ¨ç”ŸæˆåŸå§‹å›¾åƒã€æŒ‡ä»¤å’Œç¼–è¾‘å›¾åƒçš„ä¸‰å…ƒç»„ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¯¹ç¼–è¾‘è´¨é‡è¯„ä¼°çš„ä¸è¶³ã€‚é€šè¿‡ä½¿ç”¨ä»»åŠ¡è°ƒä¼˜çš„éªŒè¯å™¨ï¼Œç³»ç»Ÿç›´æ¥è¯„åˆ†æŒ‡ä»¤éµå¾ªæ€§å’Œç¾å­¦ï¼Œçœå»äº†åˆ†å‰²æˆ–åŸºç¡€æ¨¡å‹çš„éœ€æ±‚ã€‚è®ºæ–‡è¿˜å‘å¸ƒäº†NHR-Editæ•°æ®é›†ï¼ŒåŒ…å«358kä¸ªé«˜è´¨é‡ä¸‰å…ƒç»„ï¼Œæ¨åŠ¨äº†è¿™ä¸€èµ„æºå¯†é›†å‹é¢†åŸŸçš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15061",
            "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking\n  Formalization",
            "url": "https://huggingface.co/papers/2507.15061",
            "abstract": "A formalization-driven framework called WebShaper synthesizes information-seeking datasets using set theory and Knowledge Projections, enhancing the performance of LLM-powered agents on open-ended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks.",
            "score": 31,
            "issue_id": 4936,
            "pub_date": "2025-07-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ»Ñ",
                "en": "July 20",
                "zh": "7æœˆ20æ—¥"
            },
            "hash": "16ab84cfe7ace89e",
            "authors": [
                "Zhengwei Tao",
                "Jialong Wu",
                "Wenbiao Yin",
                "Junkai Zhang",
                "Baixuan Li",
                "Haiyang Shen",
                "Kuan Li",
                "Liwen Zhang",
                "Xinyu Wang",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15061.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#synthetic",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "WebShaper - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ¸ ĞŸÑ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹ Ğ—Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. WebShaper ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ WebShaper Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GAIA Ğ¸ WebWalkerQA."
                },
                "en": {
                    "title": "Enhancing LLM Agents with Structured Data Synthesis",
                    "desc": "WebShaper is a framework designed to improve information-seeking datasets for Large Language Model (LLM)-powered agents. It uses set theory and a method called Knowledge Projections to create a structured approach for synthesizing data. This helps ensure that the reasoning behind questions and answers is consistent and logical. Experiments show that WebShaper significantly enhances the performance of these agents on various benchmarks."
                },
                "zh": {
                    "title": "WebShaperï¼šæå‡ä¿¡æ¯æ£€ç´¢æ™ºèƒ½ä½“æ€§èƒ½çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "WebShaperæ˜¯ä¸€ä¸ªåŸºäºå½¢å¼åŒ–é©±åŠ¨çš„æ¡†æ¶ï¼Œåˆ©ç”¨é›†åˆè®ºå’ŒçŸ¥è¯†æŠ•å½±æŠ€æœ¯åˆæˆä¿¡æ¯æ£€ç´¢æ•°æ®é›†ï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“åœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡ç³»ç»ŸåŒ–çš„å½¢å¼åŒ–è¿‡ç¨‹ï¼Œç¡®ä¿ä¿¡æ¯ç»“æ„ä¸æ¨ç†ç»“æ„çš„ä¸€è‡´æ€§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­å¸¸è§çš„æ•°æ®ä¸ä¸€è‡´é—®é¢˜ã€‚WebShaperçš„æ ¸å¿ƒæ˜¯çŸ¥è¯†æŠ•å½±ï¼ˆKPï¼‰æ¦‚å¿µï¼Œé€šè¿‡KPæ“ä½œç»„åˆå®ç°å¯¹æ¨ç†ç»“æ„çš„ç²¾ç¡®æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWebShaperåœ¨GAIAå’ŒWebWalkerQAåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¾¾åˆ°äº†å¼€æºä¿¡æ¯æ£€ç´¢æ™ºèƒ½ä½“çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15493",
            "title": "GR-3 Technical Report",
            "url": "https://huggingface.co/papers/2507.15493",
            "abstract": "A large-scale vision-language-action model demonstrates exceptional generalization, fine-tuning efficiency, and robust performance in complex robotic tasks, outperforming existing baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through a multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, a versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing a wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, pi_0, on a wide variety of challenging tasks. We hope GR-3 can serve as a step towards building generalist robots capable of assisting humans in daily life.",
            "score": 28,
            "issue_id": 4938,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "5e91567240893b65",
            "authors": [
                "Chilam Cheang",
                "Sijin Chen",
                "Zhongren Cui",
                "Yingdong Hu",
                "Liqun Huang",
                "Tao Kong",
                "Hang Li",
                "Yifeng Li",
                "Yuxiao Liu",
                "Xiao Ma",
                "Hao Niu",
                "Wenxuan Ou",
                "Wanli Peng",
                "Zeyu Ren",
                "Haixin Shi",
                "Jiawen Tian",
                "Hongtao Wu",
                "Xin Xiao",
                "Yuyang Xiao",
                "Jiafeng Xu",
                "Yichu Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.15493.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#robotics",
                    "#agents",
                    "#training",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "GR-3: Ğ¨Ğ°Ğ³ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ°Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GR-3 - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA) Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸. GR-3 Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ»Ğ¾Ğ²ĞºĞ¾ÑÑ‚Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "GR-3: A Leap Towards Generalist Robots for Everyday Tasks",
                    "desc": "The paper presents GR-3, a large-scale vision-language-action model that excels in generalizing across various robotic tasks. It can adapt quickly to new environments and instructions with minimal human input, making it efficient for fine-tuning. GR-3 is particularly effective in performing complex tasks that require dexterity and coordination, such as bi-manual manipulation. The model's training combines web-scale data and imitation learning, leading to superior performance compared to existing methods."
                },
                "zh": {
                    "title": "GR-3ï¼šé€šç”¨æœºå™¨äººæ”¿ç­–çš„æœªæ¥",
                    "desc": "GR-3æ˜¯ä¸€ä¸ªå¤§å‹çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„æœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚å®ƒå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€‚åº”æ–°ç‰©ä½“ã€æ–°ç¯å¢ƒå’ŒæŠ½è±¡æ¦‚å¿µçš„æŒ‡ä»¤ã€‚è¯¥æ¨¡å‹å¯ä»¥é€šè¿‡å°‘é‡çš„äººç±»è½¨è¿¹æ•°æ®è¿›è¡Œé«˜æ•ˆçš„å¾®è°ƒï¼Œå¿«é€Ÿé€‚åº”æ–°ç¯å¢ƒã€‚é€šè¿‡ä¸ç½‘ç»œè§„æ¨¡çš„è§†è§‰-è¯­è¨€æ•°æ®å…±åŒè®­ç»ƒï¼ŒGR-3åœ¨é•¿æ—¶é—´å’Œçµå·§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­è¾…åŠ©äººç±»çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11061",
            "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with\n  Regularized Score Distillation Sampling",
            "url": "https://huggingface.co/papers/2507.11061",
            "abstract": "A novel framework, RoMaP, improves precise local 3D editing through robust 3D mask generation and enhanced SDS loss regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing. Code is available at https://janeyeon.github.io/romap.",
            "score": 27,
            "issue_id": 4939,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ»Ñ",
                "en": "July 15",
                "zh": "7æœˆ15æ—¥"
            },
            "hash": "4c8104d951622fec",
            "authors": [
                "Hayeon Kim",
                "Ji Ha Jang",
                "Se Young Chun"
            ],
            "affiliations": [
                "Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea",
                "INMC & IPAI Seoul National University, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11061.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "âœï¸",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "RoMaP - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ°ÑĞ¾Ğº Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ SDS. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ 3D-GALP Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². RoMaP Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ SDS Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ L1-ÑĞºĞ¾Ñ€Ğ½ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ğ¾Ğ´ SLaMP. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RoMaP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ°Ğº Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "RoMaP: Revolutionizing Local 3D Editing with Precision and Flexibility",
                    "desc": "The paper introduces RoMaP, a new framework designed to enhance local 3D editing by generating robust 3D masks and improving the Score Distillation Sampling (SDS) loss regularization. It addresses challenges in achieving precise edits in 3D content, particularly with Gaussian Splatting, by utilizing a 3D-Geometry Aware Label Prediction (3D-GALP) module for accurate part segmentations. The framework also incorporates a regularized SDS loss that includes an L1 anchor loss to ensure modifications are confined to specific areas while maintaining overall coherence. Experimental results show that RoMaP outperforms existing methods in local 3D editing, providing a more flexible and effective approach for part-level modifications."
                },
                "zh": {
                    "title": "RoMaPï¼šç²¾ç¡®å±€éƒ¨3Dç¼–è¾‘çš„æ–°æ¡†æ¶",
                    "desc": "RoMaPæ˜¯ä¸€ä¸ªæ–°é¢–çš„å±€éƒ¨3Dç¼–è¾‘æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼ºå¤§çš„3Dæ©æ¨¡ç”Ÿæˆå’Œå¢å¼ºçš„SDSæŸå¤±æ­£åˆ™åŒ–æ¥æé«˜ç²¾ç¡®çš„å±€éƒ¨3Dç¼–è¾‘èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†3Då‡ ä½•æ„ŸçŸ¥æ ‡ç­¾é¢„æµ‹æ¨¡å—ï¼Œåˆ©ç”¨çƒè°ç³»æ•°å»ºæ¨¡è§†è§’ä¾èµ–çš„æ ‡ç­¾å˜åŒ–ï¼Œä»è€Œå®ç°å‡†ç¡®ä¸€è‡´çš„éƒ¨åˆ†åˆ†å‰²ã€‚é€šè¿‡ç»“åˆæ ‡å‡†SDSæŸå¤±å’Œé¢å¤–çš„æ­£åˆ™åŒ–é¡¹ï¼ŒRoMaPèƒ½å¤Ÿåœ¨ç›®æ ‡åŒºåŸŸå†…è¿›è¡Œé«˜è´¨é‡çš„éƒ¨åˆ†ç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒä¸Šä¸‹æ–‡çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoMaPåœ¨é‡å»ºå’Œç”Ÿæˆçš„é«˜æ–¯åœºæ™¯åŠç‰©ä½“ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å±€éƒ¨3Dç¼–è¾‘æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15852",
            "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept\n  Construction",
            "url": "https://huggingface.co/papers/2507.15852",
            "abstract": "Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the human-like conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), a concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms a comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware video object segmentation.",
            "score": 24,
            "issue_id": 4940,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "8d4e431fe003417f",
            "authors": [
                "Zhixiong Zhang",
                "Shuangrui Ding",
                "Xiaoyi Dong",
                "Songxin He",
                "Jianfan Lin",
                "Junsong Tang",
                "Yuhang Zang",
                "Yuhang Cao",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15852.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#interpretability",
                    "#benchmark",
                    "#reasoning",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Segment Concept (SeC). SeC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SeCVOS Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. SeC Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Video Object Segmentation with Conceptual Understanding",
                    "desc": "This paper introduces Segment Concept (SeC), a new framework for Video Object Segmentation (VOS) that enhances object tracking and segmentation in videos. Unlike traditional methods that rely heavily on appearance matching, SeC focuses on building high-level, object-centric representations using Large Vision-Language Models (LVLMs). This approach allows the model to better understand and adapt to complex visual changes and occlusions, leading to improved segmentation accuracy. The authors also present a new benchmark, SeCVOS, to evaluate VOS methods in challenging scenarios, where SeC demonstrates significant performance improvements over existing techniques."
                },
                "zh": {
                    "title": "æ¦‚å¿µé©±åŠ¨çš„è§†é¢‘ç›®æ ‡åˆ†å‰²æ–°çªç ´",
                    "desc": "è§†é¢‘ç›®æ ‡åˆ†å‰²ï¼ˆVOSï¼‰æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹åœ¨è§†é¢‘å¸§ä¸­è·Ÿè¸ªå’Œåˆ†å‰²ç›®æ ‡ç‰©ä½“ã€‚å°½ç®¡è¿‘å¹´æ¥å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†ç°æœ‰æŠ€æœ¯åœ¨å¤„ç†å‰§çƒˆçš„è§†è§‰å˜åŒ–ã€é®æŒ¡å’Œå¤æ‚åœºæ™¯å˜åŒ–æ—¶ä»ç„¶è½åäºäººç±»èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Segment Conceptï¼ˆSeCï¼‰ï¼Œå®ƒé€šè¿‡æ„å»ºå’Œåˆ©ç”¨é«˜å±‚æ¬¡çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è¡¨ç¤ºï¼Œè½¬å˜äº†ä¼ ç»Ÿçš„ç‰¹å¾åŒ¹é…æ–¹æ³•ã€‚SeCç»“åˆäº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å½¢æˆå…¨é¢çš„è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œå®ç°å¯¹åç»­å¸§çš„ç¨³å¥åˆ†å‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15597",
            "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human\n  Videos",
            "url": "https://huggingface.co/papers/2507.15597",
            "abstract": "Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0.",
            "score": 22,
            "issue_id": 4942,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "c48aaae53a1f9330",
            "authors": [
                "Hao Luo",
                "Yicheng Feng",
                "Wanpeng Zhang",
                "Sipeng Zheng",
                "Ye Wang",
                "Haoqi Yuan",
                "Jiazheng Liu",
                "Chaoyi Xu",
                "Qin Jin",
                "Zongqing Lu"
            ],
            "affiliations": [
                "BeingBeyond",
                "Peking University",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15597.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#robotics",
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#data"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Being-H0 - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA), Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ñ‚ĞµĞ»Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸. Being-H0 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€ÑƒĞº Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, VR Ğ¸ RGB-Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Empowering Robots with Human-Like Dexterity through Vision-Language-Action!",
                    "desc": "Being-H0 is a cutting-edge Vision-Language-Action model designed to enhance robotic manipulation by learning from human videos. It tackles challenges in dexterity and generalization by utilizing physical instruction tuning and part-level motion tokenization, which allows for precise hand motion generation. The model is trained on a diverse dataset that includes various sources, ensuring it can adapt to real-world scenarios effectively. As a result, Being-H0 demonstrates superior performance in both generating hand motions and executing complex tasks in robotic applications."
                },
                "zh": {
                    "title": "Being-H0ï¼šçµå·§çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
                    "desc": "Being-H0 æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œä¸“æ³¨äºä»äººç±»è§†é¢‘ä¸­å­¦ä¹ ï¼Œä»¥è§£å†³çµå·§æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡ç‰©ç†æŒ‡ä»¤è°ƒä¼˜å’Œéƒ¨ä»¶çº§è¿åŠ¨æ ‡è®°åŒ–ï¼Œèƒ½å¤Ÿç”Ÿæˆç²¾ç¡®çš„æ‰‹éƒ¨åŠ¨ä½œå¹¶åœ¨çœŸå®ä¸–ç•Œä¸­è¿›è¡Œæœºå™¨äººæ“ä½œã€‚ä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”ï¼ŒBeing-H0 æ›´å¥½åœ°å¤„ç†å¤æ‚çš„æ“ä½œä»»åŠ¡ï¼Œå¹¶èƒ½æœ‰æ•ˆé€‚åº”æ–°åœºæ™¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒBeing-H0 åœ¨æ‰‹éƒ¨åŠ¨ä½œç”Ÿæˆå’ŒæŒ‡ä»¤è·Ÿéšæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨å®é™…æœºå™¨äººæ“ä½œä¸­ä¹Ÿå–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15778",
            "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for\n  RLVR",
            "url": "https://huggingface.co/papers/2507.15778",
            "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform training signals to all tokens, without considering the different roles of low-entropy knowledge-related tokens and high-entropy reasoning-related tokens. Some recent methods try to separate these token types by gradient masking or asynchronous updates, but these approaches may break semantic dependencies in the model output and hinder effective learning. In this work, we propose Archer, an entropy-aware RLVR approach with dual-token constraints and synchronous updates. Specifically, our method applies weaker KL regularization and higher clipping thresholds to reasoning tokens to encourage exploration, while using stronger constraints on knowledge tokens to maintain factual knowledge. Experimental results on several mathematical reasoning and code generation benchmarks show that our approach significantly outperforms previous RLVR methods, reaching or exceeding state-of-the-art performance among models of comparable size. The code is available at https://github.com/wizard-III/ArcherCodeR.",
            "score": 16,
            "issue_id": 4936,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "8e0f7bdfedf50691",
            "authors": [
                "Jiakang Wang",
                "Runze Liu",
                "Fuzheng Zhang",
                "Xiu Li",
                "Guorui Zhou"
            ],
            "affiliations": [
                "Kuaishou Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15778.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Archer. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Archer Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ°Ğ±ÑƒÑ KL-Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¸ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Archer Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RLVR Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Archer: Smart Token Training for Better Reasoning in LLMs",
                    "desc": "This paper introduces Archer, a new method for Reinforcement Learning with Verifiable Rewards (RLVR) that enhances the reasoning capabilities of Large Language Models (LLMs). Unlike previous methods that treat all tokens equally, Archer distinguishes between low-entropy knowledge tokens and high-entropy reasoning tokens, applying different training strategies to each. By using weaker KL regularization for reasoning tokens, Archer promotes exploration while enforcing stronger constraints on knowledge tokens to preserve factual accuracy. The results demonstrate that Archer significantly improves performance on mathematical reasoning and code generation tasks, achieving state-of-the-art results for models of similar size."
                },
                "zh": {
                    "title": "æå‡æ¨ç†èƒ½åŠ›çš„åŒé‡ä»¤ç‰Œå¼ºåŒ–å­¦ä¹ ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºArcherï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚Archeré€šè¿‡åŒé‡ä»¤ç‰Œçº¦æŸå’ŒåŒæ­¥æ›´æ–°ï¼Œåˆ†åˆ«å¯¹çŸ¥è¯†ç›¸å…³çš„ä½ç†µä»¤ç‰Œå’Œæ¨ç†ç›¸å…³çš„é«˜ç†µä»¤ç‰Œæ–½åŠ ä¸åŒçš„è®­ç»ƒä¿¡å·ã€‚ä¸ä»¥å¾€çš„ç®—æ³•ä¸åŒï¼ŒArcheråœ¨æ¨ç†ä»¤ç‰Œä¸Šä½¿ç”¨è¾ƒå¼±çš„KLæ­£åˆ™åŒ–ï¼Œä»¥é¼“åŠ±æ¢ç´¢ï¼ŒåŒæ—¶å¯¹çŸ¥è¯†ä»¤ç‰Œæ–½åŠ æ›´å¼ºçš„çº¦æŸï¼Œä»¥ä¿æŒäº‹å®çŸ¥è¯†çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒArcheråœ¨å¤šä¸ªæ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¹‹å‰çš„RLVRæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.14417",
            "title": "Inverse Scaling in Test-Time Compute",
            "url": "https://huggingface.co/papers/2507.14417",
            "abstract": "Evaluating Large Reasoning Models across different reasoning lengths reveals that increased test-time compute can degrade performance and exacerbate specific reasoning failures.  \t\t\t\t\tAI-generated summary \t\t\t\t We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy. Our evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks. We identify five distinct failure modes when models reason for longer: 1) Claude models become increasingly distracted by irrelevant information; 2) OpenAI o-series models resist distractors but overfit to problem framings; 3) models shift from reasonable priors to spurious correlations; 4) all models show difficulties in maintaining focus on complex deductive tasks; and 5) extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation. These findings suggest that while test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns. Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs.",
            "score": 15,
            "issue_id": 4945,
            "pub_date": "2025-07-19",
            "pub_date_card": {
                "ru": "19 Ğ¸ÑĞ»Ñ",
                "en": "July 19",
                "zh": "7æœˆ19æ—¥"
            },
            "hash": "e6c3904a07b73089",
            "authors": [
                "Aryo Pradipta Gema",
                "Alexander HÃ¤gele",
                "Runjin Chen",
                "Andy Arditi",
                "Jacob Goldman-Wetzler",
                "Kit Fraser-Taliente",
                "Henry Sleight",
                "Linda Petrini",
                "Julian Michael",
                "Beatrice Alex",
                "Pasquale Minervini",
                "Yanda Chen",
                "Joe Benton",
                "Ethan Perez"
            ],
            "affiliations": [
                "Anthropic",
                "Anthropic Fellows Program",
                "Anthropic Fellows Program, EPFL",
                "Anthropic Fellows Program, University of Edinburgh",
                "Anthropic Fellows Program, University of Texas at Austin",
                "Constellation",
                "Independent",
                "Scale AI",
                "University of Edinburgh",
                "University of Edinburgh, Miniml.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.14417.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#hallucinations",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ - Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ»ÑƒÑ‡ÑˆĞµ: Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑ…ÑƒĞ´ÑˆĞ¸Ñ‚ÑŒ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾ Ğ¿ÑÑ‚ÑŒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ñ‚Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑÑ… Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ° Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´ĞµĞ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼."
                },
                "en": {
                    "title": "Longer Reasoning, Lower Accuracy: The Inverse Scaling Dilemma",
                    "desc": "This paper investigates how increasing the reasoning length of Large Reasoning Models (LRMs) can lead to worse performance, highlighting an inverse relationship between the amount of compute used during testing and the models' accuracy. The authors create evaluation tasks that reveal five specific failure modes, such as models becoming distracted by irrelevant information or overfitting to specific problem framings. They also note that longer reasoning can exacerbate issues like reliance on spurious correlations and difficulties in complex deductive reasoning. Overall, the study emphasizes the need for careful evaluation of LRMs across varying reasoning lengths to uncover and mitigate these performance issues."
                },
                "zh": {
                    "title": "æ¨ç†é•¿åº¦ä¸æ¨¡å‹æ€§èƒ½çš„åå‘å…³ç³»",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹åœ¨ä¸åŒæ¨ç†é•¿åº¦ä¸‹çš„è¡¨ç°ï¼Œå‘ç°å¢åŠ æµ‹è¯•æ—¶è®¡ç®—é‡å¯èƒ½ä¼šé™ä½æ€§èƒ½å¹¶åŠ å‰§ç‰¹å®šçš„æ¨ç†å¤±è´¥ã€‚æˆ‘ä»¬è®¾è®¡äº†å››ç±»è¯„ä¼°ä»»åŠ¡ï¼Œç»“æœæ˜¾ç¤ºæ¨ç†é•¿åº¦çš„å»¶é•¿ä¸å‡†ç¡®ç‡ä¹‹é—´å­˜åœ¨åå‘ç¼©æ”¾å…³ç³»ã€‚ç ”ç©¶ä¸­è¯†åˆ«äº†äº”ç§ä¸åŒçš„å¤±è´¥æ¨¡å¼ï¼ŒåŒ…æ‹¬æ¨¡å‹å¯¹æ— å…³ä¿¡æ¯çš„å¹²æ‰°å’Œå¯¹é—®é¢˜æ¡†æ¶çš„è¿‡æ‹Ÿåˆç­‰ã€‚æˆ‘ä»¬çš„å‘ç°å¼ºè°ƒäº†åœ¨å¤šæ ·åŒ–æ¨ç†é•¿åº¦ä¸‹è¯„ä¼°æ¨¡å‹çš„é‡è¦æ€§ï¼Œä»¥è¯†åˆ«å’Œè§£å†³å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„è¿™äº›å¤±è´¥æ¨¡å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15028",
            "title": "Towards Video Thinking Test: A Holistic Benchmark for Advanced Video\n  Reasoning and Understanding",
            "url": "https://huggingface.co/papers/2507.15028",
            "abstract": "Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video large language models (video LLMs), existing benchmarks inadequately reflect the gap between these models and human intelligence in maintaining correctness and robustness in video interpretation. We introduce the Video Thinking Test (Video-TT), to assess if video LLMs can interpret real-world videos as effectively as humans. Video-TT reflects genuine gaps in understanding complex visual narratives, and evaluates robustness against natural adversarial questions. Video-TT comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows a significant gap between video LLMs and human performance.",
            "score": 14,
            "issue_id": 4938,
            "pub_date": "2025-07-20",
            "pub_date_card": {
                "ru": "20 Ğ¸ÑĞ»Ñ",
                "en": "July 20",
                "zh": "7æœˆ20æ—¥"
            },
            "hash": "7f71d09a9b276de8",
            "authors": [
                "Yuanhan Zhang",
                "Yunice Chew",
                "Yuhao Dong",
                "Aria Leo",
                "Bo Hu",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Independent Researcher",
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15028.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#security",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM: Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Video Thinking Test (Video-TT) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Video-TT ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 1000 ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… YouTube-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ². Ğ¢ĞµÑÑ‚ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼ Ğ² ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Video-TT Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating Video LLMs with the Video Thinking Test",
                    "desc": "This paper discusses the importance of correctness and robustness in video understanding, which are essential for mimicking human intelligence. It highlights that current benchmarks do not adequately measure how well video large language models (LLMs) interpret videos compared to humans. To address this, the authors introduce the Video Thinking Test (Video-TT), designed to evaluate the performance of video LLMs on real-world videos. The test includes 1,000 YouTube Shorts videos with questions that challenge the models' understanding of complex visual narratives, revealing a significant performance gap between the models and human interpreters."
                },
                "zh": {
                    "title": "è§†é¢‘ç†è§£çš„æŒ‘æˆ˜ï¼šäººç±»ä¸æ¨¡å‹çš„å·®è·",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†è§†é¢‘ç†è§£ä¸­çš„æ­£ç¡®æ€§å’Œé²æ£’æ€§é—®é¢˜ã€‚å°½ç®¡è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆè§†é¢‘LLMsï¼‰å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†ç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½å……åˆ†åæ˜ è¿™äº›æ¨¡å‹ä¸äººç±»æ™ºèƒ½åœ¨è§†é¢‘è§£é‡Šä¸­çš„å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†è§†é¢‘æ€ç»´æµ‹è¯•ï¼ˆVideo-TTï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°è§†é¢‘LLMsæ˜¯å¦èƒ½åƒäººç±»ä¸€æ ·æœ‰æ•ˆåœ°ç†è§£ç°å®ä¸–ç•Œçš„è§†é¢‘ã€‚æµ‹è¯•åŒ…å«1000ä¸ªYouTube Shortsè§†é¢‘ï¼Œæ¯ä¸ªè§†é¢‘é…æœ‰ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜å’Œå››ä¸ªé’ˆå¯¹è§†è§‰å’Œå™äº‹å¤æ‚æ€§çš„å¯¹æŠ—æ€§é—®é¢˜ï¼Œè¯„ä¼°ç»“æœæ˜¾ç¤ºè§†é¢‘LLMsä¸äººç±»è¡¨ç°ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15629",
            "title": "Gaussian Splatting with Discretized SDF for Relightable Assets",
            "url": "https://huggingface.co/papers/2507.15629",
            "abstract": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce a discretized SDF to represent the continuous SDF in a discrete manner by encoding it within each Gaussian using a sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching.The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradient-based constraints (\\eg Eikonal loss). For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely a projection-based consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https://github.com/NK-CS-ZZL/DiscretizedSDF.",
            "score": 13,
            "issue_id": 4940,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "f2fc3e4b855b88d5",
            "authors": [
                "Zuo-Liang Zhu",
                "Jian Yang",
                "Beibei Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15629.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ”Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ SDF Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ğ¼ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ñƒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ ÑĞ¾ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼ (SDF) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ SDF Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ²ÑĞ·Ğ°Ñ‚ÑŒ SDF Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ”Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ SDF. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¿ĞµÑ€ĞµÑ€Ğ¸ÑĞ¾Ğ²ĞºĞ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Efficient Inverse Rendering with Discretized SDF and Gaussian Splatting",
                    "desc": "This paper presents a novel approach to inverse rendering using a discretized signed distance field (SDF) integrated with 3D Gaussian splatting (3DGS). By encoding the continuous SDF within each Gaussian, the method links SDF with Gaussian opacity, allowing for efficient rendering without the heavy computational costs of ray marching. The authors introduce a projection-based consistency loss to ensure that the discrete samples align with the underlying SDF, improving the quality of relighting. Overall, this approach enhances the performance of Gaussian-based inverse rendering while maintaining low memory usage and simplifying the optimization process."
                },
                "zh": {
                    "title": "ç¦»æ•£åŒ–SDFæå‡é€†å‘æ¸²æŸ“è´¨é‡",
                    "desc": "3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰åœ¨æ–°è§†å›¾åˆæˆï¼ˆNVSï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é€†å‘æ¸²æŸ“ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¦»æ•£åŒ–çš„æœ‰ç¬¦å·è·ç¦»åœºï¼ˆSDFï¼‰ï¼Œé€šè¿‡åœ¨æ¯ä¸ªé«˜æ–¯ä¸­ç¼–ç é‡‡æ ·å€¼æ¥è¡¨ç¤ºè¿ç»­çš„SDFï¼Œä»è€Œç®€åŒ–äº†å‡ ä½•çº¦æŸçš„åº”ç”¨ã€‚è¯¥æ–¹æ³•é€šè¿‡SDFä¸é«˜æ–¯ä¸é€æ˜åº¦çš„è½¬æ¢ï¼Œé¿å…äº†å…‰çº¿è¡Œè¿›çš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶æé«˜äº†é‡å…‰ç…§è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„åŸºäºé«˜æ–¯çš„é€†å‘æ¸²æŸ“æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15375",
            "title": "STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for\n  Spoken Language Models",
            "url": "https://huggingface.co/papers/2507.15375",
            "abstract": "Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to communicate ideas clearly and concisely. Thus, integrating an unspoken thought process into SLMs is highly desirable. While naively generating a complete chain-of-thought (CoT) reasoning before starting to talk can enable thinking for SLMs, this induces additional latency for the speech response, as the CoT reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a novel generation method that alternates between the generation of unspoken reasoning chunks and spoken response chunks. Since the audio duration of a chunk of spoken response is much longer than the time to generate the tokens in a chunk of spoken response, we use the remaining free time to generate the unspoken reasoning tokens. When a chunk of audio is played to the user, the model continues to generate the next unspoken reasoning chunk, achieving simultaneous thinking and talking. Remarkably, Stitch matches the latency of baselines that cannot generate unspoken CoT by design while outperforming those baselines by 15% on math reasoning datasets; Stitch also performs equally well on non-reasoning datasets as those baseline models. Some animations and demonstrations are on the project page: https://d223302.github.io/STITCH.",
            "score": 12,
            "issue_id": 4937,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "2a7d1e1e1882f002",
            "authors": [
                "Cheng-Han Chiang",
                "Xiaofei Wang",
                "Linjie Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Shujie Liu",
                "Zhendong Wang",
                "Zhengyuan Yang",
                "Hung-yi Lee",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15375.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#audio",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Stitch: Ğ”ÑƒĞ¼Ğ°Ğ¹ Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Stitch. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ñ‡ĞµÑ€ĞµĞ´ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ²Ñ‹ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ½Ğ¾ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Stitch Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ° Ğ½ĞµĞ²Ñ‹ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Stitch Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 15% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ°ĞºÑƒÑ Ğ¶Ğµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Stitch: Simultaneous Thinking and Talking for Enhanced Spoken Language Models",
                    "desc": "This paper introduces Stitch, a new method for Spoken Language Models (SLMs) that allows them to think internally while responding to speech. Unlike traditional SLMs that generate responses without prior reasoning, Stitch alternates between generating unspoken reasoning chunks and spoken responses. This approach minimizes latency by utilizing the time taken to play audio responses to continue generating reasoning. As a result, Stitch not only matches the response time of existing models but also improves performance on math reasoning tasks by 15%."
                },
                "zh": {
                    "title": "åŒæ­¥æ€è€ƒä¸è¡¨è¾¾çš„å£è¯­æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å£è¯­è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–¹æ³•ï¼Œåä¸ºStitchã€‚è¯¥æ–¹æ³•é€šè¿‡äº¤æ›¿ç”Ÿæˆæ— å£°æ¨ç†ç‰‡æ®µå’Œå£è¯­å“åº”ç‰‡æ®µï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹åœ¨å›åº”å‰ç¼ºä¹å†…åœ¨æ€è€ƒè¿‡ç¨‹çš„é—®é¢˜ã€‚Stitchåˆ©ç”¨å£è¯­å“åº”çš„éŸ³é¢‘æŒç»­æ—¶é—´ï¼Œå……åˆ†åˆ©ç”¨å‰©ä½™æ—¶é—´ç”Ÿæˆæ¨ç†å†…å®¹ï¼Œä»è€Œå®ç°æ€è€ƒä¸è¡¨è¾¾çš„åŒæ­¥è¿›è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStitchåœ¨æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šæ¯”åŸºçº¿æ¨¡å‹æé«˜äº†15%çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨éæ¨ç†æ•°æ®é›†ä¸Šè¡¨ç°ä¹Ÿä¸åŸºçº¿æ¨¡å‹ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12806",
            "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models",
            "url": "https://huggingface.co/papers/2507.12806",
            "abstract": "MCPEval is an open-source framework that automates task generation and evaluation for Large Language Models across diverse domains, improving upon existing static benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce \\oursystemname, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.",
            "score": 12,
            "issue_id": 4952,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 Ğ¸ÑĞ»Ñ",
                "en": "July 17",
                "zh": "7æœˆ17æ—¥"
            },
            "hash": "6ec1b86d6a2164d2",
            "authors": [
                "Zhiwei Liu",
                "Jielin Qiu",
                "Shiyu Wang",
                "Jianguo Zhang",
                "Zuxin Liu",
                "Roshan Ram",
                "Haolin Chen",
                "Weiran Yao",
                "Huan Wang",
                "Shelby Heinecke",
                "Silvio Savarese",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12806.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#open_source",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "MCPEval - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ°Ñ‡ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ½ ÑƒĞ»ÑƒÑ‡ÑˆÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM. MCPEval Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ÑĞ°Ğ½ÑÑ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‚ÑƒĞ¿Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM."
                },
                "en": {
                    "title": "Automating LLM Evaluation with MCPEval",
                    "desc": "MCPEval is an innovative framework designed to automate the generation and evaluation of tasks for Large Language Models (LLMs). It addresses the limitations of traditional static benchmarks by providing a scalable and efficient solution for assessing LLM performance across various domains. By utilizing the Model Context Protocol (MCP), MCPEval standardizes evaluation metrics and integrates seamlessly with existing agent tools, reducing the need for manual data collection. Empirical tests demonstrate its ability to uncover detailed, domain-specific insights into LLM capabilities, promoting reproducibility in evaluations."
                },
                "zh": {
                    "title": "MCPEvalï¼šè‡ªåŠ¨åŒ–è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ©å™¨",
                    "desc": "MCPEvalæ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒé¢†åŸŸçš„ä»»åŠ¡ç”Ÿæˆå’Œè¯„ä¼°ã€‚å®ƒæ”¹è¿›äº†ç°æœ‰çš„é™æ€åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿè¯„ä¼°ä¸­æ•°æ®æ”¶é›†ç¹ççš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŸºäºæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ï¼Œèƒ½å¤Ÿå®ç°ç«¯åˆ°ç«¯çš„ä»»åŠ¡ç”Ÿæˆå’Œæ·±å…¥è¯„ä¼°ï¼Œæ ‡å‡†åŒ–è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶ä¸åŸç”Ÿä»£ç†å·¥å…·æ— ç¼é›†æˆã€‚é€šè¿‡åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œé¢†åŸŸçš„å®è¯ç»“æœï¼ŒMCPEvalæœ‰æ•ˆæ­ç¤ºäº†é¢†åŸŸç‰¹å®šçš„æ€§èƒ½å·®å¼‚ï¼Œä¿ƒè¿›äº†å¯é‡å¤å’Œæ ‡å‡†åŒ–çš„LLMä»£ç†è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15815",
            "title": "LLM Economist: Large Population Models and Mechanism Design in\n  Multi-Agent Generative Simulacra",
            "url": "https://huggingface.co/papers/2507.15815",
            "abstract": "We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Census-calibrated income and demographic statistics -- choose labor supply to maximize text-based utility functions learned in-context. At the upper level, a planner agent employs in-context reinforcement learning to propose piecewise-linear marginal tax schedules anchored to the current U.S. federal brackets. This construction endows economic simulacra with three capabilities requisite for credible fiscal experimentation: (i) optimization of heterogeneous utilities, (ii) principled generation of large, demographically realistic agent populations, and (iii) mechanism design -- the ultimate nudging problem -- expressed entirely in natural language. Experiments with populations of up to one hundred interacting agents show that the planner converges near Stackelberg equilibria that improve aggregate social welfare relative to Saez solutions, while a periodic, persona-level voting procedure furthers these gains under decentralized governance. These results demonstrate that large language model-based agents can jointly model, simulate, and govern complex economic systems, providing a tractable test bed for policy evaluation at the societal scale to help build better civilizations.",
            "score": 6,
            "issue_id": 4937,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "ad03ed3ae6e4256b",
            "authors": [
                "Seth Karten",
                "Wenzhe Li",
                "Zihan Ding",
                "Samuel Kleiner",
                "Yu Bai",
                "Chi Jin"
            ],
            "affiliations": [
                "Princeton University",
                "Salesforce Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15815.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#agi",
                    "#multimodal",
                    "#rl",
                    "#science"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ ĞºĞ°Ğº ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ÑÑ‚: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'LLM Economist', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ½Ğ¸Ğ¶Ğ½ĞµĞ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹-Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ½Ğ¸ĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ÑƒĞ´Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞĞ° Ğ²ĞµÑ€Ñ…Ğ½ĞµĞ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ°Ğ³ĞµĞ½Ñ‚-Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºÑƒÑĞ¾Ñ‡Ğ½Ğ¾-Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ²Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğº Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ¸ÑĞ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¼ ÑĞ¾Ğ²Ğ¾ĞºÑƒĞ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¡Ğ°ĞµĞ·Ğ°."
                },
                "en": {
                    "title": "Harnessing AI for Smarter Economic Policy Design",
                    "desc": "The LLM Economist is a new framework that combines agent-based modeling with large language models to evaluate economic policies in complex decision-making environments. It features two levels of agents: lower-level worker agents that optimize their labor supply based on learned utility functions, and an upper-level planner agent that uses reinforcement learning to create tax schedules. This approach allows for realistic simulations of diverse populations and effective mechanism design, all expressed in natural language. The framework shows promising results in improving social welfare through strategic interactions among agents, making it a valuable tool for testing economic policies."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–ç»æµæ”¿ç­–",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLLM Economistçš„æ–°æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºä»£ç†çš„å»ºæ¨¡æ¥è®¾è®¡å’Œè¯„ä¼°å…·æœ‰å±‚çº§å†³ç­–çš„ç»æµæ”¿ç­–ã€‚åœ¨ä½å±‚æ¬¡ï¼Œæœ‰é™ç†æ€§çš„å·¥äººä»£ç†æ ¹æ®ç¾å›½äººå£æ™®æŸ¥çš„æ”¶å…¥å’Œäººå£ç»Ÿè®¡æ•°æ®é€‰æ‹©åŠ³åŠ¨ä¾›ç»™ï¼Œä»¥æœ€å¤§åŒ–åŸºäºæ–‡æœ¬çš„æ•ˆç”¨å‡½æ•°ã€‚åœ¨é«˜å±‚æ¬¡ï¼Œè§„åˆ’è€…ä»£ç†ä½¿ç”¨ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ æå‡ºä¸å½“å‰ç¾å›½è”é‚¦ç¨ç‡ç›¸ç»“åˆçš„åˆ†æ®µçº¿æ€§è¾¹é™…ç¨ç‡ã€‚è¿™ç§æ„å»ºä½¿ç»æµæ¨¡æ‹Ÿå…·å¤‡äº†ä¼˜åŒ–å¼‚è´¨æ•ˆç”¨ã€ç”Ÿæˆå¤§è§„æ¨¡äººå£å’Œæœºåˆ¶è®¾è®¡ç­‰ä¸‰ç§èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨è‡ªç„¶è¯­è¨€ä¸­è¿›è¡Œæœ‰æ•ˆçš„è´¢æ”¿å®éªŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.13428",
            "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in\n  Text-to-Video Models",
            "url": "https://huggingface.co/papers/2507.13428",
            "abstract": "Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles.",
            "score": 6,
            "issue_id": 4943,
            "pub_date": "2025-07-17",
            "pub_date_card": {
                "ru": "17 Ğ¸ÑĞ»Ñ",
                "en": "July 17",
                "zh": "7æœˆ17æ—¥"
            },
            "hash": "0bcb2373e179ecb8",
            "authors": [
                "Jing Gu",
                "Xian Liu",
                "Yu Zeng",
                "Ashwin Nagarajan",
                "Fangrui Zhu",
                "Daniel Hong",
                "Yue Fan",
                "Qianqi Yan",
                "Kaiwen Zhou",
                "Ming-Yu Liu",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "NVIDIA Research",
                "Northeastern University",
                "University of California, Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.13428.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#interpretability",
                    "#optimization",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸ĞºĞ° Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PhyWorldBench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ 'ĞĞ½Ñ‚Ğ¸-Ñ„Ğ¸Ğ·Ğ¸ĞºĞ°'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ 12 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-video Ğ½Ğ° 1050 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ…. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ±Ñ‹Ğ»Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸, Ğ¸ Ğ´Ğ°Ğ½Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Evaluating Video Generation with Physics: PhyWorldBench",
                    "desc": "This paper introduces PhyWorldBench, a benchmark for evaluating video generation models based on their ability to simulate physical laws accurately. It assesses models across various physical phenomena, from basic principles like motion and energy conservation to complex interactions involving living beings. A unique 'Anti-Physics' category is included to test models' responses to prompts that contradict real-world physics, ensuring logical consistency in their outputs. The study evaluates 12 leading text-to-video models, revealing significant challenges in maintaining physical realism and providing insights for improving prompt design to enhance adherence to physical principles."
                },
                "zh": {
                    "title": "è¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç‰©ç†çœŸå®æ€§",
                    "desc": "è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨åˆ›å»ºé«˜è´¨é‡ã€é€¼çœŸçš„å†…å®¹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬å‡†ç¡®æ¨¡æ‹Ÿç‰©ç†ç°è±¡çš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®ä¸”æœªè§£å†³çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†PhyWorldBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨éµå¾ªç‰©ç†æ³•åˆ™æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬è¯„ä¼°äº†12ä¸ªæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶è¯†åˆ«å‡ºè¿™äº›æ¨¡å‹åœ¨éµå¾ªç°å®ç‰©ç†æ–¹é¢é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11539",
            "title": "Streaming 4D Visual Geometry Transformer",
            "url": "https://huggingface.co/papers/2507.11539",
            "abstract": "A streaming 4D visual geometry transformer uses causal attention and knowledge distillation to achieve real-time 4D reconstruction with high spatial consistency and competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: https://github.com/wzzheng/StreamVGGT.",
            "score": 6,
            "issue_id": 4937,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ»Ñ",
                "en": "July 15",
                "zh": "7æœˆ15æ—¥"
            },
            "hash": "03e472d31e5edcaf",
            "authors": [
                "Dong Zhuo",
                "Wenzhao Zheng",
                "Jiahe Guo",
                "Yuqi Wu",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11539.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#long_context",
                    "#optimization",
                    "#benchmark",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 4D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¹ 4D-Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Real-Time 4D Reconstruction with Streaming Transformers",
                    "desc": "This paper presents a streaming 4D visual geometry transformer that utilizes causal attention and knowledge distillation for real-time 4D reconstruction from video data. The model processes input sequences in an online manner, leveraging a causal transformer architecture to maintain high spatial consistency while integrating historical information. By employing temporal causal attention and caching past data, the system achieves efficient long-term reconstruction. The approach is validated through extensive experiments, showing improved inference speed and competitive performance, making it suitable for interactive 4D vision applications."
                },
                "zh": {
                    "title": "å®æ—¶4Dé‡å»ºçš„åˆ›æ–°å˜æ¢å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æµå¼4Dè§†è§‰å‡ ä½•å˜æ¢å™¨ï¼Œåˆ©ç”¨å› æœæ³¨æ„åŠ›å’ŒçŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œå®ç°å®æ—¶çš„4Dé‡å»ºã€‚è¯¥æ¨¡å‹é‡‡ç”¨å› æœå˜æ¢å™¨æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨çº¿å¤„ç†è¾“å…¥åºåˆ—ï¼Œå¹¶é€šè¿‡ç¼“å­˜å†å²ä¿¡æ¯æ¥æé«˜é‡å»ºæ•ˆç‡ã€‚é€šè¿‡ä»å¯†é›†åŒå‘è§†è§‰å‡ ä½•å˜æ¢å™¨ä¸­è’¸é¦çŸ¥è¯†ï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾—ä»¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒé«˜ç©ºé—´ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†åœ¨çº¿æ¨ç†é€Ÿåº¦ï¼Œé€‚ç”¨äºå¯æ‰©å±•çš„äº¤äº’å¼4Dè§†è§‰ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15856",
            "title": "Latent Denoising Makes Good Visual Tokenizers",
            "url": "https://huggingface.co/papers/2507.15856",
            "abstract": "Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian noise or masking -- a process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as a fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design.",
            "score": 5,
            "issue_id": 4940,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "60a696cb47720198",
            "authors": [
                "Jiawei Yang",
                "Tianhong Li",
                "Lijie Fan",
                "Yonglong Tian",
                "Yue Wang"
            ],
            "affiliations": [
                "Google DeepMind",
                "MIT CSAIL",
                "OpenAI",
                "USC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15856.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "ğŸ§¹",
                "ru": {
                    "title": "Ğ¨ÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Latent Denoising Tokenizer (l-DeTok), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ l-DeTok Ğ½Ğ°Ğ´ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑˆĞµÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Generative Models with Denoising Tokenizers",
                    "desc": "This paper explores how visual tokenizers can be improved for generative modeling by focusing on a process called denoising. The authors propose a new tokenizer, the Latent Denoising Tokenizer (l-DeTok), which aligns its embeddings with the goal of reconstructing clean images from corrupted inputs. By training this tokenizer to handle noise and masking, it becomes more effective at generating high-quality outputs. The results show that l-DeTok outperforms traditional tokenizers in various generative models, suggesting that denoising should be a key consideration in future tokenizer designs."
                },
                "zh": {
                    "title": "å»å™ªï¼šåˆ†è¯å™¨è®¾è®¡çš„æ–°åŸåˆ™",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†è§†è§‰åˆ†è¯å™¨åœ¨ç”Ÿæˆå»ºæ¨¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºäº†å¯¹åˆ†è¯å™¨åµŒå…¥ä¸å»å™ªç›®æ ‡è¿›è¡Œå¯¹é½çš„æ¦‚å¿µã€‚æˆ‘ä»¬å¼•å…¥äº†æ½œåœ¨å»å™ªåˆ†è¯å™¨ï¼ˆl-DeTokï¼‰ï¼Œè¯¥åˆ†è¯å™¨æ—¨åœ¨ä»å—åˆ°å¹²æ‰°çš„æ½œåœ¨åµŒå…¥ä¸­é‡å»ºå¹²å‡€å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œl-DeTokåœ¨å¤šä¸ªç”Ÿæˆæ¨¡å‹ä¸Šä¼˜äºä¼ ç»Ÿåˆ†è¯å™¨ï¼ŒéªŒè¯äº†å»å™ªä½œä¸ºåˆ†è¯å™¨è®¾è®¡çš„é‡è¦åŸåˆ™ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸€å‘ç°èƒ½å¤Ÿä¸ºæœªæ¥çš„åˆ†è¯å™¨è®¾è®¡æä¾›æ–°çš„è§†è§’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15728",
            "title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation",
            "url": "https://huggingface.co/papers/2507.15728",
            "abstract": "Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framework that leverages condensed tokens to address these issues. Our method decomposes long video generation into three core tasks: (1) inner-clip semantic control, (2) long-term consistency control, and (3) inter-clip smooth transition. First, we train To2V (Token-to-Video), a short video diffusion model guided by text and video tokens, with a Video Tokenizer that condenses short clips into semantically rich tokens. Second, we introduce T2To (Text-to-Token), a video token diffusion transformer that generates all tokens at once, ensuring global consistency across clips. Finally, during inference, an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips, reducing boundary artifacts and enhancing smooth transitions. Experimental results demonstrate that our approach significantly enhances long-term temporal and content coherence without incurring prohibitive computational overhead. By leveraging condensed tokens and pre-trained short video models, our method provides a scalable, modular solution for long video generation, opening new possibilities for storytelling, cinematic production, and immersive simulations. Please see our project page at https://vicky0522.github.io/tokensgen-webpage/ .",
            "score": 3,
            "issue_id": 4947,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "a1ab25505568523b",
            "authors": [
                "Wenqi Ouyang",
                "Zeqi Xiao",
                "Danni Yang",
                "Yifan Zhou",
                "Shuai Yang",
                "Lei Yang",
                "Jianlou Si",
                "Xingang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "SenseTime Research",
                "Wangxuan Institute of Computer Technology, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15728.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#story_generation",
                    "#long_context",
                    "#multimodal",
                    "#video",
                    "#inference"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "TokensGen: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "TokensGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ To2V Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ ĞºĞ»Ğ¸Ğ¿Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ° T2To ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ FIFO-Diffusion Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğµ ĞºĞ»Ğ¸Ğ¿Ñ‹, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."
                },
                "en": {
                    "title": "TokensGen: Seamless Long Video Generation with Condensed Tokens",
                    "desc": "This paper introduces TokensGen, a two-stage framework designed to generate long videos more effectively. It addresses challenges like memory limitations and inconsistencies in long video generation by using condensed tokens. The method involves training a short video diffusion model and a video token diffusion transformer to ensure semantic richness and global consistency. The results show that TokensGen improves the coherence of long videos while maintaining manageable computational demands, making it suitable for various applications in storytelling and simulations."
                },
                "zh": {
                    "title": "TokensGenï¼šé•¿è§†é¢‘ç”Ÿæˆçš„æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "ç”Ÿæˆä¸€è‡´çš„é•¿è§†é¢‘æ˜¯ä¸€ä¸ªå¤æ‚çš„æŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹å¯ä»¥ç”Ÿæˆè§†è§‰ä¸Šä»¤äººå°è±¡æ·±åˆ»çš„çŸ­ç‰‡ï¼Œä½†æ‰©å±•åˆ°æ›´é•¿çš„æ—¶é•¿å¸¸å¸¸ä¼šå¯¼è‡´å†…å­˜ç“¶é¢ˆå’Œé•¿æœŸä¸ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºTokensGençš„æ–°å‹ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡æµ“ç¼©çš„æ ‡è®°æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é•¿è§†é¢‘ç”Ÿæˆä¸­æ˜¾è‘—æé«˜äº†æ—¶é—´å’Œå†…å®¹çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶é¿å…äº†è¿‡é«˜çš„è®¡ç®—å¼€é”€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15640",
            "title": "Data Mixing Agent: Learning to Re-weight Domains for Continual\n  Pre-training",
            "url": "https://huggingface.co/papers/2507.15640",
            "abstract": "Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data.",
            "score": 3,
            "issue_id": 4942,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "2ce6b05c03e1226b",
            "authors": [
                "Kailai Yang",
                "Xiao Liu",
                "Lei Ji",
                "Hao Li",
                "Yeyun Gong",
                "Peng Cheng",
                "Mao Yang"
            ],
            "affiliations": [
                "Microsoft Research",
                "The University of Manchester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15640.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#transfer_learning",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Data Mixing Agent - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Data Mixing Agent Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Reinforcement Learning for Balanced Data Mixing in Language Models",
                    "desc": "The paper introduces the Data Mixing Agent, a novel framework that utilizes reinforcement learning to dynamically re-weight training data for continual pre-training of large language models. This approach addresses the challenge of catastrophic forgetting by balancing the performance between source and target fields without relying on manual heuristics. The agent learns effective data mixing strategies through interactions with a feedback-rich environment, allowing it to generalize across various domains. Experimental results demonstrate that the Data Mixing Agent significantly improves performance in tasks like math reasoning and code generation, showcasing its versatility and efficiency in leveraging limited source-field data."
                },
                "zh": {
                    "title": "æ•°æ®æ··åˆä»£ç†ï¼šå¹³è¡¡æºä¸ç›®æ ‡é¢†åŸŸçš„æ™ºèƒ½å­¦ä¹ ",
                    "desc": "æ•°æ®æ··åˆä»£ç†æ˜¯ä¸€ç§åŸºäºæ¨¡å‹çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æœ‰æ•ˆåœ°é‡æ–°åŠ æƒè®­ç»ƒæ•°æ®ï¼Œä»¥å¹³è¡¡åœ¨æŒç»­é¢„è®­ç»ƒä¸­æºé¢†åŸŸå’Œç›®æ ‡é¢†åŸŸçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å°è§„æ¨¡ç‰¹å®šä»»åŠ¡æ•°æ®ä¸ŠæŒç»­é¢„è®­ç»ƒæ—¶å¯èƒ½å‡ºç°çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚é€šè¿‡æå‡ºæ•°æ®æ··åˆä»£ç†ï¼Œç ”ç©¶è€…è¯æ˜äº†æ›´é€šç”¨çš„å¯å‘å¼æ–¹æ³•å¯ä»¥è¢«å‚æ•°åŒ–ï¼Œä»è€Œå®ç°ç«¯åˆ°ç«¯çš„å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ä»£ç†åœ¨æ•°å­¦æ¨ç†çš„æŒç»­é¢„è®­ç»ƒä¸­è¡¨ç°ä¼˜äºå¼ºåŸºçº¿ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„æºé¢†åŸŸå’Œç›®æ ‡æ¨¡å‹ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.15550",
            "title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with\n  Controlled Priors",
            "url": "https://huggingface.co/papers/2507.15550",
            "abstract": "PhysGym, a new benchmark suite, evaluates large language model-based agents' scientific reasoning in interactive physics environments, focusing on their handling of complexity and prior knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior knowledge, requires specialized benchmarks currently lacking in the landscape. To address this gap, we introduce PhysGym, a novel benchmark suite and simulation platform for rigorously assessing LLM-based scientific reasoning in interactive physics environments. PhysGym's primary contribution lies in its sophisticated control over the level of prior knowledge provided to the agent. This allows researchers to dissect agent performance along axes including the complexity of the problem and the prior knowledge levels. The benchmark comprises a suite of interactive simulations, where agents must actively probe environments, gather data sequentially under constraints and formulate hypotheses about underlying physical laws. PhysGym provides standardized evaluation protocols and metrics for assessing hypothesis accuracy and model fidelity. We demonstrate the benchmark's utility by presenting results from baseline LLMs, showcasing its ability to differentiate capabilities based on varying priors and task complexity.",
            "score": 3,
            "issue_id": 4947,
            "pub_date": "2025-07-21",
            "pub_date_card": {
                "ru": "21 Ğ¸ÑĞ»Ñ",
                "en": "July 21",
                "zh": "7æœˆ21æ—¥"
            },
            "hash": "2cbec0e62f66698a",
            "authors": [
                "Yimeng Chen",
                "Piotr PiÈ©kos",
                "Mateusz Ostaszewski",
                "Firas Laakom",
                "JÃ¼rgen Schmidhuber"
            ],
            "affiliations": [
                "Center of Excellence for Generative AI, KAUST",
                "NNAISENSE",
                "The Swiss AI Lab, IDSIA-USI/SUPSI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.15550.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#reasoning",
                    "#science"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "PhysGym: Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ",
                    "desc": "PhysGym - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. PhysGym Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ‘Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ€ĞµĞ´Ñƒ, ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹ Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ñ…."
                },
                "en": {
                    "title": "PhysGym: Benchmarking Scientific Reasoning in Physics with LLMs",
                    "desc": "PhysGym is a new benchmark suite designed to evaluate how well large language model (LLM) agents can reason scientifically in interactive physics settings. It focuses on how these agents manage different levels of complexity in their environments and how they use prior knowledge to solve problems. The benchmark includes a variety of simulations where agents must explore, collect data, and form hypotheses about physical laws. By providing standardized evaluation metrics, PhysGym helps researchers understand the performance of LLMs based on their prior knowledge and the complexity of tasks they face."
                },
                "zh": {
                    "title": "PhysGymï¼šè¯„ä¼°æ™ºèƒ½ä½“ç§‘å­¦æ¨ç†çš„æ–°åŸºå‡†",
                    "desc": "PhysGymæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“åœ¨äº’åŠ¨ç‰©ç†ç¯å¢ƒä¸­çš„ç§‘å­¦æ¨ç†èƒ½åŠ›ã€‚å®ƒä¸“æ³¨äºæ™ºèƒ½ä½“å¦‚ä½•å¤„ç†å¤æ‚æ€§å’Œåˆ©ç”¨å…ˆå‰çŸ¥è¯†ã€‚PhysGymçš„ä¸»è¦è´¡çŒ®åœ¨äºå¯¹æä¾›ç»™æ™ºèƒ½ä½“çš„å…ˆå‰çŸ¥è¯†æ°´å¹³è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œä»è€Œå¸®åŠ©ç ”ç©¶äººå‘˜åˆ†ææ™ºèƒ½ä½“åœ¨ä¸åŒé—®é¢˜å¤æ‚æ€§å’ŒçŸ¥è¯†æ°´å¹³ä¸‹çš„è¡¨ç°ã€‚è¯¥åŸºå‡†åŒ…æ‹¬ä¸€ç³»åˆ—äº’åŠ¨æ¨¡æ‹Ÿï¼Œæ™ºèƒ½ä½“éœ€è¦åœ¨çº¦æŸæ¡ä»¶ä¸‹ä¸»åŠ¨æ¢æµ‹ç¯å¢ƒã€æ”¶é›†æ•°æ®å¹¶å½¢æˆå…³äºç‰©ç†æ³•åˆ™çš„å‡è®¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.14295",
            "title": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning",
            "url": "https://huggingface.co/papers/2507.14295",
            "abstract": "Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, we observe that models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses. We ask: can LRMs learn to reflect their answers in a multi-turn context? In this work, we find that training models with multi-turn RL using only unary feedback (e.g., \"Let's try again\") after wrong answers can improve both single-turn performance and multi-turn reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement learning, which uses minimal yet common unary user feedback during iterative problem solving. It can be easily applied to existing single-turn RL training setups. Experimental results show that RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving. To further minimize the number of turns needed for a correct answer while encouraging diverse reasoning when mistakes occur, we design reward structures that guide models to produce careful and deliberate answers in each turn. Code: https://github.com/lichengliu03/unary-feedback",
            "score": 3,
            "issue_id": 4945,
            "pub_date": "2025-07-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ»Ñ",
                "en": "July 18",
                "zh": "7æœˆ18æ—¥"
            },
            "hash": "6b7877061c71a067",
            "authors": [
                "Licheng Liu",
                "Zihan Wang",
                "Linjie Li",
                "Chenwei Xu",
                "Yiping Lu",
                "Han Liu",
                "Avirup Sil",
                "Manling Li"
            ],
            "affiliations": [
                "IBM Research AI",
                "Imperial College London",
                "Northwestern University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.14295.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Unary Feedback as Observation (UFO), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑƒĞ½Ğ°Ñ€Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UFO ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ 14% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ."
                },
                "en": {
                    "title": "Empowering LRMs with Unary Feedback for Better Multi-Turn Reasoning",
                    "desc": "This paper addresses the challenges faced by Large Reasoning Models (LRMs) in multi-turn problem solving, particularly their ability to reflect on and revise their answers based on feedback. The authors highlight that traditional Reinforcement Learning (RL) methods often lead to models that struggle with multi-turn interactions and produce repetitive responses. They propose a novel approach called Unary Feedback as Observation (UFO), which utilizes simple unary feedback to enhance both single-turn and multi-turn reasoning capabilities. Experimental results demonstrate that this method improves multi-turn reasoning accuracy significantly while maintaining performance in single-turn tasks."
                },
                "zh": {
                    "title": "å¤šè½®æ¨ç†ï¼Œå•å…ƒåé¦ˆåŠ©åŠ›",
                    "desc": "å¤šè½®é—®é¢˜è§£å†³å¯¹å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰è‡³å…³é‡è¦ï¼Œä½†ä¹Ÿå¾ˆå…·æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•é€šå¸¸åœ¨å•è½®èŒƒå¼ä¸‹è®­ç»ƒæ¨¡å‹ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤šè½®ä¸Šä¸‹æ–‡ä¸­éš¾ä»¥åæ€å’Œä¿®æ­£ç­”æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºå•å…ƒåé¦ˆä½œä¸ºè§‚å¯Ÿï¼ˆUFOï¼‰ï¼Œé€šè¿‡ä½¿ç”¨ç®€å•çš„åé¦ˆæ¥æé«˜æ¨¡å‹çš„å¤šè½®æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨UFOçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¯ä»¥ä¿æŒå•è½®æ€§èƒ½ï¼Œå¹¶å°†å¤šè½®æ¨ç†çš„å‡†ç¡®æ€§æé«˜å¤šè¾¾14%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12549",
            "title": "The Serial Scaling Hypothesis",
            "url": "https://huggingface.co/papers/2507.12549",
            "abstract": "Recognizing inherently serial problems is crucial for advancing machine learning, model design, and hardware development, especially for complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While machine learning has advanced through massive parallelization, we identify a critical blind spot: some problems are fundamentally sequential. These \"inherently serial\" problems-from mathematical reasoning to physical simulations to sequential decision-making-require dependent computational steps that cannot be parallelized. Drawing from complexity theory, we formalize this distinction and demonstrate that current parallel-centric architectures face fundamental limitations on such tasks. We argue that recognizing the serial nature of computation holds profound implications on machine learning, model design, hardware development. As AI tackles increasingly complex reasoning, deliberately scaling serial computation-not just parallel computation-is essential for continued progress.",
            "score": 3,
            "issue_id": 4954,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ»Ñ",
                "en": "July 16",
                "zh": "7æœˆ16æ—¥"
            },
            "hash": "762d2d97f7da6120",
            "authors": [
                "Yuxi Liu",
                "Konpat Preechakul",
                "Kananart Kuwaranancharoen",
                "Yutong Bai"
            ],
            "affiliations": [
                "Independent Researcher",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12549.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "â³",
                "ru": {
                    "title": "ĞŸĞ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ 'Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ…' Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ´Ğ»Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ°Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Embracing Serial Computation for Advanced AI",
                    "desc": "This paper highlights the importance of recognizing inherently serial problems in machine learning, which are tasks that require sequential steps and cannot be effectively parallelized. It points out that while machine learning has benefited from parallel processing, certain complex reasoning tasks, like mathematical reasoning and sequential decision-making, depend on a series of interrelated computations. The authors use concepts from complexity theory to show that current architectures focused on parallelization struggle with these serial tasks. They argue that to advance AI capabilities, it is crucial to develop models and hardware that can efficiently handle serial computation alongside parallel processing."
                },
                "zh": {
                    "title": "è¯†åˆ«ä¸²è¡Œé—®é¢˜ï¼Œæ¨åŠ¨æœºå™¨å­¦ä¹ è¿›æ­¥",
                    "desc": "åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œè¯†åˆ«å›ºæœ‰çš„ä¸²è¡Œé—®é¢˜å¯¹äºæ¨åŠ¨æ¨¡å‹è®¾è®¡å’Œç¡¬ä»¶å¼€å‘è‡³å…³é‡è¦ã€‚è¿™äº›é—®é¢˜éœ€è¦ä¾èµ–çš„è®¡ç®—æ­¥éª¤ï¼Œæ— æ³•é€šè¿‡å¹¶è¡ŒåŒ–æ¥è§£å†³ã€‚æˆ‘ä»¬é€šè¿‡å¤æ‚æ€§ç†è®ºæ¥æ­£å¼åŒ–è¿™ä¸€åŒºåˆ«ï¼Œå¹¶å±•ç¤ºå½“å‰ä»¥å¹¶è¡Œä¸ºä¸­å¿ƒçš„æ¶æ„åœ¨å¤„ç†è¿™äº›ä»»åŠ¡æ—¶é¢ä¸´çš„åŸºæœ¬é™åˆ¶ã€‚ä¸ºäº†åº”å¯¹æ—¥ç›Šå¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œå¿…é¡»æœ‰æ„è¯†åœ°æ‰©å±•ä¸²è¡Œè®¡ç®—ï¼Œè€Œä¸ä»…ä»…æ˜¯å¹¶è¡Œè®¡ç®—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.10935",
            "title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised\n  Cross-View Localization",
            "url": "https://huggingface.co/papers/2507.10935",
            "abstract": "Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill.",
            "score": 1,
            "issue_id": 4942,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ»Ñ",
                "en": "July 15",
                "zh": "7æœˆ15æ—¥"
            },
            "hash": "8d8109c5462763ac",
            "authors": [
                "Shaowen Tong",
                "Zimin Xia",
                "Alexandre Alahi",
                "Xuming He",
                "Yujiao Shi"
            ],
            "affiliations": [
                "Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland",
                "ShanghaiTech University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.10935.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "GeoDistill: Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ½Ğ° ÑĞ»ÑƒĞ¶Ğ±Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "GeoDistill - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ»Ğ°Ğ±Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ…ĞµĞ¼Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ£Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ° ÑƒÑ‡ĞµĞ½Ğ¸Ğº Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ»Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±ĞµÑÑ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "GeoDistill: Enhancing Cross-View Localization with Weak Supervision",
                    "desc": "This paper presents GeoDistill, a novel framework for cross-view localization that estimates a camera's 3-DoF pose by aligning ground-level images with satellite images. It addresses the challenge of requiring expensive ground-truth pose annotations by employing a weakly supervised self-distillation approach. The framework utilizes a teacher-student model where the teacher localizes a panoramic image, while the student learns from a limited Field-of-View (FoV) version of the same image. By focusing on important features and ignoring irrelevant textures, GeoDistill enhances localization accuracy and reduces uncertainty, making it a scalable solution for outdoor applications like autonomous navigation."
                },
                "zh": {
                    "title": "GeoDistillï¼šé«˜æ•ˆçš„è·¨è§†è§’å®šä½è§£å†³æ–¹æ¡ˆ",
                    "desc": "è·¨è§†è§’å®šä½æ˜¯é€šè¿‡å°†åœ°é¢å›¾åƒä¸å«æ˜Ÿå›¾åƒå¯¹é½æ¥ä¼°è®¡ç›¸æœºçš„ä¸‰è‡ªç”±åº¦å§¿æ€ï¼Œè¿™åœ¨è‡ªåŠ¨å¯¼èˆªå’Œå¢å¼ºç°å®ç­‰å¤§è§„æ¨¡æˆ·å¤–åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå®Œå…¨ç›‘ç£å­¦ä¹ ï¼Œè¿™éœ€è¦æ˜‚è´µçš„çœŸå®å§¿æ€æ ‡æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†GeoDistillï¼Œä¸€ä¸ªå‡ ä½•å¼•å¯¼çš„å¼±ç›‘ç£è‡ªè’¸é¦æ¡†æ¶ï¼Œåˆ©ç”¨æ•™å¸ˆ-å­¦ç”Ÿå­¦ä¹ å’ŒåŸºäºè§†åœº(FoV)çš„æ©è”½æ¥å¢å¼ºå±€éƒ¨ç‰¹å¾å­¦ä¹ ï¼Œä»è€Œå®ç°ç¨³å¥çš„è·¨è§†è§’å®šä½ã€‚GeoDistillé€šè¿‡å¯¹é½å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹çš„é¢„æµ‹ï¼Œå¸®åŠ©å­¦ç”Ÿæ¨¡å‹ä¸“æ³¨äºå…³é”®ç‰¹å¾ï¼Œæé«˜äº†å®šä½ç²¾åº¦å¹¶å‡å°‘äº†ä¸ç¡®å®šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.14102",
            "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based\n  Classification in Computed Tomography",
            "url": "https://huggingface.co/papers/2507.14102",
            "abstract": "Accurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to detect localized abnormalities that require focused analysis. We introduce UGPL, an uncertainty-guided progressive learning framework that performs a global-to-local analysis by first identifying regions of diagnostic ambiguity and then conducting detailed examination of these critical areas. Our approach employs evidential deep learning to quantify predictive uncertainty, guiding the extraction of informative patches through a non-maximum suppression mechanism that maintains spatial diversity. This progressive refinement strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate both contextual information and fine-grained details. Experiments across three CT datasets demonstrate that UGPL consistently outperforms state-of-the-art methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our analysis shows that the uncertainty-guided component provides substantial benefits, with performance dramatically increasing when the full progressive learning pipeline is implemented. Our code is available at: https://github.com/shravan-18/UGPL",
            "score": 0,
            "issue_id": 4945,
            "pub_date": "2025-07-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ»Ñ",
                "en": "July 18",
                "zh": "7æœˆ18æ—¥"
            },
            "hash": "d4150eeda5b606d4",
            "authors": [
                "Shravan Venkatraman",
                "Pavan Kumar S",
                "Rakesh Raj Madavan",
                "Chandrakala S"
            ],
            "affiliations": [
                "Shiv Nadar University, Chennai, India",
                "Vellore Institute of Technology, Chennai, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.14102.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "UGPL: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞšĞ¢ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğº Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ",
                    "desc": "UGPL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞšĞ¢-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğº Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ¸. UGPL Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ²Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ² Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ UGPL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‡ĞµĞº, Ñ€Ğ°ĞºĞ° Ğ»ĞµĞ³ĞºĞ¸Ñ… Ğ¸ COVID-19 Ğ½Ğ° ĞšĞ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Enhancing CT Image Classification with Uncertainty-Guided Learning",
                    "desc": "This paper presents UGPL, a novel framework for improving the classification of CT images by focusing on areas of diagnostic uncertainty. Unlike traditional methods that analyze images uniformly, UGPL first identifies ambiguous regions and then conducts a detailed examination of these areas. It utilizes evidential deep learning to measure predictive uncertainty, which helps in selecting informative patches while preserving spatial diversity. The results show that UGPL significantly enhances accuracy in detecting kidney abnormalities, lung cancer, and COVID-19 compared to existing techniques."
                },
                "zh": {
                    "title": "ä¸ç¡®å®šæ€§å¼•å¯¼çš„æ¸è¿›å­¦ä¹ ï¼Œæå‡CTå›¾åƒåˆ†ç±»ç²¾åº¦",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºUGPLçš„ä¸ç¡®å®šæ€§å¼•å¯¼æ¸è¿›å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæé«˜è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒçš„åˆ†ç±»å‡†ç¡®æ€§ã€‚UGPLé€šè¿‡é¦–å…ˆè¯†åˆ«è¯Šæ–­æ¨¡ç³ŠåŒºåŸŸï¼Œç„¶åå¯¹è¿™äº›å…³é”®åŒºåŸŸè¿›è¡Œè¯¦ç»†åˆ†æï¼Œå®ç°äº†ä»å…¨å±€åˆ°å±€éƒ¨çš„åˆ†æã€‚è¯¥æ–¹æ³•åˆ©ç”¨è¯æ®æ·±åº¦å­¦ä¹ é‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§ï¼Œå¹¶é€šè¿‡éæå¤§å€¼æŠ‘åˆ¶æœºåˆ¶æå–ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒå—ï¼Œä¿æŒç©ºé—´å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUGPLåœ¨è‚¾è„å¼‚å¸¸ã€è‚ºç™Œå’ŒCOVID-19æ£€æµ‹ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†3.29%ã€2.46%å’Œ8.08%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12674",
            "title": "ParaStudent: Generating and Evaluating Realistic Student Code by\n  Teaching LLMs to Struggle",
            "url": "https://huggingface.co/papers/2507.12674",
            "abstract": "Large Language Models (LLMs) have shown strong performance on programming tasks, but can they generate student-like code like real students - imperfect, iterative, and stylistically diverse? We present ParaStudent, a systematic study of LLM-based \"student-like\" code generation in an introductory programming course setting. Using a dataset of timestamped student submissions across multiple semesters, we design low- and high-resolution experiments to model student progress and evaluate code outputs along semantic, functional, and stylistic dimensions. Our results show that fine-tuning significantly improves alignment with real student trajectories and captures error patterns, incremental improvements, and stylistic variations more faithfully. This study shows that modeling realistic student code requires capturing learning dynamics through context-aware generation, temporal modeling, and multi-dimensional evaluation. Code for experiments and evaluation is available at https://github.com/mmiroyan/ParaStudent.",
            "score": 0,
            "issue_id": 4953,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ»Ñ",
                "en": "July 16",
                "zh": "7æœˆ16æ—¥"
            },
            "hash": "60501cf4becaa52e",
            "authors": [
                "Mihran Miroyan",
                "Rose Niousha",
                "Joseph E. Gonzalez",
                "Gireeja Ranade",
                "Narges Norouzi"
            ],
            "affiliations": [
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12674.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#dataset",
                    "#plp",
                    "#optimization",
                    "#long_context",
                    "#training"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "LLM ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ ĞºĞ¾Ğ´ ĞºĞ°Ğº ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ñ‹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ParaStudent Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´, Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğ¹ - Ğ½ĞµÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ğ¹, Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ."
                },
                "en": {
                    "title": "Mimicking Student Code: Learning Dynamics in LLMs",
                    "desc": "This paper introduces ParaStudent, a study focused on how Large Language Models (LLMs) can generate code that resembles the work of real students, which is often imperfect and diverse. The authors analyze a dataset of student submissions over time to understand how students progress in their coding skills. They conduct experiments to evaluate the generated code based on its meaning, functionality, and style, finding that fine-tuning the models enhances their ability to mimic real student behavior. The findings emphasize the importance of incorporating learning dynamics and context in generating realistic student-like code."
                },
                "zh": {
                    "title": "æ¨¡æ‹Ÿå­¦ç”Ÿä»£ç ç”Ÿæˆçš„ç ”ç©¶",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¼–ç¨‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬èƒ½å¦ç”Ÿæˆç±»ä¼¼å­¦ç”Ÿçš„ä»£ç ã€‚æˆ‘ä»¬æå‡ºäº†ParaStudentï¼Œé€šè¿‡åˆ†æå¤šå­¦æœŸçš„å­¦ç”Ÿæäº¤æ•°æ®ï¼Œè®¾è®¡äº†ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡çš„å®éªŒæ¥æ¨¡æ‹Ÿå­¦ç”Ÿçš„è¿›æ­¥ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¾®è°ƒæ¨¡å‹æ˜¾è‘—æé«˜äº†ä¸çœŸå®å­¦ç”Ÿå­¦ä¹ è½¨è¿¹çš„ä¸€è‡´æ€§ï¼Œå¹¶æ›´çœŸå®åœ°æ•æ‰äº†é”™è¯¯æ¨¡å¼ã€é€æ­¥æ”¹è¿›å’Œé£æ ¼å˜åŒ–ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨ç”Ÿæˆä»£ç æ—¶ï¼Œéœ€è¦é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€æ—¶é—´å»ºæ¨¡å’Œå¤šç»´è¯„ä¼°æ¥æ•æ‰çœŸå®çš„å­¦ä¹ åŠ¨æ€ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-22.html",
    "link_next": "2025-07-24.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "22.07",
        "en": "07/22",
        "zh": "7æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "24.07",
        "en": "07/24",
        "zh": "7æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 3,
        "#benchmark": 11,
        "#agents": 7,
        "#cv": 5,
        "#rl": 7,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 1,
        "#inference": 2,
        "#3d": 2,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 4,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 13,
        "#robotics": 2,
        "#agi": 3,
        "#games": 1,
        "#interpretability": 3,
        "#reasoning": 11,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 14,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    }
}