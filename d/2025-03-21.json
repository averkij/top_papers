{
    "date": {
        "ru": "21 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 21",
        "zh": "3æœˆ21æ—¥"
    },
    "time_utc": "2025-03-21 02:20",
    "weekday": 4,
    "issue_id": 2822,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.16278",
            "title": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens",
            "url": "https://huggingface.co/papers/2503.16278",
            "abstract": "Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding ({3D GU}) in AI for science, these tasks have largely evolved independently, with autoregressive methods remaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified framework that seamlessly integrates {3D GU} tasks via autoregressive prediction. At its core, Uni-3DAR employs a novel hierarchical tokenization that compresses 3D space using an octree, leveraging the inherent sparsity of 3D structures. It then applies an additional tokenization for fine-grained structural details, capturing key attributes such as atom types and precise spatial coordinates in microscopic 3D structures. We further propose two optimizations to enhance efficiency and effectiveness. The first is a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. The second is a masked next-token prediction mechanism tailored for dynamically varying token positions, significantly boosting model performance. By combining these strategies, Uni-3DAR successfully unifies diverse {3D GU} tasks within a single autoregressive framework. Extensive experiments across multiple microscopic {3D GU} tasks, including molecules, proteins, polymers, and crystals, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256\\% relative improvement while delivering inference speeds up to 21.8x faster. The code is publicly available at https://github.com/dptech-corp/Uni-3DAR.",
            "score": 1,
            "issue_id": 2822,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "7012b17f03aeb180",
            "authors": [
                "Shuqi Lu",
                "Haowei Lin",
                "Lin Yao",
                "Zhifeng Gao",
                "Xiaohong Ji",
                "Weinan E",
                "Linfeng Zhang",
                "Guolin Ke"
            ],
            "affiliations": [
                "AI for Science Institute, Beijing 100080, China",
                "Center for Machine Learning Research, Peking University, Beijing 100084, China",
                "DP Technology, Beijing, 100080, China",
                "Institute for Artificial Intelligence, Peking University, Beijing 100871, China",
                "School of Mathematical Sciences, Peking University, Beijing, 100871, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16278.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#training",
                    "#architecture",
                    "#3d",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Uni-3DAR: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Uni-3DAR - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾ĞºÑ‚Ğ¾Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Uni-3DAR Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Unifying 3D Generation and Understanding with Uni-3DAR",
                    "desc": "This paper presents Uni-3DAR, a novel framework that integrates 3D structural generation and understanding tasks using autoregressive next-token prediction. It introduces a hierarchical tokenization method that efficiently compresses 3D space with an octree, capturing both the overall structure and fine details like atom types and spatial coordinates. The framework includes optimizations such as a two-level subtree compression strategy and a masked next-token prediction mechanism, enhancing both efficiency and model performance. Experimental results show that Uni-3DAR significantly outperforms existing models, achieving faster inference speeds and improved accuracy across various microscopic 3D tasks."
                },
                "zh": {
                    "title": "ç»Ÿä¸€3Dç»“æ„ç”Ÿæˆä¸ç†è§£çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºUni-3DARçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªå›å½’é¢„æµ‹æ•´åˆ3Dç»“æ„ç”Ÿæˆä¸ç†è§£ï¼ˆ3D GUï¼‰ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ–°é¢–çš„åˆ†å±‚æ ‡è®°åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨å…«å‰æ ‘å‹ç¼©3Dç©ºé—´ï¼Œå¹¶ä¸ºå¾®è§‚3Dç»“æ„æ•æ‰å…³é”®å±æ€§å¦‚åŸå­ç±»å‹å’Œç²¾ç¡®åæ ‡ã€‚Uni-3DARè¿˜æå‡ºäº†ä¸¤é¡¹ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥æé«˜æ•ˆç‡å’Œæ•ˆæœï¼ŒåŒ…æ‹¬ä¸¤çº§å­æ ‘å‹ç¼©å’ŒåŠ¨æ€å˜åŒ–æ ‡è®°ä½ç½®çš„æ©ç ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æœºåˆ¶ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒUni-3DARåœ¨å¤šç§å¾®è§‚3D GUä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ‰©æ•£æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16031",
            "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging\n  Fabricated Claims with Humorous Content",
            "url": "https://huggingface.co/papers/2503.16031",
            "abstract": "This paper presents the Deceptive Humor Dataset (DHD), a novel resource for studying humor derived from fabricated claims and misinformation. In an era of rampant misinformation, understanding how humor intertwines with deception is essential. DHD consists of humor-infused comments generated from false narratives, incorporating fabricated claims and manipulated information using the ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging from 1 for subtle satire to 3 for high-level satire and classified into five distinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and Absurdity. The dataset spans multiple languages including English, Telugu, Hindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En, Ta-En), making it a valuable multilingual benchmark. By introducing DHD, we establish a structured foundation for analyzing humor in deceptive contexts, paving the way for a new research direction that explores how humor not only interacts with misinformation but also influences its perception and spread. We establish strong baselines for the proposed dataset, providing a foundation for future research to benchmark and advance deceptive humor detection models.",
            "score": 1,
            "issue_id": 2822,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "80726382feca8f20",
            "authors": [
                "Sai Kartheek Reddy Kasu",
                "Shankar Biradar",
                "Sunil Saumya"
            ],
            "affiliations": [
                "IIIT Dharwad",
                "MIT Manipal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16031.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#low_resource",
                    "#benchmark",
                    "#dataset",
                    "#multilingual"
                ],
                "emoji": "ğŸ¤¡",
                "ru": {
                    "title": "Ğ¡Ğ¼ĞµÑ… ÑĞºĞ²Ğ¾Ğ·ÑŒ Ğ»Ğ¾Ğ¶ÑŒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ğ¾Ñ€Ğ°",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DHD (Deceptive Humor Dataset) Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ€Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ²Ñ‹Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. DHD ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· ÑĞ¼Ğ¾Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ², ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ChatGPT-4o. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ Ğ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ ÑĞ°Ñ‚Ğ¸Ñ€Ñ‹ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ¿Ğ¾ Ğ¿ÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ ÑĞ¼Ğ¾Ñ€Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ†ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unraveling Humor in the Age of Misinformation",
                    "desc": "The Deceptive Humor Dataset (DHD) is a new resource designed to explore the relationship between humor and misinformation. It includes humor-filled comments based on false narratives, generated using the ChatGPT-4o model, and is labeled with a Satire Level and categorized into five types of humor. This dataset supports multiple languages, making it a useful tool for multilingual research in humor and deception. By providing a structured dataset, DHD aims to enhance the understanding of how humor can affect the perception and dissemination of misinformation."
                },
                "zh": {
                    "title": "æ­ç¤ºå¹½é»˜ä¸æ¬ºéª—çš„äº¤ç»‡",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„èµ„æºâ€”â€”æ¬ºéª—å¹½é»˜æ•°æ®é›†ï¼ˆDHDï¼‰ï¼Œç”¨äºç ”ç©¶æºè‡ªè™šå‡å£°æ˜å’Œé”™è¯¯ä¿¡æ¯çš„å¹½é»˜ã€‚åœ¨ä¿¡æ¯æ³›æ»¥çš„æ—¶ä»£ï¼Œç†è§£å¹½é»˜ä¸æ¬ºéª—ä¹‹é—´çš„å…³ç³»è‡³å…³é‡è¦ã€‚DHDåŒ…å«ä»è™šå‡å™è¿°ä¸­ç”Ÿæˆçš„å¹½é»˜è¯„è®ºï¼Œå¹¶ä½¿ç”¨ChatGPT-4oæ¨¡å‹ç”Ÿæˆè™šå‡å£°æ˜å’Œæ“æ§ä¿¡æ¯ã€‚æ¯ä¸ªå®ä¾‹éƒ½æ ‡æ³¨äº†è®½åˆºæ°´å¹³ï¼Œå¹¶åˆ†ä¸ºäº”ç§å¹½é»˜ç±»åˆ«ï¼Œä¸ºåˆ†ææ¬ºéª—èƒŒæ™¯ä¸‹çš„å¹½é»˜æä¾›äº†ç»“æ„åŒ–åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15567",
            "title": "Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling",
            "url": "https://huggingface.co/papers/2503.15567",
            "abstract": "3D molecule generation is crucial for drug discovery and material science, requiring models to process complex multi-modalities, including atom types, chemical bonds, and 3D coordinates. A key challenge is integrating these modalities of different shapes while maintaining SE(3) equivariance for 3D coordinates. To achieve this, existing approaches typically maintain separate latent spaces for invariant and equivariant modalities, reducing efficiency in both training and sampling. In this work, we propose Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D), a multi-modal VAE that compresses 3D molecules into latent sequences from a unified latent space, while maintaining near-zero reconstruction error. This unified latent space eliminates the complexities of handling multi-modality and equivariance when performing latent diffusion modeling. We demonstrate this by employing the Diffusion Transformer--a general-purpose diffusion model without any molecular inductive bias--for latent generation. Extensive experiments on GEOM-Drugs and QM9 datasets demonstrate that our method significantly establishes new benchmarks in both de novo and conditional 3D molecule generation, achieving leading efficiency and quality.",
            "score": 1,
            "issue_id": 2822,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "d1787a1a75f723a2",
            "authors": [
                "Yanchen Luo",
                "Zhiyuan Liu",
                "Yi Zhao",
                "Sihang Li",
                "Kenji Kawaguchi",
                "Tat-Seng Chua",
                "Xiang Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15567.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#benchmark",
                    "#3d",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ² Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ (UAE-3D) Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 3D Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GEOM-Drugs Ğ¸ QM9 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… de novo Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»."
                },
                "en": {
                    "title": "Unified Latent Space for Efficient 3D Molecule Generation",
                    "desc": "This paper presents a new approach called Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D) aimed at generating 3D molecules for drug discovery and material science. The method addresses the challenge of integrating different types of data, such as atom types and chemical bonds, while ensuring that the 3D coordinates maintain SE(3) equivariance. By using a unified latent space, UAE-3D simplifies the process of handling multi-modalities and improves the efficiency of training and sampling. The results show that this approach outperforms existing methods in generating high-quality 3D molecules, setting new benchmarks in the field."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ½œåœ¨ç©ºé—´ï¼Œæå‡3Dåˆ†å­ç”Ÿæˆæ•ˆç‡",
                    "desc": "3Dåˆ†å­ç”Ÿæˆå¯¹è¯ç‰©å‘ç°å’Œææ–™ç§‘å­¦è‡³å…³é‡è¦ï¼Œéœ€è¦æ¨¡å‹å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŸå­ç±»å‹ã€åŒ–å­¦é”®å’Œ3Dåæ ‡ã€‚ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯å¦‚ä½•åœ¨ä¿æŒSE(3)ç­‰å˜æ€§çš„åŒæ—¶ï¼Œæ•´åˆä¸åŒå½¢çŠ¶çš„æ¨¡æ€ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆUAE-3Dï¼‰ï¼Œå®ƒå°†3Dåˆ†å­å‹ç¼©ä¸ºæ¥è‡ªç»Ÿä¸€æ½œåœ¨ç©ºé—´çš„æ½œåœ¨åºåˆ—ï¼ŒåŒæ—¶ä¿æŒè¿‘ä¹é›¶çš„é‡å»ºè¯¯å·®ã€‚é€šè¿‡ä½¿ç”¨Diffusion Transformerï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨GEOM-Drugså’ŒQM9æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ–°åˆ†å­ç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-20.html",
    "link_next": "2025-03-24.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "20.03",
        "en": "03/20",
        "zh": "3æœˆ20æ—¥"
    },
    "short_date_next": {
        "ru": "24.03",
        "en": "03/24",
        "zh": "3æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "ä¸‰è§’ç½‘æ ¼åœ¨3Dåº”ç”¨ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç”¨äºé«˜æ•ˆæ“ä½œå’Œæ¸²æŸ“ã€‚è‡ªå›å½’æ–¹æ³•é€šè¿‡é¢„æµ‹ç¦»æ•£é¡¶ç‚¹æ ‡è®°ç”Ÿæˆç»“æ„åŒ–ç½‘æ ¼ï¼Œä½†å¾€å¾€å—åˆ°æœ‰é™é¢æ•°å’Œç½‘æ ¼ä¸å®Œæ•´çš„é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†DeepMeshæ¡†æ¶ï¼Œé€šè¿‡ä¸¤é¡¹å…³é”®åˆ›æ–°ä¼˜åŒ–ç½‘æ ¼ç”Ÿæˆï¼š(1) é«˜æ•ˆçš„é¢„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…å«æ–°çš„æ ‡è®°ç®—æ³•ï¼Œä»¥åŠæ•°æ®æ•´ç†å’Œå¤„ç†çš„æ”¹è¿›ï¼›(2) å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åˆ°3Dç½‘æ ¼ç”Ÿæˆï¼Œé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å®ç°äººç±»åå¥½å¯¹é½ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç»“åˆäººç±»è¯„ä¼°å’Œ3Dåº¦é‡çš„è¯„åˆ†æ ‡å‡†ï¼Œæ”¶é›†DPOçš„åå¥½å¯¹ï¼Œç¡®ä¿è§†è§‰å¸å¼•åŠ›å’Œå‡ ä½•ç²¾åº¦ã€‚åŸºäºç‚¹äº‘å’Œå›¾åƒï¼ŒDeepMeshç”Ÿæˆå…·æœ‰å¤æ‚ç»†èŠ‚å’Œç²¾ç¡®æ‹“æ‰‘çš„ç½‘æ ¼ï¼Œåœ¨ç²¾åº¦å’Œè´¨é‡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://zhaorw02.github.io/DeepMesh/",
        "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning",
        "pinyin": "Sure, here is the pinyin transcription for the given text:\n\nSÄn jiÇo wÇng gÃ© zÃ i 3D yÃ¬ng yÃ²ng zhÅng bÃ n yÇn zhÃ²ng yÃ o jiÇo sÃ¨, yÃ²ng yÃº gÄo xiÃ o cÄo zuÃ² hÃ© xuÃ n rÃ¡n. ZÃ¬ huÃ­ guÄ« fÄng fÇ tÅng guÃ² yÃ¹ cÃ¨ lÃ¬ sÃ n dÇng biÄo jÃ¬ shÄ“ng chÃ©ng jiÃ© gÃ²u huÃ  wÇng gÃ©, dÃ n wÇng wÇng shÃ²u dÃ o yÇ’u xiÃ n miÃ n shÃ¹ hÃ© wÇng gÃ© bÃ¹ wÃ¡n zhÄ›ng de xiÃ n zhÃ¬. WÇ’ men tÃ­ chÅ« le DeepMesh kuÃ ng jiÃ , tÅng guÃ² liÇng xiÃ ng guÇn jiÃ n chuÃ ng xÄ«n yÇ’u huÃ  wÇng gÃ© shÄ“ng chÃ©ng: (1) gÄo xiÃ o de yÃ¹ xÃ¹n lÃ¼Ã¨, bÄo hÃ¡n xÄ«n de biÄo jÃ¬ suÃ n fÇ, yÇ jiÄ dÃ i shÃ¹ zhÄ›ng lÇ hÃ© chÇ” lÇ de gÇi jÃ¬n; (2) yÇn rÃ¹ qiÃ¡ng huÃ  xuÃ© xÃ­ (RL) dÃ o 3D wÇng gÃ© shÄ“ng chÃ©ng, tÅng guÃ² zhÃ­ jiÄ“ piÃ n hÃ o yÇ’u huÃ  (DPO) shÃ­ xiÃ n rÃ©n lÃ¨i piÃ n hÃ o duÃ¬ qÇ. WÇ’ men shÃ¨ jÃ¬ le yÄ« gÃ¨ jiÃ© hÃ© rÃ©n lÃ¨i pÃ­ng jiÃ  hÃ© 3D dÃ¹ liÃ ng de pÃ­ng fÄ“n biÄo zhÇ”n, shÅu jÃ­ DPO de piÃ n hÃ o duÃ¬, quÃ¨ shÃ¬ shÃ­ juÃ© xÄ« yÇn lÃ¬ hÃ© jÇ hÃ© jÄ«ng dÃ¹. JÄ« yÃº diÇn yÃºn hÃ© tÃº xiÃ ng, DeepMesh shÄ“ng chÃ©ng jÃ¹ yÇ’u fÃ¹ zÃ¡ xÃ¬ xiÃ ng hÃ© jÄ«ng xiÃ o tuÃ³ pÇ” de wÇng gÃ©, zÃ i jÄ«ng dÃ¹ hÃ© zhÃ¬ lÃ¬ shÃ ng chÄo yuÃ¨ xiÃ n yÇ’u fÄng fÇ. XiÃ ng mÃ¹ yÃ¨ miÃ n: https://zhaorw02.github.io/DeepMesh/\n\nPlease note that the URL at the end is not transcribed into pinyin as it is not Chinese text.",
        "vocab": "[\n    {\"word\": \"ä¸‰è§’ç½‘æ ¼\", \"pinyin\": \"sÄnjiÇo wÇnggÃ©\", \"trans\": \"triangular mesh\"},\n    {\"word\": \"æ‰®æ¼”\", \"pinyin\": \"bÃ nyÇn\", \"trans\": \"play a role\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄoxiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"æ¸²æŸ“\", \"pinyin\": \"xuÃ nrÃ¡n\", \"trans\": \"rendering\"},\n    {\"word\": \"è‡ªå›å½’\", \"pinyin\": \"zÃ¬ huÃ­guÄ«\", \"trans\": \"autoregressive\"},\n    {\"word\": \"é¢„æµ‹\", \"pinyin\": \"yÃ¹cÃ¨\", \"trans\": \"predict\"},\n    {\"word\": \"ç¦»æ•£\", \"pinyin\": \"lÃ­sÃ n\", \"trans\": \"discrete\"},\n    {\"word\": \"é¡¶ç‚¹\", \"pinyin\": \"dÇngdiÇn\", \"trans\": \"vertex\"},\n    {\"word\": \"æ ‡è®°\", \"pinyin\": \"biÄojÃ¬\", \"trans\": \"label\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"ç»“æ„åŒ–\", \"pinyin\": \"jiÃ©gÃ²uhuÃ \", \"trans\": \"structured\"},\n    {\"word\": \"å¾€å¾€\", \"pinyin\": \"wÇngwÇng\", \"trans\": \"often\"},\n    {\"word\": \"å—åˆ°\", \"pinyin\": \"shÃ²udÃ o\", \"trans\": \"be subject to\"},\n    {\"word\": \"æœ‰é™\", \"pinyin\": \"yÇ’uxiÃ n\", \"trans\": \"limited\"},\n    {\"word\": \"é¢æ•°\", \"pinyin\": \"miÃ nshÃ¹\", \"trans\": \"number of faces\"},\n    {\"word\": \"ä¸å®Œæ•´\", \"pinyin\": \"bÃ¹ wÃ¡nzhÄ›ng\", \"trans\": \"incomplete\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ngjiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"ä¼˜åŒ–\", \"pinyin\": \"yÅuhuÃ \", \"trans\": \"optimize\"},\n    {\"word\": \"å…³é”®\", \"pinyin\": \"guÇnjiÃ n\", \"trans\": \"key\"},\n    {\"word\": \"åˆ›æ–°\", \"pinyin\": \"chuÃ ngxÄ«n\", \"trans\": \"innovation\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹nliÃ n\", \"trans\": \"pre-training\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"åŒ…å«\", \"pinyin\": \"bÄohÃ¡n\", \"trans\": \"include\"},\n    {\"word\": \"ç®—æ³•\", \"pinyin\": \"suÃ nfÇ\", \"trans\": \"algorithm\"},\n    {\"word\": \"æ•°æ®\", \"pinyin\": \"shÃ¹jÃ¹\", \"trans\": \"data\"},\n    {\"word\": \"æ•´ç†\", \"pinyin\": \"zhÄ›nglÇ\", \"trans\": \"organize\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ”lÇ\", \"trans\": \"process\"},\n    {\"word\": \"æ”¹è¿›\", \"pinyin\": \"gÇijÃ¬n\", \"trans\": \"improve\"},\n    {\"word\": \"å¼•å…¥\", \"pinyin\": \"yÇnrÃ¹\", \"trans\": \"introduce\"},\n    {\"word\": \"å¼ºåŒ–å­¦ä¹ \", \"pinyin\": \"qiÃ¡ng huÃ  xuÃ©xÃ­\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"åå¥½\", \"pinyin\": \"piÄnhÃ o\", \"trans\": \"preference\"},\n    {\"word\": \"å¯¹é½\", \"pinyin\": \"duÃ¬qÃ­\", \"trans\": \"align\"},\n    {\"word\": \"è®¾è®¡\", \"pinyin\": \"shÃ¨jÃ¬\", \"trans\": \"design\"},\n    {\"word\": \"ç»“åˆ\", \"pinyin\": \"jiÃ©hÃ©\", \"trans\": \"combine\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­nggÅ«\", \"trans\": \"evaluate\"},\n    {\"word\": \"åº¦é‡\", \"pinyin\": \"dÃ¹liÃ ng\", \"trans\": \"measure\"},\n    {\"word\": \"è¯„åˆ†\", \"pinyin\": \"pÃ­ngfÄ“n\", \"trans\": \"score\"},\n    {\"word\": \"æ ‡å‡†\", \"pinyin\": \"biÄozhÇ”n\", \"trans\": \"standard\"},\n    {\"word\": \"æ”¶é›†\", \"pinyin\": \"shÅujÃ­\", \"trans\": \"collect\"},\n    {\"word\": \"ç¡®ä¿\", \"pinyin\": \"quÃ¨bÇo\", \"trans\": \"ensure\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"å¸å¼•åŠ›\", \"pinyin\": \"xÄ«yÇnlÃ¬\", \"trans\": \"attractiveness\"},\n    {\"word\": \"å‡ ä½•\", \"pinyin\": \"jÇhÃ©\", \"trans\": \"geometric\"},\n    {\"word\": \"ç²¾åº¦\", \"pinyin\": \"jÄ«ngdÃ¹\", \"trans\": \"precision\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ«yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"ç‚¹äº‘\", \"pinyin\": \"diÇn yÃºn\", \"trans\": \"point cloud\"},\n    {\"word\": \"å›¾åƒ\", \"pinyin\": \"tÃºxiÃ ng\", \"trans\": \"image\"},\n    {\"word\": \"å¤æ‚\", \"pinyin\": \"fÃ¹zÃ¡\", \"trans\": \"complex\"},\n    {\"word\": \"ç»†èŠ‚\", \"pinyin\": \"xÃ¬jiÃ©\", \"trans\": \"detail\"},\n    {\"word\": \"ç²¾ç¡®\", \"pinyin\": \"jÄ«ngquÃ¨\", \"trans\": \"precise\"},\n    {\"word\": \"æ‹“æ‰‘\", \"pinyin\": \"tuÃ²pÇ”\", \"trans\": \"topology\"},\n    {\"word\": \"è¶…è¶Š\", \"pinyin\": \"chÄoyuÃ¨\", \"trans\": \"surpass\"},\n    {\"word\": \"ç°æœ‰\", \"pinyin\": \"xiÃ nyÇ’u\", \"trans\": \"existing\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄngfÇ\", \"trans\": \"method\"},\n    {\"word\": \"é¡¹ç›®\", \"pinyin\": \"xiÃ ngmÃ¹\", \"trans\": \"project\"},\n    {\"word\": \"é¡µé¢\", \"pinyin\": \"yÃ¨miÃ n\", \"trans\": \"page\"}\n]",
        "trans": "Triangular meshes play a crucial role in 3D applications, enabling efficient operations and rendering. Autoregressive methods generate structured meshes by predicting discrete vertex labels but are often constrained by limited face counts and incomplete meshes. We introduce the DeepMesh framework, which optimizes mesh generation through two key innovations: (1) an efficient pre-training strategy that includes a new labeling algorithm, as well as improvements in data curation and processing; (2) the introduction of reinforcement learning (RL) into 3D mesh generation, achieving human preference alignment through direct preference optimization (DPO). We designed a scoring standard that combines human evaluation and 3D metrics to collect DPO preference pairs, ensuring visual appeal and geometric accuracy. Based on point clouds and images, DeepMesh generates meshes with complex details and precise topology, surpassing existing methods in both accuracy and quality. Project page: https://zhaorw02.github.io/DeepMesh/",
        "update_ts": "2025-03-20 09:11"
    }
}