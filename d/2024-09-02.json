{
    "date": {
        "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 2",
        "zh": "9æœˆ2æ—¥"
    },
    "time_utc": "2024-09-02 09:00",
    "weekday": 0,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-02",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2408.15545",
            "title": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding",
            "url": "https://huggingface.co/papers/2408.15545",
            "abstract": "Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery. Despite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with specialized scientific tasks.   To develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instruction-following capabilities for domain-specific tasks.cIn this process, we identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions. We address these challenges through a meticulous pipeline, including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation. Applying this strategy, we present a suite of LLMs: SciLitLLM, specialized in scientific literature understanding. These models demonstrate promising performance on scientific literature understanding benchmarks.   Our contributions are threefold: (1) We present an effective framework that integrates CPT and SFT to adapt LLMs to scientific literature understanding, which can also be easily adapted to other domains. (2) We propose an LLM-based synthesis method to generate diverse and high-quality scientific instructions, resulting in a new instruction set -- SciLitIns -- for supervised fine-tuning in less-represented scientific domains. (3) SciLitLLM achieves promising performance improvements on scientific literature understanding benchmarks.",
            "score": 34,
            "issue_id": 1,
            "pub_date": "2024-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "3a813f632e634481",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#training",
                    "#benchmark",
                    "#multilingual"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SciLitLLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ SciLitIns Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ² Ğ¼Ğ°Ğ»Ğ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Empowering LLMs for Scientific Literature Mastery",
                    "desc": "This paper addresses the challenges faced by Large Language Models (LLMs) in understanding scientific literature due to their lack of domain-specific knowledge and unfamiliarity with specialized tasks. The authors propose a hybrid approach that combines continual pre-training (CPT) and supervised fine-tuning (SFT) to enhance LLMs' capabilities in this area. They tackle the difficulties of creating high-quality training data and generating diverse instructions through a detailed pipeline that includes content extraction and error correction. The resulting model, SciLitLLM, shows significant improvements on benchmarks for scientific literature understanding, demonstrating the effectiveness of their proposed framework."
                },
                "zh": {
                    "title": "æå‡ç§‘å­¦æ–‡çŒ®ç†è§£çš„æ™ºèƒ½æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆç­–ç•¥ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç§‘å­¦æ–‡çŒ®ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„ç»“åˆï¼Œæ¨¡å‹èƒ½å¤ŸåŒæ—¶å¸æ”¶ç§‘å­¦é¢†åŸŸçŸ¥è¯†å¹¶å¢å¼ºå¯¹ç‰¹å®šä»»åŠ¡çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚æˆ‘ä»¬è¯†åˆ«äº†æ„å»ºé«˜è´¨é‡CPTè¯­æ–™åº“å’Œç”Ÿæˆå¤šæ ·åŒ–SFTæŒ‡ä»¤çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡ç²¾ç»†çš„æµç¨‹æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸“é—¨ç”¨äºç§‘å­¦æ–‡çŒ®ç†è§£çš„æ¨¡å‹SciLitLLMï¼Œå¹¶åœ¨ç›¸å…³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.17267",
            "title": "UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal Models in Multi-View Urban Scenarios",
            "url": "https://huggingface.co/papers/2408.17267",
            "abstract": "Recent evaluations of Large Multimodal Models (LMMs) have explored their capabilities in various domains, with only few benchmarks specifically focusing on urban environments. Moreover, existing urban benchmarks have been limited to evaluating LMMs with basic region-level urban tasks under singular views, leading to incomplete evaluations of LMMs' abilities in urban environments. To address these issues, we present UrBench, a comprehensive benchmark designed for evaluating LMMs in complex multi-view urban scenarios. UrBench contains 11.6K meticulously curated questions at both region-level and role-level that cover 4 task dimensions: Geo-Localization, Scene Reasoning, Scene Understanding, and Object Understanding, totaling 14 task types. In constructing UrBench, we utilize data from existing datasets and additionally collect data from 11 cities, creating new annotations using a cross-view detection-matching method. With these images and annotations, we then integrate LMM-based, rule-based, and human-based methods to construct large-scale high-quality questions. Our evaluations on 21 LMMs show that current LMMs struggle in the urban environments in several aspects. Even the best performing GPT-4o lags behind humans in most tasks, ranging from simple tasks such as counting to complex tasks such as orientation, localization and object attribute recognition, with an average performance gap of 17.4%. Our benchmark also reveals that LMMs exhibit inconsistent behaviors with different urban views, especially with respect to understanding cross-view relations. UrBench datasets and benchmark results will be publicly available at https://opendatalab.github.io/UrBench/.",
            "score": 23,
            "issue_id": 1,
            "pub_date": "2024-08-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 30",
                "zh": "8æœˆ30æ—¥"
            },
            "hash": "46054a14f1990e6c",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "UrBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ UrBench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 11.6 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 4 Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 14 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞÑ†ĞµĞ½ĞºĞ° 21 LMM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…, Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ñ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 17.4%. UrBench Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ½ĞµĞ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ LMM Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "UrBench: Elevating LMM Evaluation in Urban Landscapes",
                    "desc": "This paper introduces UrBench, a new benchmark specifically designed to evaluate Large Multimodal Models (LMMs) in urban environments. It addresses the limitations of existing benchmarks by providing a comprehensive set of 11.6K questions that assess LMMs across various task dimensions, including Geo-Localization and Scene Understanding. The authors collected data from 11 cities and used a cross-view detection-matching method to create high-quality annotations for their tasks. The evaluation results indicate that current LMMs, including the top performer GPT-4o, significantly underperform compared to human capabilities in urban scenarios, highlighting the need for improved model training in this domain."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°åŸå¸‚ç¯å¢ƒä¸­çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºUrBenchçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å¤æ‚åŸå¸‚ç¯å¢ƒä¸­çš„èƒ½åŠ›ã€‚ç°æœ‰çš„åŸå¸‚åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨å•ä¸€è§†è§’çš„åŸºæœ¬åŒºåŸŸä»»åŠ¡ï¼Œå¯¼è‡´å¯¹LMMsèƒ½åŠ›çš„è¯„ä¼°ä¸å¤Ÿå…¨é¢ã€‚UrBenchåŒ…å«11600ä¸ªç²¾å¿ƒç­–åˆ’çš„é—®é¢˜ï¼Œæ¶µç›–åœ°ç†å®šä½ã€åœºæ™¯æ¨ç†ã€åœºæ™¯ç†è§£å’Œç‰©ä½“ç†è§£ç­‰å››ä¸ªä»»åŠ¡ç»´åº¦ã€‚é€šè¿‡å¯¹11ä¸ªåŸå¸‚çš„æ•°æ®æ”¶é›†å’Œäº¤å‰è§†è§’æ£€æµ‹åŒ¹é…æ–¹æ³•çš„åº”ç”¨ï¼ŒUrBenchä¸ºLMMsæä¾›äº†æ›´å…¨é¢çš„è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå½“å‰LMMsåœ¨åŸå¸‚ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£ä¸åŒè§†è§’å…³ç³»æ–¹é¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.15914",
            "title": "CoRe: Context-Regularized Text Embedding Learning for Text-to-Image Personalization",
            "url": "https://huggingface.co/papers/2408.15914",
            "abstract": "Recent advances in text-to-image personalization have enabled high-quality and controllable image synthesis for user-provided concepts. However, existing methods still struggle to balance identity preservation with text alignment. Our approach is based on the fact that generating prompt-aligned images requires a precise semantic understanding of the prompt, which involves accurately processing the interactions between the new concept and its surrounding context tokens within the CLIP text encoder. To address this, we aim to embed the new concept properly into the input embedding space of the text encoder, allowing for seamless integration with existing tokens. We introduce Context Regularization (CoRe), which enhances the learning of the new concept's text embedding by regularizing its context tokens in the prompt. This is based on the insight that appropriate output vectors of the text encoder for the context tokens can only be achieved if the new concept's text embedding is correctly learned. CoRe can be applied to arbitrary prompts without requiring the generation of corresponding images, thus improving the generalization of the learned text embedding. Additionally, CoRe can serve as a test-time optimization technique to further enhance the generations for specific prompts. Comprehensive experiments demonstrate that our method outperforms several baseline methods in both identity preservation and text alignment. Code will be made publicly available.",
            "score": 21,
            "issue_id": 1,
            "pub_date": "2024-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "837ea41abf0b83ee",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "CoRe: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ text-to-image Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Context Regularization (CoRe), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. CoRe Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Text-to-Image Synthesis with Context Regularization",
                    "desc": "This paper presents a new method for improving text-to-image synthesis by focusing on how new concepts are integrated into existing text prompts. The authors introduce Context Regularization (CoRe), which helps the model better understand the relationships between a new concept and its context in the text. By embedding the new concept effectively within the input space of the CLIP text encoder, CoRe enhances the learning of the concept's text representation. The results show that this approach leads to better identity preservation and text alignment in generated images compared to previous methods."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸ªæ€§åŒ–ä¸å¯¹é½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒä¸ªæ€§åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç”¨æˆ·æä¾›æ¦‚å¿µçš„å›¾åƒåˆæˆè´¨é‡å’Œå¯æ§æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç²¾ç¡®ç†è§£æç¤ºçš„è¯­ä¹‰ï¼Œå¤„ç†æ–°æ¦‚å¿µä¸ä¸Šä¸‹æ–‡æ ‡è®°ä¹‹é—´çš„äº¤äº’ï¼Œæ¥å®ç°æ›´å¥½çš„å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸Šä¸‹æ–‡æ­£åˆ™åŒ–ï¼ˆCoReï¼‰ï¼Œé€šè¿‡æ­£åˆ™åŒ–æç¤ºä¸­çš„ä¸Šä¸‹æ–‡æ ‡è®°ï¼Œå¢å¼ºæ–°æ¦‚å¿µçš„æ–‡æœ¬åµŒå…¥å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨èº«ä»½ä¿ç•™å’Œæ–‡æœ¬å¯¹é½æ–¹é¢ä¼˜äºå¤šç§åŸºçº¿æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.14765",
            "title": "CrossViewDiff: A Cross-View Diffusion Model for Satellite-to-Street View Synthesis",
            "url": "https://huggingface.co/papers/2408.14765",
            "abstract": "Satellite-to-street view synthesis aims at generating a realistic street-view image from its corresponding satellite-view image. Although stable diffusion models have exhibit remarkable performance in a variety of image generation applications, their reliance on similar-view inputs to control the generated structure or texture restricts their application to the challenging cross-view synthesis task. In this work, we propose CrossViewDiff, a cross-view diffusion model for satellite-to-street view synthesis. To address the challenges posed by the large discrepancy across views, we design the satellite scene structure estimation and cross-view texture mapping modules to construct the structural and textural controls for street-view image synthesis. We further design a cross-view control guided denoising process that incorporates the above controls via an enhanced cross-view attention module. To achieve a more comprehensive evaluation of the synthesis results, we additionally design a GPT-based scoring method as a supplement to standard evaluation metrics. We also explore the effect of different data sources (e.g., text, maps, building heights, and multi-temporal satellite imagery) on this task. Results on three public cross-view datasets show that CrossViewDiff outperforms current state-of-the-art on both standard and GPT-based evaluation metrics, generating high-quality street-view panoramas with more realistic structures and textures across rural, suburban, and urban scenes. The code and models of this work will be released at https://opendatalab.github.io/CrossViewDiff/.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "2b0e5d03556e552c",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#benchmark"
                ],
                "emoji": "ğŸ›°ï¸",
                "ru": {
                    "title": "ĞÑ‚ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ° Ğº ÑƒĞ»Ğ¸Ñ†Ğµ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ CrossViewDiff",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CrossViewDiff - Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑƒĞ»Ğ¸Ñ†Ñ‹ Ğ¸Ğ· ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ğ¿Ğ¿Ğ¸Ğ½Ğ³Ğ° Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ CrossViewDiff Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼ ÑƒĞ»Ğ¸Ñ†."
                },
                "en": {
                    "title": "Bridging Views: Realistic Street-View Synthesis from Satellite Imagery",
                    "desc": "This paper introduces CrossViewDiff, a novel cross-view diffusion model designed for synthesizing street-view images from satellite-view images. The model addresses the challenges of significant differences between the two views by incorporating satellite scene structure estimation and cross-view texture mapping modules. Additionally, it employs a cross-view control guided denoising process that utilizes an enhanced attention mechanism to improve the quality of the generated images. The results demonstrate that CrossViewDiff surpasses existing methods in generating realistic street-view panoramas across various environments, supported by both traditional and GPT-based evaluation metrics."
                },
                "zh": {
                    "title": "è·¨è§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œç”ŸæˆçœŸå®è¡—æ™¯ï¼",
                    "desc": "å«æ˜Ÿåˆ°è¡—æ™¯åˆæˆæ—¨åœ¨ä»å«æ˜Ÿè§†å›¾ç”Ÿæˆé€¼çœŸçš„è¡—æ™¯å›¾åƒã€‚å°½ç®¡ç¨³å®šæ‰©æ•£æ¨¡å‹åœ¨å¤šç§å›¾åƒç”Ÿæˆåº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¯¹ç›¸ä¼¼è§†å›¾è¾“å…¥çš„ä¾èµ–é™åˆ¶äº†åœ¨è·¨è§†å›¾åˆæˆä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è§†å›¾ä¹‹é—´çš„å·¨å¤§å·®å¼‚ï¼Œæˆ‘ä»¬æå‡ºäº†CrossViewDiffæ¨¡å‹ï¼Œè®¾è®¡äº†å«æ˜Ÿåœºæ™¯ç»“æ„ä¼°è®¡å’Œè·¨è§†å›¾çº¹ç†æ˜ å°„æ¨¡å—ï¼Œä»¥æ„å»ºè¡—æ™¯å›¾åƒåˆæˆçš„ç»“æ„å’Œçº¹ç†æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCrossViewDiffåœ¨å¤šä¸ªå…¬å…±è·¨è§†å›¾æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œç”Ÿæˆäº†æ›´é«˜è´¨é‡çš„è¡—æ™¯å…¨æ™¯å›¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.17024",
            "title": "InkubaLM: A small language model for low-resource African languages",
            "url": "https://huggingface.co/papers/2408.17024",
            "abstract": "High-resource language models often fall short in the African context, where there is a critical need for models that are efficient, accessible, and locally relevant, even amidst significant computing and data constraints. This paper introduces InkubaLM, a small language model with 0.4 billion parameters, which achieves performance comparable to models with significantly larger parameter counts and more extensive training data on tasks such as machine translation, question-answering, AfriMMLU, and the AfriXnli task. Notably, InkubaLM outperforms many larger models in sentiment analysis and demonstrates remarkable consistency across multiple languages. This work represents a pivotal advancement in challenging the conventional paradigm that effective language models must rely on substantial resources. Our model and datasets are publicly available \\url{https://huggingface.co/lelapa} to encourage research and development on low-resource languages.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2024-08-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 30",
                "zh": "8æœˆ30æ—¥"
            },
            "hash": "16f2149ded7ace2d",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#translation"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ñ„Ñ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²",
                    "desc": "InkubaLM - ÑÑ‚Ğ¾ Ğ¼Ğ°Ğ»Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 0,4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ°Ñ„Ñ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ…. InkubaLM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾ÑĞ¿Ğ°Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ´ĞµÑ Ğ¾ Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¾Ğ¿Ğ¸Ñ€Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹."
                },
                "en": {
                    "title": "InkubaLM: Efficient Language Modeling for Africa",
                    "desc": "This paper presents InkubaLM, a compact language model designed specifically for the African context, which operates efficiently with only 0.4 billion parameters. Despite its smaller size, InkubaLM performs comparably to larger models on various tasks, including machine translation and question-answering, showcasing its effectiveness in low-resource settings. The model excels particularly in sentiment analysis and maintains strong performance across multiple languages, challenging the notion that high-resource models are necessary for effective language processing. By making InkubaLM and its datasets publicly available, the authors aim to foster further research and development in the area of low-resource languages."
                },
                "zh": {
                    "title": "InkubaLMï¼šå°å‹è¯­è¨€æ¨¡å‹çš„å·¨å¤§æ½œåŠ›",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†InkubaLMï¼Œè¿™æ˜¯ä¸€ç§å°å‹è¯­è¨€æ¨¡å‹ï¼Œå‚æ•°é‡ä¸º4äº¿ã€‚å°½ç®¡å‚æ•°é‡è¾ƒå°ï¼ŒInkubaLMåœ¨æœºå™¨ç¿»è¯‘ã€é—®ç­”ã€AfriMMLUå’ŒAfriXnliä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¸å¤§å‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨æƒ…æ„Ÿåˆ†ææ–¹é¢ï¼ŒInkubaLMçš„è¡¨ç°ä¼˜äºè®¸å¤šæ›´å¤§çš„æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨å¤šç§è¯­è¨€ä¸­å±•ç°å‡ºæ˜¾è‘—çš„ä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶æŒ‘æˆ˜äº†æœ‰æ•ˆè¯­è¨€æ¨¡å‹å¿…é¡»ä¾èµ–å¤§é‡èµ„æºçš„ä¼ ç»Ÿè§‚å¿µï¼Œå¹¶æä¾›äº†å…¬å¼€çš„æ•°æ®é›†ä»¥ä¿ƒè¿›ä½èµ„æºè¯­è¨€çš„ç ”ç©¶å’Œå¼€å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.17131",
            "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
            "url": "https://huggingface.co/papers/2408.17131",
            "abstract": "The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-08-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 30",
                "zh": "8æœˆ30æ—¥"
            },
            "hash": "8d6caa5c8402ee57",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ DiT Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VQ4DiT - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Diffusion Transformers (DiTs). Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ VQ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ ĞºĞ½Ğ¸Ğ³Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼. VQ4DiT Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ğ²ĞµÑĞ° Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ´Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑĞ° Ğ´Ğ¾ 2-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ² ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "VQ4DiT: Optimizing Diffusion Transformers for Efficient Image Generation",
                    "desc": "The paper introduces VQ4DiT, a novel post-training vector quantization method specifically designed for Diffusion Transformers Models (DiTs) to enhance their efficiency for edge device deployment. Traditional vector quantization methods only focus on calibrating the codebook, which can lead to suboptimal weight assignments and inconsistent gradients. VQ4DiT improves this by calculating candidate assignments based on Euclidean distance and reconstructing weight sub-vectors through a weighted average approach. The method achieves significant model size reduction while maintaining high-quality image generation, setting a new benchmark in the trade-off between model size and performance."
                },
                "zh": {
                    "title": "VQ4DiTï¼šæå‡æ‰©æ•£å˜æ¢å™¨æ¨¡å‹çš„é‡åŒ–æ•ˆç‡",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¿«é€Ÿåè®­ç»ƒå‘é‡é‡åŒ–æ–¹æ³•VQ4DiTï¼Œä¸“é—¨ç”¨äºæ‰©æ•£å˜æ¢å™¨æ¨¡å‹ï¼ˆDiTsï¼‰ã€‚ä¼ ç»Ÿçš„å‘é‡é‡åŒ–æ–¹æ³•åªæ ¡å‡†äº†ä»£ç æœ¬ï¼Œè€Œæ²¡æœ‰æ ¡å‡†åˆ†é…ï¼Œå¯¼è‡´æƒé‡å­å‘é‡è¢«é”™è¯¯åˆ†é…ï¼Œä»è€Œå½±å“äº†æ¨¡å‹æ€§èƒ½ã€‚VQ4DiTé€šè¿‡è®¡ç®—æ¯ä¸ªæƒé‡å­å‘é‡çš„å€™é€‰åˆ†é…é›†ï¼Œå¹¶åŸºäºåŠ æƒå¹³å‡é‡æ„å­å‘é‡ï¼Œæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒVQ4DiTåœ¨æ¨¡å‹å¤§å°å’Œæ€§èƒ½ä¹‹é—´å»ºç«‹äº†æ–°çš„æœ€ä½³å¹³è¡¡ï¼Œèƒ½å¤Ÿå°†æƒé‡é‡åŒ–åˆ°2ä½ç²¾åº¦ï¼ŒåŒæ—¶ä¿æŒè‰¯å¥½çš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.16444",
            "title": "SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section",
            "url": "https://huggingface.co/papers/2408.16444",
            "abstract": "Document summarization is a task to shorten texts into concise and informative summaries. This paper introduces a novel dataset designed for summarizing multiple scientific articles into a section of a survey. Our contributions are: (1) SurveySum, a new dataset addressing the gap in domain-specific summarization tools; (2) two specific pipelines to summarize scientific articles into a section of a survey; and (3) the evaluation of these pipelines using multiple metrics to compare their performance. Our results highlight the importance of high-quality retrieval stages and the impact of different configurations on the quality of generated summaries.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "1fb964fde1f5ed8a",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "SurveySum: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SurveySum Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ» Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑĞ¼Ğµ."
                },
                "en": {
                    "title": "Enhancing Scientific Summarization with SurveySum Dataset",
                    "desc": "This paper focuses on document summarization, specifically creating concise summaries from multiple scientific articles for surveys. It introduces SurveySum, a new dataset that fills a gap in tools for summarizing domain-specific content. The authors present two distinct pipelines designed to effectively summarize scientific articles into survey sections. Their evaluation of these pipelines reveals the significance of high-quality retrieval processes and how various configurations can influence the quality of the summaries produced."
                },
                "zh": {
                    "title": "æå‡ç§‘å­¦æ–‡ç« æ‘˜è¦è´¨é‡çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–‡æ¡£æ‘˜è¦çš„ä»»åŠ¡ï¼Œæ—¨åœ¨å°†æ–‡æœ¬ç¼©çŸ­ä¸ºç®€æ´ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ‘˜è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°æ•°æ®é›†SurveySumï¼Œä¸“é—¨ç”¨äºå°†å¤šç¯‡ç§‘å­¦æ–‡ç« æ€»ç»“ä¸ºè°ƒæŸ¥æŠ¥å‘Šçš„ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸¤ç§ç‰¹å®šçš„å¤„ç†æµç¨‹æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œå¹¶ä½¿ç”¨å¤šç§è¯„ä¼°æŒ‡æ ‡å¯¹å…¶æ€§èƒ½è¿›è¡Œäº†æ¯”è¾ƒã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†é«˜è´¨é‡æ£€ç´¢é˜¶æ®µçš„é‡è¦æ€§ä»¥åŠä¸åŒé…ç½®å¯¹ç”Ÿæˆæ‘˜è¦è´¨é‡çš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.14886",
            "title": "The VoxCeleb Speaker Recognition Challenge: A Retrospective",
            "url": "https://huggingface.co/papers/2408.14886",
            "abstract": "The VoxCeleb Speaker Recognition Challenges (VoxSRC) were a series of challenges and workshops that ran annually from 2019 to 2023. The challenges primarily evaluated the tasks of speaker recognition and diarisation under various settings including: closed and open training data; as well as supervised, self-supervised, and semi-supervised training for domain adaptation. The challenges also provided publicly available training and evaluation datasets for each task and setting, with new test sets released each year. In this paper, we provide a review of these challenges that covers: what they explored; the methods developed by the challenge participants and how these evolved; and also the current state of the field for speaker verification and diarisation. We chart the progress in performance over the five installments of the challenge on a common evaluation dataset and provide a detailed analysis of how each year's special focus affected participants' performance. This paper is aimed both at researchers who want an overview of the speaker recognition and diarisation field, and also at challenge organisers who want to benefit from the successes and avoid the mistakes of the VoxSRC challenges. We end with a discussion of the current strengths of the field and open challenges. Project page : https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/workshop.html",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "6a8cd96b781f16f0",
            "data": {
                "categories": [
                    "#benchmark",
                    "#audio",
                    "#survey"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "VoxSRC: Ğ¿ÑÑ‚ÑŒ Ğ»ĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ Ğ´Ğ¸Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "VoxCeleb Speaker Recognition Challenges (VoxSRC) - ÑÑ‚Ğ¾ ÑĞµÑ€Ğ¸Ñ ĞµĞ¶ĞµĞ³Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞµĞ¼Ğ¸Ğ½Ğ°Ñ€Ğ¾Ğ², Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ²ÑˆĞ¸Ñ…ÑÑ Ñ 2019 Ğ¿Ğ¾ 2023 Ğ³Ğ¾Ğ´. ĞĞ½Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ Ğ´Ğ¸Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğµ, ÑĞ°Ğ¼Ğ¾ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑÑ‚Ğ¸Ñ… ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸, Ğ¸ Ğ¸Ñ… ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Advancing Speaker Recognition: Insights from VoxSRC Challenges",
                    "desc": "The VoxCeleb Speaker Recognition Challenges (VoxSRC) were annual competitions from 2019 to 2023 that focused on improving speaker recognition and diarisation techniques. Participants used various training methods, including supervised and self-supervised learning, to adapt models to different domains. The paper reviews the evolution of methods used in these challenges and tracks performance improvements over the years using a common evaluation dataset. It serves as a resource for researchers and challenge organizers, highlighting successes, mistakes, and ongoing challenges in the field."
                },
                "zh": {
                    "title": "è¯´è¯äººè¯†åˆ«çš„è¿›æ­¥ä¸æŒ‘æˆ˜",
                    "desc": "VoxCelebè¯´è¯äººè¯†åˆ«æŒ‘æˆ˜ï¼ˆVoxSRCï¼‰æ˜¯ä¸€ä¸ªä»2019å¹´åˆ°2023å¹´æ¯å¹´ä¸¾è¡Œçš„ç³»åˆ—æŒ‘æˆ˜å’Œç ”è®¨ä¼šã€‚è¯¥æŒ‘æˆ˜ä¸»è¦è¯„ä¼°åœ¨ä¸åŒè®¾ç½®ä¸‹çš„è¯´è¯äººè¯†åˆ«å’Œåˆ†æ®µä»»åŠ¡ï¼ŒåŒ…æ‹¬å°é—­å’Œå¼€æ”¾è®­ç»ƒæ•°æ®ï¼Œä»¥åŠç›‘ç£ã€è‡ªç›‘ç£å’ŒåŠç›‘ç£è®­ç»ƒçš„é¢†åŸŸé€‚åº”ã€‚æœ¬æ–‡å›é¡¾äº†è¿™äº›æŒ‘æˆ˜ï¼Œæ¢è®¨äº†å‚ä¸è€…å¼€å‘çš„æ–¹æ³•åŠå…¶æ¼”å˜ï¼Œå¹¶åˆ†æäº†è¯´è¯äººéªŒè¯å’Œåˆ†æ®µé¢†åŸŸçš„å½“å‰çŠ¶æ€ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨äº”å±ŠæŒ‘æˆ˜ä¸­ï¼Œå‚ä¸è€…åœ¨å…±åŒè¯„ä¼°æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¿›å±•ï¼Œä»¥åŠæ¯å¹´ç‰¹åˆ«å…³æ³¨ç‚¹å¯¹å‚ä¸è€…è¡¨ç°çš„å½±å“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.16176",
            "title": "VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images",
            "url": "https://huggingface.co/papers/2408.16176",
            "abstract": "Images are increasingly becoming the currency for documenting biodiversity on the planet, providing novel opportunities for accelerating scientific discoveries in the field of organismal biology, especially with the advent of large vision-language models (VLMs). We ask if pre-trained VLMs can aid scientists in answering a range of biologically relevant questions without any additional fine-tuning. In this paper, we evaluate the effectiveness of 12 state-of-the-art (SOTA) VLMs in the field of organismal biology using a novel dataset, VLM4Bio, consisting of 469K question-answer pairs involving 30K images from three groups of organisms: fishes, birds, and butterflies, covering five biologically relevant tasks. We also explore the effects of applying prompting techniques and tests for reasoning hallucination on the performance of VLMs, shedding new light on the capabilities of current SOTA VLMs in answering biologically relevant questions using images. The code and datasets for running all the analyses reported in this paper can be found at https://github.com/sammarfy/VLM4Bio.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2024-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "d0a805442038e4da",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#multimodal",
                    "#reasoning",
                    "#hallucinations"
                ],
                "emoji": "ğŸ¦‹",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ±Ğ¸Ğ¾Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ 12 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLM Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VLM4Bio, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ 469 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ´Ğ»Ñ 30 Ñ‚Ñ‹ÑÑÑ‡ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ñ‹Ğ±, Ğ¿Ñ‚Ğ¸Ñ† Ğ¸ Ğ±Ğ°Ğ±Ğ¾Ñ‡ĞµĞº. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿ÑÑ‚ÑŒ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ²ĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLM Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Unlocking Biodiversity Insights with Vision-Language Models",
                    "desc": "This paper investigates the use of pre-trained vision-language models (VLMs) to assist scientists in answering biological questions without needing further training. It evaluates 12 state-of-the-art VLMs on a new dataset called VLM4Bio, which includes 469,000 question-answer pairs related to images of fishes, birds, and butterflies. The study examines how different prompting techniques and reasoning tests affect the models' performance in biological contexts. The findings highlight the potential of VLMs to enhance research in organismal biology by leveraging large datasets of images and questions."
                },
                "zh": {
                    "title": "åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹åŠ é€Ÿç”Ÿç‰©å¤šæ ·æ€§ç ”ç©¶",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç”Ÿç‰©å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å›ç­”ä¸ç”Ÿç‰©ç›¸å…³çš„é—®é¢˜æ–¹é¢ã€‚æˆ‘ä»¬è¯„ä¼°äº†12ç§æœ€å…ˆè¿›çš„VLMåœ¨å¤„ç†åŒ…å«469Ké—®ç­”å¯¹å’Œ30Kå›¾åƒçš„æ•°æ®é›†VLM4Bioä¸­çš„è¡¨ç°ã€‚ç ”ç©¶æ¶µç›–äº†é±¼ç±»ã€é¸Ÿç±»å’Œè´è¶ä¸‰ç»„ç”Ÿç‰©ï¼Œæ¶‰åŠäº”ä¸ªç”Ÿç‰©å­¦ç›¸å…³ä»»åŠ¡ã€‚é€šè¿‡åº”ç”¨æç¤ºæŠ€æœ¯å’Œæ¨ç†å¹»è§‰æµ‹è¯•ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å½“å‰VLMåœ¨åˆ©ç”¨å›¾åƒå›ç­”ç”Ÿç‰©é—®é¢˜æ—¶çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.15993",
            "title": "ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution",
            "url": "https://huggingface.co/papers/2408.15993",
            "abstract": "Detecting and attributing temperature increases due to climate change is crucial for understanding global warming and guiding adaptation strategies. The complexity of distinguishing human-induced climate signals from natural variability has challenged traditional detection and attribution (D&A) approaches, which seek to identify specific \"fingerprints\" in climate response variables. Deep learning offers potential for discerning these complex patterns in expansive spatial datasets. However, lack of standard protocols has hindered consistent comparisons across studies. We introduce ClimDetect, a standardized dataset of over 816k daily climate snapshots, designed to enhance model accuracy in identifying climate change signals. ClimDetect integrates various input and target variables used in past research, ensuring comparability and consistency. We also explore the application of vision transformers (ViT) to climate data, a novel and modernizing approach in this context. Our open-access data and code serve as a benchmark for advancing climate science through improved model evaluations. ClimDetect is publicly accessible via Huggingface dataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2024-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "75efd489e981a0a5",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸŒ¡ï¸",
                "ru": {
                    "title": "ClimDetect: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "ClimDetect - ÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 816 Ñ‚Ñ‹ÑÑÑ‡ ĞµĞ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ñ… ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚Ğ°. ĞĞ½ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ² Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ vision transformers (ViT) Ğº ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ClimDetect ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "ClimDetect: Standardizing Climate Change Detection with Deep Learning",
                    "desc": "This paper presents ClimDetect, a new standardized dataset aimed at improving the detection and attribution of climate change signals. It addresses the challenges of distinguishing human-induced climate changes from natural variability by providing over 816,000 daily climate snapshots. The study also explores the use of vision transformers (ViT) to analyze climate data, which represents a modern approach to understanding complex climate patterns. By offering open-access data and code, ClimDetect aims to enhance model evaluations and foster advancements in climate science."
                },
                "zh": {
                    "title": "ClimDetectï¼šæå‡æ°”å€™å˜åŒ–ä¿¡å·è¯†åˆ«çš„æ ‡å‡†åŒ–æ•°æ®é›†",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ClimDetectï¼Œè¿™æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡816,000ä¸ªæ¯æ—¥æ°”å€™å¿«ç…§ï¼Œæ—¨åœ¨æé«˜æ°”å€™å˜åŒ–ä¿¡å·è¯†åˆ«çš„æ¨¡å‹å‡†ç¡®æ€§ã€‚ä¼ ç»Ÿçš„æ£€æµ‹å’Œå½’å› æ–¹æ³•åœ¨åŒºåˆ†äººç±»å¼•èµ·çš„æ°”å€™ä¿¡å·ä¸è‡ªç„¶å˜å¼‚æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œè€Œæ·±åº¦å­¦ä¹ èƒ½å¤Ÿå¸®åŠ©è¯†åˆ«è¿™äº›å¤æ‚æ¨¡å¼ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰åœ¨æ°”å€™æ•°æ®ä¸­çš„åº”ç”¨ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ç°ä»£åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„å¼€æ”¾è®¿é—®æ•°æ®å’Œä»£ç ä¸ºæ°”å€™ç§‘å­¦çš„è¿›æ­¥æä¾›äº†åŸºå‡†ï¼Œä¿ƒè¿›äº†æ¨¡å‹è¯„ä¼°çš„æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.14572",
            "title": "CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation",
            "url": "https://huggingface.co/papers/2408.14572",
            "abstract": "This paper introduces CURLoRA, a novel approach to fine-tuning large language models (LLMs) that leverages CUR matrix decomposition in the context of Low-Rank Adaptation (LoRA). Our method addresses two critical challenges in LLM fine-tuning: mitigating catastrophic forgetting during continual learning and reducing the number of trainable parameters. We propose a unique modification to the CUR decomposition process, utilizing inverted probabilities for column and row selection which acts as an implicit regularization, and initializing the U matrix as a zero matrix, and only fine-tuning it. We demonstrate through experiments on multiple datasets that CURLoRA outperforms standard LoRA in mitigating catastrophic forgetting. It maintains model stability and performance across tasks while significantly reducing the number of trainable parameters. Our results show that CURLoRA achieves very good and stable task accuracy while maintaining base model's perplexity scores fixed compared to LoRA upon continual fine-tuning, particularly in scenarios with limited data.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2024-08-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 26",
                "zh": "8æœˆ26æ—¥"
            },
            "hash": "7e1ffebe22276d0d",
            "data": {
                "categories": [
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CURLoRA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CURLoRA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ CUR-Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Low-Rank Adaptation (LoRA). ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². CURLoRA Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ CUR-Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ½ĞµÑĞ²Ğ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CURLoRA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ LoRA Ğ² ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "CURLoRA: Fine-Tuning LLMs with Stability and Efficiency",
                    "desc": "CURLoRA is a new method for fine-tuning large language models that uses CUR matrix decomposition within the Low-Rank Adaptation framework. It tackles the problems of catastrophic forgetting during continual learning and the need to reduce the number of parameters that need to be trained. The approach modifies the CUR decomposition by using inverted probabilities for selecting rows and columns, which helps in regularization, and initializes the U matrix as a zero matrix to focus on fine-tuning. Experiments show that CURLoRA not only outperforms traditional LoRA in maintaining task accuracy but also keeps the model's perplexity scores stable, especially when data is limited."
                },
                "zh": {
                    "title": "CURLoRAï¼šå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCURLoRAçš„æ–°æ–¹æ³•ï¼Œç”¨äºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨CURçŸ©é˜µåˆ†è§£å’Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ã€‚CURLoRAè§£å†³äº†LLMå¾®è°ƒä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šåœ¨æŒç»­å­¦ä¹ ä¸­å‡è½»ç¾éš¾æ€§é—å¿˜å’Œå‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚æˆ‘ä»¬å¯¹CURåˆ†è§£è¿‡ç¨‹è¿›è¡Œäº†ç‹¬ç‰¹çš„ä¿®æ”¹ï¼Œä½¿ç”¨åå‘æ¦‚ç‡è¿›è¡Œåˆ—å’Œè¡Œé€‰æ‹©ï¼Œä½œä¸ºéšå¼æ­£åˆ™åŒ–ï¼Œå¹¶å°†UçŸ©é˜µåˆå§‹åŒ–ä¸ºé›¶çŸ©é˜µï¼Œä»…å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCURLoRAåœ¨å‡è½»ç¾éš¾æ€§é—å¿˜æ–¹é¢ä¼˜äºæ ‡å‡†çš„LoRAï¼ŒåŒæ—¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­ä¿æŒæ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.16672",
            "title": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever",
            "url": "https://huggingface.co/papers/2408.16672",
            "abstract": "Multi-vector dense models, such as ColBERT, have proven highly effective in information retrieval. ColBERT's late interaction scoring approximates the joint query-document attention seen in cross-encoders while maintaining inference efficiency closer to traditional dense retrieval models, thanks to its bi-encoder architecture and recent optimizations in indexing and search. In this paper, we introduce several improvements to the ColBERT model architecture and training pipeline, leveraging techniques successful in the more established single-vector embedding model paradigm, particularly those suited for heterogeneous multilingual data. Our new model, Jina-ColBERT-v2, demonstrates strong performance across a range of English and multilingual retrieval tasks, while also cutting storage requirements by up to 50% compared to previous models.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2024-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "de191a3e234adaf4",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Jina-ColBERT-v2: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Jina-ColBERT-v2 Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ ColBERT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ĞºÑ€Ğ¾ÑÑ-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ´Ğ½Ğ¾Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ñƒ Ğ´Ğ¾ 50%."
                },
                "en": {
                    "title": "Efficient Multilingual Retrieval with Jina-ColBERT-v2",
                    "desc": "This paper presents Jina-ColBERT-v2, an enhanced version of the ColBERT model designed for information retrieval. It combines the efficiency of bi-encoder architectures with improvements from single-vector embedding techniques, particularly for multilingual data. The model achieves high performance in various retrieval tasks while significantly reducing storage needs by up to 50%. These advancements make Jina-ColBERT-v2 a competitive option for both English and multilingual information retrieval applications."
                },
                "zh": {
                    "title": "æå‡ä¿¡æ¯æ£€ç´¢æ•ˆç‡çš„æ–°æ¨¡å‹",
                    "desc": "å¤šå‘é‡å¯†é›†æ¨¡å‹ï¼Œå¦‚ColBERTï¼Œåœ¨ä¿¡æ¯æ£€ç´¢ä¸­è¡¨ç°å‡ºè‰²ã€‚ColBERTé€šè¿‡åæœŸäº¤äº’è¯„åˆ†ï¼Œè¿‘ä¼¼äºäº¤å‰ç¼–ç å™¨ä¸­çš„è”åˆæŸ¥è¯¢-æ–‡æ¡£æ³¨æ„åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†æ¥è¿‘ä¼ ç»Ÿå¯†é›†æ£€ç´¢æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚æœ¬æ–‡ä»‹ç»äº†å¯¹ColBERTæ¨¡å‹æ¶æ„å’Œè®­ç»ƒæµç¨‹çš„å¤šé¡¹æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¼‚æ„å¤šè¯­è¨€æ•°æ®çš„æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æ–°æ¨¡å‹Jina-ColBERT-v2åœ¨å¤šç§è‹±è¯­å’Œå¤šè¯­è¨€æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å¼ºåŠ²ï¼ŒåŒæ—¶ç›¸æ¯”äºä¹‹å‰çš„æ¨¡å‹ï¼Œå­˜å‚¨éœ€æ±‚å‡å°‘äº†å¤šè¾¾50%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.15827",
            "title": "Automatic Differential Diagnosis using Transformer-Based Multi-Label Sequence Classification",
            "url": "https://huggingface.co/papers/2408.15827",
            "abstract": "As the field of artificial intelligence progresses, assistive technologies are becoming more widely used across all industries. The healthcare industry is no different, with numerous studies being done to develop assistive tools for healthcare professionals. Automatic diagnostic systems are one such beneficial tool that can assist with a variety of tasks, including collecting patient information, analyzing test results, and diagnosing patients. However, the idea of developing systems that can provide a differential diagnosis has been largely overlooked in most of these research studies. In this study, we propose a transformer-based approach for providing differential diagnoses based on a patient's age, sex, medical history, and symptoms. We use the DDXPlus dataset, which provides differential diagnosis information for patients based on 49 disease types. Firstly, we propose a method to process the tabular patient data from the dataset and engineer them into patient reports to make them suitable for our research. In addition, we introduce two data modification modules to diversify the training data and consequently improve the robustness of the models. We approach the task as a multi-label classification problem and conduct extensive experiments using four transformer models. All the models displayed promising results by achieving over 97% F1 score on the held-out test set. Moreover, we design additional behavioral tests to get a broader understanding of the models. In particular, for one of our test cases, we prepared a custom test set of 100 samples with the assistance of a doctor. The results on the custom set showed that our proposed data modification modules improved the model's generalization capabilities. We hope our findings will provide future researchers with valuable insights and inspire them to develop reliable systems for automatic differential diagnosis.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2024-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "4341ae6b1eb888e2",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#medicine",
                    "#training"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğµ. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DDXPlus Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€ÑŒĞ¼Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ F1-Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 97% Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 100 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ñ€Ğ°Ñ‡Ğ°, Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Transforming Healthcare: Accurate Differential Diagnosis with AI",
                    "desc": "This paper presents a novel transformer-based approach for automatic differential diagnosis in healthcare, addressing a gap in existing assistive technologies. The authors utilize the DDXPlus dataset to train models that analyze patient data, including age, sex, medical history, and symptoms, to generate differential diagnoses. They introduce data modification techniques to enhance the diversity of training data, which improves model robustness. The results demonstrate that their models achieve over 97% F1 score, indicating high accuracy, and the study aims to inspire further research in reliable diagnostic systems."
                },
                "zh": {
                    "title": "æ™ºèƒ½è¾…åŠ©è¯Šæ–­ï¼ŒåŠ©åŠ›åŒ»ç–—å†³ç­–",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå˜æ¢å™¨çš„è‡ªåŠ¨åŒ–è¾…åŠ©è¯Šæ–­ç³»ç»Ÿï¼Œæ—¨åœ¨æ ¹æ®æ‚£è€…çš„å¹´é¾„ã€æ€§åˆ«ã€ç—…å²å’Œç—‡çŠ¶æä¾›é‰´åˆ«è¯Šæ–­ã€‚æˆ‘ä»¬ä½¿ç”¨DDXPlusæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«49ç§ç–¾ç—…ç±»å‹çš„é‰´åˆ«è¯Šæ–­ä¿¡æ¯ã€‚é€šè¿‡å¯¹æ‚£è€…æ•°æ®è¿›è¡Œå¤„ç†å’Œå·¥ç¨‹åŒ–ï¼Œæˆ‘ä»¬æ„å»ºäº†é€‚åˆç ”ç©¶çš„æ‚£è€…æŠ¥å‘Šï¼Œå¹¶å¼•å…¥äº†æ•°æ®ä¿®æ”¹æ¨¡å—ä»¥å¢å¼ºè®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†è¶…è¿‡97%çš„F1åˆ†æ•°ï¼Œè¡¨æ˜å…¶åœ¨è‡ªåŠ¨åŒ–é‰´åˆ«è¯Šæ–­ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.16245",
            "title": "Large-Scale Multi-omic Biosequence Transformers for Modeling Peptide-Nucleotide Interactions",
            "url": "https://huggingface.co/papers/2408.16245",
            "abstract": "The transformer architecture has revolutionized bioinformatics and driven progress in the understanding and prediction of the properties of biomolecules. Almost all research on large-scale biosequence transformers has focused on one domain at a time (single-omic), usually nucleotides or peptides. These models have seen incredible success in downstream tasks in each domain and have achieved particularly noteworthy breakthroughs in sequences of peptides and structural modeling. However, these single-omic models are naturally incapable of modeling multi-omic tasks, one of the most biologically critical being nucleotide-peptide interactions.   We present our work training the first multi-omic nucleotide-peptide foundation models. We show that these multi-omic models (MOMs) can learn joint representations between various single-omic distributions that are emergently consistent with the Central Dogma of molecular biology, despite only being trained on unlabeled biosequences. We further demonstrate that MOMs can be fine-tuned to achieve state-of-the-art results on peptide-nucleotide interaction tasks, namely predicting the change in Gibbs free energy ({\\Delta}G) of the binding interaction between a given oligonucleotide and peptide, as well as the effect on this binding interaction due to mutations in the oligonucleotide sequence ({\\Delta}{\\Delta}G).   Remarkably, we show that multi-omic biosequence transformers emergently learn useful structural information without any prior structural training, allowing us to predict which peptide residues are most involved in the peptide-nucleotide binding interaction. Lastly, we provide evidence that multi-omic biosequence models are non-inferior to foundation models trained on single-omics distributions, suggesting a more generalized or foundational approach to building these models.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2024-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "e76539fcf9b40424",
            "data": {
                "categories": [
                    "#architecture",
                    "#medicine",
                    "#training",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¸Ğ¾Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½ÑƒĞºĞ»ĞµĞ¾Ñ‚Ğ¸Ğ´Ğ¾Ğ² Ğ¸ Ğ¿ĞµĞ¿Ñ‚Ğ¸Ğ´Ğ¾Ğ². Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ½ĞµĞ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¦ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ³Ğ¼Ğµ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞŸĞ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿ĞµĞ¿Ñ‚Ğ¸Ğ´Ğ¾Ğ² Ğ¸ Ğ½ÑƒĞºĞ»ĞµĞ¾Ñ‚Ğ¸Ğ´Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ“Ğ¸Ğ±Ğ±ÑĞ° Ğ¿Ñ€Ğ¸ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Bioinformatics with Multi-Omic Models",
                    "desc": "This paper introduces a novel multi-omic model (MOM) that integrates nucleotide and peptide data to enhance the understanding of their interactions. Unlike traditional single-omic models, which focus on one type of biological sequence, MOMs can learn joint representations from both nucleotides and peptides, aligning with the Central Dogma of molecular biology. The authors demonstrate that these models can predict the Gibbs free energy changes in binding interactions and the effects of mutations, achieving state-of-the-art results in these tasks. Additionally, the study reveals that MOMs can extract structural information without prior training, indicating their potential as foundational models in bioinformatics."
                },
                "zh": {
                    "title": "å¤šç»„å­¦æ¨¡å‹ï¼šç”Ÿç‰©ä¿¡æ¯å­¦çš„æ–°çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šç»„å­¦æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†æ ¸è‹·é…¸å’Œè‚½çš„ç›¸äº’ä½œç”¨ã€‚ä¸ä»¥å¾€åªå…³æ³¨å•ä¸€ç»„å­¦çš„æ¨¡å‹ä¸åŒï¼Œè¿™äº›å¤šç»„å­¦æ¨¡å‹å¯ä»¥å­¦ä¹ ä¸åŒç»„å­¦ä¹‹é—´çš„è”åˆè¡¨ç¤ºï¼Œå¹¶ä¸åˆ†å­ç”Ÿç‰©å­¦çš„ä¸­å¿ƒæ³•åˆ™ä¸€è‡´ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨é¢„æµ‹æ ¸è‹·é…¸ä¸è‚½çš„ç»“åˆè‡ªç”±èƒ½å˜åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³åœ¨æ²¡æœ‰ç»“æ„è®­ç»ƒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å­¦ä¹ åˆ°æœ‰ç”¨çš„ç»“æ„ä¿¡æ¯ã€‚æœ€ç»ˆï¼Œç»“æœæ˜¾ç¤ºå¤šç»„å­¦æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸é€Šè‰²äºå•ç»„å­¦æ¨¡å‹ï¼Œè¡¨æ˜å®ƒä»¬åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.15300",
            "title": "GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs",
            "url": "https://huggingface.co/papers/2408.15300",
            "abstract": "Parameter Efficient Fine-Tuning (PEFT) methods have gained popularity and democratized the usage of Large Language Models (LLMs). Recent studies have shown that a small subset of weights significantly impacts performance. Based on this observation, we introduce a novel PEFT method, called Gaussian noise Injected Fine Tuning of Salient Weights (GIFT-SW). Our method updates only salient columns, while injecting Gaussian noise into non-salient ones. To identify these columns, we developeda generalized sensitivity metric that extends and unifies metrics from previous studies. Experiments with LLaMA models demonstrate that GIFT-SW outperforms full fine-tuning and modern PEFT methods under the same computational budget. Moreover, GIFT-SW offers practical advantages to recover performance of models subjected to mixed-precision quantization with keeping salient weights in full precision.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2024-08-27",
            "pub_date_card": {
                "ru": "27 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 27",
                "zh": "8æœˆ27æ—¥"
            },
            "hash": "97d0a5f80506ca74",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GIFT-SW. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ ÑˆÑƒĞ¼ Ğ² Ğ¼ĞµĞ½ĞµĞµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ GIFT-SW Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ PEFT Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ."
                },
                "en": {
                    "title": "Efficient Fine-Tuning with GIFT-SW: Focus on What Matters!",
                    "desc": "This paper presents a new method called Gaussian noise Injected Fine Tuning of Salient Weights (GIFT-SW) for efficiently fine-tuning Large Language Models (LLMs). The method focuses on updating only the most important weights, known as salient columns, while adding Gaussian noise to the less important weights. A new sensitivity metric is introduced to identify these salient weights, improving upon previous metrics. Experiments show that GIFT-SW not only outperforms traditional fine-tuning methods but also maintains model performance when using mixed-precision quantization."
                },
                "zh": {
                    "title": "æ˜¾è‘—æƒé‡å¾®è°ƒï¼šé«˜æ•ˆä¸æ€§èƒ½çš„å®Œç¾ç»“åˆ",
                    "desc": "å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä½¿ç”¨ä¸­å˜å¾—è¶Šæ¥è¶Šæµè¡Œã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°‘é‡é‡è¦çš„æƒé‡å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„PEFTæ–¹æ³•ï¼Œç§°ä¸ºæ˜¾è‘—æƒé‡çš„é«˜æ–¯å™ªå£°æ³¨å…¥å¾®è°ƒï¼ˆGIFT-SWï¼‰ï¼Œè¯¥æ–¹æ³•ä»…æ›´æ–°æ˜¾è‘—åˆ—ï¼ŒåŒæ—¶å¯¹éæ˜¾è‘—åˆ—æ³¨å…¥é«˜æ–¯å™ªå£°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGIFT-SWåœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹ä¼˜äºå®Œå…¨å¾®è°ƒå’Œç°ä»£PEFTæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ··åˆç²¾åº¦é‡åŒ–çš„æƒ…å†µä¸‹èƒ½å¤Ÿæœ‰æ•ˆæ¢å¤æ¨¡å‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2408.16667",
            "title": "Iterative Graph Alignment",
            "url": "https://huggingface.co/papers/2408.16667",
            "abstract": "By compressing diverse narratives, LLMs go beyond memorization, achieving intelligence by capturing generalizable causal relationships. However, they suffer from local 'representation gaps' due to insufficient training data diversity, limiting their real-world utility, especially in tasks requiring strict alignment to rules. Traditional alignment methods relying on heavy human annotations are inefficient and unscalable. Recent self-alignment techniques also fall short, as they often depend on self-selection based prompting and memorization-based learning. To address these issues, we introduce Iterative Graph Alignment (IGA), an annotation-free rule-based alignment algorithm. A teacher model (VLM) employs Iterative Graph Prompting (IGP) to create logical graphs and reference answers. The student model (LLM) identifies local knowledge gaps by attempting to align its responses with these references, collaborating with helper models to generate diverse answers. These aligned responses are then used for iterative supervised fine-tuning (SFT). Our evaluations across five rule-based scenarios demonstrate IGP's effectiveness, with a 73.12\\% alignment improvement in Claude Sonnet 3.5, and Llama3-8B-Instruct achieving an 86.20\\% improvement, outperforming Claude Sonnet 3.5 in rule-based alignment.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2024-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "a3fdd3c0b33eb72a",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Iterative Graph Alignment (IGA). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. IGA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»."
                },
                "en": {
                    "title": "Bridging Gaps in Rule-Based Alignment with Iterative Graphs",
                    "desc": "This paper discusses the limitations of large language models (LLMs) in understanding and applying rules due to local representation gaps caused by a lack of diverse training data. It introduces Iterative Graph Alignment (IGA), a new method that does not require human annotations for aligning LLMs with rule-based tasks. The approach uses a teacher model to create logical graphs and reference answers, allowing the student model to identify and fill knowledge gaps. The results show significant improvements in alignment performance across various scenarios, demonstrating the effectiveness of the proposed method."
                },
                "zh": {
                    "title": "è¿­ä»£å›¾å¯¹é½ï¼šæå‡è¯­è¨€æ¨¡å‹çš„è§„åˆ™å¯¹é½èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— æ³¨é‡Šè§„åˆ™å¯¹é½ç®—æ³•ï¼Œç§°ä¸ºè¿­ä»£å›¾å¯¹é½ï¼ˆIGAï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒæ•°æ®å¤šæ ·æ€§ä¸è¶³æ—¶å‡ºç°çš„å±€éƒ¨è¡¨ç¤ºå·®è·é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨æ•™å¸ˆæ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆé€»è¾‘å›¾å’Œå‚è€ƒç­”æ¡ˆï¼Œå­¦ç”Ÿæ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿè¯†åˆ«å…¶å“åº”ä¸­çš„çŸ¥è¯†ç¼ºå£ï¼Œå¹¶ä¸è¾…åŠ©æ¨¡å‹åˆä½œç”Ÿæˆå¤šæ ·åŒ–çš„ç­”æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥æé«˜æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIGAåœ¨å¤šä¸ªåŸºäºè§„åˆ™çš„åœºæ™¯ä¸­æ˜¾è‘—æé«˜äº†å¯¹é½æ•ˆæœï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-08-30.html",
    "link_next": "2024-09-03.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "30.08",
        "en": "08/30",
        "zh": "8æœˆ30æ—¥"
    },
    "short_date_next": {
        "ru": "03.09",
        "en": "09/03",
        "zh": "9æœˆ3æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 3,
        "#benchmark": 6,
        "#agents": 0,
        "#cv": 5,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 3,
        "#architecture": 3,
        "#medicine": 2,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 2,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 1
    }
}