{
    "date": {
        "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 23",
        "zh": "9æœˆ23æ—¥"
    },
    "time_utc": "2025-09-23 07:11",
    "weekday": 1,
    "issue_id": 6034,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.17567",
            "title": "LIMI: Less is More for Agency",
            "url": "https://huggingface.co/papers/2509.17567",
            "abstract": "LIMI demonstrates that sophisticated agentic intelligence can emerge from minimal, strategically curated demonstrations, outperforming data-intensive models on agency benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t We define Agency as the emergent capacity of AI systems to function as autonomous agents actively discovering problems, formulating hypotheses, and executing solutions through self-directed engagement with environments and tools. This fundamental capability marks the dawn of the Age of AI Agency, driven by a critical industry shift: the urgent need for AI systems that don't just think, but work. While current AI excels at reasoning and generating responses, industries demand autonomous agents that can execute tasks, operate tools, and drive real-world outcomes. As agentic intelligence becomes the defining characteristic separating cognitive systems from productive workers, efficiently cultivating machine autonomy becomes paramount. Current approaches assume that more data yields better agency, following traditional scaling laws from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is More for Intelligent Agency) demonstrates that agency follows radically different development principles. Through strategic focus on collaborative software development and scientific research workflows, we show that sophisticated agentic intelligence can emerge from minimal but strategically curated demonstrations of autonomous behavior. Using only 78 carefully designed training samples, LIMI achieves 73.5% on comprehensive agency benchmarks, dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%), DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%). Most strikingly, LIMI demonstrates 53.7% improvement over models trained on 10,000 samples-achieving superior agentic intelligence with 128 times fewer samples. Our findings establish the Agency Efficiency Principle: machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.",
            "score": 40,
            "issue_id": 6032,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "abed9c1916ee6dc8",
            "authors": [
                "Yang Xiao",
                "Mohan Jiang",
                "Jie Sun",
                "Keyu Li",
                "Jifan Lin",
                "Yumin Zhuang",
                "Ji Zeng",
                "Shijie Xia",
                "Qishuo Hua",
                "Xuefeng Li",
                "Xiaojie Cai",
                "Tongyu Wang",
                "Yue Zhang",
                "Liming Liu",
                "Xia Wu",
                "Jinlong Hou",
                "Yuan Cheng",
                "Wenjie Li",
                "Xiang Wang",
                "Dequan Wang",
                "Pengfei Liu"
            ],
            "affiliations": [
                "GAIR",
                "PolyU",
                "SII",
                "SJTU",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17567.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LIMI (Less Is More for Intelligent Agency). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, LIMI Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½ÑƒÑ‚ÑŒ Ğ¸Ğ· Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾, Ğ½Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 78 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², LIMI Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 73.5% Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ĞŸÑ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ½Ğµ Ğ¸Ğ· Ğ¾Ğ±Ğ¸Ğ»Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ğ¸Ğ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Less Data, More Agency: Redefining AI Intelligence",
                    "desc": "LIMI introduces a new approach to developing agentic intelligence in AI systems, showing that high-quality, strategically curated demonstrations can lead to better performance than traditional data-intensive methods. The paper defines agency as the ability of AI to autonomously identify problems, create hypotheses, and implement solutions. By using only 78 carefully selected training samples, LIMI significantly outperforms existing models that rely on larger datasets, achieving a remarkable 73.5% on agency benchmarks. This research challenges the conventional belief that more data is always better, establishing the Agency Efficiency Principle, which emphasizes the importance of quality over quantity in training AI for autonomous tasks."
                },
                "zh": {
                    "title": "å°‘å³æ˜¯å¤šï¼šè‡ªä¸»æ™ºèƒ½çš„æ–°èŒƒå¼",
                    "desc": "LIMIå±•ç¤ºäº†å¤æ‚çš„è‡ªä¸»æ™ºèƒ½å¯ä»¥é€šè¿‡æœ€å°åŒ–ã€æˆ˜ç•¥æ€§ç­–åˆ’çš„ç¤ºèŒƒè€Œå‡ºç°ï¼Œè¶…è¶Šäº†æ•°æ®å¯†é›†å‹æ¨¡å‹åœ¨è‡ªä¸»æ€§åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å°†è‡ªä¸»æ€§å®šä¹‰ä¸ºäººå·¥æ™ºèƒ½ç³»ç»Ÿä½œä¸ºè‡ªä¸»ä»£ç†çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸»åŠ¨å‘ç°é—®é¢˜ã€åˆ¶å®šå‡è®¾å¹¶é€šè¿‡è‡ªæˆ‘å¼•å¯¼ä¸ç¯å¢ƒå’Œå·¥å…·çš„äº’åŠ¨æ¥æ‰§è¡Œè§£å†³æ–¹æ¡ˆã€‚å½“å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œæœºå™¨è‡ªä¸»æ€§å¹¶éæ¥è‡ªæ•°æ®çš„ä¸°å¯Œï¼Œè€Œæ˜¯æ¥è‡ªé«˜è´¨é‡è‡ªä¸»è¡Œä¸ºç¤ºèŒƒçš„æˆ˜ç•¥æ€§ç­–åˆ’ã€‚é€šè¿‡ä»…ä½¿ç”¨78ä¸ªç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒæ ·æœ¬ï¼ŒLIMIåœ¨å…¨é¢çš„è‡ªä¸»æ€§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†73.5%çš„æˆç»©ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17627",
            "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion\n  Transformer Models",
            "url": "https://huggingface.co/papers/2509.17627",
            "abstract": "OmniInsert addresses challenges in mask-free video insertion using a novel data pipeline, feature injection, progressive training, and context-aware rephrasing, outperforming commercial solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video insertion based on diffusion models are impressive. However, existing methods rely on complex control signals but struggle with subject consistency, limiting their practical applicability. In this paper, we focus on the task of Mask-free Video Insertion and aim to resolve three key challenges: data scarcity, subject-scene equilibrium, and insertion harmonization. To address the data scarcity, we propose a new data pipeline InsertPipe, constructing diverse cross-pair data automatically. Building upon our data pipeline, we develop OmniInsert, a novel unified framework for mask-free video insertion from both single and multiple subject references. Specifically, to maintain subject-scene equilibrium, we introduce a simple yet effective Condition-Specific Feature Injection mechanism to distinctly inject multi-source conditions and propose a novel Progressive Training strategy that enables the model to balance feature injection from subjects and source video. Meanwhile, we design the Subject-Focused Loss to improve the detailed appearance of the subjects. To further enhance insertion harmonization, we propose an Insertive Preference Optimization methodology to optimize the model by simulating human preferences, and incorporate a Context-Aware Rephraser module during reference to seamlessly integrate the subject into the original scenes. To address the lack of a benchmark for the field, we introduce InsertBench, a comprehensive benchmark comprising diverse scenes with meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert outperforms state-of-the-art closed-source commercial solutions. The code will be released.",
            "score": 37,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "15e2a15d37cb464c",
            "authors": [
                "Jinshu Chen",
                "Xinghui Li",
                "Xu Bai",
                "Tianxiang Ma",
                "Pengze Zhang",
                "Zhuowei Chen",
                "Gen Li",
                "Lijie Liu",
                "Songtao Zhao",
                "Bingchuan Li",
                "Qian He"
            ],
            "affiliations": [
                "Intelligent Creation Lab, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17627.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#benchmark",
                    "#optimization",
                    "#video",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¼Ğ°ÑĞ¾Ğº",
                    "desc": "OmniInsert - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼ Ğ¸ ÑÑ†ĞµĞ½Ğ¾Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. OmniInsert Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ InsertBench."
                },
                "en": {
                    "title": "Seamless Video Insertion with OmniInsert",
                    "desc": "OmniInsert is a novel framework designed for mask-free video insertion that tackles key challenges such as data scarcity and subject-scene equilibrium. It introduces a new data pipeline called InsertPipe to automatically create diverse training data, enhancing the model's learning capabilities. The framework employs Condition-Specific Feature Injection and Progressive Training to ensure that the inserted subjects harmonize well with the original video scenes. Additionally, it features a Context-Aware Rephraser and a Subject-Focused Loss to improve the visual quality and integration of subjects, outperforming existing commercial solutions in evaluations."
                },
                "zh": {
                    "title": "æ— æ©ç è§†é¢‘æ’å…¥çš„æ–°çªç ´",
                    "desc": "OmniInsertæ˜¯ä¸€ç§æ–°é¢–çš„æ— æ©ç è§†é¢‘æ’å…¥æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ•°æ®ç¨€ç¼ºã€ä¸»ä½“åœºæ™¯å¹³è¡¡å’Œæ’å…¥å’Œè°æ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†InsertPipeæ•°æ®ç®¡é“ï¼Œè‡ªåŠ¨æ„å»ºå¤šæ ·åŒ–çš„äº¤å‰é…å¯¹æ•°æ®ï¼Œä»¥åº”å¯¹æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚é€šè¿‡æ¡ä»¶ç‰¹å®šç‰¹å¾æ³¨å…¥æœºåˆ¶å’Œæ¸è¿›è®­ç»ƒç­–ç•¥ï¼ŒOmniInsertèƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡æ¥è‡ªä¸åŒæ¥æºçš„ç‰¹å¾æ³¨å…¥ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†æ’å…¥åå¥½ä¼˜åŒ–æ–¹æ³•å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡è¿°æ¨¡å—ï¼Œä»¥æé«˜æ’å…¥çš„å’Œè°æ€§ï¼Œä½¿ä¸»ä½“æ›´è‡ªç„¶åœ°èå…¥åŸå§‹åœºæ™¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18091",
            "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System",
            "url": "https://huggingface.co/papers/2509.18091",
            "abstract": "OnePiece integrates LLM-style context engineering and reasoning into industrial search and recommendation systems, achieving significant improvements in key business metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the growing interest in replicating the scaled success of large language models (LLMs) in industrial search and recommender systems, most existing industrial efforts remain limited to transplanting Transformer architectures, which bring only incremental improvements over strong Deep Learning Recommendation Models (DLRMs). From a first principle perspective, the breakthroughs of LLMs stem not only from their architectures but also from two complementary mechanisms: context engineering, which enriches raw input queries with contextual cues to better elicit model capabilities, and multi-step reasoning, which iteratively refines model outputs through intermediate reasoning paths. However, these two mechanisms and their potential to unlock substantial improvements remain largely underexplored in industrial ranking systems.   In this paper, we propose OnePiece, a unified framework that seamlessly integrates LLM-style context engineering and reasoning into both retrieval and ranking models of industrial cascaded pipelines. OnePiece is built on a pure Transformer backbone and further introduces three key innovations: (1) structured context engineering, which augments interaction history with preference and scenario signals and unifies them into a structured tokenized input sequence for both retrieval and ranking; (2) block-wise latent reasoning, which equips the model with multi-step refinement of representations and scales reasoning bandwidth via block size; (3) progressive multi-task training, which leverages user feedback chains to effectively supervise reasoning steps during training. OnePiece has been deployed in the main personalized search scenario of Shopee and achieves consistent online gains across different key business metrics, including over +2% GMV/UU and a +2.90% increase in advertising revenue.",
            "score": 23,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "a4d1025bbd0ac828",
            "authors": [
                "Sunhao Dai",
                "Jiakai Tang",
                "Jiahua Wu",
                "Kun Wang",
                "Yuxuan Zhu",
                "Bingjun Chen",
                "Bangyang Hong",
                "Yu Zhao",
                "Cong Fu",
                "Kangle Wu",
                "Yabo Ni",
                "Anxiang Zeng",
                "Wenjie Wang",
                "Xu Chen",
                "Jun Xu",
                "See-Kiong Ng"
            ],
            "affiliations": [
                "National University of Singapore",
                "Renmin University of China",
                "Shopee",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18091.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¼Ğ¾Ñ‰ÑŒ LLM Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "OnePiece - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ, Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ±Ñ‹Ğ»Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ° Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Shopee. OnePiece Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ¾ÑÑ‚ GMV/UU Ğ¸ Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ‚ Ñ€ĞµĞºĞ»Ğ°Ğ¼Ñ‹."
                },
                "en": {
                    "title": "Unlocking Search Potential with LLM-inspired Innovations",
                    "desc": "OnePiece is a novel framework that enhances industrial search and recommendation systems by incorporating techniques from large language models (LLMs). It focuses on two main mechanisms: context engineering, which enriches input queries with relevant contextual information, and multi-step reasoning, which refines outputs through iterative processes. The framework introduces structured context engineering, block-wise latent reasoning, and progressive multi-task training to improve model performance. As a result, OnePiece has shown significant improvements in key business metrics, such as increased gross merchandise value and advertising revenue."
                },
                "zh": {
                    "title": "OnePieceï¼šæå‡æœç´¢ä¸æ¨èçš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "OnePiece æ˜¯ä¸€ä¸ªå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é£æ ¼çš„ä¸Šä¸‹æ–‡å·¥ç¨‹å’Œæ¨ç†æœºåˆ¶æ•´åˆåˆ°å·¥ä¸šæœç´¢å’Œæ¨èç³»ç»Ÿä¸­çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡ç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡å·¥ç¨‹å¢å¼ºç”¨æˆ·çš„äº¤äº’å†å²ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºç»Ÿä¸€çš„è¾“å…¥åºåˆ—ï¼Œä»è€Œæé«˜æ£€ç´¢å’Œæ’åºçš„æ•ˆæœã€‚æ­¤å¤–ï¼ŒOnePiece é‡‡ç”¨å—çº§æ½œåœ¨æ¨ç†ï¼Œå…è®¸æ¨¡å‹é€šè¿‡å¤šæ­¥æ¨ç†é€æ­¥ä¼˜åŒ–è¾“å‡ºã€‚è¯¥æ¡†æ¶åœ¨ Shopee çš„ä¸ªæ€§åŒ–æœç´¢åœºæ™¯ä¸­åº”ç”¨ï¼Œæ˜¾è‘—æå‡äº†å¤šä¸ªå…³é”®ä¸šåŠ¡æŒ‡æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18056",
            "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs",
            "url": "https://huggingface.co/papers/2509.18056",
            "abstract": "TempSamp-R1, a reinforcement fine-tuning framework, enhances multimodal large language models for video temporal grounding by using off-policy supervision and a hybrid Chain-of-Thought training paradigm, achieving state-of-the-art performance on benchmark datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions (R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: https://github.com/HVision-NKU/TempSamp-R1",
            "score": 18,
            "issue_id": 6033,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "8d946cb9cc09008c",
            "authors": [
                "Yunheng Li",
                "Jing Cheng",
                "Shaoyong Jia",
                "Hangyi Kuang",
                "Shaohui Jiao",
                "Qibin Hou",
                "Ming-Ming Cheng"
            ],
            "affiliations": [
                "ByteDance Inc.",
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18056.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#rag",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "TempSamp-R1: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "TempSamp-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ off-policy Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Chain-of-Thought Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ğ¾Ğ². TempSamp-R1 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑĞ³ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "TempSamp-R1: Revolutionizing Video Temporal Grounding with Off-Policy Supervision",
                    "desc": "This paper presents TempSamp-R1, a novel reinforcement fine-tuning framework aimed at enhancing multimodal large language models (MLLMs) for video temporal grounding tasks. It addresses the inefficiencies of existing methods that rely on on-policy sampling by utilizing off-policy supervision from ground-truth annotations, which helps in achieving more accurate temporal solutions. Additionally, TempSamp-R1 incorporates a non-linear soft advantage computation to stabilize training and improve reward feedback. The framework also employs a hybrid Chain-of-Thought training paradigm, allowing it to efficiently manage varying reasoning complexities and outperform previous state-of-the-art methods on benchmark datasets."
                },
                "zh": {
                    "title": "TempSamp-R1ï¼šè§†é¢‘æ—¶é—´å®šä½çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†TempSamp-R1ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ—¶é—´å®šä½ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¦‚ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä¾èµ–äºç­–ç•¥æ›´æ–°çš„åœ¨çº¿é‡‡æ ·ï¼Œè¿™åœ¨å¤§æ—¶é—´æœç´¢ç©ºé—´çš„ä»»åŠ¡ä¸­æ•ˆç‡ä½ä¸‹ä¸”æ€§èƒ½æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒTempSamp-R1åˆ©ç”¨çœŸå®æ ‡ç­¾ä½œä¸ºç¦»çº¿ç›‘ç£ï¼Œæä¾›æ—¶é—´ä¸Šç²¾ç¡®çš„æŒ‡å¯¼ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†åœ¨çº¿è§£å†³æ–¹æ¡ˆä¸­çš„ç¨€ç–æ€§å’Œä¸å¯¹é½é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTempSamp-R1åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†GRPOåŸºçº¿ï¼Œå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17765",
            "title": "Qwen3-Omni Technical Report",
            "url": "https://huggingface.co/papers/2509.17765",
            "abstract": "Qwen3-Omni, a multimodal model, achieves state-of-the-art performance across text, image, audio, and video, using a Thinker-Talker MoE architecture and a lightweight causal ConvNet for efficient streaming synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.",
            "score": 15,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "464827796e5c676c",
            "authors": [
                "Jin Xu",
                "Zhifang Guo",
                "Hangrui Hu",
                "Yunfei Chu",
                "Xiong Wang",
                "Jinzheng He",
                "Yuxuan Wang",
                "Xian Shi",
                "Ting He",
                "Xinfa Zhu",
                "Yuanjun Lv",
                "Yongqi Wang",
                "Dake Guo",
                "He Wang",
                "Linhan Ma",
                "Pei Zhang",
                "Xinyu Zhang",
                "Hongkun Hao",
                "Zishan Guo",
                "Baosong Yang",
                "Bin Zhang",
                "Ziyang Ma",
                "Xipin Wei",
                "Shuai Bai",
                "Keqin Chen",
                "Xuejing Liu",
                "Peng Wang",
                "Mingkun Yang",
                "Dayiheng Liu",
                "Xingzhang Ren",
                "Bo Zheng",
                "Rui Men",
                "Fan Zhou",
                "Bowen Yu",
                "Jianxin Yang",
                "Le Yu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17765.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#audio",
                    "#multimodal",
                    "#architecture",
                    "#open_source",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Qwen3-Omni: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Qwen3-Omni - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Thinker-Talker MoE Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ½Ğ° 119 ÑĞ·Ñ‹ĞºĞ°Ñ…, Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° 19 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° 10 ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ”Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ°Ñ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ."
                },
                "en": {
                    "title": "Unifying Multimodal Mastery with Qwen3-Omni",
                    "desc": "Qwen3-Omni is a cutting-edge multimodal model that excels in processing text, images, audio, and video simultaneously without losing performance compared to single-modal models. It utilizes a Thinker-Talker MoE architecture to integrate perception and generation, achieving state-of-the-art results in various audio and audio-visual tasks. The model is designed for efficient streaming synthesis, significantly reducing latency by employing a lightweight causal ConvNet and a multi-codebook scheme for speech codecs. Additionally, it introduces a Thinking model for enhanced multimodal reasoning and provides a specialized audio captioning capability, making it a versatile tool for diverse applications."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ¨¡å‹çš„å…¨èƒ½ä¹‹é€‰",
                    "desc": "Qwen3-Omniæ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨¡å‹ï¼Œé¦–æ¬¡åœ¨æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œæ²¡æœ‰ç›¸å¯¹äºå•æ¨¡æ€æ¨¡å‹çš„æ€§èƒ½ä¸‹é™ã€‚è¯¥æ¨¡å‹é‡‡ç”¨Thinker-Talker MoEæ¶æ„ï¼Œç»Ÿä¸€äº†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘çš„æ„ŸçŸ¥ä¸ç”Ÿæˆï¼Œç‰¹åˆ«åœ¨éŸ³é¢‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚Qwen3-Omniåœ¨36ä¸ªéŸ³é¢‘å’ŒéŸ³é¢‘-è§†è§‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œå–å¾—äº†32ä¸ªåŸºå‡†çš„å¼€æºæœ€ä¼˜æ€§èƒ½ï¼Œå¹¶åœ¨22ä¸ªåŸºå‡†ä¸Šè¾¾åˆ°äº†æ•´ä½“æœ€ä¼˜ï¼Œè¶…è¶Šäº†è®¸å¤šå¼ºå¤§çš„é—­æºæ¨¡å‹ã€‚ä¸ºäº†æé«˜æµåª’ä½“åˆæˆçš„æ•ˆç‡ï¼ŒQwen3-Omniä½¿ç”¨è½»é‡çº§å› æœå·ç§¯ç½‘ç»œï¼Œæ˜¾è‘—é™ä½äº†é¦–æ¬¡æ•°æ®åŒ…çš„å»¶è¿Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17437",
            "title": "GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric\n  Reasoning",
            "url": "https://huggingface.co/papers/2509.17437",
            "abstract": "A two-stage reinforcement learning framework improves geometric reasoning and problem-solving in multimodal language models by first enhancing visual perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in reinforcement learning (RL) have enhanced the reasoning abilities of large language models (LLMs), yet the impact on multimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like geometric reasoning, MLLMs hallucinate frequently, leading to inaccurate reasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps the benefits of reasoning training. To quantify this, we design a Geo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric concepts and spatial relationships. Experiments on GeoPQA reveal significant shortcomings of MLLMs in visual perception, which constrain RL reward signals for effective training. To address this bottleneck, we propose a two-stage RL training framework by first enhancing the visual perception of geometric structures, then fostering reasoning capabilities. Applied to Qwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by 9.7% and geometric problem solving by 9.1%, compared to the direct reasoning training approach. Our method also generalizes to other vision-intensive domains like figure understanding, highlighting the importance of perceptual grounding in effective MLLM reasoning.",
            "score": 12,
            "issue_id": 6033,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "bd0dcbd688b7cc83",
            "authors": [
                "Guizhen Chen",
                "Weiwen Xu",
                "Hao Zhang",
                "Hou Pong Chan",
                "Deli Zhao",
                "Anh Tuan Luu",
                "Yu Rong"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17437.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#hallucinations",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GeoPQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-VL-3B-Instruct Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Enhancing Visual Perception for Better Geometric Reasoning in MLLMs",
                    "desc": "This paper presents a two-stage reinforcement learning framework aimed at improving geometric reasoning in multimodal language models (MLLMs). The authors identify a perceptual bottleneck that limits the effectiveness of reasoning training in MLLMs, particularly in tasks requiring visual understanding. They introduce a benchmark called Geo-Perception Question-Answering (GeoPQA) to evaluate the visual perception capabilities of MLLMs. By first enhancing visual perception and then focusing on reasoning, their approach significantly boosts performance in geometric reasoning and problem-solving tasks."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹çš„å‡ ä½•æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å‡ ä½•æ¨ç†å’Œé—®é¢˜è§£å†³æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒMLLMsåœ¨è§†è§‰æ„ŸçŸ¥ä¸Šå­˜åœ¨ç“¶é¢ˆï¼Œå¯¼è‡´åœ¨å‡ ä½•æ¨ç†ä»»åŠ¡ä¸­é¢‘ç¹å‡ºç°é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…è®¾è®¡äº†Geo-Perception Question-Answeringï¼ˆGeoPQAï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°æ¨¡å‹åœ¨åŸºæœ¬å‡ ä½•æ¦‚å¿µå’Œç©ºé—´å…³ç³»ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡å¢å¼ºè§†è§‰æ„ŸçŸ¥åå†è¿›è¡Œæ¨ç†è®­ç»ƒï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å‡ ä½•æ¨ç†å’Œé—®é¢˜è§£å†³ä¸Šåˆ†åˆ«æé«˜äº†9.7%å’Œ9.1%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18084",
            "title": "ByteWrist: A Parallel Robotic Wrist Enabling Flexible and\n  Anthropomorphic Motion for Confined Spaces",
            "url": "https://huggingface.co/papers/2509.18084",
            "abstract": "This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic parallel wrist for robotic manipulation. ByteWrist addresses the critical limitations of existing serial and parallel wrists in narrow-space operations through a compact three-stage parallel drive mechanism integrated with arc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw) motion while maintaining exceptional compactness, making it particularly suitable for complex unstructured environments such as home services, medical assistance, and precision assembly. The key innovations include: (1) a nested three-stage motor-driven linkages that minimize volume while enabling independent multi-DOF control, (2) arc-shaped end linkages that optimize force transmission and expand motion range, and (3) a central supporting ball functioning as a spherical joint that enhances structural stiffness without compromising flexibility. Meanwhile, we present comprehensive kinematic modeling including forward / inverse kinematics and a numerical Jacobian solution for precise control. Empirically, we observe ByteWrist demonstrates strong performance in narrow-space maneuverability and dual-arm cooperative manipulation tasks, outperforming Kinova-based systems. Results indicate significant improvements in compactness, efficiency, and stiffness compared to traditional designs, establishing ByteWrist as a promising solution for next-generation robotic manipulation in constrained environments.",
            "score": 10,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "5aa65074f4ca8fcb",
            "authors": [
                "Jiawen Tian",
                "Liqun Huang",
                "Zhongren Cui",
                "Jingchao Qiao",
                "Jiafeng Xu",
                "Xiao Ma",
                "Zeyu Ren"
            ],
            "affiliations": [
                "Bytedance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18084.jpg",
            "data": {
                "categories": [
                    "#robotics"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "ByteWrist: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿ÑÑÑ‚ÑŒÑÑ… Ğ´Ğ»Ñ ÑƒĞ·ĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ByteWrist - Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¸ Ğ°Ğ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¾Ğ¼Ğ¾Ñ€Ñ„Ğ½Ğ¾Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ°Ğ¿ÑÑÑ‚ÑŒĞµ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹. ByteWrist Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿ÑÑÑ‚Ğ¸Ğ¹ Ğ² Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ÑÑ… Ğ² ÑƒĞ·ĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€ĞµÑ…ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ Ğ´ÑƒĞ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ·Ğ²ĞµĞ½ÑŒÑĞ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµÑ…ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ²ĞµĞ½ÑŒÑ, Ğ´ÑƒĞ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ²Ñ‹Ğµ Ğ·Ğ²ĞµĞ½ÑŒÑ Ğ¸ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğ¹ ÑˆĞ°Ñ€, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ°Ğº ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑˆĞ°Ñ€Ğ½Ğ¸Ñ€. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ByteWrist Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½ĞµĞ²Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑƒĞ·ĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… Ğ¸ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ²ÑƒĞ¼Ñ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Kinova."
                },
                "en": {
                    "title": "ByteWrist: Revolutionizing Robotic Manipulation in Tight Spaces",
                    "desc": "This paper presents ByteWrist, an innovative robotic wrist designed for flexible and efficient manipulation in tight spaces. It features a compact three-stage parallel drive mechanism that allows for precise Roll-Pitch-Yaw (RPY) motion, making it ideal for complex tasks in unstructured environments. Key advancements include multi-degree-of-freedom control through nested linkages and arc-shaped end linkages that enhance force transmission. The paper also details kinematic modeling techniques for accurate control and demonstrates ByteWrist's superior performance in narrow-space tasks compared to existing systems."
                },
                "zh": {
                    "title": "ByteWristï¼šç‹­å°ç©ºé—´ä¸­çš„çµæ´»æœºå™¨äººè…•å…³èŠ‚",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é«˜çµæ´»æ€§å’Œç±»äººå¹¶è¡Œè…•å…³èŠ‚ï¼Œåä¸ºByteWristï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ä¸²è¡Œå’Œå¹¶è¡Œè…•å…³èŠ‚åœ¨ç‹­å°ç©ºé—´æ“ä½œä¸­çš„å…³é”®é™åˆ¶ã€‚ByteWristé‡‡ç”¨ç´§å‡‘çš„ä¸‰é˜¶æ®µå¹¶è¡Œé©±åŠ¨æœºåˆ¶ï¼Œç»“åˆå¼§å½¢æœ«ç«¯è¿æ†ï¼Œå®ç°äº†ç²¾ç¡®çš„æ»šè½¬-ä¿¯ä»°-åèˆªï¼ˆRPYï¼‰è¿åŠ¨ï¼ŒåŒæ—¶ä¿æŒäº†å“è¶Šçš„ç´§å‡‘æ€§ï¼Œç‰¹åˆ«é€‚åˆå¤æ‚çš„éç»“æ„åŒ–ç¯å¢ƒï¼Œå¦‚å®¶åº­æœåŠ¡ã€åŒ»ç–—è¾…åŠ©å’Œç²¾å¯†ç»„è£…ã€‚å…¶ä¸»è¦åˆ›æ–°åŒ…æ‹¬ï¼šåµŒå¥—çš„ä¸‰é˜¶æ®µç”µæœºé©±åŠ¨è¿æ†ï¼Œæœ€å°åŒ–ä½“ç§¯å¹¶å®ç°ç‹¬ç«‹çš„å¤šè‡ªç”±åº¦æ§åˆ¶ï¼›ä¼˜åŒ–åŠ›ä¼ è¾“å’Œæ‰©å±•è¿åŠ¨èŒƒå›´çš„å¼§å½¢æœ«ç«¯è¿æ†ï¼›ä»¥åŠä½œä¸ºçƒå½¢å…³èŠ‚çš„ä¸­å¤®æ”¯æ’‘çƒï¼Œå¢å¼ºç»“æ„åˆšåº¦è€Œä¸å½±å“çµæ´»æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†å…¨é¢çš„è¿åŠ¨å­¦å»ºæ¨¡ï¼ŒåŒ…æ‹¬æ­£/é€†è¿åŠ¨å­¦å’Œæ•°å€¼é›…å¯æ¯”è§£ï¼Œä»¥å®ç°ç²¾ç¡®æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17396",
            "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
            "url": "https://huggingface.co/papers/2509.17396",
            "abstract": "EpiCache is a KV cache management framework for long conversational question answering that reduces memory usage and improves accuracy through block-wise prefill, episodic KV compression, and adaptive layer-wise budget allocation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have extended context lengths, enabling assistants to sustain long histories for coherent, personalized responses. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly dominates under strict resource constraints. An active line of research for reducing this overhead is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting entries after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to degraded accuracy in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent baselines, sustains near-full KV accuracy under 4-6x compression, and reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient multi-turn interaction under strict resource constraints.",
            "score": 9,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "10727268d6361b72",
            "authors": [
                "Minsoo Kim",
                "Arnav Kundu",
                "Han-Byul Kim",
                "Richa Dixit",
                "Minsik Cho"
            ],
            "affiliations": [
                "Apple",
                "Hanyang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17396.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#inference",
                    "#optimization",
                    "#long_context",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ Ğ˜Ğ˜",
                    "desc": "EpiCache - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ KV-ĞºÑÑˆĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ KV-ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ¿Ğ¾ ÑĞ»Ğ¾ÑĞ¼ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. EpiCache Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ñ€Ğ¾ÑÑ‚ ĞºÑÑˆĞ° Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ‚ĞµĞ¼Ñ‹. Ğ’ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, EpiCache ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ 40% Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "EpiCache: Efficient Memory Management for Long Conversations",
                    "desc": "EpiCache is a framework designed to manage Key-Value (KV) caches for long conversational question answering, aiming to reduce memory usage while enhancing accuracy. It employs block-wise prefill and episodic KV compression to maintain relevant context without excessive memory growth. The framework also features an adaptive layer-wise budget allocation that optimizes memory distribution based on each layer's sensitivity to eviction. Overall, EpiCache significantly improves performance in multi-turn conversations, achieving higher accuracy and lower latency under strict resource limitations."
                },
                "zh": {
                    "title": "EpiCacheï¼šé«˜æ•ˆçš„é•¿å¯¹è¯é—®ç­”ç¼“å­˜ç®¡ç†",
                    "desc": "EpiCacheæ˜¯ä¸€ä¸ªç”¨äºé•¿å¯¹è¯é—®ç­”çš„é”®å€¼ç¼“å­˜ç®¡ç†æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘å†…å­˜ä½¿ç”¨å¹¶æé«˜å‡†ç¡®æ€§ã€‚å®ƒé€šè¿‡å—çº§é¢„å¡«å……ã€æƒ…èŠ‚é”®å€¼å‹ç¼©å’Œè‡ªé€‚åº”å±‚çº§é¢„ç®—åˆ†é…æ¥å®ç°è¿™äº›ç›®æ ‡ã€‚EpiCacheèƒ½å¤Ÿåœ¨å›ºå®šå†…å­˜é¢„ç®—ä¸‹æ§åˆ¶ç¼“å­˜å¢é•¿ï¼Œå¹¶é€šè¿‡å°†å¯¹è¯å†å²èšç±»ä¸ºä¸€è‡´çš„æƒ…èŠ‚æ¥ä¿ç•™ä¸ä¸»é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEpiCacheåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æé«˜äº†å‡†ç¡®æ€§ï¼Œå¹¶æ˜¾è‘—é™ä½äº†å»¶è¿Ÿå’Œå†…å­˜ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16941",
            "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering\n  Tasks?",
            "url": "https://huggingface.co/papers/2509.16941",
            "abstract": "SWE-Bench Pro is a challenging benchmark for coding models, featuring complex, enterprise-level problems that require substantial code modifications, with performance evaluations showing significant limitations in current models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SWE-Bench Pro, a substantially more challenging benchmark that builds upon the best practices of SWE-BENCH [25], but is explicitly designed to capture realistic, complex, enterprise-level problems beyond the scope of SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of 41 actively maintained repositories spanning business applications, B2B services, and developer tools. The benchmark is partitioned into a public set with open access to problems sourced from 11 repositories, a held-out set of 12 repositories and a commercial set of 18 proprietary repositories where we have formal partnership agreements with early-stage startups. Problems in the held-out and the commercial set are not publicly accessible, but we release results on the commercial set. Our benchmark features long-horizon tasks that may require hours to days for a professional software engineer to complete, often involving patches across multiple files and substantial code modifications. All tasks are human-verified and augmented with sufficient context to ensure resolvability. In our evaluation of widely used coding models, under a unified scaffold, we observe that their performance on SWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest score to date at 23.3%. To better understand these limitations, we cluster the failure modes observed in the collected agent trajectories for a clearer characterization of the error patterns exhibited by current models. Overall, SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully captures the complexity and diversity of real-world software development, advancing the pursuit of truly autonomous software engineering agents at a professional level.",
            "score": 9,
            "issue_id": 6030,
            "pub_date": "2025-09-21",
            "pub_date_card": {
                "ru": "21 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 21",
                "zh": "9æœˆ21æ—¥"
            },
            "hash": "e407b0b4d298f1ec",
            "authors": [
                "Xiang Deng",
                "Jeff Da",
                "Edwin Pan",
                "Yannis Yiming He",
                "Charles Ide",
                "Kanak Garg",
                "Niklas Lauffer",
                "Andrew Park",
                "Nitin Pasari",
                "Chetan Rane",
                "Karmini Sampath",
                "Maya Krishnan",
                "Srivatsa Kundurthy",
                "Sean Hendryx",
                "Zifan Wang",
                "Chen Bo Calvin Zhang",
                "Noah Jacobson",
                "Bing Liu",
                "Brad Kenstler"
            ],
            "affiliations": [
                "Scale AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16941.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#interpretability",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ§‘â€ğŸ’»",
                "ru": {
                    "title": "SWE-Bench Pro: Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ AI Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ",
                    "desc": "SWE-Bench Pro - ÑÑ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1865 Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸Ğ· 41 Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, B2B-ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ². Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñƒ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ñ‡Ğ°ÑÑ‹ Ğ¸Ğ»Ğ¸ Ğ´Ğ½Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 25% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ (Pass@1) Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ."
                },
                "en": {
                    "title": "SWE-Bench Pro: Elevating the Challenge for Coding Models",
                    "desc": "SWE-Bench Pro is a new benchmark designed to evaluate coding models on complex, real-world software engineering tasks. It includes 1,865 problems from various business applications and developer tools, emphasizing long-horizon tasks that require significant code changes. The benchmark reveals that current coding models, including GPT-5, struggle to achieve high performance, with a maximum score of only 23.3%. By analyzing the failure modes of these models, SWE-Bench Pro aims to enhance our understanding of their limitations and improve the development of autonomous software engineering agents."
                },
                "zh": {
                    "title": "SWE-Bench Proï¼šæŒ‘æˆ˜ç¼–ç æ¨¡å‹çš„æé™",
                    "desc": "SWE-Bench Pro æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“ä¸ºç¼–ç æ¨¡å‹è®¾è®¡ï¼Œæ¶µç›–å¤æ‚çš„ä¼ä¸šçº§é—®é¢˜ã€‚è¿™äº›é—®é¢˜éœ€è¦è¿›è¡Œå¤§é‡çš„ä»£ç ä¿®æ”¹ï¼Œä¸”å½“å‰æ¨¡å‹çš„è¡¨ç°æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å±€é™æ€§ã€‚åŸºå‡†æµ‹è¯•åŒ…å«æ¥è‡ª41ä¸ªæ´»è·ƒç»´æŠ¤çš„ä»£ç åº“çš„1865ä¸ªé—®é¢˜ï¼Œåˆ†ä¸ºå…¬å…±é›†ã€ä¿ç•™é›†å’Œå•†ä¸šé›†ã€‚é€šè¿‡å¯¹ç°æœ‰ç¼–ç æ¨¡å‹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å®ƒä»¬åœ¨SWE-Bench Proä¸Šçš„è¡¨ç°ä½äº25%ï¼Œè¿™è¡¨æ˜åœ¨çœŸå®è½¯ä»¶å¼€å‘ä¸­ï¼Œå½“å‰æ¨¡å‹ä»é¢ä¸´è®¸å¤šæŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16117",
            "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process",
            "url": "https://huggingface.co/papers/2509.16117",
            "abstract": "Diffusion Negative-aware FineTuning (DiffusionNFT) optimizes diffusion models directly on the forward process via flow matching, improving efficiency and performance compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Online reinforcement learning (RL) has been central to post-training language models, but its extension to diffusion models remains challenging due to intractable likelihoods. Recent works discretize the reverse sampling process to enable GRPO-style training, yet they inherit fundamental drawbacks, including solver restrictions, forward-reverse inconsistency, and complicated integration with classifier-free guidance (CFG). We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that optimizes diffusion models directly on the forward process via flow matching. DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective. This formulation enables training with arbitrary black-box solvers, eliminates the need for likelihood estimation, and requires only clean images rather than sampling trajectories for policy optimization. DiffusionNFT is up to 25times more efficient than FlowGRPO in head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 with over 5k steps and additional CFG employment. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested.",
            "score": 8,
            "issue_id": 6030,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 19",
                "zh": "9æœˆ19æ—¥"
            },
            "hash": "1d4f4b6ec61af4cd",
            "authors": [
                "Kaiwen Zheng",
                "Huayu Chen",
                "Haotian Ye",
                "Haoxiang Wang",
                "Qinsheng Zhang",
                "Kai Jiang",
                "Hang Su",
                "Stefano Ermon",
                "Jun Zhu",
                "Ming-Yu Liu"
            ],
            "affiliations": [
                "NVIDIA",
                "Stanford University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16117.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#optimization",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "DiffusionNFT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ DiffusionNFT. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸. DiffusionNFT ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ñ†ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº FlowGRPO."
                },
                "en": {
                    "title": "Revolutionizing Diffusion Models with Efficient FineTuning",
                    "desc": "Diffusion Negative-aware FineTuning (DiffusionNFT) is a novel approach that enhances diffusion models by optimizing them directly during the forward process using flow matching. This method addresses challenges in online reinforcement learning for diffusion models, such as intractable likelihoods and inconsistencies between forward and reverse processes. By contrasting positive and negative outputs, DiffusionNFT effectively integrates reinforcement signals into the training process without needing likelihood estimation or complex sampling. The results show that DiffusionNFT is significantly more efficient than previous methods, achieving higher performance scores in fewer training steps."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹çš„æ–°ä¼˜åŒ–ï¼šè´Ÿå‘å¾®è°ƒçš„åŠ›é‡",
                    "desc": "æ‰©æ•£è´Ÿå‘å¾®è°ƒï¼ˆDiffusionNFTï¼‰é€šè¿‡æµåŒ¹é…ç›´æ¥ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„å‰å‘è¿‡ç¨‹ï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDiffusionNFTå…‹æœäº†è®¸å¤šæŒ‘æˆ˜ï¼Œå¦‚æ±‚è§£å™¨é™åˆ¶å’Œå‰å‘-åå‘ä¸ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”æ­£å‘å’Œè´Ÿå‘ç”Ÿæˆï¼Œå®šä¹‰äº†éšå¼ç­–ç•¥æ”¹è¿›æ–¹å‘ï¼Œè‡ªç„¶åœ°å°†å¼ºåŒ–ä¿¡å·èå…¥ç›‘ç£å­¦ä¹ ç›®æ ‡ä¸­ã€‚DiffusionNFTåœ¨æ•ˆç‡ä¸Šæ¯”FlowGRPOé«˜å‡º25å€ï¼Œå¹¶ä¸”ä¸éœ€è¦åˆ†ç±»å™¨å¼•å¯¼ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17158",
            "title": "ARE: Scaling Up Agent Environments and Evaluations",
            "url": "https://huggingface.co/papers/2509.17158",
            "abstract": "Meta Agents Research Environments (ARE) facilitate the creation and execution of complex environments for agent research, and Gaia2, a benchmark built on ARE, evaluates general agent capabilities in dynamic, asynchronous settings.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Meta Agents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap between model development and real-world deployment. We also propose Gaia2, a benchmark built in ARE and designed to measure general agent capabilities. Beyond search and execution, Gaia2 requires agents to handle ambiguities and noise, adapt to dynamic environments, collaborate with other agents, and operate under temporal constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new failure modes that are invisible in static settings. Our experiments show that no system dominates across the intelligence spectrum: stronger reasoning often comes at the cost of efficiency, and budget scaling curves plateau, highlighting the need for new architectures and adaptive compute strategies. Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2 to other environments, empowering the community to rapidly create new benchmarks tailored to their domains. In AI's second half, progress increasingly depends on defining meaningful tasks and robust evaluations to drive frontier capabilities forward.",
            "score": 6,
            "issue_id": 6031,
            "pub_date": "2025-09-21",
            "pub_date_card": {
                "ru": "21 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 21",
                "zh": "9æœˆ21æ—¥"
            },
            "hash": "b2f18867a3844e4b",
            "authors": [
                "Pierre Andrews",
                "Amine Benhalloum",
                "Gerard Moreno-Torres Bertran",
                "Matteo Bettini",
                "Amar Budhiraja",
                "Ricardo Silveira Cabral",
                "Virginie Do",
                "Romain Froger",
                "Emilien Garreau",
                "Jean-Baptiste Gaya",
                "Hugo LaurenÃ§on",
                "Maxime Lecanu",
                "Kunal Malkan",
                "Dheeraj Mekala",
                "Pierre MÃ©nard",
                "GrÃ©goire Mialon",
                "Ulyana Piterbarg",
                "Mikhail Plekhanov",
                "Mathieu Rita",
                "Andrey Rusakov",
                "Thomas Scialom",
                "Vladislav Vorotilov",
                "Mengjue Wang",
                "Ian Yu"
            ],
            "affiliations": [
                "Meta Superintelligence Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17158.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#agi",
                    "#optimization",
                    "#transfer_learning",
                    "#games",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ARE Ğ¸ Gaia2: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Meta Agents Research Environments (ARE) - Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ° Ğ±Ğ°Ğ·Ğµ ARE Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Gaia2, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Gaia2 Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰ĞµĞ¹ÑÑ ÑÑ€ĞµĞ´Ğµ, ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğµ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾ Ğ²ÑĞµĞ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Empowering Agent Research with Dynamic Environments and Robust Benchmarks",
                    "desc": "Meta Agents Research Environments (ARE) is a platform designed to create and manage complex environments for agent research, allowing for the integration of both synthetic and real applications. It simplifies the process of building diverse environments with unique rules and tools, facilitating the transition from model development to real-world applications. The Gaia2 benchmark, developed within ARE, assesses general agent capabilities in dynamic and asynchronous settings, requiring agents to adapt to uncertainties and collaborate effectively. The findings indicate that no single system excels across all intelligence measures, emphasizing the need for innovative architectures and adaptive strategies in agent design."
                },
                "zh": {
                    "title": "å…ƒä»£ç†ç ”ç©¶ç¯å¢ƒï¼šæ¨åŠ¨æ™ºèƒ½ä»£ç†çš„è¿›æ­¥",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†å…ƒä»£ç†ç ”ç©¶ç¯å¢ƒï¼ˆAREï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¯æ‰©å±•åˆ›å»ºç¯å¢ƒçš„ç ”ç©¶å¹³å°ï¼Œèƒ½å¤Ÿé›†æˆåˆæˆæˆ–çœŸå®åº”ç”¨ï¼Œå¹¶æ‰§è¡Œä»£ç†åè°ƒã€‚AREæä¾›ç®€å•çš„æŠ½è±¡ï¼Œå¸®åŠ©æ„å»ºå¤æ‚å¤šæ ·çš„ç¯å¢ƒï¼Œæ¯ä¸ªç¯å¢ƒéƒ½æœ‰è‡ªå·±çš„è§„åˆ™ã€å·¥å…·ã€å†…å®¹å’ŒéªŒè¯å™¨ï¼Œä»è€Œç¼©å°æ¨¡å‹å¼€å‘ä¸å®é™…éƒ¨ç½²ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åŸºäºAREæ„å»ºçš„åŸºå‡†Gaia2ï¼Œæ—¨åœ¨æµ‹é‡ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„ä¸€èˆ¬èƒ½åŠ›ã€‚Gaia2è¦æ±‚ä»£ç†å¤„ç†æ¨¡ç³Šæ€§å’Œå™ªå£°ï¼Œé€‚åº”åŠ¨æ€ç¯å¢ƒï¼Œä¸å…¶ä»–ä»£ç†åä½œï¼Œå¹¶åœ¨æ—¶é—´é™åˆ¶ä¸‹æ“ä½œï¼Œå±•ç¤ºäº†åœ¨é™æ€è®¾ç½®ä¸­æ— æ³•å‘ç°çš„æ–°å¤±è´¥æ¨¡å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16596",
            "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from\n  Token and Parameter Levels",
            "url": "https://huggingface.co/papers/2509.16596",
            "abstract": "Supervised fine-tuning of large language models can negatively impact closed-book question answering performance, with up to 90% of parameter updates not contributing to knowledge enhancement.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) acquire substantial world knowledge during pre-training, which is further shaped by post-training techniques such as supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge remains underexplored, limiting our ability to control knowledge change behavior in fine-tuned models. To address this gap, we evaluate closed-book question answering (CBQA) performance across five LLMs from the LLaMA-2 and LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying the level of knowledge mastery in the fine-tuning data leads to performance fluctuations of over 12%. To investigate these effects, we analyze model behavior at both the token and parameter levels. Our analysis reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these updates can improve performance on the CBQA task, depending on the characteristics of the fine-tuning data. These insights offer practical guidance for developing fine-tuning strategies that more effectively strengthen model knowledge.",
            "score": 6,
            "issue_id": 6030,
            "pub_date": "2025-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "ad73526f5b38ef1d",
            "authors": [
                "Junjie Ye",
                "Yuming Yang",
                "Yang Nan",
                "Shuo Li",
                "Qi Zhang",
                "Tao Gui",
                "Xuanjing Huang",
                "Peng Wang",
                "Zhongchao Shi",
                "Jianping Fan"
            ],
            "affiliations": [
                "Fudan University",
                "Lenovo Research, Beijing, China",
                "Shanghai Innovation Institute",
                "Shanghai Key Lab of Intelligent Information Processing"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16596.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾ Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ»ÑƒÑ‡ÑˆĞµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² LLaMA-2 Ğ¸ LLaMA-3 Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑ…ÑƒĞ´ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 14%. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ 90% Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Optimize Fine-Tuning to Preserve Knowledge in Language Models",
                    "desc": "This paper investigates how supervised fine-tuning (SFT) affects the knowledge retention of large language models (LLMs) during closed-book question answering (CBQA). The authors find that a significant portion of parameter updates during SFT, up to 90%, do not enhance the model's knowledge, leading to performance drops. They demonstrate that fine-tuning on fewer samples can sometimes yield better results than on larger datasets, indicating that the quality of fine-tuning data is crucial. Their findings provide valuable insights for optimizing fine-tuning strategies to improve knowledge retention in LLMs."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å¾®è°ƒç­–ç•¥ï¼Œæå‡æ¨¡å‹çŸ¥è¯†",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿‡ç¨‹ä¸­å¯¹é—­å·é—®ç­”ï¼ˆCBQAï¼‰æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œå¾®è°ƒè¿‡ç¨‹ä¸­é«˜è¾¾90%çš„å‚æ•°æ›´æ–°å¹¶æœªæå‡æ¨¡å‹çš„çŸ¥è¯†æ°´å¹³ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¾®è°ƒæ ·æœ¬æ•°é‡çš„å¢åŠ åè€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚é€šè¿‡åˆ†ææ¨¡å‹åœ¨æ ‡è®°å’Œå‚æ•°å±‚é¢çš„è¡Œä¸ºï¼Œä½œè€…æ­ç¤ºäº†å¾®è°ƒæ•°æ®çš„çŸ¥è¯†æŒæ¡ç¨‹åº¦å¯¹æ¨¡å‹æ€§èƒ½çš„æ˜¾è‘—å½±å“ã€‚è¯¥ç ”ç©¶ä¸ºä¼˜åŒ–å¾®è°ƒç­–ç•¥ä»¥å¢å¼ºæ¨¡å‹çŸ¥è¯†æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17985",
            "title": "VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video\n  Diffusion Models",
            "url": "https://huggingface.co/papers/2509.17985",
            "abstract": "VideoFrom3D synthesizes high-quality 3D scene videos using a combination of image and video diffusion models, achieving style consistency without requiring paired datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we propose VideoFrom3D, a novel framework for synthesizing high-quality 3D scene videos from coarse geometry, a camera trajectory, and a reference image. Our approach streamlines the 3D graphic design workflow, enabling flexible design exploration and rapid production of deliverables. A straightforward approach to synthesizing a video from coarse geometry might condition a video diffusion model on geometric structure. However, existing video diffusion models struggle to generate high-fidelity results for complex scenes due to the difficulty of jointly modeling visual quality, motion, and temporal consistency. To address this, we propose a generative framework that leverages the complementary strengths of image and video diffusion models. Specifically, our framework consists of a Sparse Anchor-view Generation (SAG) and a Geometry-guided Generative Inbetweening (GGI) module. The SAG module generates high-quality, cross-view consistent anchor views using an image diffusion model, aided by Sparse Appearance-guided Sampling. Building on these anchor views, GGI module faithfully interpolates intermediate frames using a video diffusion model, enhanced by flow-based camera control and structural guidance. Notably, both modules operate without any paired dataset of 3D scene models and natural images, which is extremely difficult to obtain. Comprehensive experiments show that our method produces high-quality, style-consistent scene videos under diverse and challenging scenarios, outperforming simple and extended baselines.",
            "score": 5,
            "issue_id": 6032,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "1ca4bfe39743e387",
            "authors": [
                "Geonung Kim",
                "Janghyeok Han",
                "Sunghyun Cho"
            ],
            "affiliations": [
                "POSTECH, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17985.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "VideoFrom3D: Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ³Ñ€ÑƒĞ±Ğ¾Ğ¹ 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoFrom3D - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ 3D-ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. VideoFrom3D Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ‚Ğ¸Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Transforming 3D Designs into Stunning Videos!",
                    "desc": "VideoFrom3D is a new framework that creates high-quality 3D scene videos using image and video diffusion models. It allows for the generation of videos from basic 3D shapes, camera paths, and reference images without needing matched datasets. The framework includes two main components: Sparse Anchor-view Generation (SAG) for creating consistent anchor views and Geometry-guided Generative Inbetweening (GGI) for generating smooth transitions between frames. This innovative approach results in visually appealing and coherent videos, even in complex scenarios, outperforming existing methods."
                },
                "zh": {
                    "title": "VideoFrom3Dï¼šé«˜è´¨é‡3Dåœºæ™¯è§†é¢‘åˆæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶VideoFrom3Dï¼Œç”¨äºä»ç²—ç³™å‡ ä½•ä½“ã€ç›¸æœºè½¨è¿¹å’Œå‚è€ƒå›¾åƒåˆæˆé«˜è´¨é‡çš„3Dåœºæ™¯è§†é¢‘ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å›¾åƒå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œç®€åŒ–äº†3Då›¾å½¢è®¾è®¡å·¥ä½œæµç¨‹ï¼Œæ”¯æŒçµæ´»çš„è®¾è®¡æ¢ç´¢å’Œå¿«é€Ÿçš„äº¤ä»˜ç”Ÿäº§ã€‚é€šè¿‡ç¨€ç–é”šè§†å›¾ç”Ÿæˆæ¨¡å—(SAG)å’Œå‡ ä½•å¼•å¯¼ç”Ÿæˆæ’å€¼æ¨¡å—(GGI)ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆé£æ ¼ä¸€è‡´çš„é«˜è´¨é‡è§†é¢‘ï¼Œè€Œæ— éœ€é…å¯¹çš„3Dåœºæ™¯æ¨¡å‹å’Œè‡ªç„¶å›¾åƒæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸‹ï¼Œç”Ÿæˆçš„åœºæ™¯è§†é¢‘è´¨é‡é«˜ä¸”é£æ ¼ä¸€è‡´ï¼Œä¼˜äºç®€å•å’Œæ‰©å±•çš„åŸºçº¿æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17671",
            "title": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG\n  Applications",
            "url": "https://huggingface.co/papers/2509.17671",
            "abstract": "Turk-LettuceDetect, a suite of hallucination detection models for Turkish RAG applications, achieves high performance using fine-tuned encoder architectures on a machine-translated RAGTruth dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t The widespread adoption of Large Language Models (LLMs) has been hindered by their tendency to hallucinate, generating plausible but factually incorrect information. While Retrieval-Augmented Generation (RAG) systems attempt to address this issue by grounding responses in external knowledge, hallucination remains a persistent challenge, particularly for morphologically complex, low-resource languages like Turkish. This paper introduces Turk-LettuceDetect, the first suite of hallucination detection models specifically designed for Turkish RAG applications. Building on the LettuceDetect framework, we formulate hallucination detection as a token-level classification task and fine-tune three distinct encoder architectures: a Turkish-specific ModernBERT, TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a machine-translated version of the RAGTruth benchmark dataset containing 17,790 instances across question answering, data-to-text generation, and summarization tasks. Our experimental results show that the ModernBERT-based model achieves an F1-score of 0.7266 on the complete test set, with particularly strong performance on structured tasks. The models maintain computational efficiency while supporting long contexts up to 8,192 tokens, making them suitable for real-time deployment. Comparative analysis reveals that while state-of-the-art LLMs demonstrate high recall, they suffer from low precision due to over-generation of hallucinated content, underscoring the necessity of specialized detection mechanisms. By releasing our models and translated dataset, this work addresses a critical gap in multilingual NLP and establishes a foundation for developing more reliable and trustworthy AI applications for Turkish and other languages.",
            "score": 4,
            "issue_id": 6034,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "0985231370fd3145",
            "authors": [
                "Selva TaÅŸ",
                "Mahmut El Huseyni",
                "Ã–zay Ezerceli",
                "Reyhan Bayraktar",
                "Fatma BetÃ¼l TerzioÄŸlu"
            ],
            "affiliations": [
                "Newmind AI Istanbul, Turkiye"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17671.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#open_source",
                    "#architecture",
                    "#long_context",
                    "#hallucinations",
                    "#low_resource",
                    "#rag",
                    "#dataset"
                ],
                "emoji": "ğŸ¦ƒ",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Turk-LettuceDetect",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Turk-LettuceDetect - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¸Ñ… RAG-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾-Ğ¿ĞµÑ€ĞµĞ²ĞµĞ´ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… RAGTruth. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ°Ğº Ñ‚Ğ¾ĞºĞµĞ½-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ModernBERT, TurkEmbed4STS Ğ¸ EuroBERT. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ModernBERT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ F1-Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 0,7266 Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Detecting Hallucinations in Turkish RAG: Turk-LettuceDetect",
                    "desc": "The paper presents Turk-LettuceDetect, a set of models designed to detect hallucinations in Turkish Retrieval-Augmented Generation (RAG) applications. It addresses the challenge of Large Language Models (LLMs) generating incorrect information, particularly in low-resource languages like Turkish. The authors fine-tune three encoder architectures on a machine-translated RAGTruth dataset, treating hallucination detection as a token-level classification task. Experimental results show that the ModernBERT-based model achieves a high F1-score, demonstrating its effectiveness in real-time applications while highlighting the need for specialized detection mechanisms in multilingual NLP."
                },
                "zh": {
                    "title": "åœŸè€³å…¶è¯­å¹»è§‰æ£€æµ‹çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Turk-LettuceDetectï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºåœŸè€³å…¶è¯­æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åº”ç”¨è®¾è®¡çš„å¹»è§‰æ£€æµ‹æ¨¡å‹å¥—ä»¶ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹æœºå™¨ç¿»è¯‘çš„RAGTruthæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨äº†ä¸‰ç§ä¸åŒçš„ç¼–ç å™¨æ¶æ„ï¼Œæ—¨åœ¨æé«˜å¯¹åœŸè€³å…¶è¯­çš„å¹»è§‰æ£€æµ‹èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºModernBERTçš„æ¨¡å‹åœ¨å®Œæ•´æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†0.7266çš„F1åˆ†æ•°ï¼Œå°¤å…¶åœ¨ç»“æ„åŒ–ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚é€šè¿‡å‘å¸ƒè¿™äº›æ¨¡å‹å’Œç¿»è¯‘æ•°æ®é›†ï¼Œæœ¬æ–‡å¡«è¡¥äº†å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…³é”®ç©ºç™½ï¼Œä¸ºå¼€å‘æ›´å¯é çš„AIåº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15709",
            "title": "Understanding Embedding Scaling in Collaborative Filtering",
            "url": "https://huggingface.co/papers/2509.15709",
            "abstract": "Large-scale experiments reveal double-peak and logarithmic performance patterns in collaborative filtering models as embedding dimensions scale, and provide theoretical insights into their causes.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling recommendation models into large recommendation models has become one of the most widely discussed topics. Recent efforts focus on components beyond the scaling embedding dimension, as it is believed that scaling embedding may lead to performance degradation. Although there have been some initial observations on embedding, the root cause of their non-scalability remains unclear. Moreover, whether performance degradation occurs across different types of models and datasets is still an unexplored area. Regarding the effect of embedding dimensions on performance, we conduct large-scale experiments across 10 datasets with varying sparsity levels and scales, using 4 representative classical architectures. We surprisingly observe two novel phenomenon: double-peak and logarithmic. For the former, as the embedding dimension increases, performance first improves, then declines, rises again, and eventually drops. For the latter, it exhibits a perfect logarithmic curve. Our contributions are threefold. First, we discover two novel phenomena when scaling collaborative filtering models. Second, we gain an understanding of the underlying causes of the double-peak phenomenon. Lastly, we theoretically analyze the noise robustness of collaborative filtering models, with results matching empirical observations.",
            "score": 3,
            "issue_id": 6033,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 19",
                "zh": "9æœˆ19æ—¥"
            },
            "hash": "5728feeb510e7393",
            "authors": [
                "Zhuangzhuang He",
                "Zhou Kaiyu",
                "Haoyue Bai",
                "Fengbin Zhu",
                "Yonghui Yang"
            ],
            "affiliations": [
                "ASU",
                "NTU",
                "NUS",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15709.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ°: Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ¸Ğº Ğ¸ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¾ĞºĞ¾Ğ½Ñ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑˆÑƒĞ¼Ñƒ."
                },
                "en": {
                    "title": "Unveiling Performance Patterns in Collaborative Filtering Models",
                    "desc": "This paper investigates how the size of embedding dimensions in collaborative filtering models affects their performance. Through large-scale experiments on various datasets, the authors identify two unique performance patterns: double-peak and logarithmic. The double-peak pattern shows that performance can improve and then decline as embedding dimensions increase, while the logarithmic pattern indicates a steady performance curve. The study also provides theoretical insights into why these phenomena occur and explores the noise robustness of these models."
                },
                "zh": {
                    "title": "æ­ç¤ºååŒè¿‡æ»¤æ¨¡å‹çš„åŒå³°ä¸å¯¹æ•°æ€§èƒ½ç°è±¡",
                    "desc": "æœ¬ç ”ç©¶é€šè¿‡å¤§è§„æ¨¡å®éªŒæ­ç¤ºäº†ååŒè¿‡æ»¤æ¨¡å‹åœ¨åµŒå…¥ç»´åº¦æ‰©å±•æ—¶çš„åŒå³°å’Œå¯¹æ•°æ€§èƒ½æ¨¡å¼ï¼Œå¹¶æä¾›äº†å…¶åŸå› çš„ç†è®ºè§è§£ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œéšç€åµŒå…¥ç»´åº¦çš„å¢åŠ ï¼Œæ¨¡å‹æ€§èƒ½å…ˆæå‡åä¸‹é™ï¼Œå†æ¬¡ä¸Šå‡ï¼Œæœ€ååˆä¸‹é™ï¼Œå½¢æˆåŒå³°ç°è±¡ã€‚åŒæ—¶ï¼Œæ€§èƒ½è¿˜å‘ˆç°å‡ºå®Œç¾çš„å¯¹æ•°æ›²çº¿ã€‚æˆ‘ä»¬çš„è´¡çŒ®åœ¨äºå‘ç°äº†è¿™ä¸¤ç§æ–°ç°è±¡ï¼Œç†è§£äº†åŒå³°ç°è±¡çš„æ ¹æœ¬åŸå› ï¼Œå¹¶ç†è®ºåˆ†æäº†ååŒè¿‡æ»¤æ¨¡å‹çš„å™ªå£°é²æ£’æ€§ï¼Œç»“æœä¸ç»éªŒè§‚å¯Ÿç›¸ç¬¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15248",
            "title": "Synthetic bootstrapped pretraining",
            "url": "https://huggingface.co/papers/2509.15248",
            "abstract": "Synthetic Bootstrapped Pretraining (SBP) enhances language model performance by learning inter-document correlations and synthesizing new training data, leading to significant improvements over standard pretraining methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM) pretraining procedure that first learns a model of relations between documents from the pretraining dataset and then leverages it to synthesize a vast new corpus for joint training. While the standard pretraining teaches LMs to learn causal correlations among tokens within a single document, it is not designed to efficiently model the rich, learnable inter-document correlations that can potentially lead to better performance. We validate SBP by designing a compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T tokens from scratch. We find SBP consistently improves upon a strong repetition baseline and delivers a significant fraction of performance improvement attainable by an oracle upper bound with access to 20x more unique data. Qualitative analysis reveals that the synthesized documents go beyond mere paraphrases -- SBP first abstracts a core concept from the seed material and then crafts a new narration on top of it. Besides strong empirical performance, SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns to abstract the latent concepts shared between related documents.",
            "score": 3,
            "issue_id": 6030,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "99a93d8361bcbcf6",
            "authors": [
                "Zitong Yang",
                "Aonan Zhang",
                "Hong Liu",
                "Tatsunori Hashimoto",
                "Emmanuel CandÃ¨s",
                "Chong Wang",
                "Ruoming Pang"
            ],
            "affiliations": [
                "Apple",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15248.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#synthetic",
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Synthetic Bootstrapped Pretraining (SBP). SBP ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²ĞµĞ´ĞµÑ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SBP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ´Ğ¾Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² 20 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Unlocking Language Models with Inter-Document Insights",
                    "desc": "Synthetic Bootstrapped Pretraining (SBP) is a novel approach that enhances language model performance by focusing on the relationships between different documents rather than just within a single document. It first learns inter-document correlations from the pretraining dataset and then uses this knowledge to create a large amount of new training data. This method allows the model to capture richer contextual information, leading to significant performance improvements compared to traditional pretraining techniques. Additionally, SBP's ability to synthesize documents that abstract core concepts demonstrates its effectiveness in generating diverse and informative training examples."
                },
                "zh": {
                    "title": "åˆæˆè‡ªä¸¾é¢„è®­ç»ƒï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "åˆæˆè‡ªä¸¾é¢„è®­ç»ƒï¼ˆSBPï¼‰é€šè¿‡å­¦ä¹ æ–‡æ¡£ä¹‹é—´çš„å…³ç³»å¹¶åˆæˆæ–°çš„è®­ç»ƒæ•°æ®ï¼Œæå‡äº†è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„é¢„è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒSBPèƒ½å¤Ÿæœ‰æ•ˆå»ºæ¨¡æ–‡æ¡£é—´çš„ä¸°å¯Œç›¸å…³æ€§ï¼Œä»è€Œå®ç°æ›´å¥½çš„è¡¨ç°ã€‚æˆ‘ä»¬é€šè¿‡è®¾è®¡è®¡ç®—åŒ¹é…çš„é¢„è®­ç»ƒè®¾ç½®ï¼ŒéªŒè¯äº†SBPçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨ä»é›¶å¼€å§‹çš„æƒ…å†µä¸‹å¯¹ä¸€ä¸ª3Bå‚æ•°çš„æ¨¡å‹è¿›è¡Œäº†é¢„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSBPåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œå¹¶æ¥è¿‘äºç†æƒ³æƒ…å†µä¸‹çš„æ€§èƒ½ä¸Šé™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18095",
            "title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late\n  Interaction",
            "url": "https://huggingface.co/papers/2509.18095",
            "abstract": "MetaEmbed, a new framework for multimodal retrieval, uses learnable Meta Tokens to provide compact yet expressive multi-vector embeddings, enabling scalable and efficient retrieval performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Universal multimodal embedding models have achieved great success in capturing semantic relevance between queries and candidates. However, current methods either condense queries and candidates into a single vector, potentially limiting the expressiveness for fine-grained information, or produce too many vectors that are prohibitively expensive for multi-vector retrieval. In this work, we introduce MetaEmbed, a new framework for multimodal retrieval that rethinks how multimodal embeddings are constructed and interacted with at scale. During training, a fixed number of learnable Meta Tokens are appended to the input sequence. At test-time, their last-layer contextualized representations serve as compact yet expressive multi-vector embeddings. Through the proposed Matryoshka Multi-Vector Retrieval training, MetaEmbed learns to organize information by granularity across multiple vectors. As a result, we enable test-time scaling in multimodal retrieval, where users can balance retrieval quality against efficiency demands by selecting the number of tokens used for indexing and retrieval interactions. Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed achieves state-of-the-art retrieval performance while scaling robustly to models with 32B parameters.",
            "score": 2,
            "issue_id": 6032,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "dce11b4bc2712ec3",
            "authors": [
                "Zilin Xiao",
                "Qi Ma",
                "Mengting Gu",
                "Chun-cheng Jason Chen",
                "Xintao Chen",
                "Vicente Ordonez",
                "Vijai Mohan"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "Rice University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18095.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#optimization",
                    "#rag",
                    "#games"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "MetaEmbed: ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
                    "desc": "MetaEmbed - Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ ĞœĞµÑ‚Ğ°-Ğ¢Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ…, Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ. MetaEmbed Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Matryoshka Multi-Vector Retrieval, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MMEB Ğ¸ ViDoRe, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ 32 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "MetaEmbed: Efficient Multimodal Retrieval with Learnable Meta Tokens",
                    "desc": "MetaEmbed is a novel framework designed for multimodal retrieval that enhances the way embeddings are created and utilized. It introduces learnable Meta Tokens, which allow for the generation of compact yet expressive multi-vector embeddings, improving retrieval efficiency. By employing a training method called Matryoshka Multi-Vector Retrieval, MetaEmbed organizes information by granularity, enabling users to adjust the balance between retrieval quality and efficiency. Evaluations on benchmark datasets demonstrate that MetaEmbed achieves top-tier performance while effectively scaling to large models with billions of parameters."
                },
                "zh": {
                    "title": "MetaEmbedï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ£€ç´¢æ–°æ¡†æ¶",
                    "desc": "MetaEmbedæ˜¯ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ£€ç´¢æ¡†æ¶ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„Meta Tokensæ¥æä¾›ç´§å‡‘è€Œå¯Œæœ‰è¡¨ç°åŠ›çš„å¤šå‘é‡åµŒå…¥ï¼Œä»è€Œå®ç°å¯æ‰©å±•å’Œé«˜æ•ˆçš„æ£€ç´¢æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†æŸ¥è¯¢å’Œå€™é€‰é¡¹å‹ç¼©ä¸ºå•ä¸ªå‘é‡ï¼Œé™åˆ¶äº†ç»†ç²’åº¦ä¿¡æ¯çš„è¡¨è¾¾ï¼Œæˆ–è€…ç”Ÿæˆè¿‡å¤šå‘é‡ï¼Œå¯¼è‡´å¤šå‘é‡æ£€ç´¢æˆæœ¬è¿‡é«˜ã€‚MetaEmbedé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†å›ºå®šæ•°é‡çš„å¯å­¦ä¹ Meta Tokensé™„åŠ åˆ°è¾“å…¥åºåˆ—ä¸­ï¼Œé‡æ–°æ€è€ƒäº†å¤šæ¨¡æ€åµŒå…¥çš„æ„å»ºå’Œäº¤äº’æ–¹å¼ã€‚é€šè¿‡Matryoshkaå¤šå‘é‡æ£€ç´¢è®­ç»ƒï¼ŒMetaEmbedèƒ½å¤Ÿæ ¹æ®ä¿¡æ¯çš„ç»†ç²’åº¦ç»„ç»‡å¤šä¸ªå‘é‡ï¼Œä»è€Œåœ¨æ£€ç´¢æ—¶å®ç°å¯æ‰©å±•æ€§ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€æ±‚é€‰æ‹©ç”¨äºç´¢å¼•å’Œæ£€ç´¢äº¤äº’çš„Tokenæ•°é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17641",
            "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge\n  without Hearing?",
            "url": "https://huggingface.co/papers/2509.17641",
            "abstract": "AuditoryBench++ and AIR-CoT enhance text-only models' auditory reasoning and knowledge integration, outperforming existing models in multimodal interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, a comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIR-CoT, a novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io.",
            "score": 2,
            "issue_id": 6032,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "d97bd061de9c7180",
            "authors": [
                "Hyunjong Ok",
                "Suho Yoo",
                "Hyeonjun Kim",
                "Jaeho Lee"
            ],
            "affiliations": [
                "HJ AILAB",
                "Korea Advanced Institute of Science and Technology, South Korea",
                "Pohang University of Science and Technology, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17641.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#benchmark",
                    "#audio",
                    "#reasoning"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ»ÑƒÑ…Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AuditoryBench++, ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»ÑƒÑ…Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ AIR-CoT - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ ÑĞ»ÑƒÑ…Ğ¾Ğ²Ğ¾Ğ¼ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»ÑƒÑ…Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AIR-CoT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ»ÑƒÑ…Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ ÑĞ»ÑƒÑ…Ğ¾Ğ²Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ±ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Auditory Reasoning in Text Models",
                    "desc": "This paper introduces AuditoryBench++, a benchmark designed to evaluate how well text-only models understand and reason about auditory concepts. It highlights the limitations of current language models in processing auditory information, which is crucial for effective multimodal interactions. The authors also present AIR-CoT, a new method that enhances these models by integrating auditory reasoning during inference. Through extensive experiments, they show that AIR-CoT significantly improves performance compared to existing models, demonstrating the importance of auditory knowledge in language understanding."
                },
                "zh": {
                    "title": "æå‡æ–‡æœ¬æ¨¡å‹çš„å¬è§‰æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†AuditoryBench++å’ŒAIR-CoTï¼Œæ—¨åœ¨æå‡æ–‡æœ¬æ¨¡å‹çš„å¬è§‰æ¨ç†å’ŒçŸ¥è¯†æ•´åˆèƒ½åŠ›ã€‚AuditoryBench++æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°æ–‡æœ¬æ¨¡å‹åœ¨å¬è§‰çŸ¥è¯†å’Œæ¨ç†æ–¹é¢çš„è¡¨ç°ï¼Œæ¶µç›–ä»åŸºæœ¬çš„å¬è§‰æ¯”è¾ƒåˆ°ä¸Šä¸‹æ–‡ç›¸å…³çš„æ¨ç†ä»»åŠ¡ã€‚AIR-CoTæ˜¯ä¸€ç§æ–°é¢–çš„å¬è§‰æƒ³è±¡æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡ç‰¹æ®Šæ ‡è®°å’ŒçŸ¥è¯†æ³¨å…¥ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆå’Œæ•´åˆå¬è§‰ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAIR-CoTåœ¨å¤šæ¨¡æ€äº¤äº’ä¸­ä¼˜äºç°æœ‰çš„æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºæ›´å¼ºçš„å¬è§‰æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17336",
            "title": "Mano Report",
            "url": "https://huggingface.co/papers/2509.17336",
            "abstract": "A robust GUI agent, Mano, integrates reinforcement learning with vision-language models for high-fidelity data generation and improved performance on GUI benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical user interfaces (GUIs) are the primary medium for human-computer interaction, yet automating GUI interactions remains challenging due to the complexity of visual elements, dynamic environments, and the need for multi-step reasoning. Existing methods based on vision-language models (VLMs) often suffer from limited resolution, domain mismatch, and insufficient sequential decisionmaking capability. To address these issues, we propose Mano, a robust GUI agent built upon a multi-modal foundation model pre-trained on extensive web and computer system data. Our approach integrates a novel simulated environment for high-fidelity data generation, a three-stage training pipeline (supervised fine-tuning, offline reinforcement learning, and online reinforcement learning), and a verification module for error recovery. Mano demonstrates state-of-the-art performance on multiple GUI benchmarks, including Mind2Web and OSWorld, achieving significant improvements in success rate and operational accuracy. Our work provides new insights into the effective integration of reinforcement learning with VLMs for practical GUI agent deployment, highlighting the importance of domain-specific data, iterative training, and holistic reward design.",
            "score": 2,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "379006ad6024a25b",
            "authors": [
                "Tianyu Fu",
                "Anyang Su",
                "Chenxu Zhao",
                "Hanning Wang",
                "Minghui Wu",
                "Zhe Yu",
                "Fei Hu",
                "Mingjia Shi",
                "Wei Dong",
                "Jiayao Wang",
                "Yuyang Chen",
                "Ruiyang Yu",
                "Siran Peng",
                "Menglin Li",
                "Nan Huang",
                "Haitian Wei",
                "Jiawei Yu",
                "Yi Xin",
                "Xilin Zhao",
                "Kai Gu",
                "Ping Jiang",
                "Sifan Zhou",
                "Shuo Wang"
            ],
            "affiliations": [
                "Mininglamp Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17336.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#agents",
                    "#rl",
                    "#games",
                    "#multimodal",
                    "#rlhf",
                    "#benchmark",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "Mano: Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Mano - Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Mano Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ³ĞµĞ½Ñ‚ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ´Ğ»Ñ GUI, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Mind2Web Ğ¸ OSWorld. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Mano: Revolutionizing GUI Automation with Reinforcement Learning and Vision-Language Models",
                    "desc": "This paper presents Mano, a GUI agent that combines reinforcement learning with vision-language models to enhance data generation and performance on GUI tasks. The authors identify challenges in automating GUI interactions, such as visual complexity and the need for multi-step reasoning, which existing methods struggle to address. Mano utilizes a multi-modal foundation model and a three-stage training process that includes supervised fine-tuning and both offline and online reinforcement learning. The results show that Mano achieves state-of-the-art performance on various benchmarks, emphasizing the importance of tailored data and comprehensive training strategies in developing effective GUI agents."
                },
                "zh": {
                    "title": "Manoï¼šå¼ºåŒ–å­¦ä¹ ä¸è§†è§‰è¯­è¨€æ¨¡å‹çš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºManoçš„å¼ºå¤§å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ï¼Œå®ƒå°†å¼ºåŒ–å­¦ä¹ ä¸è§†è§‰è¯­è¨€æ¨¡å‹ç»“åˆï¼Œä»¥ç”Ÿæˆé«˜ä¿çœŸæ•°æ®å¹¶æé«˜GUIåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„è§†è§‰å…ƒç´ å’ŒåŠ¨æ€ç¯å¢ƒæ—¶å¸¸å¸¸é¢ä¸´åˆ†è¾¨ç‡æœ‰é™å’Œå†³ç­–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒManoé‡‡ç”¨äº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªæ–°é¢–çš„æ¨¡æ‹Ÿç¯å¢ƒè¿›è¡Œé«˜ä¿çœŸæ•°æ®ç”Ÿæˆï¼Œç»“åˆä¸‰é˜¶æ®µçš„è®­ç»ƒæµç¨‹ã€‚Manoåœ¨å¤šä¸ªGUIåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†æˆåŠŸç‡å’Œæ“ä½œå‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ä¸è§†è§‰è¯­è¨€æ¨¡å‹æœ‰æ•ˆç»“åˆçš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17818",
            "title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context\n  Enrichment",
            "url": "https://huggingface.co/papers/2509.17818",
            "abstract": "ContextFlow, a training-free framework for Diffusion Transformers, enhances video object editing by using a high-order Rectified Flow solver and Adaptive Context Enrichment to achieve precise, temporally consistent, and high-fidelity object manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Training-free video object editing aims to achieve precise object-level manipulation, including object insertion, swapping, and deletion. However, it faces significant challenges in maintaining fidelity and temporal consistency. Existing methods, often designed for U-Net architectures, suffer from two primary limitations: inaccurate inversion due to first-order solvers, and contextual conflicts caused by crude \"hard\" feature replacement. These issues are more challenging in Diffusion Transformers (DiTs), where the unsuitability of prior layer-selection heuristics makes effective guidance challenging. To address these limitations, we introduce ContextFlow, a novel training-free framework for DiT-based video object editing. In detail, we first employ a high-order Rectified Flow solver to establish a robust editing foundation. The core of our framework is Adaptive Context Enrichment (for specifying what to edit), a mechanism that addresses contextual conflicts. Instead of replacing features, it enriches the self-attention context by concatenating Key-Value pairs from parallel reconstruction and editing paths, empowering the model to dynamically fuse information. Additionally, to determine where to apply this enrichment (for specifying where to edit), we propose a systematic, data-driven analysis to identify task-specific vital layers. Based on a novel Guidance Responsiveness Metric, our method pinpoints the most influential DiT blocks for different tasks (e.g., insertion, swapping), enabling targeted and highly effective guidance. Extensive experiments show that ContextFlow significantly outperforms existing training-free methods and even surpasses several state-of-the-art training-based approaches, delivering temporally coherent, high-fidelity results.",
            "score": 1,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 22",
                "zh": "9æœˆ22æ—¥"
            },
            "hash": "e6039499f6d1d2dc",
            "authors": [
                "Yiyang Chen",
                "Xuanhua He",
                "Xiujun Ma",
                "Yue Ma"
            ],
            "affiliations": [
                "State Key Laboratory of General Artificial Intelligence, Peking University, Beijing, China",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17818.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ContextFlow: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "ContextFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒ Rectified Flow Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ContextFlow Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Video Editing with ContextFlow!",
                    "desc": "ContextFlow is a novel framework designed for video object editing using Diffusion Transformers without the need for training. It addresses key challenges in maintaining high fidelity and temporal consistency during object manipulation tasks like insertion and swapping. By utilizing a high-order Rectified Flow solver and Adaptive Context Enrichment, it enhances the editing process by dynamically fusing information from different paths instead of simply replacing features. The framework also employs a data-driven approach to identify the most effective layers for specific editing tasks, leading to superior performance compared to existing methods."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒè§†é¢‘å¯¹è±¡ç¼–è¾‘çš„æ–°çªç ´",
                    "desc": "ContextFlow æ˜¯ä¸€ä¸ªæ— è®­ç»ƒçš„æ¡†æ¶ï¼Œä¸“ä¸ºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiffusion Transformersï¼‰è®¾è®¡ï¼Œæ—¨åœ¨æå‡è§†é¢‘å¯¹è±¡ç¼–è¾‘çš„ç²¾ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚å®ƒé€šè¿‡é«˜é˜¶ä¿®æ­£æµæ±‚è§£å™¨å’Œè‡ªé€‚åº”ä¸Šä¸‹æ–‡ä¸°å¯Œæœºåˆ¶ï¼Œè§£å†³äº†å¯¹è±¡æ’å…¥ã€äº¤æ¢å’Œåˆ é™¤ä¸­çš„æ—¶é—´ä¸€è‡´æ€§å’Œä¿çœŸåº¦é—®é¢˜ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒContextFlow é€šè¿‡åŠ¨æ€èåˆä¿¡æ¯ï¼Œé¿å…äº†ç‰¹å¾æ›¿æ¢å¸¦æ¥çš„ä¸Šä¸‹æ–‡å†²çªã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒContextFlow åœ¨æ— è®­ç»ƒæ–¹æ³•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¶Šäº†ä¸€äº›åŸºäºè®­ç»ƒçš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16633",
            "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for\n  Efficient Visual Question Answering using Small VLMs",
            "url": "https://huggingface.co/papers/2509.16633",
            "abstract": "The Model Parity Aligner (MPA) framework improves Small Vision-Language Models (S-VLMs) by leveraging unlabeled images and knowledge transfer from Large Vision-Language Models (L-VLMs) to reduce performance gaps in vision and language tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Vision-Language Models (L-VLMs) have demonstrated remarkable performance in various vision and language tasks, including visual question answering (VQA). However, their high computational cost makes them impractical for resource-constrained settings and inference-heavy applications. In contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer from a significant performance gap compared to their larger counterparts. In this work, we introduce the Model Parity Aligner (MPA), a novel framework designed to systematically improve S-VLMs by leveraging unlabeled images and effective knowledge transfer from L-VLMs. Instead of traditional knowledge distillation methods that rely on labeled training data, MPA employs a strategic parity-based approach that precisely identifies the knowledge disparities between S-VLMs and L-VLMs, and optimizes training by targeting only these disparities. We conduct extensive experiments on four diverse VQA benchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires specialized reasoning capabilities such as text recognition, chart interpretation, and commonsense and factual understanding. Our results demonstrate that MPA consistently enhances the performance of S-VLMs on all benchmarks, reducing the performance gap while maintaining computational efficiency. We make our code publicly available.",
            "score": 1,
            "issue_id": 6032,
            "pub_date": "2025-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "f51723c28433f366",
            "authors": [
                "Abhirama Subramanyam Penamakuri",
                "Navlika Singh",
                "Piyush Arora",
                "Anand Mishra"
            ],
            "affiliations": [
                "Indian Institute of Technology Jodhpur"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16633.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#small_models",
                    "#transfer_learning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "ĞœĞ¾Ğ´ĞµĞ»ÑŒ Model Parity Aligner (MPA) Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (S-VLMs). MPA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (L-VLMs) Ğ´Ğ»Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ S-VLMs Ğ¸ L-VLMs, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ S-VLMs Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Small Models with Big Model Insights",
                    "desc": "The Model Parity Aligner (MPA) framework enhances Small Vision-Language Models (S-VLMs) by utilizing unlabeled images and transferring knowledge from Large Vision-Language Models (L-VLMs). This approach addresses the performance gap between S-VLMs and L-VLMs without relying on labeled data, focusing instead on identifying and optimizing specific knowledge disparities. MPA employs a parity-based strategy to improve training efficiency while maintaining the computational advantages of S-VLMs. Extensive experiments on various visual question answering benchmarks show that MPA significantly boosts S-VLM performance across diverse reasoning tasks."
                },
                "zh": {
                    "title": "æå‡å°å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ¨¡å‹å¹³è¡¡å¯¹é½å™¨ï¼ˆMPAï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨æœªæ ‡è®°å›¾åƒå’Œä»å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆL-VLMsï¼‰è½¬ç§»çŸ¥è¯†æ¥æ”¹å–„å°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆS-VLMsï¼‰çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ä¾èµ–äºæ ‡è®°è®­ç»ƒæ•°æ®ï¼Œè€ŒMPAé‡‡ç”¨äº†ä¸€ç§åŸºäºå¹³è¡¡çš„ç­–ç•¥ï¼Œç²¾ç¡®è¯†åˆ«S-VLMsä¸L-VLMsä¹‹é—´çš„çŸ¥è¯†å·®è·ï¼Œå¹¶ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨å››ä¸ªä¸åŒçš„è§†è§‰é—®ç­”åŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜MPAåœ¨æ‰€æœ‰åŸºå‡†ä¸Šéƒ½èƒ½æ˜¾è‘—æå‡S-VLMsçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€ï¼Œä¾›ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16591",
            "title": "From Uniform to Heterogeneous: Tailoring Policy Optimization to Every\n  Token's Nature",
            "url": "https://huggingface.co/papers/2509.16591",
            "abstract": "Heterogeneous Adaptive Policy Optimization (HAPO) enhances reinforcement learning in LLMs by dynamically adapting token optimization based on entropy, improving performance across various model scales.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning has emerged as the fundamental technique for enhancing reasoning in LLMs. However, existing algorithms apply uniform optimization to all tokens, ignoring their different roles in reasoning process. To address this limitation, we introduce Heterogeneous Adaptive Policy Optimization (HAPO), a comprehensive token-aware algorithm that dynamically adapts optimization based on token entropy. For rollout sampling, we propose Adaptive Temperature Sampling, which adjusts sampling temperature in real time, promoting exploration at high-entropy tokens while preserving coherence at low-entropy ones. For advantage calculation, we introduce Token Level Group Average that normalizes advantages at token level, jointly accounting for sequence-length as in token-mean loss while preserving non-biased treatment. We then develop Differential Advantage Redistribution that leverages entropy and importance ratios to modulate rewards-adjusting updates for tokens with clear signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing aggressive probability reduction for noisy low-entropy tokens while enabling exploration for high-entropy tokens. Through systematic investigation between entropy and training dynamics, we embedded token-level treatment into every stages to achieve fine-grained control. Extensive experiments demonstrate that HAPO consistently outperforms DAPO across multiple model scales. Our code can be found in https://github.com/starriver030515/HAPO.",
            "score": 1,
            "issue_id": 6033,
            "pub_date": "2025-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "b2e9069d03b40295",
            "authors": [
                "Zheng Liu",
                "Mengjie Liu",
                "Siwei Wen",
                "Mengzhang Cai",
                "Bin Cui",
                "Conghui He",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Beihang University",
                "Peking University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16591.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Heterogeneous Adaptive Policy Optimization (HAPO). HAPO Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ñ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹, Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğµ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HAPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ²."
                },
                "en": {
                    "title": "Dynamic Token Optimization for Enhanced Reinforcement Learning",
                    "desc": "Heterogeneous Adaptive Policy Optimization (HAPO) is a novel approach in reinforcement learning that enhances the performance of large language models (LLMs) by adapting token optimization based on their entropy levels. Unlike traditional methods that apply uniform optimization, HAPO recognizes the varying importance of tokens in the reasoning process and adjusts the optimization dynamically. It introduces techniques like Adaptive Temperature Sampling for real-time adjustment of sampling temperature and Token Level Group Average for normalized advantage calculations. The method also incorporates Differential Advantage Redistribution and Asymmetric Adaptive Clipping to fine-tune reward updates and loss clipping, leading to improved training dynamics and overall performance across different model scales."
                },
                "zh": {
                    "title": "åŠ¨æ€ä¼˜åŒ–ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "å¼‚æ„è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ˆHAPOï¼‰é€šè¿‡æ ¹æ®ç†µåŠ¨æ€è°ƒæ•´ä»¤ç‰Œä¼˜åŒ–ï¼Œå¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¼ºåŒ–å­¦ä¹ æ€§èƒ½ã€‚ç°æœ‰ç®—æ³•å¯¹æ‰€æœ‰ä»¤ç‰Œåº”ç”¨ç»Ÿä¸€ä¼˜åŒ–ï¼Œå¿½è§†äº†å®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸åŒè§’è‰²ã€‚HAPOæ˜¯ä¸€ç§å…¨é¢çš„ä»¤ç‰Œæ„ŸçŸ¥ç®—æ³•ï¼Œèƒ½å¤Ÿå®æ—¶è°ƒæ•´é‡‡æ ·æ¸©åº¦ï¼Œä¿ƒè¿›é«˜ç†µä»¤ç‰Œçš„æ¢ç´¢ï¼ŒåŒæ—¶ä¿æŒä½ç†µä»¤ç‰Œçš„ä¸€è‡´æ€§ã€‚é€šè¿‡ç³»ç»Ÿçš„å®éªŒï¼ŒHAPOåœ¨å¤šä¸ªæ¨¡å‹è§„æ¨¡ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰çš„ç®—æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨è®­ç»ƒåŠ¨æ€ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16415",
            "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes",
            "url": "https://huggingface.co/papers/2509.16415",
            "abstract": "StereoAdapter is a parameter-efficient self-supervised framework that integrates a LoRA-adapted monocular encoder with a recurrent stereo refinement module for underwater stereo depth estimation, improving accuracy and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Underwater stereo depth estimation provides accurate 3D geometry for robotics tasks such as navigation, inspection, and mapping, offering metric depth from low-cost passive cameras while avoiding the scale ambiguity of monocular methods. However, existing approaches face two critical challenges: (i) parameter-efficiently adapting large vision foundation encoders to the underwater domain without extensive labeled data, and (ii) tightly fusing globally coherent but scale-ambiguous monocular priors with locally metric yet photometrically fragile stereo correspondences. To address these challenges, we propose StereoAdapter, a parameter-efficient self-supervised framework that integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo refinement module. We further introduce dynamic LoRA adaptation for efficient rank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to enhance robustness under diverse underwater conditions. Comprehensive evaluations on both simulated and real-world benchmarks show improvements of 6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods, while real-world deployment with the BlueROV2 robot further demonstrates the consistent robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter. Website: https://aigeeksgroup.github.io/StereoAdapter.",
            "score": 1,
            "issue_id": 6032,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 19",
                "zh": "9æœˆ19æ—¥"
            },
            "hash": "5e1fea9f33bd29b1",
            "authors": [
                "Zhengri Wu",
                "Yiran Wang",
                "Yu Wen",
                "Zeyu Zhang",
                "Biao Wu",
                "Hao Tang"
            ],
            "affiliations": [
                "AI Geeks",
                "Australian Centre for Robotics",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16415.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#robotics",
                    "#benchmark",
                    "#optimization",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹",
                    "desc": "StereoAdapter - ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾Ğ´ Ğ²Ğ¾Ğ´Ğ¾Ğ¹ Ğ¿Ğ¾ ÑÑ‚ĞµÑ€ĞµĞ¾Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LoRA, Ğ¸ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚ĞµÑ€ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ğ´Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ±ĞµĞ· Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. StereoAdapter Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 6.11% Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ TartanAir Ğ¸ 5.12% Ğ½Ğ° SQUID Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Underwater Depth Estimation with StereoAdapter",
                    "desc": "StereoAdapter is a self-supervised framework designed for underwater stereo depth estimation, which combines a LoRA-adapted monocular encoder with a recurrent stereo refinement module. This approach addresses the challenges of adapting large vision models to underwater environments and effectively merging monocular and stereo data. By utilizing dynamic LoRA adaptation and pre-training on a synthetic dataset, it enhances the model's robustness in varying underwater conditions. The results show significant improvements in depth estimation accuracy on benchmark datasets and successful real-world application with a robotic platform."
                },
                "zh": {
                    "title": "æ°´ä¸‹æ·±åº¦ä¼°è®¡çš„æ–°çªç ´ï¼šStereoAdapter",
                    "desc": "StereoAdapter æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„è‡ªç›‘ç£æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ°´ä¸‹ç«‹ä½“æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚å®ƒç»“åˆäº†ç»è¿‡ LoRA è°ƒæ•´çš„å•ç›®ç¼–ç å™¨å’Œé€’å½’ç«‹ä½“ç»†åŒ–æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨ç¼ºä¹å¤§é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹æœ‰æ•ˆé€‚åº”æ°´ä¸‹ç¯å¢ƒã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€ LoRA è°ƒæ•´å’Œåœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¢å¼ºäº†åœ¨å¤šæ ·æ°´ä¸‹æ¡ä»¶ä¸‹çš„é²æ£’æ€§ã€‚ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒStereoAdapter åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æœ‰æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14856",
            "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End\n  Code Review Evaluation in Python Projects",
            "url": "https://huggingface.co/papers/2509.14856",
            "abstract": "A new benchmark, CodeFuse-CR-Bench, evaluates LLMs in repository-level code review with comprehensive, context-rich data and a novel evaluation framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated code review (CR) is a key application for Large Language Models (LLMs), but progress is hampered by a \"reality gap\": existing benchmarks evaluate models on isolated sub-tasks using simplified, context-poor data. This fails to reflect the holistic context-rich nature of real-world CR. To bridge this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601 high-quality instances from 70 Python projects covering nine Pull-Request (PR) problem domains, where each instance provides rich, multi-faceted context including the associated issue, PR details, and repository state, enabling end-to-end evaluation. Beyond superficial metrics, we also propose a novel evaluation framework that combines rule-based checks for location and syntax with model-based judgments of review quality. We present the first large-scale assessment of state-of-the-art LLMs on this comprehensive CR task. Our results establish crucial baselines and reveal that (1) no single LLM dominates all aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive performance; and (3) different LLMs exhibit varying robustness to redundant context. These findings highlight the necessity of holistic, multi-dimensional evaluation and provide actionable insights for advancing truly intelligent yet practical CR assistants.",
            "score": 1,
            "issue_id": 6033,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 18",
                "zh": "9æœˆ18æ—¥"
            },
            "hash": "cabf265ab40f85c9",
            "authors": [
                "Hanyang Guo",
                "Xunjin Zheng",
                "Zihan Liao",
                "Hang Yu",
                "Peng DI",
                "Ziyin Zhang",
                "Hong-Ning Dai"
            ],
            "affiliations": [
                "Ant Group",
                "Hong Kong Baptist University",
                "UNSW Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14856.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "ğŸ§‘â€ğŸ’»",
                "ru": {
                    "title": "CodeFuse-CR-Bench: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ LLM Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "CodeFuse-CR-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 601 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ¸Ğ· 70 Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Python, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 9 Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Pull Request. Ğ¢ĞµÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ PR Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Bridging the Reality Gap in Code Review with CodeFuse-CR-Bench",
                    "desc": "The paper introduces CodeFuse-CR-Bench, a new benchmark designed to evaluate Large Language Models (LLMs) in the context of repository-level code review (CR). It addresses the limitations of existing benchmarks that use simplified data and isolated tasks, which do not reflect the complexity of real-world code reviews. CodeFuse-CR-Bench includes 601 instances from 70 Python projects, providing rich context such as issue details and repository state for a more comprehensive evaluation. The study also presents a novel evaluation framework that combines rule-based checks with model-based assessments, revealing that no single LLM excels in all CR aspects, with Gemini 2.5 Pro performing the best overall."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°ä»£ç å®¡æŸ¥çš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼ŒCodeFuse-CR-Benchï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç å®¡æŸ¥ä¸­çš„è¡¨ç°ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¾€å¾€åªå…³æ³¨å­¤ç«‹çš„å­ä»»åŠ¡ï¼Œç¼ºä¹çœŸå®åœºæ™¯ä¸­çš„ä¸°å¯Œä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å¤Ÿå…¨é¢ã€‚CodeFuse-CR-BenchåŒ…å«æ¥è‡ª70ä¸ªPythoné¡¹ç›®çš„601ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œæ¶µç›–ä¹ä¸ªæ‹‰å–è¯·æ±‚ï¼ˆPRï¼‰é—®é¢˜é¢†åŸŸï¼Œæä¾›å¤šç»´åº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ²¡æœ‰å•ä¸€çš„LLMåœ¨æ‰€æœ‰ä»£ç å®¡æŸ¥æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè€ŒGemini 2.5 Proåœ¨ç»¼åˆæ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¼ºè°ƒäº†å…¨é¢ã€å¤šç»´åº¦è¯„ä¼°çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17191",
            "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery",
            "url": "https://huggingface.co/papers/2509.17191",
            "abstract": "VaseVL, an SFT-then-RL system, enhances MLLMs for ancient Greek pottery analysis by addressing performance gaps through taxonomy-conditioned rewards, achieving state-of-the-art results in style classification and historical attribution.  \t\t\t\t\tAI-generated summary \t\t\t\t Analyzing cultural-heritage artifacts remains challenging for MLLMs: general models lack domain expertise, and SFT often overfits superficial patterns, yielding brittle reasoning for authentication and historical attribution. This raises the question of how to equip MLLMs with robust, expert-level reasoning for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns evaluation into supervision: we construct a taxonomy of question types, probe the SFT model to localize type-specific performance gaps, and optimize with type-conditioned, compositionality-oriented rewards targeting those gaps. We also release VaseVQA, a comprehensive benchmark of 31,773 images designed to probe deep understanding. Experiments show state-of-the-art results on style classification and historical attribution with marked gains in compositional robustness over SFT-only baselines, validating diagnosis-guided, taxonomy-conditioned reward engineering and providing a reusable resource for future research. Code and dataset will be available at https://github.com/AIGeeksGroup/VaseVQA.",
            "score": 0,
            "issue_id": 6033,
            "pub_date": "2025-09-21",
            "pub_date_card": {
                "ru": "21 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 21",
                "zh": "9æœˆ21æ—¥"
            },
            "hash": "68511afc4e6acc3e",
            "authors": [
                "Jinchao Ge",
                "Tengfei Cheng",
                "Biao Wu",
                "Zeyu Zhang",
                "Shiya Huang",
                "Judith Bishop",
                "Gillian Shepherd",
                "Meng Fang",
                "Ling Chen",
                "Yang Zhao"
            ],
            "affiliations": [
                "AI Geeks",
                "Australian Artificial Intelligence Institute",
                "La Trobe University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17191.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#reasoning",
                    "#dataset",
                    "#science"
                ],
                "emoji": "ğŸº",
                "ru": {
                    "title": "Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ´Ñ€ĞµĞ²Ğ½ĞµĞ³Ñ€ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ°Ğ·",
                    "desc": "VaseVL - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ñ€ĞµĞ²Ğ½ĞµĞ³Ñ€ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞµÑ€Ğ°Ğ¼Ğ¸ĞºĞ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. VaseVL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VaseVQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ñ€ĞµĞ²Ğ½ĞµĞ³Ñ€ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞµÑ€Ğ°Ğ¼Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing Ancient Pottery Analysis with VaseVL: A Smart Approach to Machine Learning",
                    "desc": "VaseVL is a machine learning system designed to improve the analysis of ancient Greek pottery by using a two-step approach: supervised fine-tuning (SFT) followed by reinforcement learning (RL). It addresses the limitations of general models that struggle with domain-specific tasks by implementing taxonomy-conditioned rewards that focus on specific types of questions. This method enhances the model's ability to classify styles and attribute historical context accurately, achieving state-of-the-art performance. Additionally, the study introduces VaseVQA, a large dataset that aids in evaluating the model's understanding and robustness in this specialized field."
                },
                "zh": {
                    "title": "VaseVLï¼šå¤å¸Œè…Šé™¶å™¨åˆ†æçš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ",
                    "desc": "VaseVLæ˜¯ä¸€ä¸ªå…ˆè¿›è¡Œç›‘ç£å­¦ä¹ (SFT)å†è¿›è¡Œå¼ºåŒ–å­¦ä¹ (RL)çš„ç³»ç»Ÿï¼Œæ—¨åœ¨æå‡å¤šè¯­è¨€å¤§æ¨¡å‹(MLLMs)åœ¨å¤å¸Œè…Šé™¶å™¨åˆ†æä¸­çš„è¡¨ç°ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ„å»ºé—®é¢˜ç±»å‹çš„åˆ†ç±»æ³•ï¼Œè¯†åˆ«æ¨¡å‹åœ¨ç‰¹å®šç±»å‹ä¸Šçš„æ€§èƒ½å·®è·ï¼Œå¹¶ä½¿ç”¨æ¡ä»¶å¥–åŠ±è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œå®ç°äº†é£æ ¼åˆ†ç±»å’Œå†å²å½’å±çš„æœ€æ–°æˆæœã€‚VaseVQAæ˜¯ä¸€ä¸ªåŒ…å«31,773å¼ å›¾åƒçš„åŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨æ·±å…¥æ¢æµ‹æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVaseVLåœ¨ç»„åˆé²æ£’æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨SFTçš„åŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯äº†åŸºäºè¯Šæ–­çš„å¥–åŠ±å·¥ç¨‹æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16548",
            "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward\n  Learning",
            "url": "https://huggingface.co/papers/2509.16548",
            "abstract": "SCAN, a self-denoising Monte Carlo framework, improves PRM performance with synthetic data, achieving high F1 scores and surpassing human-annotated baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t Process reward models (PRMs) offer fine-grained, step-level evaluations that facilitate deeper reasoning processes in large language models (LLMs), proving effective in complex tasks like mathematical reasoning. However, developing PRMs is challenging due to the high cost and limited scalability of human-annotated data. Synthetic data from Monte Carlo (MC) estimation is a promising alternative but suffers from a high noise ratio, which can cause overfitting and hinder large-scale training. In this work, we conduct a preliminary study on the noise distribution in synthetic data from MC estimation, identifying that annotation models tend to both underestimate and overestimate step correctness due to limitations in their annotation capabilities. Building on these insights, we propose Self-Denoising Monte Carlo Annotation (SCAN), an efficient data synthesis and noise-tolerant learning framework. Our key findings indicate that: (1) Even lightweight models (e.g., 1.5B parameters) can produce high-quality annotations through a self-denoising strategy, enabling PRMs to achieve superior performance with only 6% the inference cost required by vanilla MC estimation. (2) With our robust learning strategy, PRMs can effectively learn from this weak supervision, achieving a 39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using only a compact synthetic dataset, our models surpass strong baselines, including those trained on large-scale human-annotated datasets such as PRM800K. Furthermore, performance continues to improve as we scale up the synthetic data, highlighting the potential of SCAN for scalable, cost-efficient, and robust PRM training.",
            "score": 0,
            "issue_id": 6033,
            "pub_date": "2025-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "d1b79e93c49676a7",
            "authors": [
                "Yuyang Ding",
                "Xinyu Shi",
                "Juntao Li",
                "Xiaobo Liang",
                "Zhaopeng Tu",
                "Min Zhang"
            ],
            "affiliations": [
                "Soochow University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16548.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#training",
                    "#data",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ²",
                "ru": {
                    "title": "SCAN: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ PRM Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SCAN - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² (PRM). SCAN Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ¶Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞ³ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SCAN Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ, ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ PRM."
                },
                "en": {
                    "title": "SCAN: Enhancing PRM Performance with Self-Denoising Synthetic Data",
                    "desc": "This paper introduces SCAN, a self-denoising Monte Carlo framework designed to enhance the performance of Process Reward Models (PRMs) using synthetic data. The authors address the challenges of high noise levels in synthetic data, which can lead to overfitting and poor model training. By leveraging a self-denoising strategy, even smaller models can generate high-quality annotations, significantly reducing inference costs. The results show that PRMs trained with SCAN achieve substantial improvements in performance metrics, demonstrating the framework's effectiveness for scalable and efficient training."
                },
                "zh": {
                    "title": "è‡ªå»å™ªè’™ç‰¹å¡æ´›æ¡†æ¶æå‡PRMæ€§èƒ½",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºSCANçš„è‡ªå»å™ªè’™ç‰¹å¡æ´›æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„æ€§èƒ½ã€‚é€šè¿‡åˆæˆæ•°æ®ï¼ŒSCANèƒ½å¤Ÿåœ¨ä»…éœ€6%ä¼ ç»Ÿè’™ç‰¹å¡æ´›ä¼°è®¡æ¨ç†æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨è½»é‡çº§æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„æ³¨é‡Šã€‚ç ”ç©¶è¡¨æ˜ï¼ŒPRMåœ¨å¼±ç›‘ç£å­¦ä¹ ä¸‹ï¼ŒF1åˆ†æ•°ä»19.9æå‡è‡³59.1ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚SCANå±•ç¤ºäº†åœ¨åˆæˆæ•°æ®è§„æ¨¡æ‰©å¤§æ—¶ï¼ŒPRMè®­ç»ƒçš„å¯æ‰©å±•æ€§ã€æˆæœ¬æ•ˆç›Šå’Œé²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09873",
            "title": "From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI\n  Ecosystem",
            "url": "https://huggingface.co/papers/2509.09873",
            "abstract": "The study audits licenses in the Hugging Face ecosystem, revealing systemic non-compliance and proposing a rule engine to detect and resolve license conflicts in open-source AI.  \t\t\t\t\tAI-generated summary \t\t\t\t Hidden license conflicts in the open-source AI ecosystem pose serious legal and ethical risks, exposing organizations to potential litigation and users to undisclosed risk. However, the field lacks a data-driven understanding of how frequently these conflicts occur, where they originate, and which communities are most affected. We present the first end-to-end audit of licenses for datasets and models on Hugging Face, as well as their downstream integration into open-source software applications, covering 364 thousand datasets, 1.6 million models, and 140 thousand GitHub projects. Our empirical analysis reveals systemic non-compliance in which 35.5% of model-to-application transitions eliminate restrictive license clauses by relicensing under permissive terms. In addition, we prototype an extensible rule engine that encodes almost 200 SPDX and model-specific clauses for detecting license conflicts, which can solve 86.4% of license conflicts in software applications. To support future research, we release our dataset and the prototype engine. Our study highlights license compliance as a critical governance challenge in open-source AI and provides both the data and tools necessary to enable automated, AI-aware compliance at scale.",
            "score": 0,
            "issue_id": 6030,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "ccff3e22dddfc1aa",
            "authors": [
                "James Jewitt",
                "Hao Li",
                "Bram Adams",
                "Gopi Krishnan Rajbahadur",
                "Ahmed E. Hassan"
            ],
            "affiliations": [
                "School of Computing, Queens University, Kingston, ON, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09873.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#dataset",
                    "#ethics"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ğ¹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ˜Ğ˜: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ°ÑƒĞ´Ğ¸Ñ‚ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ğ¹ Ğ² ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Hugging Face, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğµ Ğ½ĞµÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 364 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², 1,6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ 140 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° GitHub. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 35,5% Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿ÑƒĞ½ĞºÑ‚Ñ‹ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ğ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€ĞµĞ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ğ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ 86,4% Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Ensuring License Compliance in Open-Source AI",
                    "desc": "This paper examines the licensing issues within the Hugging Face ecosystem, identifying significant non-compliance with open-source licenses. It highlights that 35.5% of transitions from models to applications ignore restrictive license terms, which can lead to legal and ethical problems. The authors introduce a rule engine that can detect and resolve these license conflicts, successfully addressing 86.4% of issues identified. By providing a comprehensive dataset and tools, the study aims to enhance license compliance in the open-source AI community."
                },
                "zh": {
                    "title": "å¼€æºAIè®¸å¯è¯åˆè§„æ€§ï¼šæŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬ç ”ç©¶å®¡è®¡äº†Hugging Faceç”Ÿæ€ç³»ç»Ÿä¸­çš„è®¸å¯è¯ï¼Œæ­ç¤ºäº†ç³»ç»Ÿæ€§çš„åˆè§„æ€§é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§è§„åˆ™å¼•æ“æ¥æ£€æµ‹å’Œè§£å†³å¼€æºAIä¸­çš„è®¸å¯è¯å†²çªã€‚ç ”ç©¶è¡¨æ˜ï¼Œ35.5%çš„æ¨¡å‹åˆ°åº”ç”¨ç¨‹åºçš„è½¬ç§»é€šè¿‡é‡æ–°è®¸å¯åœ¨å®½æ¾æ¡æ¬¾ä¸‹æ¶ˆé™¤äº†é™åˆ¶æ€§è®¸å¯è¯æ¡æ¬¾ã€‚æˆ‘ä»¬å¯¹364,000ä¸ªæ•°æ®é›†ã€1.6ç™¾ä¸‡ä¸ªæ¨¡å‹å’Œ140,000ä¸ªGitHubé¡¹ç›®è¿›è¡Œäº†é¦–æ¬¡ç«¯åˆ°ç«¯çš„è®¸å¯è¯å®¡è®¡ï¼Œå‘ç°äº†æ½œåœ¨çš„æ³•å¾‹å’Œä¼¦ç†é£é™©ã€‚æˆ‘ä»¬çš„è§„åˆ™å¼•æ“èƒ½å¤Ÿæ£€æµ‹è¿‘200ä¸ªSPDXå’Œç‰¹å®šæ¨¡å‹æ¡æ¬¾çš„è®¸å¯è¯å†²çªï¼Œè§£å†³äº†86.4%çš„è½¯ä»¶åº”ç”¨ä¸­çš„è®¸å¯è¯å†²çªã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-22.html",
    "link_next": "2025-09-24.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "22.09",
        "en": "09/22",
        "zh": "9æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "24.09",
        "en": "09/24",
        "zh": "9æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 5,
        "#benchmark": 15,
        "#agents": 4,
        "#cv": 1,
        "#rl": 6,
        "#rlhf": 2,
        "#rag": 3,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 2,
        "#video": 3,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 8,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 2,
        "#agi": 2,
        "#games": 3,
        "#interpretability": 3,
        "#reasoning": 9,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 20,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 3,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 1
    }
}