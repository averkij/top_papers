{
    "date": {
        "ru": "23 сентября",
        "en": "September 23",
        "zh": "9月23日"
    },
    "time_utc": "2025-09-23 07:11",
    "weekday": 1,
    "issue_id": 6034,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.17567",
            "title": "LIMI: Less is More for Agency",
            "url": "https://huggingface.co/papers/2509.17567",
            "abstract": "LIMI demonstrates that sophisticated agentic intelligence can emerge from minimal, strategically curated demonstrations, outperforming data-intensive models on agency benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t We define Agency as the emergent capacity of AI systems to function as autonomous agents actively discovering problems, formulating hypotheses, and executing solutions through self-directed engagement with environments and tools. This fundamental capability marks the dawn of the Age of AI Agency, driven by a critical industry shift: the urgent need for AI systems that don't just think, but work. While current AI excels at reasoning and generating responses, industries demand autonomous agents that can execute tasks, operate tools, and drive real-world outcomes. As agentic intelligence becomes the defining characteristic separating cognitive systems from productive workers, efficiently cultivating machine autonomy becomes paramount. Current approaches assume that more data yields better agency, following traditional scaling laws from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is More for Intelligent Agency) demonstrates that agency follows radically different development principles. Through strategic focus on collaborative software development and scientific research workflows, we show that sophisticated agentic intelligence can emerge from minimal but strategically curated demonstrations of autonomous behavior. Using only 78 carefully designed training samples, LIMI achieves 73.5% on comprehensive agency benchmarks, dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%), DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%). Most strikingly, LIMI demonstrates 53.7% improvement over models trained on 10,000 samples-achieving superior agentic intelligence with 128 times fewer samples. Our findings establish the Agency Efficiency Principle: machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.",
            "score": 40,
            "issue_id": 6032,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "abed9c1916ee6dc8",
            "authors": [
                "Yang Xiao",
                "Mohan Jiang",
                "Jie Sun",
                "Keyu Li",
                "Jifan Lin",
                "Yumin Zhuang",
                "Ji Zeng",
                "Shijie Xia",
                "Qishuo Hua",
                "Xuefeng Li",
                "Xiaojie Cai",
                "Tongyu Wang",
                "Yue Zhang",
                "Liming Liu",
                "Xia Wu",
                "Jinlong Hou",
                "Yuan Cheng",
                "Wenjie Li",
                "Xiang Wang",
                "Dequan Wang",
                "Pengfei Liu"
            ],
            "affiliations": [
                "GAIR",
                "PolyU",
                "SII",
                "SJTU",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17567.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#agi"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Меньше данных, больше агентности: революция в обучении ИИ",
                    "desc": "Статья представляет новый подход к обучению агентного интеллекта под названием LIMI (Less Is More for Intelligent Agency). В отличие от традиционных методов, основанных на больших объемах данных, LIMI демонстрирует, что сложный агентный интеллект может возникнуть из минимального, но стратегически подобранного набора демонстраций. Используя всего 78 тщательно разработанных обучающих примеров, LIMI достигает 73.5% на комплексных тестах агентности, значительно превосходя современные модели. Исследование устанавливает Принцип Эффективности Агентности: автономность машин возникает не из обилия данных, а из стратегического отбора высококачественных агентных демонстраций."
                },
                "en": {
                    "title": "Less Data, More Agency: Redefining AI Intelligence",
                    "desc": "LIMI introduces a new approach to developing agentic intelligence in AI systems, showing that high-quality, strategically curated demonstrations can lead to better performance than traditional data-intensive methods. The paper defines agency as the ability of AI to autonomously identify problems, create hypotheses, and implement solutions. By using only 78 carefully selected training samples, LIMI significantly outperforms existing models that rely on larger datasets, achieving a remarkable 73.5% on agency benchmarks. This research challenges the conventional belief that more data is always better, establishing the Agency Efficiency Principle, which emphasizes the importance of quality over quantity in training AI for autonomous tasks."
                },
                "zh": {
                    "title": "少即是多：自主智能的新范式",
                    "desc": "LIMI展示了复杂的自主智能可以通过最小化、战略性策划的示范而出现，超越了数据密集型模型在自主性基准测试中的表现。我们将自主性定义为人工智能系统作为自主代理的能力，能够主动发现问题、制定假设并通过自我引导与环境和工具的互动来执行解决方案。当前的研究表明，机器自主性并非来自数据的丰富，而是来自高质量自主行为示范的战略性策划。通过仅使用78个精心设计的训练样本，LIMI在全面的自主性基准测试中达到了73.5%的成绩，显著优于其他最先进的模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17627",
            "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion\n  Transformer Models",
            "url": "https://huggingface.co/papers/2509.17627",
            "abstract": "OmniInsert addresses challenges in mask-free video insertion using a novel data pipeline, feature injection, progressive training, and context-aware rephrasing, outperforming commercial solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video insertion based on diffusion models are impressive. However, existing methods rely on complex control signals but struggle with subject consistency, limiting their practical applicability. In this paper, we focus on the task of Mask-free Video Insertion and aim to resolve three key challenges: data scarcity, subject-scene equilibrium, and insertion harmonization. To address the data scarcity, we propose a new data pipeline InsertPipe, constructing diverse cross-pair data automatically. Building upon our data pipeline, we develop OmniInsert, a novel unified framework for mask-free video insertion from both single and multiple subject references. Specifically, to maintain subject-scene equilibrium, we introduce a simple yet effective Condition-Specific Feature Injection mechanism to distinctly inject multi-source conditions and propose a novel Progressive Training strategy that enables the model to balance feature injection from subjects and source video. Meanwhile, we design the Subject-Focused Loss to improve the detailed appearance of the subjects. To further enhance insertion harmonization, we propose an Insertive Preference Optimization methodology to optimize the model by simulating human preferences, and incorporate a Context-Aware Rephraser module during reference to seamlessly integrate the subject into the original scenes. To address the lack of a benchmark for the field, we introduce InsertBench, a comprehensive benchmark comprising diverse scenes with meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert outperforms state-of-the-art closed-source commercial solutions. The code will be released.",
            "score": 37,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "15e2a15d37cb464c",
            "authors": [
                "Jinshu Chen",
                "Xinghui Li",
                "Xu Bai",
                "Tianxiang Ma",
                "Pengze Zhang",
                "Zhuowei Chen",
                "Gen Li",
                "Lijie Liu",
                "Songtao Zhao",
                "Bingchuan Li",
                "Qian He"
            ],
            "affiliations": [
                "Intelligent Creation Lab, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17627.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#benchmark",
                    "#optimization",
                    "#video",
                    "#open_source",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Умная вставка в видео без масок",
                    "desc": "OmniInsert - это новая система для вставки объектов в видео без использования масок. Она решает проблемы нехватки данных, баланса между объектом и сценой, а также гармоничной интеграции. Система использует инновационный конвейер данных, прогрессивное обучение и контекстно-зависимое перефразирование. OmniInsert превосходит коммерческие решения на новом бенчмарке InsertBench."
                },
                "en": {
                    "title": "Seamless Video Insertion with OmniInsert",
                    "desc": "OmniInsert is a novel framework designed for mask-free video insertion that tackles key challenges such as data scarcity and subject-scene equilibrium. It introduces a new data pipeline called InsertPipe to automatically create diverse training data, enhancing the model's learning capabilities. The framework employs Condition-Specific Feature Injection and Progressive Training to ensure that the inserted subjects harmonize well with the original video scenes. Additionally, it features a Context-Aware Rephraser and a Subject-Focused Loss to improve the visual quality and integration of subjects, outperforming existing commercial solutions in evaluations."
                },
                "zh": {
                    "title": "无掩码视频插入的新突破",
                    "desc": "OmniInsert是一种新颖的无掩码视频插入方法，旨在解决数据稀缺、主体场景平衡和插入和谐性等关键挑战。我们提出了InsertPipe数据管道，自动构建多样化的交叉配对数据，以应对数据稀缺问题。通过条件特定特征注入机制和渐进训练策略，OmniInsert能够有效地平衡来自不同来源的特征注入。最终，我们设计了插入偏好优化方法和上下文感知重述模块，以提高插入的和谐性，使主体更自然地融入原始场景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18091",
            "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System",
            "url": "https://huggingface.co/papers/2509.18091",
            "abstract": "OnePiece integrates LLM-style context engineering and reasoning into industrial search and recommendation systems, achieving significant improvements in key business metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the growing interest in replicating the scaled success of large language models (LLMs) in industrial search and recommender systems, most existing industrial efforts remain limited to transplanting Transformer architectures, which bring only incremental improvements over strong Deep Learning Recommendation Models (DLRMs). From a first principle perspective, the breakthroughs of LLMs stem not only from their architectures but also from two complementary mechanisms: context engineering, which enriches raw input queries with contextual cues to better elicit model capabilities, and multi-step reasoning, which iteratively refines model outputs through intermediate reasoning paths. However, these two mechanisms and their potential to unlock substantial improvements remain largely underexplored in industrial ranking systems.   In this paper, we propose OnePiece, a unified framework that seamlessly integrates LLM-style context engineering and reasoning into both retrieval and ranking models of industrial cascaded pipelines. OnePiece is built on a pure Transformer backbone and further introduces three key innovations: (1) structured context engineering, which augments interaction history with preference and scenario signals and unifies them into a structured tokenized input sequence for both retrieval and ranking; (2) block-wise latent reasoning, which equips the model with multi-step refinement of representations and scales reasoning bandwidth via block size; (3) progressive multi-task training, which leverages user feedback chains to effectively supervise reasoning steps during training. OnePiece has been deployed in the main personalized search scenario of Shopee and achieves consistent online gains across different key business metrics, including over +2% GMV/UU and a +2.90% increase in advertising revenue.",
            "score": 23,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "a4d1025bbd0ac828",
            "authors": [
                "Sunhao Dai",
                "Jiakai Tang",
                "Jiahua Wu",
                "Kun Wang",
                "Yuxuan Zhu",
                "Bingjun Chen",
                "Bangyang Hong",
                "Yu Zhao",
                "Cong Fu",
                "Kangle Wu",
                "Yabo Ni",
                "Anxiang Zeng",
                "Wenjie Wang",
                "Xu Chen",
                "Jun Xu",
                "See-Kiong Ng"
            ],
            "affiliations": [
                "National University of Singapore",
                "Renmin University of China",
                "Shopee",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18091.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Объединяя мощь LLM и промышленных рекомендательных систем",
                    "desc": "OnePiece - это унифицированная система, интегрирующая методы контекстной инженерии и рассуждений, характерные для больших языковых моделей, в промышленные системы поиска и рекомендаций. Она использует структурированную контекстную инженерию, блочное латентное рассуждение и прогрессивное многозадачное обучение. Система построена на чистой архитектуре трансформера и была успешно внедрена в персонализированный поиск Shopee. OnePiece показала значительное улучшение ключевых бизнес-метрик, включая рост GMV/UU и доходов от рекламы."
                },
                "en": {
                    "title": "Unlocking Search Potential with LLM-inspired Innovations",
                    "desc": "OnePiece is a novel framework that enhances industrial search and recommendation systems by incorporating techniques from large language models (LLMs). It focuses on two main mechanisms: context engineering, which enriches input queries with relevant contextual information, and multi-step reasoning, which refines outputs through iterative processes. The framework introduces structured context engineering, block-wise latent reasoning, and progressive multi-task training to improve model performance. As a result, OnePiece has shown significant improvements in key business metrics, such as increased gross merchandise value and advertising revenue."
                },
                "zh": {
                    "title": "OnePiece：提升搜索与推荐的智能框架",
                    "desc": "OnePiece 是一个将大语言模型（LLM）风格的上下文工程和推理机制整合到工业搜索和推荐系统中的框架。它通过结构化的上下文工程增强用户的交互历史，并将其转化为统一的输入序列，从而提高检索和排序的效果。此外，OnePiece 采用块级潜在推理，允许模型通过多步推理逐步优化输出。该框架在 Shopee 的个性化搜索场景中应用，显著提升了多个关键业务指标。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18056",
            "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs",
            "url": "https://huggingface.co/papers/2509.18056",
            "abstract": "TempSamp-R1, a reinforcement fine-tuning framework, enhances multimodal large language models for video temporal grounding by using off-policy supervision and a hybrid Chain-of-Thought training paradigm, achieving state-of-the-art performance on benchmark datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions (R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: https://github.com/HVision-NKU/TempSamp-R1",
            "score": 18,
            "issue_id": 6033,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "8d946cb9cc09008c",
            "authors": [
                "Yunheng Li",
                "Jing Cheng",
                "Shaoyong Jia",
                "Hangyi Kuang",
                "Shaohui Jiao",
                "Qibin Hou",
                "Ming-Ming Cheng"
            ],
            "affiliations": [
                "ByteDance Inc.",
                "VCIP, School of Computer Science, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18056.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#rag",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "TempSamp-R1: Прорыв в точности временной локализации видео",
                    "desc": "TempSamp-R1 - это новая система обучения с подкреплением для улучшения мультимодальных больших языковых моделей в задаче временной локализации видео. Она использует off-policy обучение и гибридную парадигму Chain-of-Thought для более эффективного поиска временных интервалов. TempSamp-R1 применяет нелинейный метод вычисления мягкого преимущества для стабилизации обучения. Система достигает наилучших результатов на нескольких бенчмарках, превосходя существующие подходы."
                },
                "en": {
                    "title": "TempSamp-R1: Revolutionizing Video Temporal Grounding with Off-Policy Supervision",
                    "desc": "This paper presents TempSamp-R1, a novel reinforcement fine-tuning framework aimed at enhancing multimodal large language models (MLLMs) for video temporal grounding tasks. It addresses the inefficiencies of existing methods that rely on on-policy sampling by utilizing off-policy supervision from ground-truth annotations, which helps in achieving more accurate temporal solutions. Additionally, TempSamp-R1 incorporates a non-linear soft advantage computation to stabilize training and improve reward feedback. The framework also employs a hybrid Chain-of-Thought training paradigm, allowing it to efficiently manage varying reasoning complexities and outperform previous state-of-the-art methods on benchmark datasets."
                },
                "zh": {
                    "title": "TempSamp-R1：视频时间定位的新突破",
                    "desc": "本文介绍了TempSamp-R1，这是一种新的强化微调框架，旨在提高多模态大语言模型在视频时间定位任务中的有效性。我们发现现有的强化学习方法，如组相对策略优化（GRPO），依赖于策略更新的在线采样，这在大时间搜索空间的任务中效率低下且性能有限。为了解决这个问题，TempSamp-R1利用真实标签作为离线监督，提供时间上精确的指导，有效弥补了在线解决方案中的稀疏性和不对齐问题。实验结果表明，TempSamp-R1在多个基准数据集上超越了GRPO基线，建立了新的最先进性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17765",
            "title": "Qwen3-Omni Technical Report",
            "url": "https://huggingface.co/papers/2509.17765",
            "abstract": "Qwen3-Omni, a multimodal model, achieves state-of-the-art performance across text, image, audio, and video, using a Thinker-Talker MoE architecture and a lightweight causal ConvNet for efficient streaming synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.",
            "score": 15,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "464827796e5c676c",
            "authors": [
                "Jin Xu",
                "Zhifang Guo",
                "Hangrui Hu",
                "Yunfei Chu",
                "Xiong Wang",
                "Jinzheng He",
                "Yuxuan Wang",
                "Xian Shi",
                "Ting He",
                "Xinfa Zhu",
                "Yuanjun Lv",
                "Yongqi Wang",
                "Dake Guo",
                "He Wang",
                "Linhan Ma",
                "Pei Zhang",
                "Xinyu Zhang",
                "Hongkun Hao",
                "Zishan Guo",
                "Baosong Yang",
                "Bin Zhang",
                "Ziyang Ma",
                "Xipin Wei",
                "Shuai Bai",
                "Keqin Chen",
                "Xuejing Liu",
                "Peng Wang",
                "Mingkun Yang",
                "Dayiheng Liu",
                "Xingzhang Ren",
                "Bo Zheng",
                "Rui Men",
                "Fan Zhou",
                "Bowen Yu",
                "Jianxin Yang",
                "Le Yu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17765.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#audio",
                    "#multimodal",
                    "#architecture",
                    "#open_source",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Qwen3-Omni: Единая мультимодальная модель для ИИ нового поколения",
                    "desc": "Qwen3-Omni - это мультимодальная модель, достигающая передовых результатов в обработке текста, изображений, аудио и видео. Она использует архитектуру Thinker-Talker MoE для унификации восприятия и генерации контента разных модальностей. Модель поддерживает текстовое взаимодействие на 119 языках, распознавание речи на 19 языках и генерацию речи на 10 языках. Для снижения задержки при потоковом синтезе речи используется легковесная каузальная сверточная нейронная сеть."
                },
                "en": {
                    "title": "Unifying Multimodal Mastery with Qwen3-Omni",
                    "desc": "Qwen3-Omni is a cutting-edge multimodal model that excels in processing text, images, audio, and video simultaneously without losing performance compared to single-modal models. It utilizes a Thinker-Talker MoE architecture to integrate perception and generation, achieving state-of-the-art results in various audio and audio-visual tasks. The model is designed for efficient streaming synthesis, significantly reducing latency by employing a lightweight causal ConvNet and a multi-codebook scheme for speech codecs. Additionally, it introduces a Thinking model for enhanced multimodal reasoning and provides a specialized audio captioning capability, making it a versatile tool for diverse applications."
                },
                "zh": {
                    "title": "多模态模型的全能之选",
                    "desc": "Qwen3-Omni是一种多模态模型，首次在文本、图像、音频和视频上实现了最先进的性能，而没有相对于单模态模型的性能下降。该模型采用Thinker-Talker MoE架构，统一了文本、图像、音频和视频的感知与生成，特别在音频任务上表现优异。Qwen3-Omni在36个音频和音频-视觉基准测试中，取得了32个基准的开源最优性能，并在22个基准上达到了整体最优，超越了许多强大的闭源模型。为了提高流媒体合成的效率，Qwen3-Omni使用轻量级因果卷积网络，显著降低了首次数据包的延迟。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17437",
            "title": "GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric\n  Reasoning",
            "url": "https://huggingface.co/papers/2509.17437",
            "abstract": "A two-stage reinforcement learning framework improves geometric reasoning and problem-solving in multimodal language models by first enhancing visual perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in reinforcement learning (RL) have enhanced the reasoning abilities of large language models (LLMs), yet the impact on multimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like geometric reasoning, MLLMs hallucinate frequently, leading to inaccurate reasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps the benefits of reasoning training. To quantify this, we design a Geo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric concepts and spatial relationships. Experiments on GeoPQA reveal significant shortcomings of MLLMs in visual perception, which constrain RL reward signals for effective training. To address this bottleneck, we propose a two-stage RL training framework by first enhancing the visual perception of geometric structures, then fostering reasoning capabilities. Applied to Qwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by 9.7% and geometric problem solving by 9.1%, compared to the direct reasoning training approach. Our method also generalizes to other vision-intensive domains like figure understanding, highlighting the importance of perceptual grounding in effective MLLM reasoning.",
            "score": 12,
            "issue_id": 6033,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "bd0dcbd688b7cc83",
            "authors": [
                "Guizhen Chen",
                "Weiwen Xu",
                "Hao Zhang",
                "Hou Pong Chan",
                "Deli Zhao",
                "Anh Tuan Luu",
                "Yu Rong"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17437.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#hallucinations",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "📐",
                "ru": {
                    "title": "Двухэтапное обучение с подкреплением улучшает геометрические рассуждения в мультимодальных ИИ",
                    "desc": "Статья представляет двухэтапный подход к обучению с подкреплением для улучшения геометрических рассуждений в мультимодальных языковых моделях. Авторы разработали бенчмарк GeoPQA для оценки восприятия геометрических концепций. Предложенный метод сначала улучшает визуальное восприятие, а затем развивает способности к рассуждению. Применение этого подхода к модели Qwen2.5-VL-3B-Instruct показало значительное улучшение в геометрических рассуждениях и решении задач."
                },
                "en": {
                    "title": "Enhancing Visual Perception for Better Geometric Reasoning in MLLMs",
                    "desc": "This paper presents a two-stage reinforcement learning framework aimed at improving geometric reasoning in multimodal language models (MLLMs). The authors identify a perceptual bottleneck that limits the effectiveness of reasoning training in MLLMs, particularly in tasks requiring visual understanding. They introduce a benchmark called Geo-Perception Question-Answering (GeoPQA) to evaluate the visual perception capabilities of MLLMs. By first enhancing visual perception and then focusing on reasoning, their approach significantly boosts performance in geometric reasoning and problem-solving tasks."
                },
                "zh": {
                    "title": "提升多模态模型的几何推理能力",
                    "desc": "本文提出了一种两阶段的强化学习框架，旨在改善多模态语言模型（MLLMs）在几何推理和问题解决方面的能力。研究发现，MLLMs在视觉感知上存在瓶颈，导致在几何推理任务中频繁出现错误。为了解决这一问题，作者设计了Geo-Perception Question-Answering（GeoPQA）基准测试，评估模型在基本几何概念和空间关系上的表现。通过增强视觉感知后再进行推理训练，实验结果显示该方法在几何推理和问题解决上分别提高了9.7%和9.1%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18084",
            "title": "ByteWrist: A Parallel Robotic Wrist Enabling Flexible and\n  Anthropomorphic Motion for Confined Spaces",
            "url": "https://huggingface.co/papers/2509.18084",
            "abstract": "This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic parallel wrist for robotic manipulation. ByteWrist addresses the critical limitations of existing serial and parallel wrists in narrow-space operations through a compact three-stage parallel drive mechanism integrated with arc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw) motion while maintaining exceptional compactness, making it particularly suitable for complex unstructured environments such as home services, medical assistance, and precision assembly. The key innovations include: (1) a nested three-stage motor-driven linkages that minimize volume while enabling independent multi-DOF control, (2) arc-shaped end linkages that optimize force transmission and expand motion range, and (3) a central supporting ball functioning as a spherical joint that enhances structural stiffness without compromising flexibility. Meanwhile, we present comprehensive kinematic modeling including forward / inverse kinematics and a numerical Jacobian solution for precise control. Empirically, we observe ByteWrist demonstrates strong performance in narrow-space maneuverability and dual-arm cooperative manipulation tasks, outperforming Kinova-based systems. Results indicate significant improvements in compactness, efficiency, and stiffness compared to traditional designs, establishing ByteWrist as a promising solution for next-generation robotic manipulation in constrained environments.",
            "score": 10,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "5aa65074f4ca8fcb",
            "authors": [
                "Jiawen Tian",
                "Liqun Huang",
                "Zhongren Cui",
                "Jingchao Qiao",
                "Jiafeng Xu",
                "Xiao Ma",
                "Zeyu Ren"
            ],
            "affiliations": [
                "Bytedance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18084.jpg",
            "data": {
                "categories": [
                    "#robotics"
                ],
                "emoji": "🦾",
                "ru": {
                    "title": "ByteWrist: Революция в роботизированных запястьях для узких пространств",
                    "desc": "Статья представляет ByteWrist - новое высокогибкое и антропоморфное параллельное запястье для роботизированных манипуляций. ByteWrist решает критические ограничения существующих последовательных и параллельных запястий в операциях в узких пространствах с помощью компактного трехступенчатого параллельного приводного механизма, интегрированного с дугообразными концевыми звеньями. Ключевые инновации включают вложенные трехступенчатые моторизованные звенья, дугообразные концевые звенья и центральный опорный шар, функционирующий как сферический шарнир. Эмпирические результаты показывают, что ByteWrist демонстрирует высокую производительность в задачах маневрирования в узких пространствах и кооперативной манипуляции двумя руками, превосходя системы на базе Kinova."
                },
                "en": {
                    "title": "ByteWrist: Revolutionizing Robotic Manipulation in Tight Spaces",
                    "desc": "This paper presents ByteWrist, an innovative robotic wrist designed for flexible and efficient manipulation in tight spaces. It features a compact three-stage parallel drive mechanism that allows for precise Roll-Pitch-Yaw (RPY) motion, making it ideal for complex tasks in unstructured environments. Key advancements include multi-degree-of-freedom control through nested linkages and arc-shaped end linkages that enhance force transmission. The paper also details kinematic modeling techniques for accurate control and demonstrates ByteWrist's superior performance in narrow-space tasks compared to existing systems."
                },
                "zh": {
                    "title": "ByteWrist：狭小空间中的灵活机器人腕关节",
                    "desc": "本文介绍了一种新型的高灵活性和类人并行腕关节，名为ByteWrist，旨在解决现有串行和并行腕关节在狭小空间操作中的关键限制。ByteWrist采用紧凑的三阶段并行驱动机制，结合弧形末端连杆，实现了精确的滚转-俯仰-偏航（RPY）运动，同时保持了卓越的紧凑性，特别适合复杂的非结构化环境，如家庭服务、医疗辅助和精密组装。其主要创新包括：嵌套的三阶段电机驱动连杆，最小化体积并实现独立的多自由度控制；优化力传输和扩展运动范围的弧形末端连杆；以及作为球形关节的中央支撑球，增强结构刚度而不影响灵活性。此外，本文还提供了全面的运动学建模，包括正/逆运动学和数值雅可比解，以实现精确控制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17396",
            "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
            "url": "https://huggingface.co/papers/2509.17396",
            "abstract": "EpiCache is a KV cache management framework for long conversational question answering that reduces memory usage and improves accuracy through block-wise prefill, episodic KV compression, and adaptive layer-wise budget allocation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have extended context lengths, enabling assistants to sustain long histories for coherent, personalized responses. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly dominates under strict resource constraints. An active line of research for reducing this overhead is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting entries after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to degraded accuracy in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent baselines, sustains near-full KV accuracy under 4-6x compression, and reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient multi-turn interaction under strict resource constraints.",
            "score": 9,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "10727268d6361b72",
            "authors": [
                "Minsoo Kim",
                "Arnav Kundu",
                "Han-Byul Kim",
                "Richa Dixit",
                "Minsik Cho"
            ],
            "affiliations": [
                "Apple",
                "Hanyang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17396.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#inference",
                    "#optimization",
                    "#long_context",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное кэширование для длительных диалогов с ИИ",
                    "desc": "EpiCache - это фреймворк управления KV-кэшем для длительных диалоговых систем вопросов и ответов. Он использует блочное предзаполнение, эпизодическое KV-сжатие и адаптивное распределение бюджета по слоям для снижения использования памяти и повышения точности. EpiCache позволяет ограничить рост кэша и сохранить релевантный контекст темы. В сравнении с существующими методами, EpiCache улучшает точность до 40% и обеспечивает эффективное многоходовое взаимодействие при строгих ресурсных ограничениях."
                },
                "en": {
                    "title": "EpiCache: Efficient Memory Management for Long Conversations",
                    "desc": "EpiCache is a framework designed to manage Key-Value (KV) caches for long conversational question answering, aiming to reduce memory usage while enhancing accuracy. It employs block-wise prefill and episodic KV compression to maintain relevant context without excessive memory growth. The framework also features an adaptive layer-wise budget allocation that optimizes memory distribution based on each layer's sensitivity to eviction. Overall, EpiCache significantly improves performance in multi-turn conversations, achieving higher accuracy and lower latency under strict resource limitations."
                },
                "zh": {
                    "title": "EpiCache：高效的长对话问答缓存管理",
                    "desc": "EpiCache是一个用于长对话问答的键值缓存管理框架，旨在减少内存使用并提高准确性。它通过块级预填充、情节键值压缩和自适应层级预算分配来实现这些目标。EpiCache能够在固定内存预算下控制缓存增长，并通过将对话历史聚类为一致的情节来保留与主题相关的上下文。实验结果表明，EpiCache在多个基准测试中提高了准确性，并显著降低了延迟和内存使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16941",
            "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering\n  Tasks?",
            "url": "https://huggingface.co/papers/2509.16941",
            "abstract": "SWE-Bench Pro is a challenging benchmark for coding models, featuring complex, enterprise-level problems that require substantial code modifications, with performance evaluations showing significant limitations in current models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SWE-Bench Pro, a substantially more challenging benchmark that builds upon the best practices of SWE-BENCH [25], but is explicitly designed to capture realistic, complex, enterprise-level problems beyond the scope of SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of 41 actively maintained repositories spanning business applications, B2B services, and developer tools. The benchmark is partitioned into a public set with open access to problems sourced from 11 repositories, a held-out set of 12 repositories and a commercial set of 18 proprietary repositories where we have formal partnership agreements with early-stage startups. Problems in the held-out and the commercial set are not publicly accessible, but we release results on the commercial set. Our benchmark features long-horizon tasks that may require hours to days for a professional software engineer to complete, often involving patches across multiple files and substantial code modifications. All tasks are human-verified and augmented with sufficient context to ensure resolvability. In our evaluation of widely used coding models, under a unified scaffold, we observe that their performance on SWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest score to date at 23.3%. To better understand these limitations, we cluster the failure modes observed in the collected agent trajectories for a clearer characterization of the error patterns exhibited by current models. Overall, SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully captures the complexity and diversity of real-world software development, advancing the pursuit of truly autonomous software engineering agents at a professional level.",
            "score": 9,
            "issue_id": 6030,
            "pub_date": "2025-09-21",
            "pub_date_card": {
                "ru": "21 сентября",
                "en": "September 21",
                "zh": "9月21日"
            },
            "hash": "e407b0b4d298f1ec",
            "authors": [
                "Xiang Deng",
                "Jeff Da",
                "Edwin Pan",
                "Yannis Yiming He",
                "Charles Ide",
                "Kanak Garg",
                "Niklas Lauffer",
                "Andrew Park",
                "Nitin Pasari",
                "Chetan Rane",
                "Karmini Sampath",
                "Maya Krishnan",
                "Srivatsa Kundurthy",
                "Sean Hendryx",
                "Zifan Wang",
                "Chen Bo Calvin Zhang",
                "Noah Jacobson",
                "Bing Liu",
                "Brad Kenstler"
            ],
            "affiliations": [
                "Scale AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16941.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#interpretability",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🧑‍💻",
                "ru": {
                    "title": "SWE-Bench Pro: Вызов для AI в реальной разработке ПО",
                    "desc": "SWE-Bench Pro - это сложный бенчмарк для моделей кодирования, содержащий комплексные задачи корпоративного уровня. Он включает 1865 проблем из 41 активно поддерживаемого репозитория, охватывающих бизнес-приложения, B2B-сервисы и инструменты разработчиков. Задачи требуют значительных модификаций кода и могут занимать у профессиональных разработчиков часы или дни. Оценка производительности показывает, что современные модели кодирования достигают менее 25% успешности (Pass@1) на этом бенчмарке."
                },
                "en": {
                    "title": "SWE-Bench Pro: Elevating the Challenge for Coding Models",
                    "desc": "SWE-Bench Pro is a new benchmark designed to evaluate coding models on complex, real-world software engineering tasks. It includes 1,865 problems from various business applications and developer tools, emphasizing long-horizon tasks that require significant code changes. The benchmark reveals that current coding models, including GPT-5, struggle to achieve high performance, with a maximum score of only 23.3%. By analyzing the failure modes of these models, SWE-Bench Pro aims to enhance our understanding of their limitations and improve the development of autonomous software engineering agents."
                },
                "zh": {
                    "title": "SWE-Bench Pro：挑战编码模型的极限",
                    "desc": "SWE-Bench Pro 是一个具有挑战性的基准测试，专为编码模型设计，涵盖复杂的企业级问题。这些问题需要进行大量的代码修改，且当前模型的表现显示出显著的局限性。基准测试包含来自41个活跃维护的代码库的1865个问题，分为公共集、保留集和商业集。通过对现有编码模型的评估，我们发现它们在SWE-Bench Pro上的表现低于25%，这表明在真实软件开发中，当前模型仍面临许多挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16117",
            "title": "DiffusionNFT: Online Diffusion Reinforcement with Forward Process",
            "url": "https://huggingface.co/papers/2509.16117",
            "abstract": "Diffusion Negative-aware FineTuning (DiffusionNFT) optimizes diffusion models directly on the forward process via flow matching, improving efficiency and performance compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Online reinforcement learning (RL) has been central to post-training language models, but its extension to diffusion models remains challenging due to intractable likelihoods. Recent works discretize the reverse sampling process to enable GRPO-style training, yet they inherit fundamental drawbacks, including solver restrictions, forward-reverse inconsistency, and complicated integration with classifier-free guidance (CFG). We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that optimizes diffusion models directly on the forward process via flow matching. DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective. This formulation enables training with arbitrary black-box solvers, eliminates the need for likelihood estimation, and requires only clean images rather than sampling trajectories for policy optimization. DiffusionNFT is up to 25times more efficient than FlowGRPO in head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 with over 5k steps and additional CFG employment. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested.",
            "score": 8,
            "issue_id": 6030,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "1d4f4b6ec61af4cd",
            "authors": [
                "Kaiwen Zheng",
                "Huayu Chen",
                "Haotian Ye",
                "Haoxiang Wang",
                "Qinsheng Zhang",
                "Kai Jiang",
                "Hang Su",
                "Stefano Ermon",
                "Jun Zhu",
                "Ming-Yu Liu"
            ],
            "affiliations": [
                "NVIDIA",
                "Stanford University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16117.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#optimization",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "DiffusionNFT: Эффективная оптимизация диффузионных моделей через прямой процесс",
                    "desc": "Статья представляет новый метод оптимизации диффузионных моделей под названием DiffusionNFT. Этот подход основан на прямом процессе и использует сопоставление потоков, что позволяет избежать проблем, связанных с оценкой вероятности. DiffusionNFT сравнивает положительные и отрицательные генерации для определения направления улучшения политики, естественно интегрируя сигналы подкрепления в цель обучения с учителем. Метод демонстрирует значительное повышение эффективности и производительности по сравнению с существующими методами, такими как FlowGRPO."
                },
                "en": {
                    "title": "Revolutionizing Diffusion Models with Efficient FineTuning",
                    "desc": "Diffusion Negative-aware FineTuning (DiffusionNFT) is a novel approach that enhances diffusion models by optimizing them directly during the forward process using flow matching. This method addresses challenges in online reinforcement learning for diffusion models, such as intractable likelihoods and inconsistencies between forward and reverse processes. By contrasting positive and negative outputs, DiffusionNFT effectively integrates reinforcement signals into the training process without needing likelihood estimation or complex sampling. The results show that DiffusionNFT is significantly more efficient than previous methods, achieving higher performance scores in fewer training steps."
                },
                "zh": {
                    "title": "扩散模型的新优化：负向微调的力量",
                    "desc": "扩散负向微调（DiffusionNFT）通过流匹配直接优化扩散模型的前向过程，从而提高了效率和性能。与现有方法相比，DiffusionNFT克服了许多挑战，如求解器限制和前向-反向不一致性。该方法通过对比正向和负向生成，定义了隐式策略改进方向，自然地将强化信号融入监督学习目标中。DiffusionNFT在效率上比FlowGRPO高出25倍，并且不需要分类器引导，显著提升了模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17158",
            "title": "ARE: Scaling Up Agent Environments and Evaluations",
            "url": "https://huggingface.co/papers/2509.17158",
            "abstract": "Meta Agents Research Environments (ARE) facilitate the creation and execution of complex environments for agent research, and Gaia2, a benchmark built on ARE, evaluates general agent capabilities in dynamic, asynchronous settings.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Meta Agents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap between model development and real-world deployment. We also propose Gaia2, a benchmark built in ARE and designed to measure general agent capabilities. Beyond search and execution, Gaia2 requires agents to handle ambiguities and noise, adapt to dynamic environments, collaborate with other agents, and operate under temporal constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new failure modes that are invisible in static settings. Our experiments show that no system dominates across the intelligence spectrum: stronger reasoning often comes at the cost of efficiency, and budget scaling curves plateau, highlighting the need for new architectures and adaptive compute strategies. Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2 to other environments, empowering the community to rapidly create new benchmarks tailored to their domains. In AI's second half, progress increasingly depends on defining meaningful tasks and robust evaluations to drive frontier capabilities forward.",
            "score": 6,
            "issue_id": 6031,
            "pub_date": "2025-09-21",
            "pub_date_card": {
                "ru": "21 сентября",
                "en": "September 21",
                "zh": "9月21日"
            },
            "hash": "b2f18867a3844e4b",
            "authors": [
                "Pierre Andrews",
                "Amine Benhalloum",
                "Gerard Moreno-Torres Bertran",
                "Matteo Bettini",
                "Amar Budhiraja",
                "Ricardo Silveira Cabral",
                "Virginie Do",
                "Romain Froger",
                "Emilien Garreau",
                "Jean-Baptiste Gaya",
                "Hugo Laurençon",
                "Maxime Lecanu",
                "Kunal Malkan",
                "Dheeraj Mekala",
                "Pierre Ménard",
                "Grégoire Mialon",
                "Ulyana Piterbarg",
                "Mikhail Plekhanov",
                "Mathieu Rita",
                "Andrey Rusakov",
                "Thomas Scialom",
                "Vladislav Vorotilov",
                "Mengjue Wang",
                "Ian Yu"
            ],
            "affiliations": [
                "Meta Superintelligence Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17158.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#agi",
                    "#optimization",
                    "#transfer_learning",
                    "#games",
                    "#architecture"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ARE и Gaia2: новые горизонты в исследовании интеллектуальных агентов",
                    "desc": "Исследователи представили Meta Agents Research Environments (ARE) - платформу для создания сложных сред для исследования агентов искусственного интеллекта. На базе ARE разработан бенчмарк Gaia2, оценивающий общие возможности агентов в динамических асинхронных условиях. Gaia2 требует от агентов адаптации к изменяющейся среде, сотрудничества и работы в условиях временных ограничений. Эксперименты показали, что ни одна система не доминирует во всем спектре задач, что указывает на необходимость разработки новых архитектур ИИ."
                },
                "en": {
                    "title": "Empowering Agent Research with Dynamic Environments and Robust Benchmarks",
                    "desc": "Meta Agents Research Environments (ARE) is a platform designed to create and manage complex environments for agent research, allowing for the integration of both synthetic and real applications. It simplifies the process of building diverse environments with unique rules and tools, facilitating the transition from model development to real-world applications. The Gaia2 benchmark, developed within ARE, assesses general agent capabilities in dynamic and asynchronous settings, requiring agents to adapt to uncertainties and collaborate effectively. The findings indicate that no single system excels across all intelligence measures, emphasizing the need for innovative architectures and adaptive strategies in agent design."
                },
                "zh": {
                    "title": "元代理研究环境：推动智能代理的进步",
                    "desc": "本文介绍了元代理研究环境（ARE），这是一个用于可扩展创建环境的研究平台，能够集成合成或真实应用，并执行代理协调。ARE提供简单的抽象，帮助构建复杂多样的环境，每个环境都有自己的规则、工具、内容和验证器，从而缩小模型开发与实际部署之间的差距。我们还提出了基于ARE构建的基准Gaia2，旨在测量代理在动态环境中的一般能力。Gaia2要求代理处理模糊性和噪声，适应动态环境，与其他代理协作，并在时间限制下操作，展示了在静态设置中无法发现的新失败模式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16596",
            "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from\n  Token and Parameter Levels",
            "url": "https://huggingface.co/papers/2509.16596",
            "abstract": "Supervised fine-tuning of large language models can negatively impact closed-book question answering performance, with up to 90% of parameter updates not contributing to knowledge enhancement.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) acquire substantial world knowledge during pre-training, which is further shaped by post-training techniques such as supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge remains underexplored, limiting our ability to control knowledge change behavior in fine-tuned models. To address this gap, we evaluate closed-book question answering (CBQA) performance across five LLMs from the LLaMA-2 and LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying the level of knowledge mastery in the fine-tuning data leads to performance fluctuations of over 12%. To investigate these effects, we analyze model behavior at both the token and parameter levels. Our analysis reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these updates can improve performance on the CBQA task, depending on the characteristics of the fine-tuning data. These insights offer practical guidance for developing fine-tuning strategies that more effectively strengthen model knowledge.",
            "score": 6,
            "issue_id": 6030,
            "pub_date": "2025-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "ad73526f5b38ef1d",
            "authors": [
                "Junjie Ye",
                "Yuming Yang",
                "Yang Nan",
                "Shuo Li",
                "Qi Zhang",
                "Tao Gui",
                "Xuanjing Huang",
                "Peng Wang",
                "Zhongchao Shi",
                "Jianping Fan"
            ],
            "affiliations": [
                "Fudan University",
                "Lenovo Research, Beijing, China",
                "Shanghai Innovation Institute",
                "Shanghai Key Lab of Intelligent Information Processing"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16596.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Осторожно с дообучением: больше не всегда лучше для языковых моделей",
                    "desc": "Исследование показывает, что контролируемая дообучение больших языковых моделей может негативно влиять на их способность отвечать на вопросы без доступа к внешней информации. Анализ моделей семейств LLaMA-2 и LLaMA-3 выявил, что увеличение объема данных для дообучения может ухудшить производительность на 14%. Обнаружено, что до 90% обновлений параметров во время дообучения не способствуют улучшению знаний модели. Результаты исследования предлагают практические рекомендации по разработке стратегий дообучения для более эффективного усиления знаний модели."
                },
                "en": {
                    "title": "Optimize Fine-Tuning to Preserve Knowledge in Language Models",
                    "desc": "This paper investigates how supervised fine-tuning (SFT) affects the knowledge retention of large language models (LLMs) during closed-book question answering (CBQA). The authors find that a significant portion of parameter updates during SFT, up to 90%, do not enhance the model's knowledge, leading to performance drops. They demonstrate that fine-tuning on fewer samples can sometimes yield better results than on larger datasets, indicating that the quality of fine-tuning data is crucial. Their findings provide valuable insights for optimizing fine-tuning strategies to improve knowledge retention in LLMs."
                },
                "zh": {
                    "title": "优化微调策略，提升模型知识",
                    "desc": "这篇论文探讨了大型语言模型在监督微调（SFT）过程中对闭卷问答（CBQA）性能的影响。研究发现，微调过程中高达90%的参数更新并未提升模型的知识水平，甚至在某些情况下，微调样本数量的增加反而导致性能下降。通过分析模型在标记和参数层面的行为，作者揭示了微调数据的知识掌握程度对模型性能的显著影响。该研究为优化微调策略以增强模型知识提供了实用指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17985",
            "title": "VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video\n  Diffusion Models",
            "url": "https://huggingface.co/papers/2509.17985",
            "abstract": "VideoFrom3D synthesizes high-quality 3D scene videos using a combination of image and video diffusion models, achieving style consistency without requiring paired datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we propose VideoFrom3D, a novel framework for synthesizing high-quality 3D scene videos from coarse geometry, a camera trajectory, and a reference image. Our approach streamlines the 3D graphic design workflow, enabling flexible design exploration and rapid production of deliverables. A straightforward approach to synthesizing a video from coarse geometry might condition a video diffusion model on geometric structure. However, existing video diffusion models struggle to generate high-fidelity results for complex scenes due to the difficulty of jointly modeling visual quality, motion, and temporal consistency. To address this, we propose a generative framework that leverages the complementary strengths of image and video diffusion models. Specifically, our framework consists of a Sparse Anchor-view Generation (SAG) and a Geometry-guided Generative Inbetweening (GGI) module. The SAG module generates high-quality, cross-view consistent anchor views using an image diffusion model, aided by Sparse Appearance-guided Sampling. Building on these anchor views, GGI module faithfully interpolates intermediate frames using a video diffusion model, enhanced by flow-based camera control and structural guidance. Notably, both modules operate without any paired dataset of 3D scene models and natural images, which is extremely difficult to obtain. Comprehensive experiments show that our method produces high-quality, style-consistent scene videos under diverse and challenging scenarios, outperforming simple and extended baselines.",
            "score": 5,
            "issue_id": 6032,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "1ca4bfe39743e387",
            "authors": [
                "Geonung Kim",
                "Janghyeok Han",
                "Sunghyun Cho"
            ],
            "affiliations": [
                "POSTECH, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17985.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VideoFrom3D: Синтез реалистичных видео из грубой 3D-геометрии",
                    "desc": "Статья представляет VideoFrom3D - новый метод синтеза высококачественных видео 3D-сцен с использованием диффузионных моделей для изображений и видео. Подход сочетает генерацию ключевых кадров высокого качества с помощью модели диффузии изображений и интерполяцию промежуточных кадров с помощью модели диффузии видео. VideoFrom3D не требует наборов парных данных 3D-моделей и реальных изображений. Эксперименты показывают, что метод превосходит базовые подходы в создании согласованных по стилю видео для разнообразных сложных сцен."
                },
                "en": {
                    "title": "Transforming 3D Designs into Stunning Videos!",
                    "desc": "VideoFrom3D is a new framework that creates high-quality 3D scene videos using image and video diffusion models. It allows for the generation of videos from basic 3D shapes, camera paths, and reference images without needing matched datasets. The framework includes two main components: Sparse Anchor-view Generation (SAG) for creating consistent anchor views and Geometry-guided Generative Inbetweening (GGI) for generating smooth transitions between frames. This innovative approach results in visually appealing and coherent videos, even in complex scenarios, outperforming existing methods."
                },
                "zh": {
                    "title": "VideoFrom3D：高质量3D场景视频合成的新方法",
                    "desc": "本文提出了一种新颖的框架VideoFrom3D，用于从粗糙几何体、相机轨迹和参考图像合成高质量的3D场景视频。该方法结合了图像和视频扩散模型的优势，简化了3D图形设计工作流程，支持灵活的设计探索和快速的交付生产。通过稀疏锚视图生成模块(SAG)和几何引导生成插值模块(GGI)，该框架能够生成风格一致的高质量视频，而无需配对的3D场景模型和自然图像数据集。实验结果表明，该方法在多样化和具有挑战性的场景下，生成的场景视频质量高且风格一致，优于简单和扩展的基线方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17671",
            "title": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG\n  Applications",
            "url": "https://huggingface.co/papers/2509.17671",
            "abstract": "Turk-LettuceDetect, a suite of hallucination detection models for Turkish RAG applications, achieves high performance using fine-tuned encoder architectures on a machine-translated RAGTruth dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t The widespread adoption of Large Language Models (LLMs) has been hindered by their tendency to hallucinate, generating plausible but factually incorrect information. While Retrieval-Augmented Generation (RAG) systems attempt to address this issue by grounding responses in external knowledge, hallucination remains a persistent challenge, particularly for morphologically complex, low-resource languages like Turkish. This paper introduces Turk-LettuceDetect, the first suite of hallucination detection models specifically designed for Turkish RAG applications. Building on the LettuceDetect framework, we formulate hallucination detection as a token-level classification task and fine-tune three distinct encoder architectures: a Turkish-specific ModernBERT, TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a machine-translated version of the RAGTruth benchmark dataset containing 17,790 instances across question answering, data-to-text generation, and summarization tasks. Our experimental results show that the ModernBERT-based model achieves an F1-score of 0.7266 on the complete test set, with particularly strong performance on structured tasks. The models maintain computational efficiency while supporting long contexts up to 8,192 tokens, making them suitable for real-time deployment. Comparative analysis reveals that while state-of-the-art LLMs demonstrate high recall, they suffer from low precision due to over-generation of hallucinated content, underscoring the necessity of specialized detection mechanisms. By releasing our models and translated dataset, this work addresses a critical gap in multilingual NLP and establishes a foundation for developing more reliable and trustworthy AI applications for Turkish and other languages.",
            "score": 4,
            "issue_id": 6034,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "0985231370fd3145",
            "authors": [
                "Selva Taş",
                "Mahmut El Huseyni",
                "Özay Ezerceli",
                "Reyhan Bayraktar",
                "Fatma Betül Terzioğlu"
            ],
            "affiliations": [
                "Newmind AI Istanbul, Turkiye"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17671.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#open_source",
                    "#architecture",
                    "#long_context",
                    "#hallucinations",
                    "#low_resource",
                    "#rag",
                    "#dataset"
                ],
                "emoji": "🦃",
                "ru": {
                    "title": "Борьба с галлюцинациями в турецком языке: точность и эффективность Turk-LettuceDetect",
                    "desc": "Эта статья представляет Turk-LettuceDetect - набор моделей для обнаружения галлюцинаций в турецких RAG-приложениях. Модели основаны на дообученных энкодерных архитектурах и достигают высокой производительности на машинно-переведенном наборе данных RAGTruth. Исследователи формулируют задачу обнаружения галлюцинаций как токен-уровневую классификацию и сравнивают три различные модели: ModernBERT, TurkEmbed4STS и EuroBERT. Результаты показывают, что модель на основе ModernBERT достигает F1-оценки 0,7266 на полном тестовом наборе, демонстрируя особенно хорошие результаты на структурированных задачах."
                },
                "en": {
                    "title": "Detecting Hallucinations in Turkish RAG: Turk-LettuceDetect",
                    "desc": "The paper presents Turk-LettuceDetect, a set of models designed to detect hallucinations in Turkish Retrieval-Augmented Generation (RAG) applications. It addresses the challenge of Large Language Models (LLMs) generating incorrect information, particularly in low-resource languages like Turkish. The authors fine-tune three encoder architectures on a machine-translated RAGTruth dataset, treating hallucination detection as a token-level classification task. Experimental results show that the ModernBERT-based model achieves a high F1-score, demonstrating its effectiveness in real-time applications while highlighting the need for specialized detection mechanisms in multilingual NLP."
                },
                "zh": {
                    "title": "土耳其语幻觉检测的创新之路",
                    "desc": "本文介绍了Turk-LettuceDetect，这是一个专为土耳其语检索增强生成（RAG）应用设计的幻觉检测模型套件。该模型通过对机器翻译的RAGTruth数据集进行微调，使用了三种不同的编码器架构，旨在提高对土耳其语的幻觉检测能力。实验结果表明，基于ModernBERT的模型在完整测试集上达到了0.7266的F1分数，尤其在结构化任务上表现优异。通过发布这些模型和翻译数据集，本文填补了多语言自然语言处理中的关键空白，为开发更可靠的AI应用奠定了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15709",
            "title": "Understanding Embedding Scaling in Collaborative Filtering",
            "url": "https://huggingface.co/papers/2509.15709",
            "abstract": "Large-scale experiments reveal double-peak and logarithmic performance patterns in collaborative filtering models as embedding dimensions scale, and provide theoretical insights into their causes.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling recommendation models into large recommendation models has become one of the most widely discussed topics. Recent efforts focus on components beyond the scaling embedding dimension, as it is believed that scaling embedding may lead to performance degradation. Although there have been some initial observations on embedding, the root cause of their non-scalability remains unclear. Moreover, whether performance degradation occurs across different types of models and datasets is still an unexplored area. Regarding the effect of embedding dimensions on performance, we conduct large-scale experiments across 10 datasets with varying sparsity levels and scales, using 4 representative classical architectures. We surprisingly observe two novel phenomenon: double-peak and logarithmic. For the former, as the embedding dimension increases, performance first improves, then declines, rises again, and eventually drops. For the latter, it exhibits a perfect logarithmic curve. Our contributions are threefold. First, we discover two novel phenomena when scaling collaborative filtering models. Second, we gain an understanding of the underlying causes of the double-peak phenomenon. Lastly, we theoretically analyze the noise robustness of collaborative filtering models, with results matching empirical observations.",
            "score": 3,
            "issue_id": 6033,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "5728feeb510e7393",
            "authors": [
                "Zhuangzhuang He",
                "Zhou Kaiyu",
                "Haoyue Bai",
                "Fengbin Zhu",
                "Yonghui Yang"
            ],
            "affiliations": [
                "ASU",
                "NTU",
                "NUS",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15709.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Неожиданные закономерности в масштабировании рекомендательных систем",
                    "desc": "Исследователи провели масштабные эксперименты с моделями коллаборативной фильтрации, варьируя размерность эмбеддингов. Они обнаружили два новых феномена: двойной пик и логарифмическую зависимость производительности от размерности. Первый феномен характеризуется улучшением, затем ухудшением, повторным улучшением и окончательным падением производительности при увеличении размерности. Авторы также предоставили теоретическое обоснование наблюдаемых явлений и проанализировали устойчивость моделей к шуму."
                },
                "en": {
                    "title": "Unveiling Performance Patterns in Collaborative Filtering Models",
                    "desc": "This paper investigates how the size of embedding dimensions in collaborative filtering models affects their performance. Through large-scale experiments on various datasets, the authors identify two unique performance patterns: double-peak and logarithmic. The double-peak pattern shows that performance can improve and then decline as embedding dimensions increase, while the logarithmic pattern indicates a steady performance curve. The study also provides theoretical insights into why these phenomena occur and explores the noise robustness of these models."
                },
                "zh": {
                    "title": "揭示协同过滤模型的双峰与对数性能现象",
                    "desc": "本研究通过大规模实验揭示了协同过滤模型在嵌入维度扩展时的双峰和对数性能模式，并提供了其原因的理论见解。我们观察到，随着嵌入维度的增加，模型性能先提升后下降，再次上升，最后又下降，形成双峰现象。同时，性能还呈现出完美的对数曲线。我们的贡献在于发现了这两种新现象，理解了双峰现象的根本原因，并理论分析了协同过滤模型的噪声鲁棒性，结果与经验观察相符。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.15248",
            "title": "Synthetic bootstrapped pretraining",
            "url": "https://huggingface.co/papers/2509.15248",
            "abstract": "Synthetic Bootstrapped Pretraining (SBP) enhances language model performance by learning inter-document correlations and synthesizing new training data, leading to significant improvements over standard pretraining methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM) pretraining procedure that first learns a model of relations between documents from the pretraining dataset and then leverages it to synthesize a vast new corpus for joint training. While the standard pretraining teaches LMs to learn causal correlations among tokens within a single document, it is not designed to efficiently model the rich, learnable inter-document correlations that can potentially lead to better performance. We validate SBP by designing a compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T tokens from scratch. We find SBP consistently improves upon a strong repetition baseline and delivers a significant fraction of performance improvement attainable by an oracle upper bound with access to 20x more unique data. Qualitative analysis reveals that the synthesized documents go beyond mere paraphrases -- SBP first abstracts a core concept from the seed material and then crafts a new narration on top of it. Besides strong empirical performance, SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns to abstract the latent concepts shared between related documents.",
            "score": 3,
            "issue_id": 6030,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "99a93d8361bcbcf6",
            "authors": [
                "Zitong Yang",
                "Aonan Zhang",
                "Hong Liu",
                "Tatsunori Hashimoto",
                "Emmanuel Candès",
                "Chong Wang",
                "Ruoming Pang"
            ],
            "affiliations": [
                "Apple",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.15248.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#synthetic",
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Синтетическое предобучение: новый подход к улучшению языковых моделей",
                    "desc": "Статья представляет новый метод предобучения языковых моделей под названием Synthetic Bootstrapped Pretraining (SBP). SBP сначала обучается моделировать отношения между документами из набора данных для предобучения, а затем использует эту модель для синтеза нового обширного корпуса. Этот подход позволяет языковым моделям эффективнее учитывать междокументные корреляции, что потенциально ведет к улучшению производительности. Эксперименты показали, что SBP превосходит стандартные методы предобучения и обеспечивает значительную долю улучшения производительности, достижимого при использовании в 20 раз большего объема уникальных данных."
                },
                "en": {
                    "title": "Unlocking Language Models with Inter-Document Insights",
                    "desc": "Synthetic Bootstrapped Pretraining (SBP) is a novel approach that enhances language model performance by focusing on the relationships between different documents rather than just within a single document. It first learns inter-document correlations from the pretraining dataset and then uses this knowledge to create a large amount of new training data. This method allows the model to capture richer contextual information, leading to significant performance improvements compared to traditional pretraining techniques. Additionally, SBP's ability to synthesize documents that abstract core concepts demonstrates its effectiveness in generating diverse and informative training examples."
                },
                "zh": {
                    "title": "合成自举预训练：提升语言模型的新方法",
                    "desc": "合成自举预训练（SBP）通过学习文档之间的关系并合成新的训练数据，提升了语言模型的性能。与传统的预训练方法不同，SBP能够有效建模文档间的丰富相关性，从而实现更好的表现。我们通过设计计算匹配的预训练设置，验证了SBP的有效性，并在从零开始的情况下对一个3B参数的模型进行了预训练。实验结果表明，SBP在性能上显著超越了强基线，并接近于理想情况下的性能上限。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.18095",
            "title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late\n  Interaction",
            "url": "https://huggingface.co/papers/2509.18095",
            "abstract": "MetaEmbed, a new framework for multimodal retrieval, uses learnable Meta Tokens to provide compact yet expressive multi-vector embeddings, enabling scalable and efficient retrieval performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Universal multimodal embedding models have achieved great success in capturing semantic relevance between queries and candidates. However, current methods either condense queries and candidates into a single vector, potentially limiting the expressiveness for fine-grained information, or produce too many vectors that are prohibitively expensive for multi-vector retrieval. In this work, we introduce MetaEmbed, a new framework for multimodal retrieval that rethinks how multimodal embeddings are constructed and interacted with at scale. During training, a fixed number of learnable Meta Tokens are appended to the input sequence. At test-time, their last-layer contextualized representations serve as compact yet expressive multi-vector embeddings. Through the proposed Matryoshka Multi-Vector Retrieval training, MetaEmbed learns to organize information by granularity across multiple vectors. As a result, we enable test-time scaling in multimodal retrieval, where users can balance retrieval quality against efficiency demands by selecting the number of tokens used for indexing and retrieval interactions. Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed achieves state-of-the-art retrieval performance while scaling robustly to models with 32B parameters.",
            "score": 2,
            "issue_id": 6032,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "dce11b4bc2712ec3",
            "authors": [
                "Zilin Xiao",
                "Qi Ma",
                "Mengting Gu",
                "Chun-cheng Jason Chen",
                "Xintao Chen",
                "Vicente Ordonez",
                "Vijai Mohan"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "Rice University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.18095.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#optimization",
                    "#rag",
                    "#games"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "MetaEmbed: Масштабируемые мультимодальные вложения для эффективного поиска",
                    "desc": "MetaEmbed - новая структура для мультимодального поиска, использующая обучаемые Мета-Токены для создания компактных, но выразительных многовекторных вложений. Этот подход позволяет организовать информацию по уровням детализации в нескольких векторах, обеспечивая масштабируемость и эффективность при поиске. MetaEmbed применяет метод обучения Matryoshka Multi-Vector Retrieval, что дает возможность балансировать между качеством и эффективностью поиска. Модель показала высокие результаты на бенчмарках MMEB и ViDoRe, демонстрируя масштабируемость до 32 миллиардов параметров."
                },
                "en": {
                    "title": "MetaEmbed: Efficient Multimodal Retrieval with Learnable Meta Tokens",
                    "desc": "MetaEmbed is a novel framework designed for multimodal retrieval that enhances the way embeddings are created and utilized. It introduces learnable Meta Tokens, which allow for the generation of compact yet expressive multi-vector embeddings, improving retrieval efficiency. By employing a training method called Matryoshka Multi-Vector Retrieval, MetaEmbed organizes information by granularity, enabling users to adjust the balance between retrieval quality and efficiency. Evaluations on benchmark datasets demonstrate that MetaEmbed achieves top-tier performance while effectively scaling to large models with billions of parameters."
                },
                "zh": {
                    "title": "MetaEmbed：高效的多模态检索新框架",
                    "desc": "MetaEmbed是一种新的多模态检索框架，使用可学习的Meta Tokens来提供紧凑而富有表现力的多向量嵌入，从而实现可扩展和高效的检索性能。现有方法通常将查询和候选项压缩为单个向量，限制了细粒度信息的表达，或者生成过多向量，导致多向量检索成本过高。MetaEmbed通过在训练过程中将固定数量的可学习Meta Tokens附加到输入序列中，重新思考了多模态嵌入的构建和交互方式。通过Matryoshka多向量检索训练，MetaEmbed能够根据信息的细粒度组织多个向量，从而在检索时实现可扩展性，用户可以根据需求选择用于索引和检索交互的Token数量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17641",
            "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge\n  without Hearing?",
            "url": "https://huggingface.co/papers/2509.17641",
            "abstract": "AuditoryBench++ and AIR-CoT enhance text-only models' auditory reasoning and knowledge integration, outperforming existing models in multimodal interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, a comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIR-CoT, a novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io.",
            "score": 2,
            "issue_id": 6032,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "d97bd061de9c7180",
            "authors": [
                "Hyunjong Ok",
                "Suho Yoo",
                "Hyeonjun Kim",
                "Jaeho Lee"
            ],
            "affiliations": [
                "HJ AILAB",
                "Korea Advanced Institute of Science and Technology, South Korea",
                "Pohang University of Science and Technology, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17641.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#benchmark",
                    "#audio",
                    "#reasoning"
                ],
                "emoji": "🎧",
                "ru": {
                    "title": "Улучшение слухового рассуждения в текстовых ИИ-моделях",
                    "desc": "Статья представляет AuditoryBench++, комплексный бенчмарк для оценки слухового знания и рассуждения в текстовых моделях. Авторы также предлагают AIR-CoT - новый метод рассуждения о слуховом воображении, который генерирует и интегрирует слуховую информацию во время вывода. Эксперименты показывают, что AIR-CoT превосходит как базовые модели, так и модели с дополнительными слуховыми знаниями. Это исследование направлено на улучшение способности языковых моделей рассуждать о слуховых свойствах без прямого восприятия звуков."
                },
                "en": {
                    "title": "Enhancing Auditory Reasoning in Text Models",
                    "desc": "This paper introduces AuditoryBench++, a benchmark designed to evaluate how well text-only models understand and reason about auditory concepts. It highlights the limitations of current language models in processing auditory information, which is crucial for effective multimodal interactions. The authors also present AIR-CoT, a new method that enhances these models by integrating auditory reasoning during inference. Through extensive experiments, they show that AIR-CoT significantly improves performance compared to existing models, demonstrating the importance of auditory knowledge in language understanding."
                },
                "zh": {
                    "title": "提升文本模型的听觉推理能力",
                    "desc": "本论文提出了AuditoryBench++和AIR-CoT，旨在提升文本模型的听觉推理和知识整合能力。AuditoryBench++是一个全面的基准测试，评估文本模型在听觉知识和推理方面的表现，涵盖从基本的听觉比较到上下文相关的推理任务。AIR-CoT是一种新颖的听觉想象推理方法，通过特殊标记和知识注入，在推理过程中生成和整合听觉信息。实验结果表明，AIR-CoT在多模态交互中优于现有的模型，显示出更强的听觉推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17336",
            "title": "Mano Report",
            "url": "https://huggingface.co/papers/2509.17336",
            "abstract": "A robust GUI agent, Mano, integrates reinforcement learning with vision-language models for high-fidelity data generation and improved performance on GUI benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical user interfaces (GUIs) are the primary medium for human-computer interaction, yet automating GUI interactions remains challenging due to the complexity of visual elements, dynamic environments, and the need for multi-step reasoning. Existing methods based on vision-language models (VLMs) often suffer from limited resolution, domain mismatch, and insufficient sequential decisionmaking capability. To address these issues, we propose Mano, a robust GUI agent built upon a multi-modal foundation model pre-trained on extensive web and computer system data. Our approach integrates a novel simulated environment for high-fidelity data generation, a three-stage training pipeline (supervised fine-tuning, offline reinforcement learning, and online reinforcement learning), and a verification module for error recovery. Mano demonstrates state-of-the-art performance on multiple GUI benchmarks, including Mind2Web and OSWorld, achieving significant improvements in success rate and operational accuracy. Our work provides new insights into the effective integration of reinforcement learning with VLMs for practical GUI agent deployment, highlighting the importance of domain-specific data, iterative training, and holistic reward design.",
            "score": 2,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "379006ad6024a25b",
            "authors": [
                "Tianyu Fu",
                "Anyang Su",
                "Chenxu Zhao",
                "Hanning Wang",
                "Minghui Wu",
                "Zhe Yu",
                "Fei Hu",
                "Mingjia Shi",
                "Wei Dong",
                "Jiayao Wang",
                "Yuyang Chen",
                "Ruiyang Yu",
                "Siran Peng",
                "Menglin Li",
                "Nan Huang",
                "Haitian Wei",
                "Jiawei Yu",
                "Yi Xin",
                "Xilin Zhao",
                "Kai Gu",
                "Ping Jiang",
                "Sifan Zhou",
                "Shuo Wang"
            ],
            "affiliations": [
                "Mininglamp Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17336.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#agents",
                    "#rl",
                    "#games",
                    "#multimodal",
                    "#rlhf",
                    "#benchmark",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Mano: ИИ-агент нового поколения для автоматизации графических интерфейсов",
                    "desc": "Статья представляет Mano - надежного GUI-агента, интегрирующего обучение с подкреплением и визуально-языковые модели. Mano использует симулированную среду для генерации высококачественных данных и трехэтапный процесс обучения. Агент демонстрирует улучшенные результаты на нескольких эталонных тестах для GUI, включая Mind2Web и OSWorld. Исследование показывает эффективность интеграции обучения с подкреплением и визуально-языковых моделей для практического применения GUI-агентов."
                },
                "en": {
                    "title": "Mano: Revolutionizing GUI Automation with Reinforcement Learning and Vision-Language Models",
                    "desc": "This paper presents Mano, a GUI agent that combines reinforcement learning with vision-language models to enhance data generation and performance on GUI tasks. The authors identify challenges in automating GUI interactions, such as visual complexity and the need for multi-step reasoning, which existing methods struggle to address. Mano utilizes a multi-modal foundation model and a three-stage training process that includes supervised fine-tuning and both offline and online reinforcement learning. The results show that Mano achieves state-of-the-art performance on various benchmarks, emphasizing the importance of tailored data and comprehensive training strategies in developing effective GUI agents."
                },
                "zh": {
                    "title": "Mano：强化学习与视觉语言模型的完美结合",
                    "desc": "本文介绍了一种名为Mano的强大图形用户界面（GUI）代理，它将强化学习与视觉语言模型结合，以生成高保真数据并提高GUI基准测试的性能。现有的视觉语言模型在处理复杂的视觉元素和动态环境时常常面临分辨率有限和决策能力不足的问题。为了解决这些问题，Mano采用了多模态基础模型，并通过一个新颖的模拟环境进行高保真数据生成，结合三阶段的训练流程。Mano在多个GUI基准测试中表现出色，显著提高了成功率和操作准确性，展示了强化学习与视觉语言模型有效结合的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17818",
            "title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context\n  Enrichment",
            "url": "https://huggingface.co/papers/2509.17818",
            "abstract": "ContextFlow, a training-free framework for Diffusion Transformers, enhances video object editing by using a high-order Rectified Flow solver and Adaptive Context Enrichment to achieve precise, temporally consistent, and high-fidelity object manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Training-free video object editing aims to achieve precise object-level manipulation, including object insertion, swapping, and deletion. However, it faces significant challenges in maintaining fidelity and temporal consistency. Existing methods, often designed for U-Net architectures, suffer from two primary limitations: inaccurate inversion due to first-order solvers, and contextual conflicts caused by crude \"hard\" feature replacement. These issues are more challenging in Diffusion Transformers (DiTs), where the unsuitability of prior layer-selection heuristics makes effective guidance challenging. To address these limitations, we introduce ContextFlow, a novel training-free framework for DiT-based video object editing. In detail, we first employ a high-order Rectified Flow solver to establish a robust editing foundation. The core of our framework is Adaptive Context Enrichment (for specifying what to edit), a mechanism that addresses contextual conflicts. Instead of replacing features, it enriches the self-attention context by concatenating Key-Value pairs from parallel reconstruction and editing paths, empowering the model to dynamically fuse information. Additionally, to determine where to apply this enrichment (for specifying where to edit), we propose a systematic, data-driven analysis to identify task-specific vital layers. Based on a novel Guidance Responsiveness Metric, our method pinpoints the most influential DiT blocks for different tasks (e.g., insertion, swapping), enabling targeted and highly effective guidance. Extensive experiments show that ContextFlow significantly outperforms existing training-free methods and even surpasses several state-of-the-art training-based approaches, delivering temporally coherent, high-fidelity results.",
            "score": 1,
            "issue_id": 6030,
            "pub_date": "2025-09-22",
            "pub_date_card": {
                "ru": "22 сентября",
                "en": "September 22",
                "zh": "9月22日"
            },
            "hash": "e6039499f6d1d2dc",
            "authors": [
                "Yiyang Chen",
                "Xuanhua He",
                "Xiujun Ma",
                "Yue Ma"
            ],
            "affiliations": [
                "State Key Laboratory of General Artificial Intelligence, Peking University, Beijing, China",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17818.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "ContextFlow: Прорыв в редактировании видео без обучения",
                    "desc": "ContextFlow - это новая система для редактирования объектов в видео без дополнительного обучения, основанная на диффузионных трансформерах. Она использует высокоточный решатель Rectified Flow и механизм адаптивного обогащения контекста для точного и согласованного манипулирования объектами. ContextFlow решает проблемы предыдущих методов, такие как неточная инверсия и конфликты контекста. Система превосходит существующие подходы без обучения и даже некоторые современные методы с обучением, обеспечивая высококачественные результаты."
                },
                "en": {
                    "title": "Revolutionizing Video Editing with ContextFlow!",
                    "desc": "ContextFlow is a novel framework designed for video object editing using Diffusion Transformers without the need for training. It addresses key challenges in maintaining high fidelity and temporal consistency during object manipulation tasks like insertion and swapping. By utilizing a high-order Rectified Flow solver and Adaptive Context Enrichment, it enhances the editing process by dynamically fusing information from different paths instead of simply replacing features. The framework also employs a data-driven approach to identify the most effective layers for specific editing tasks, leading to superior performance compared to existing methods."
                },
                "zh": {
                    "title": "无训练视频对象编辑的新突破",
                    "desc": "ContextFlow 是一个无训练的框架，专为扩散变换器（Diffusion Transformers）设计，旨在提升视频对象编辑的精确性和一致性。它通过高阶修正流求解器和自适应上下文丰富机制，解决了对象插入、交换和删除中的时间一致性和保真度问题。与传统方法相比，ContextFlow 通过动态融合信息，避免了特征替换带来的上下文冲突。实验结果表明，ContextFlow 在无训练方法中表现优异，甚至超越了一些基于训练的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16633",
            "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for\n  Efficient Visual Question Answering using Small VLMs",
            "url": "https://huggingface.co/papers/2509.16633",
            "abstract": "The Model Parity Aligner (MPA) framework improves Small Vision-Language Models (S-VLMs) by leveraging unlabeled images and knowledge transfer from Large Vision-Language Models (L-VLMs) to reduce performance gaps in vision and language tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Vision-Language Models (L-VLMs) have demonstrated remarkable performance in various vision and language tasks, including visual question answering (VQA). However, their high computational cost makes them impractical for resource-constrained settings and inference-heavy applications. In contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer from a significant performance gap compared to their larger counterparts. In this work, we introduce the Model Parity Aligner (MPA), a novel framework designed to systematically improve S-VLMs by leveraging unlabeled images and effective knowledge transfer from L-VLMs. Instead of traditional knowledge distillation methods that rely on labeled training data, MPA employs a strategic parity-based approach that precisely identifies the knowledge disparities between S-VLMs and L-VLMs, and optimizes training by targeting only these disparities. We conduct extensive experiments on four diverse VQA benchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires specialized reasoning capabilities such as text recognition, chart interpretation, and commonsense and factual understanding. Our results demonstrate that MPA consistently enhances the performance of S-VLMs on all benchmarks, reducing the performance gap while maintaining computational efficiency. We make our code publicly available.",
            "score": 1,
            "issue_id": 6032,
            "pub_date": "2025-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "f51723c28433f366",
            "authors": [
                "Abhirama Subramanyam Penamakuri",
                "Navlika Singh",
                "Piyush Arora",
                "Anand Mishra"
            ],
            "affiliations": [
                "Indian Institute of Technology Jodhpur"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16633.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#small_models",
                    "#transfer_learning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Выравнивание малых и больших мультимодальных моделей для эффективного визуального понимания",
                    "desc": "Модель Model Parity Aligner (MPA) предлагает новый подход к улучшению малых моделей визуального и языкового понимания (S-VLMs). MPA использует неразмеченные изображения и эффективный перенос знаний от больших моделей (L-VLMs) для сокращения разрыва в производительности. Метод фокусируется на точном выявлении различий в знаниях между S-VLMs и L-VLMs, оптимизируя обучение только для этих различий. Эксперименты на четырех наборах данных для визуальных вопросно-ответных систем показали значительное улучшение производительности S-VLMs при сохранении вычислительной эффективности."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Small Models with Big Model Insights",
                    "desc": "The Model Parity Aligner (MPA) framework enhances Small Vision-Language Models (S-VLMs) by utilizing unlabeled images and transferring knowledge from Large Vision-Language Models (L-VLMs). This approach addresses the performance gap between S-VLMs and L-VLMs without relying on labeled data, focusing instead on identifying and optimizing specific knowledge disparities. MPA employs a parity-based strategy to improve training efficiency while maintaining the computational advantages of S-VLMs. Extensive experiments on various visual question answering benchmarks show that MPA significantly boosts S-VLM performance across diverse reasoning tasks."
                },
                "zh": {
                    "title": "提升小型视觉语言模型的性能",
                    "desc": "本文提出了一种名为模型平衡对齐器（MPA）的框架，旨在通过利用未标记图像和从大型视觉语言模型（L-VLMs）转移知识来改善小型视觉语言模型（S-VLMs）的性能。传统的知识蒸馏方法依赖于标记训练数据，而MPA采用了一种基于平衡的策略，精确识别S-VLMs与L-VLMs之间的知识差距，并优化训练过程。我们在四个不同的视觉问答基准上进行了广泛的实验，结果表明MPA在所有基准上都能显著提升S-VLMs的性能，同时保持计算效率。我们的代码已公开，供研究人员使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16591",
            "title": "From Uniform to Heterogeneous: Tailoring Policy Optimization to Every\n  Token's Nature",
            "url": "https://huggingface.co/papers/2509.16591",
            "abstract": "Heterogeneous Adaptive Policy Optimization (HAPO) enhances reinforcement learning in LLMs by dynamically adapting token optimization based on entropy, improving performance across various model scales.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning has emerged as the fundamental technique for enhancing reasoning in LLMs. However, existing algorithms apply uniform optimization to all tokens, ignoring their different roles in reasoning process. To address this limitation, we introduce Heterogeneous Adaptive Policy Optimization (HAPO), a comprehensive token-aware algorithm that dynamically adapts optimization based on token entropy. For rollout sampling, we propose Adaptive Temperature Sampling, which adjusts sampling temperature in real time, promoting exploration at high-entropy tokens while preserving coherence at low-entropy ones. For advantage calculation, we introduce Token Level Group Average that normalizes advantages at token level, jointly accounting for sequence-length as in token-mean loss while preserving non-biased treatment. We then develop Differential Advantage Redistribution that leverages entropy and importance ratios to modulate rewards-adjusting updates for tokens with clear signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing aggressive probability reduction for noisy low-entropy tokens while enabling exploration for high-entropy tokens. Through systematic investigation between entropy and training dynamics, we embedded token-level treatment into every stages to achieve fine-grained control. Extensive experiments demonstrate that HAPO consistently outperforms DAPO across multiple model scales. Our code can be found in https://github.com/starriver030515/HAPO.",
            "score": 1,
            "issue_id": 6033,
            "pub_date": "2025-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "b2e9069d03b40295",
            "authors": [
                "Zheng Liu",
                "Mengjie Liu",
                "Siwei Wen",
                "Mengzhang Cai",
                "Bin Cui",
                "Conghui He",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Beihang University",
                "Peking University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16591.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптивная оптимизация токенов для улучшения обучения языковых моделей",
                    "desc": "Статья представляет новый алгоритм обучения с подкреплением для языковых моделей - Heterogeneous Adaptive Policy Optimization (HAPO). HAPO динамически адаптирует оптимизацию токенов на основе их энтропии, что позволяет улучшить процесс обучения. Алгоритм включает в себя адаптивную выборку с температурой, групповое усреднение на уровне токенов и асимметричное адаптивное отсечение. Эксперименты показывают, что HAPO превосходит существующие методы для моделей различных масштабов."
                },
                "en": {
                    "title": "Dynamic Token Optimization for Enhanced Reinforcement Learning",
                    "desc": "Heterogeneous Adaptive Policy Optimization (HAPO) is a novel approach in reinforcement learning that enhances the performance of large language models (LLMs) by adapting token optimization based on their entropy levels. Unlike traditional methods that apply uniform optimization, HAPO recognizes the varying importance of tokens in the reasoning process and adjusts the optimization dynamically. It introduces techniques like Adaptive Temperature Sampling for real-time adjustment of sampling temperature and Token Level Group Average for normalized advantage calculations. The method also incorporates Differential Advantage Redistribution and Asymmetric Adaptive Clipping to fine-tune reward updates and loss clipping, leading to improved training dynamics and overall performance across different model scales."
                },
                "zh": {
                    "title": "动态优化，提升推理能力！",
                    "desc": "异构自适应策略优化（HAPO）通过根据熵动态调整令牌优化，增强了大语言模型（LLM）中的强化学习性能。现有算法对所有令牌应用统一优化，忽视了它们在推理过程中的不同角色。HAPO是一种全面的令牌感知算法，能够实时调整采样温度，促进高熵令牌的探索，同时保持低熵令牌的一致性。通过系统的实验，HAPO在多个模型规模上始终优于现有的算法，展示了其在训练动态中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16415",
            "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes",
            "url": "https://huggingface.co/papers/2509.16415",
            "abstract": "StereoAdapter is a parameter-efficient self-supervised framework that integrates a LoRA-adapted monocular encoder with a recurrent stereo refinement module for underwater stereo depth estimation, improving accuracy and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Underwater stereo depth estimation provides accurate 3D geometry for robotics tasks such as navigation, inspection, and mapping, offering metric depth from low-cost passive cameras while avoiding the scale ambiguity of monocular methods. However, existing approaches face two critical challenges: (i) parameter-efficiently adapting large vision foundation encoders to the underwater domain without extensive labeled data, and (ii) tightly fusing globally coherent but scale-ambiguous monocular priors with locally metric yet photometrically fragile stereo correspondences. To address these challenges, we propose StereoAdapter, a parameter-efficient self-supervised framework that integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo refinement module. We further introduce dynamic LoRA adaptation for efficient rank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to enhance robustness under diverse underwater conditions. Comprehensive evaluations on both simulated and real-world benchmarks show improvements of 6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods, while real-world deployment with the BlueROV2 robot further demonstrates the consistent robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter. Website: https://aigeeksgroup.github.io/StereoAdapter.",
            "score": 1,
            "issue_id": 6032,
            "pub_date": "2025-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "5e1fea9f33bd29b1",
            "authors": [
                "Zhengri Wu",
                "Yiran Wang",
                "Yu Wen",
                "Zeyu Zhang",
                "Biao Wu",
                "Hao Tang"
            ],
            "affiliations": [
                "AI Geeks",
                "Australian Centre for Robotics",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16415.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#robotics",
                    "#benchmark",
                    "#optimization",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "🐠",
                "ru": {
                    "title": "Эффективная адаптация моделей компьютерного зрения для подводной оценки глубины",
                    "desc": "StereoAdapter - это самоконтролируемая система для оценки глубины под водой по стереоизображениям. Она объединяет монокулярный энкодер, адаптированный с помощью LoRA, и рекуррентный модуль уточнения стерео. Система эффективно настраивает большие предобученные модели компьютерного зрения на подводную среду без обширных размеченных данных. StereoAdapter демонстрирует улучшение точности на 6.11% на датасете TartanAir и 5.12% на SQUID по сравнению с современными методами."
                },
                "en": {
                    "title": "Enhancing Underwater Depth Estimation with StereoAdapter",
                    "desc": "StereoAdapter is a self-supervised framework designed for underwater stereo depth estimation, which combines a LoRA-adapted monocular encoder with a recurrent stereo refinement module. This approach addresses the challenges of adapting large vision models to underwater environments and effectively merging monocular and stereo data. By utilizing dynamic LoRA adaptation and pre-training on a synthetic dataset, it enhances the model's robustness in varying underwater conditions. The results show significant improvements in depth estimation accuracy on benchmark datasets and successful real-world application with a robotic platform."
                },
                "zh": {
                    "title": "水下深度估计的新突破：StereoAdapter",
                    "desc": "StereoAdapter 是一个高效的自监督框架，旨在提高水下立体深度估计的准确性和鲁棒性。它结合了经过 LoRA 调整的单目编码器和递归立体细化模块，能够在缺乏大量标注数据的情况下有效适应水下环境。该方法通过动态 LoRA 调整和在合成数据集上进行预训练，增强了在多样水下条件下的鲁棒性。综合评估显示，StereoAdapter 在多个基准测试中相较于现有最先进的方法有显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14856",
            "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End\n  Code Review Evaluation in Python Projects",
            "url": "https://huggingface.co/papers/2509.14856",
            "abstract": "A new benchmark, CodeFuse-CR-Bench, evaluates LLMs in repository-level code review with comprehensive, context-rich data and a novel evaluation framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated code review (CR) is a key application for Large Language Models (LLMs), but progress is hampered by a \"reality gap\": existing benchmarks evaluate models on isolated sub-tasks using simplified, context-poor data. This fails to reflect the holistic context-rich nature of real-world CR. To bridge this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601 high-quality instances from 70 Python projects covering nine Pull-Request (PR) problem domains, where each instance provides rich, multi-faceted context including the associated issue, PR details, and repository state, enabling end-to-end evaluation. Beyond superficial metrics, we also propose a novel evaluation framework that combines rule-based checks for location and syntax with model-based judgments of review quality. We present the first large-scale assessment of state-of-the-art LLMs on this comprehensive CR task. Our results establish crucial baselines and reveal that (1) no single LLM dominates all aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive performance; and (3) different LLMs exhibit varying robustness to redundant context. These findings highlight the necessity of holistic, multi-dimensional evaluation and provide actionable insights for advancing truly intelligent yet practical CR assistants.",
            "score": 1,
            "issue_id": 6033,
            "pub_date": "2025-09-18",
            "pub_date_card": {
                "ru": "18 сентября",
                "en": "September 18",
                "zh": "9月18日"
            },
            "hash": "cabf265ab40f85c9",
            "authors": [
                "Hanyang Guo",
                "Xunjin Zheng",
                "Zihan Liao",
                "Hang Yu",
                "Peng DI",
                "Ziyin Zhang",
                "Hong-Ning Dai"
            ],
            "affiliations": [
                "Ant Group",
                "Hong Kong Baptist University",
                "UNSW Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14856.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "🧑‍💻",
                "ru": {
                    "title": "CodeFuse-CR-Bench: Революция в оценке LLM для проверки кода",
                    "desc": "CodeFuse-CR-Bench - это новый эталонный тест для оценки больших языковых моделей (LLM) в области проверки кода на уровне репозитория. Он включает в себя 601 высококачественный пример из 70 проектов на Python, охватывающих 9 проблемных областей Pull Request. Тест предоставляет богатый контекст для каждого примера, включая связанные задачи, детали PR и состояние репозитория. Авторы также предлагают новую систему оценки, сочетающую проверки на основе правил с моделью оценки качества рецензирования."
                },
                "en": {
                    "title": "Bridging the Reality Gap in Code Review with CodeFuse-CR-Bench",
                    "desc": "The paper introduces CodeFuse-CR-Bench, a new benchmark designed to evaluate Large Language Models (LLMs) in the context of repository-level code review (CR). It addresses the limitations of existing benchmarks that use simplified data and isolated tasks, which do not reflect the complexity of real-world code reviews. CodeFuse-CR-Bench includes 601 instances from 70 Python projects, providing rich context such as issue details and repository state for a more comprehensive evaluation. The study also presents a novel evaluation framework that combines rule-based checks with model-based assessments, revealing that no single LLM excels in all CR aspects, with Gemini 2.5 Pro performing the best overall."
                },
                "zh": {
                    "title": "全面评估代码审查的智能助手",
                    "desc": "本文介绍了一个新的基准测试，CodeFuse-CR-Bench，用于评估大型语言模型（LLMs）在代码审查中的表现。现有的基准测试往往只关注孤立的子任务，缺乏真实场景中的丰富上下文，导致评估结果不够全面。CodeFuse-CR-Bench包含来自70个Python项目的601个高质量实例，涵盖九个拉取请求（PR）问题领域，提供多维度的上下文信息。研究结果表明，没有单一的LLM在所有代码审查方面表现优异，而Gemini 2.5 Pro在综合性能上表现最佳，强调了全面、多维度评估的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.17191",
            "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery",
            "url": "https://huggingface.co/papers/2509.17191",
            "abstract": "VaseVL, an SFT-then-RL system, enhances MLLMs for ancient Greek pottery analysis by addressing performance gaps through taxonomy-conditioned rewards, achieving state-of-the-art results in style classification and historical attribution.  \t\t\t\t\tAI-generated summary \t\t\t\t Analyzing cultural-heritage artifacts remains challenging for MLLMs: general models lack domain expertise, and SFT often overfits superficial patterns, yielding brittle reasoning for authentication and historical attribution. This raises the question of how to equip MLLMs with robust, expert-level reasoning for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns evaluation into supervision: we construct a taxonomy of question types, probe the SFT model to localize type-specific performance gaps, and optimize with type-conditioned, compositionality-oriented rewards targeting those gaps. We also release VaseVQA, a comprehensive benchmark of 31,773 images designed to probe deep understanding. Experiments show state-of-the-art results on style classification and historical attribution with marked gains in compositional robustness over SFT-only baselines, validating diagnosis-guided, taxonomy-conditioned reward engineering and providing a reusable resource for future research. Code and dataset will be available at https://github.com/AIGeeksGroup/VaseVQA.",
            "score": 0,
            "issue_id": 6033,
            "pub_date": "2025-09-21",
            "pub_date_card": {
                "ru": "21 сентября",
                "en": "September 21",
                "zh": "9月21日"
            },
            "hash": "68511afc4e6acc3e",
            "authors": [
                "Jinchao Ge",
                "Tengfei Cheng",
                "Biao Wu",
                "Zeyu Zhang",
                "Shiya Huang",
                "Judith Bishop",
                "Gillian Shepherd",
                "Meng Fang",
                "Ling Chen",
                "Yang Zhao"
            ],
            "affiliations": [
                "AI Geeks",
                "Australian Artificial Intelligence Institute",
                "La Trobe University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.17191.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#benchmark",
                    "#reasoning",
                    "#dataset",
                    "#science"
                ],
                "emoji": "🏺",
                "ru": {
                    "title": "Искусственный интеллект раскрывает тайны древнегреческих ваз",
                    "desc": "VaseVL - это система машинного обучения, сочетающая обучение с учителем и обучение с подкреплением для анализа древнегреческой керамики. Она использует таксономию вопросов и целевые награды для устранения пробелов в производительности моделей. VaseVL достигает передовых результатов в классификации стилей и исторической атрибуции артефактов. Система также включает новый набор данных VaseVQA для оценки глубокого понимания моделями древнегреческой керамики."
                },
                "en": {
                    "title": "Enhancing Ancient Pottery Analysis with VaseVL: A Smart Approach to Machine Learning",
                    "desc": "VaseVL is a machine learning system designed to improve the analysis of ancient Greek pottery by using a two-step approach: supervised fine-tuning (SFT) followed by reinforcement learning (RL). It addresses the limitations of general models that struggle with domain-specific tasks by implementing taxonomy-conditioned rewards that focus on specific types of questions. This method enhances the model's ability to classify styles and attribute historical context accurately, achieving state-of-the-art performance. Additionally, the study introduces VaseVQA, a large dataset that aids in evaluating the model's understanding and robustness in this specialized field."
                },
                "zh": {
                    "title": "VaseVL：古希腊陶器分析的智能解决方案",
                    "desc": "VaseVL是一个先进行监督学习(SFT)再进行强化学习(RL)的系统，旨在提升多语言大模型(MLLMs)在古希腊陶器分析中的表现。该系统通过构建问题类型的分类法，识别模型在特定类型上的性能差距，并使用条件奖励进行优化，从而实现了风格分类和历史归属的最新成果。VaseVQA是一个包含31,773张图像的基准数据集，旨在深入探测模型的理解能力。实验结果表明，VaseVL在组合鲁棒性方面显著优于仅使用SFT的基线模型，验证了基于诊断的奖励工程方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16548",
            "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward\n  Learning",
            "url": "https://huggingface.co/papers/2509.16548",
            "abstract": "SCAN, a self-denoising Monte Carlo framework, improves PRM performance with synthetic data, achieving high F1 scores and surpassing human-annotated baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t Process reward models (PRMs) offer fine-grained, step-level evaluations that facilitate deeper reasoning processes in large language models (LLMs), proving effective in complex tasks like mathematical reasoning. However, developing PRMs is challenging due to the high cost and limited scalability of human-annotated data. Synthetic data from Monte Carlo (MC) estimation is a promising alternative but suffers from a high noise ratio, which can cause overfitting and hinder large-scale training. In this work, we conduct a preliminary study on the noise distribution in synthetic data from MC estimation, identifying that annotation models tend to both underestimate and overestimate step correctness due to limitations in their annotation capabilities. Building on these insights, we propose Self-Denoising Monte Carlo Annotation (SCAN), an efficient data synthesis and noise-tolerant learning framework. Our key findings indicate that: (1) Even lightweight models (e.g., 1.5B parameters) can produce high-quality annotations through a self-denoising strategy, enabling PRMs to achieve superior performance with only 6% the inference cost required by vanilla MC estimation. (2) With our robust learning strategy, PRMs can effectively learn from this weak supervision, achieving a 39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using only a compact synthetic dataset, our models surpass strong baselines, including those trained on large-scale human-annotated datasets such as PRM800K. Furthermore, performance continues to improve as we scale up the synthetic data, highlighting the potential of SCAN for scalable, cost-efficient, and robust PRM training.",
            "score": 0,
            "issue_id": 6033,
            "pub_date": "2025-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "d1b79e93c49676a7",
            "authors": [
                "Yuyang Ding",
                "Xinyu Shi",
                "Juntao Li",
                "Xiaobo Liang",
                "Zhaopeng Tu",
                "Min Zhang"
            ],
            "affiliations": [
                "Soochow University",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16548.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#training",
                    "#data",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "🎲",
                "ru": {
                    "title": "SCAN: эффективное обучение PRM на синтетических данных",
                    "desc": "Статья представляет SCAN - фреймворк для самоочищающейся аннотации методом Монте-Карло, который улучшает работу моделей вознаграждения процессов (PRM). SCAN позволяет создавать качественные синтетические данные даже с помощью легких моделей, что значительно снижает вычислительные затраты. Предложенный подход превосходит базовые модели, обученные на больших наборах данных с человеческой разметкой. Результаты показывают, что SCAN обеспечивает масштабируемое, экономичное и надежное обучение PRM."
                },
                "en": {
                    "title": "SCAN: Enhancing PRM Performance with Self-Denoising Synthetic Data",
                    "desc": "This paper introduces SCAN, a self-denoising Monte Carlo framework designed to enhance the performance of Process Reward Models (PRMs) using synthetic data. The authors address the challenges of high noise levels in synthetic data, which can lead to overfitting and poor model training. By leveraging a self-denoising strategy, even smaller models can generate high-quality annotations, significantly reducing inference costs. The results show that PRMs trained with SCAN achieve substantial improvements in performance metrics, demonstrating the framework's effectiveness for scalable and efficient training."
                },
                "zh": {
                    "title": "自去噪蒙特卡洛框架提升PRM性能",
                    "desc": "本研究提出了一种名为SCAN的自去噪蒙特卡洛框架，旨在提高过程奖励模型（PRM）的性能。通过合成数据，SCAN能够在仅需6%传统蒙特卡洛估计推理成本的情况下，使用轻量级模型生成高质量的注释。研究表明，PRM在弱监督学习下，F1分数从19.9提升至59.1，显示出显著的性能提升。SCAN展示了在合成数据规模扩大时，PRM训练的可扩展性、成本效益和鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09873",
            "title": "From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI\n  Ecosystem",
            "url": "https://huggingface.co/papers/2509.09873",
            "abstract": "The study audits licenses in the Hugging Face ecosystem, revealing systemic non-compliance and proposing a rule engine to detect and resolve license conflicts in open-source AI.  \t\t\t\t\tAI-generated summary \t\t\t\t Hidden license conflicts in the open-source AI ecosystem pose serious legal and ethical risks, exposing organizations to potential litigation and users to undisclosed risk. However, the field lacks a data-driven understanding of how frequently these conflicts occur, where they originate, and which communities are most affected. We present the first end-to-end audit of licenses for datasets and models on Hugging Face, as well as their downstream integration into open-source software applications, covering 364 thousand datasets, 1.6 million models, and 140 thousand GitHub projects. Our empirical analysis reveals systemic non-compliance in which 35.5% of model-to-application transitions eliminate restrictive license clauses by relicensing under permissive terms. In addition, we prototype an extensible rule engine that encodes almost 200 SPDX and model-specific clauses for detecting license conflicts, which can solve 86.4% of license conflicts in software applications. To support future research, we release our dataset and the prototype engine. Our study highlights license compliance as a critical governance challenge in open-source AI and provides both the data and tools necessary to enable automated, AI-aware compliance at scale.",
            "score": 0,
            "issue_id": 6030,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "ccff3e22dddfc1aa",
            "authors": [
                "James Jewitt",
                "Hao Li",
                "Bram Adams",
                "Gopi Krishnan Rajbahadur",
                "Ahmed E. Hassan"
            ],
            "affiliations": [
                "School of Computing, Queens University, Kingston, ON, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09873.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#dataset",
                    "#ethics"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Скрытые конфликты лицензий в открытом ИИ: выявление и решение",
                    "desc": "Исследование проводит аудит лицензий в экосистеме Hugging Face, выявляя системное несоблюдение правил. Анализ охватывает 364 тысячи датасетов, 1,6 миллиона моделей и 140 тысяч проектов на GitHub. Результаты показывают, что 35,5% переходов от модели к приложению устраняют ограничительные пункты лицензий путем релицензирования на более свободных условиях. Авторы разработали прототип движка правил для обнаружения конфликтов лицензий, способный решить 86,4% таких конфликтов в программных приложениях."
                },
                "en": {
                    "title": "Ensuring License Compliance in Open-Source AI",
                    "desc": "This paper examines the licensing issues within the Hugging Face ecosystem, identifying significant non-compliance with open-source licenses. It highlights that 35.5% of transitions from models to applications ignore restrictive license terms, which can lead to legal and ethical problems. The authors introduce a rule engine that can detect and resolve these license conflicts, successfully addressing 86.4% of issues identified. By providing a comprehensive dataset and tools, the study aims to enhance license compliance in the open-source AI community."
                },
                "zh": {
                    "title": "开源AI许可证合规性：挑战与解决方案",
                    "desc": "本研究审计了Hugging Face生态系统中的许可证，揭示了系统性的合规性问题，并提出了一种规则引擎来检测和解决开源AI中的许可证冲突。研究表明，35.5%的模型到应用程序的转移通过重新许可在宽松条款下消除了限制性许可证条款。我们对364,000个数据集、1.6百万个模型和140,000个GitHub项目进行了首次端到端的许可证审计，发现了潜在的法律和伦理风险。我们的规则引擎能够检测近200个SPDX和特定模型条款的许可证冲突，解决了86.4%的软件应用中的许可证冲突。"
                }
            }
        }
    ],
    "link_prev": "2025-09-22.html",
    "link_next": "2025-09-24.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "22.09",
        "en": "09/22",
        "zh": "9月22日"
    },
    "short_date_next": {
        "ru": "24.09",
        "en": "09/24",
        "zh": "9月24日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 5,
        "#benchmark": 15,
        "#agents": 4,
        "#cv": 1,
        "#rl": 6,
        "#rlhf": 2,
        "#rag": 3,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 2,
        "#video": 3,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 8,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 2,
        "#agi": 2,
        "#games": 3,
        "#interpretability": 3,
        "#reasoning": 9,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 20,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 3,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 1,
        "#low_resource": 1
    }
}