{
    "date": {
        "ru": "12 –º–∞—Ä—Ç–∞",
        "en": "March 12",
        "zh": "3Êúà12Êó•"
    },
    "time_utc": "2025-03-12 03:21",
    "weekday": 2,
    "issue_id": 2654,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.07920",
            "title": "Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia",
            "url": "https://huggingface.co/papers/2503.07920",
            "abstract": "Southeast Asia (SEA) is a region of extraordinary linguistic and cultural diversity, yet it remains significantly underrepresented in vision-language (VL) research. This often results in artificial intelligence (AI) models that fail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an open-source initiative dedicated to developing high-quality, culturally relevant data for SEA languages. By involving contributors from SEA countries, SEA-VL aims to ensure better cultural relevance and diversity, fostering greater inclusivity of underrepresented languages in VL research. Beyond crowdsourcing, our initiative goes one step further in the exploration of the automatic collection of culturally relevant images through crawling and image generation. First, we find that image crawling achieves approximately ~85% cultural relevance while being more cost- and time-efficient than crowdsourcing. Second, despite the substantial progress in generative vision models, synthetic images remain unreliable in accurately reflecting SEA cultures. The generated images often fail to reflect the nuanced traditions and cultural contexts of the region. Collectively, we gather 1.28M SEA culturally-relevant images, more than 50 times larger than other existing datasets. Through SEA-VL, we aim to bridge the representation gap in SEA, fostering the development of more inclusive AI systems that authentically represent diverse cultures across SEA.",
            "score": 11,
            "issue_id": 2654,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "32c690b6ffb8b143",
            "authors": [
                "Samuel Cahyawijaya",
                "Holy Lovenia",
                "Joel Ruben Antony Moniz",
                "Tack Hwa Wong",
                "Mohammad Rifqi Farhansyah",
                "Thant Thiri Maung",
                "Frederikus Hudi",
                "David Anugraha",
                "Muhammad Ravi Shulthan Habibi",
                "Muhammad Reza Qorib",
                "Amit Agarwal",
                "Joseph Marvin Imperial",
                "Hitesh Laxmichand Patel",
                "Vicky Feliren",
                "Bahrul Ilmi Nasution",
                "Manuel Antonio Rufino",
                "Genta Indra Winata",
                "Rian Adam Rajagede",
                "Carlos Rafael Catalan",
                "Mohamed Fazli Imam",
                "Priyaranjan Pattnayak",
                "Salsabila Zahirah Pranida",
                "Kevin Pratama",
                "Yeshil Bangera",
                "Adisai Na-Thalang",
                "Patricia Nicole Monderin",
                "Yueqi Song",
                "Christian Simon",
                "Lynnette Hui Xian Ng",
                "Richardy Lobo' Sapan",
                "Taki Hasan Rafi",
                "Bin Wang",
                "Supryadi",
                "Kanyakorn Veerakanjana",
                "Piyalitt Ittichaiwong",
                "Matthew Theodore Roque",
                "Karissa Vincentio",
                "Takdanai Kreangphet",
                "Phakphum Artkaew",
                "Kadek Hendrawan Palgunadi",
                "Yanzhi Yu",
                "Rochana Prih Hastuti",
                "William Nixon",
                "Mithil Bangera",
                "Adrian Xuan Wei Lim",
                "Aye Hninn Khine",
                "Hanif Muhammad Zhafran",
                "Teddy Ferdinan",
                "Audra Aurora Izzani",
                "Ayushman Singh",
                "Evan",
                "Jauza Akbar Krito",
                "Michael Anugraha",
                "Fenal Ashokbhai Ilasariya",
                "Haochen Li",
                "John Amadeo Daniswara",
                "Filbert Aurelian Tjiaranata",
                "Eryawan Presma Yulianrifat",
                "Can Udomcharoenchaikit",
                "Fadil Risdian Ansori",
                "Mahardika Krisna Ihsani",
                "Giang Nguyen",
                "Anab Maulana Barik",
                "Dan John Velasco",
                "Rifo Ahmad Genadi",
                "Saptarshi Saha",
                "Chengwei Wei",
                "Isaiah Flores",
                "Kenneth Ko Han Chen",
                "Anjela Gail Santos",
                "Wan Shen Lim",
                "Kaung Si Phyo",
                "Tim Santos",
                "Meisyarah Dwiastuti",
                "Jiayun Luo",
                "Jan Christian Blaise Cruz",
                "Ming Shan Hee",
                "Ikhlasul Akmal Hanif",
                "M. Alif Al Hakim",
                "Muhammad Rizky Sya'ban",
                "Kun Kerdthaisong",
                "Lester James V. Miranda",
                "Fajri Koto",
                "Tirana Noor Fatyanosa",
                "Alham Fikri Aji",
                "Jostin Jerico Rosal",
                "Jun Kevin",
                "Robert Wijaya",
                "Onno P. Kampman",
                "Ruochen Zhang",
                "B√∂rje F. Karlsson",
                "Peerat Limkonchotiwat"
            ],
            "affiliations": [
                "AI Singapore",
                "Allen AI",
                "Ateneo de Manila University",
                "Auburn University",
                "Bandung Institute of Technology",
                "Beijing Academy of Artificial Intelligence (BAAI)",
                "Binus University",
                "Brawijaya University",
                "Brown University",
                "Capital One",
                "Carnegie Mellon University",
                "Chulalongkorn University",
                "Cohere",
                "Dataxet:Sonar",
                "Faculty of Medicine Siriraj Hospital, Mahidol University",
                "Graphcore",
                "Hanyang University",
                "Independent",
                "Indian Statistical Institute, Kolkata",
                "IndoNLP",
                "Institut Teknologi Sepuluh Nopember",
                "Institute for Infocomm Research, Singapore",
                "King Mongkuts University of Technology Thonburi",
                "MBZUAI",
                "MOH Office for Healthcare Transformation",
                "Macau University of Science and Technology",
                "Meta",
                "Mila - Quebec AI Institute",
                "Monash University, Indonesia",
                "Nara Institute of Science and Technology",
                "National University Philippines",
                "National University of Singapore",
                "New York University",
                "Oracle",
                "Polytechnique Montreal",
                "SCB 10X",
                "SEACrowd",
                "Samsung R&D Institute Philippines",
                "Seoul National University of Science and Technology",
                "Singapore Polytechnic",
                "Singapore University of Technology and Design",
                "Sony Group Corporation",
                "Srinakharinwirot University",
                "Thammasat University",
                "The University of Manchester",
                "Tianjin University",
                "Ton Duc Thang University",
                "Universitas Gadjah Mada",
                "Universitas Islam Indonesia",
                "Universitas Pelita Harapan",
                "University of Bath",
                "University of Illiinois, Urbana-Champaign",
                "University of Indonesia",
                "University of New Haven",
                "University of Toronto",
                "University of the Philippines",
                "Vidyasirimedhi Institute of Science and Technology",
                "Works Applications",
                "Wroc≈Çaw Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07920.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#dataset",
                    "#open_source",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "üåè",
                "ru": {
                    "title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ —Ä–∞–∑—Ä—ã–≤–∞ –≤ –ò–ò –¥–ª—è –Æ–≥–æ-–í–æ—Å—Ç–æ—á–Ω–æ–π –ê–∑–∏–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SEA-VL - –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤—É –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —è–∑—ã–∫–æ–≤ –Æ–≥–æ-–í–æ—Å—Ç–æ—á–Ω–æ–π –ê–∑–∏–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ vision-language –º–æ–¥–µ–ª–µ–π. –ü—Ä–æ–µ–∫—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö –ò–ò. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç –º–µ—Ç–æ–¥—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ –∫—É–ª—å—Ç—É—Ä–Ω–æ –∑–Ω–∞—á–∏–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∫—Ä–∞—É–ª–∏–Ω–≥ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —Å–æ–±—Ä–∞–Ω–æ 1,28 –º–ª–Ω –∫—É–ª—å—Ç—É—Ä–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —á—Ç–æ –≤ 50 —Ä–∞–∑ –±–æ–ª—å—à–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤."
                },
                "en": {
                    "title": "Bridging the Cultural Gap in AI with SEA-VL",
                    "desc": "The paper introduces SEA-VL, an initiative aimed at enhancing representation of Southeast Asian cultures in vision-language (VL) research. It highlights the creation of a large dataset containing 1.28 million culturally relevant images, which is significantly larger than existing datasets. The initiative combines crowdsourcing with automated image crawling, achieving high cultural relevance efficiently. However, it also notes challenges with generative models, as synthetic images often fail to accurately depict the region's diverse cultural nuances."
                },
                "zh": {
                    "title": "Â°´Ë°•‰∏úÂçó‰∫öÊñáÂåñÂú®AIÁ†îÁ©∂‰∏≠ÁöÑÁ©∫ÁôΩ",
                    "desc": "‰∏úÂçó‰∫öÂú∞Âå∫ËØ≠Ë®ÄÂíåÊñáÂåñÂ§öÊ†∑ÊÄßÊûÅ‰∏∫‰∏∞ÂØåÔºå‰ΩÜÂú®ËßÜËßâËØ≠Ë®ÄÁ†îÁ©∂‰∏≠Âç¥‰∏•ÈáçÁº∫‰πè‰ª£Ë°®ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSEA-VLÔºåËøôÊòØ‰∏Ä‰∏™ÂºÄÊîæÊ∫ê‰ª£Á†ÅÁöÑÈ°πÁõÆÔºåÊó®Âú®‰∏∫‰∏úÂçó‰∫öËØ≠Ë®ÄÂºÄÂèëÈ´òË¥®Èáè„ÄÅÊñáÂåñÁõ∏ÂÖ≥ÁöÑÊï∞ÊçÆ„ÄÇÈÄöËøáÂê∏ÂºïÊù•Ëá™‰∏úÂçó‰∫öÂõΩÂÆ∂ÁöÑË¥°ÁåÆËÄÖÔºåSEA-VLÁ°Æ‰øù‰∫ÜÊõ¥Â•ΩÁöÑÊñáÂåñÁõ∏ÂÖ≥ÊÄßÂíåÂ§öÊ†∑ÊÄßÔºå‰øÉËøõ‰∫ÜÂú®ËßÜËßâËØ≠Ë®ÄÁ†îÁ©∂‰∏≠ÂØπË¢´‰Ωé‰º∞ËØ≠Ë®ÄÁöÑÂåÖÂÆπÊÄß„ÄÇÊàë‰ª¨Êî∂ÈõÜ‰∫Ü128‰∏áÂº†‰∏é‰∏úÂçó‰∫öÊñáÂåñÁõ∏ÂÖ≥ÁöÑÂõæÂÉèÔºåËøúË∂ÖÁé∞ÊúâÊï∞ÊçÆÈõÜÁöÑËßÑÊ®°ÔºåÊó®Âú®Áº©Â∞è‰∏úÂçó‰∫öÁöÑ‰ª£Ë°®ÊÄßÂ∑ÆË∑ùÔºåÊé®Âä®Êõ¥ÂÖ∑ÂåÖÂÆπÊÄßÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÂèëÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07703",
            "title": "Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model",
            "url": "https://huggingface.co/papers/2503.07703",
            "abstract": "Rapid advancement of diffusion models has catalyzed remarkable progress in the field of image generation. However, prevalent models such as Flux, SD3.5 and Midjourney, still grapple with issues like model bias, limited text rendering capabilities, and insufficient understanding of Chinese cultural nuances. To address these limitations, we present Seedream 2.0, a native Chinese-English bilingual image generation foundation model that excels across diverse dimensions, which adeptly manages text prompt in both Chinese and English, supporting bilingual image generation and text rendering. We develop a powerful data system that facilitates knowledge integration, and a caption system that balances the accuracy and richness for image description. Particularly, Seedream is integrated with a self-developed bilingual large language model as a text encoder, allowing it to learn native knowledge directly from massive data. This enable it to generate high-fidelity images with accurate cultural nuances and aesthetic expressions described in either Chinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible character-level text rendering, while a Scaled ROPE generalizes well to untrained resolutions. Multi-phase post-training optimizations, including SFT and RLHF iterations, further improve the overall capability. Through extensive experimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art performance across multiple aspects, including prompt-following, aesthetics, text rendering, and structural correctness. Furthermore, Seedream 2.0 has been optimized through multiple RLHF iterations to closely align its output with human preferences, as revealed by its outstanding ELO score. In addition, it can be readily adapted to an instruction-based image editing model, such as SeedEdit, with strong editing capability that balances instruction-following and image consistency.",
            "score": 10,
            "issue_id": 2654,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "00cd4369f3c531f9",
            "authors": [
                "Lixue Gong",
                "Xiaoxia Hou",
                "Fanshi Li",
                "Liang Li",
                "Xiaochen Lian",
                "Fei Liu",
                "Liyang Liu",
                "Wei Liu",
                "Wei Lu",
                "Yichun Shi",
                "Shiqi Sun",
                "Yu Tian",
                "Zhi Tian",
                "Peng Wang",
                "Xun Wang",
                "Ye Wang",
                "Guofeng Wu",
                "Jie Wu",
                "Xin Xia",
                "Xuefeng Xiao",
                "Linjie Yang",
                "Zhonghua Zhai",
                "Xinyu Zhang",
                "Qi Zhang",
                "Yuwei Zhang",
                "Shijia Zhao",
                "Jianchao Yang",
                "Weilin Huang"
            ],
            "affiliations": [
                "Seed Vision Team, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07703.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#rlhf",
                    "#alignment",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "Seedream 2.0: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –¥–≤—É—è–∑—ã—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "Seedream 2.0 - —ç—Ç–æ –¥–≤—É—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–∑–¥–∞–Ω–Ω–∞—è –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ—â–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–∞–Ω–Ω—ã—Ö –∏ –¥–≤—É—è–∑—ã—á–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –Ω—é–∞–Ω—Å–æ–≤. –ú–æ–¥–µ–ª—å –ø—Ä–∏–º–µ–Ω—è–µ—Ç Glyph-Aligned ByT5 –¥–ª—è –≥–∏–±–∫–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞ –∏ Scaled ROPE –¥–ª—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ –Ω–µ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –ú–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è, –≤–∫–ª—é—á–∞—è SFT –∏ RLHF, —É–ª—É—á—à–∞–µ—Ç –æ–±—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Seedream 2.0: Bridging Cultures in Image Generation",
                    "desc": "This paper introduces Seedream 2.0, a bilingual image generation model designed to overcome limitations found in existing models like Flux and Midjourney. It effectively handles text prompts in both Chinese and English, allowing for culturally nuanced image generation and accurate text rendering. The model incorporates a bilingual large language model for enhanced understanding and a robust data system for knowledge integration. Extensive optimizations, including reinforcement learning from human feedback, ensure that Seedream 2.0 excels in aesthetics, prompt adherence, and overall image quality."
                },
                "zh": {
                    "title": "ÂèåËØ≠ÂõæÂÉèÁîüÊàêÁöÑÊú™Êù•ÔºöSeedream 2.0",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜSeedream 2.0ÔºåËøôÊòØ‰∏Ä‰∏™‰∏≠Ëã±ÂèåËØ≠ÁöÑÂõæÂÉèÁîüÊàêÂü∫Á°ÄÊ®°ÂûãÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊ®°ÂûãÂú®ÊñáÊú¨Ê∏≤ÊüìÂíåÊñáÂåñÁêÜËß£ÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇËØ•Ê®°ÂûãËÉΩÂ§üÂ§ÑÁêÜ‰∏≠ÊñáÂíåËã±ÊñáÁöÑÊñáÊú¨ÊèêÁ§∫ÔºåÊîØÊåÅÂèåËØ≠ÂõæÂÉèÁîüÊàêÔºåÂπ∂ÈÄöËøáÂº∫Â§ßÁöÑÊï∞ÊçÆÁ≥ªÁªüÂíåÊèèËø∞Á≥ªÁªüÊèêÂçáÂõæÂÉèÊèèËø∞ÁöÑÂáÜÁ°ÆÊÄßÂíå‰∏∞ÂØåÊÄß„ÄÇSeedream 2.0ÁªìÂêà‰∫ÜËá™Á†îÁöÑÂèåËØ≠Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§ü‰ªéÊµ∑ÈáèÊï∞ÊçÆ‰∏≠Áõ¥Êé•Â≠¶‰π†Êú¨ÂúüÁü•ËØÜÔºåÁîüÊàêÈ´ò‰øùÁúüÂ∫¶ÁöÑÂõæÂÉèÔºåÂáÜÁ°ÆË°®ËææÊñáÂåñÁªÜËäÇÂíåÁæéÂ≠¶ÁâπÂæÅ„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÂ§öÈò∂ÊÆµÁöÑÂêéËÆ≠ÁªÉ‰ºòÂåñÔºåSeedream 2.0Âú®Â§ö‰∏™ÊñπÈù¢ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂåÖÊã¨ÈÅµÂæ™ÊèêÁ§∫„ÄÅÂÆ°Áæé„ÄÅÊñáÊú¨Ê∏≤ÊüìÂíåÁªìÊûÑÊ≠£Á°ÆÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07604",
            "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
            "url": "https://huggingface.co/papers/2503.07604",
            "abstract": "Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on un<PRE_TAG>fixed-pattern data</POST_TAG> tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization.",
            "score": 10,
            "issue_id": 2654,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "313594788f663498",
            "authors": [
                "Tianhe Lin",
                "Jian Xie",
                "Siyu Yuan",
                "Deqing Yang"
            ],
            "affiliations": [
                "School of Computer Science, Fudan University",
                "School of Data Science, Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07604.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#math",
                    "#data"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ù–µ—è–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: —Å–∏–ª–∞ —à–∞–±–ª–æ–Ω–æ–≤ –∏ –ø—Ä–æ–±–ª–µ–º—ã –æ–±–æ–±—â–µ–Ω–∏—è",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å–ø–æ—Å–æ–±–Ω—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø–æ—à–∞–≥–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –¥–æ—Å—Ç–∏–≥–∞—Ç—å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –Ω–µ—è–≤–Ω–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏, –Ω–æ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —à–∞–±–ª–æ–Ω–æ–º. –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å –Ω–µ—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —à–∞–±–ª–æ–Ω–æ–º –º–æ–¥–µ–ª–∏ —Å–∫–ª–æ–Ω–Ω—ã –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É –∏ –Ω–µ –º–æ–≥—É—Ç –æ–±–æ–±—â–∞—Ç—å. –≠—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –¥–∞–∂–µ —É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç–æ, —á—Ç–æ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏–æ–±—Ä–µ—Ç–∞—é—Ç –Ω–∞–≤—ã–∫–∏ –Ω–µ—è–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏–º –ø—É—Ç—è–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏ —Å—Ö–æ–∂–µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞, –Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ."
                },
                "en": {
                    "title": "Unlocking Implicit Reasoning in Language Models",
                    "desc": "This paper explores how language models, specifically GPT-2, can perform implicit reasoning in multi-step mathematical tasks. It shows that while these models can achieve high accuracy through implicit reasoning, this ability is contingent on being trained with fixed-pattern data. When trained on variable patterns, the models tend to overfit and struggle to generalize their reasoning skills. The research highlights that language models often rely on shortcut learning, which allows them to excel in specific tasks but limits their broader reasoning capabilities."
                },
                "zh": {
                    "title": "ÈöêÂºèÊé®ÁêÜÁöÑÊç∑ÂæÑ‰∏éÂ±ÄÈôêÊÄß",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®ÊµãËØïÊó∂ËÆ°ÁÆó‰∏≠ÔºåÈöêÂºèÊé®ÁêÜ‰∏éÊòæÂºèÊé®ÁêÜÁöÑÊïàÁéáÂ∑ÆÂºÇ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåËØ≠Ë®ÄÊ®°ÂûãÂú®Âõ∫ÂÆöÊ®°ÂºèÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÊó∂ÔºåËÉΩÂ§üÈÄöËøáÈöêÂºèÊé®ÁêÜÂÆûÁé∞ÈÄêÊ≠•Êé®ÁêÜÂπ∂Âú®Â§öÊ≠•‰ªªÂä°‰∏≠ÂèñÂæóÈ´òÂáÜÁ°ÆÁéá„ÄÇÁõ∏ÂèçÔºåÂú®ÈùûÂõ∫ÂÆöÊ®°ÂºèÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÁöÑÈöêÂºèÊé®ÁêÜËÉΩÂäõÂÆπÊòìËøáÊãüÂêàÁâπÂÆöÊ®°ÂºèÔºåÂØºËá¥Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥„ÄÇÊÄªÁöÑÊù•ËØ¥ÔºåËØ≠Ë®ÄÊ®°ÂûãÈÄöËøáÊç∑ÂæÑÂ≠¶‰π†Ëé∑ÂæóÈöêÂºèÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÂú®Èù¢ÂØπ‰∏çÂêåÊ®°ÂºèÊó∂Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08605",
            "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling",
            "url": "https://huggingface.co/papers/2503.08605",
            "abstract": "While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuning-free approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and a gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this, SynCoS synchronizes them through a grounded timestep and a fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior long-range coherence, outperforming previous approaches both quantitatively and qualitatively.",
            "score": 7,
            "issue_id": 2653,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 –º–∞—Ä—Ç–∞",
                "en": "March 11",
                "zh": "3Êúà11Êó•"
            },
            "hash": "6f7bf7b6c171af43",
            "authors": [
                "Subin Kim",
                "Seoung Wug Oh",
                "Jui-Hsien Wang",
                "Joon-Young Lee",
                "Jinwoo Shin"
            ],
            "affiliations": [
                "Adobe Research",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08605.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "SynCoS: –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π Synchronized Coupled Sampling (SynCoS). –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏ –≤—ã–±–æ—Ä–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –ø–ª–∞–≤–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏ –∏ –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. SynCoS —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥ –∏ –±–∞–∑–æ–≤—ã–π —à—É–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SynCoS –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–æ–±—ã—Ç–∏—è–º–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫–∞–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ, —Ç–∞–∫ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ."
                },
                "en": {
                    "title": "Achieving Long-Range Coherence in Video Generation with SynCoS",
                    "desc": "This paper introduces Synchronized Coupled Sampling (SynCoS), a new framework for generating long videos from text prompts while maintaining semantic coherence. Traditional methods struggle with content drift and frame transitions, especially over extended sequences. SynCoS addresses these issues by synchronizing denoising paths across the video, combining reverse and optimization-based sampling strategies for better local and global coherence. Experimental results demonstrate that SynCoS enhances the quality of multi-event long video generation, outperforming existing techniques in both smoothness and consistency."
                },
                "zh": {
                    "title": "ÂêåÊ≠•ËÄ¶ÂêàÈááÊ†∑ÔºöÊèêÂçáÈïøËßÜÈ¢ëÁîüÊàêÁöÑ‰∏ÄËá¥ÊÄß",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊé®ÁêÜÊ°ÜÊû∂ÔºåÁß∞‰∏∫ÂêåÊ≠•ËÄ¶ÂêàÈááÊ†∑ÔºàSynCoSÔºâÔºåÊó®Âú®Ëß£ÂÜ≥ÈïøËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑ‰∏ÄËá¥ÊÄßÈóÆÈ¢ò„ÄÇÈÄöËøáÂêåÊ≠•ÂéªÂô™Ë∑ØÂæÑÔºåSynCoSÁ°Æ‰øù‰∫ÜÁõ∏ÈÇªÂ∏ßÂíåËøúÁ®ãÂ∏ß‰πãÈó¥ÁöÑÈïøËåÉÂõ¥‰∏ÄËá¥ÊÄß„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫ÜÂèçÂêëÈááÊ†∑ÂíåÂü∫‰∫é‰ºòÂåñÁöÑÈááÊ†∑Á≠ñÁï•Ôºå‰ª•ÂÆûÁé∞Â±ÄÈÉ®Âπ≥ÊªëËøáÊ∏°ÂíåÂÖ®Â±Ä‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSynCoSÂú®Â§ö‰∫ã‰ª∂ÈïøËßÜÈ¢ëÁîüÊàêÊñπÈù¢ÊòæËëó‰ºò‰∫é‰πãÂâçÁöÑÊñπÊ≥ïÔºåÊèê‰æõ‰∫ÜÊõ¥Âπ≥ÊªëÁöÑËøáÊ∏°ÂíåÊõ¥Â•ΩÁöÑÈïøËåÉÂõ¥ËØ≠‰πâ‰∏ÄËá¥ÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07860",
            "title": "Video Action Differencing",
            "url": "https://huggingface.co/papers/2503.07860",
            "abstract": "How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has many applications, such as coaching and skill learning. To enable development on this new task, we first create VidDiffBench, a benchmark dataset containing 549 video pairs, with human annotations of 4,469 fine-grained action differences and 2,075 localization timestamps indicating where these differences occur. Our experiments demonstrate that VidDiffBench poses a significant challenge for state-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL. By analyzing failure cases of LMMs on VidDiffBench, we highlight two key challenges for this task: localizing relevant sub-actions over two videos and fine-grained frame comparison. To overcome these, we propose the VidDiff method, an agentic workflow that breaks the task into three stages: action difference proposal, keyframe localization, and frame differencing, each stage utilizing specialized foundation models. To encourage future research in this new task, we release the benchmark at https://huggingface.co/datasets/jmhb/VidDiffBench and code at http://jmhb0.github.io/viddiff.",
            "score": 5,
            "issue_id": 2653,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "76b8b9c677de83cd",
            "authors": [
                "James Burgess",
                "Xiaohan Wang",
                "Yuhui Zhang",
                "Anita Rau",
                "Alejandro Lozano",
                "Lisa Dunlap",
                "Trevor Darrell",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Stanford",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07860.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#multimodal",
                    "#dataset",
                    "#video"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "–ù–æ–≤—ã–π —Ñ—Ä–æ–Ω—Ç–∏—Ä –≤ –∞–Ω–∞–ª–∏–∑–µ –≤–∏–¥–µ–æ: –≤—ã—è–≤–ª–µ–Ω–∏–µ —Ç–æ–Ω–∫–∏—Ö —Ä–∞–∑–ª–∏—á–∏–π –≤ –¥–µ–π—Å—Ç–≤–∏—è—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è - Video Action Differencing (VidDiff), –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ –≤—ã—è–≤–ª–µ–Ω–∏–µ —Ç–æ–Ω–∫–∏—Ö —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –≤–∏–¥–µ–æ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç VidDiffBench, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 549 –ø–∞—Ä –≤–∏–¥–µ–æ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ —Ä–∞–∑–ª–∏—á–∏–π –≤ –¥–µ–π—Å—Ç–≤–∏—è—Ö –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å —ç—Ç–æ–π –∑–∞–¥–∞—á–µ–π. –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ VidDiff, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "Unveiling Subtle Differences in Action Videos with VidDiff",
                    "desc": "This paper introduces Video Action Differencing (VidDiff), a new task focused on identifying subtle differences in videos depicting the same action. To support this task, the authors created VidDiffBench, a benchmark dataset with 549 video pairs and detailed annotations of action differences and localization timestamps. The study reveals that current large multimodal models struggle with this task, particularly in localizing sub-actions and performing fine-grained frame comparisons. To address these challenges, the authors propose a structured approach called VidDiff, which divides the task into three stages, each leveraging specialized foundation models for improved performance."
                },
                "zh": {
                    "title": "ËßÜÈ¢ëÂä®‰ΩúÂ∑ÆÂºÇÂåñÔºöËØÜÂà´ÁªÜÂæÆÂ∑ÆÂà´ÁöÑÊñ∞ÊåëÊàò",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞‰ªªÂä°ÔºåÁß∞‰∏∫ËßÜÈ¢ëÂä®‰ΩúÂ∑ÆÂºÇÂåñÔºàVidDiffÔºâÔºåÊó®Âú®ËØÜÂà´Âêå‰∏ÄÂä®‰ΩúËßÜÈ¢ë‰πãÈó¥ÁöÑÁªÜÂæÆÂ∑ÆÂà´„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÂàõÂª∫‰∫ÜVidDiffBenchÔºå‰∏Ä‰∏™ÂåÖÂê´549ÂØπËßÜÈ¢ëÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÊ†áÊ≥®‰∫Ü4469‰∏™ÁªÜÁ≤íÂ∫¶Âä®‰ΩúÂ∑ÆÂºÇÂíå2075‰∏™Êó∂Èó¥Êà≥„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåVidDiffBenchÂØπÁé∞ÊúâÁöÑÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÊèêÂá∫‰∫ÜÈáçÂ§ßÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®Â±ÄÈÉ®ÂåñÁõ∏ÂÖ≥Â≠êÂä®‰ΩúÂíåÁªÜÁ≤íÂ∫¶Â∏ßÊØîËæÉÊñπÈù¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVidDiffÊñπÊ≥ïÔºåÂ∞Ü‰ªªÂä°ÂàÜ‰∏∫‰∏â‰∏™Èò∂ÊÆµÔºöÂä®‰ΩúÂ∑ÆÂºÇÊèêËÆÆ„ÄÅÂÖ≥ÈîÆÂ∏ßÂÆö‰ΩçÂíåÂ∏ßÂ∑ÆÂºÇÂåñÔºåÊØè‰∏™Èò∂ÊÆµÈÉΩÂà©Áî®‰∫Ü‰∏ìÈó®ÁöÑÂü∫Á°ÄÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07572",
            "title": "Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning",
            "url": "https://huggingface.co/papers/2503.07572",
            "abstract": "Training models to effectively use test-time compute is crucial for improving the reasoning performance of LLMs. Current methods mostly do so via fine-tuning on search traces or running RL with 0/1 outcome reward, but do these approaches efficiently utilize test-time compute? Would these approaches continue to scale as the budget improves? In this paper, we try to answer these questions. We formalize the problem of optimizing test-time compute as a meta-reinforcement learning (RL) problem, which provides a principled perspective on spending test-time compute. This perspective enables us to view the long output stream from the LLM as consisting of several episodes run at test time and leads us to use a notion of cumulative regret over output tokens as a way to measure the efficacy of test-time compute. Akin to how RL algorithms can best tradeoff exploration and exploitation over training, minimizing cumulative regret would also provide the best balance between exploration and exploitation in the token stream. While we show that state-of-the-art models do not minimize regret, one can do so by maximizing a dense reward bonus in conjunction with the outcome 0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in the output stream, quantified by the change in the likelihood of eventual success. Using these insights, we develop Meta Reinforcement Fine-Tuning, or MRT, a new class of fine-tuning methods for optimizing test-time compute. MRT leads to a 2-3x relative gain in performance and roughly a 1.5x gain in token efficiency for math reasoning compared to outcome-reward RL.",
            "score": 5,
            "issue_id": 2653,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "49ca445bc3322f0d",
            "authors": [
                "Yuxiao Qu",
                "Matthew Y. R. Yang",
                "Amrith Setlur",
                "Lewis Tunstall",
                "Edward Emanuel Beeching",
                "Ruslan Salakhutdinov",
                "Aviral Kumar"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Hugging Face"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07572.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π LLM —á–µ—Ä–µ–∑ –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º–∞–ª–∏–∑—É—é—Ç —ç—Ç—É –∑–∞–¥–∞—á—É –∫–∞–∫ –ø—Ä–æ–±–ª–µ–º—É –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –≤–≤–æ–¥—è –ø–æ–Ω—è—Ç–∏–µ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–∂–∞–ª–µ–Ω–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –û–Ω–∏ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –º–µ—Ç–æ–¥ Meta Reinforcement Fine-Tuning (MRT), –∫–æ—Ç–æ—Ä—ã–π –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–ª–æ—Ç–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ '–ø—Ä–æ–≥—Ä–µ—Å—Å' –≤ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ –±–∏–Ω–∞—Ä–Ω–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É. MRT –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Optimizing Test-Time Compute for Enhanced LLM Reasoning",
                    "desc": "This paper addresses the challenge of optimizing test-time compute for large language models (LLMs) to enhance their reasoning capabilities. It introduces a meta-reinforcement learning framework that treats the output generation process as a series of episodes, allowing for a structured approach to manage compute resources effectively. The authors propose minimizing cumulative regret over output tokens as a metric for evaluating the efficiency of test-time compute, which balances exploration and exploitation in the model's output. They present a new fine-tuning method called Meta Reinforcement Fine-Tuning (MRT), which significantly improves performance and token efficiency in mathematical reasoning tasks compared to traditional outcome-reward reinforcement learning methods."
                },
                "zh": {
                    "title": "‰ºòÂåñÊµãËØïÊó∂ËÆ°ÁÆóÔºåÊèêÂçáÊé®ÁêÜÊÄßËÉΩÔºÅ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÊúâÊïàÂà©Áî®ÊµãËØïÊó∂ËÆ°ÁÆóËµÑÊ∫êÊù•ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇÊàë‰ª¨Â∞Ü‰ºòÂåñÊµãËØïÊó∂ËÆ°ÁÆóÁöÑÈóÆÈ¢òÂΩ¢ÂºèÂåñ‰∏∫ÂÖÉÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÈóÆÈ¢òÔºå‰ªéËÄåÊèê‰æõ‰∫Ü‰∏Ä‰∏™Á≥ªÁªüÂåñÁöÑËßÜËßíÊù•ÊîØÈÖçÊµãËØïÊó∂ËÆ°ÁÆó„ÄÇÈÄöËøáÂ∞ÜLLMÁöÑËæìÂá∫ÊµÅËßÜ‰∏∫Â§ö‰∏™ÊµãËØïÊó∂ÁöÑÂõûÂêàÔºåÂπ∂‰ΩøÁî®Á¥ØÁßØÈÅóÊÜæÁöÑÊ¶ÇÂøµÊù•Ë°°ÈáèÊµãËØïÊó∂ËÆ°ÁÆóÁöÑÊúâÊïàÊÄßÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂæÆË∞ÉÊñπÊ≥ïÔºåÁß∞‰∏∫ÂÖÉÂº∫ÂåñÂæÆË∞ÉÔºàMRTÔºâ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMRTÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠Áõ∏ËæÉ‰∫é‰º†ÁªüÁöÑÁªìÊûúÂ•ñÂä±RLÊñπÊ≥ïÔºåÊÄßËÉΩÊèêÂçá‰∫Ü2-3ÂÄçÔºå‰ª§‰ª§ÁâåÊïàÁéáÊèêÈ´ò‰∫ÜÁ∫¶1.5ÂÄç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07536",
            "title": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL",
            "url": "https://huggingface.co/papers/2503.07536",
            "abstract": "Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges from the complex interplay between visual perception and logical reasoning, particularly in compact 3B-parameter architectures where architectural constraints limit reasoning capacity and modality alignment.   While rule-based reinforcement learning (RL) excels in text-only domains, its multimodal extension confronts two critical barriers: (1) data limitations due to ambiguous answers and scarce complex reasoning examples, and (2) degraded foundational reasoning induced by multimodal pretraining.   To address these challenges, we propose \\method, a two-stage framework adapting rule-based RL for multimodal reasoning through Foundational Reasoning Enhancement (FRE) followed by Multimodal Generalization Training (MGT). The FRE stage first strengthens reasoning abilities using text-only data with rule-based RL, then the MGT stage generalizes these reasoning capabilities to multimodal domains.   Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves 4.83\\% and 4.5\\% average improvements over baselines in multimodal and text-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game tasks. These results validate that text-based reasoning enhancement enables effective multimodal generalization, offering a data-efficient paradigm that bypasses costly high-quality multimodal training data.",
            "score": 3,
            "issue_id": 2654,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 –º–∞—Ä—Ç–∞",
                "en": "March 10",
                "zh": "3Êúà10Êó•"
            },
            "hash": "59c304598f64f1e6",
            "authors": [
                "Yingzhe Peng",
                "Gongrui Zhang",
                "Miaosen Zhang",
                "Zhiyuan You",
                "Jie Liu",
                "Qipeng Zhu",
                "Kai Yang",
                "Xingzhong Xu",
                "Xin Geng",
                "Xu Yang"
            ],
            "affiliations": [
                "Ant Group",
                "Fudan University",
                "Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07536.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#rl",
                    "#benchmark",
                    "#small_models",
                    "#multimodal"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–£—Å–∏–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö (LMM) —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π 3B-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥: —Å–Ω–∞—á–∞–ª–∞ —É—Å–∏–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∑–∞—Ç–µ–º –æ–±–æ–±—â–µ–Ω–∏–µ —ç—Ç–∏—Ö –Ω–∞–≤—ã–∫–æ–≤ –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª–∏ Qwen2.5-VL-Instruct-3B –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∫ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö, —Ç–∞–∫ –∏ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–µ—Å—Ç–∞—Ö. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "Boosting Reasoning in Multimodal Models Efficiently",
                    "desc": "This paper addresses the challenges of improving reasoning in Large Multimodal Models (LMMs), particularly in smaller architectures with limited parameters. It identifies two main issues: the lack of sufficient data for complex reasoning in multimodal contexts and the negative impact of multimodal pretraining on foundational reasoning. The authors propose a two-stage framework that first enhances reasoning using rule-based reinforcement learning on text-only data, followed by training to generalize these skills to multimodal scenarios. Experimental results show significant improvements in reasoning performance across both multimodal and text-only tasks, demonstrating the effectiveness of their approach in reducing the need for extensive multimodal training data."
                },
                "zh": {
                    "title": "Â¢ûÂº∫Â§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõÁöÑÈ´òÊïàÊñπÊ≥ï",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂú®Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâ‰∏≠Â¢ûÂº∫Êé®ÁêÜËÉΩÂäõÁöÑÊåëÊàòÔºåÁâπÂà´ÊòØÂú®ÂèÇÊï∞Èáè‰∏∫3BÁöÑÁ¥ßÂáëÊû∂ÊûÑ‰∏≠ÔºåËßÜËßâÊÑüÁü•‰∏éÈÄªËæëÊé®ÁêÜ‰πãÈó¥ÁöÑÂ§çÊùÇ‰∫íÂä®„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫\textit{method}ÁöÑ‰∏§Èò∂ÊÆµÊ°ÜÊû∂ÔºåÈÄöËøáÂü∫Á°ÄÊé®ÁêÜÂ¢ûÂº∫ÔºàFREÔºâÂíåÂ§öÊ®°ÊÄÅÊ≥õÂåñËÆ≠ÁªÉÔºàMGTÔºâÊù•ÈÄÇÂ∫îÂ§öÊ®°ÊÄÅÊé®ÁêÜ„ÄÇFREÈò∂ÊÆµÂà©Áî®Âü∫‰∫éËßÑÂàôÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂ¢ûÂº∫ÊñáÊú¨Êï∞ÊçÆÁöÑÊé®ÁêÜËÉΩÂäõÔºåMGTÈò∂ÊÆµÂàôÂ∞ÜËøô‰∫õÊé®ÁêÜËÉΩÂäõÊé®ÂπøÂà∞Â§öÊ®°ÊÄÅÈ¢ÜÂüü„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå\textit{method}Âú®Â§öÊ®°ÊÄÅÂíåÊñáÊú¨Âü∫ÂáÜÊµãËØï‰∏≠ÂàÜÂà´ÊØîÂü∫Á∫øÊèêÈ´ò‰∫Ü4.83%Âíå4.5%ÔºåÂú®Â§çÊùÇÁöÑË∂≥ÁêÉÊØîËµõ‰ªªÂä°‰∏≠ÊèêÈ´ò‰∫Ü3.63%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08685",
            "title": "\"Principal Components\" Enable A New Language of Images",
            "url": "https://huggingface.co/papers/2503.08685",
            "abstract": "We introduce a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent space -- a critical factor for both interpretability and downstream tasks. Our method generates a 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved a semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging a diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, auto-regressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference.",
            "score": 2,
            "issue_id": 2654,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 –º–∞—Ä—Ç–∞",
                "en": "March 11",
                "zh": "3Êúà11Êó•"
            },
            "hash": "a013cfdc2d1e9d7c",
            "authors": [
                "Xin Wen",
                "Bingchen Zhao",
                "Ismail Elezi",
                "Jiankang Deng",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "Imperial College London",
                "Noahs Ark Lab",
                "University of Edinburgh",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08685.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#interpretability",
                    "#diffusion",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "üß©",
                "ru": {
                    "title": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è: –ü–ö-–∞–Ω–∞–ª–∏–∑ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –≤—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –¥–æ–∫–∞–∑—É–µ–º—É—é PCA-–ø–æ–¥–æ–±–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–¥–Ω–æ–º–µ—Ä–Ω—É—é –ø—Ä–∏—á–∏–Ω–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≥–¥–µ –∫–∞–∂–¥—ã–π –ø–æ—Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω –≤–Ω–æ—Å–∏—Ç –Ω–µ–ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â—É—é—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —É–±—ã–≤–∞—é—â–µ–π –æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–≤—è–∑–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∏ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π –≤ —Ç–æ–∫–µ–Ω–∞—Ö —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–µ—Ä–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª—É—á—à—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å."
                },
                "en": {
                    "title": "Enhancing Visual Tokenization with Structured Latent Spaces",
                    "desc": "This paper presents a new visual tokenization framework that incorporates a PCA-like structure into the latent token space. Unlike traditional visual tokenizers that focus mainly on how well they can recreate images, this method emphasizes the importance of the latent space's structure for better interpretability and performance in other tasks. The framework produces a sequence of tokens that each provide unique information, ensuring that the most important visual features are captured first, similar to how principal component analysis works. Additionally, the authors address a problem where high-level semantic information and low-level details become mixed in the tokens, leading to improved clarity and efficiency in training auto-regressive models."
                },
                "zh": {
                    "title": "ÂàõÊñ∞ËßÜËßâÊ†áËÆ∞ÂåñÔºåÊèêÂçáÂèØËß£ÈáäÊÄß‰∏éÊÄßËÉΩ",
                    "desc": "Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËßÜËßâÊ†áËÆ∞ÂåñÊ°ÜÊû∂ÔºåÂ∞ÜÂèØËØÅÊòéÁöÑ‰∏ªÊàêÂàÜÂàÜÊûêÔºàPCAÔºâÁªìÊûÑÂµåÂÖ•ÊΩúÂú®Ê†áËÆ∞Á©∫Èó¥„ÄÇÁé∞ÊúâÁöÑËßÜËßâÊ†áËÆ∞Âô®‰∏ªË¶Å‰ºòÂåñÈáçÂª∫Á≤æÂ∫¶Ôºå‰ΩÜÂæÄÂæÄÂøΩËßÜÊΩúÂú®Á©∫Èó¥ÁöÑÁªìÊûÑÁâπÊÄßÔºåËøôÂØπÂèØËß£ÈáäÊÄßÂíå‰∏ãÊ∏∏‰ªªÂä°Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ï‰∏∫ÂõæÂÉèÁîüÊàê‰∏ÄÁª¥Âõ†ÊûúÊ†áËÆ∞Â∫èÂàóÔºåÊØè‰∏™ÂêéÁª≠Ê†áËÆ∞Êèê‰æõ‰∏çÈáçÂè†ÁöÑ‰ø°ÊÅØÔºåÂπ∂‰∏îÂÖ∑ÊúâÊï∞Â≠¶‰∏ä‰øùËØÅÁöÑÈÄíÂáèËß£ÈáäÊñπÂ∑ÆÔºåÁ±ª‰ºº‰∫é‰∏ªÊàêÂàÜÂàÜÊûê„ÄÇÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÈáçÂª∫ÊÄßËÉΩ‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥ÔºåÂπ∂ÊèêÈ´ò‰∫Ü‰∏é‰∫∫Á±ªËßÜËßâÁ≥ªÁªüÁöÑÂØπÈΩêÂèØËß£ÈáäÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07639",
            "title": "Mixture of Experts Made Intrinsically Interpretable",
            "url": "https://huggingface.co/papers/2503.07639",
            "abstract": "Neurons in large language models often exhibit polysemanticity, simultaneously encoding multiple unrelated concepts and obscuring interpretability. Instead of relying on post-hoc methods, we present MoE-X, a Mixture-of-Experts (MoE) language model designed to be intrinsically interpretable. Our approach is motivated by the observation that, in language models, wider networks with <PRE_TAG>sparse activations</POST_TAG> are more likely to capture interpretable factors. However, directly training such large sparse networks is computationally prohibitive. MoE architectures offer a scalable alternative by activating only a subset of experts for any given input, inherently aligning with interpretability objectives. In MoE-X, we establish this connection by rewriting the MoE layer as an equivalent sparse, large MLP. This approach enables efficient scaling of the hidden size while maintaining sparsity. To further enhance interpretability, we enforce sparse activation within each expert and redesign the routing mechanism to prioritize experts with the highest activation sparsity. These designs ensure that only the most salient features are routed and processed by the experts. We evaluate MoE-X on chess and natural language tasks, showing that it achieves performance comparable to dense models while significantly improving interpretability. MoE-X achieves a perplexity better than GPT-2, with interpretability surpassing even sparse autoencoder (SAE)-based approaches.",
            "score": 2,
            "issue_id": 2653,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 –º–∞—Ä—Ç–∞",
                "en": "March 5",
                "zh": "3Êúà5Êó•"
            },
            "hash": "7e9a13248a2692b5",
            "authors": [
                "Xingyi Yang",
                "Constantin Venhoff",
                "Ashkan Khakzar",
                "Christian Schroeder de Witt",
                "Puneet K. Dokania",
                "Adel Bibi",
                "Philip Torr"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07639.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "MoE-X: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç MoE-X - –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–µ Mixture-of-Experts. –≠—Ç–∞ –º–æ–¥–µ–ª—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –ø—É—Ç–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–π –∏ —Å–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. MoE-X –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç —Å–ª–æ–π MoE –∫–∞–∫ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –±–æ–ª—å—à—É—é –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—É—é –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å —Å–∫—Ä—ã—Ç—ã–π —Ä–∞–∑–º–µ—Ä –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MoE-X –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å –ø–ª–æ—Ç–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "MoE-X: Enhancing Interpretability in Language Models with Sparse Activations",
                    "desc": "This paper introduces MoE-X, a Mixture-of-Experts language model that aims to improve interpretability in large language models by utilizing sparse activations. The authors argue that wider networks with sparse activations can better capture distinct concepts, making them more interpretable. MoE-X efficiently scales by activating only a subset of experts for each input, which aligns with the goal of enhancing interpretability. The model is evaluated on chess and natural language tasks, demonstrating performance on par with dense models while offering superior interpretability compared to existing methods."
                },
                "zh": {
                    "title": "MoE-XÔºöÂèØËß£ÈáäÁöÑÊ∑∑Âêà‰∏ìÂÆ∂ËØ≠Ë®ÄÊ®°Âûã",
                    "desc": "Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÔºåÁ•ûÁªèÂÖÉÂ∏∏Â∏∏Ë°®Áé∞Âá∫Â§ö‰πâÊÄßÔºåÂêåÊó∂ÁºñÁ†ÅÂ§ö‰∏™Êó†ÂÖ≥ÁöÑÊ¶ÇÂøµÔºåÂØºËá¥ÂèØËß£ÈáäÊÄßÂ∑Æ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜMoE-XÔºåËøôÊòØ‰∏ÄÁßçÊ∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâËØ≠Ë®ÄÊ®°ÂûãÔºåÊó®Âú®ÂÜÖÂú®‰∏äÂÖ∑ÊúâÂèØËß£ÈáäÊÄß„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂÖ∑ÊúâÁ®ÄÁñèÊøÄÊ¥ªÁöÑÂÆΩÁΩëÁªúÊõ¥ÊúâÂèØËÉΩÊçïÊçâÂèØËß£ÈáäÁöÑÂõ†Á¥†„ÄÇÈÄöËøáÊøÄÊ¥ª‰ªÖ‰∏ÄÈÉ®ÂàÜ‰∏ìÂÆ∂ÔºåMoEÊû∂ÊûÑÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÊõø‰ª£ÊñπÊ°àÔºå‰ªéËÄåÂú®‰øùÊåÅÁ®ÄÁñèÊÄßÁöÑÂêåÊó∂ÂÆûÁé∞È´òÊïàÁöÑÈöêËóèÂ±ÇËßÑÊ®°„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08120",
            "title": "UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models",
            "url": "https://huggingface.co/papers/2503.08120",
            "abstract": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in foundational computer vision research, demonstrating significant potential in both image understanding and generation. However, existing research in the face domain primarily focuses on coarse facial attribute understanding, with limited capacity to handle fine-grained facial attributes and without addressing generation capabilities. To overcome these limitations, we propose UniF^2ace, the first UMM tailored specifically for fine-grained face understanding and generation. In general, we train UniF^2ace on a self-constructed, specialized dataset utilizing two mutually beneficial diffusion techniques and a two-level mixture-of-experts architecture. Specifically, we first build a large-scale facial dataset, UniF^2ace-130K, which contains 130K image-text pairs with one million question-answering pairs that span a wide range of facial attributes. Second, we establish a theoretical connection between discrete diffusion score matching and masked generative models, optimizing both evidence lower bounds simultaneously, which significantly improves the model's ability to synthesize facial details. Finally, we introduce both token-level and sequence-level mixture-of-experts, enabling efficient fine-grained representation learning for both understanding and generation tasks. Extensive experiments on UniF^2ace-130K demonstrate that UniF^2ace outperforms existing UMMs and generative models, achieving superior performance across both understanding and generation tasks.",
            "score": 1,
            "issue_id": 2654,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 –º–∞—Ä—Ç–∞",
                "en": "March 11",
                "zh": "3Êúà11Êó•"
            },
            "hash": "7c6e7c685b283d61",
            "authors": [
                "Junzhe Li",
                "Xuerui Qiu",
                "Linrui Xu",
                "Liya Guo",
                "Delin Qu",
                "Tingting Long",
                "Chun Fan",
                "Ming Li"
            ],
            "affiliations": [
                "Central South University",
                "Computer Center, Peking University",
                "Fudan University",
                "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Computer Science, Peking University",
                "Yau Mathematical Sciences Center and Department of Mathematical Sciences, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08120.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#cv",
                    "#dataset",
                    "#multimodal",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "üßë",
                "ru": {
                    "title": "UniF^2ace: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–∏—Ü",
                    "desc": "UniF^2ace - —ç—Ç–æ –Ω–æ–≤–∞—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (UMM), —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–∏—Ü. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–µ–º 130 —Ç—ã—Å—è—á –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–æ–≤ —Å –º–∏–ª–ª–∏–æ–Ω–æ–º –ø–∞—Ä –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤ –æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–∞—Ö –ª–∏—Ü–∞. –í UniF^2ace –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –¥–≤–µ –≤–∑–∞–∏–º–æ–¥–æ–ø–æ–ª–Ω—è—é—â–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∏ –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ UniF^2ace –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ UMM –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–∏—Ü."
                },
                "en": {
                    "title": "UniF^2ace: Mastering Fine-Grained Facial Understanding and Generation",
                    "desc": "This paper introduces UniF^2ace, a unified multimodal model designed for fine-grained understanding and generation of facial attributes. Unlike previous models that only focus on coarse attributes, UniF^2ace leverages a large-scale dataset of 130K image-text pairs and one million question-answering pairs to enhance its learning capabilities. The model employs innovative diffusion techniques and a mixture-of-experts architecture to optimize its performance in synthesizing detailed facial features. Experimental results show that UniF^2ace significantly outperforms existing models in both understanding and generating facial attributes."
                },
                "zh": {
                    "title": "ÁªÜÁ≤íÂ∫¶Èù¢ÈÉ®ÁêÜËß£‰∏éÁîüÊàêÁöÑÁªü‰∏ÄÊ®°Âûã",
                    "desc": "Áªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàUMMsÔºâÂú®ËÆ°ÁÆóÊú∫ËßÜËßâÁ†îÁ©∂‰∏≠Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂõæÂÉèÁêÜËß£ÂíåÁîüÊàêÊñπÈù¢„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÈù¢ÈÉ®È¢ÜÂüüÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Á≤óÁï•ÁöÑÈù¢ÈÉ®Â±ûÊÄßÁêÜËß£‰∏äÔºåÁº∫‰πèÂ§ÑÁêÜÁªÜÁ≤íÂ∫¶Èù¢ÈÉ®Â±ûÊÄßÁöÑËÉΩÂäõÔºåÂπ∂‰∏îÊú™ËÉΩËß£ÂÜ≥ÁîüÊàêËÉΩÂäõÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜUniF^2aceÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™‰∏ìÈó®ÈíàÂØπÁªÜÁ≤íÂ∫¶Èù¢ÈÉ®ÁêÜËß£ÂíåÁîüÊàêÁöÑUMM„ÄÇÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™ÂåÖÂê´13‰∏áÂº†ÂõæÂÉè-ÊñáÊú¨ÂØπÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÂπ∂ÈááÁî®‰∏§Áßç‰∫íË°•ÁöÑÊâ©Êï£ÊäÄÊúØÂíåÂèåÂ±Ç‰∏ìÂÆ∂Ê∑∑ÂêàÊû∂ÊûÑÔºåUniF^2aceÂú®ÁêÜËß£ÂíåÁîüÊàê‰ªªÂä°‰∏äÂùáË°®Áé∞Âá∫Ëâ≤„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-03-11.html",
    "link_next": "2025-03-13.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "11.03",
        "en": "03/11",
        "zh": "3Êúà11Êó•"
    },
    "short_date_next": {
        "ru": "13.03",
        "en": "03/13",
        "zh": "3Êúà13Êó•"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑËøõÊ≠•Ôºå‰∫∫Â∑•ÊñáÊú¨Ê£ÄÊµãÔºàATDÔºâÂèòÂæóË∂äÊù•Ë∂äÈáçË¶Å„ÄÇÂ∞ΩÁÆ°ÊúâËÆ∏Â§öÂä™ÂäõÔºå‰ΩÜÊ≤°Êúâ‰∏Ä‰∏™ÁÆóÊ≥ïËÉΩÂú®‰∏çÂêåÁ±ªÂûãÁöÑÊú™Áü•ÊñáÊú¨‰∏≠Ë°®Áé∞‰∏ÄËá¥ÔºåÊàñ‰øùËØÅÊúâÊïàÂú∞Ê≥õÂåñÂà∞Êñ∞ÁöÑLLMs„ÄÇÂèØËß£ÈáäÊÄßÂú®ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†á‰∏≠Ëµ∑ÁùÄÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇÂú®ËøôÈ°πÁ†îÁ©∂‰∏≠ÔºåÊàë‰ª¨ÈÄöËøá‰ΩøÁî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEÔºâ‰ªéGemma-2-2bÊÆãÂ∑ÆÊµÅ‰∏≠ÊèêÂèñÁâπÂæÅÊù•Â¢ûÂº∫ATDÁöÑÂèØËß£ÈáäÊÄß„ÄÇÊàë‰ª¨Á°ÆÂÆö‰∫ÜÂèØËß£ÈáäÂíåÈ´òÊïàÁöÑÁâπÂæÅÔºåÈÄöËøáÈ¢ÜÂüüÂíåÊ®°ÂûãÁâπÂÆöÁöÑÁªüËÆ°Êï∞ÊçÆ„ÄÅÂºïÂØºÊñπÊ≥ï‰ª•ÂèäÊâãÂä®ÊàñÂü∫‰∫éLLMÁöÑËß£ÈáäÊù•ÂàÜÊûêÂÖ∂ËØ≠‰πâÂíåÁõ∏ÂÖ≥ÊÄß„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÊèê‰æõ‰∫ÜÂÖ≥‰∫é‰∏çÂêåÊ®°ÂûãÁîüÊàêÁöÑÊñáÊú¨‰∏é‰∫∫Á±ªÂÜô‰ΩúÂÜÖÂÆπ‰πãÈó¥Â∑ÆÂºÇÁöÑÂÆùË¥µËßÅËß£„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜÁé∞‰ª£LLMsÂú®‰ø°ÊÅØÂØÜÈõÜÈ¢ÜÂüüÂÖ∑ÊúâÁã¨ÁâπÁöÑÂÜô‰ΩúÈ£éÊ†ºÔºåÂç≥‰ΩøÂÆÉ‰ª¨ÂèØ‰ª•ÈÄöËøá‰∏™ÊÄßÂåñÊèêÁ§∫ÁîüÊàêÁ±ª‰ºº‰∫∫Á±ªÁöÑËæìÂá∫„ÄÇ",
        "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
        "pinyin": "ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑËøõÊ≠•Ôºå‰∫∫Â∑•ÊñáÊú¨Ê£ÄÊµãÔºàATDÔºâÂèòÂæóË∂äÊù•Ë∂äÈáçË¶Å„ÄÇÂ∞ΩÁÆ°ÊúâËÆ∏Â§öÂä™ÂäõÔºå‰ΩÜÊ≤°Êúâ‰∏Ä‰∏™ÁÆóÊ≥ïËÉΩÂú®‰∏çÂêåÁ±ªÂûãÁöÑÊú™Áü•ÊñáÊú¨‰∏≠Ë°®Áé∞‰∏ÄËá¥ÔºåÊàñ‰øùËØÅÊúâÊïàÂú∞Ê≥õÂåñÂà∞Êñ∞ÁöÑLLMs„ÄÇÂèØËß£ÈáäÊÄßÂú®ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†á‰∏≠Ëµ∑ÁùÄÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇÂú®ËøôÈ°πÁ†îÁ©∂‰∏≠ÔºåÊàë‰ª¨ÈÄöËøá‰ΩøÁî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSAEÔºâ‰ªéGemma-2-2bÊÆãÂ∑ÆÊµÅ‰∏≠ÊèêÂèñÁâπÂæÅÊù•Â¢ûÂº∫ATDÁöÑÂèØËß£ÈáäÊÄß„ÄÇÊàë‰ª¨Á°ÆÂÆö‰∫ÜÂèØËß£ÈáäÂíåÈ´òÊïàÁöÑÁâπÂæÅÔºåÈÄöËøáÈ¢ÜÂüüÂíåÊ®°ÂûãÁâπÂÆöÁöÑÁªüËÆ°Êï∞ÊçÆ„ÄÅÂºïÂØºÊñπÊ≥ï‰ª•ÂèäÊâãÂä®ÊàñÂü∫‰∫éLLMÁöÑËß£ÈáäÊù•ÂàÜÊûêÂÖ∂ËØ≠‰πâÂíåÁõ∏ÂÖ≥ÊÄß„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÊèê‰æõ‰∫ÜÂÖ≥‰∫é‰∏çÂêåÊ®°ÂûãÁîüÊàêÁöÑÊñáÊú¨‰∏é‰∫∫Á±ªÂÜô‰ΩúÂÜÖÂÆπ‰πãÈó¥Â∑ÆÂºÇÁöÑÂÆùË¥µËßÅËß£„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜÁé∞‰ª£LLMsÂú®‰ø°ÊÅØÂØÜÈõÜÈ¢ÜÂüüÂÖ∑ÊúâÁã¨ÁâπÁöÑÂÜô‰ΩúÈ£éÊ†ºÔºåÂç≥‰ΩøÂÆÉ‰ª¨ÂèØ‰ª•ÈÄöËøá‰∏™ÊÄßÂåñÊèêÁ§∫ÁîüÊàêÁ±ª‰ºº‰∫∫Á±ªÁöÑËæìÂá∫„ÄÇ\n\nSu√≠zhe d√†x√≠ng y«îy√°n m√≥x√≠ng (LLMs) de j√¨nb√π, r√©ng≈çng w√©nbƒõn ji«énc√® (ATD) bi√†n d√© yu√®l√°iyu√® zh√≤ngy√†o. J«êngu«én y«íu x«îdu≈ç n«îl√¨, d√†n m√©iy«íu yƒ´g√® su√†nf«é n√©ng z√†i b√πt√≥ng l√®ix√≠ng de w√®izhƒ´ w√©nbƒõn zh≈çng bi«éoxi√†n yƒ´zh√¨, hu√≤ b«éozh√®ng y«íuxi√†o de f√†nhu√† d√†o xƒ´n de LLMs. Kƒõ jiƒõsh√¨x√¨ng z√†i sh√≠xi√†n zh√® yƒ´ m√πbiƒÅo zh≈çng q«êzhe gu«énji√†n zu√≤y√≤ng. Z√†i zh√® xi√†ng y√°nji≈´ zh≈çng, w«ímen t≈çnggu√≤ sh«êy√≤ng xƒ´sh≈´ z√¨biƒÅnm«éq√¨ (SAE) c√≥ng Gemma-2-2b c√°nch√° li√∫ zh≈çng tƒ´qu t√©di«én l√°i zƒìngqi√°ng ATD de kƒõ jiƒõsh√¨x√¨ng. W«ímen qu√®d√¨ngle kƒõ jiƒõsh√¨ h√© gƒÅoxi√†o de t√©di«én, t≈çnggu√≤ l«êngy√π h√© m√≥x√≠ng t√®d√¨ng de t«íngj√¨ sh√πj√π, y«ênd«éo fƒÅngf«é y«êji«é sh«íud√≤ng hu√≤ jƒ´y√∫ LLM de jiƒõsh√¨ l√°i fƒìnxi q√≠ y√πy√°n h√© xiƒÅngguƒÅnx√¨ng. W«ímen de fƒÅngf«é t√≠g≈çngle guƒÅny√∫ b√πt√≥ng m√≥x√≠ng shƒìngch√©ng de w√©nbƒõn y«î r√©nl√®i xiƒõzu√≤ n√®ir√≥ng zhƒ´jiƒÅn chƒÅy√¨ de b«éogu√¨ ji√†nsh√¨. W«ímen zh«énsh√¨le xi√†nd√†i LLMs z√†i x√¨nxƒ´ m√¨ji√© l«êngy√π y«íu d√∫t√® de xiƒõzu√≤ fƒìngg√©, j√≠sh«ê tƒÅmen kƒõy«ê t≈çnggu√≤ g√®x√¨nghu√† t«êsh√¨ shƒìngch√©ng l√®ix√≠ r√©nl√®i de sh√πch≈´.",
        "vocab": "[{'word': 'ÈöèÁùÄ', 'pinyin': 'su√≠zhe', 'trans': 'with'},\n{'word': 'Â§ßÂûã', 'pinyin': 'd√†x√≠ng', 'trans': 'large-scale'},\n{'word': 'ËØ≠Ë®ÄÊ®°Âûã', 'pinyin': 'y«îy√°n m√≥x√≠ng', 'trans': 'language model'},\n{'word': 'ËøõÊ≠•', 'pinyin': 'j√¨nb√π', 'trans': 'progress'},\n{'word': '‰∫∫Â∑•ÊñáÊú¨Ê£ÄÊµã', 'pinyin': 'r√©ng≈çng w√©nbƒõn ji«énc√®', 'trans': 'artificial text detection'},\n{'word': 'ÂèòÂæó', 'pinyin': 'bi√†nd√©', 'trans': 'become'},\n{'word': 'Ë∂äÊù•Ë∂ä', 'pinyin': 'yu√®l√°iyu√®', 'trans': 'increasingly'},\n{'word': 'ÈáçË¶Å', 'pinyin': 'zh√≤ngy√†o', 'trans': 'important'},\n{'word': 'Â∞ΩÁÆ°', 'pinyin': 'j«êngu«én', 'trans': 'although'},\n{'word': 'Âä™Âäõ', 'pinyin': 'n«îl√¨', 'trans': 'effort'},\n{'word': 'ÁÆóÊ≥ï', 'pinyin': 'su√†nf«é', 'trans': 'algorithm'},\n{'word': '‰∏ÄËá¥', 'pinyin': 'yƒ´zh√¨', 'trans': 'consistent'},\n{'word': 'Ë°®Áé∞', 'pinyin': 'bi«éoxi√†n', 'trans': 'performance'},\n{'word': 'Êú™Áü•', 'pinyin': 'w√®izhƒ´', 'trans': 'unknown'},\n{'word': '‰øùËØÅ', 'pinyin': 'b«éozh√®ng', 'trans': 'guarantee'},\n{'word': 'ÊúâÊïà', 'pinyin': 'y«íuxi√†o', 'trans': 'effective'},\n{'word': 'Ê≥õÂåñ', 'pinyin': 'f√†nhu√†', 'trans': 'generalize'},\n{'word': 'ÂèØËß£ÈáäÊÄß', 'pinyin': 'kƒõ jiƒõsh√¨ x√¨ng', 'trans': 'interpretability'},\n{'word': 'Ëµ∑ÁùÄ', 'pinyin': 'q«êzhe', 'trans': 'playing'},\n{'word': 'ÂÖ≥ÈîÆ', 'pinyin': 'gu«énji√†n', 'trans': 'key'},\n{'word': '‰ΩúÁî®', 'pinyin': 'zu√≤y√≤ng', 'trans': 'role'},\n{'word': 'Á®ÄÁñè', 'pinyin': 'xƒ´sh≈´', 'trans': 'sparse'},\n{'word': 'Ëá™ÁºñÁ†ÅÂô®', 'pinyin': 'z√¨biƒÅnm«éq√¨', 'trans': 'autoencoder'},\n{'word': 'ÊÆãÂ∑ÆÊµÅ', 'pinyin': 'c√°nchƒÅ li√∫', 'trans': 'residual flow'},\n{'word': 'ÊèêÂèñ', 'pinyin': 't√≠q«î', 'trans': 'extract'},\n{'word': 'ÁâπÂæÅ', 'pinyin': 't√®zhƒìng', 'trans': 'feature'},\n{'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìngqi√°ng', 'trans': 'enhance'},\n{'word': 'È¢ÜÂüü', 'pinyin': 'l«êngy√π', 'trans': 'domain'},\n{'word': 'ÁªüËÆ°Êï∞ÊçÆ', 'pinyin': 't«íngj√¨ sh√πj√π', 'trans': 'statistical data'},\n{'word': 'ÂºïÂØºÊñπÊ≥ï', 'pinyin': 'y«ênd«éo fƒÅngf«é', 'trans': 'guidance method'},\n{'word': 'ÊâãÂä®', 'pinyin': 'sh«íud√≤ng', 'trans': 'manual'},\n{'word': 'Âü∫‰∫é', 'pinyin': 'jƒ´y√∫', 'trans': 'based on'},\n{'word': 'Ëß£Èáä', 'pinyin': 'jiƒõsh√¨', 'trans': 'explanation'},\n{'word': 'ËØ≠‰πâ', 'pinyin': 'y«îy√¨', 'trans': 'semantics'},\n{'word': 'Áõ∏ÂÖ≥ÊÄß', 'pinyin': 'xiƒÅngguƒÅnx√¨ng', 'trans': 'relevance'},\n{'word': 'ËßÅËß£', 'pinyin': 'ji√†njiƒõ', 'trans': 'insight'},\n{'word': 'Â∑ÆÂºÇ', 'pinyin': 'chƒÅy√¨', 'trans': 'difference'},\n{'word': 'ÂÜÖÂÆπ', 'pinyin': 'n√®ir√≥ng', 'trans': 'content'},\n{'word': 'Â±ïÁ§∫', 'pinyin': 'zh«énsh√¨', 'trans': 'demonstrate'},\n{'word': '‰ø°ÊÅØÂØÜÈõÜ', 'pinyin': 'x√¨nxƒ´ m√¨j√≠', 'trans': 'information-intensive'},\n{'word': 'Áã¨Áâπ', 'pinyin': 'd√∫t√®', 'trans': 'unique'},\n{'word': 'ÂÜô‰ΩúÈ£éÊ†º', 'pinyin': 'xiƒõzu√≤ fƒìngg√©', 'trans': 'writing style'},\n{'word': '‰∏™ÊÄßÂåñ', 'pinyin': 'g√®x√¨nghu√†', 'trans': 'personalized'},\n{'word': 'ÊèêÁ§∫', 'pinyin': 't√≠sh√¨', 'trans': 'prompt'},\n{'word': 'ËæìÂá∫', 'pinyin': 'sh≈´ch≈´', 'trans': 'output'}]",
        "trans": "With the advancement of large language models (LLMs), artificial text detection (ATD) has become increasingly important. Despite numerous efforts, no single algorithm has been able to perform consistently across different types of unknown texts or guarantee effective generalization to new LLMs. Explainability plays a crucial role in achieving this goal. In this research, we enhance the explainability of ATD by extracting features from the residual stream of Gemma-2-2b using sparse autoencoders (SAE). We identify interpretable and efficient features and analyze their semantics and relevance through domain- and model-specific statistics, guided methods, and manual or LLM-based explanations. Our approach provides valuable insights into the differences between texts generated by different models and human-written content. We demonstrate that modern LLMs have a unique writing style in information-intensive domains, even though they can generate human-like outputs through personalized prompts.",
        "update_ts": "2025-03-11 09:12"
    }
}