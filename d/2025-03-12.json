{
    "date": {
        "ru": "12 марта",
        "en": "March 12",
        "zh": "3月12日"
    },
    "time_utc": "2025-03-12 02:16",
    "weekday": 2,
    "issue_id": 2653,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.07860",
            "title": "Video Action Differencing",
            "url": "https://huggingface.co/papers/2503.07860",
            "abstract": "How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has many applications, such as coaching and skill learning. To enable development on this new task, we first create VidDiffBench, a benchmark dataset containing 549 video pairs, with human annotations of 4,469 fine-grained action differences and 2,075 localization timestamps indicating where these differences occur. Our experiments demonstrate that VidDiffBench poses a significant challenge for state-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL. By analyzing failure cases of LMMs on VidDiffBench, we highlight two key challenges for this task: localizing relevant sub-actions over two videos and fine-grained frame comparison. To overcome these, we propose the VidDiff method, an agentic workflow that breaks the task into three stages: action difference proposal, keyframe localization, and frame differencing, each stage utilizing specialized foundation models. To encourage future research in this new task, we release the benchmark at https://huggingface.co/datasets/jmhb/VidDiffBench and code at http://jmhb0.github.io/viddiff.",
            "score": 3,
            "issue_id": 2653,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 марта",
                "en": "March 10",
                "zh": "3月10日"
            },
            "hash": "76b8b9c677de83cd",
            "authors": [
                "James Burgess",
                "Xiaohan Wang",
                "Yuhui Zhang",
                "Anita Rau",
                "Alejandro Lozano",
                "Lisa Dunlap",
                "Trevor Darrell",
                "Serena Yeung-Levy"
            ],
            "affiliations": [
                "Stanford",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07860.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#multimodal",
                    "#dataset",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Новый фронтир в анализе видео: выявление тонких различий в действиях",
                    "desc": "Статья представляет новую задачу в области компьютерного зрения - Video Action Differencing (VidDiff), которая направлена на выявление тонких различий между видео с одинаковыми действиями. Авторы создали датасет VidDiffBench, содержащий 549 пар видео с аннотациями различий в действиях и временными метками. Эксперименты показали, что современные мультимодальные языковые модели (LLM) испытывают трудности с этой задачей. Для решения проблемы предложен метод VidDiff, использующий трехэтапный подход с применением специализированных фундаментальных моделей."
                },
                "en": {
                    "title": "Unveiling Subtle Differences in Action Videos with VidDiff",
                    "desc": "This paper introduces Video Action Differencing (VidDiff), a new task focused on identifying subtle differences in videos depicting the same action. To support this task, the authors created VidDiffBench, a benchmark dataset with 549 video pairs and detailed annotations of action differences and localization timestamps. The study reveals that current large multimodal models struggle with this task, particularly in localizing sub-actions and performing fine-grained frame comparisons. To address these challenges, the authors propose a structured approach called VidDiff, which divides the task into three stages, each leveraging specialized foundation models for improved performance."
                },
                "zh": {
                    "title": "视频动作差异化：识别细微差别的新挑战",
                    "desc": "本文介绍了一种新任务，称为视频动作差异化（VidDiff），旨在识别同一动作视频之间的细微差别。为此，我们创建了VidDiffBench，一个包含549对视频的基准数据集，标注了4469个细粒度动作差异和2075个时间戳。我们的实验表明，VidDiffBench对现有的大型多模态模型（LMMs）提出了重大挑战，尤其是在局部化相关子动作和细粒度帧比较方面。为了解决这些问题，我们提出了VidDiff方法，将任务分为三个阶段：动作差异提议、关键帧定位和帧差异化，每个阶段都利用了专门的基础模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07572",
            "title": "Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning",
            "url": "https://huggingface.co/papers/2503.07572",
            "abstract": "Training models to effectively use test-time compute is crucial for improving the reasoning performance of LLMs. Current methods mostly do so via fine-tuning on search traces or running RL with 0/1 outcome reward, but do these approaches efficiently utilize test-time compute? Would these approaches continue to scale as the budget improves? In this paper, we try to answer these questions. We formalize the problem of optimizing test-time compute as a meta-reinforcement learning (RL) problem, which provides a principled perspective on spending test-time compute. This perspective enables us to view the long output stream from the LLM as consisting of several episodes run at test time and leads us to use a notion of cumulative regret over output tokens as a way to measure the efficacy of test-time compute. Akin to how RL algorithms can best tradeoff exploration and exploitation over training, minimizing cumulative regret would also provide the best balance between exploration and exploitation in the token stream. While we show that state-of-the-art models do not minimize regret, one can do so by maximizing a dense reward bonus in conjunction with the outcome 0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in the output stream, quantified by the change in the likelihood of eventual success. Using these insights, we develop Meta Reinforcement Fine-Tuning, or MRT, a new class of fine-tuning methods for optimizing test-time compute. MRT leads to a 2-3x relative gain in performance and roughly a 1.5x gain in token efficiency for math reasoning compared to outcome-reward RL.",
            "score": 3,
            "issue_id": 2653,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 марта",
                "en": "March 10",
                "zh": "3月10日"
            },
            "hash": "49ca445bc3322f0d",
            "authors": [
                "Yuxiao Qu",
                "Matthew Y. R. Yang",
                "Amrith Setlur",
                "Lewis Tunstall",
                "Edward Emanuel Beeching",
                "Ruslan Salakhutdinov",
                "Aviral Kumar"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Hugging Face"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07572.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оптимизация вычислений LLM через мета-обучение с подкреплением",
                    "desc": "Статья представляет новый подход к оптимизации вычислений во время тестирования для улучшения рассуждений в больших языковых моделях (LLM). Авторы формализуют эту задачу как проблему мета-обучения с подкреплением, вводя понятие кумулятивного сожаления для оценки эффективности вычислений. Они разрабатывают метод Meta Reinforcement Fine-Tuning (MRT), который максимизирует плотное вознаграждение за 'прогресс' в дополнение к бинарному результату. MRT показывает значительное улучшение производительности и эффективности использования токенов в задачах математических рассуждений."
                },
                "en": {
                    "title": "Optimizing Test-Time Compute for Enhanced LLM Reasoning",
                    "desc": "This paper addresses the challenge of optimizing test-time compute for large language models (LLMs) to enhance their reasoning capabilities. It introduces a meta-reinforcement learning framework that treats the output generation process as a series of episodes, allowing for a structured approach to manage compute resources effectively. The authors propose minimizing cumulative regret over output tokens as a metric for evaluating the efficiency of test-time compute, which balances exploration and exploitation in the model's output. They present a new fine-tuning method called Meta Reinforcement Fine-Tuning (MRT), which significantly improves performance and token efficiency in mathematical reasoning tasks compared to traditional outcome-reward reinforcement learning methods."
                },
                "zh": {
                    "title": "优化测试时计算，提升推理性能！",
                    "desc": "本文探讨了如何有效利用测试时计算资源来提升大型语言模型（LLM）的推理性能。我们将优化测试时计算的问题形式化为元强化学习（RL）问题，从而提供了一个系统化的视角来支配测试时计算。通过将LLM的输出流视为多个测试时的回合，并使用累积遗憾的概念来衡量测试时计算的有效性，我们提出了一种新的微调方法，称为元强化微调（MRT）。实验结果表明，MRT在数学推理任务中相较于传统的结果奖励RL方法，性能提升了2-3倍，令令牌效率提高了约1.5倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07639",
            "title": "Mixture of Experts Made Intrinsically Interpretable",
            "url": "https://huggingface.co/papers/2503.07639",
            "abstract": "Neurons in large language models often exhibit polysemanticity, simultaneously encoding multiple unrelated concepts and obscuring interpretability. Instead of relying on post-hoc methods, we present MoE-X, a Mixture-of-Experts (MoE) language model designed to be intrinsically interpretable. Our approach is motivated by the observation that, in language models, wider networks with <PRE_TAG>sparse activations</POST_TAG> are more likely to capture interpretable factors. However, directly training such large sparse networks is computationally prohibitive. MoE architectures offer a scalable alternative by activating only a subset of experts for any given input, inherently aligning with interpretability objectives. In MoE-X, we establish this connection by rewriting the MoE layer as an equivalent sparse, large MLP. This approach enables efficient scaling of the hidden size while maintaining sparsity. To further enhance interpretability, we enforce sparse activation within each expert and redesign the routing mechanism to prioritize experts with the highest activation sparsity. These designs ensure that only the most salient features are routed and processed by the experts. We evaluate MoE-X on chess and natural language tasks, showing that it achieves performance comparable to dense models while significantly improving interpretability. MoE-X achieves a perplexity better than GPT-2, with interpretability surpassing even sparse autoencoder (SAE)-based approaches.",
            "score": 1,
            "issue_id": 2653,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 марта",
                "en": "March 5",
                "zh": "3月5日"
            },
            "hash": "7e9a13248a2692b5",
            "authors": [
                "Xingyi Yang",
                "Constantin Venhoff",
                "Ashkan Khakzar",
                "Christian Schroeder de Witt",
                "Puneet K. Dokania",
                "Adel Bibi",
                "Philip Torr"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07639.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MoE-X: Интерпретируемые языковые модели через разреженные экспертные системы",
                    "desc": "Статья представляет MoE-X - новую архитектуру языковой модели, основанную на принципе Mixture-of-Experts. Эта модель разработана для повышения интерпретируемости нейронных сетей путем использования разреженных активаций и селективного задействования экспертов. MoE-X переписывает слой MoE как эквивалентную разреженную большую многослойную перцептронную сеть, что позволяет эффективно масштабировать скрытый размер при сохранении разреженности. Результаты экспериментов показывают, что MoE-X достигает производительности, сравнимой с плотными моделями, при значительно улучшенной интерпретируемости."
                },
                "en": {
                    "title": "MoE-X: Enhancing Interpretability in Language Models with Sparse Activations",
                    "desc": "This paper introduces MoE-X, a Mixture-of-Experts language model that aims to improve interpretability in large language models by utilizing sparse activations. The authors argue that wider networks with sparse activations can better capture distinct concepts, making them more interpretable. MoE-X efficiently scales by activating only a subset of experts for each input, which aligns with the goal of enhancing interpretability. The model is evaluated on chess and natural language tasks, demonstrating performance on par with dense models while offering superior interpretability compared to existing methods."
                },
                "zh": {
                    "title": "MoE-X：可解释的混合专家语言模型",
                    "desc": "在大型语言模型中，神经元常常表现出多义性，同时编码多个无关的概念，导致可解释性差。我们提出了MoE-X，这是一种混合专家（MoE）语言模型，旨在内在上具有可解释性。我们的研究表明，具有稀疏激活的宽网络更有可能捕捉可解释的因素。通过激活仅一部分专家，MoE架构提供了一种可扩展的替代方案，从而在保持稀疏性的同时实现高效的隐藏层规模。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.08605",
            "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling",
            "url": "https://huggingface.co/papers/2503.08605",
            "abstract": "While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuning-free approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and a gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this, SynCoS synchronizes them through a grounded timestep and a fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior long-range coherence, outperforming previous approaches both quantitatively and qualitatively.",
            "score": 0,
            "issue_id": 2653,
            "pub_date": "2025-03-11",
            "pub_date_card": {
                "ru": "11 марта",
                "en": "March 11",
                "zh": "3月11日"
            },
            "hash": "6f7bf7b6c171af43",
            "authors": [
                "Subin Kim",
                "Seoung Wug Oh",
                "Jui-Hsien Wang",
                "Joon-Young Lee",
                "Jinwoo Shin"
            ],
            "affiliations": [
                "Adobe Research",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.08605.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "SynCoS: Синхронизированная генерация длинных видео с сохранением согласованности",
                    "desc": "Статья представляет новый метод генерации длинных видео с использованием текстовых подсказок, названный Synchronized Coupled Sampling (SynCoS). Этот подход объединяет обратную выборку и выборку на основе оптимизации для обеспечения плавных переходов между кадрами и глобальной согласованности контента. SynCoS синхронизирует траектории шумоподавления через фиксированный временной шаг и базовый шум, что позволяет избежать нежелательных изменений содержания. Эксперименты показывают, что SynCoS значительно улучшает генерацию длинных видео с несколькими событиями, превосходя предыдущие подходы как количественно, так и качественно."
                },
                "en": {
                    "title": "Achieving Long-Range Coherence in Video Generation with SynCoS",
                    "desc": "This paper introduces Synchronized Coupled Sampling (SynCoS), a new framework for generating long videos from text prompts while maintaining semantic coherence. Traditional methods struggle with content drift and frame transitions, especially over extended sequences. SynCoS addresses these issues by synchronizing denoising paths across the video, combining reverse and optimization-based sampling strategies for better local and global coherence. Experimental results demonstrate that SynCoS enhances the quality of multi-event long video generation, outperforming existing techniques in both smoothness and consistency."
                },
                "zh": {
                    "title": "同步耦合采样：提升长视频生成的一致性",
                    "desc": "本文提出了一种新的推理框架，称为同步耦合采样（SynCoS），旨在解决长视频生成中的一致性问题。通过同步去噪路径，SynCoS确保了相邻帧和远程帧之间的长范围一致性。该方法结合了反向采样和基于优化的采样策略，以实现局部平滑过渡和全局一致性。实验结果表明，SynCoS在多事件长视频生成方面显著优于之前的方法，提供了更平滑的过渡和更好的长范围语义一致性。"
                }
            }
        }
    ],
    "link_prev": "2025-03-11.html",
    "link_next": "2025-03-13.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "11.03",
        "en": "03/11",
        "zh": "3月11日"
    },
    "short_date_next": {
        "ru": "13.03",
        "en": "03/13",
        "zh": "3月13日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "随着大型语言模型（LLMs）的进步，人工文本检测（ATD）变得越来越重要。尽管有许多努力，但没有一个算法能在不同类型的未知文本中表现一致，或保证有效地泛化到新的LLMs。可解释性在实现这一目标中起着关键作用。在这项研究中，我们通过使用稀疏自编码器（SAE）从Gemma-2-2b残差流中提取特征来增强ATD的可解释性。我们确定了可解释和高效的特征，通过领域和模型特定的统计数据、引导方法以及手动或基于LLM的解释来分析其语义和相关性。我们的方法提供了关于不同模型生成的文本与人类写作内容之间差异的宝贵见解。我们展示了现代LLMs在信息密集领域具有独特的写作风格，即使它们可以通过个性化提示生成类似人类的输出。",
        "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
        "pinyin": "随着大型语言模型（LLMs）的进步，人工文本检测（ATD）变得越来越重要。尽管有许多努力，但没有一个算法能在不同类型的未知文本中表现一致，或保证有效地泛化到新的LLMs。可解释性在实现这一目标中起着关键作用。在这项研究中，我们通过使用稀疏自编码器（SAE）从Gemma-2-2b残差流中提取特征来增强ATD的可解释性。我们确定了可解释和高效的特征，通过领域和模型特定的统计数据、引导方法以及手动或基于LLM的解释来分析其语义和相关性。我们的方法提供了关于不同模型生成的文本与人类写作内容之间差异的宝贵见解。我们展示了现代LLMs在信息密集领域具有独特的写作风格，即使它们可以通过个性化提示生成类似人类的输出。\n\nSuízhe dàxíng yǔyán móxíng (LLMs) de jìnbù, réngōng wénběn jiǎncè (ATD) biàn dé yuèláiyuè zhòngyào. Jǐnguǎn yǒu xǔduō nǔlì, dàn méiyǒu yīgè suànfǎ néng zài bùtóng lèixíng de wèizhī wénběn zhōng biǎoxiàn yīzhì, huò bǎozhèng yǒuxiào de fànhuà dào xīn de LLMs. Kě jiěshìxìng zài shíxiàn zhè yī mùbiāo zhōng qǐzhe guǎnjiàn zuòyòng. Zài zhè xiàng yánjiū zhōng, wǒmen tōngguò shǐyòng xīshū zìbiānmǎqì (SAE) cóng Gemma-2-2b cánchá liú zhōng tīqu tédiǎn lái zēngqiáng ATD de kě jiěshìxìng. Wǒmen quèdìngle kě jiěshì hé gāoxiào de tédiǎn, tōngguò lǐngyù hé móxíng tèdìng de tǒngjì shùjù, yǐndǎo fāngfǎ yǐjiǎ shǒudòng huò jīyú LLM de jiěshì lái fēnxi qí yùyán hé xiāngguānxìng. Wǒmen de fāngfǎ tígōngle guānyú bùtóng móxíng shēngchéng de wénběn yǔ rénlèi xiězuò nèiróng zhījiān chāyì de bǎoguì jiànshì. Wǒmen zhǎnshìle xiàndài LLMs zài xìnxī mìjié lǐngyù yǒu dútè de xiězuò fēnggé, jíshǐ tāmen kěyǐ tōngguò gèxìnghuà tǐshì shēngchéng lèixí rénlèi de shùchū.",
        "vocab": "[{'word': '随着', 'pinyin': 'suízhe', 'trans': 'with'},\n{'word': '大型', 'pinyin': 'dàxíng', 'trans': 'large-scale'},\n{'word': '语言模型', 'pinyin': 'yǔyán móxíng', 'trans': 'language model'},\n{'word': '进步', 'pinyin': 'jìnbù', 'trans': 'progress'},\n{'word': '人工文本检测', 'pinyin': 'réngōng wénběn jiǎncè', 'trans': 'artificial text detection'},\n{'word': '变得', 'pinyin': 'biàndé', 'trans': 'become'},\n{'word': '越来越', 'pinyin': 'yuèláiyuè', 'trans': 'increasingly'},\n{'word': '重要', 'pinyin': 'zhòngyào', 'trans': 'important'},\n{'word': '尽管', 'pinyin': 'jǐnguǎn', 'trans': 'although'},\n{'word': '努力', 'pinyin': 'nǔlì', 'trans': 'effort'},\n{'word': '算法', 'pinyin': 'suànfǎ', 'trans': 'algorithm'},\n{'word': '一致', 'pinyin': 'yīzhì', 'trans': 'consistent'},\n{'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'},\n{'word': '未知', 'pinyin': 'wèizhī', 'trans': 'unknown'},\n{'word': '保证', 'pinyin': 'bǎozhèng', 'trans': 'guarantee'},\n{'word': '有效', 'pinyin': 'yǒuxiào', 'trans': 'effective'},\n{'word': '泛化', 'pinyin': 'fànhuà', 'trans': 'generalize'},\n{'word': '可解释性', 'pinyin': 'kě jiěshì xìng', 'trans': 'interpretability'},\n{'word': '起着', 'pinyin': 'qǐzhe', 'trans': 'playing'},\n{'word': '关键', 'pinyin': 'guǎnjiàn', 'trans': 'key'},\n{'word': '作用', 'pinyin': 'zuòyòng', 'trans': 'role'},\n{'word': '稀疏', 'pinyin': 'xīshū', 'trans': 'sparse'},\n{'word': '自编码器', 'pinyin': 'zìbiānmǎqì', 'trans': 'autoencoder'},\n{'word': '残差流', 'pinyin': 'cánchā liú', 'trans': 'residual flow'},\n{'word': '提取', 'pinyin': 'tíqǔ', 'trans': 'extract'},\n{'word': '特征', 'pinyin': 'tèzhēng', 'trans': 'feature'},\n{'word': '增强', 'pinyin': 'zēngqiáng', 'trans': 'enhance'},\n{'word': '领域', 'pinyin': 'lǐngyù', 'trans': 'domain'},\n{'word': '统计数据', 'pinyin': 'tǒngjì shùjù', 'trans': 'statistical data'},\n{'word': '引导方法', 'pinyin': 'yǐndǎo fāngfǎ', 'trans': 'guidance method'},\n{'word': '手动', 'pinyin': 'shǒudòng', 'trans': 'manual'},\n{'word': '基于', 'pinyin': 'jīyú', 'trans': 'based on'},\n{'word': '解释', 'pinyin': 'jiěshì', 'trans': 'explanation'},\n{'word': '语义', 'pinyin': 'yǔyì', 'trans': 'semantics'},\n{'word': '相关性', 'pinyin': 'xiāngguānxìng', 'trans': 'relevance'},\n{'word': '见解', 'pinyin': 'jiànjiě', 'trans': 'insight'},\n{'word': '差异', 'pinyin': 'chāyì', 'trans': 'difference'},\n{'word': '内容', 'pinyin': 'nèiróng', 'trans': 'content'},\n{'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'demonstrate'},\n{'word': '信息密集', 'pinyin': 'xìnxī mìjí', 'trans': 'information-intensive'},\n{'word': '独特', 'pinyin': 'dútè', 'trans': 'unique'},\n{'word': '写作风格', 'pinyin': 'xiězuò fēnggé', 'trans': 'writing style'},\n{'word': '个性化', 'pinyin': 'gèxìnghuà', 'trans': 'personalized'},\n{'word': '提示', 'pinyin': 'tíshì', 'trans': 'prompt'},\n{'word': '输出', 'pinyin': 'shūchū', 'trans': 'output'}]",
        "trans": "With the advancement of large language models (LLMs), artificial text detection (ATD) has become increasingly important. Despite numerous efforts, no single algorithm has been able to perform consistently across different types of unknown texts or guarantee effective generalization to new LLMs. Explainability plays a crucial role in achieving this goal. In this research, we enhance the explainability of ATD by extracting features from the residual stream of Gemma-2-2b using sparse autoencoders (SAE). We identify interpretable and efficient features and analyze their semantics and relevance through domain- and model-specific statistics, guided methods, and manual or LLM-based explanations. Our approach provides valuable insights into the differences between texts generated by different models and human-written content. We demonstrate that modern LLMs have a unique writing style in information-intensive domains, even though they can generate human-like outputs through personalized prompts.",
        "update_ts": "2025-03-11 09:12"
    }
}