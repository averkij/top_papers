{
    "date": {
        "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 2",
        "zh": "9æœˆ2æ—¥"
    },
    "time_utc": "2025-09-02 02:24",
    "weekday": 1,
    "issue_id": 5660,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.21104",
            "title": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic\n  Reasoning",
            "url": "https://huggingface.co/papers/2508.21104",
            "abstract": "PVPO, an enhanced reinforcement learning method using a reference anchor and data pre-sampling, achieves state-of-the-art performance with reduced computational cost and improved generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estimate advantage, which may cause the policy to fall into local optimum and increase computational cost. To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling. Specifically, we use the reference model to rollout in advance and employ the calculated reward score as a reference anchor. Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts. Meanwhile, the reference model can assess sample difficulty during data pre-sampling, enabling effective selection of high-gain data to improve training efficiency. Experiments conducted on nine datasets across two domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our approach not only demonstrates robust generalization across multiple tasks, but also exhibits scalable performance across models of varying scales.",
            "score": 5,
            "issue_id": 5660,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 28",
                "zh": "8æœˆ28æ—¥"
            },
            "hash": "4abb06016c3bb8f5",
            "authors": [
                "Wenfeng Feng",
                "Penghong Zhao",
                "Guochao Jiang",
                "Chuzhan Hao",
                "Yuewei Zhang",
                "Hao Wang"
            ],
            "affiliations": [
                "Alibaba Cloud Computing"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21104.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#optimization",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "PVPO: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "PVPO - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ¿Ğ¾Ñ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ. PVPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ğ¾Ñ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´ĞµĞ²ÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PVPO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "PVPO: Efficient Reinforcement Learning with Reference Anchors and Pre-Sampling",
                    "desc": "PVPO is a novel reinforcement learning method that enhances efficiency by using a reference anchor and data pre-sampling techniques. This approach mitigates the issues of local optima and high computational costs commonly faced in critic-free methods. By utilizing a reference model to evaluate sample difficulty and guide data selection, PVPO improves training efficiency and reduces the need for extensive rollouts. Experimental results show that PVPO achieves state-of-the-art performance across various datasets, demonstrating strong generalization and scalability."
                },
                "zh": {
                    "title": "PVPOï¼šé«˜æ•ˆå¼ºåŒ–å­¦ä¹ çš„æ–°çªç ´",
                    "desc": "PVPOæ˜¯ä¸€ç§å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨å‚è€ƒé”šç‚¹å’Œæ•°æ®é¢„é‡‡æ ·æ¥æé«˜æ€§èƒ½ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ç”±äºå¤šæ¬¡é‡‡æ ·å’Œæ¯”è¾ƒå¯¼è‡´çš„å±€éƒ¨æœ€ä¼˜å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨å‚è€ƒæ¨¡å‹æå‰è¿›è¡Œå›æ»šï¼Œå¹¶å°†è®¡ç®—çš„å¥–åŠ±åˆ†æ•°ä½œä¸ºå‚è€ƒé”šç‚¹ï¼ŒPVPOæœ‰æ•ˆåœ°çº æ­£äº†ç»„å†…æ¯”è¾ƒå¼•å…¥çš„ç´¯ç§¯åå·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPVPOåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-01.html",
    "link_next": "2025-09-03.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "01.09",
        "en": "09/01",
        "zh": "9æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "03.09",
        "en": "09/03",
        "zh": "9æœˆ3æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}