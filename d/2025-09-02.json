{
    "date": {
        "ru": "2 сентября",
        "en": "September 2",
        "zh": "9月2日"
    },
    "time_utc": "2025-09-02 02:24",
    "weekday": 1,
    "issue_id": 5660,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.21104",
            "title": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic\n  Reasoning",
            "url": "https://huggingface.co/papers/2508.21104",
            "abstract": "PVPO, an enhanced reinforcement learning method using a reference anchor and data pre-sampling, achieves state-of-the-art performance with reduced computational cost and improved generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estimate advantage, which may cause the policy to fall into local optimum and increase computational cost. To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an advantage reference anchor and data pre-sampling. Specifically, we use the reference model to rollout in advance and employ the calculated reward score as a reference anchor. Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts. Meanwhile, the reference model can assess sample difficulty during data pre-sampling, enabling effective selection of high-gain data to improve training efficiency. Experiments conducted on nine datasets across two domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our approach not only demonstrates robust generalization across multiple tasks, but also exhibits scalable performance across models of varying scales.",
            "score": 5,
            "issue_id": 5660,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 августа",
                "en": "August 28",
                "zh": "8月28日"
            },
            "hash": "4abb06016c3bb8f5",
            "authors": [
                "Wenfeng Feng",
                "Penghong Zhao",
                "Guochao Jiang",
                "Chuzhan Hao",
                "Yuewei Zhang",
                "Hao Wang"
            ],
            "affiliations": [
                "Alibaba Cloud Computing"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21104.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#optimization",
                    "#training",
                    "#rl"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "PVPO: Эффективное обучение с подкреплением с помощью опорной модели",
                    "desc": "PVPO - это усовершенствованный метод обучения с подкреплением, использующий опорную модель и предварительную выборку данных. Он достигает улучшенных результатов по сравнению с существующими методами, снижая вычислительные затраты и улучшая обобщающую способность. PVPO использует опорную модель для предварительного развертывания и оценки сложности выборки, что позволяет эффективно выбирать наиболее информативные данные. Эксперименты на девяти наборах данных показали, что PVPO достигает наилучших результатов и демонстрирует устойчивую обобщающую способность на различных задачах."
                },
                "en": {
                    "title": "PVPO: Efficient Reinforcement Learning with Reference Anchors and Pre-Sampling",
                    "desc": "PVPO is a novel reinforcement learning method that enhances efficiency by using a reference anchor and data pre-sampling techniques. This approach mitigates the issues of local optima and high computational costs commonly faced in critic-free methods. By utilizing a reference model to evaluate sample difficulty and guide data selection, PVPO improves training efficiency and reduces the need for extensive rollouts. Experimental results show that PVPO achieves state-of-the-art performance across various datasets, demonstrating strong generalization and scalability."
                },
                "zh": {
                    "title": "PVPO：高效强化学习的新突破",
                    "desc": "PVPO是一种增强的强化学习方法，利用参考锚点和数据预采样来提高性能。该方法解决了传统方法中由于多次采样和比较导致的局部最优和计算成本高的问题。通过使用参考模型提前进行回滚，并将计算的奖励分数作为参考锚点，PVPO有效地纠正了组内比较引入的累积偏差。实验结果表明，PVPO在多个数据集上表现出色，具有良好的泛化能力和可扩展性。"
                }
            }
        }
    ],
    "link_prev": "2025-09-01.html",
    "link_next": "2025-09-03.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "01.09",
        "en": "09/01",
        "zh": "9月1日"
    },
    "short_date_next": {
        "ru": "03.09",
        "en": "09/03",
        "zh": "9月3日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}