{
    "date": {
        "ru": "23 декабря",
        "en": "December 23",
        "zh": "12月23日"
    },
    "time_utc": "2024-12-23 10:11",
    "weekday": 0,
    "issue_id": 1265,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.15119",
            "title": "Parallelized Autoregressive Visual Generation",
            "url": "https://huggingface.co/papers/2412.15119",
            "abstract": "Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependencies-tokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6x speedup with comparable quality and up to 9.5x speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.",
            "score": 24,
            "issue_id": 1258,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "0933582baa02f7a6",
            "authors": [
                "Yuqing Wang",
                "Shuhuai Ren",
                "Zhijie Lin",
                "Yujin Han",
                "Haoyuan Guo",
                "Zhenheng Yang",
                "Difan Zou",
                "Jiashi Feng",
                "Xihui Liu"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Peking University",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15119.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#video",
                    "#cv",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение авторегрессионной генерации изображений и видео без потери качества",
                    "desc": "Статья предлагает метод параллельного авторегрессионного генерирования визуального контента, который ускоряет процесс, сохраняя преимущества авторегрессионного моделирования. Авторы разработали стратегию, которая генерирует удаленные токены с слабыми зависимостями параллельно, но сохраняет последовательное генерирование для сильно зависимых локальных токенов. Метод легко интегрируется в стандартные авторегрессионные модели без изменения архитектуры или токенизатора. Эксперименты показали ускорение до 3.6 раз с сопоставимым качеством и до 9.5 раз с минимальной деградацией качества для задач генерации изображений и видео."
                },
                "en": {
                    "title": "Speeding Up Visual Generation with Smart Token Parallelization",
                    "desc": "This paper addresses the slow inference speed of autoregressive models used for visual generation, which typically generate images or videos one token at a time. The authors propose a new method that allows for parallel generation of visual tokens, focusing on the dependencies between tokens to determine which can be generated simultaneously. By identifying weakly dependent tokens that can be generated in parallel, while keeping strongly dependent tokens in a sequential order, the method enhances efficiency without altering the existing model architecture. Experiments show significant speed improvements in generating images and videos, making this approach a promising direction for future research in visual generation."
                },
                "zh": {
                    "title": "并行自回归生成，提升视觉生成效率",
                    "desc": "自回归模型在视觉生成中表现出色，但由于逐个预测的过程，推理速度较慢。本文提出了一种简单有效的并行自回归视觉生成方法，旨在提高生成效率，同时保留自回归建模的优点。我们的关键见解是，视觉标记之间的依赖关系决定了并行生成的可能性，弱依赖的标记可以并行生成，而强依赖的标记则难以一起生成。实验结果表明，我们的方法在图像和视频生成任务中实现了3.6倍的速度提升，且质量保持相当，甚至在某些情况下可达到9.5倍的速度提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13649",
            "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
            "url": "https://huggingface.co/papers/2412.13649",
            "abstract": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the following two observations: (i) Excessive compression during the prefill phase, which requires specific full context impairs the comprehension of the reasoning task; (ii) Deviation of heavy hitters occurs in the reasoning tasks with long outputs. Therefore, SCOPE, a simple yet efficient framework that separately performs KV cache optimization during the prefill and decoding phases, is introduced. Specifically, the KV cache during the prefill phase is preserved to maintain the essential information, while a novel strategy based on sliding is proposed to select essential heavy hitters for the decoding phase. Memory usage and memory transfer are further optimized using adaptive and discontinuous strategies. Extensive experiments on LongGenBench show the effectiveness and generalization of SCOPE and its compatibility as a plug-in to other prefill-only KV compression methods.",
            "score": 14,
            "issue_id": 1258,
            "pub_date": "2024-12-18",
            "pub_date_card": {
                "ru": "18 декабря",
                "en": "December 18",
                "zh": "12月18日"
            },
            "hash": "885d11532659dd95",
            "authors": [
                "Jialong Wu",
                "Zhenglin Wang",
                "Linhai Zhang",
                "Yilong Lai",
                "Yulan He",
                "Deyu Zhou"
            ],
            "affiliations": [
                "Department of Informatics, Kings College London, UK",
                "School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China",
                "The Alan Turing Institute, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13649.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективная оптимизация KV-кэша для длинных контекстов в LLM",
                    "desc": "Статья представляет SCOPE - фреймворк для оптимизации KV-кэша в моделях LLM при генерации длинных текстов. Авторы предлагают раздельную оптимизацию для этапов prefill и decoding, сохраняя важную информацию на первом этапе и выбирая ключевые элементы на втором. Используются адаптивные и прерывистые стратегии для оптимизации использования памяти. Эксперименты на LongGenBench показывают эффективность и обобщаемость SCOPE, а также его совместимость с другими методами сжатия KV-кэша."
                },
                "en": {
                    "title": "Optimizing KV Caches for Enhanced Long-Context Generation",
                    "desc": "This paper addresses the limitations of Key-Value (KV) caches in large language models (LLMs) when generating long outputs. It highlights that optimizing the decoding phase is essential, as excessive compression during the prefill phase can hinder reasoning tasks. The proposed framework, SCOPE, optimizes KV cache usage by preserving crucial information during prefill and employing a sliding strategy to select important data during decoding. Experimental results demonstrate that SCOPE improves memory efficiency and can be integrated with existing KV compression methods."
                },
                "zh": {
                    "title": "优化KV缓存，提升长输出生成效率",
                    "desc": "本文提出了一种名为SCOPE的框架，旨在优化长输出生成任务中的KV缓存。研究表明，在预填充阶段过度压缩会影响推理任务的理解，因此需要保留关键信息。SCOPE通过在预填充和解码阶段分别优化KV缓存，采用滑动策略选择重要的重击项，从而提高解码效率。实验结果表明，SCOPE在LongGenBench上表现出色，并且可以作为其他KV压缩方法的插件使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16145",
            "title": "Offline Reinforcement Learning for LLM Multi-Step Reasoning",
            "url": "https://huggingface.co/papers/2412.16145",
            "abstract": "Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for multi-step reasoning tasks because (1) DPO relies on paired preference data, which is not readily available for multi-step reasoning tasks, and (2) it treats all tokens uniformly, making it ineffective for credit assignment in multi-step reasoning tasks, which often come with sparse reward. In this work, we propose OREO (Offline Reasoning Optimization), an offline RL method for enhancing LLM multi-step reasoning. Building on insights from previous works of maximum entropy reinforcement learning, it jointly learns a policy model and value function by optimizing the soft Bellman Equation. We show in principle that it reduces the need to collect pairwise data and enables better credit assignment. Empirically, OREO surpasses existing offline learning methods on multi-step reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and embodied agent control (ALFWorld). The approach can be extended to a multi-iteration framework when additional resources are available. Furthermore, the learned value function can be leveraged to guide the tree search for free, which can further boost performance during test time.",
            "score": 9,
            "issue_id": 1260,
            "pub_date": "2024-12-20",
            "pub_date_card": {
                "ru": "20 декабря",
                "en": "December 20",
                "zh": "12月20日"
            },
            "hash": "5779a845f782fb45",
            "authors": [
                "Huaijie Wang",
                "Shibo Hao",
                "Hanze Dong",
                "Shenao Zhang",
                "Yilin Bao",
                "Ziran Yang",
                "Yi Wu"
            ],
            "affiliations": [
                "Northwestern University",
                "Salesforce Research",
                "Tsinghua University",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16145.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#math",
                    "#rlhf",
                    "#agents",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "OREO: Оптимизация многошаговых рассуждений для языковых моделей",
                    "desc": "В этой статье представлен метод OREO (Offline Reasoning Optimization) для улучшения способностей больших языковых моделей (LLM) к многошаговым рассуждениям с помощью обучения с подкреплением в офлайн-режиме. OREO совместно обучает модель политики и функцию ценности, оптимизируя мягкое уравнение Беллмана. Метод превосходит существующие офлайн-методы обучения на задачах многошагового рассуждения, включая математические задачи и управление агентами. OREO также может быть расширен до многоитерационной структуры и использован для направления древовидного поиска во время тестирования."
                },
                "en": {
                    "title": "OREO: Enhancing Multi-Step Reasoning in LLMs with Offline RL",
                    "desc": "This paper introduces OREO, an offline reinforcement learning method designed to improve the multi-step reasoning capabilities of large language models (LLMs). Unlike Direct Preference Optimization, which struggles with multi-step tasks due to its reliance on paired preference data and uniform token treatment, OREO effectively addresses these challenges by optimizing the soft Bellman Equation. The method enhances credit assignment and reduces the need for extensive data collection, leading to superior performance on reasoning benchmarks. Additionally, OREO's learned value function can be utilized to enhance search strategies during testing, further improving outcomes."
                },
                "zh": {
                    "title": "OREO：提升大型语言模型的多步推理能力",
                    "desc": "本论文提出了一种名为OREO的离线强化学习方法，旨在提高大型语言模型（LLMs）的多步推理能力。与直接偏好优化（DPO）不同，OREO不依赖于成对的偏好数据，适用于多步推理任务。该方法通过优化软贝尔曼方程，联合学习策略模型和价值函数，从而改善了奖励稀疏情况下的信用分配问题。实验结果表明，OREO在数学推理和智能体控制等多步推理基准测试中优于现有的离线学习方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16112",
            "title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up",
            "url": "https://huggingface.co/papers/2412.16112",
            "abstract": "Diffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at a linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with a comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K self-generated samples for 10K iterations, we can effectively transfer knowledge from a pre-trained DiT to a student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization cross various models and plugins, and improved support for multi-GPU parallel inference. Models and codes are available here: https://github.com/Huage001/CLEAR.",
            "score": 9,
            "issue_id": 1259,
            "pub_date": "2024-12-20",
            "pub_date_card": {
                "ru": "20 декабря",
                "en": "December 20",
                "zh": "12月20日"
            },
            "hash": "c17ca50dc03ea86c",
            "authors": [
                "Songhua Liu",
                "Zhenxiong Tan",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16112.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#inference",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "CLEAR: Ускорение DiT без потери качества",
                    "desc": "Статья представляет новый метод линейного внимания для Diffusion Transformers (DiT), называемый CLEAR. Этот подход снижает сложность предобученных DiT с квадратичной до линейной, сохраняя при этом качество генерации изображений. CLEAR использует локальное внимание, подобное свёрточным операциям, ограничивая взаимодействие признаков локальным окном вокруг каждого токена запроса. Эксперименты показывают, что fine-tuning слоя внимания на всего 10 тысячах сгенерированных образцов позволяет эффективно передать знания от предобученной модели к модели с линейной сложностью."
                },
                "en": {
                    "title": "Linearizing Attention for Faster Image Generation with CLEAR",
                    "desc": "This paper presents a new approach to improve the efficiency of Diffusion Transformers (DiT) in image generation by introducing a linear attention mechanism. The authors identify key factors necessary for linearizing DiTs, such as locality and feature integrity, and propose a local attention strategy called CLEAR. This method significantly reduces the computational complexity of attention mechanisms, achieving a 99.5% reduction in attention computations and a 6.3 times speedup in generating high-resolution images. Additionally, the study shows that the distilled model retains performance comparable to the original DiT while enabling better generalization and multi-GPU support."
                },
                "zh": {
                    "title": "线性注意力，快速生成高分辨率图像！",
                    "desc": "本文提出了一种新的线性注意力机制，旨在解决扩散变换器（DiT）在生成高分辨率图像时的延迟问题。我们总结了现有的高效注意力机制，并确定了成功线性化预训练DiT的四个关键因素。基于这些见解，我们引入了一种名为CLEAR的局部注意力策略，限制特征交互在每个查询标记周围的局部窗口内，从而实现线性复杂度。实验结果表明，通过对注意力层进行微调，我们可以有效地将知识从预训练的DiT转移到学生模型，同时显著减少计算量并加速生成过程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15322",
            "title": "Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis",
            "url": "https://huggingface.co/papers/2412.15322",
            "abstract": "We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily available text-audio data to learn to generate semantically aligned high-quality audio samples. Additionally, we improve audio-visual synchrony with a conditional synchronization module that aligns video conditions with audio latents at the frame level. Trained with a flow matching objective, MMAudio achieves new video-to-audio state-of-the-art among public models in terms of audio quality, semantic alignment, and audio-visual synchronization, while having a low inference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio also achieves surprisingly competitive performance in text-to-audio generation, showing that joint training does not hinder single-modality performance. Code and demo are available at: https://hkchengrex.github.io/MMAudio",
            "score": 7,
            "issue_id": 1258,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "9d724184f50de930",
            "authors": [
                "Ho Kei Cheng",
                "Masato Ishii",
                "Akio Hayakawa",
                "Takashi Shibuya",
                "Alexander Schwing",
                "Yuki Mitsufuji"
            ],
            "affiliations": [
                "Sony AI",
                "Sony Group Corporation",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15322.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#audio",
                    "#inference",
                    "#video",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "MMAudio: Революция в синтезе аудио по видео и тексту",
                    "desc": "MMAudio - это новая мультимодальная система для синтеза высококачественного и синхронизированного аудио на основе видео и опционального текста. Она использует совместное обучение на аудио-текстовых данных и условный модуль синхронизации для улучшения соответствия видео и звука. MMAudio достигает нового уровня качества в задаче генерации аудио по видео, превосходя существующие модели по качеству звука, семантическому соответствию и синхронизации. Модель также показывает хорошие результаты в генерации аудио по тексту."
                },
                "en": {
                    "title": "MMAudio: High-Quality Audio Synthesis with Video and Text Integration",
                    "desc": "This paper introduces MMAudio, a novel framework for generating high-quality audio that is synchronized with video and can also utilize text inputs. Unlike traditional methods that rely solely on video data, MMAudio leverages a larger dataset of text-audio pairs to enhance the semantic alignment of the generated audio. The framework includes a conditional synchronization module that ensures audio is aligned with video at the frame level, improving overall coherence. With a flow matching objective, MMAudio sets new benchmarks in audio quality and synchronization while maintaining efficient performance with a low number of parameters."
                },
                "zh": {
                    "title": "MMAudio：高质量音频合成的新方法",
                    "desc": "我们提出了一种新的多模态联合训练框架MMAudio，用于合成高质量和同步的音频，基于视频和可选的文本条件。与仅依赖视频数据的单模态训练不同，MMAudio结合了大规模的文本-音频数据进行联合训练，以生成语义对齐的高质量音频样本。此外，我们通过条件同步模块在帧级别上对视频条件和音频潜在特征进行对齐，从而提高音频与视频的同步性。MMAudio在音频质量、语义对齐和音频-视觉同步方面达到了新的公共模型的最佳水平，同时推理时间低（生成8秒片段仅需1.23秒），参数量仅为157M。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14590",
            "title": "MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design",
            "url": "https://huggingface.co/papers/2412.14590",
            "abstract": "Quantization has become one of the most effective methodologies to compress LLMs into smaller size. However, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency. In this paper, we make a comprehensive analysis of the general quantization principles on their effect to the triangle of accuracy, memory consumption and system efficiency. We propose MixLLM that explores the new optimization space of mixed-precision quantization between output features based on the insight that different output features matter differently in the model. MixLLM identifies the output features with high salience in the global view rather than within each single layer, effectively assigning the larger bit-width to output features that need it most to achieve good accuracy with low memory consumption. We present the sweet spot of quantization configuration of algorithm-system co-design that leads to high accuracy and system efficiency. To address the system challenge, we design the two-step dequantization to make use of the int8 Tensor Core easily and fast data type conversion to reduce dequantization overhead significantly, and present the software pipeline to overlap the memory access, dequantization and the MatMul to the best. Extensive experiments show that with only 10% more bits, the PPL increasement can be reduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on average MMLU-Pro improves by 0.93 over the SOTA of three popular models. In addition to its superior accuracy, MixLLM also achieves state-of-the-art system efficiency.",
            "score": 3,
            "issue_id": 1260,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "3a5b6d590eec2c6e",
            "authors": [
                "Zhen Zheng",
                "Xiaonan Song",
                "Chuanjie Liu"
            ],
            "affiliations": [
                "Microsoft"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14590.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MixLLM: Оптимальное квантование для эффективных языковых моделей",
                    "desc": "Статья представляет новый метод квантования больших языковых моделей под названием MixLLM. Авторы предлагают использовать смешанную точность квантования для выходных признаков, основываясь на их важности в глобальном контексте модели. MixLLM оптимизирует баланс между точностью, потреблением памяти и эффективностью системы. Эксперименты показывают, что MixLLM достигает лучшей точности и эффективности по сравнению с существующими методами квантования для популярных языковых моделей."
                },
                "en": {
                    "title": "MixLLM: Optimizing Quantization for Accuracy and Efficiency in LLMs",
                    "desc": "This paper discusses a new method called MixLLM for quantizing large language models (LLMs) to make them smaller and more efficient. The authors analyze how different quantization techniques affect accuracy, memory use, and system performance. MixLLM uses mixed-precision quantization, assigning more bits to important output features, which helps maintain accuracy while reducing memory consumption. The proposed method also includes a two-step dequantization process to improve speed and efficiency, resulting in better performance compared to existing solutions."
                },
                "zh": {
                    "title": "MixLLM：高效的混合精度量化方案",
                    "desc": "量化技术已成为压缩大型语言模型（LLMs）的一种有效方法，但现有的量化方案在准确性和系统效率上仍存在局限性。本文对量化原则进行了全面分析，探讨了准确性、内存消耗和系统效率之间的关系。我们提出了MixLLM，利用混合精度量化优化输出特征，确保重要特征获得更高的位宽，从而在保持良好准确性的同时降低内存消耗。通过设计两步反量化和优化软件管道，MixLLM在准确性和系统效率上都达到了最先进的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15450",
            "title": "Fietje: An open, efficient LLM for Dutch",
            "url": "https://huggingface.co/papers/2412.15450",
            "abstract": "This paper introduces Fietje, a family of small language models (SLMs) specifically designed for the Dutch language. The model is based on Phi 2, an English-centric model of 2.7 billion parameters. Fietje demonstrated competitive results with larger language models upon its release. A core emphasis of this work is transparency and reproducibility: Fietje is fully open-source, with model weights, datasets, training, and evaluation code all publicly accessible.   The paper discusses the performance of Fietje and many other models on an extensive evaluation suite of benchmarks on reasoning, sentiment analysis, world knowledge, linguistic acceptability and word sense disambiguation. Evaluation results illustrate the rapid progress in the field of LLMs, where recent small models outperform older, larger models that were fine-tuned for Dutch. This trend signals an exciting future for Dutch language processing, suggesting that even compact LLMs are becoming increasingly capable. Furthermore, ongoing and future efforts to adapt LLMs to Dutch are poised to enhance these models even further, broadening their applicability and accessibility. Fietje is only an intermediate step in improving accessibility to language technology for users of the Dutch language.",
            "score": 2,
            "issue_id": 1263,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "0a377666ad38be9a",
            "authors": [
                "Bram Vanroy"
            ],
            "affiliations": [
                "Dutch Language Institute, Rapenburg 61, 2311 GJ Leiden, The Netherlands",
                "KU Leuven, Blijde Inkomststraat 21, 3000 Leuven, Belgium"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15450.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#reasoning",
                    "#open_source",
                    "#small_models",
                    "#benchmark"
                ],
                "emoji": "🇳🇱",
                "ru": {
                    "title": "Fietje: Открытая малая языковая модель для нидерландского языка",
                    "desc": "Статья представляет Fietje - семейство малых языковых моделей (SLM), специально разработанных для нидерландского языка. Модель основана на Phi 2 и показывает конкурентоспособные результаты с более крупными языковыми моделями. Особое внимание уделяется прозрачности и воспроизводимости: Fietje полностью открыт, включая веса модели, наборы данных и код. Оценка производительности Fietje на различных бенчмарках демонстрирует быстрый прогресс в области языковых моделей, где недавние малые модели превосходят более старые и крупные модели, адаптированные для нидерландского языка."
                },
                "en": {
                    "title": "Fietje: Small Models, Big Impact for Dutch Language Processing",
                    "desc": "This paper presents Fietje, a series of small language models tailored for the Dutch language, built upon the Phi 2 architecture. Despite its smaller size, Fietje achieves competitive performance against larger models, showcasing the advancements in small language models (SLMs). The authors emphasize the importance of transparency and reproducibility by making all resources, including model weights and training data, publicly available. The evaluation results indicate that recent small models like Fietje are outperforming older, larger models in various linguistic tasks, highlighting a promising trend in Dutch language processing."
                },
                "zh": {
                    "title": "Fietje：荷兰语处理的新希望",
                    "desc": "本文介绍了Fietje，一个专为荷兰语设计的小型语言模型（SLM）系列。该模型基于一个拥有27亿参数的以英语为中心的Phi 2模型。Fietje在发布时展示了与更大语言模型的竞争性结果，强调了透明性和可重复性，所有模型权重、数据集、训练和评估代码均为开源。评估结果表明，最近的小型模型在荷兰语处理上超越了较旧的、更大的模型，预示着荷兰语处理的未来充满希望。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15035",
            "title": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps",
            "url": "https://huggingface.co/papers/2412.15035",
            "abstract": "Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, following the detailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in the category crime_tax for Italian but remains safe in other languages. Similar differences can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure safe and responsible usage across diverse user communities.",
            "score": 1,
            "issue_id": 1265,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "ec88d82d8720bf3d",
            "authors": [
                "Felix Friedrich",
                "Simone Tedeschi",
                "Patrick Schramowski",
                "Manuel Brack",
                "Roberto Navigli",
                "Huu Nguyen",
                "Bo Li",
                "Kristian Kersting"
            ],
            "affiliations": [
                "CERTAIN",
                "DFKI",
                "Hessian.AI",
                "Ontocord.AI",
                "Sapienza University of Rome",
                "TU Darmstadt",
                "UIUC",
                "University of Chicago",
                "Virtue.ai"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15035.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#multilingual",
                    "#ethics",
                    "#benchmark"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Многоязычная оценка безопасности LLM: неожиданные различия между языками",
                    "desc": "Статья представляет M-ALERT - многоязычный бенчмарк для оценки безопасности больших языковых моделей (LLM) на пяти языках. Авторы провели эксперименты на 10 современных LLM, выявив значительные различия в безопасности между языками и категориями. Например, модель Llama3.2 показала высокую небезопасность в категории преступлений и налогов для итальянского языка, оставаясь безопасной на других языках. Результаты подчеркивают необходимость разработки надежных многоязычных практик безопасности для LLM."
                },
                "en": {
                    "title": "Ensuring Multilingual Safety in Large Language Models",
                    "desc": "This paper presents M-ALERT, a multilingual benchmark designed to assess the safety of Large Language Models (LLMs) in five different languages. It includes a comprehensive set of 75,000 prompts categorized according to the ALERT taxonomy, allowing for detailed safety evaluations. The study reveals that LLMs often show varying levels of safety across languages, with some models performing poorly in specific categories for certain languages. These results highlight the necessity for tailored safety measures in LLMs to accommodate linguistic diversity and ensure responsible usage."
                },
                "zh": {
                    "title": "构建安全的多语言大型语言模型",
                    "desc": "本文介绍了M-ALERT，这是一个多语言基准，用于评估大型语言模型（LLMs）的安全性。该基准涵盖英语、法语、德语、意大利语和西班牙语，共包含75,000个高质量提示。研究表明，不同语言和类别之间的安全性存在显著不一致，某些模型在特定语言中表现出高风险。研究结果强调了在多语言环境中实施强有力的安全实践的重要性，以确保用户的安全和负责任的使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.11525",
            "title": "Sequence Matters: Harnessing Video Models in 3D Super-Resolution",
            "url": "https://huggingface.co/papers/2412.11525",
            "abstract": "3D super-resolution aims to reconstruct high-fidelity 3D models from low-resolution (LR) multi-view images. Early studies primarily focused on single-image super-resolution (SISR) models to upsample LR images into high-resolution images. However, these methods often lack view consistency because they operate independently on each image. Although various post-processing techniques have been extensively explored to mitigate these inconsistencies, they have yet to fully resolve the issues. In this paper, we perform a comprehensive study of 3D super-resolution by leveraging video super-resolution (VSR) models. By utilizing VSR models, we ensure a higher degree of spatial consistency and can reference surrounding spatial information, leading to more accurate and detailed reconstructions. Our findings reveal that VSR models can perform remarkably well even on sequences that lack precise spatial alignment. Given this observation, we propose a simple yet practical approach to align LR images without involving fine-tuning or generating 'smooth' trajectory from the trained 3D models over LR images. The experimental results show that the surprisingly simple algorithms can achieve the state-of-the-art results of 3D super-resolution tasks on standard benchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets. Project page: https://ko-lani.github.io/Sequence-Matters",
            "score": 1,
            "issue_id": 1265,
            "pub_date": "2024-12-16",
            "pub_date_card": {
                "ru": "16 декабря",
                "en": "December 16",
                "zh": "12月16日"
            },
            "hash": "39960ceb17dab1a5",
            "authors": [
                "Hyun-kyu Ko",
                "Dongheok Park",
                "Youngin Park",
                "Byeonghyeon Lee",
                "Juhee Han",
                "Eunbyung Park"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Sungkyunkwan University, Suwon, Korea",
                "Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea",
                "Visual Display Division, Samsung Electronics"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.11525.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Видео-суперразрешение открывает новые горизонты в 3D реконструкции",
                    "desc": "Эта статья посвящена 3D суперразрешению - методу реконструкции высококачественных 3D моделей из низкоразрешающих многоракурсных изображений. Авторы предлагают использовать модели видео-суперразрешения (VSR) вместо традиционных методов суперразрешения одиночных изображений. Такой подход обеспечивает лучшую пространственную согласованность и позволяет использовать информацию из окружающего контекста. Предложенный метод включает простой алгоритм выравнивания изображений низкого разрешения и демонстрирует высокие результаты на стандартных наборах данных."
                },
                "en": {
                    "title": "Enhancing 3D Models with Video Super-Resolution Techniques",
                    "desc": "This paper focuses on improving 3D super-resolution, which is the process of creating high-quality 3D models from low-resolution images taken from different angles. Traditional methods often struggle with maintaining consistency across different views because they treat each image separately. The authors propose using video super-resolution (VSR) techniques to enhance spatial consistency and leverage information from surrounding images, resulting in better 3D reconstructions. Their approach is simple and effective, achieving state-of-the-art results on well-known datasets without the need for complex fine-tuning."
                },
                "zh": {
                    "title": "利用视频超分辨率提升3D重建质量",
                    "desc": "本文研究了3D超分辨率技术，旨在从低分辨率的多视图图像中重建高保真度的3D模型。传统的单图像超分辨率(SISR)方法在处理每张图像时缺乏视图一致性，导致重建效果不佳。我们提出利用视频超分辨率(VSR)模型来提高空间一致性，并参考周围的空间信息，从而实现更准确和详细的重建。实验结果表明，我们的方法在标准基准数据集上达到了最先进的3D超分辨率效果。"
                }
            }
        }
    ],
    "link_prev": "2024-12-20.html",
    "link_next": "2024-12-24.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "20.12",
        "en": "12/20",
        "zh": "12月20日"
    },
    "short_date_next": {
        "ru": "24.12",
        "en": "12/24",
        "zh": "12月24日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 5,
        "#3d": 1,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 2,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "自回归模型在视觉生成中表现出强大的性能，但由于其顺序逐标记预测的过程，推理速度较慢。本文提出了一种简单有效的并行自回归视觉生成方法，提高生成效率，同时保留自回归建模的优势。我们的关键洞见是并行生成依赖于视觉标记的依赖关系：弱依赖的标记可以并行生成，而强依赖的相邻标记难以一起生成，因为它们的独立采样可能导致不一致。基于此观察，我们开发了一种并行生成策略，并行生成弱依赖的远距离标记，同时保持强依赖的局部标记的顺序生成。我们的方法可以无缝集成到标准自回归模型中，无需修改架构或标记器。在ImageNet和UCF-101上的实验表明，我们的方法在图像和视频生成任务中实现了最高9.5倍的加速，同时质量损失最小。我们希望这项工作能激发未来在高效视觉生成和统一自回归建模方面的研究。项目页面：https://epiphqny.github.io/PAR-project。",
        "title": "Parallelized Autoregressive Visual Generation",
        "pinyin": "Zì huíguī móxíng zài shìjué shēngchéng zhōng biǎoxiàn chū qiángdà de xìngnéng, dàn yóuyú qí shùnxù zhú biāojì yùcè de guòchéng, tuīlǐ sùdù jiào màn. Běnwén tíchū le yīzhǒng jiǎndān yǒuxiào de bìngxíng zìhuíguī shìjué shēngchéng fāngfǎ, tígāo shēngchéng xiàolǜ, tóngshí bǎoliú zìhuíguī jiànmó de yōushì. Wǒmen de guǎnjiàn dòngjiàn shì bìngxíng shēngchéng yīlài yú shìjué biāojì de yīlài guānxì: ruò yīlài de biāojì kěyǐ bìngxíng shēngchéng, ér qiáng yīlài de xiānglín biāojì nán yǐ yīqǐ shēngchéng, yīnwèi tāmen de dúlì cǎiyǎng kěnéng dǎozhì bù yīzhì. Jīyú cǐ guānchá, wǒmen kāifā le yīzhǒng bìngxíng shēngchéng cèlüè, bìngxíng shēngchéng ruò yīlài de yuǎn jùlí biāojì, tóngshí bǎochí qiáng yīlài de júbù biāojì de shùnxù shēngchéng. Wǒmen de fāngfǎ kěyǐ wúfèng jíchéng dào biāozhǔn zìhuíguī móxíng zhōng, wúxū xiūgǎi jiàgòu huò biāojìqì. Zài ImageNet hé UCF-101 shàng de shíyàn biǎomíng, wǒmen de fāngfǎ zài túxiàng hé shìpǐn shēngchéng rènwù zhōng shíxiàn le zuìgāo 9.5 bèi de jiāsù, tóngshí zhìliàng sǔnshī zuìshǎo. Wǒmen xīwàng zhè jiàn gōngzuò néng jīfā wèilái zài gāoxiào shìjué shēngchéng hé tǒngyī zìhuíguī jiànmó fāngmiàn de yánjiū. Xiàngmù yèmiàn: https://epiphqny.github.io/PAR-project.",
        "vocab": "[{'word': '自回归', 'pinyin': 'zì huí guī', 'trans': 'autoregressive'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},\n{'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generation'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '强大', 'pinyin': 'qiáng dà', 'trans': 'powerful'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '顺序', 'pinyin': 'shùn xù', 'trans': 'sequential'},\n{'word': '逐', 'pinyin': 'zhú', 'trans': 'gradual'},\n{'word': '标记', 'pinyin': 'biāo jì', 'trans': 'token'},\n{'word': '预测', 'pinyin': 'yù cè', 'trans': 'prediction'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'},\n{'word': '速度', 'pinyin': 'sù dù', 'trans': 'speed'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '并行', 'pinyin': 'bìng xíng', 'trans': 'parallel'},\n{'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'},\n{'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'},\n{'word': '效率', 'pinyin': 'xiào lǜ', 'trans': 'efficiency'},\n{'word': '保留', 'pinyin': 'bǎo liú', 'trans': 'retain'},\n{'word': '优势', 'pinyin': 'yōu shì', 'trans': 'advantage'},\n{'word': '关键', 'pinyin': 'guǎn jiàn', 'trans': 'key'},\n{'word': '洞见', 'pinyin': 'dòng jiàn', 'trans': 'insight'},\n{'word': '依赖', 'pinyin': 'yī lài', 'trans': 'dependency'},\n{'word': '关系', 'pinyin': 'guān xì', 'trans': 'relationship'},\n{'word': '弱', 'pinyin': 'ruò', 'trans': 'weak'},\n{'word': '强', 'pinyin': 'qiáng', 'trans': 'strong'},\n{'word': '相邻', 'pinyin': 'xiāng lín', 'trans': 'adjacent'},\n{'word': '难以', 'pinyin': 'nán yǐ', 'trans': 'difficult'},\n{'word': '独立', 'pinyin': 'dú lì', 'trans': 'independent'},\n{'word': '采样', 'pinyin': 'cǎi yàng', 'trans': 'sampling'},\n{'word': '不一致', 'pinyin': 'bù yī zhì', 'trans': 'inconsistency'},\n{'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'},\n{'word': '远距离', 'pinyin': 'yuǎn jù lí', 'trans': 'long-distance'},\n{'word': '局部', 'pinyin': 'jú bù', 'trans': 'local'},\n{'word': '无缝', 'pinyin': 'wú fèng', 'trans': 'seamless'},\n{'word': '集成', 'pinyin': 'jí chéng', 'trans': 'integrate'},\n{'word': '标准', 'pinyin': 'biāo zhǔn', 'trans': 'standard'},\n{'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'},\n{'word': '修改', 'pinyin': 'xiū gǎi', 'trans': 'modify'},\n{'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'},\n{'word': '图像', 'pinyin': 'tú xiàng', 'trans': 'image'},\n{'word': '视频', 'pinyin': 'shì pín', 'trans': 'video'},\n{'word': '任务', 'pinyin': 'rèn wù', 'trans': 'task'},\n{'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'},\n{'word': '加速', 'pinyin': 'jiā sù', 'trans': 'acceleration'},\n{'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'},\n{'word': '损失', 'pinyin': 'sǔn shī', 'trans': 'loss'},\n{'word': '激发', 'pinyin': 'jī fā', 'trans': 'inspire'},\n{'word': '未来', 'pinyin': 'wèi lái', 'trans': 'future'},\n{'word': '高效', 'pinyin': 'gāo xiào', 'trans': 'efficient'},\n{'word': '统一', 'pinyin': 'tǒng yī', 'trans': 'unified'},\n{'word': '项目', 'pinyin': 'xiàng mù', 'trans': 'project'},\n{'word': '页面', 'pinyin': 'yè miàn', 'trans': 'page'}]",
        "trans": "Autoregressive models have demonstrated strong performance in visual generation, but their sequential, token-by-token prediction process results in slow inference speeds. This paper proposes a simple and effective parallel autoregressive visual generation method that improves generation efficiency while retaining the advantages of autoregressive modeling. Our key insight is that parallel generation depends on the dependency relationships among visual tokens: weakly dependent tokens can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together because their independent sampling may lead to inconsistencies. Based on this observation, we developed a parallel generation strategy that generates weakly dependent distant tokens in parallel while maintaining the sequential generation of strongly dependent local tokens. Our method can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 show that our method achieves up to a 9.5x speedup in image and video generation tasks with minimal quality loss. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.",
        "update_ts": "2024-12-23 09:11"
    }
}