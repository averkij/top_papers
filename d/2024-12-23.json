{
    "date": {
        "ru": "23 декабря",
        "en": "December 23",
        "zh": "12月23日"
    },
    "time_utc": "2024-12-23 03:17",
    "weekday": 0,
    "issue_id": 1258,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.15119",
            "title": "Parallelized Autoregressive Visual Generation",
            "url": "https://huggingface.co/papers/2412.15119",
            "abstract": "Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependencies-tokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6x speedup with comparable quality and up to 9.5x speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.",
            "score": 4,
            "issue_id": 1258,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "0933582baa02f7a6",
            "authors": [
                "Yuqing Wang",
                "Shuhuai Ren",
                "Zhijie Lin",
                "Yujin Han",
                "Haoyuan Guo",
                "Zhenheng Yang",
                "Difan Zou",
                "Jiashi Feng",
                "Xihui Liu"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Peking University",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15119.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#video",
                    "#cv",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение авторегрессионной генерации изображений и видео без потери качества",
                    "desc": "Статья предлагает метод параллельного авторегрессионного генерирования визуального контента, который ускоряет процесс, сохраняя преимущества авторегрессионного моделирования. Авторы разработали стратегию, которая генерирует удаленные токены с слабыми зависимостями параллельно, но сохраняет последовательное генерирование для сильно зависимых локальных токенов. Метод легко интегрируется в стандартные авторегрессионные модели без изменения архитектуры или токенизатора. Эксперименты показали ускорение до 3.6 раз с сопоставимым качеством и до 9.5 раз с минимальной деградацией качества для задач генерации изображений и видео."
                },
                "en": {
                    "title": "Speeding Up Visual Generation with Smart Token Parallelization",
                    "desc": "This paper addresses the slow inference speed of autoregressive models used for visual generation, which typically generate images or videos one token at a time. The authors propose a new method that allows for parallel generation of visual tokens, focusing on the dependencies between tokens to determine which can be generated simultaneously. By identifying weakly dependent tokens that can be generated in parallel, while keeping strongly dependent tokens in a sequential order, the method enhances efficiency without altering the existing model architecture. Experiments show significant speed improvements in generating images and videos, making this approach a promising direction for future research in visual generation."
                },
                "zh": {
                    "title": "并行自回归生成，提升视觉生成效率",
                    "desc": "自回归模型在视觉生成中表现出色，但由于逐个预测的过程，推理速度较慢。本文提出了一种简单有效的并行自回归视觉生成方法，旨在提高生成效率，同时保留自回归建模的优点。我们的关键见解是，视觉标记之间的依赖关系决定了并行生成的可能性，弱依赖的标记可以并行生成，而强依赖的标记则难以一起生成。实验结果表明，我们的方法在图像和视频生成任务中实现了3.6倍的速度提升，且质量保持相当，甚至在某些情况下可达到9.5倍的速度提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.13649",
            "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
            "url": "https://huggingface.co/papers/2412.13649",
            "abstract": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the following two observations: (i) Excessive compression during the prefill phase, which requires specific full context impairs the comprehension of the reasoning task; (ii) Deviation of heavy hitters occurs in the reasoning tasks with long outputs. Therefore, SCOPE, a simple yet efficient framework that separately performs KV cache optimization during the prefill and decoding phases, is introduced. Specifically, the KV cache during the prefill phase is preserved to maintain the essential information, while a novel strategy based on sliding is proposed to select essential heavy hitters for the decoding phase. Memory usage and memory transfer are further optimized using adaptive and discontinuous strategies. Extensive experiments on LongGenBench show the effectiveness and generalization of SCOPE and its compatibility as a plug-in to other prefill-only KV compression methods.",
            "score": 2,
            "issue_id": 1258,
            "pub_date": "2024-12-18",
            "pub_date_card": {
                "ru": "18 декабря",
                "en": "December 18",
                "zh": "12月18日"
            },
            "hash": "885d11532659dd95",
            "authors": [
                "Jialong Wu",
                "Zhenglin Wang",
                "Linhai Zhang",
                "Yilong Lai",
                "Yulan He",
                "Deyu Zhou"
            ],
            "affiliations": [
                "Department of Informatics, Kings College London, UK",
                "School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China",
                "The Alan Turing Institute, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.13649.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективная оптимизация KV-кэша для длинных контекстов в LLM",
                    "desc": "Статья представляет SCOPE - фреймворк для оптимизации KV-кэша в моделях LLM при генерации длинных текстов. Авторы предлагают раздельную оптимизацию для этапов prefill и decoding, сохраняя важную информацию на первом этапе и выбирая ключевые элементы на втором. Используются адаптивные и прерывистые стратегии для оптимизации использования памяти. Эксперименты на LongGenBench показывают эффективность и обобщаемость SCOPE, а также его совместимость с другими методами сжатия KV-кэша."
                },
                "en": {
                    "title": "Optimizing KV Caches for Enhanced Long-Context Generation",
                    "desc": "This paper addresses the limitations of Key-Value (KV) caches in large language models (LLMs) when generating long outputs. It highlights that optimizing the decoding phase is essential, as excessive compression during the prefill phase can hinder reasoning tasks. The proposed framework, SCOPE, optimizes KV cache usage by preserving crucial information during prefill and employing a sliding strategy to select important data during decoding. Experimental results demonstrate that SCOPE improves memory efficiency and can be integrated with existing KV compression methods."
                },
                "zh": {
                    "title": "优化KV缓存，提升长输出生成效率",
                    "desc": "本文提出了一种名为SCOPE的框架，旨在优化长输出生成任务中的KV缓存。研究表明，在预填充阶段过度压缩会影响推理任务的理解，因此需要保留关键信息。SCOPE通过在预填充和解码阶段分别优化KV缓存，采用滑动策略选择重要的重击项，从而提高解码效率。实验结果表明，SCOPE在LongGenBench上表现出色，并且可以作为其他KV压缩方法的插件使用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15322",
            "title": "Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis",
            "url": "https://huggingface.co/papers/2412.15322",
            "abstract": "We propose to synthesize high-quality and synchronized audio, given video and optional text conditions, using a novel multimodal joint training framework MMAudio. In contrast to single-modality training conditioned on (limited) video data only, MMAudio is jointly trained with larger-scale, readily available text-audio data to learn to generate semantically aligned high-quality audio samples. Additionally, we improve audio-visual synchrony with a conditional synchronization module that aligns video conditions with audio latents at the frame level. Trained with a flow matching objective, MMAudio achieves new video-to-audio state-of-the-art among public models in terms of audio quality, semantic alignment, and audio-visual synchronization, while having a low inference time (1.23s to generate an 8s clip) and just 157M parameters. MMAudio also achieves surprisingly competitive performance in text-to-audio generation, showing that joint training does not hinder single-modality performance. Code and demo are available at: https://hkchengrex.github.io/MMAudio",
            "score": 1,
            "issue_id": 1258,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "9d724184f50de930",
            "authors": [
                "Ho Kei Cheng",
                "Masato Ishii",
                "Akio Hayakawa",
                "Takashi Shibuya",
                "Alexander Schwing",
                "Yuki Mitsufuji"
            ],
            "affiliations": [
                "Sony AI",
                "Sony Group Corporation",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15322.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#audio",
                    "#inference",
                    "#video",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "MMAudio: Революция в синтезе аудио по видео и тексту",
                    "desc": "MMAudio - это новая мультимодальная система для синтеза высококачественного и синхронизированного аудио на основе видео и опционального текста. Она использует совместное обучение на аудио-текстовых данных и условный модуль синхронизации для улучшения соответствия видео и звука. MMAudio достигает нового уровня качества в задаче генерации аудио по видео, превосходя существующие модели по качеству звука, семантическому соответствию и синхронизации. Модель также показывает хорошие результаты в генерации аудио по тексту."
                },
                "en": {
                    "title": "MMAudio: High-Quality Audio Synthesis with Video and Text Integration",
                    "desc": "This paper introduces MMAudio, a novel framework for generating high-quality audio that is synchronized with video and can also utilize text inputs. Unlike traditional methods that rely solely on video data, MMAudio leverages a larger dataset of text-audio pairs to enhance the semantic alignment of the generated audio. The framework includes a conditional synchronization module that ensures audio is aligned with video at the frame level, improving overall coherence. With a flow matching objective, MMAudio sets new benchmarks in audio quality and synchronization while maintaining efficient performance with a low number of parameters."
                },
                "zh": {
                    "title": "MMAudio：高质量音频合成的新方法",
                    "desc": "我们提出了一种新的多模态联合训练框架MMAudio，用于合成高质量和同步的音频，基于视频和可选的文本条件。与仅依赖视频数据的单模态训练不同，MMAudio结合了大规模的文本-音频数据进行联合训练，以生成语义对齐的高质量音频样本。此外，我们通过条件同步模块在帧级别上对视频条件和音频潜在特征进行对齐，从而提高音频与视频的同步性。MMAudio在音频质量、语义对齐和音频-视觉同步方面达到了新的公共模型的最佳水平，同时推理时间低（生成8秒片段仅需1.23秒），参数量仅为157M。"
                }
            }
        }
    ],
    "link_prev": "2024-12-20.html",
    "link_next": "2024-12-24.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "20.12",
        "en": "12/20",
        "zh": "12月20日"
    },
    "short_date_next": {
        "ru": "24.12",
        "en": "12/24",
        "zh": "12月24日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 0,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了Qwen2.5，一系列大型语言模型。与前一版本相比，Qwen2.5在预训练和后训练阶段都有显著改进。预训练数据集从7万亿个标记扩展到18万亿个，增强了常识、专业知识和推理能力。后训练使用了超过100万个样本的监督微调和多阶段强化学习，提升了人类偏好和长文本生成等能力。Qwen2.5系列提供多种尺寸和版本，包括基础模型和指令微调模型，以及量化版本。",
        "title": "Qwen2.5 Technical Report",
        "pinyin": "Zhè piān wénzhāng jièshào le Qwen2.5, yī xìliè dàxíng yǔyán móxíng. Yǔ qián yī bǎnběn xiāngbǐ, Qwen2.5 zài yùxùnliàn hé hòuxùnliàn jiēduàn dōu yǒu xiǎnzhù gǎijìn. Yùxùnliàn shùjùjí cóng 7 wàn yì gè biāojì kuòzhǎn dào 18 wàn yì gè, zēngqiáng le chángshí, zhuānxiàng zhīshì hé tuīlǐ nénglì. Hòuxùnliàn shǐyòng le chāoguò 100 wàn gè yàngbǎn de jiàndū wēitiáo hé duō jiēduàn qiángzhù xuéxí, tíshēng le rénlèi piānfú hé cháng wénběn shēngchéng děng nénglì. Qwen2.5 xìliè tígōng duōzhǒng chǐcùn hé bǎnběn, bāokuò jīchǔ móxíng hé zhǐlǐng wēitiáo móxíng, yǐjià liàngzhù bǎnběn.",
        "vocab": "[{'word': '篇', 'pinyin': 'piān', 'trans': 'piece of writing'}, {'word': '介绍', 'pinyin': 'jièshào', 'trans': 'introduce'}, {'word': '语言模型', 'pinyin': 'yǔyán móxíng', 'trans': 'language model'}, {'word': '前一版本', 'pinyin': 'qián yī bǎnběn', 'trans': 'previous version'}, {'word': '相比', 'pinyin': 'xiāngbǐ', 'trans': 'compare'}, {'word': '预训练', 'pinyin': 'yù xùnliàn', 'trans': 'pre-training'}, {'word': '后训练', 'pinyin': 'hòu xùnliàn', 'trans': 'post-training'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '改进', 'pinyin': 'gǎijìn', 'trans': 'improvement'}, {'word': '标记', 'pinyin': 'biāojì', 'trans': 'token'}, {'word': '扩展', 'pinyin': 'kuòzhǎn', 'trans': 'expand'}, {'word': '常识', 'pinyin': 'chángshí', 'trans': 'common sense'}, {'word': '专业知识', 'pinyin': 'zhuānyè zhīshi', 'trans': 'professional knowledge'}, {'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'reasoning'}, {'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'}, {'word': '监督', 'pinyin': 'jiàndū', 'trans': 'supervised'}, {'word': '微调', 'pinyin': 'wēitiáo', 'trans': 'fine-tuning'}, {'word': '多阶段', 'pinyin': 'duō jiēduàn', 'trans': 'multi-stage'}, {'word': '强化学习', 'pinyin': 'qiángjià xuéxí', 'trans': 'reinforcement learning'}, {'word': '提升', 'pinyin': 'tíshēng', 'trans': 'enhance'}, {'word': '人类偏好', 'pinyin': 'rénlèi piānhào', 'trans': 'human preference'}, {'word': '长文本', 'pinyin': 'cháng wénběn', 'trans': 'long text'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generation'}, {'word': '尺寸', 'pinyin': 'chǐcùn', 'trans': 'size'}, {'word': '基础模型', 'pinyin': 'jīchǔ móxíng', 'trans': 'base model'}, {'word': '指令', 'pinyin': 'zhǐlìng', 'trans': 'instruction'}, {'word': '量化', 'pinyin': 'liànghuà', 'trans': 'quantization'}]",
        "trans": "This article introduces Qwen2.5, a series of large language models. Compared to the previous version, Qwen2.5 has significant improvements in both the pre-training and post-training stages. The pre-training dataset has been expanded from 7 trillion tokens to 18 trillion tokens, enhancing common sense, professional knowledge, and reasoning capabilities. Post-training involved supervised fine-tuning with over 1 million samples and multi-stage reinforcement learning, improving human preference and long text generation capabilities. The Qwen2.5 series offers various sizes and versions, including base models, instruction-tuned models, and quantized versions.",
        "update_ts": "2024-12-22 12:38"
    }
}