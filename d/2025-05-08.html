
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 17 papers. May 7.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">7 мая</span> | <span id="title-articles-count">17 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-06.html">⬅️ <span id="prev-date">06.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-08.html">➡️ <span id="next-date">08.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '7 мая', 'en': 'May 7', 'zh': '5月7日'};
        let feedDateNext = {'ru': '08.05', 'en': '05/08', 'zh': '5月8日'};
        let feedDatePrev = {'ru': '06.05', 'en': '05/06', 'zh': '5月6日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.03318', 'title': 'Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.03318', 'abstract': "Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model.", 'score': 63, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'f0871f80f0b8fdd9', 'authors': ['Yibin Wang', 'Zhimin Li', 'Yuhang Zang', 'Chunyu Wang', 'Qinglin Lu', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Hunyuan, Tencent', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.03318.jpg', 'data': {'categories': ['#rlhf', '#alignment', '#multimodal', '#training', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Улучшение надежности мультимодальных моделей вознаграждения через цепочки рассуждений', 'desc': 'В статье представлен UnifiedReward-Think - первая унифицированная мультимодальная модель вознаграждения, основанная на цепочках рассуждений (CoT). Модель способна проводить многомерные пошаговые рассуждения для задач визуального понимания и генерации. Авторы используют подход обучения с подкреплением для выявления и стимулирования скрытых способностей модели к сложным рассуждениям. Эксперименты показывают превосходство предложенной модели в различных задачах визуального вознаграждения.'}, 'en': {'title': 'Empowering Vision Models with Long-Chain Reasoning', 'desc': 'This paper introduces UnifiedReward-Think, a novel multimodal reward model that enhances the alignment of vision models with human preferences through long-chain reasoning. By integrating explicit chains of thought (CoT) into the reward reasoning process, the model improves the accuracy and reliability of reward signals. The approach involves a two-step training process: first, distilling reasoning from a small dataset, and then fine-tuning with large-scale multimodal preference data. Experimental results show that this method significantly outperforms existing models in various vision tasks, demonstrating its effectiveness in complex reasoning scenarios.'}, 'zh': {'title': '长链思维提升多模态奖励模型的可靠性', 'desc': '最近在多模态奖励模型（RMs）方面的进展显示出将视觉模型与人类偏好对齐的潜力。然而，目前的RMs通常只能提供直接响应或进行浅层推理，导致奖励信号不准确。我们认为，将明确的长链思维（CoT）纳入奖励推理过程可以显著增强其可靠性和稳健性。本文提出了UnifiedReward-Think，这是第一个统一的基于CoT的多模态奖励模型，能够进行多维度、逐步的长链推理，适用于视觉理解和生成奖励任务。'}}}, {'id': 'https://huggingface.co/papers/2505.03335', 'title': 'Absolute Zero: Reinforced Self-play Reasoning with Zero Data', 'url': 'https://huggingface.co/papers/2505.03335', 'abstract': 'Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.', 'score': 60, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'b53e736d1884218d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#rlhf', '#training', '#rl', '#math', '#optimization', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Самообучающийся ИИ: революция в обучении с подкреплением', 'desc': 'Статья представляет новую парадигму обучения с подкреплением с проверяемыми наградами (RLVR) под названием Absolute Zero. В рамках этой парадигмы модель самостоятельно генерирует задачи для максимизации собственного прогресса обучения, не полагаясь на внешние данные. Авторы представляют систему Absolute Zero Reasoner (AZR), которая развивает свою учебную программу и способности к рассуждению, используя исполнитель кода для проверки предложенных задач и ответов. Несмотря на отсутствие внешних данных при обучении, AZR достигает лучших результатов в задачах кодирования и математических рассуждений по сравнению с существующими моделями.'}, 'en': {'title': 'Self-Learning AI: No Data, No Problem!', 'desc': 'This paper introduces a new approach in reinforcement learning called Absolute Zero, which allows a model to learn and improve its reasoning skills without needing external data or human supervision. The proposed Absolute Zero Reasoner (AZR) autonomously generates tasks that enhance its learning and validates its own reasoning through a code executor. This self-sufficient learning method leads to state-of-the-art performance in coding and mathematical reasoning tasks, surpassing models that rely on large datasets of human-created examples. The findings suggest that AZR can adapt to different model sizes and types, showcasing its versatility and potential for future AI development.'}, 'zh': {'title': '绝对零：自我进化的推理模型', 'desc': '强化学习与可验证奖励（RLVR）在提升大型语言模型的推理能力方面表现出色，能够直接从结果导向的奖励中学习。最近的RLVR研究在零设置下运行，避免了对推理过程的监督，但仍依赖于人工策划的问题和答案集合进行训练。由于高质量人类生成示例的稀缺性，依赖人类监督的长期可扩展性受到质疑。为了解决这些问题，我们提出了一种新的RLVR范式，称为绝对零（Absolute Zero），该范式下的模型能够自我提出任务以最大化学习进展，并通过解决这些任务来提升推理能力，而无需依赖任何外部数据。'}}}, {'id': 'https://huggingface.co/papers/2505.03005', 'title': 'RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale', 'url': 'https://huggingface.co/papers/2505.03005', 'abstract': "We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \\$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.   Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper", 'score': 22, 'issue_id': 3625, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '0fe1c0b1575b6708', 'authors': ['Daniel Goldstein', 'Eric Alcaide', 'Janna Lu', 'Eugene Cheah'], 'affiliations': ['Dalle Molle Institute for Artificial Intelligence USI-SUPSI', 'EleutherAI', 'George Mason University', 'Recursal AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.03005.jpg', 'data': {'categories': ['#architecture', '#training', '#open_source', '#inference', '#benchmark', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Эффективное преобразование трансформеров в модели с линейным вниманием', 'desc': 'В статье представлен метод Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS) для быстрого преобразования трансформеров с софтмакс-вниманием в модели декодеров с линейным вниманием. Авторы разработали две новые архитектуры на основе RWKV и конвертировали популярные модели Qwen2.5 размером 7B, 32B и 72B. Процесс конвертации требует всего 350-700 млн токенов, что составляет менее 0,005% от количества токенов, использованных для обучения исходных моделей. Полученные модели с линейным вниманием демонстрируют высокую производительность на стандартных бенчмарках.'}, 'en': {'title': 'Transforming Transformers: Efficient Linear Attention Models with RADLADS', 'desc': 'The paper introduces Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a method for transforming softmax attention transformers into efficient linear attention models. This conversion process is highly efficient, requiring only a small fraction of the original training data, specifically 350-700M tokens. Despite the reduced training cost, the resulting 72B linear attention model maintains performance levels comparable to its transformer counterparts. The authors also present new RWKV-variant architectures and make their models available on HuggingFace, demonstrating state-of-the-art results on standard benchmarks for linear attention models.'}, 'zh': {'title': '快速转换，线性注意力的未来', 'desc': '我们提出了一种快速注意力蒸馏到线性注意解码器的协议（RADLADS），可以迅速将软最大注意力变换器转换为线性注意解码模型。我们的转换过程只需350-700M个标记，远低于原始教师模型训练所需的0.005%的标记数量。转换为我们的72B线性注意模型的成本不到2000美元，但推理质量仍接近原始变换器。我们在标准基准测试中实现了同类最佳的下游性能，并将所有模型发布在HuggingFace上。'}}}, {'id': 'https://huggingface.co/papers/2505.03730', 'title': 'FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios', 'url': 'https://huggingface.co/papers/2505.03730', 'abstract': 'Action customization involves generating videos where the subject performs actions dictated by input control signals. Current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure, such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. To overcome these limitations, we propose FlexiAct, which transfers actions from a reference video to an arbitrary target image. Unlike existing methods, FlexiAct allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. Achieving this requires precise action control, spatial structure adaptation, and consistency preservation. To this end, we introduce RefAdapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. Additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. Experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. We release our code and model weights to support further research at https://shiyi-zh0408.github.io/projectpages/FlexiAct/', 'score': 21, 'issue_id': 3624, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '2329fe6d2462c9c9', 'authors': ['Shiyi Zhang', 'Junhao Zhuang', 'Zhaoyang Zhang', 'Ying Shan', 'Yansong Tang'], 'affiliations': ['Tencent ARC Lab, China', 'Tsinghua Shenzhen International Graduate School, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03730.jpg', 'data': {'categories': ['#open_source', '#transfer_learning', '#multimodal', '#video'], 'emoji': '🎭', 'ru': {'title': 'Гибкий перенос действий между разными субъектами и сценариями', 'desc': 'Статья представляет FlexiAct - метод для переноса действий с референсного видео на произвольное целевое изображение. В отличие от существующих подходов, FlexiAct позволяет варьировать компоновку, ракурс и скелетную структуру между субъектом референсного видео и целевым изображением, сохраняя при этом идентичность. Для достижения этой цели авторы вводят RefAdapter - легковесный адаптер, обусловленный изображением, который превосходит существующие методы в балансировке согласованности внешнего вида и структурной гибкости. Также предлагается FAE (Frequency-aware Action Extraction) для извлечения действий непосредственно в процессе шумоподавления.'}, 'en': {'title': 'FlexiAct: Flexible Action Transfer for Diverse Video Customization', 'desc': 'The paper presents FlexiAct, a novel approach for customizing action videos by transferring actions from a reference video to a target image, regardless of differences in layout, viewpoint, and skeletal structure. This method addresses the limitations of existing techniques that require strict spatial consistency, allowing for greater adaptability across various subjects and scenarios. FlexiAct utilizes a lightweight image-conditioned adapter called RefAdapter to ensure identity consistency while adapting spatial structures. Additionally, it introduces Frequency-aware Action Extraction (FAE) to enhance action extraction during the denoising process, achieving superior results in maintaining both appearance and structural flexibility.'}, 'zh': {'title': '灵活的动作转移，打破空间限制', 'desc': '本文提出了一种名为FlexiAct的方法，用于根据输入控制信号生成视频，允许在不同布局、视角和骨架结构之间进行动作转移。与现有方法不同，FlexiAct能够在保持身份一致性的同时，适应目标图像的空间结构变化。为实现这一目标，文章引入了RefAdapter，一个轻量级的图像条件适配器，能够在外观一致性和结构灵活性之间取得良好平衡。此外，提出的FAE方法在去噪过程中直接提取动作，克服了传统方法的局限性。'}}}, {'id': 'https://huggingface.co/papers/2505.02922', 'title': 'RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference', 'url': 'https://huggingface.co/papers/2505.02922', 'abstract': 'The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.', 'score': 18, 'issue_id': 3624, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'd7e3545dcade10b4', 'authors': ['Yaoqi Chen', 'Jinkai Zhang', 'Baotong Lu', 'Qianxi Zhang', 'Chengruidong Zhang', 'Jingjia Luo', 'Di Liu', 'Huiqiang Jiang', 'Qi Chen', 'Jing Liu', 'Bailu Ding', 'Xiao Yan', 'Jiawei Jiang', 'Chen Chen', 'Mingxing Zhang', 'Yuqing Yang', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Microsoft Research', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02922.jpg', 'data': {'categories': ['#long_context', '#inference', '#benchmark', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'RetroInfer: революция в эффективности вывода LLM с длинным контекстом', 'desc': 'RetroInfer - это новая система, которая переосмысливает кэш ключ-значение как систему хранения векторов для ускорения вывода LLM с длинным контекстом. В основе системы лежит волновой индекс, использующий разреженность внимания для эффективного извлечения критических токенов. RetroInfer также включает волновой буфер для координации размещения кэша и оптимизации вычислений между GPU и CPU. Эксперименты показывают ускорение до 4.5 раз по сравнению с полным вниманием в пределах памяти GPU и до 10.5 раз по сравнению с базовыми методами разреженного внимания при сохранении точности.'}, 'en': {'title': 'Accelerating Long-Context Inference with RetroInfer', 'desc': 'This paper introduces RetroInfer, a system designed to improve the efficiency of large language models (LLMs) during inference by addressing GPU memory and bandwidth limitations. It innovatively redefines the key-value (KV) cache as a vector storage system that leverages attention sparsity to speed up processing of long contexts. The core component, the wave index, utilizes advanced techniques for token retrieval, ensuring both efficiency and accuracy. Additionally, the wave buffer optimizes the coordination of KV cache and computation, achieving significant speed improvements while maintaining high model accuracy.'}, 'zh': {'title': '高效推理，突破上下文限制！', 'desc': '随着大型语言模型（LLMs）上下文长度的增加，推理效率面临显著挑战，主要是由于GPU内存和带宽的限制。我们提出了RetroInfer，这是一种新颖的系统，将关键值（KV）缓存重新概念化为向量存储系统，利用内在的注意力稀疏性来加速长上下文LLM推理。其核心是波动索引（wave index），一种注意力感知向量索引，能够通过三方注意力近似、精度受限的注意力估计和分段聚类等技术高效准确地检索关键标记。与以往在标记选择和硬件协调上存在困难的稀疏性方法不同，RetroInfer在不影响模型准确性的情况下提供了强大的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.02872', 'title': 'Decoding Open-Ended Information Seeking Goals from Eye Movements in\n  Reading', 'url': 'https://huggingface.co/papers/2505.02872', 'abstract': "When reading, we often have specific information that interests us in a text. For example, you might be reading this paper because you are curious about LLMs for eye movements in reading, the experimental design, or perhaps you only care about the question ``but does it work?''. More broadly, in daily life, people approach texts with any number of text-specific goals that guide their reading behavior. In this work, we ask, for the first time, whether open-ended reading goals can be automatically decoded from eye movements in reading. To address this question, we introduce goal classification and goal reconstruction tasks and evaluation frameworks, and use large-scale eye tracking for reading data in English with hundreds of text-specific information seeking tasks. We develop and compare several discriminative and generative multimodal LLMs that combine eye movements and text for goal classification and goal reconstruction. Our experiments show considerable success on both tasks, suggesting that LLMs can extract valuable information about the readers' text-specific goals from eye movements.", 'score': 14, 'issue_id': 3629, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': 'aae5b0f63189eb32', 'authors': ['Cfir Avraham Hadar', 'Omer Shubi', 'Yoav Meiri', 'Yevgeni Berzak'], 'affiliations': ['Faculty of Data and Decision Sciences, Technion - Israel Institute of Technology, Haifa, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2505.02872.jpg', 'data': {'categories': ['#multimodal', '#science', '#dataset', '#benchmark', '#interpretability', '#agi'], 'emoji': '👁️', 'ru': {'title': 'Расшифровка целей чтения по движениям глаз с помощью ИИ', 'desc': 'В этой статье исследуется возможность автоматического определения целей чтения на основе движений глаз. Авторы разработали задачи классификации и реконструкции целей, используя крупномасштабные данные отслеживания движений глаз при чтении на английском языке. Они создали и сравнили несколько дискриминативных и генеративных мультимодальных языковых моделей (LLM), объединяющих движения глаз и текст. Эксперименты показали значительный успех в обеих задачах, что указывает на способность LLM извлекать ценную информацию о целях чтения из движений глаз.'}, 'en': {'title': 'Decoding Reading Goals from Eye Movements with LLMs', 'desc': "This paper explores how eye movements during reading can reveal a reader's specific goals, such as information seeking. The authors introduce new tasks for classifying and reconstructing these goals using large-scale eye tracking data. They develop and evaluate various multimodal large language models (LLMs) that integrate eye movement data with text to improve goal understanding. The results indicate that these models can effectively decode readers' intentions, demonstrating the potential of LLMs in understanding reading behavior."}, 'zh': {'title': '从眼动数据解码阅读目标的创新研究', 'desc': '本研究首次探讨了如何从阅读时的眼动数据自动解码开放式阅读目标。我们引入了目标分类和目标重建任务，并建立了评估框架，使用了大规模的眼动追踪数据。通过结合眼动和文本信息，我们开发并比较了多种判别式和生成式的多模态大语言模型（LLMs）。实验结果表明，这些模型在提取读者的文本特定目标方面表现出显著的成功。'}}}, {'id': 'https://huggingface.co/papers/2505.02214', 'title': 'An Empirical Study of Qwen3 Quantization', 'url': 'https://huggingface.co/papers/2505.02214', 'abstract': "The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.", 'score': 12, 'issue_id': 3627, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': 'be32ffc34f30d354', 'authors': ['Xingyu Zheng', 'Yuye Li', 'Haoran Chu', 'Yue Feng', 'Xudong Ma', 'Jie Luo', 'Jinyang Guo', 'Haotong Qin', 'Michele Magno', 'Xianglong Liu'], 'affiliations': ['Beihang University', 'ETH Zürich', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02214.jpg', 'data': {'categories': ['#inference', '#optimization', '#training', '#open_source', '#low_resource'], 'emoji': '🔬', 'ru': {'title': 'Квантование Qwen3: баланс между эффективностью и точностью', 'desc': 'Исследование посвящено оценке эффективности квантования модели Qwen3, одной из ведущих открытых больших языковых моделей (LLM). Авторы применили 5 классических методов пост-тренировочного квантования с различной битовой глубиной от 1 до 8 бит. Результаты показали, что Qwen3 сохраняет высокую производительность при умеренном квантовании, но значительно теряет в качестве при сверхнизкой точности. Это исследование подчеркивает необходимость дальнейших разработок для минимизации потерь производительности при экстремальном квантовании LLM.'}, 'en': {'title': "Unlocking Efficiency: Evaluating Qwen3's Performance Under Quantization", 'desc': "The Qwen series represents a significant advancement in open-source Large Language Models (LLMs), particularly with the introduction of Qwen3, which excels in natural language understanding tasks. This paper investigates the effects of low-bit quantization on Qwen3's performance, focusing on how different quantization techniques impact its robustness. By evaluating five classic post-training quantization methods across various bit-widths, the study reveals that while Qwen3 performs well at moderate bit-widths, it struggles with linguistic tasks at ultra-low precision. The findings highlight the challenges of compressing LLMs and suggest the need for further research to improve quantization strategies without sacrificing model accuracy."}, 'zh': {'title': '探索Qwen3的量化挑战与机遇', 'desc': 'Qwen系列是一个领先的开源大型语言模型（LLM），在自然语言理解任务中表现出色。最近发布的Qwen3在多个基准测试中表现优异，吸引了在资源受限环境中高效部署的关注。本文系统评估了Qwen3在不同量化设置下的鲁棒性，探讨了压缩这一先进模型的机遇与挑战。研究发现，尽管在中等位宽下Qwen3的性能仍具竞争力，但在超低精度下语言任务的表现显著下降，强调了LLM压缩中的持续难题。'}}}, {'id': 'https://huggingface.co/papers/2505.03735', 'title': 'Multi-Agent System for Comprehensive Soccer Understanding', 'url': 'https://huggingface.co/papers/2505.03735', 'abstract': 'Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributions in this paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, featuring around 10K standardized multimodal (text, image, video) multi-choice QA pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce SoccerAgent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from SoccerWiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our proposed agentic system. All data and code are publicly available at: https://jyrao.github.io/SoccerAgent/.', 'score': 9, 'issue_id': 3625, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'f62fbd87d3f6f548', 'authors': ['Jiayuan Rao', 'Zifeng Li', 'Haoning Wu', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['SAI, Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.03735.jpg', 'data': {'categories': ['#open_source', '#survey', '#benchmark', '#dataset', '#reasoning', '#multimodal', '#agents'], 'emoji': '⚽', 'ru': {'title': 'Революция в ИИ-анализе футбола: от знаний к пониманию', 'desc': 'Статья представляет комплексный подход к пониманию футбола с помощью искусственного интеллекта. Авторы создали SoccerWiki - первую крупномасштабную мультимодальную базу знаний о футболе, и SoccerBench - обширный набор тестовых заданий для оценки понимания футбола ИИ-системами. Также они разработали SoccerAgent - мультиагентную систему, которая декомпозирует сложные вопросы о футболе путем совместных рассуждений. Исследование демонстрирует превосходство предложенного агентного подхода над современными мультимодальными языковыми моделями в задачах понимания футбола.'}, 'en': {'title': 'Revolutionizing Soccer Understanding with AI', 'desc': 'This paper presents a new framework for understanding soccer using AI, addressing the limitations of previous research that focused on narrow tasks. The authors introduce SoccerWiki, a large multimodal knowledge base that contains detailed information about various aspects of soccer, enabling better reasoning. They also create SoccerBench, a comprehensive benchmark with thousands of multimodal question-answer pairs to evaluate soccer understanding tasks. Finally, the paper introduces SoccerAgent, a multi-agent system that collaborates to answer complex soccer questions, demonstrating improved performance through extensive evaluations.'}, 'zh': {'title': '全面提升足球理解的智能框架', 'desc': '本论文提出了一个全面的足球理解框架，以填补现有研究的空白。我们构建了SoccerWiki，这是第一个大规模的多模态足球知识库，整合了关于球员、球队、裁判和场馆的丰富领域知识。我们还推出了SoccerBench，这是最大的足球特定基准，包含约10,000个标准化的多模态多选问答对，涵盖13个不同的理解任务。最后，我们介绍了SoccerAgent，一个新颖的多智能体系统，通过协作推理分解复杂的足球问题，展示了其卓越的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.03739', 'title': 'VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model', 'url': 'https://huggingface.co/papers/2505.03739', 'abstract': 'With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.', 'score': 6, 'issue_id': 3632, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': 'd1a6985b437b7562', 'authors': ['Zuwei Long', 'Yunhang Shen', 'Chaoyou Fu', 'Heting Gao', 'Lijiang Li', 'Peixian Chen', 'Mengdan Zhang', 'Hang Shao', 'Jian Li', 'Jinlong Peng', 'Haoyu Cao', 'Ke Li', 'Rongrong Ji', 'Xing Sun'], 'affiliations': ['Nanjing University', 'Tencent Youtu Lab', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03739.jpg', 'data': {'categories': ['#training', '#multimodal', '#audio', '#inference', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'Революция в речевых технологиях: мгновенная генерация аудио с минимальной задержкой', 'desc': 'VITA-Audio - это инновационная крупномасштабная речевая модель, решающая проблему высокой задержки при генерации первого аудиотокена в потоковых системах. Модель использует легковесный модуль множественного кросс-модального предсказания токенов (MCTP), который эффективно генерирует несколько аудиотокенов за один проход модели. Применяется четырехэтапная стратегия прогрессивного обучения для ускорения модели при минимальной потере качества речи. VITA-Audio демонстрирует ускорение вывода в 3-5 раз и превосходит модели с открытым исходным кодом аналогичного размера по нескольким бенчмаркам для задач ASR, TTS и SQA.'}, 'en': {'title': 'VITA-Audio: Fast, Real-Time Speech Generation for Seamless Interaction', 'desc': 'The paper introduces VITA-Audio, a novel end-to-end speech model designed to enhance real-time human-computer interaction by reducing latency in audio generation. It features a Multiple Cross-modal Token Prediction (MCTP) module that allows for the simultaneous generation of multiple audio tokens, significantly speeding up the inference process. The model employs a four-stage progressive training strategy to maintain high speech quality while achieving faster performance. VITA-Audio stands out as the first multi-modal large language model capable of producing audio output during the initial forward pass, demonstrating a 3-5x speedup compared to existing models while excelling in various speech-related tasks.'}, 'zh': {'title': 'VITA-Audio：实时语音生成的新突破', 'desc': '随着人机交互需求的增加，基于语音的系统受到越来越多的关注。现有的语音模型在生成第一个音频标记时存在高延迟，这限制了其应用。为了解决这个问题，我们提出了VITA-Audio，这是一种端到端的大型语音模型，能够快速生成音频文本标记。我们的模型通过引入轻量级的多模态交叉标记预测模块，显著加快了推理速度，并在流媒体场景中减少了延迟。'}}}, {'id': 'https://huggingface.co/papers/2505.03368', 'title': 'Geospatial Mechanistic Interpretability of Large Language Models', 'url': 'https://huggingface.co/papers/2505.03368', 'abstract': 'Large Language Models (LLMs) have demonstrated unprecedented capabilities across various natural language processing tasks. Their ability to process and generate viable text and code has made them ubiquitous in many fields, while their deployment as knowledge bases and "reasoning" tools remains an area of ongoing research. In geography, a growing body of literature has been focusing on evaluating LLMs\' geographical knowledge and their ability to perform spatial reasoning. However, very little is still known about the internal functioning of these models, especially about how they process geographical information.   In this chapter, we establish a novel framework for the study of geospatial mechanistic interpretability - using spatial analysis to reverse engineer how LLMs handle geographical information. Our aim is to advance our understanding of the internal representations that these complex models generate while processing geographical information - what one might call "how LLMs think about geographic information" if such phrasing was not an undue anthropomorphism.   We first outline the use of probing in revealing internal structures within LLMs. We then introduce the field of mechanistic interpretability, discussing the superposition hypothesis and the role of sparse autoencoders in disentangling polysemantic internal representations of LLMs into more interpretable, monosemantic features. In our experiments, we use spatial autocorrelation to show how features obtained for placenames display spatial patterns related to their geographic location and can thus be interpreted geospatially, providing insights into how these models process geographical information. We conclude by discussing how our framework can help shape the study and use of foundation models in geography.', 'score': 5, 'issue_id': 3629, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '1901718a6f967dbe', 'authors': ['Stef De Sabbata', 'Stefano Mizzaro', 'Kevin Roitero'], 'affiliations': ['University of Leicester, UK', 'University of Udine, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2505.03368.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#science', '#data', '#architecture', '#interpretability'], 'emoji': '🌍', 'ru': {'title': 'Заглядывая в мозг ИИ: как языковые модели понимают географию', 'desc': 'Статья исследует внутренние механизмы обработки географической информации в больших языковых моделях (LLM). Авторы предлагают новый подход к геопространственной интерпретируемости LLM, используя методы пространственного анализа. Они применяют пробинг и разреженные автоэнкодеры для выявления внутренних представлений географических данных в моделях. Исследование показывает, что извлеченные признаки для топонимов демонстрируют пространственные паттерны, связанные с их географическим положением.'}, 'en': {'title': 'Unveiling How LLMs Think Geographically', 'desc': 'This paper explores how Large Language Models (LLMs) understand and process geographical information. It introduces a framework for geospatial mechanistic interpretability, which aims to reverse engineer the internal workings of LLMs using spatial analysis. The authors utilize probing techniques and sparse autoencoders to uncover how LLMs represent geographic concepts, revealing patterns in their internal features. By demonstrating spatial autocorrelation in placenames, the study provides insights into the spatial reasoning capabilities of LLMs and their implications for geography.'}, 'zh': {'title': '揭示大型语言模型的地理信息处理机制', 'desc': '大型语言模型（LLMs）在自然语言处理任务中展现了前所未有的能力，尤其是在文本和代码的处理与生成方面。本文提出了一种新框架，旨在研究LLMs如何处理地理信息，特别是其内部机制的可解释性。我们通过空间分析和探测技术，揭示LLMs内部结构，并利用稀疏自编码器将多义性特征分解为更易解释的单义特征。实验结果表明，地名特征与其地理位置之间存在空间相关性，从而为理解LLMs处理地理信息的方式提供了新的视角。'}}}, {'id': 'https://huggingface.co/papers/2504.21650', 'title': 'HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene\n  Generation', 'url': 'https://huggingface.co/papers/2504.21650', 'abstract': "The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.", 'score': 5, 'issue_id': 3632, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': 'a04fa605a99969e5', 'authors': ['Haiyang Zhou', 'Wangbo Yu', 'Jiawen Guan', 'Xinhua Cheng', 'Yonghong Tian', 'Li Yuan'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Peng Cheng Laboratory', 'School of Electronic and Computer Engineering, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21650.jpg', 'data': {'categories': ['#3d', '#video', '#cv', '#dataset', '#diffusion'], 'emoji': '🌐', 'ru': {'title': 'Погружение в виртуальные миры: от панорамных видео к 4D-сценам', 'desc': 'HoloTime - это новая система для создания панорамных видео и 4D-сцен для VR и AR на основе диффузионных моделей. Она включает в себя генерацию панорамных видео по одному изображению или текстовому запросу с помощью модели Panoramic Animator. Затем производится реконструкция 4D-сцены методом Panoramic Space-Time Reconstruction. Для обучения моделей был создан набор данных 360World с панорамными видео.'}, 'en': {'title': 'Transforming VR and AR with HoloTime: Immersive 4D Experiences from Panoramic Videos', 'desc': 'This paper introduces HoloTime, a novel framework that enhances virtual reality (VR) and augmented reality (AR) experiences by generating immersive 4D assets from video diffusion models. It addresses the limitations of current models that focus on static 3D scenes by proposing a method to create panoramic videos from a single image, which are then transformed into 4D representations. The authors present the 360World dataset, a unique collection of panoramic videos that supports advanced 4D scene reconstruction tasks. Their approach, validated through comparative analysis, shows significant improvements in generating high-quality panoramic videos and reconstructing 4D scenes, ultimately leading to more engaging user experiences in immersive environments.'}, 'zh': {'title': 'HoloTime：提升VR和AR沉浸体验的全景视频生成框架', 'desc': '本论文提出了一种名为HoloTime的框架，旨在通过视频扩散模型生成全景视频，从而提升虚拟现实（VR）和增强现实（AR）技术的沉浸体验。我们引入了360World数据集，这是第一个专门用于4D场景重建任务的全景视频集合。通过Panoramic Animator模型，我们能够将全景图像转换为高质量的全景视频。最后，利用空间-时间深度估计方法，我们将生成的视频转化为4D点云，实现了更真实的4D场景重建。'}}}, {'id': 'https://huggingface.co/papers/2505.03164', 'title': 'InfoVids: Reimagining the Viewer Experience with Alternative\n  Visualization-Presenter Relationships', 'url': 'https://huggingface.co/papers/2505.03164', 'abstract': "Traditional data presentations typically separate the presenter and visualization into two separate spaces--the 3D world and a 2D screen--enforcing visualization-centric stories. To create a more human-centric viewing experience, we establish a more equitable relationship between the visualization and the presenter through our InfoVids. These infographics-inspired informational videos are crafted to redefine relationships between the presenter and visualizations. As we design InfoVids, we explore how the use of layout, form, and interactions affects the viewer experience. We compare InfoVids against their baseline 2D `slides' equivalents across 9 metrics with 30 participants and provide practical, long-term insights from an autobiographical perspective. Our mixed methods analyses reveal that this paradigm reduced viewer attention splitting, shifted the focus from the visualization to the presenter, and led to more interactive, natural, and engaging full-body data performances for viewers. Ultimately, InfoVids helped viewers re-imagine traditional dynamics between the presenter and visualizations.", 'score': 4, 'issue_id': 3625, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'}, 'hash': '377a2c082764680a', 'authors': ['Ji Won Chung', 'Tongyu Zhou', 'Ivy Chen', 'Kevin Hsu', 'Ryan A. Rossi', 'Alexa Siu', 'Shunan Guo', 'Franck Dernoncourt', 'James Tompkin', 'Jeff Huang'], 'affiliations': ['Adobe Research', 'Brown University'], 'pdf_title_img': 'assets/pdf/title_img/2505.03164.jpg', 'data': {'categories': ['#multimodal', '#video'], 'emoji': '🎭', 'ru': {'title': 'InfoVids: Новый подход к визуализации данных через призму человека', 'desc': 'Это исследование представляет концепцию InfoVids - информационных видео, вдохновленных инфографикой. Они призваны создать более сбалансированные отношения между презентатором и визуализацией данных. Авторы изучают, как макет, форма и интерактивность влияют на восприятие зрителей. Эксперименты показали, что InfoVids снижают рассеивание внимания, переносят фокус на презентатора и создают более естественное и увлекательное взаимодействие с данными.'}, 'en': {'title': 'Revolutionizing Data Presentation with InfoVids', 'desc': 'This paper introduces InfoVids, a new format for presenting data that integrates the presenter and visualizations in a more cohesive manner. By using infographics-inspired videos, the authors aim to enhance viewer engagement and reduce distractions that typically arise from traditional 2D slides. The study evaluates InfoVids against standard presentation methods across various metrics, revealing that they foster a more interactive and natural experience for viewers. The findings suggest that this innovative approach can transform the dynamics of data presentation, making it more human-centric and effective.'}, 'zh': {'title': '重新定义演示者与可视化的关系', 'desc': '传统的数据展示通常将演示者和可视化分开，分别在3D世界和2D屏幕中进行，强调以可视化为中心的叙述。为了创造更以人为本的观看体验，我们通过InfoVids建立了可视化与演示者之间更平等的关系。这些受信息图启发的信息视频旨在重新定义演示者与可视化之间的关系。我们的研究表明，InfoVids减少了观众的注意力分散，使焦点从可视化转向演示者，并为观众提供了更互动、自然和引人入胜的数据表现。'}}}, {'id': 'https://huggingface.co/papers/2504.21798', 'title': 'SWE-smith: Scaling Data for Software Engineering Agents', 'url': 'https://huggingface.co/papers/2504.21798', 'abstract': 'Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.', 'score': 3, 'issue_id': 3638, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '35185d0e663ab134', 'authors': ['John Yang', 'Kilian Leret', 'Carlos E. Jimenez', 'Alexander Wettig', 'Kabir Khandpur', 'Yanzhe Zhang', 'Binyuan Hui', 'Ofir Press', 'Ludwig Schmidt', 'Diyi Yang'], 'affiliations': ['Alibaba Qwen', 'Indepedent', 'Princeton University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.21798.jpg', 'data': {'categories': ['#benchmark', '#data', '#synthetic', '#open_source', '#dataset', '#training'], 'emoji': '🛠️', 'ru': {'title': 'SWE-smith: революция в создании обучающих данных для ИИ в программировании', 'desc': 'SWE-smith - это новый конвейер для генерации обучающих данных в области программной инженерии в больших масштабах. Он создает среду выполнения для любой кодовой базы Python и автоматически синтезирует сотни или тысячи экземпляров задач, нарушающих существующие тесты. С помощью SWE-smith авторы создали набор данных из 50 тысяч экземпляров из 128 репозиториев GitHub, что на порядок больше, чем в предыдущих работах. Обученная на этих данных языковая модель SWE-agent-LM-32B достигла показателя Pass@1 в 40.2% на бенчмарке SWE-bench Verified, что является лучшим результатом среди моделей с открытым исходным кодом.'}, 'en': {'title': 'Scaling Software Engineering Data Generation with SWE-smith', 'desc': 'This paper presents SWE-smith, a new method for generating large-scale training data for language models in software engineering. Traditional datasets are limited in size and require extensive manual effort to curate, which hinders their effectiveness. SWE-smith automates the creation of execution environments and synthesizes numerous task instances from Python codebases, resulting in a dataset of 50,000 instances from 128 GitHub repositories. The authors demonstrate that their model, SWE-agent-LM-32B, achieves a state-of-the-art performance on the SWE-bench Verified benchmark, and they provide all resources to facilitate further research in this area.'}, 'zh': {'title': 'SWE-smith：大规模生成软件工程训练数据的解决方案', 'desc': '尽管语言模型在软件工程领域取得了进展，但收集训练数据仍然是一个重大挑战。现有的数据集规模较小，通常来自11个或更少的GitHub仓库，最多只有几千个训练实例。为了应对这一问题，我们提出了SWE-smith，这是一种用于大规模生成软件工程训练数据的新型管道。通过SWE-smith，我们创建了一个包含5万个实例的数据集，显著超过了以往的工作，并且开源了相关资源，以降低自动化软件工程研究的门槛。'}}}, {'id': 'https://huggingface.co/papers/2505.03052', 'title': 'Teaching Models to Understand (but not Generate) High-risk Data', 'url': 'https://huggingface.co/papers/2505.03052', 'abstract': "Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.", 'score': 2, 'issue_id': 3642, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'a988d97eaa0eb984', 'authors': ['Ryan Wang', 'Matthew Finlayson', 'Luca Soldaini', 'Swabha Swayamdipta', 'Robin Jia'], 'affiliations': ['Allen Institute for AI', 'Department of Computer Science, University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2505.03052.jpg', 'data': {'categories': ['#training', '#hallucinations', '#rlhf', '#ethics'], 'emoji': '🛡️', 'ru': {'title': 'Понимать, но не генерировать: безопасное обучение языковых моделей', 'desc': 'Статья представляет новый подход к обучению языковых моделей под названием SLUNG. Этот метод позволяет моделям понимать потенциально опасный контент, не генерируя его. SLUNG избирательно применяет функцию потерь, избегая стимулирования генерации рискованных токенов, но сохраняя их в контексте модели. Эксперименты показывают, что SLUNG улучшает понимание моделями опасного контента без увеличения его генерации.'}, 'en': {'title': 'Learn to Understand, Not to Generate Toxicity', 'desc': "This paper presents a new training method called Selective Loss to Understand but Not Generate (SLUNG) for language models. SLUNG allows models to learn from high-risk content, like toxic text, without generating it in their outputs. By selectively applying loss functions, the model is trained to predict safe tokens that follow harmful ones, enhancing its understanding of sensitive topics. The results show that SLUNG improves the model's ability to recognize toxic content while preventing it from producing harmful responses."}, 'zh': {'title': '理解高风险内容，避免生成有害输出', 'desc': '本文提出了一种新的预训练方法，称为选择性损失以理解但不生成（SLUNG）。该方法允许模型在不生成高风险内容的情况下，理解这些内容。通过选择性地避免激励生成高风险标记，SLUNG确保这些内容仍然在模型的上下文窗口内。实验结果表明，SLUNG显著提高了模型对高风险数据的理解能力，同时没有增加生成的有害内容。'}}}, {'id': 'https://huggingface.co/papers/2505.02311', 'title': 'Invoke Interfaces Only When Needed: Adaptive Invocation for Large\n  Language Models in Question Answering', 'url': 'https://huggingface.co/papers/2505.02311', 'abstract': 'The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baseline in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.', 'score': 2, 'issue_id': 3624, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '2a0b4af232b71fc8', 'authors': ['Jihao Zhao', 'Chunlai Zhou', 'Biao Qin'], 'affiliations': ['School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02311.jpg', 'data': {'categories': ['#hallucinations', '#small_models', '#training', '#optimization', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Умное обнаружение галлюцинаций для эффективного сотрудничества языковых моделей', 'desc': 'Статья предлагает новый метод AttenHScore для оценки галлюцинаций в малых языковых моделях в реальном времени. Этот подход позволяет точнее определять момент для вызова большой языковой модели в коллаборативной парадигме. Авторы также используют реорганизацию знаний с учетом неопределенности, чтобы помочь малым моделям лучше обрабатывать ключевую информацию. Эксперименты показывают, что AttenHScore превосходит базовые методы в обнаружении галлюцинаций на нескольких наборах данных вопросно-ответных систем.'}, 'en': {'title': 'Enhancing Hallucination Detection in Language Models with AttenHScore', 'desc': 'This paper introduces a new metric called AttenHScore to improve the detection of hallucinations in small language models (LMs) during their generation process. Hallucinations refer to incorrect or nonsensical outputs produced by LMs, and the proposed metric helps identify when these errors occur in real-time. By adjusting the detection threshold dynamically, the method enhances the invocation of larger LMs to provide more accurate responses. Additionally, the paper discusses how uncertainty-aware knowledge reorganization can help small LMs better understand and utilize critical information from text, leading to improved performance without requiring extra training.'}, 'zh': {'title': '提升小型语言模型的幻觉检测能力', 'desc': '本文提出了一种新的评估指标AttenHScore，用于在小型语言模型生成过程中检测和传播幻觉。通过动态调整检测阈值，我们能够更准确地实时调用大型语言模型，从而提高幻觉检测的能力。我们还利用不确定性感知的知识重组，帮助小型语言模型更好地捕捉关键信息。实验结果表明，AttenHScore在多个问答数据集上优于大多数基线方法，尤其是在处理复杂查询时。'}}}, {'id': 'https://huggingface.co/papers/2504.18373', 'title': 'Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant', 'url': 'https://huggingface.co/papers/2504.18373', 'abstract': 'In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at https://github.com/lorashen/Auto-SLURP/.', 'score': 2, 'issue_id': 3629, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 апреля', 'en': 'April 25', 'zh': '4月25日'}, 'hash': '296dd197ef9ea331', 'authors': ['Lei Shen', 'Xiaoyu Shen'], 'affiliations': ['GEB Tech', 'Ningbo Institute of Digital Twin, EIT, Ningbo'], 'pdf_title_img': 'assets/pdf/title_img/2504.18373.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Auto-SLURP: новый стандарт для оценки многоагентных ИИ-ассистентов', 'desc': 'Авторы представляют Auto-SLURP - набор данных для оценки многоагентных систем на основе больших языковых моделей (LLM) в контексте интеллектуальных персональных ассистентов. Auto-SLURP расширяет исходный набор SLURP, добавляя симулированные серверы и внешние сервисы для комплексной оценки понимания языка, выполнения задач и генерации ответов. Эксперименты показывают, что Auto-SLURP представляет серьезную проблему для современных передовых систем. Результаты подчеркивают, что по-настоящему надежные и интеллектуальные многоагентные персональные ассистенты все еще находятся в стадии разработки.'}, 'en': {'title': 'Auto-SLURP: Benchmarking Multi-Agent LLMs for Intelligent Assistants', 'desc': 'This paper introduces Auto-SLURP, a new benchmark dataset designed to evaluate multi-agent frameworks that utilize large language models (LLMs) in the realm of intelligent personal assistants. It enhances the original SLURP dataset by relabeling data and incorporating simulated servers and external services, allowing for a more thorough assessment of language understanding, task execution, and response generation. The authors demonstrate that Auto-SLURP poses significant challenges to existing state-of-the-art frameworks, indicating that the development of reliable multi-agent personal assistants is still ongoing. The dataset and its associated code are made publicly available for further research and development.'}, 'zh': {'title': 'Auto-SLURP：评估智能个人助理的基准数据集', 'desc': '近年来，基于大型语言模型（LLMs）的多智能体框架发展迅速。然而，目前缺乏专门用于评估这些框架性能的基准数据集。为了解决这个问题，我们推出了Auto-SLURP，这是一个旨在评估基于LLM的多智能体框架的基准数据集，特别是在智能个人助理的背景下。Auto-SLURP通过重新标记数据并整合模拟服务器和外部服务，扩展了原始的SLURP数据集，从而实现了全面的端到端评估流程。'}}}, {'id': 'https://huggingface.co/papers/2505.00212', 'title': 'Which Agent Causes Task Failures and When? On Automated Failure\n  Attribution of LLM Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2505.00212', 'abstract': "Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution", 'score': 1, 'issue_id': 3639, 'pub_date': '2025-04-30', 'pub_date_card': {'ru': '30 апреля', 'en': 'April 30', 'zh': '4月30日'}, 'hash': '1b127d9b18e51c40', 'authors': ['Shaokun Zhang', 'Ming Yin', 'Jieyu Zhang', 'Jiale Liu', 'Zhiguang Han', 'Jingyang Zhang', 'Beibin Li', 'Chi Wang', 'Huazheng Wang', 'Yiran Chen', 'Qingyun Wu'], 'affiliations': ['Duke University', 'Google DeepMind', 'Meta', 'Nanyang Technological University', 'Oregon State University', 'Pennsylvania State University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.00212.jpg', 'data': {'categories': ['#dataset', '#open_source', '#agents', '#reasoning'], 'emoji': '🕵️', 'ru': {'title': 'Кто виноват и когда: автоматическое расследование сбоев в многоагентных LLM-системах', 'desc': 'Статья представляет новую область исследований: автоматическое определение причин сбоев в многоагентных системах на основе больших языковых моделей (LLM). Авторы создали датасет Who&When, содержащий подробные логи сбоев из 127 многоагентных LLM-систем с аннотациями, связывающими сбои с конкретными агентами и этапами ошибок. На основе этого датасета разработаны и оценены три метода автоматического определения причин сбоев. Результаты показывают сложность задачи: лучший метод достигает точности 53.5% в идентификации агентов, ответственных за сбои, но только 14.2% в определении этапов сбоев.'}, 'en': {'title': 'Automating Failure Attribution in Multi-Agent Systems', 'desc': 'This paper addresses the challenge of identifying which agent and which step in a multi-agent system led to task failures, a process known as failure attribution. The authors introduce a new dataset called Who&When, which contains detailed failure logs from 127 LLM multi-agent systems, annotated to link failures to specific agents and error steps. They propose three automated methods for failure attribution and evaluate their performance, revealing that the best method only achieves 53.5% accuracy in identifying responsible agents and 14.2% in pinpointing failure steps. The findings indicate that current state-of-the-art models struggle with this task, emphasizing the complexity of automated failure attribution and the need for further research.'}, 'zh': {'title': '自动化失败归因：LLM多智能体系统的新挑战', 'desc': '本文探讨了在大型语言模型（LLM）多智能体系统中，如何自动识别导致任务失败的智能体和步骤。我们提出了一个新的研究领域：自动化失败归因，并引入了Who&When数据集，该数据集包含127个LLM多智能体系统的详细失败日志。通过使用Who&When数据集，我们开发并评估了三种自动化失败归因方法，并总结了它们的优缺点。尽管最佳方法在识别失败责任智能体方面达到了53.5%的准确率，但在确定失败步骤方面仅为14.2%，显示出该任务的复杂性和进一步研究的必要性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (3)', '#agi (1)', '#alignment (1)', '#architecture (3)', '#audio (1)', '#benchmark (6)', '#cv (1)', '#data (2)', '#dataset (6)', '#diffusion (1)', '#ethics (1)', '#games', '#graphs', '#hallucinations (2)', '#healthcare', '#inference (4)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (7)', '#open_source (7)', '#optimization (6)', '#plp', '#rag', '#reasoning (6)', '#rl (1)', '#rlhf (3)', '#robotics', '#science (2)', '#security', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (8)', '#transfer_learning (1)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-07 23:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-07 23:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-07 23:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    