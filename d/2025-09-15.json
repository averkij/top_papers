{
    "date": {
        "ru": "15 сентября",
        "en": "September 15",
        "zh": "9月15日"
    },
    "time_utc": "2025-09-15 03:35",
    "weekday": 0,
    "issue_id": 5884,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.09677",
            "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in\n  LLMs",
            "url": "https://huggingface.co/papers/2509.09677",
            "abstract": "Scaling large language models improves their ability to execute longer tasks by isolating execution capability and mitigating self-conditioning effects, despite diminishing single-step accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Does continued scaling of large language models (LLMs) yield diminishing returns? Real-world value often stems from the length of task an agent can complete. We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. We propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. We find that larger models can correctly execute significantly more turns even when small models have 100\\% single-turn accuracy. We observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations -- curiously, we observe a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. In contrast, recent thinking models do not self-condition, and can also execute much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of task they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks.",
            "score": 7,
            "issue_id": 5883,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 сентября",
                "en": "September 11",
                "zh": "9月11日"
            },
            "hash": "03dddc4470e66eb9",
            "authors": [
                "Akshit Sinha",
                "Arvindh Arun",
                "Shashwat Goel",
                "Steffen Staab",
                "Jonas Geiping"
            ],
            "affiliations": [
                "ELLIS Institute Tübingen",
                "Institute for AI, University of Stuttgart",
                "Max Planck Institute for Intelligent Systems",
                "Tübingen AI Center",
                "University of Cambridge",
                "University of Southampton"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09677.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#architecture",
                    "#reasoning",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "Масштабирование LLM: путь к длительным задачам",
                    "desc": "Данная статья исследует влияние масштабирования больших языковых моделей (LLM) на их способность выполнять более длительные задачи. Авторы обнаружили, что увеличение размера модели может экспоненциально улучшить длину успешно выполняемых задач, несмотря на уменьшение точности отдельных шагов. Исследование показало, что ошибки в выполнении длительных задач связаны с проблемами в исполнении, а не с неспособностью рассуждать. Кроме того, было выявлено явление самообусловливания, когда модели становятся более склонны к ошибкам при наличии их предыдущих ошибок в контексте."
                },
                "en": {
                    "title": "Scaling Models for Better Long-Task Execution",
                    "desc": "This paper explores how scaling large language models (LLMs) enhances their performance on longer tasks, despite a decrease in accuracy for individual steps. The authors argue that the challenges LLMs face with extended tasks stem from execution errors rather than reasoning limitations. They propose a method to isolate execution capability by providing explicit knowledge and planning for long-horizon tasks. The findings suggest that larger models can handle more complex tasks effectively, even when smaller models achieve perfect accuracy on single-step tasks, highlighting the importance of model size and compute resources for task execution."
                },
                "zh": {
                    "title": "扩大模型规模，提升长任务执行能力",
                    "desc": "本论文探讨了大型语言模型（LLMs）在执行长任务时的能力提升，尽管单步准确率可能下降。我们发现，单步准确率的微小提升可以在任务长度上带来指数级的改善。我们提出通过明确提供解决长时间任务所需的知识和计划来隔离执行能力。研究表明，尽管小模型在单步任务中表现完美，但大型模型在执行多轮任务时的表现显著更好。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09716",
            "title": "VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions",
            "url": "https://huggingface.co/papers/2509.09716",
            "abstract": "Voice Style Adaptation (VSA) evaluates the ability of spoken language models to modify their speaking style based on spoken instructions, using a bilingual benchmark and a Large Audio Language Model as a Judge framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Spoken language models (SLMs) have emerged as a unified paradigm for speech understanding and generation, enabling natural human machine interaction. However, while most progress has focused on semantic accuracy and instruction following, the ability of SLMs to adapt their speaking style based on spoken instructions has received limited attention. We introduce Voice Style Adaptation (VSA), a new task that examines whether SLMs can modify their speaking style, such as timbre, prosody, or persona following natural language spoken commands. To study this task, we present VStyle, a bilingual (Chinese & English) benchmark covering four categories of speech generation: acoustic attributes, natural language instruction, role play, and implicit empathy. We also introduce the Large Audio Language Model as a Judge (LALM as a Judge) framework, which progressively evaluates outputs along textual faithfulness, style adherence, and naturalness, ensuring reproducible and objective assessment. Experiments on commercial systems and open source SLMs demonstrate that current models face clear limitations in controllable style adaptation, highlighting both the novelty and challenge of this task. By releasing VStyle and its evaluation toolkit, we aim to provide the community with a foundation for advancing human centered spoken interaction. The dataset and code are publicly available at https://junzhan2000.github.io/VStyle.github.io/{project's homepage}.",
            "score": 6,
            "issue_id": 5883,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 сентября",
                "en": "September 9",
                "zh": "9月9日"
            },
            "hash": "a1fdfdb8fb8b7486",
            "authors": [
                "Jun Zhan",
                "Mingyang Han",
                "Yuxuan Xie",
                "Chen Wang",
                "Dong Zhang",
                "Kexin Huang",
                "Haoxiang Shi",
                "DongXiao Wang",
                "Tengtao Song",
                "Qinyuan Cheng",
                "Shimin Li",
                "Jun Song",
                "Xipeng Qiu",
                "Bo Zheng"
            ],
            "affiliations": [
                "Alibaba Group",
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09716.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#audio",
                    "#dataset",
                    "#alignment",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Новый рубеж в разговорном ИИ: адаптация стиля речи по голосовым командам",
                    "desc": "Статья представляет новую задачу под названием Voice Style Adaptation (VSA), которая оценивает способность разговорных языковых моделей адаптировать свой стиль речи на основе устных инструкций. Авторы создали двуязычный бенчмарк VStyle для изучения этой задачи, охватывающий четыре категории генерации речи. Они также предложили фреймворк Large Audio Language Model as a Judge для объективной оценки результатов. Эксперименты показали, что существующие модели имеют ограничения в контролируемой адаптации стиля речи."
                },
                "en": {
                    "title": "Transforming Speech: Adapting Style with Voice Commands",
                    "desc": "Voice Style Adaptation (VSA) is a new task that tests how well spoken language models (SLMs) can change their speaking style based on spoken commands. This includes adjusting aspects like tone, rhythm, and character portrayal. The study introduces a bilingual benchmark called VStyle, which evaluates SLMs on various speech generation categories, including acoustic features and empathy. The research highlights the limitations of current models in adapting their style, aiming to improve human-machine interaction through better control of speech characteristics."
                },
                "zh": {
                    "title": "语音风格适应：让机器更懂人类的说话风格",
                    "desc": "语音风格适应（VSA）是一项新任务，旨在评估口语模型根据口头指令调整说话风格的能力。我们提出了VStyle，这是一个双语基准，涵盖了声学属性、自然语言指令、角色扮演和隐性共情等四个类别的语音生成。通过引入大型音频语言模型作为评估框架，我们能够客观地评估模型在文本忠实性、风格遵循和自然性方面的表现。实验结果表明，当前模型在可控风格适应方面存在明显局限，突显了这一任务的新颖性和挑战性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10441",
            "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
            "url": "https://huggingface.co/papers/2509.10441",
            "abstract": "InfGen, a one-step generator replacing the VAE decoder, enables arbitrary high-resolution image generation from a fixed-size latent, significantly reducing computational complexity and generation time.  \t\t\t\t\tAI-generated summary \t\t\t\t Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the InfGen, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds.",
            "score": 2,
            "issue_id": 5883,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "c6a96819cd15917d",
            "authors": [
                "Tao Han",
                "Wanghan Xu",
                "Junchao Gong",
                "Xiaoyu Yue",
                "Song Guo",
                "Luping Zhou",
                "Lei Bai"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Shanghai Artificial Intelligence Laboratory",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10441.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "InfGen: мгновенная генерация изображений любого разрешения",
                    "desc": "InfGen - это новый генератор изображений, заменяющий декодер VAE в моделях латентной диффузии. Он позволяет создавать изображения произвольного высокого разрешения из латентного представления фиксированного размера. InfGen значительно снижает вычислительную сложность и время генерации по сравнению с обычными диффузионными моделями. Эксперименты показывают, что InfGen способен улучшить многие модели для создания изображений сверхвысокого разрешения, сократив время генерации 4K-изображений до менее 10 секунд."
                },
                "en": {
                    "title": "Revolutionizing High-Resolution Image Generation with InfGen",
                    "desc": "InfGen is a novel one-step generator that replaces the traditional VAE decoder, allowing for high-resolution image generation from a fixed-size latent representation. This approach significantly reduces the computational complexity and generation time associated with creating images, particularly at 4K resolution. By utilizing a compact generated latent from diffusion models, InfGen enables arbitrary resolution outputs without the need for retraining existing models. Experiments demonstrate that InfGen can enhance various models, achieving 4K image generation in under 10 seconds, thus streamlining the image generation process."
                },
                "zh": {
                    "title": "InfGen：高效生成任意分辨率图像的创新解决方案",
                    "desc": "InfGen是一种新型生成器，取代了变分自编码器（VAE）的解码器，能够从固定大小的潜在空间生成任意高分辨率的图像。这种方法显著降低了计算复杂性和生成时间，使得生成4K图像的时间缩短到10秒以内。通过将扩散模型生成的固定潜在视为内容表示，InfGen能够在不重新训练扩散模型的情况下，解码任意分辨率的图像。实验表明，InfGen可以提升多种模型的性能，推动高分辨率图像生成的进程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10147",
            "title": "Virtual Agent Economies",
            "url": "https://huggingface.co/papers/2509.10147",
            "abstract": "The sandbox economy framework analyzes the emerging AI agent economy, focusing on its origins and permeability, and discusses design choices for safe and steerable AI markets.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid adoption of autonomous AI agents is giving rise to a new economic layer where agents transact and coordinate at scales and speeds beyond direct human oversight. We propose the \"sandbox economy\" as a framework for analyzing this emergent system, characterizing it along two key dimensions: its origins (emergent vs. intentional) and its degree of separateness from the established human economy (permeable vs. impermeable). Our current trajectory points toward a spontaneous emergence of a vast and highly permeable AI agent economy, presenting us with opportunities for an unprecedented degree of coordination as well as significant challenges, including systemic economic risk and exacerbated inequality. Here we discuss a number of possible design choices that may lead to safely steerable AI agent markets. In particular, we consider auction mechanisms for fair resource allocation and preference resolution, the design of AI \"mission economies\" to coordinate around achieving collective goals, and socio-technical infrastructure needed to ensure trust, safety, and accountability. By doing this, we argue for the proactive design of steerable agent markets to ensure the coming technological shift aligns with humanity's long-term collective flourishing.",
            "score": 2,
            "issue_id": 5884,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "2fd9f178e62d64b0",
            "authors": [
                "Nenad Tomasev",
                "Matija Franklin",
                "Joel Z. Leibo",
                "Julian Jacobs",
                "William A. Cunningham",
                "Iason Gabriel",
                "Simon Osindero"
            ],
            "affiliations": [
                "Google DeepMind",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10147.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#ethics",
                    "#alignment"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Проектирование безопасной экономики ИИ-агентов для общего блага",
                    "desc": "Статья представляет концепцию 'экономики песочницы' для анализа зарождающейся экономики ИИ-агентов. Авторы рассматривают два ключевых аспекта: происхождение (спонтанное или намеренное) и степень отделенности от человеческой экономики. Обсуждаются возможности и риски, связанные с развитием экономики ИИ-агентов, включая системные экономические риски и усиление неравенства. Предлагаются механизмы для создания безопасных и управляемых рынков ИИ-агентов, такие как аукционы для справедливого распределения ресурсов и 'миссионерские экономики' для достижения коллективных целей."
                },
                "en": {
                    "title": "Navigating the Future: Designing Safe AI Agent Economies",
                    "desc": "The paper introduces the 'sandbox economy' framework to analyze the new economic landscape created by autonomous AI agents. It highlights two main aspects: the origins of these agents, whether they emerge spontaneously or are intentionally designed, and their connection to the existing human economy, which can be either permeable or impermeable. The authors emphasize the potential benefits of this AI agent economy, such as enhanced coordination, while also warning of risks like economic instability and inequality. They propose design strategies, including auction mechanisms and mission-oriented AI systems, to create safe and effective markets for AI agents that align with human values."
                },
                "zh": {
                    "title": "设计可控的AI代理市场，迎接经济新机遇",
                    "desc": "这篇论文分析了新兴的人工智能代理经济，提出了“沙盒经济”框架来理解这一系统。它主要关注人工智能代理的起源和与人类经济的关系，探讨了安全可控的AI市场设计选择。研究表明，AI代理经济的自发出现可能带来前所未有的协调机会，但也伴随系统性经济风险和不平等加剧的挑战。作者建议通过拍卖机制和AI“使命经济”的设计，确保资源分配的公平性和信任、安全、问责的社会技术基础设施。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09734",
            "title": "MCP-AgentBench: Evaluating Real-World Language Agent Performance with\n  MCP-Mediated Tools",
            "url": "https://huggingface.co/papers/2509.09734",
            "abstract": "MCP-AgentBench is a benchmark designed to evaluate language agents in MCP-mediated tool interactions, providing a standardized framework for assessing real-world performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The Model Context Protocol (MCP) is rapidly emerging as a pivotal open standard, designed to enhance agent-tool integration and interoperability, and is positioned to unlock a new era of powerful, interconnected, and genuinely utilitarian agentic AI. However, despite MCP's growing adoption, existing benchmarks often fail to capture real-world agent performance within this new paradigm, leading to a distorted perception of their true operational value and an inability to reliably differentiate proficiencies. To bridge this critical evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark specifically engineered to rigorously assess language agent capabilities in MCP-mediated tool interactions. Core contributions of MCP-AgentBench include: the establishment of a robust MCP testbed comprising 33 operational servers with 188 distinct tools; the development of a benchmark featuring 600 systematically designed queries distributed across 6 distinct categories of varying interaction complexity; and the introduction of MCP-Eval, a novel outcome-oriented evaluation methodology prioritizing real-world task success. Through extensive empirical evaluation of leading language agents, we provide foundational insights. MCP-AgentBench aims to equip the research community with a standardized and reliable framework to build, validate, and advance agents capable of fully leveraging MCP's transformative benefits, thereby accelerating progress toward truly capable and interoperable AI systems.",
            "score": 2,
            "issue_id": 5883,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "56883c4d7c401e99",
            "authors": [
                "Zikang Guo",
                "Benfeng Xu",
                "Chiwei Zhu",
                "Wentao Hong",
                "Xiaorui Wang",
                "Zhendong Mao"
            ],
            "affiliations": [
                "Metastone Technology, Beijing, China",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09734.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Новый стандарт оценки ИИ-агентов в реальном мире",
                    "desc": "MCP-AgentBench - это новый бенчмарк для оценки языковых агентов в контексте взаимодействия с инструментами через протокол MCP. Он включает в себя тестовую среду с 33 серверами и 188 инструментами, а также 600 запросов разной сложности. Бенчмарк вводит новую методологию оценки MCP-Eval, ориентированную на успешность выполнения реальных задач. MCP-AgentBench призван обеспечить стандартизированную основу для разработки и валидации агентов, способных полноценно использовать преимущества MCP."
                },
                "en": {
                    "title": "Empowering Language Agents with MCP-AgentBench",
                    "desc": "MCP-AgentBench is a benchmark created to evaluate language agents that interact with tools using the Model Context Protocol (MCP). It addresses the shortcomings of existing benchmarks by providing a standardized framework that reflects real-world performance. The benchmark includes a testbed with 33 servers and 188 tools, along with 600 queries across various complexity levels. By introducing a new evaluation method focused on task success, MCP-AgentBench aims to enhance the development of more effective and interconnected AI systems."
                },
                "zh": {
                    "title": "MCP-AgentBench：评估语言代理的新标准",
                    "desc": "MCP-AgentBench是一个基准测试，旨在评估语言代理在MCP介导的工具交互中的表现。它提供了一个标准化的框架，以便更准确地评估代理在现实世界中的能力。该基准包括33个操作服务器和188种不同工具，设计了600个系统化的查询，涵盖6种不同复杂度的交互类别。通过这种方式，MCP-AgentBench帮助研究人员更好地理解和提升代理的性能，推动智能代理的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.10058",
            "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings\n  for Improved Diffusion Generation",
            "url": "https://huggingface.co/papers/2509.10058",
            "abstract": "A training-free framework uses a large language model to disambiguate color terms and refine text embeddings for improved color accuracy in text-to-image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate color alignment in text-to-image (T2I) generation is critical for applications such as fashion, product visualization, and interior design, yet current diffusion models struggle with nuanced and compound color terms (e.g., Tiffany blue, lime green, hot pink), often producing images that are misaligned with human intent. Existing approaches rely on cross-attention manipulation, reference images, or fine-tuning but fail to systematically resolve ambiguous color descriptions. To precisely render colors under prompt ambiguity, we propose a training-free framework that enhances color fidelity by leveraging a large language model (LLM) to disambiguate color-related prompts and guiding color blending operations directly in the text embedding space. Our method first employs a large language model (LLM) to resolve ambiguous color terms in the text prompt, and then refines the text embeddings based on the spatial relationships of the resulting color terms in the CIELAB color space. Unlike prior methods, our approach improves color accuracy without requiring additional training or external reference images. Experimental results demonstrate that our framework improves color alignment without compromising image quality, bridging the gap between text semantics and visual generation.",
            "score": 1,
            "issue_id": 5883,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "e647125383aba6a5",
            "authors": [
                "Sung-Lin Tsai",
                "Bo-Lun Huang",
                "Yu Ting Shen",
                "Cheng Yu Yeo",
                "Chiang Tseng",
                "Bo-Kai Ruan",
                "Wen-Sheng Lien",
                "Hong-Han Shuai"
            ],
            "affiliations": [
                "National Yang Ming Chiao Tung University Hsinchu, Taiwan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.10058.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#data",
                    "#diffusion",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Точные цвета в генерации изображений без дообучения",
                    "desc": "Предложена система, использующая большую языковую модель для уточнения цветовых терминов в запросах для генерации изображений по тексту. Метод улучшает точность цветопередачи путем доработки текстовых эмбеддингов на основе пространственных отношений цветов в цветовом пространстве CIELAB. В отличие от существующих подходов, данный метод не требует дополнительного обучения или использования референсных изображений. Эксперименты показали, что система повышает соответствие цветов без ущерба для качества генерируемых изображений."
                },
                "en": {
                    "title": "Enhancing Color Accuracy in T2I with a Training-Free Framework",
                    "desc": "This paper presents a novel training-free framework that enhances color accuracy in text-to-image (T2I) generation by utilizing a large language model (LLM). The framework addresses the challenge of ambiguous color terms, which often lead to misaligned images in applications like fashion and product visualization. By disambiguating color prompts and refining text embeddings in the CIELAB color space, the method improves the fidelity of generated colors without the need for additional training or reference images. Experimental results show that this approach successfully aligns colors with human intent while maintaining high image quality."
                },
                "zh": {
                    "title": "无训练框架提升文本到图像生成中的颜色准确性",
                    "desc": "本文提出了一种无训练框架，利用大型语言模型（LLM）来消歧义颜色术语，并优化文本嵌入，以提高文本到图像生成中的颜色准确性。当前的扩散模型在处理复杂的颜色描述时表现不佳，常常导致生成的图像与人类意图不符。我们的方法通过解析文本提示中的模糊颜色术语，并在CIELAB颜色空间中根据颜色术语的空间关系来细化文本嵌入，从而实现更精确的颜色渲染。实验结果表明，该框架在不影响图像质量的情况下，显著改善了颜色对齐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09995",
            "title": "QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading",
            "url": "https://huggingface.co/papers/2509.09995",
            "abstract": "QuantAgent, a multi-agent LLM framework, excels in high-frequency trading by leveraging specialized agents for technical indicators, chart patterns, trends, and risk, outperforming existing neural and rule-based systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have demonstrated impressive capabilities in financial reasoning and market understanding. Multi-agent LLM frameworks such as TradingAgent and FINMEM augment these models to long-horizon investment tasks, leveraging fundamental and sentiment-based inputs for strategic decision-making. However, such systems are ill-suited for the high-speed, precision-critical demands of High-Frequency Trading (HFT). HFT requires rapid, risk-aware decisions based on structured, short-horizon signals, including technical indicators, chart patterns, and trend-based features, distinct from the long-term semantic reasoning typical of traditional financial LLM applications. To this end, we introduce QuantAgent, the first multi-agent LLM framework explicitly designed for high-frequency algorithmic trading. The system decomposes trading into four specialized agents, Indicator, Pattern, Trend, and Risk, each equipped with domain-specific tools and structured reasoning capabilities to capture distinct aspects of market dynamics over short temporal windows. In zero-shot evaluations across ten financial instruments, including Bitcoin and Nasdaq futures, QuantAgent demonstrates superior performance in both predictive accuracy and cumulative return over 4-hour trading intervals, outperforming strong neural and rule-based baselines. Our findings suggest that combining structured financial priors with language-native reasoning unlocks new potential for traceable, real-time decision systems in high-frequency financial markets.",
            "score": 1,
            "issue_id": 5883,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "5964ddeaa46fcc92",
            "authors": [
                "Fei Xiong",
                "Xiang Zhang",
                "Aosong Feng",
                "Siqi Sun",
                "Chenyu You"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Fudan University",
                "Stony Brook University",
                "University of British Columbia",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09995.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#science",
                    "#multimodal",
                    "#agents",
                    "#reasoning",
                    "#games"
                ],
                "emoji": "📈",
                "ru": {
                    "title": "QuantAgent: Революция в высокочастотной торговле с помощью мультиагентных языковых моделей",
                    "desc": "QuantAgent - это инновационная мультиагентная система на основе больших языковых моделей (LLM), разработанная специально для высокочастотной торговли. Система использует четыре специализированных агента: Indicator, Pattern, Trend и Risk, каждый из которых оснащен специфическими инструментами и возможностями структурированного рассуждения для анализа различных аспектов рыночной динамики в коротких временных окнах. В ходе оценки на десяти финансовых инструментах QuantAgent продемонстрировал превосходную производительность как в точности прогнозирования, так и в кумулятивной доходности за 4-часовые торговые интервалы, превзойдя сильные нейронные и основанные на правилах базовые модели. Результаты исследования показывают, что сочетание структурированных финансовых приоров с языковым рассуждением открывает новые возможности для создания прослеживаемых систем принятия решений в реальном времени на высокочастотных финансовых рынках."
                },
                "en": {
                    "title": "Revolutionizing High-Frequency Trading with Specialized Agents",
                    "desc": "QuantAgent is a multi-agent framework designed for high-frequency trading (HFT) that utilizes specialized agents to analyze technical indicators, chart patterns, trends, and risk. Unlike traditional large language models (LLMs) that focus on long-term investment strategies, QuantAgent is tailored for rapid decision-making in fast-paced trading environments. Each agent within the framework is equipped with specific tools to effectively interpret short-term market signals. In tests, QuantAgent outperformed existing neural and rule-based systems, demonstrating its effectiveness in achieving higher predictive accuracy and returns in HFT scenarios."
                },
                "zh": {
                    "title": "QuantAgent：高频交易的智能决策新工具",
                    "desc": "QuantAgent 是一个多智能体大语言模型框架，专门为高频交易设计。它通过四个专业代理（技术指标、图表模式、趋势和风险）来处理市场动态，能够快速做出基于短期信号的决策。与传统的金融大语言模型不同，QuantAgent 更加注重快速、精准的交易需求。实验结果显示，QuantAgent 在预测准确性和累计收益方面优于现有的神经网络和规则基础系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09926",
            "title": "LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised\n  Learning in Open-World Scenarios",
            "url": "https://huggingface.co/papers/2509.09926",
            "abstract": "LoFT, a parameter-efficient fine-tuning framework for long-tailed semi-supervised learning, improves reliability of pseudolabels and discriminative ability in open-world scenarios, outperforming previous methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-tailed learning has garnered increasing attention due to its wide applicability in real-world scenarios. Among existing approaches, Long-Tailed Semi-Supervised Learning (LTSSL) has emerged as an effective solution by incorporating a large amount of unlabeled data into the imbalanced labeled dataset. However, most prior LTSSL methods are designed to train models from scratch, which often leads to issues such as overconfidence and low-quality pseudo-labels. To address these challenges, we extend LTSSL into the foundation model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate that fine-tuned foundation models can generate more reliable pseudolabels, thereby benefiting imbalanced learning. Furthermore, we explore a more practical setting by investigating semi-supervised learning under open-world conditions, where the unlabeled data may include out-of-distribution (OOD) samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World scenarios) to improve the discriminative ability. Experimental results on multiple benchmarks demonstrate that our method achieves superior performance compared to previous approaches, even when utilizing only 1\\% of the unlabeled data compared with previous works.",
            "score": 1,
            "issue_id": 5884,
            "pub_date": "2025-09-12",
            "pub_date_card": {
                "ru": "12 сентября",
                "en": "September 12",
                "zh": "9月12日"
            },
            "hash": "e85c5f480d51fb5c",
            "authors": [
                "Jiahao Chen",
                "Zhiyuan Huang",
                "Yurou Liu",
                "Bing Su"
            ],
            "affiliations": [
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09926.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#transfer_learning"
                ],
                "emoji": "🦚",
                "ru": {
                    "title": "Эффективная тонкая настройка для обучения с длинным хвостом",
                    "desc": "Статья представляет LoFT - новый фреймворк для эффективной тонкой настройки моделей машинного обучения в условиях несбалансированного полу-контролируемого обучения с длинным хвостом. LoFT улучшает качество псевдо-меток и способность к различению в сценариях открытого мира. Авторы также предлагают расширение LoFT-OW для работы с данными вне распределения. Эксперименты показывают превосходство LoFT над существующими методами даже при использовании только 1% немеченых данных."
                },
                "en": {
                    "title": "Enhancing Long-Tailed Learning with LoFT: Fine-Tuning for Better Pseudolabels",
                    "desc": "LoFT is a new framework designed for long-tailed semi-supervised learning that enhances the quality of pseudolabels and improves model performance in open-world scenarios. It builds on the foundation model fine-tuning approach, allowing for better utilization of unlabeled data alongside imbalanced labeled datasets. By addressing issues like overconfidence and low-quality pseudolabels, LoFT enables more reliable learning outcomes. The framework also includes a variant, LoFT-OW, which specifically tackles challenges posed by out-of-distribution samples, demonstrating superior results on various benchmarks with minimal unlabeled data usage."
                },
                "zh": {
                    "title": "LoFT：提升长尾半监督学习的可靠性与区分能力",
                    "desc": "LoFT是一种高效的参数微调框架，专为长尾半监督学习设计，旨在提高伪标签的可靠性和在开放世界场景中的区分能力。该方法通过将大量未标记数据与不平衡的标记数据集结合，克服了传统方法中常见的过度自信和低质量伪标签的问题。LoFT在基础模型微调的基础上进行扩展，能够生成更可靠的伪标签，从而促进不平衡学习的效果。实验结果表明，LoFT在多个基准测试中表现优于以往方法，即使只使用1%的未标记数据。"
                }
            }
        }
    ],
    "link_prev": "2025-09-12.html",
    "link_next": "2025-09-16.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "12.09",
        "en": "09/12",
        "zh": "9月12日"
    },
    "short_date_next": {
        "ru": "16.09",
        "en": "09/16",
        "zh": "9月16日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}