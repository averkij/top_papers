{
    "date": {
        "ru": "15 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 15",
        "zh": "9æœˆ15æ—¥"
    },
    "time_utc": "2025-09-15 00:54",
    "weekday": 0,
    "issue_id": 5882,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.09372",
            "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model",
            "url": "https://huggingface.co/papers/2509.09372",
            "abstract": "VLA-Adapter reduces reliance on large-scale VLMs and extensive pre-training by using a lightweight Policy module with Bridge Attention, achieving state-of-the-art performance and fast inference speed with minimal computational resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models typically bridge the gap between perceptual and action spaces by pre-training a large-scale Vision-Language Model (VLM) on robotic data. While this approach greatly enhances performance, it also incurs significant training costs. In this paper, we investigate how to effectively bridge vision-language (VL) representations to action (A). We introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA models on large-scale VLMs and extensive pre-training. To this end, we first systematically analyze the effectiveness of various VL conditions and present key findings on which conditions are essential for bridging perception and action spaces. Based on these insights, we propose a lightweight Policy module with Bridge Attention, which autonomously injects the optimal condition into the action space. In this way, our method achieves high performance using only a 0.5B-parameter backbone, without any robotic data pre-training. Extensive experiments on both simulated and real-world robotic benchmarks demonstrate that VLA-Adapter not only achieves state-of-the-art level performance, but also offers the fast inference speed reported to date. Furthermore, thanks to the proposed advanced bridging paradigm, VLA-Adapter enables the training of a powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly lowering the barrier to deploying the VLA model. Project page: https://vla-adapter.github.io/.",
            "score": 169,
            "issue_id": 5858,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "518f1161098c39e1",
            "authors": [
                "Yihao Wang",
                "Pengxiang Ding",
                "Lingxiao Li",
                "Can Cui",
                "Zirui Ge",
                "Xinyang Tong",
                "Wenxuan Song",
                "Han Zhao",
                "Wei Zhao",
                "Pengxu Hou",
                "Siteng Huang",
                "Yifan Tang",
                "Wenhui Wang",
                "Ru Zhang",
                "Jianyi Liu",
                "Donglin Wang"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "OpenHelix Team",
                "State Key Laboratory of Networking and Switching Technology",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09372.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#agents",
                    "#rl",
                    "#robotics",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ±ĞµĞ· Ğ³Ñ€Ğ¾Ğ¼Ğ¾Ğ·Ğ´ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "VLA-Adapter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Policy Ñ Bridge Attention Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. VLA-Adapter Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 0.5B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²ÑƒÑ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, VLA-Adapter Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½ÑƒÑ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 8 Ñ‡Ğ°ÑĞ¾Ğ² Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ GPU."
                },
                "en": {
                    "title": "Efficient Vision-Language-Action with VLA-Adapter",
                    "desc": "The paper introduces VLA-Adapter, a new approach that minimizes the need for large-scale Vision-Language Models (VLMs) and extensive pre-training in Vision-Language-Action (VLA) tasks. It employs a lightweight Policy module with Bridge Attention to effectively connect vision-language representations to action spaces. This method achieves high performance with a compact 0.5B-parameter backbone and eliminates the need for robotic data pre-training. The results show that VLA-Adapter not only reaches state-of-the-art performance but also allows for rapid training and inference on standard hardware."
                },
                "zh": {
                    "title": "VLA-Adapterï¼šé«˜æ•ˆçš„è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹",
                    "desc": "VLA-Adapteræ˜¯ä¸€ç§æ–°é¢–çš„æ¨¡å‹ï¼Œæ—¨åœ¨å‡å°‘å¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¹¿æ³›é¢„è®­ç»ƒçš„ä¾èµ–ã€‚å®ƒé€šè¿‡å¼•å…¥è½»é‡çº§çš„ç­–ç•¥æ¨¡å—å’Œæ¡¥æ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨0.5äº¿å‚æ•°çš„æƒ…å†µä¸‹å®ç°é«˜æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æœºå™¨äººåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”æ¨ç†é€Ÿåº¦éå¸¸å¿«ã€‚VLA-Adapterçš„è®¾è®¡ä½¿å¾—åœ¨æ™®é€šæ¶ˆè´¹çº§GPUä¸Šä»…éœ€8å°æ—¶å³å¯è®­ç»ƒå‡ºå¼ºå¤§çš„è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†éƒ¨ç½²çš„é—¨æ§›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.08519",
            "title": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal\n  Conditioning",
            "url": "https://huggingface.co/papers/2509.08519",
            "abstract": "HuMo is a unified framework for human-centric video generation that addresses challenges in multimodal control through a two-stage training paradigm and novel strategies for subject preservation and audio-visual synchronization.  \t\t\t\t\tAI-generated summary \t\t\t\t Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo.",
            "score": 105,
            "issue_id": 5855,
            "pub_date": "2025-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "2db390fc41f9f85b",
            "authors": [
                "Liyang Chen",
                "Tianxiang Ma",
                "Jiawei Liu",
                "Bingchuan Li",
                "Zhuowei Chen",
                "Lijie Liu",
                "Xu He",
                "Gen Li",
                "Qian He",
                "Zhiyong Wu"
            ],
            "affiliations": [
                "Intelligent Creation Lab, ByteDance",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.08519.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "HuMo: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ",
                    "desc": "HuMo - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. HuMo Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸."
                },
                "en": {
                    "title": "HuMo: Unifying Multimodal Control for Human-Centric Video Generation",
                    "desc": "HuMo is a novel framework designed for generating human-centric videos by effectively integrating multiple input modalities such as text, images, and audio. It tackles the challenges of limited training data and the need for precise coordination between subject preservation and audio-visual synchronization. The framework employs a two-stage training approach, utilizing a high-quality dataset and innovative strategies like minimal-invasive image injection and focus-by-predicting for improved task performance. HuMo demonstrates superior capabilities in multimodal control, outperforming existing methods in generating coherent and synchronized human videos."
                },
                "zh": {
                    "title": "HuMoï¼šäººæœ¬è§†é¢‘ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "HuMoæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„äººæœ¬è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼å’Œæ–°é¢–çš„ç­–ç•¥è§£å†³å¤šæ¨¡æ€æ§åˆ¶ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä»æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šç§è¾“å…¥ä¸­åˆæˆè§†é¢‘ï¼Œå…‹æœäº†è®­ç»ƒæ•°æ®ç¨€ç¼ºå’Œå¤šæ¨¡æ€è¾“å…¥ä¸‹çš„ä»»åŠ¡åä½œéš¾é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒHuMoæ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†æ¸è¿›å¼çš„å¤šæ¨¡æ€è®­ç»ƒæ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHuMoåœ¨å­ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå»ºç«‹äº†ä¸€ä¸ªåä½œçš„å¤šæ¨¡æ€æ¡ä»¶è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09674",
            "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.09674",
            "abstract": "SimpleVLA-RL, an RL framework for VLA models, enhances long-horizon action planning, achieves state-of-the-art performance, and discovers novel patterns during training.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0 on RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
            "score": 68,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "36851aee36c7e5a0",
            "authors": [
                "Haozhan Li",
                "Yuxin Zuo",
                "Jiale Yu",
                "Yuhao Zhang",
                "Zhaohui Yang",
                "Kaiyan Zhang",
                "Xuekai Zhu",
                "Yuchen Zhang",
                "Tianxing Chen",
                "Ganqu Cui",
                "Dehui Wang",
                "Dingxiang Luo",
                "Yuchen Fan",
                "Youbang Sun",
                "Jia Zeng",
                "Jiangmiao Pang",
                "Shanghang Zhang",
                "Yu Wang",
                "Yao Mu",
                "Bowen Zhou",
                "Ning Ding"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai AI Lab",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09674.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#rl",
                    "#robotics",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "SimpleVLA-RL - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ° Vision-Language-Action (VLA). ĞĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ÑĞ´Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. SimpleVLA-RL ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸ĞµÑÑ Ğ² Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Robotic Action Planning with SimpleVLA-RL",
                    "desc": "SimpleVLA-RL is a reinforcement learning framework designed to improve Vision-Language-Action (VLA) models for robotic manipulation. It addresses challenges like the need for extensive human-operated robotic trajectories and the difficulty in generalizing to new tasks. By implementing techniques such as VLA-specific trajectory sampling and multi-environment rendering, SimpleVLA-RL enhances long-horizon action planning and achieves state-of-the-art performance. Additionally, it uncovers new patterns during training, demonstrating its ability to go beyond traditional supervised fine-tuning methods."
                },
                "zh": {
                    "title": "SimpleVLA-RLï¼šæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„é•¿æ—¶é—´è§„åˆ’èƒ½åŠ›",
                    "desc": "SimpleVLA-RLæ˜¯ä¸€ä¸ªé’ˆå¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºé•¿æ—¶é—´è·¨åº¦çš„åŠ¨ä½œè§„åˆ’èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ç‰¹å®šçš„è½¨è¿¹é‡‡æ ·ã€å¯æ‰©å±•çš„å¹¶è¡Œå¤„ç†å’Œå¤šç¯å¢ƒæ¸²æŸ“ç­‰æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚SimpleVLA-RLåœ¨LIBEROæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œå¹¶åœ¨RoboTwin 1.0å’Œ2.0ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åŸºå‡†ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¯¥æ¡†æ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç°äº†ä¸€ç§æ–°ç°è±¡â€œpushcutâ€ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½å¤Ÿè¯†åˆ«å‡ºæ–°çš„æ¨¡å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09174",
            "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for\n  Speech-to-Speech LLMs",
            "url": "https://huggingface.co/papers/2509.09174",
            "abstract": "EchoX, a speech-to-speech large language model, addresses the acoustic-semantic gap by integrating semantic representations, preserving reasoning abilities, and achieving advanced performance on knowledge-based benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as a speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at https://github.com/FreedomIntelligence/EchoX.",
            "score": 55,
            "issue_id": 5853,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "b6e2cc4088bce9ac",
            "authors": [
                "Yuhao Zhang",
                "Yuhao Du",
                "Zhanchen Dai",
                "Xiangnan Ma",
                "Kaiqi Kou",
                "Benyou Wang",
                "Haizhou Li"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09174.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#benchmark",
                    "#audio",
                    "#dataset"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ²ÑƒĞºĞ¾Ğ¼ Ğ¸ ÑĞ¼Ñ‹ÑĞ»Ğ¾Ğ¼ Ğ² Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "EchoX - ÑÑ‚Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ°ĞºÑƒÑÑ‚Ğ¸ĞºĞ¾-ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ°. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. EchoX Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ Ğ¾ĞºĞ¾Ğ»Ğ¾ 6000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Bridging the Acoustic-Semantic Gap with EchoX",
                    "desc": "EchoX is a speech-to-speech large language model (SLLM) designed to overcome the challenges of the acoustic-semantic gap in speech processing. By integrating semantic representations into its training, EchoX maintains strong reasoning capabilities that are often lost in traditional SLLMs. This model dynamically generates speech training targets, allowing it to effectively learn from both acoustic and semantic features. As a result, EchoX demonstrates superior performance on various knowledge-based benchmarks, showcasing its advanced capabilities in understanding and generating speech."
                },
                "zh": {
                    "title": "EchoXï¼šæ‰“ç ´å£°å­¦ä¸è¯­ä¹‰çš„å£å’",
                    "desc": "EchoXæ˜¯ä¸€ç§è¯­éŸ³åˆ°è¯­éŸ³çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å£°å­¦ä¸è¯­ä¹‰ä¹‹é—´çš„å·®è·ã€‚å®ƒé€šè¿‡æ•´åˆè¯­ä¹‰è¡¨ç¤ºï¼Œä¿æŒæ¨ç†èƒ½åŠ›ï¼Œä»è€Œåœ¨çŸ¥è¯†åŸºç¡€çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚çš„è¡¨ç°ã€‚å½“å‰çš„è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹åœ¨çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä¸Šå¸¸å¸¸è¡¨ç°ä¸ä½³ï¼ŒEchoXé€šè¿‡åŠ¨æ€ç”Ÿæˆè¯­éŸ³è®­ç»ƒç›®æ ‡æ¥å…‹æœè¿™ä¸€é™åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEchoXåœ¨çº¦å…­åƒå°æ—¶çš„è®­ç»ƒæ•°æ®ä¸‹ï¼Œåœ¨å¤šä¸ªçŸ¥è¯†é—®ç­”åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06806",
            "title": "MachineLearningLM: Continued Pretraining Language Models on Millions of\n  Synthetic Tabular Prediction Tasks Scales In-Context ML",
            "url": "https://huggingface.co/papers/2509.06806",
            "abstract": "MachineLearningLM enhances a general-purpose LLM with robust in-context machine learning capabilities through continued pretraining with synthesized ML tasks, achieving high performance across various domains without task-specific training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.   Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.   Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU.",
            "score": 53,
            "issue_id": 5873,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "949af9972afa4814",
            "authors": [
                "Haoyu Dong",
                "Pengkun Zhang",
                "Mingzhe Lu",
                "Yanzhen Shen",
                "Guolin Ke"
            ],
            "affiliations": [
                "SCUT",
                "Stanford",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06806.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#dataset",
                    "#training",
                    "#multimodal",
                    "#transfer_learning",
                    "#agi",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "MachineLearningLM - ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ - ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ»ĞµÑĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ°. MachineLearningLM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ LLM Ğ½Ğ° 15% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "Empowering LLMs with In-Context Machine Learning Skills",
                    "desc": "MachineLearningLM is a framework that enhances a general-purpose large language model (LLM) by enabling it to perform machine learning tasks effectively through continued pretraining. It synthesizes a variety of machine learning tasks from structural causal models, allowing the model to learn from many examples without needing specific training for each task. The approach improves the model's performance in various domains, achieving significant accuracy in tabular classification tasks while maintaining its general knowledge and reasoning abilities. This method demonstrates that the model's accuracy improves as the number of in-context examples increases, showcasing its robust in-context learning capabilities."
                },
                "zh": {
                    "title": "å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›",
                    "desc": "MachineLearningLM æ˜¯ä¸€ç§å¢å¼ºé€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡ç»§ç»­é¢„è®­ç»ƒåˆæˆçš„æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œèµ‹äºˆå…¶å¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸä¸­è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒã€‚é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨æ•°ç™¾ä¸‡ä¸ªç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰åˆæˆæœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œæ”¯æŒå¤šè¾¾1024ä¸ªç¤ºä¾‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚å°½ç®¡è®¾ç½®è¾ƒä¸ºç®€å•ï¼ŒMachineLearningLM åœ¨é‡‘èã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ç­‰é¢†åŸŸçš„è¡¨æ ¼åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¹³å‡è¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„å¤šç¤ºä¾‹æ‰©å±•è§„å¾‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09595",
            "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded\n  Long-Duration Avatar Animation Synthesis",
            "url": "https://huggingface.co/papers/2509.09595",
            "abstract": "Kling-Avatar, a cascaded framework, enhances audio-driven avatar video generation by integrating multimodal instruction understanding with photorealistic portrait generation, resulting in high-fidelity, semantically grounded videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.",
            "score": 42,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "9128c939612e1d3e",
            "authors": [
                "Yikang Ding",
                "Jiwen Liu",
                "Wenyuan Zhang",
                "Zekun Wang",
                "Wentao Hu",
                "Liyuan Cui",
                "Mingming Lao",
                "Yingchao Shao",
                "Hui Liu",
                "Xiaohan Li",
                "Ming Chen",
                "Xiaoqiang Liu",
                "Yu-Shen Liu",
                "Pengfei Wan"
            ],
            "affiliations": [
                "Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09595.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#story_generation",
                    "#video",
                    "#games"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹",
                    "desc": "Kling-Avatar - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ². ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ°ĞºĞµÑ‚, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ·Ğ°Ğ¼Ñ‹ÑĞ»Ğ°."
                },
                "en": {
                    "title": "Kling-Avatar: Bridging Audio and Visual Realism in Avatar Generation",
                    "desc": "Kling-Avatar is a new framework designed to improve the generation of avatar videos driven by audio instructions. It combines understanding of multimodal instructions with the creation of realistic portraits, resulting in videos that are both visually appealing and semantically meaningful. The framework operates in two stages: first, it uses a large language model to create a blueprint video that captures high-level semantics like character emotions and movements. Then, it generates detailed sub-clips based on this blueprint, allowing for fast and stable production of long videos while maintaining high fidelity and expressiveness."
                },
                "zh": {
                    "title": "Kling-Avatarï¼šéŸ³é¢‘é©±åŠ¨è™šæ‹Ÿå½¢è±¡ç”Ÿæˆçš„æ–°æ ‡æ†",
                    "desc": "Kling-Avataræ˜¯ä¸€ä¸ªçº§è”æ¡†æ¶ï¼Œæ—¨åœ¨æå‡éŸ³é¢‘é©±åŠ¨çš„è™šæ‹Ÿå½¢è±¡è§†é¢‘ç”Ÿæˆã€‚å®ƒé€šè¿‡æ•´åˆå¤šæ¨¡æ€æŒ‡ä»¤ç†è§£ä¸é€¼çœŸçš„è‚–åƒç”Ÿæˆï¼Œç”Ÿæˆé«˜ä¿çœŸä¸”è¯­ä¹‰æ˜ç¡®çš„è§†é¢‘ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼Œé¦–å…ˆåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆè“å›¾è§†é¢‘ï¼Œç„¶åæ ¹æ®è“å›¾å…³é”®å¸§å¹¶è¡Œç”Ÿæˆå¤šä¸ªå­ç‰‡æ®µã€‚å®éªŒè¡¨æ˜ï¼ŒKling-Avataråœ¨è§†é¢‘ç”Ÿæˆçš„æ¸…æ™°åº¦ã€æƒ…æ„Ÿè¡¨è¾¾å’ŒæŒ‡ä»¤æ§åˆ¶ç­‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œé€‚ç”¨äºæ•°å­—äººç›´æ’­å’Œè§†é¢‘åšå®¢ç­‰å®é™…åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09265",
            "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for\n  Long-Horizon LLM Agents",
            "url": "https://huggingface.co/papers/2509.09265",
            "abstract": "Entropy-Modulated Policy Gradients (EMPG) addresses learning dynamics issues in LLMs by recalibrating policy gradients based on uncertainty and task outcomes, leading to improved performance in long-horizon tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/",
            "score": 37,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "7850d32271ef8349",
            "authors": [
                "Jiawei Wang",
                "Jiacai Liu",
                "Yuqian Fu",
                "Yingru Li",
                "Xintao Wang",
                "Yuan Lin",
                "Yu Yue",
                "Lin Zhang",
                "Yang Wang",
                "Ke Wang"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09265.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#rl",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ EMPG (Entropy-Modulated Policy Gradients) Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½ Ğ¿ĞµÑ€ĞµĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. EMPG ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ÑˆÑ‚Ñ€Ğ°Ñ„ÑƒĞµÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ EMPG Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Boosting Learning with Entropy Awareness",
                    "desc": "Entropy-Modulated Policy Gradients (EMPG) is a new approach that improves learning in Large Language Models (LLMs) by adjusting policy gradients based on uncertainty and task results. In long-horizon tasks, LLMs struggle with sparse rewards, making it hard to credit intermediate actions. EMPG addresses this by recalibrating the learning signal, enhancing updates for confident actions while reducing the impact of uncertain ones. This method leads to better performance in complex tasks, as shown in experiments with various challenging agent environments."
                },
                "zh": {
                    "title": "ç†µè°ƒåˆ¶ç­–ç•¥æ¢¯åº¦ï¼šæå‡é•¿æ—¶é—´ä»»åŠ¡å­¦ä¹ æ•ˆç‡çš„å…³é”®",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç†µè°ƒåˆ¶ç­–ç•¥æ¢¯åº¦ï¼ˆEMPGï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­çš„å­¦ä¹ åŠ¨æ€é—®é¢˜ã€‚é€šè¿‡æ ¹æ®ä¸ç¡®å®šæ€§å’Œä»»åŠ¡ç»“æœé‡æ–°æ ¡å‡†ç­–ç•¥æ¢¯åº¦ï¼ŒEMPGèƒ½å¤Ÿæé«˜åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆç‡ã€‚è¯¥æ–¹æ³•æ”¾å¤§äº†å¯¹æ­£ç¡®è‡ªä¿¡åŠ¨ä½œçš„æ›´æ–°ï¼Œæƒ©ç½šè‡ªä¿¡é”™è¯¯ï¼Œå¹¶å‡å¼±æ¥è‡ªä¸ç¡®å®šæ­¥éª¤çš„æ›´æ–°ï¼Œä»è€Œç¨³å®šæ¢ç´¢è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEMPGåœ¨å¤šä¸ªå¤æ‚ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09680",
            "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
            "url": "https://huggingface.co/papers/2509.09680",
            "abstract": "FLUX-Reason-6M and PRISM-Bench address the lack of reasoning-focused datasets and benchmarks for text-to-image models, providing a large-scale dataset and evaluation standard to improve model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .",
            "score": 35,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "60acc7b8f0e01329",
            "authors": [
                "Rongyao Fang",
                "Aldrich Yu",
                "Chengqi Duan",
                "Linjiang Huang",
                "Shuai Bai",
                "Yuxuan Cai",
                "Kun Wang",
                "Si Liu",
                "Xihui Liu",
                "Hongsheng Li"
            ],
            "affiliations": [
                "Alibaba",
                "BUAA",
                "CUHK",
                "HKU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09680.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#benchmark",
                    "#open_source",
                    "#long_context",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ",
                    "desc": "FLUX-Reason-6M Ğ¸ PRISM-Bench Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. FLUX-Reason-6M Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ 20 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. PRISM-Bench Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ ÑĞµĞ¼ÑŒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Long Text Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Generation Chain-of-Thought (GCoT). ĞĞ±ÑˆĞ¸Ñ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° 19 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° PRISM-Bench Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Text-to-Image Models with Reasoning Datasets and Benchmarks",
                    "desc": "FLUX-Reason-6M and PRISM-Bench are initiatives aimed at enhancing text-to-image (T2I) models by providing a large-scale dataset and a robust evaluation framework. FLUX-Reason-6M includes 6 million images and 20 million bilingual descriptions that focus on teaching complex reasoning through structured characteristics. The dataset is meticulously curated using extensive computational resources, making it a valuable asset for researchers. PRISM-Bench introduces a new evaluation standard with multiple tracks to assess model performance, revealing significant gaps and guiding future improvements in T2I generation."
                },
                "zh": {
                    "title": "æ¨åŠ¨æ¨ç†å¯¼å‘çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ",
                    "desc": "FLUX-Reason-6Må’ŒPRISM-Benchæ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç¼ºä¹ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„æ•°æ®é›†å’ŒåŸºå‡†çš„é—®é¢˜ã€‚FLUX-Reason-6Mæ˜¯ä¸€ä¸ªåŒ…å«600ä¸‡å¼ é«˜è´¨é‡å›¾åƒå’Œ2000ä¸‡æ¡åŒè¯­æè¿°çš„å¤§å‹æ•°æ®é›†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºæ•™æˆå¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚PRISM-Benchæä¾›äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ ‡å‡†ï¼ŒåŒ…å«ä¸ƒä¸ªä¸åŒçš„è¯„ä¼°è½¨é“ï¼Œç‰¹åˆ«æ˜¯ä¸€ä¸ªä½¿ç”¨ç”Ÿæˆé“¾æ€ç»´ï¼ˆGCoTï¼‰çš„é•¿æ–‡æœ¬æŒ‘æˆ˜ã€‚é€šè¿‡è¿™äº›èµ„æºï¼Œæˆ‘ä»¬å¸Œæœ›æ¨åŠ¨æ¨ç†å¯¼å‘çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸‹ä¸€æ³¢å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09666",
            "title": "Can Understanding and Generation Truly Benefit Together -- or Just\n  Coexist?",
            "url": "https://huggingface.co/papers/2509.09666",
            "abstract": "A novel framework UAE uses reinforcement learning to unify image-to-text and text-to-image processes, enhancing mutual understanding and generation fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce an insightful paradigm through the Auto-Encoder lens-understanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding and generation processes, bringing mutual gains. To implement this, we propose UAE, a novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose Unified-GRPO via reinforcement learning (RL), which covers three stages: (1) A cold-start phase to gently initialize both encoder and decoder with a semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoder's reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. A surprising \"aha moment\" arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates a profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity.",
            "score": 30,
            "issue_id": 5856,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "d213e626e6faeaa5",
            "authors": [
                "Zhiyuan Yan",
                "Kaiqing Lin",
                "Zongjian Li",
                "Junyan Ye",
                "Hui Han",
                "Zhendong Wang",
                "Hao Liu",
                "Bin Lin",
                "Hao Li",
                "Xue Xu",
                "Xinyan Xiao",
                "Jingdong Wang",
                "Haifeng Wang",
                "Li Yuan"
            ],
            "affiliations": [
                "Baidu ERNIE",
                "PKU",
                "Rabbitpre AI",
                "SYSU",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09666.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#multimodal",
                    "#games",
                    "#rl"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº UAE, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğº ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ (I2T), ÑĞ¶Ğ¸Ğ¼Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚, Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ (T2I), Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ°Ğº ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ†ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Unified-GRPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unifying Image and Text with Reinforcement Learning",
                    "desc": "This paper presents a new framework called UAE that uses reinforcement learning to connect image-to-text (I2T) and text-to-image (T2I) processes. It employs an Auto-Encoder approach where the encoder compresses images into text and the decoder reconstructs images from that text. The framework focuses on improving the mutual understanding between these two processes by using reconstruction fidelity as a training goal. The authors also introduce a benchmark called Unified-Bench to evaluate the effectiveness of this unified multimodal learning approach."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¤šæ¨¡æ€å­¦ä¹ çš„æ–°æ¡†æ¶UAE",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶UAEï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç»Ÿä¸€å›¾åƒåˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒçš„è¿‡ç¨‹ï¼Œå¢å¼ºäº†ç›¸äº’ç†è§£å’Œç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬é€šè¿‡è‡ªç¼–ç å™¨çš„è§†è§’ï¼Œå°†ç†è§£è¿‡ç¨‹è§†ä¸ºç¼–ç å™¨ï¼ˆI2Tï¼‰ï¼Œå°†ç”Ÿæˆè¿‡ç¨‹è§†ä¸ºè§£ç å™¨ï¼ˆT2Iï¼‰ï¼Œå¹¶ä»¥é‡å»ºç²¾åº¦ä½œä¸ºç»Ÿä¸€è®­ç»ƒç›®æ ‡ã€‚UAEæ¡†æ¶é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ å®ç°ï¼šå†·å¯åŠ¨é˜¶æ®µã€ç”Ÿæˆç†è§£é˜¶æ®µå’Œç†è§£ç”Ÿæˆé˜¶æ®µï¼Œç¡®ä¿ä¿¡æ¯åœ¨ç†è§£å’Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„åŒå‘æµåŠ¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†Unified-BenchåŸºå‡†ï¼Œè¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€å­¦ä¹ çš„ç¨‹åº¦ï¼Œå‘ç°éšç€å¼ºåŒ–å­¦ä¹ çš„è¿›å±•ï¼Œç¼–ç å™¨èƒ½å¤Ÿç”Ÿæˆæ›´å…·æè¿°æ€§çš„æ–‡æœ¬ï¼Œè€Œè§£ç å™¨åˆ™èƒ½æ›´å¥½åœ°ç†è§£è¿™äº›å¤æ‚æè¿°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09676",
            "title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
            "url": "https://huggingface.co/papers/2509.09676",
            "abstract": "SpatialVID, a large-scale dataset with diverse videos and dense 3D annotations, enhances model generalization and performance in video and 3D vision research.  \t\t\t\t\tAI-generated summary \t\t\t\t Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect SpatialVID, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community.",
            "score": 22,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "b2d981674edaecf5",
            "authors": [
                "Jiahao Wang",
                "Yufeng Yuan",
                "Rujie Zheng",
                "Youtian Lin",
                "Jian Gao",
                "Lin-Zhuo Chen",
                "Yajie Bao",
                "Yi Zhang",
                "Chang Zeng",
                "Yanxi Zhou",
                "Xiaoxiao Long",
                "Hao Zhu",
                "Zhaoxiang Zhang",
                "Xun Cao",
                "Yao Yao"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Science",
                "Nanjing University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09676.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "SpatialVID: Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ",
                    "desc": "SpatialVID - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ 3D-Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 21 000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² 2,7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ 7 089 Ñ‡Ğ°ÑĞ¾Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ·Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ°ÑĞºĞ¸, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğ¸ ÑĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. SpatialVID ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unlocking 3D Vision with SpatialVID: A New Era of Video Datasets",
                    "desc": "SpatialVID is a comprehensive dataset designed to improve machine learning models in video and 3D vision tasks. It contains over 21,000 hours of diverse, real-world video footage, which has been meticulously processed into 2.7 million clips. Each clip is enriched with dense 3D annotations, including camera poses, depth maps, and motion instructions, providing a rich source of training data. This dataset addresses the limitations of existing datasets by offering high-quality, large-scale data that enhances model generalization and performance in spatial intelligence applications."
                },
                "zh": {
                    "title": "SpatialVIDï¼šæå‡è§†é¢‘ä¸3Dè§†è§‰ç ”ç©¶çš„å…³é”®æ•°æ®é›†",
                    "desc": "SpatialVIDæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·åŒ–çš„è§†é¢‘å’Œå¯†é›†çš„3Dæ³¨é‡Šï¼Œæ—¨åœ¨æå‡è§†é¢‘å’Œ3Dè§†è§‰ç ”ç©¶ä¸­çš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ã€‚è¯¥æ•°æ®é›†æ”¶é›†äº†è¶…è¿‡21,000å°æ—¶çš„åŸå§‹è§†é¢‘ï¼Œå¹¶é€šè¿‡åˆ†å±‚è¿‡æ»¤ç®¡é“å¤„ç†æˆ270ä¸‡æ®µè§†é¢‘ç‰‡æ®µï¼Œæä¾›äº†ä¸°å¯Œçš„åŠ¨æ€å†…å®¹ã€‚æ¯ä¸ªç‰‡æ®µéƒ½é™„æœ‰è¯¦ç»†çš„ç©ºé—´å’Œè¯­ä¹‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç›¸æœºä½å§¿ã€æ·±åº¦å›¾ã€åŠ¨æ€æ©ç ã€ç»“æ„åŒ–æ ‡é¢˜å’Œåºåˆ—åŒ–è¿åŠ¨æŒ‡ä»¤ã€‚SpatialVIDçš„æ•°æ®ç»Ÿè®¡åˆ†ææ˜¾ç¤ºå‡ºå…¶ä¸°å¯Œæ€§å’Œå¤šæ ·æ€§ï¼Œç›´æ¥ä¿ƒè¿›äº†æ¨¡å‹çš„æ³›åŒ–å’Œæ€§èƒ½æå‡ï¼Œæˆä¸ºè§†é¢‘å’Œ3Dè§†è§‰ç ”ç©¶é¢†åŸŸçš„é‡è¦èµ„äº§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.08031",
            "title": "AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs",
            "url": "https://huggingface.co/papers/2509.08031",
            "abstract": "AU-Harness is an efficient and comprehensive evaluation framework for Large Audio Language Models (LALMs) that addresses issues of speed, reproducibility, and task coverage, revealing gaps in temporal understanding and spoken language reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Audio Language Models (LALMs) are rapidly advancing, but evaluating them remains challenging due to inefficient toolkits that limit fair comparison and systematic assessment. Current frameworks suffer from three critical issues: slow processing that bottlenecks large-scale studies, inconsistent prompting that hurts reproducibility, and narrow task coverage that misses important audio reasoning capabilities. We introduce AU-Harness, an efficient and comprehensive evaluation framework for LALMs. Our system achieves a speedup of up to 127% over existing toolkits through optimized batch processing and parallel execution, enabling large-scale evaluations previously impractical. We provide standardized prompting protocols and flexible configurations for fair model comparison across diverse scenarios. Additionally, we introduce two new evaluation categories: LLM-Adaptive Diarization for temporal audio understanding and Spoken Language Reasoning for complex audio-based cognitive tasks. Through evaluation across 380+ tasks, we reveal significant gaps in current LALMs, particularly in temporal understanding and complex spoken language reasoning tasks. Our findings also highlight a lack of standardization in instruction modality existent across audio benchmarks, which can lead up performance differences up to 9.5 absolute points on the challenging complex instruction following downstream tasks. AU-Harness provides both practical evaluation tools and insights into model limitations, advancing systematic LALM development.",
            "score": 18,
            "issue_id": 5868,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 9",
                "zh": "9æœˆ9æ—¥"
            },
            "hash": "bb86095d23b97040",
            "authors": [
                "Sidharth Surapaneni",
                "Hoang Nguyen",
                "Jash Mehta",
                "Aman Tiwari",
                "Oluwanifemi Bamgbose",
                "Akshay Kalkunte",
                "Sai Rajeswar",
                "Sathwik Tejaswi Madhusudhan"
            ],
            "affiliations": [
                "ServiceNow",
                "University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.08031.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#optimization",
                    "#audio",
                    "#reasoning"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "AU-Harness: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "AU-Harness - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LALM). ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ LALM. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°ĞºĞµÑ‚Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ´Ğ¾ 127% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. AU-Harness Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: LLM-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "AU-Harness: Revolutionizing Evaluation for Large Audio Language Models",
                    "desc": "AU-Harness is a new evaluation framework designed specifically for Large Audio Language Models (LALMs). It addresses key challenges such as slow processing speeds, inconsistent evaluation methods, and limited task coverage, which hinder effective model assessment. By optimizing batch processing and enabling parallel execution, AU-Harness improves evaluation speed by up to 127%, allowing for more extensive and fair comparisons of models. Additionally, it introduces new evaluation categories to assess temporal understanding and spoken language reasoning, revealing significant gaps in current LALMs' capabilities."
                },
                "zh": {
                    "title": "AU-Harnessï¼šæå‡éŸ³é¢‘è¯­è¨€æ¨¡å‹è¯„ä¼°æ•ˆç‡çš„åˆ©å™¨",
                    "desc": "AU-Harnessæ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰ã€‚å®ƒè§£å†³äº†è¯„ä¼°é€Ÿåº¦æ…¢ã€å¯é‡å¤æ€§å·®å’Œä»»åŠ¡è¦†ç›–é¢çª„çš„é—®é¢˜ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨æ—¶é—´ç†è§£å’Œå£è¯­æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡ä¼˜åŒ–æ‰¹å¤„ç†å’Œå¹¶è¡Œæ‰§è¡Œï¼ŒAU-Harnessçš„å¤„ç†é€Ÿåº¦æ¯”ç°æœ‰å·¥å…·æé«˜äº†127%ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›äº†æ ‡å‡†åŒ–çš„æç¤ºåè®®å’Œçµæ´»çš„é…ç½®ï¼Œæ”¯æŒåœ¨å¤šç§åœºæ™¯ä¸‹è¿›è¡Œå…¬å¹³çš„æ¨¡å‹æ¯”è¾ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06888",
            "title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning",
            "url": "https://huggingface.co/papers/2509.06888",
            "abstract": "mmBERT, an encoder-only language model pretrained on multilingual text, achieves high performance on classification and retrieval tasks using an inverse mask ratio schedule and inverse temperature sampling ratio, particularly benefiting from the inclusion of low-resource languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. We introduce mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, we show that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages.",
            "score": 10,
            "issue_id": 5866,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "22f0ea59278ee621",
            "authors": [
                "Marc Marone",
                "Orion Weller",
                "William Fleshman",
                "Eugene Yang",
                "Dawn Lawrie",
                "Benjamin Van Durme"
            ],
            "affiliations": [
                "Johns Hopkins University Center for Language and Speech Processing (CLSP)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06888.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multilingual",
                    "#low_resource",
                    "#dataset"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "mmBERT: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ mmBERT - Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ¸Ğ¿Ğ° encoder-only, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 3T Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1800 ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ’ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. mmBERT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ°Ğº Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Multilingual Understanding with mmBERT",
                    "desc": "mmBERT is a new encoder-only language model that has been pretrained on a vast dataset of multilingual text, covering over 1800 languages. It introduces innovative techniques such as an inverse mask ratio schedule and inverse temperature sampling ratio, which enhance its performance on classification and retrieval tasks. Notably, mmBERT incorporates low-resource languages during a specific training phase, leading to significant improvements in model accuracy. As a result, mmBERT demonstrates competitive performance compared to leading models like OpenAI's o3 and Google's Gemini 2.5 Pro, especially in handling both high and low-resource languages."
                },
                "zh": {
                    "title": "mmBERTï¼šå¤šè¯­è¨€æ¨¡å‹çš„æ–°çªç ´",
                    "desc": "mmBERTæ˜¯ä¸€ç§ä»…ä½¿ç”¨ç¼–ç å™¨çš„è¯­è¨€æ¨¡å‹ï¼Œç»è¿‡å¤šè¯­è¨€æ–‡æœ¬çš„é¢„è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†é€†æ©ç æ¯”ç‡è°ƒåº¦å’Œé€†æ¸©åº¦é‡‡æ ·æ¯”ç‡ç­‰æ–°é¢–æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€çš„å¼•å…¥ä¸Šå–å¾—äº†æ˜¾è‘—æ•ˆæœã€‚é€šè¿‡åœ¨è¡°å‡é˜¶æ®µä»…åŠ å…¥1700å¤šç§ä½èµ„æºè¯­è¨€ï¼ŒmmBERTåœ¨è®­ç»ƒæ•°æ®è¾ƒå°‘çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚æœ€ç»ˆï¼ŒmmBERTåœ¨åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä»¥å¾€çš„æ¨¡å‹ï¼Œè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09286",
            "title": "Visual Programmability: A Guide for Code-as-Thought in Chart\n  Understanding",
            "url": "https://huggingface.co/papers/2509.09286",
            "abstract": "VLMs are enhanced with an adaptive framework that selects between code-based and direct visual reasoning for chart understanding, improving performance and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Chart understanding presents a critical test to the reasoning capabilities of Vision-Language Models (VLMs). Prior approaches face critical limitations: some rely on external tools, making them brittle and constrained by a predefined toolkit, while others fine-tune specialist models that often adopt a single reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate steps of text-based reasoning are difficult to verify, which complicates the use of reinforcement-learning signals that reward factual accuracy. To address this, we propose a Code-as-Thought (CaT) approach to represent the visual information of a chart in a verifiable, symbolic format. Our key insight is that this strategy must be adaptive: a fixed, code-only implementation consistently fails on complex charts where symbolic representation is unsuitable. This finding leads us to introduce Visual Programmability: a learnable property that determines if a chart-question pair is better solved with code or direct visual analysis. We implement this concept in an adaptive framework where a VLM learns to choose between the CaT pathway and a direct visual reasoning pathway. The selection policy of the model is trained with reinforcement learning using a novel dual-reward system. This system combines a data-accuracy reward to ground the model in facts and prevent numerical hallucination, with a decision reward that teaches the model when to use each strategy, preventing it from defaulting to a single reasoning mode. Experiments demonstrate strong and robust performance across diverse chart-understanding benchmarks. Our work shows that VLMs can be taught not only to reason but also how to reason, dynamically selecting the optimal reasoning pathway for each task.",
            "score": 9,
            "issue_id": 5853,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "73de225642b07635",
            "authors": [
                "Bohao Tang",
                "Yan Ma",
                "Fei Zhang",
                "Jiadi Su",
                "Ethan Chern",
                "Zhulin Hu",
                "Zhixin Wang",
                "Pengfei Liu",
                "Ya Zhang"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09286.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#benchmark",
                    "#multimodal",
                    "#training",
                    "#hallucinations"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ VLM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (VLM). ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Code-as-Thought (CaT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ° Ğ² ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ¾ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ - Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ñ‹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº-Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ CaT Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Adaptive Reasoning for Enhanced Chart Understanding in VLMs",
                    "desc": "This paper introduces an adaptive framework for Vision-Language Models (VLMs) that enhances their ability to understand charts by selecting between code-based reasoning and direct visual analysis. The authors highlight the limitations of previous methods, which either rely on rigid external tools or single reasoning strategies that are hard to verify. They propose a novel approach called Code-as-Thought (CaT) that represents visual information in a symbolic format, allowing for better verification and accuracy. By implementing Visual Programmability, the model learns to dynamically choose the most effective reasoning pathway for each chart-question pair, leading to improved performance across various benchmarks."
                },
                "zh": {
                    "title": "åŠ¨æ€é€‰æ‹©æœ€ä½³æ¨ç†è·¯å¾„çš„è§†è§‰è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”æ¡†æ¶åœ¨ä»£ç åŸºç¡€æ¨ç†å’Œç›´æ¥è§†è§‰æ¨ç†ä¹‹é—´è¿›è¡Œé€‰æ‹©ï¼Œä»¥æé«˜å›¾è¡¨ç†è§£çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚ä»¥å¾€çš„æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œä¾èµ–å¤–éƒ¨å·¥å…·æˆ–å•ä¸€æ¨ç†ç­–ç•¥ï¼Œå¯¼è‡´åœ¨å¤æ‚å›¾è¡¨ä¸Šè¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬å¼•å…¥äº†ä»£ç ä½œä¸ºæ€ç»´ï¼ˆCaTï¼‰çš„æ–¹æ³•ï¼Œå°†å›¾è¡¨çš„è§†è§‰ä¿¡æ¯ä»¥å¯éªŒè¯çš„ç¬¦å·æ ¼å¼è¡¨ç¤ºï¼Œå¹¶æå‡ºè§†è§‰å¯ç¼–ç¨‹æ€§ï¼Œå…è®¸æ¨¡å‹æ ¹æ®å›¾è¡¨å’Œé—®é¢˜çš„ç‰¹æ€§é€‰æ‹©æœ€ä½³çš„æ¨ç†æ–¹å¼ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹çš„é€‰æ‹©ç­–ç•¥ï¼Œç»“åˆæ•°æ®å‡†ç¡®æ€§å¥–åŠ±å’Œå†³ç­–å¥–åŠ±ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­åŠ¨æ€é€‰æ‹©æœ€ä¼˜æ¨ç†è·¯å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.06266",
            "title": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View\n  Scenes",
            "url": "https://huggingface.co/papers/2509.06266",
            "abstract": "Ego3D-Bench evaluates VLMs on ego-centric, multi-view outdoor data, revealing performance gaps, and Ego3D-VLM enhances 3D spatial reasoning through cognitive map generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding 3D spatial relationships remains a major limitation of current Vision-Language Models (VLMs). Prior work has addressed this issue by creating spatial question-answering (QA) datasets based on single images or indoor videos. However, real-world embodied AI agents such as robots and self-driving cars typically rely on ego-centric, multi-view observations. To this end, we introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial reasoning abilities of VLMs using ego-centric, multi-view outdoor data. Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement from human annotators to ensure quality and diversity. We benchmark 16 SOTA VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results reveal a notable performance gap between human level scores and VLM performance, highlighting that current VLMs still fall short of human level spatial understanding. To bridge this gap, we propose Ego3D-VLM, a post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM generates cognitive map based on estimated global 3D coordinates, resulting in 12% average improvement on multi-choice QA and 56% average improvement on absolute distance estimation. Ego3D-VLM is modular and can be integrated with any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for advancing toward human level spatial understanding in real-world, multi-view environments.",
            "score": 8,
            "issue_id": 5868,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "421233f9095a7b13",
            "authors": [
                "Mohsen Gholami",
                "Ahmad Rezaei",
                "Zhou Weimin",
                "Yong Zhang",
                "Mohammad Akbari"
            ],
            "affiliations": [
                "Huawei Cloud",
                "Huawei Technologies Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.06266.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#3d",
                    "#reasoning",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ VLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ego3D",
                    "desc": "Ego3D-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. Ğ¢ĞµÑÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 8600 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ»ÑĞ´ĞµĞ¹-Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLM Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ VLM Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ego3D-VLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… 3D-ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚."
                },
                "en": {
                    "title": "Bridging the Gap in 3D Spatial Reasoning for VLMs",
                    "desc": "This paper introduces Ego3D-Bench, a benchmark for evaluating Vision-Language Models (VLMs) on ego-centric, multi-view outdoor data, addressing the challenge of understanding 3D spatial relationships. It highlights the performance gaps between human-level spatial reasoning and that of current VLMs, based on a comprehensive evaluation of 16 state-of-the-art models. To improve VLMs' spatial reasoning, the authors propose Ego3D-VLM, a framework that generates cognitive maps from 3D coordinates, leading to significant performance enhancements in spatial question-answering tasks. Together, these contributions aim to advance VLMs towards achieving human-like spatial understanding in real-world scenarios."
                },
                "zh": {
                    "title": "æå‡VLMçš„ä¸‰ç»´ç©ºé—´æ¨ç†èƒ½åŠ›",
                    "desc": "Ego3D-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å¤šè§†è§’æˆ·å¤–æ•°æ®ä¸Šçš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡8600å¯¹é—®ç­”å¯¹ï¼Œç¡®ä¿äº†æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„VLMåœ¨ç©ºé—´ç†è§£æ–¹é¢ä¸äººç±»æ°´å¹³å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†Ego3D-VLMæ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆè®¤çŸ¥åœ°å›¾æ¥å¢å¼ºVLMçš„ä¸‰ç»´ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09118",
            "title": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust\n  Text-based Person Retrieval",
            "url": "https://huggingface.co/papers/2509.09118",
            "abstract": "GA-DMS framework enhances CLIP for person representation learning by improving data quality and model architecture, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks.",
            "score": 7,
            "issue_id": 5853,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "7693d45e6980cf3f",
            "authors": [
                "Tianlu Zheng",
                "Yifan Zhang",
                "Xiang An",
                "Ziyong Feng",
                "Kaicheng Yang",
                "Qichuan Ding"
            ],
            "affiliations": [
                "DeepGlint",
                "Northeastern University",
                "South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09118.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#benchmark",
                    "#transfer_learning",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "ğŸ‘¤",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ CLIP Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GA-DMS Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ CLIP Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° WebPerson Ğ¸Ğ· 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚. GA-DMS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾-Ğ°Ñ‚Ñ‚ĞµĞ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ Ñ†ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing CLIP for Superior Person Representation Learning",
                    "desc": "The GA-DMS framework enhances the CLIP model for person representation learning by addressing data quality and model architecture challenges. It introduces a new data construction pipeline that uses MLLMs to create a large dataset of 5 million high-quality person-centric image-text pairs, called WebPerson. The framework also employs a dual-masking technique that adapts to noisy text tokens, improving the model's ability to align visual and textual information. As a result, GA-DMS achieves state-of-the-art performance in various benchmarks for person representation tasks."
                },
                "zh": {
                    "title": "GA-DMSï¼šæå‡CLIPçš„äººç‰©è¡¨ç¤ºå­¦ä¹ ",
                    "desc": "GA-DMSæ¡†æ¶é€šè¿‡æ”¹è¿›æ•°æ®è´¨é‡å’Œæ¨¡å‹æ¶æ„ï¼Œå¢å¼ºäº†CLIPåœ¨äººç‰©è¡¨ç¤ºå­¦ä¹ ä¸­çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶è§£å†³äº†äººç‰©ä¸­å¿ƒå›¾åƒçš„æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œå…¨å±€å¯¹æ¯”å­¦ä¹ çš„å±€é™æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æŠ—å™ªå£°çš„æ•°æ®æ„å»ºæµç¨‹ï¼Œç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«500ä¸‡é«˜è´¨é‡äººç‰©å›¾åƒ-æ–‡æœ¬å¯¹çš„å¤§å‹æ•°æ®é›†WebPersonã€‚GA-DMSæ¡†æ¶é€šè¿‡åŸºäºæ¢¯åº¦æ³¨æ„åŠ›ç›¸ä¼¼åº¦åˆ†æ•°è‡ªé€‚åº”åœ°å±è”½å™ªå£°æ–‡æœ¬æ ‡è®°ï¼Œæ˜¾è‘—æé«˜äº†è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01964",
            "title": "2D Gaussian Splatting with Semantic Alignment for Image Inpainting",
            "url": "https://huggingface.co/papers/2509.01964",
            "abstract": "A novel image inpainting framework using 2D Gaussian Splatting achieves competitive performance by combining continuous field representation with pretrained DINO model features for global semantic consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Gaussian Splatting (GS), a recent technique for converting discrete points into continuous spatial representations, has shown promising results in 3D scene modeling and 2D image super-resolution. In this paper, we explore its untapped potential for image inpainting, which demands both locally coherent pixel synthesis and globally consistent semantic restoration. We propose the first image inpainting framework based on 2D Gaussian Splatting, which encodes incomplete images into a continuous field of 2D Gaussian splat coefficients and reconstructs the final image via a differentiable rasterization process. The continuous rendering paradigm of GS inherently promotes pixel-level coherence in the inpainted results. To improve efficiency and scalability, we introduce a patch-wise rasterization strategy that reduces memory overhead and accelerates inference. For global semantic consistency, we incorporate features from a pretrained DINO model. We observe that DINO's global features are naturally robust to small missing regions and can be effectively adapted to guide semantic alignment in large-mask scenarios, ensuring that the inpainted content remains contextually consistent with the surrounding scene. Extensive experiments on standard benchmarks demonstrate that our method achieves competitive performance in both quantitative metrics and perceptual quality, establishing a new direction for applying Gaussian Splatting to 2D image processing.",
            "score": 5,
            "issue_id": 5855,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "60a42770e646820d",
            "authors": [
                "Hongyu Li",
                "Chaofeng Chen",
                "Xiaoming Li",
                "Guangming Lu"
            ],
            "affiliations": [
                "Harbin Institute of Technology, Shenzhen",
                "Nanyang Technological University",
                "School of Artificial Intelligence, Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01964.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ–Œï¸",
                "ru": {
                    "title": "Ğ”Ğ¾Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 2D Gaussian Splatting Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²",
                    "desc": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¾Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ 2D Gaussian Splatting, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ñ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DINO Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğµ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² 2D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ."
                },
                "en": {
                    "title": "Revolutionizing Image Inpainting with 2D Gaussian Splatting",
                    "desc": "This paper presents a new framework for image inpainting using 2D Gaussian Splatting, which transforms incomplete images into continuous representations. By leveraging pretrained DINO model features, the framework ensures that the inpainted areas maintain global semantic consistency with the surrounding content. The method employs a differentiable rasterization process to reconstruct images, promoting pixel-level coherence in the results. Additionally, a patch-wise rasterization strategy is introduced to enhance efficiency and reduce memory usage, leading to competitive performance in both quantitative and perceptual evaluations."
                },
                "zh": {
                    "title": "é«˜æ•ˆå›¾åƒä¿®å¤çš„æ–°æ–¹å‘ï¼šäºŒç»´é«˜æ–¯ç‚¹äº‘æŠ€æœ¯",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å›¾åƒä¿®å¤æ¡†æ¶ï¼Œåˆ©ç”¨äºŒç»´é«˜æ–¯ç‚¹äº‘ï¼ˆ2D Gaussian Splattingï¼‰æŠ€æœ¯ï¼Œç»“åˆé¢„è®­ç»ƒçš„DINOæ¨¡å‹ç‰¹å¾ï¼Œä»¥å®ç°å…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶å°†ä¸å®Œæ•´çš„å›¾åƒç¼–ç ä¸ºäºŒç»´é«˜æ–¯ç‚¹ç³»æ•°çš„è¿ç»­åœºï¼Œå¹¶é€šè¿‡å¯å¾®åˆ†å…‰æ …åŒ–è¿‡ç¨‹é‡å»ºæœ€ç»ˆå›¾åƒã€‚é«˜æ–¯ç‚¹äº‘çš„è¿ç»­æ¸²æŸ“æ–¹å¼è‡ªç„¶ä¿ƒè¿›äº†ä¿®å¤ç»“æœçš„åƒç´ çº§ä¸€è‡´æ€§ã€‚é€šè¿‡å¼•å…¥åŸºäºè¡¥ä¸çš„å…‰æ …åŒ–ç­–ç•¥ï¼Œæœ¬æ–‡åœ¨æé«˜æ•ˆç‡å’Œå¯æ‰©å±•æ€§çš„åŒæ—¶ï¼Œç¡®ä¿ä¿®å¤å†…å®¹ä¸å‘¨å›´åœºæ™¯ä¿æŒä¸€è‡´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09614",
            "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering",
            "url": "https://huggingface.co/papers/2509.09614",
            "abstract": "LoCoBench evaluates long-context language models in complex software development scenarios, addressing the gap in understanding entire codebases and maintaining architectural consistency across large-scale systems.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of long-context language models with context windows extending to millions of tokens has created new opportunities for sophisticated code understanding and software development evaluation. We propose LoCoBench, a comprehensive benchmark specifically designed to evaluate long-context LLMs in realistic, complex software development scenarios. Unlike existing code evaluation benchmarks that focus on single-function completion or short-context tasks, LoCoBench addresses the critical evaluation gap for long-context capabilities that require understanding entire codebases, reasoning across multiple files, and maintaining architectural consistency across large-scale software systems. Our benchmark provides 8,000 evaluation scenarios systematically generated across 10 programming languages, with context lengths spanning 10K to 1M tokens, a 100x variation that enables precise assessment of long-context performance degradation in realistic software development settings. LoCoBench introduces 8 task categories that capture essential long-context capabilities: architectural understanding, cross-file refactoring, multi-session development, bug investigation, feature implementation, code comprehension, integration testing, and security analysis. Through a 5-phase pipeline, we create diverse, high-quality scenarios that challenge LLMs to reason about complex codebases at unprecedented scale. We introduce a comprehensive evaluation framework with 17 metrics across 4 dimensions, including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our evaluation of state-of-the-art long-context models reveals substantial performance gaps, demonstrating that long-context understanding in complex software development represents a significant unsolved challenge that demands more attention. LoCoBench is released at: https://github.com/SalesforceAIResearch/LoCoBench.",
            "score": 4,
            "issue_id": 5852,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "15a23d38c535fc1c",
            "authors": [
                "Jielin Qiu",
                "Zuxin Liu",
                "Zhiwei Liu",
                "Rithesh Murthy",
                "Jianguo Zhang",
                "Haolin Chen",
                "Shiyu Wang",
                "Ming Zhu",
                "Liangwei Yang",
                "Juntao Tan",
                "Zhepeng Cen",
                "Cheng Qian",
                "Shelby Heinecke",
                "Weiran Yao",
                "Silvio Savarese",
                "Caiming Xiong",
                "Huan Wang"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09614.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#long_context"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LoCoBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ",
                    "desc": "LoCoBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 8000 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° 10 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¾Ñ‚ 10 Ñ‚Ñ‹Ñ. Ğ´Ğ¾ 1 Ğ¼Ğ»Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². LoCoBench Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ 8 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ½ĞµÑ€ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¾Ğ¹."
                },
                "en": {
                    "title": "Evaluating Long-Context LLMs for Complex Software Development",
                    "desc": "LoCoBench is a new benchmark designed to evaluate long-context language models (LLMs) in complex software development tasks. It focuses on understanding entire codebases and maintaining architectural consistency, which is crucial for large-scale systems. The benchmark includes 8,000 scenarios across 10 programming languages, with context lengths ranging from 10,000 to 1,000,000 tokens, allowing for a thorough assessment of LLM performance. By introducing 8 task categories and a comprehensive evaluation framework, LoCoBench highlights significant performance gaps in current models, emphasizing the need for improved long-context understanding in software development."
                },
                "zh": {
                    "title": "è¯„ä¼°é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„å…¨æ–°åŸºå‡†",
                    "desc": "LoCoBenchæ˜¯ä¸€ä¸ªä¸“é—¨è¯„ä¼°é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è½¯ä»¶å¼€å‘åœºæ™¯ä¸­çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚å®ƒå¡«è¡¥äº†å¯¹æ•´ä¸ªä»£ç åº“ç†è§£å’Œåœ¨å¤§è§„æ¨¡ç³»ç»Ÿä¸­ä¿æŒæ¶æ„ä¸€è‡´æ€§çš„è¯„ä¼°ç©ºç™½ã€‚è¯¥åŸºå‡†æä¾›äº†8000ä¸ªè¯„ä¼°åœºæ™¯ï¼Œæ¶µç›–10ç§ç¼–ç¨‹è¯­è¨€ï¼Œèƒ½å¤Ÿç²¾ç¡®è¯„ä¼°é•¿ä¸Šä¸‹æ–‡æ€§èƒ½çš„ä¸‹é™ã€‚é€šè¿‡å¼•å…¥å¤šç§ä»»åŠ¡ç±»åˆ«å’Œè¯„ä¼°æŒ‡æ ‡ï¼ŒLoCoBenchä¸ºé•¿ä¸Šä¸‹æ–‡ç†è§£åœ¨è½¯ä»¶å¼€å‘ä¸­çš„æŒ‘æˆ˜æä¾›äº†å…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09594",
            "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation",
            "url": "https://huggingface.co/papers/2509.09594",
            "abstract": "A new object-relative control paradigm using a topometric map representation and a local controller achieves better invariance and generalization in visual navigation tasks compared to image-relative methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an \"image-relative\" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning \"object-relative\" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a \"relative\" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed \"ObjectReact\", conditioned directly on a high-level \"WayObject Costmap\" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/",
            "score": 3,
            "issue_id": 5864,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "ebc4a50525683497",
            "authors": [
                "Sourav Garg",
                "Dustin Craggs",
                "Vineeth Bhat",
                "Lachlan Mares",
                "Stefan Podgorski",
                "Madhava Krishna",
                "Feras Dayoub",
                "Ian Reid"
            ],
            "affiliations": [
                "IIIT Hyderabad, India",
                "MBZUAI, UAE",
                "University of Adelaide, Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09594.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#video",
                    "#games",
                    "#agents",
                    "#optimization",
                    "#graphs"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ¿Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¿Ñ€Ğ¾ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ñ‹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ€Ñ‚Ğµ."
                },
                "en": {
                    "title": "Navigating with Objects: A New Control Paradigm for Better Generalization",
                    "desc": "This paper introduces a new approach to visual navigation called object-relative control, which uses a topometric map representation instead of relying solely on images. Unlike traditional image-relative methods, this approach allows for better generalization and invariance, meaning it can adapt to new routes without needing to replicate past experiences. The proposed method utilizes a 3D scene graph to enhance path planning and employs a local controller named 'ObjectReact' that operates independently of direct image inputs. The results show that this method outperforms image-based techniques in various navigation tasks and can effectively transfer learned policies to real-world scenarios."
                },
                "zh": {
                    "title": "å¯¹è±¡ç›¸å¯¹æ§åˆ¶ï¼šè¶…è¶Šå›¾åƒçš„å¯¼èˆªæ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯¹è±¡ç›¸å¯¹æ§åˆ¶èŒƒå¼ï¼Œåˆ©ç”¨æ‹“æ‰‘åœ°å›¾è¡¨ç¤ºå’Œå±€éƒ¨æ§åˆ¶å™¨ï¼Œåœ¨è§†è§‰å¯¼èˆªä»»åŠ¡ä¸­æ¯”å›¾åƒç›¸å¯¹æ–¹æ³•è¡¨ç°æ›´å¥½ã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒç›¸å¯¹æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹è±¡çš„ç‰¹æ€§æä¾›äº†ä¸€ç§ä¸ä»£ç†çš„å§¿æ€å’Œè½¨è¿¹æ— å…³çš„ä¸–ç•Œè¡¨ç¤ºã€‚æˆ‘ä»¬æå‡ºçš„â€œObjectReactâ€å±€éƒ¨æ§åˆ¶å™¨ç›´æ¥åŸºäºé«˜å±‚æ¬¡çš„â€œWayObject Costmapâ€è¡¨ç¤ºè¿›è¡Œè®­ç»ƒï¼Œæ¶ˆé™¤äº†å¯¹æ˜¾å¼RGBè¾“å…¥çš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒä¼ æ„Ÿå™¨é«˜åº¦å’Œå¤šç§å¯¼èˆªä»»åŠ¡ä¸­å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09332",
            "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and\n  Embodiment-aware Reasoning",
            "url": "https://huggingface.co/papers/2509.09332",
            "abstract": "OmniEVA addresses spatial and embodiment gaps in multimodal large language models for embodied intelligence through a task-adaptive 3D grounding mechanism and an embodiment-aware reasoning framework, achieving state-of-the-art performance across diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible.To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io",
            "score": 3,
            "issue_id": 5853,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "65192793a84b5158",
            "authors": [
                "Yuecheng Liu",
                "Dafeng Chi",
                "Shiguang Wu",
                "Zhanguang Zhang",
                "Yuzheng Zhuang",
                "Bowen Yang",
                "He Zhu",
                "Lingfeng Zhang",
                "Pengwei Xie",
                "David Gamaliel Arcos Bravo",
                "Yingxue Zhang",
                "Jianye Hao",
                "Xingyue Quan"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09332.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#games",
                    "#3d"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "OmniEVA: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "OmniEVA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒÑ‡ĞµÑ‚Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ 3D-Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ. OmniEVA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Bridging Gaps for Smarter Embodied Intelligence",
                    "desc": "OmniEVA is a new approach that improves how large language models understand and interact with the physical world. It tackles two main problems: the Geometric Adaptability Gap, which limits models trained on 2D data from effectively handling 3D tasks, and the Embodiment Constraint Gap, where models fail to consider the real-world limitations of robots. The paper introduces a Task-Adaptive 3D Grounding mechanism that allows the model to adjust its understanding of 3D space based on the task at hand. Additionally, it presents an Embodiment-Aware Reasoning framework that ensures planning decisions are practical and achievable, leading to superior performance in various embodied tasks."
                },
                "zh": {
                    "title": "OmniEVAï¼šæå‡å…·èº«æ™ºèƒ½çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›",
                    "desc": "OmniEVA æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åœ¨å…·èº«æ™ºèƒ½ä¸­çš„ç©ºé—´å’Œå…·èº«æ€§å·®è·ã€‚å®ƒé€šè¿‡ä»»åŠ¡è‡ªé€‚åº”çš„ä¸‰ç»´å®šä½æœºåˆ¶å’Œå…·èº«æ„ŸçŸ¥æ¨ç†æ¡†æ¶ï¼Œæå‡äº†æ¨¡å‹åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡éœ€æ±‚è¿›è¡Œä¸‰ç»´ä¿¡æ¯çš„é€‰æ‹©æ€§èåˆï¼Œä»è€Œå®ç°æ›´å¥½çš„ç©ºé—´ç†è§£å’Œå†³ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniEVA åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„æ¨ç†èƒ½åŠ›å’Œçµæ´»çš„è§„åˆ’èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09254",
            "title": "Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset\n  for Panoramic X-ray Analysis",
            "url": "https://huggingface.co/papers/2509.09254",
            "abstract": "A new dataset and benchmark, MMOral, and a fine-tuned model, OralGPT, address the challenges of interpreting panoramic X-rays in dentistry, showing significant performance improvements over existing large vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large vision-language models (LVLMs) have demonstrated strong performance on general-purpose medical tasks. However, their effectiveness in specialized domains such as dentistry remains underexplored. In particular, panoramic X-rays, a widely used imaging modality in oral radiology, pose interpretative challenges due to dense anatomical structures and subtle pathological cues, which are not captured by existing medical benchmarks or instruction datasets. To this end, we introduce MMOral, the first large-scale multimodal instruction dataset and benchmark tailored for panoramic X-ray interpretation. MMOral consists of 20,563 annotated images paired with 1.3 million instruction-following instances across diverse task types, including attribute extraction, report generation, visual question answering, and image-grounded dialogue. In addition, we present MMOral-Bench, a comprehensive evaluation suite covering five key diagnostic dimensions in dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing significant limitations of current models in this domain. To promote the progress of this specific domain, we also propose OralGPT, which conducts supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated MMOral instruction dataset. Remarkably, a single epoch of SFT yields substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a 24.73% improvement. Both MMOral and OralGPT hold significant potential as a critical foundation for intelligent dentistry and enable more clinically impactful multimodal AI systems in the dental field. The dataset, model, benchmark, and evaluation suite are available at https://github.com/isbrycee/OralGPT.",
            "score": 3,
            "issue_id": 5867,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "4897a9ba7cdb2683",
            "authors": [
                "Jing Hao",
                "Yuxuan Fan",
                "Yanpeng Sun",
                "Kaixin Guo",
                "Lizhuo Lin",
                "Jinrong Yang",
                "Qi Yong H. Ai",
                "Lun M. Wong",
                "Hao Tang",
                "Kuo Feng Hung"
            ],
            "affiliations": [
                "CVTE",
                "Department of Diagnostic Radiology, The University of Hong Kong",
                "Faculty of Dentistry, The University of Hong Kong",
                "Imaging and Interventional Radiology, Faculty of Medicine, The Chinese University of Hong Kong",
                "National University of Singapore",
                "School of Computer Science, Peking University",
                "Sun Yat-sen University",
                "The Hong Kong University of Science and Technology (GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09254.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#survey",
                    "#optimization",
                    "#dataset",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "ğŸ¦·",
                "ru": {
                    "title": "OralGPT: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ MMOral, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OralGPT Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ² ÑÑ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. MMOral Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 20 563 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ 1,3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹-Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. ĞÑ†ĞµĞ½ĞºĞ° 64 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° MMOral-Bench Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. OralGPT, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Qwen2.5-VL-7B Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMOral, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 24,73%."
                },
                "en": {
                    "title": "Revolutionizing Dental Imaging with MMOral and OralGPT",
                    "desc": "This paper introduces MMOral, a new dataset and benchmark specifically designed for interpreting panoramic X-rays in dentistry, addressing the limitations of existing large vision-language models (LVLMs) in this specialized field. The dataset includes over 20,000 annotated images and 1.3 million instruction-following instances for various tasks like attribute extraction and report generation. The authors also present OralGPT, a fine-tuned model that significantly improves performance on these tasks, achieving a 24.73% increase in accuracy after supervised fine-tuning. Overall, MMOral and OralGPT aim to enhance the capabilities of AI in dental diagnostics, paving the way for more effective multimodal AI systems in oral healthcare."
                },
                "zh": {
                    "title": "æ™ºèƒ½ç‰™ç§‘çš„æ–°çªç ´ï¼šMMOralä¸OralGPT",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†MMOralå’Œä¸€ä¸ªå¾®è°ƒæ¨¡å‹OralGPTï¼Œæ—¨åœ¨è§£å†³ç‰™ç§‘å…¨æ™¯Xå…‰ç‰‡è§£è¯»ä¸­çš„æŒ‘æˆ˜ã€‚MMOralæ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹å…¨æ™¯Xå…‰è§£è¯»çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«20563å¼ æ ‡æ³¨å›¾åƒå’Œ130ä¸‡æ¡æŒ‡ä»¤å®ä¾‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¿™ä¸€é¢†åŸŸçš„è¡¨ç°æœ‰é™ï¼Œæœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡ä»…ä¸º41.45%ã€‚é€šè¿‡å¯¹Qwen2.5-VL-7Bè¿›è¡Œç›‘ç£å¾®è°ƒï¼ŒOralGPTåœ¨æ€§èƒ½ä¸Šå®ç°äº†24.73%çš„æ˜¾è‘—æå‡ï¼Œä¸ºæ™ºèƒ½ç‰™ç§‘å’Œå¤šæ¨¡æ€AIç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09313",
            "title": "Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on\n  Open & Industry Data",
            "url": "https://huggingface.co/papers/2509.09313",
            "abstract": "A recommender system integrated into CI/CD pipelines uses fine-tuned CodeBERT to detect and localize vulnerabilities in code without disrupting workflows, showing improved performance with appropriate undersampling techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep learning solutions for vulnerability detection proposed in academic research are not always accessible to developers, and their applicability in industrial settings is rarely addressed. Transferring such technologies from academia to industry presents challenges related to trustworthiness, legacy systems, limited digital literacy, and the gap between academic and industrial expertise. For deep learning in particular, performance and integration into existing workflows are additional concerns. In this work, we first evaluate the performance of CodeBERT for detecting vulnerable functions in industrial and open-source software. We analyse its cross-domain generalisation when fine-tuned on open-source data and tested on industrial data, and vice versa, also exploring strategies for handling class imbalance. Based on these results, we develop AI-DO(Automating vulnerability detection Integration for Developers' Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated recommender system that uses fine-tuned CodeBERT to detect and localise vulnerabilities during code review without disrupting workflows. Finally, we assess the tool's perceived usefulness through a survey with the company's IT professionals. Our results show that models trained on industrial data detect vulnerabilities accurately within the same domain but lose performance on open-source code, while a deep learner fine-tuned on open data, with appropriate undersampling techniques, improves the detection of vulnerabilities.",
            "score": 2,
            "issue_id": 5867,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "a5c0da74ce905048",
            "authors": [
                "Moritz Mock",
                "Thomas Forrer",
                "Barbara Russo"
            ],
            "affiliations": [
                "Faculty of Engineering, Free University of Bozen-Bolzano, Bolzano, Italy",
                "R&D Department, WÃ¼rth Phoenix, Bolzano, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09313.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#transfer_learning",
                    "#architecture",
                    "#security",
                    "#survey",
                    "#open_source",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "AI-DO: Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² CI/CD Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AI-DO - Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ² CI/CD Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ĞºĞ¾Ğ´Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CodeBERT Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº undersampling. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ CodeBERT Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ ĞŸĞ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ˜Ğ¢-ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Seamless Vulnerability Detection in CI/CD with CodeBERT",
                    "desc": "This paper presents a recommender system that integrates a fine-tuned version of CodeBERT into Continuous Integration and Continuous Deployment (CI/CD) pipelines to identify and locate vulnerabilities in code. The study evaluates CodeBERT's performance in detecting vulnerable functions across both industrial and open-source software, highlighting the challenges of transferring deep learning solutions from academia to industry. It also addresses issues like class imbalance and the need for seamless integration into existing workflows. The findings indicate that while models trained on industrial data excel in their domain, those fine-tuned on open-source data, when combined with undersampling techniques, enhance vulnerability detection capabilities."
                },
                "zh": {
                    "title": "æ™ºèƒ½æ¨èï¼Œå®‰å…¨æ— å¿§ï¼",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é›†æˆåœ¨æŒç»­é›†æˆ/æŒç»­éƒ¨ç½²ï¼ˆCI/CDï¼‰ç®¡é“ä¸­çš„æ¨èç³»ç»Ÿï¼Œåˆ©ç”¨å¾®è°ƒåçš„CodeBERTæ¥æ£€æµ‹å’Œå®šä½ä»£ç ä¸­çš„æ¼æ´ï¼Œè€Œä¸ä¼šå¹²æ‰°å·¥ä½œæµç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨é€‚å½“çš„æ¬ é‡‡æ ·æŠ€æœ¯å¯ä»¥æé«˜æ¼æ´æ£€æµ‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¯„ä¼°äº†CodeBERTåœ¨å·¥ä¸šå’Œå¼€æºè½¯ä»¶ä¸­æ£€æµ‹è„†å¼±å‡½æ•°çš„è¡¨ç°ï¼Œå¹¶åˆ†æäº†å…¶è·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¼€å‘äº†AI-DOå·¥å…·ï¼Œé€šè¿‡è°ƒæŸ¥è¯„ä¼°å…¶åœ¨ITä¸“ä¸šäººå‘˜ä¸­çš„å®ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.09114",
            "title": "Modality Alignment with Multi-scale Bilateral Attention for Multimodal\n  Recommendation",
            "url": "https://huggingface.co/papers/2509.09114",
            "abstract": "MambaRec enhances multimodal recommendation systems by integrating local feature alignment and global distribution regularization to improve cross-modal fusion and reduce representational bias.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal recommendation systems are increasingly becoming foundational technologies for e-commerce and content platforms, enabling personalized services by jointly modeling users' historical behaviors and the multimodal features of items (e.g., visual and textual). However, most existing methods rely on either static fusion strategies or graph-based local interaction modeling, facing two critical limitations: (1) insufficient ability to model fine-grained cross-modal associations, leading to suboptimal fusion quality; and (2) a lack of global distribution-level consistency, causing representational bias. To address these, we propose MambaRec, a novel framework that integrates local feature alignment and global distribution regularization via attention-guided learning. At its core, we introduce the Dilated Refinement Attention Module (DREAM), which uses multi-scale dilated convolutions with channel-wise and spatial attention to align fine-grained semantic patterns between visual and textual modalities. This module captures hierarchical relationships and context-aware associations, improving cross-modal semantic modeling. Additionally, we apply Maximum Mean Discrepancy (MMD) and contrastive loss functions to constrain global modality alignment, enhancing semantic consistency. This dual regularization reduces mode-specific deviations and boosts robustness. To improve scalability, MambaRec employs a dimensionality reduction strategy to lower the computational cost of high-dimensional multimodal features. Extensive experiments on real-world e-commerce datasets show that MambaRec outperforms existing methods in fusion quality, generalization, and efficiency. Our code has been made publicly available at https://github.com/rkl71/MambaRec.",
            "score": 2,
            "issue_id": 5862,
            "pub_date": "2025-09-11",
            "pub_date_card": {
                "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 11",
                "zh": "9æœˆ11æ—¥"
            },
            "hash": "7b4a6bd1f51ddfc9",
            "authors": [
                "Kelin Ren",
                "Chan-Yang Ju",
                "Dong-Ho Lee"
            ],
            "affiliations": [
                "Department of Applied Artificial Intelligence Hanyang University Ansan, Republic of Korea",
                "Department of Computer Science and Engineering Hanyang University Ansan, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.09114.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#open_source",
                    "#games",
                    "#optimization"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "MambaRec: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "MambaRec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²ĞµÑ€Ñ‚ĞºĞ°Ğ¼Ğ¸ (DREAM) Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. MambaRec Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (MMD) Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MambaRec Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "MambaRec: Bridging Modalities for Better Recommendations",
                    "desc": "MambaRec is a new framework designed to enhance multimodal recommendation systems by improving how different types of data, like images and text, work together. It addresses two main issues: the inability to effectively model detailed relationships between different data types and the inconsistency in overall data representation. The framework uses a special module called DREAM, which employs advanced techniques to align and refine the features from different modalities. By applying regularization methods, MambaRec ensures that the recommendations are more accurate and reliable, leading to better performance in real-world applications."
                },
                "zh": {
                    "title": "MambaRecï¼šæå‡å¤šæ¨¡æ€æ¨èçš„æ™ºèƒ½èåˆ",
                    "desc": "MambaRec æ˜¯ä¸€ç§å¢å¼ºå¤šæ¨¡æ€æ¨èç³»ç»Ÿçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå±€éƒ¨ç‰¹å¾å¯¹é½å’Œå…¨å±€åˆ†å¸ƒæ­£åˆ™åŒ–æ¥æ”¹å–„è·¨æ¨¡æ€èåˆï¼Œå‡å°‘è¡¨ç¤ºåå·®ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ‰©å¼ ç²¾ç‚¼æ³¨æ„åŠ›æ¨¡å—ï¼ˆDREAMï¼‰ï¼Œåˆ©ç”¨å¤šå°ºåº¦æ‰©å¼ å·ç§¯å’Œé€šé“ã€ç©ºé—´æ³¨æ„åŠ›å¯¹è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„ç»†ç²’åº¦è¯­ä¹‰æ¨¡å¼è¿›è¡Œå¯¹é½ã€‚é€šè¿‡æœ€å¤§å‡å€¼å·®å¼‚ï¼ˆMMDï¼‰å’Œå¯¹æ¯”æŸå¤±å‡½æ•°ï¼ŒMambaRec è¿˜å¢å¼ºäº†å…¨å±€æ¨¡æ€å¯¹é½çš„ä¸€è‡´æ€§ï¼Œæé«˜äº†è¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMambaRec åœ¨èåˆè´¨é‡ã€æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07430",
            "title": "The Choice of Divergence: A Neglected Key to Mitigating Diversity\n  Collapse in Reinforcement Learning with Verifiable Reward",
            "url": "https://huggingface.co/papers/2509.07430",
            "abstract": "A new framework, DPH-RL, uses mass-covering f-divergences to address Pass@k degradation and catastrophic forgetting in fine-tuning LLMs with RLVR, improving both Pass@1 and Pass@k.  \t\t\t\t\tAI-generated summary \t\t\t\t A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. While various methods have been proposed, the choice and function of the divergence term have been surprisingly unexamined as a proactive solution. We argue that standard RLVR objectives -- both those using the mode-seeking reverse KL-divergence and those forgoing a divergence term entirely -- lack a crucial mechanism for knowledge retention. The reverse-KL actively accelerates this decay by narrowing the policy, while its absence provides no safeguard against the model drifting from its diverse knowledge base. We propose a fundamental shift in perspective: using the divergence term itself as the solution. Our framework, Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences (like forward-KL and JS-divergence) to function as a rehearsal mechanism. By continuously referencing the initial policy, this approach forces the model to maintain broad solution coverage. Extensive experiments on math and SQL generation demonstrate that DPH-RL not only resolves the Pass@k degradation but improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is more training-efficient because it computes f-divergence using generator functions, requiring only sampling from the initial policy and no online reference model. Our work highlights a crucial, overlooked axis for improving RLVR, demonstrating that the proper selection of a divergence measure is a powerful tool for building more general and diverse reasoning models.",
            "score": 2,
            "issue_id": 5854,
            "pub_date": "2025-09-09",
            "pub_date_card": {
                "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 9",
                "zh": "9æœˆ9æ—¥"
            },
            "hash": "1d2012ade6917cee",
            "authors": [
                "Long Li",
                "Jiaran Hao",
                "Jason Klein Liu",
                "Zhijian Zhou",
                "Xiaoyu Tan",
                "Wei Chu",
                "Zhe Wang",
                "Shirui Pan",
                "Chao Qu",
                "Yuan Qi"
            ],
            "affiliations": [
                "Fudan University",
                "Griffith University",
                "INFLY TECH"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07430.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº DPH-RL Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. DPH-RL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ f-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ±Ñ‹Ğ²Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DPH-RL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº Pass@1, Ñ‚Ğ°Ğº Ğ¸ Pass@k Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸Ğ· Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Preserving Knowledge in LLMs with DPH-RL",
                    "desc": "The paper introduces a new framework called Diversity-Preserving Hybrid Reinforcement Learning (DPH-RL) to tackle issues in fine-tuning Large Language Models (LLMs) using Reinforcement Learning with Verifiable Reward (RLVR). It addresses the problem of Pass@k degradation and catastrophic forgetting by utilizing mass-covering f-divergences, which help maintain a diverse knowledge base during training. The authors argue that traditional divergence measures, like reverse KL-divergence, can lead to knowledge loss, while DPH-RL promotes knowledge retention by continuously referencing the initial policy. Experimental results show that DPH-RL improves both single-attempt accuracy (Pass@1) and multi-attempt performance (Pass@k), making it a more efficient and effective approach for training LLMs."
                },
                "zh": {
                    "title": "åˆ©ç”¨f-æ•£åº¦æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæ•ˆæœ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶DPH-RLï¼Œåˆ©ç”¨è´¨é‡è¦†ç›–çš„f-æ•£åº¦æ¥è§£å†³åœ¨ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶å‡ºç°çš„Pass@kæ€§èƒ½ä¸‹é™å’Œç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ä¼ ç»Ÿçš„RLVRç›®æ ‡å¾€å¾€å¿½è§†äº†æ•£åº¦é¡¹çš„é€‰æ‹©å’ŒåŠŸèƒ½ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ä¿æŒçŸ¥è¯†æ–¹é¢ç¼ºä¹æœ‰æ•ˆæœºåˆ¶ã€‚DPH-RLé€šè¿‡ä½¿ç”¨å‰å‘KLæ•£åº¦å’ŒJSæ•£åº¦ç­‰è´¨é‡è¦†ç›–çš„f-æ•£åº¦ï¼Œä½œä¸ºä¸€ç§æ’ç»ƒæœºåˆ¶ï¼Œå¸®åŠ©æ¨¡å‹ä¿æŒå¹¿æ³›çš„è§£å†³æ–¹æ¡ˆè¦†ç›–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDPH-RLä¸ä»…è§£å†³äº†Pass@kçš„ä¸‹é™é—®é¢˜ï¼Œè¿˜åœ¨é¢†åŸŸå†…å¤–åŒæ—¶æé«˜äº†Pass@1å’ŒPass@kçš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.07225",
            "title": "All You Need Is A Fuzzing Brain: An LLM-Powered System for Automated\n  Vulnerability Detection and Patching",
            "url": "https://huggingface.co/papers/2509.07225",
            "abstract": "A Cyber Reasoning System using LLMs autonomously discovered and patched security vulnerabilities in open-source projects, with a public leaderboard for benchmarking LLMs on these tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Our team, All You Need Is A Fuzzing Brain, was one of seven finalists in DARPA's Artificial Intelligence Cyber Challenge (AIxCC), placing fourth in the final round. During the competition, we developed a Cyber Reasoning System (CRS) that autonomously discovered 28 security vulnerabilities - including six previously unknown zero-days - in real-world open-source C and Java projects, and successfully patched 14 of them. The complete CRS is open source at https://github.com/o2lab/afc-crs-all-you-need-is-a-fuzzing-brain. This paper provides a detailed technical description of our CRS, with an emphasis on its LLM-powered components and strategies. Building on AIxCC, we further introduce a public leaderboard for benchmarking state-of-the-art LLMs on vulnerability detection and patching tasks, derived from the AIxCC dataset. The leaderboard is available at https://o2lab.github.io/FuzzingBrain-Leaderboard/.",
            "score": 2,
            "issue_id": 5867,
            "pub_date": "2025-09-08",
            "pub_date_card": {
                "ru": "8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 8",
                "zh": "9æœˆ8æ—¥"
            },
            "hash": "1cf7418f34a5be45",
            "authors": [
                "Ze Sheng",
                "Qingxiao Xu",
                "Jianwei Huang",
                "Matthew Woodcock",
                "Heqing Huang",
                "Alastair F. Donaldson",
                "Guofei Gu",
                "Jeff Huang"
            ],
            "affiliations": [
                "City University of Hong Kong Hong Kong, China",
                "Imperial College London London, UK",
                "Texas A&M University College Station, US"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.07225.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#agents"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ ĞºĞ¾Ğ´Ğ°: Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ñƒ ĞšĞ¸Ğ±ĞµÑ€Ğ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (Ğ¡ĞšĞ ), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ ĞœĞ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. Ğ¡ĞšĞ  ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ° 28 ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 6 Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ½ĞµĞµ, Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ğ½Ğ° C Ğ¸ Java, Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° 14 Ğ¸Ğ· Ğ½Ğ¸Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ°Ğ½ÑĞ»Ğ° 4-Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ² ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ DARPA Ğ¿Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ Ğ² ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ»Ğ¸Ğ´ĞµÑ€Ğ±Ğ¾Ñ€Ğ´ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Empowering Cybersecurity with Autonomous LLMs",
                    "desc": "This paper presents a Cyber Reasoning System (CRS) that utilizes large language models (LLMs) to autonomously identify and fix security vulnerabilities in open-source software. The CRS successfully discovered 28 vulnerabilities, including six zero-day exploits, and patched 14 of them during the DARPA AIxCC competition. The authors also introduce a public leaderboard for evaluating the performance of various LLMs on tasks related to vulnerability detection and patching. This work highlights the potential of LLMs in enhancing software security through automated reasoning and patching capabilities."
                },
                "zh": {
                    "title": "è‡ªä¸»å‘ç°ä¸ä¿®å¤å®‰å…¨æ¼æ´çš„æ™ºèƒ½ç³»ç»Ÿ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç½‘ç»œæ¨ç†ç³»ç»Ÿï¼ˆCRSï¼‰ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿè‡ªä¸»å‘ç°å’Œä¿®å¤å¼€æºé¡¹ç›®ä¸­çš„å®‰å…¨æ¼æ´ã€‚åœ¨DARPAçš„äººå·¥æ™ºèƒ½ç½‘ç»œæŒ‘æˆ˜èµ›ä¸­ï¼Œæˆ‘ä»¬çš„å›¢é˜ŸæˆåŠŸå‘ç°äº†28ä¸ªå®‰å…¨æ¼æ´ï¼Œå…¶ä¸­åŒ…æ‹¬6ä¸ªä¹‹å‰æœªçŸ¥çš„é›¶æ—¥æ¼æ´ï¼Œå¹¶æˆåŠŸä¿®å¤äº†14ä¸ªæ¼æ´ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†ä¸€ä¸ªå…¬å…±æ’è¡Œæ¦œï¼Œç”¨äºè¯„ä¼°æœ€æ–°çš„LLMåœ¨æ¼æ´æ£€æµ‹å’Œä¿®å¤ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥ç³»ç»Ÿçš„å®Œæ•´ä»£ç æ˜¯å¼€æºçš„ï¼Œä¾›ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…ä½¿ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.05739",
            "title": "Reasoning Introduces New Poisoning Attacks Yet Makes Them More\n  Complicated",
            "url": "https://huggingface.co/papers/2509.05739",
            "abstract": "Data poisoning attacks on Large Language Models can target the reasoning process by decomposing triggers, but the models' reasoning capabilities and architectural design provide a form of backdoor robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Early research into data poisoning attacks against Large Language Models (LLMs) demonstrated the ease with which backdoors could be injected. More recent LLMs add step-by-step reasoning, expanding the attack surface to include the intermediate chain-of-thought (CoT) and its inherent trait of decomposing problems into subproblems. Using these vectors for more stealthy poisoning, we introduce ``decomposed reasoning poison'', in which the attacker modifies only the reasoning path, leaving prompts and final answers clean, and splits the trigger across multiple, individually harmless components.   Fascinatingly, while it remains possible to inject these decomposed poisons, reliably activating them to change final answers (rather than just the CoT) is surprisingly difficult. This difficulty arises because the models can often recover from backdoors that are activated within their thought processes. Ultimately, it appears that an emergent form of backdoor robustness is originating from the reasoning capabilities of these advanced LLMs, as well as from the architectural separation between reasoning and final answer generation.",
            "score": 1,
            "issue_id": 5859,
            "pub_date": "2025-09-06",
            "pub_date_card": {
                "ru": "6 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 6",
                "zh": "9æœˆ6æ—¥"
            },
            "hash": "bc4e67f715e40467",
            "authors": [
                "Hanna Foerster",
                "Ilia Shumailov",
                "Yiren Zhao",
                "Harsh Chaudhari",
                "Jamie Hayes",
                "Robert Mullins",
                "Yarin Gal"
            ],
            "affiliations": [
                "Anthropic",
                "DeepMind",
                "OpenAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.05739.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#security",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', Ğ³Ğ´Ğµ Ğ°Ñ‚Ğ°ĞºÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿ÑƒÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¼Ğ¸. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¸Ñ… Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ±ÑĞºĞ´Ğ¾Ñ€Ğ°Ğ¼."
                },
                "en": {
                    "title": "Enhancing Backdoor Robustness in LLMs through Reasoning",
                    "desc": "This paper discusses data poisoning attacks on Large Language Models (LLMs) that specifically target their reasoning processes. It introduces a novel method called 'decomposed reasoning poison', where attackers modify the reasoning steps without altering the prompts or final outputs. Despite the potential for these stealthy attacks, the paper finds that activating such backdoors to influence final answers is challenging. This is due to the models' inherent robustness, which stems from their advanced reasoning capabilities and the structural separation between reasoning and answer generation."
                },
                "zh": {
                    "title": "æ¨ç†èƒ½åŠ›æå‡åé—¨é²æ£’æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°æ®ä¸­æ¯’æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡åˆ†è§£è§¦å‘å™¨æ¥å½±å“æ¨ç†è¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å¯ä»¥æ³¨å…¥è¿™äº›åˆ†è§£çš„æ¯’ç´ ï¼Œä½†è¦å¯é åœ°æ¿€æ´»å®ƒä»¬ä»¥æ”¹å˜æœ€ç»ˆç­”æ¡ˆå´éå¸¸å›°éš¾ã€‚è¿™ç§å›°éš¾æºäºæ¨¡å‹åœ¨å…¶æ€ç»´è¿‡ç¨‹ä¸­èƒ½å¤Ÿä»åé—¨ä¸­æ¢å¤ã€‚æœ€ç»ˆï¼Œç ”ç©¶å‘ç°ï¼Œå…ˆè¿›çš„LLMsçš„æ¨ç†èƒ½åŠ›å’Œæ¨ç†ä¸æœ€ç»ˆç­”æ¡ˆç”Ÿæˆä¹‹é—´çš„æ¶æ„åˆ†ç¦»ï¼Œå½¢æˆäº†ä¸€ç§æ–°å…´çš„åé—¨é²æ£’æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-12.html",
    "link_next": "2025-09-16.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "12.09",
        "en": "09/12",
        "zh": "9æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "16.09",
        "en": "09/16",
        "zh": "9æœˆ16æ—¥"
    },
    "categories": {
        "#dataset": 11,
        "#data": 3,
        "#benchmark": 13,
        "#agents": 6,
        "#cv": 1,
        "#rl": 6,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 4,
        "#audio": 2,
        "#video": 4,
        "#multimodal": 9,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 10,
        "#robotics": 2,
        "#agi": 1,
        "#games": 5,
        "#interpretability": 2,
        "#reasoning": 9,
        "#transfer_learning": 3,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 3,
        "#optimization": 11,
        "#survey": 2,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    }
}