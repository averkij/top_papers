
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 29 papers. June 17.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">17 июня</span> | <span id="title-articles-count">29 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-16.html">⬅️ <span id="prev-date">16.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-18.html">➡️ <span id="next-date">18.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'};
        let feedDateNext = {'ru': '18.06', 'en': '06/18', 'zh': '6月18日'};
        let feedDatePrev = {'ru': '16.06', 'en': '06/16', 'zh': '6月16日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.13585', 'title': 'MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention', 'url': 'https://huggingface.co/papers/2506.13585', 'abstract': "A hybrid-attention reasoning model called MiniMax-M1, featuring a Mixture-of-Experts architecture and lightning attention mechanism, is introduced for efficient long-input processing and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.", 'score': 168, 'issue_id': 4324, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '05163c188bd37051', 'authors': ['MiniMax', ':', 'Aili Chen', 'Aonian Li', 'Bangwei Gong', 'Binyang Jiang', 'Bo Fei', 'Bo Yang', 'Boji Shan', 'Changqing Yu', 'Chao Wang', 'Cheng Zhu', 'Chengjun Xiao', 'Chengyu Du', 'Chi Zhang', 'Chu Qiao', 'Chunhao Zhang', 'Chunhui Du', 'Congchao Guo', 'Da Chen', 'Deming Ding', 'Dianjun Sun', 'Dong Li', 'Enwei Jiao', 'Haigang Zhou', 'Haimo Zhang', 'Han Ding', 'Haohai Sun', 'Haoyu Feng', 'Huaiguang Cai', 'Haichao Zhu', 'Jian Sun', 'Jiaqi Zhuang', 'Jiaren Cai', 'Jiayuan Song', 'Jin Zhu', 'Jingyang Li', 'Jinhao Tian', 'Jinli Liu', 'Junhao Xu', 'Junjie Yan', 'Junteng Liu', 'Junxian He', 'Kaiyi Feng', 'Ke Yang', 'Kecheng Xiao', 'Le Han', 'Leyang Wang', 'Lianfei Yu', 'Liheng Feng', 'Lin Li', 'Lin Zheng', 'Linge Du', 'Lingyu Yang', 'Lunbin Zeng', 'Minghui Yu', 'Mingliang Tao', 'Mingyuan Chi', 'Mozhi Zhang', 'Mujie Lin', 'Nan Hu', 'Nongyu Di', 'Peng Gao', 'Pengfei Li', 'Pengyu Zhao', 'Qibing Ren', 'Qidi Xu', 'Qile Li', 'Qin Wang', 'Rong Tian', 'Ruitao Leng', 'Shaoxiang Chen', 'Shaoyu Chen', 'Shengmin Shi', 'Shitong Weng', 'Shuchang Guan', 'Shuqi Yu', 'Sichen Li', 'Songquan Zhu', 'Tengfei Li', 'Tianchi Cai', 'Tianrun Liang', 'Weiyu Cheng', 'Weize Kong', 'Wenkai Li', 'Xiancai Chen', 'Xiangjun Song', 'Xiao Luo', 'Xiao Su', 'Xiaobo Li', 'Xiaodong Han', 'Xinzhu Hou', 'Xuan Lu', 'Xun Zou', 'Xuyang Shen', 'Yan Gong', 'Yan Ma', 'Yang Wang', 'Yiqi Shi', 'Yiran Zhong', 'Yonghong Duan', 'Yongxiang Fu', 'Yongyi Hu', 'Yu Gao', 'Yuanxiang Fan', 'Yufeng Yang', 'Yuhao Li', 'Yulin Hu', 'Yunan Huang', 'Yunji Li', 'Yunzhi Xu', 'Yuxin Mao', 'Yuxuan Shi', 'Yuze Wenren', 'Zehan Li', 'Zelin Li', 'Zhanxu Tian', 'Zhengmao Zhu', 'Zhenhua Fan', 'Zhenzhen Wu', 'Zhichao Xu', 'Zhihang Yu', 'Zhiheng Lyu', 'Zhuo Jiang', 'Zibo Gao', 'Zijia Wu', 'Zijian Song', 'Zijun Sun'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.13585.jpg', 'data': {'categories': ['#rl', '#training', '#open_source', '#architecture', '#reasoning', '#long_context', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'MiniMax-M1: Гибридный ИИ для эффективной обработки сложных задач', 'desc': 'MiniMax-M1 - это гибридная модель рассуждений с архитектурой Mixture-of-Experts и механизмом молниеносного внимания. Модель поддерживает контекст длиной 1 миллион токенов и эффективно обрабатывает длинные входные данные. MiniMax-M1 обучена с помощью масштабного обучения с подкреплением на разнообразных задачах, включая реальные среды разработки программного обеспечения. Эксперименты показывают, что модель превосходит аналоги в сложных задачах программирования, использовании инструментов и работе с длинным контекстом.'}, 'en': {'title': 'Revolutionizing Long-Input Processing with MiniMax-M1', 'desc': 'MiniMax-M1 is a groundbreaking hybrid-attention reasoning model that utilizes a Mixture-of-Experts architecture and a lightning attention mechanism to efficiently handle long input sequences. With 456 billion parameters and the ability to process up to 1 million tokens, it significantly outperforms previous models in context length. The model is trained using a novel reinforcement learning algorithm called CISPO, which enhances training efficiency by clipping importance sampling weights. MiniMax-M1 demonstrates superior performance in complex tasks, particularly in software engineering and long-context applications, making it a valuable tool for various AI challenges.'}, 'zh': {'title': '高效长输入处理的混合注意力模型', 'desc': 'MiniMax-M1是一种混合注意力推理模型，采用混合专家架构和闪电注意力机制，旨在高效处理长输入和强化学习任务。该模型基于之前的MiniMax-Text-01模型，具有4560亿个参数，并支持高达100万个token的上下文长度。MiniMax-M1在复杂任务中表现出色，尤其是在软件工程和工具利用方面。我们还提出了一种新颖的强化学习算法CISPO，进一步提高了训练效率。'}}}, {'id': 'https://huggingface.co/papers/2506.10521', 'title': "Scientists' First Exam: Probing Cognitive Abilities of MLLM via\n  Perception, Understanding, and Reasoning", 'url': 'https://huggingface.co/papers/2506.10521', 'abstract': "Scientists' First Exam (SFE) benchmark assesses scientific cognitive capacities of Multimodal Large Language Models through perception, understanding, and comparative reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries.", 'score': 53, 'issue_id': 4325, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '3e2672b026127b5e', 'authors': ['Yuhao Zhou', 'Yiheng Wang', 'Xuming He', 'Ruoyao Xiao', 'Zhiwei Li', 'Qiantai Feng', 'Zijie Guo', 'Yuejin Yang', 'Hao Wu', 'Wenxuan Huang', 'Jiaqi Wei', 'Dan Si', 'Xiuqi Yao', 'Jia Bu', 'Haiwen Huang', 'Tianfan Fu', 'Shixiang Tang', 'Ben Fei', 'Dongzhan Zhou', 'Fenghua Ling', 'Yan Lu', 'Siqi Sun', 'Chenhui Li', 'Guanjie Zheng', 'Jiancheng Lv', 'Wenlong Zhang', 'Lei Bai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.10521.jpg', 'data': {'categories': ['#reasoning', '#science', '#multimodal', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Новый бенчмарк для оценки научного мышления ИИ', 'desc': "Учёные разработали новый бенчмарк под названием Scientists' First Exam (SFE) для оценки научных когнитивных способностей мультимодальных больших языковых моделей (MLLM). SFE оценивает восприятие, понимание и сравнительное рассуждение в научном контексте. Бенчмарк состоит из 830 экспертно проверенных пар вопросов-ответов по визуальным данным, охватывающих 66 мультимодальных задач в пяти важных научных дисциплинах. Эксперименты показали, что современные модели GPT-4 и InternVL-3 достигают лишь 34.08% и 26.52% соответственно на SFE, что указывает на значительный потенциал для улучшения MLLM в научной сфере."}, 'en': {'title': 'Enhancing Scientific Discovery with MLLMs: The SFE Benchmark', 'desc': "The Scientists' First Exam (SFE) benchmark evaluates the cognitive abilities of Multimodal Large Language Models (MLLMs) in scientific contexts. It focuses on three key areas: perception of scientific signals, understanding of scientific attributes, and comparative reasoning. The benchmark includes 830 expert-verified visual question-answering pairs across various multimodal tasks in five important scientific disciplines. Results show that leading models like GPT-o3 and InternVL-3 perform below expectations, indicating a need for improvement in their scientific reasoning capabilities."}, 'zh': {'title': '科学认知能力的新评估标准', 'desc': '科学家首次考试（SFE）基准测试评估多模态大型语言模型（MLLMs）的科学认知能力，主要通过感知、理解和比较推理三个方面进行评估。当前的科学基准主要关注MLLMs的知识理解能力，未能充分评估其感知和推理能力。SFE基准包含830个经过专家验证的视觉问答对，涵盖66个多模态任务，涉及五个高价值学科。实验结果显示，现有的最先进模型在SFE上的表现仍有很大提升空间，表明MLLMs在科学领域的应用潜力巨大。'}}}, {'id': 'https://huggingface.co/papers/2506.11763', 'title': 'DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents', 'url': 'https://huggingface.co/papers/2506.11763', 'abstract': "DeepResearch Bench offers a benchmark framework to evaluate the capabilities of Deep Research Agents in terms of research quality and information retrieval accuracy across multiple fields.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.", 'score': 35, 'issue_id': 4324, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '197213635094ee83', 'authors': ['Mingxuan Du', 'Benfeng Xu', 'Chiwei Zhu', 'Xiaorui Wang', 'Zhendong Mao'], 'affiliations': ['MetastoneTechnology, Beijing, China', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.11763.jpg', 'data': {'categories': ['#open_source', '#science', '#agents', '#alignment', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Комплексная оценка ИИ-агентов для глубоких исследований', 'desc': 'DeepResearch Bench - это система оценки возможностей агентов глубоких исследований в области качества исследований и точности поиска информации. Бенчмарк включает 100 исследовательских задач уровня PhD по 22 различным областям, разработанных экспертами. Предложены две новые методологии оценки, хорошо согласующиеся с человеческим суждением: метод на основе эталонов и оценка возможностей поиска информации. DeepResearch Bench доступен в открытом доступе для ускорения разработки практических агентов на основе языковых моделей.'}, 'en': {'title': 'Benchmarking Deep Research Agents for Superior Research Quality', 'desc': "DeepResearch Bench is a new framework designed to evaluate the performance of Deep Research Agents (DRAs) in generating high-quality research outputs. It includes 100 PhD-level tasks across 22 fields, allowing for a comprehensive assessment of DRAs' capabilities in information retrieval and report synthesis. The framework introduces two innovative evaluation methods: one that uses reference-based criteria to judge report quality, and another that measures citation effectiveness and accuracy. By providing these tools, DeepResearch Bench aims to enhance the development of LLM-based agents and improve their research efficiency."}, 'zh': {'title': '评估深度研究代理的新基准', 'desc': 'DeepResearch Bench是一个基准框架，用于评估深度研究代理在研究质量和信息检索准确性方面的能力。该框架包含100个由领域专家精心设计的博士级研究任务，涵盖22个不同领域。我们提出了两种新方法来评估这些代理的能力，一种是基于参考的评估方法，另一种是评估信息检索和引用准确性的框架。通过开源DeepResearch Bench及其关键组件，我们希望加速基于大型语言模型的代理的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.12571', 'title': 'DoTA-RAG: Dynamic of Thought Aggregation RAG', 'url': 'https://huggingface.co/papers/2506.12571', 'abstract': "DoTA-RAG improves retrieval and generation accuracy over massive web datasets using a dynamic routing pipeline and optimized embedding models, achieving high correctness scores while maintaining low latency.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a retrieval-augmented generation system optimized for high-throughput, large-scale web knowledge indexes. Traditional RAG pipelines often suffer from high latency and limited accuracy over massive, diverse datasets. DoTA-RAG addresses these challenges with a three-stage pipeline: query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking. We further enhance retrieval by evaluating and selecting a superior embedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we create a diverse Q&A dataset of 500 questions generated via the DataMorgana setup across a broad range of WebOrganizer topics and formats. DoTA-RAG improves the answer correctness score from 0.752 (baseline, using LiveRAG pre-built vector store) to 1.478 while maintaining low latency, and it achieves a 0.929 correctness score on the Live Challenge Day. These results highlight DoTA-RAG's potential for practical deployment in domains requiring fast, reliable access to large and evolving knowledge sources.", 'score': 28, 'issue_id': 4332, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': '3676dd66819fb868', 'authors': ['Saksorn Ruangtanusak', 'Natthapath Rungseesiripak', 'Peerawat Rojratchadakorn', 'Monthol Charattrakool', 'Natapong Nitarach'], 'affiliations': ['SCB 10X Bangkok, Thailand', 'SCBX Bangkok, Thailand'], 'pdf_title_img': 'assets/pdf/title_img/2506.12571.jpg', 'data': {'categories': ['#optimization', '#survey', '#data', '#dataset', '#rag'], 'emoji': '🌐', 'ru': {'title': 'DoTA-RAG: Динамическая оптимизация поиска и генерации для масштабных веб-знаний', 'desc': 'В статье представлена система DoTA-RAG, улучшающая точность поиска и генерации в больших веб-датасетах. Система использует динамическую маршрутизацию и оптимизированные модели эмбеддингов. DoTA-RAG включает трехэтапный конвейер: переписывание запросов, динамическую маршрутизацию и многоступенчатый поиск. Система значительно повышает оценку корректности ответов при сохранении низкой задержки.'}, 'en': {'title': 'Boosting Retrieval and Generation with DoTA-RAG!', 'desc': 'DoTA-RAG is a new system designed to improve how we retrieve and generate information from large web datasets. It uses a three-stage process that includes rewriting queries, dynamically routing them to specialized sub-indexes, and retrieving and ranking results in multiple stages. By optimizing the embedding models and re-embedding a large dataset, DoTA-RAG significantly increases the accuracy of answers while keeping response times low. This makes it a promising tool for applications that need quick and reliable access to vast amounts of information.'}, 'zh': {'title': 'DoTA-RAG：快速可靠的知识检索与生成系统', 'desc': '本文介绍了DoTA-RAG（动态思维聚合RAG），这是一种优化的检索增强生成系统，旨在处理大规模网络知识索引。传统的RAG管道在处理庞大多样的数据集时，常常面临高延迟和准确性不足的问题。DoTA-RAG通过查询重写、动态路由到专业子索引以及多阶段检索和排名的三阶段管道来解决这些挑战。该系统在FineWeb-10BT语料库上重新嵌入并选择了优越的嵌入模型，从而显著提高了答案的正确性分数，同时保持了低延迟。'}}}, {'id': 'https://huggingface.co/papers/2506.13654', 'title': 'Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning', 'url': 'https://huggingface.co/papers/2506.13654', 'abstract': 'Ego-R1, a reinforcement learning-based framework, uses a structured tool-augmented chain-of-thought process to reason over ultra-long egocentric videos, achieving better performance than existing methods by extending time coverage to a week.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e., in days and weeks) egocentric videos, which leverages a structured Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained via reinforcement learning (RL). Inspired by human problem-solving strategies, CoTT decomposes complex reasoning into modular steps, with the RL agent invoking specific tools, one per step, to iteratively and collaboratively answer sub-questions tackling such tasks as temporal retrieval and multi-modal understanding. We design a two-stage training paradigm involving supervised finetuning (SFT) of a pretrained language model using CoTT data and RL to enable our agent to dynamically propose step-by-step tools for long-range reasoning. To facilitate training, we construct a dataset called Ego-R1 Data, which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark, Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources. Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of understanding ultra-long egocentric videos, significantly extending the time coverage from few hours to a week.', 'score': 25, 'issue_id': 4326, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '52b4ddc8a62e646b', 'authors': ['Shulin Tian', 'Ruiqi Wang', 'Hongming Guo', 'Penghao Wu', 'Yuhao Dong', 'Xiuying Wang', 'Jingkang Yang', 'Hao Zhang', 'Hongyuan Zhu', 'Ziwei Liu'], 'affiliations': ['A*STAR, Singapore', 'S-Lab, Nanyang Technological University', 'Shanghai AI Lab', 'Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13654.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#training', '#multimodal', '#long_context', '#rl'], 'emoji': '🎥', 'ru': {'title': 'Ego-R1: Революция в анализе длительных эгоцентрических видео', 'desc': 'Ego-R1 - это новая система обработки сверхдлинных эгоцентрических видео, использующая структурированный процесс цепочки рассуждений с инструментами (Chain-of-Tool-Thought). Система применяет агента, обученного с помощью обучения с подкреплением, для декомпозиции сложных задач на модульные шаги. Ego-R1 использует двухэтапную парадигму обучения, включающую тонкую настройку предобученной языковой модели и обучение с подкреплением. Система значительно расширяет временной охват анализа видео с нескольких часов до недели, превосходя существующие методы.'}, 'en': {'title': 'Revolutionizing Video Understanding with Ego-R1', 'desc': 'Ego-R1 is a new framework designed to analyze ultra-long egocentric videos, which can last for days or even weeks. It employs a structured Chain-of-Tool-Thought (CoTT) process, allowing the system to break down complex reasoning tasks into manageable steps. The framework is powered by a reinforcement learning agent that learns to select and use specific tools for each step, enhancing its ability to answer questions about the video content. By training on a specially created dataset and evaluating on a new benchmark, Ego-R1 demonstrates improved performance in understanding long-duration videos compared to existing methods.'}, 'zh': {'title': 'Ego-R1：超长视频推理的新突破', 'desc': 'Ego-R1是一个基于强化学习的框架，旨在处理超长的自我中心视频。它采用了一种结构化的工具增强思维链（CoTT）过程，将复杂的推理分解为模块化步骤。通过强化学习训练的Ego-R1代理能够动态地提出逐步工具，以应对长时间范围内的推理任务。实验结果表明，Ego-R1在理解超长视频方面表现优异，时间覆盖范围从几小时扩展到一周。'}}}, {'id': 'https://huggingface.co/papers/2506.08343', 'title': 'Wait, We Don\'t Need to "Wait"! Removing Thinking Tokens Improves\n  Reasoning Efficiency', 'url': 'https://huggingface.co/papers/2506.08343', 'abstract': 'NoWait suppresses explicit self-reflection tokens during inference to enhance efficiency in multimodal reasoning without reducing model utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning.', 'score': 24, 'issue_id': 4324, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'bc2b3e7cb2a8d002', 'authors': ['Chenlong Wang', 'Yuanning Feng', 'Dongping Chen', 'Zhaoyang Chu', 'Ranjay Krishna', 'Tianyi Zhou'], 'affiliations': ['University College London', 'University of Maryland', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.08343.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#inference', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение без лишних слов', 'desc': 'Исследование представляет метод NoWait, который подавляет токены явной саморефлексии в больших языковых моделях во время вывода. Это позволяет сократить длину цепочки рассуждений на 27-51% без ущерба для полезности модели. Эксперименты проводились на десяти бенчмарках, охватывающих задачи рассуждения с текстом, изображениями и видео. NoWait предлагает простое решение для повышения эффективности мультимодального рассуждения в ИИ-системах.'}, 'en': {'title': 'NoWait: Streamlining Multimodal Reasoning for Efficiency', 'desc': "The paper introduces NoWait, a method that improves the efficiency of multimodal reasoning models by suppressing explicit self-reflection tokens during inference. These tokens, like 'Wait' and 'Hmm', often lead to unnecessary verbosity and can slow down the reasoning process. By removing these tokens, NoWait significantly shortens the reasoning paths while maintaining the effectiveness of the model. The results from various benchmarks demonstrate that this approach can reduce the length of reasoning trajectories by up to 51%, making it a valuable enhancement for large reasoning models."}, 'zh': {'title': 'NoWait：提升多模态推理效率的创新方法', 'desc': 'NoWait是一种新方法，通过在推理过程中抑制显式自我反思的标记（如“等一下”和“嗯”），来提高多模态推理的效率，而不降低模型的实用性。研究表明，传统的推理模型在复杂推理时常常会出现过度思考，导致输出冗长且重复，影响效率。通过在十个基准测试中进行广泛实验，NoWait能够将思维链的长度减少27%到51%。因此，NoWait为高效且保持实用性的多模态推理提供了一种简单有效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2506.10055', 'title': 'TaskCraft: Automated Generation of Agentic Tasks', 'url': 'https://huggingface.co/papers/2506.10055', 'abstract': 'TaskCraft automates the generation of scalable, multi-tool, and complex agentic tasks to enhance prompt optimization and fine-tuning of agentic models.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic tasks, which require multi-step problem solving with autonomy, tool use, and adaptive reasoning, are becoming increasingly central to the advancement of NLP and AI. However, existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting their scalability. We introduce TaskCraft, an automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories. TaskCraft expands atomic tasks using depth-based and width-based extensions to create structurally and hierarchically complex challenges. Empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. We present a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty to support future research on agent tuning and evaluation.', 'score': 19, 'issue_id': 4329, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'cc5a94a8870a39e9', 'authors': ['Dingfeng Shi', 'Jingyi Cao', 'Qianben Chen', 'Weichen Sun', 'Weizhen Li', 'Hongxuan Lu', 'Fangchen Dong', 'Tianrui Qin', 'King Zhu', 'Minghao Yang', 'Jian Yang', 'Ge Zhang', 'Jiaheng Liu', 'Changwang Zhang', 'Jun Wang', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['OPPO'], 'pdf_title_img': 'assets/pdf/title_img/2506.10055.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#optimization', '#training', '#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'TaskCraft: автоматизация создания сложных агентных задач для улучшения ИИ', 'desc': 'TaskCraft - это автоматизированный метод создания масштабируемых агентных задач для улучшения оптимизации промптов и дообучения агентных моделей. Он генерирует сложные многоэтапные задачи с использованием различных инструментов и адаптивных рассуждений. TaskCraft расширяет атомарные задачи, создавая структурно и иерархически сложные проблемы. Эмпирические результаты показывают, что такие задачи улучшают оптимизацию промптов и повышают качество обучения агентных фундаментальных моделей.'}, 'en': {'title': 'Automating Complex Tasks for Smarter AI Agents', 'desc': 'TaskCraft is a system designed to automatically create complex tasks that require agents to use multiple tools and solve problems independently. It addresses the limitations of current instruction data, which often lacks examples of tool interaction and relies on expensive human-created benchmarks. By generating scalable and verifiable agentic tasks, TaskCraft enhances the training and fine-tuning of AI models, making them more effective in handling multi-step challenges. The system has produced a large dataset of around 36,000 tasks of varying difficulty to aid future research in improving agent performance.'}, 'zh': {'title': 'TaskCraft：自动化生成复杂代理任务的利器', 'desc': 'TaskCraft 是一种自动化工具，旨在生成可扩展的多工具复杂任务，以优化代理模型的提示和微调。代理任务需要多步骤的问题解决能力、工具使用和自适应推理，这在自然语言处理和人工智能的发展中变得越来越重要。现有的指令数据缺乏工具交互，而当前的代理基准依赖昂贵的人类标注，限制了其可扩展性。TaskCraft 通过深度和宽度扩展来生成结构复杂的任务，并提供约36,000个不同难度的合成任务数据集，以支持未来的代理调优和评估研究。'}}}, {'id': 'https://huggingface.co/papers/2506.13759', 'title': 'Discrete Diffusion in Large Language and Multimodal Models: A Survey', 'url': 'https://huggingface.co/papers/2506.13759', 'abstract': 'Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs) enable parallel generation and faster inference compared to autoregressive models through denoising-based strategies and full attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed.   The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025.   In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment.   Paper collection: https://github.com/LiQiiiii/DLLM-Survey', 'score': 18, 'issue_id': 4324, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '0a523ab9b7563360', 'authors': ['Runpeng Yu', 'Qi Li', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.13759.jpg', 'data': {'categories': ['#math', '#training', '#diffusion', '#inference', '#multimodal', '#survey'], 'emoji': '🧠', 'ru': {'title': 'Революция в языковом моделировании: дискретные диффузионные модели', 'desc': 'Статья представляет собой систематический обзор дискретных диффузионных языковых моделей (dLLMs) и мультимодальных языковых моделей (dMLLMs). В отличие от авторегрессионных моделей, dLLMs и dMLLMs используют параллельное декодирование и стратегию генерации на основе шумоподавления. Эти модели обеспечивают более быстрый вывод и лучшую контролируемость по сравнению с авторегрессионными аналогами. В работе рассматриваются математические основы, ключевые техники обучения и вывода, а также применения dLLMs и dMLLMs в различных областях.'}, 'en': {'title': 'Accelerating Language Generation with Discrete Diffusion Models', 'desc': 'This paper surveys Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs), highlighting their advantages over traditional autoregressive models. dLLMs and dMLLMs utilize a parallel decoding approach with full attention and denoising strategies, allowing for faster generation and improved output control. The paper reviews the historical development, mathematical foundations, and key techniques for training these models, as well as their applications in various domains. It also discusses the future potential of dLLMs and dMLLMs in advancing machine learning research and deployment.'}, 'zh': {'title': '离散扩散模型：加速生成与控制的未来', 'desc': '本文系统性地调查了离散扩散语言模型（dLLMs）和离散扩散多模态语言模型（dMLLMs）。与自回归模型不同，dLLMs和dMLLMs采用多标记并行解码的范式，利用全注意力机制和去噪生成策略，从而实现并行生成和更快的推理速度。这种新方法使得细粒度的输出控制和动态响应感知成为可能，这在自回归模型中是难以实现的。研究表明，dLLMs和dMLLMs在推理速度上可实现高达10倍的加速，且在多个领域的应用中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.13750', 'title': 'Test3R: Learning to Reconstruct 3D at Test Time', 'url': 'https://huggingface.co/papers/2506.13750', 'abstract': 'Test3R, a test-time learning technique for 3D reconstruction, enhances geometric accuracy by optimizing network consistency using self-supervised learning on image triplets.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense matching methods like DUSt3R regress pairwise pointmaps for 3D reconstruction. However, the reliance on pairwise prediction and the limited generalization capability inherently restrict the global geometric consistency. In this work, we introduce Test3R, a surprisingly simple test-time learning technique that significantly boosts geometric accuracy. Using image triplets (I_1,I_2,I_3), Test3R generates reconstructions from pairs (I_1,I_2) and (I_1,I_3). The core idea is to optimize the network at test time via a self-supervised objective: maximizing the geometric consistency between these two reconstructions relative to the common image I_1. This ensures the model produces cross-pair consistent outputs, regardless of the inputs. Extensive experiments demonstrate that our technique significantly outperforms previous state-of-the-art methods on the 3D reconstruction and multi-view depth estimation tasks. Moreover, it is universally applicable and nearly cost-free, making it easily applied to other models and implemented with minimal test-time training overhead and parameter footprint. Code is available at https://github.com/nopQAQ/Test3R.', 'score': 15, 'issue_id': 4324, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '68d521856e78273a', 'authors': ['Yuheng Yuan', 'Qiuhong Shen', 'Shizun Wang', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.13750.jpg', 'data': {'categories': ['#3d', '#training', '#optimization'], 'emoji': '🏛️', 'ru': {'title': 'Test3R: Повышение точности 3D-реконструкции через самообучение на тестовых данных', 'desc': 'Test3R - это новая техника обучения во время тестирования для 3D-реконструкции, которая улучшает геометрическую точность. Метод оптимизирует согласованность нейронной сети, используя самоконтролируемое обучение на триплетах изображений. Test3R генерирует реконструкции из пар изображений и максимизирует их геометрическую согласованность относительно общего изображения. Эксперименты показывают, что техника значительно превосходит предыдущие методы в задачах 3D-реконструкции и многоракурсной оценки глубины.'}, 'en': {'title': 'Boosting 3D Reconstruction Accuracy with Test3R', 'desc': "Test3R is a novel test-time learning approach designed to improve the accuracy of 3D reconstruction by leveraging self-supervised learning on image triplets. It addresses the limitations of traditional dense matching methods that rely on pairwise predictions, which can lead to inconsistencies in global geometry. By optimizing the network's output during testing, Test3R ensures that reconstructions from different image pairs maintain geometric consistency relative to a common reference image. This technique not only enhances performance on 3D reconstruction tasks but is also easy to implement and applicable to various models with minimal additional training requirements."}, 'zh': {'title': 'Test3R：提升3D重建精度的简单方法', 'desc': 'Test3R是一种用于3D重建的测试时学习技术，通过自监督学习优化网络一致性，从而提高几何精度。该方法利用图像三元组生成重建，确保不同图像对之间的一致性。Test3R的核心思想是在测试时最大化重建之间的几何一致性，确保模型输出在不同输入下保持一致。实验结果表明，Test3R在3D重建和多视角深度估计任务中显著优于现有的最先进方法，且适用性广泛，几乎不增加成本。'}}}, {'id': 'https://huggingface.co/papers/2506.11991', 'title': 'VGR: Visual Grounded Reasoning', 'url': 'https://huggingface.co/papers/2506.11991', 'abstract': 'VGR, a novel multimodal large language model, improves visual reasoning by detecting relevant image regions and integrating them into the reasoning process, outperforming existing models on multimodal benchmarks with reduced resource usage.  \t\t\t\t\tAI-generated summary \t\t\t\t In the field of multimodal chain-of-thought (CoT) reasoning, existing approaches predominantly rely on reasoning on pure language space, which inherently suffers from language bias and is largely confined to math or science domains. This narrow focus limits their ability to handle complex visual reasoning tasks that demand comprehensive understanding of image details. To address these limitations, this paper introduces VGR, a novel reasoning multimodal large language model (MLLM) with enhanced fine-grained visual perception capabilities. Unlike traditional MLLMs that answer the question or reasoning solely on the language space, our VGR first detects relevant regions that may help to solve problems, and then provides precise answers based on replayed image regions. To achieve this, we conduct a large-scale SFT dataset called VGR -SFT that contains reasoning data with mixed vision grounding and language deduction. The inference pipeline of VGR allows the model to choose bounding boxes for visual reference and a replay stage is introduced to integrates the corresponding regions into the reasoning process, enhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline show that VGR achieves superior performance on multi-modal benchmarks requiring comprehensive image detail understanding. Compared to the baseline, VGR uses only 30\\% of the image token count while delivering scores of +4.1 on MMStar, +7.1 on AI2D, and a +12.9 improvement on ChartQA.', 'score': 15, 'issue_id': 4330, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '48fd0bae72ad378e', 'authors': ['Jiacong Wang', 'Zijian Kang', 'Haochen Wang', 'Haiyong Jiang', 'Jiawen Li', 'Bohong Wu', 'Ya Wang', 'Jiao Ran', 'Xiao Liang', 'Chao Feng', 'Jun Xiao'], 'affiliations': ['ByteDance Inc.', 'School of Artificial Intelligence, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.11991.jpg', 'data': {'categories': ['#reasoning', '#games', '#benchmark', '#dataset', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'VGR: Точное визуальное рассуждение с меньшими затратами', 'desc': 'VGR - это новая мультимодальная большая языковая модель, которая улучшает визуальное рассуждение путем обнаружения релевантных областей изображения и интеграции их в процесс рассуждения. Модель использует комбинацию визуального обоснования и языковой дедукции, что позволяет ей лучше понимать детали изображений. VGR превосходит существующие модели на мультимодальных тестах, используя при этом меньше вычислительных ресурсов. Эксперименты показали значительное улучшение производительности на нескольких бенчмарках, требующих комплексного понимания деталей изображений.'}, 'en': {'title': 'VGR: Revolutionizing Visual Reasoning with Multimodal Integration', 'desc': 'The paper presents VGR, a new multimodal large language model designed to enhance visual reasoning by effectively integrating relevant image regions into its reasoning process. Unlike traditional models that rely solely on language, VGR detects important areas in images to improve its understanding and answers. This model is trained on a large-scale dataset that combines visual grounding with language reasoning, allowing it to perform better on complex visual tasks. VGR demonstrates significant improvements on multimodal benchmarks while using fewer resources, showcasing its efficiency and effectiveness in visual reasoning.'}, 'zh': {'title': 'VGR：提升视觉推理的新型多模态模型', 'desc': 'VGR是一种新型的多模态大型语言模型，旨在通过检测相关图像区域来改善视觉推理能力。与传统模型不同，VGR在推理过程中首先识别可能有助于解决问题的图像区域，然后基于这些区域提供准确的答案。该模型使用了一种名为VGR-SFT的大规模数据集，结合了视觉基础和语言推理的数据。实验结果表明，VGR在多模态基准测试中表现优异，同时资源使用效率更高。'}}}, {'id': 'https://huggingface.co/papers/2506.12915', 'title': 'PersonaFeedback: A Large-scale Human-annotated Benchmark For\n  Personalization', 'url': 'https://huggingface.co/papers/2506.12915', 'abstract': "A new benchmark, PersonaFeedback, evaluates Large Language Models' ability to generate personalized responses given explicit user personas, revealing limitations in current systems.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid improvement in the general capabilities of LLMs, LLM personalization, i.e., how to build LLM systems that can generate personalized responses or services that are tailored to distinct user personas, has become an increasingly important research and engineering problem. However, unlike many new challenging benchmarks being released for evaluating the general/reasoning capabilities, the lack of high-quality benchmarks for evaluating LLM personalization greatly hinders progress in this field. To address this, we introduce PersonaFeedback, a new benchmark that directly evaluates LLMs' ability to provide personalized responses given pre-defined user personas and queries. Unlike existing benchmarks that require models to infer implicit user personas from historical interactions, PersonaFeedback decouples persona inference from personalization, focusing on evaluating the model's ability to generate responses tailored to explicit personas. PersonaFeedback consists of 8298 human-annotated test cases, which are categorized into easy, medium, and hard tiers based on the contextual complexity of the user personas and the difficulty in distinguishing subtle differences between two personalized responses. We conduct comprehensive evaluations across a wide range of models. The empirical results reveal that even state-of-the-art LLMs that can solve complex real-world reasoning tasks could fall short on the hard tier of PersonaFeedback where even human evaluators may find the distinctions challenging. Furthermore, we conduct an in-depth analysis of failure modes across various types of systems, demonstrating that the current retrieval-augmented framework should not be seen as a de facto solution for personalization tasks. All benchmark data, annotation protocols, and the evaluation pipeline will be publicly available to facilitate future research on LLM personalization.", 'score': 13, 'issue_id': 4325, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': '7f12645dbf1aa58d', 'authors': ['Meiling Tao', 'Chenghao Zhu', 'Dongyi Ding', 'Tiannan Wang', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['OPPO', 'South China Agricultural University', 'The Chinese University of Hong Kong, Shenzhen', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.12915.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#multimodal', '#benchmark'], 'emoji': '🎭', 'ru': {'title': 'PersonaFeedback: новый стандарт оценки персонализации языковых моделей', 'desc': 'Представлен новый бенчмарк PersonaFeedback для оценки способности больших языковых моделей (LLM) генерировать персонализированные ответы на основе явно заданных пользовательских персон. Бенчмарк состоит из 8298 аннотированных тестовых примеров, разделенных на легкие, средние и сложные уровни. Результаты показывают, что даже современные LLM, способные решать сложные задачи рассуждений, испытывают трудности на сложном уровне PersonaFeedback. Анализ выявил ограничения текущих подходов к персонализации LLM, включая retrieval-augmented фреймворки.'}, 'en': {'title': 'Enhancing Personalization in LLMs with PersonaFeedback', 'desc': 'The paper introduces PersonaFeedback, a new benchmark designed to assess the ability of Large Language Models (LLMs) to generate personalized responses based on explicit user personas. This benchmark addresses the gap in evaluating LLM personalization, which has been overlooked compared to general reasoning capabilities. PersonaFeedback includes 8,298 human-annotated test cases categorized by complexity, revealing that even advanced LLMs struggle with nuanced personalization tasks. The findings highlight the limitations of current models and emphasize the need for improved frameworks in LLM personalization.'}, 'zh': {'title': '个性化生成的新基准：PersonaFeedback', 'desc': '本文介绍了一个新的基准测试，名为PersonaFeedback，旨在评估大型语言模型（LLM）生成个性化响应的能力。随着LLM能力的快速提升，个性化生成已成为一个重要的研究问题，但缺乏高质量的基准测试限制了这一领域的进展。PersonaFeedback通过提供预定义的用户角色和查询，直接评估模型生成针对特定用户角色的响应能力。研究结果表明，即使是最先进的LLM在处理复杂的个性化任务时也可能表现不佳，强调了当前模型在个性化生成方面的局限性。'}}}, {'id': 'https://huggingface.co/papers/2506.03968', 'title': 'From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding', 'url': 'https://huggingface.co/papers/2506.03968', 'abstract': 'The paper presents a method for generating diverse and complex instruction data for large language models using attributed grounding, achieving top performance on benchmarks with a large synthesized dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.', 'score': 12, 'issue_id': 4324, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '991991c3f686afa8', 'authors': ['Chiwei Zhu', 'Benfeng Xu', 'Xiaorui Wang', 'Zhendong Mao'], 'affiliations': ['Metastone Technology', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03968.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#alignment', '#data', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Синтез сложных инструкций для эффективного обучения языковых моделей', 'desc': 'Статья представляет метод генерации разнообразных и сложных инструкций для обучения больших языковых моделей (LLM) с использованием атрибутивного заземления. Авторы создали набор данных SynthQuestions, содержащий 1 миллион синтетических инструкций. Метод включает в себя нисходящий процесс атрибуции, который связывает реальные инструкции с конкретными пользователями, и восходящий процесс синтеза, использующий веб-документы для создания ситуаций и соответствующих инструкций. Модели, обученные на этом наборе данных, показали ведущие результаты на нескольких стандартных тестах.'}, 'en': {'title': 'Harnessing Attributed Grounding for Diverse Instruction Generation', 'desc': 'This paper introduces a novel method for generating diverse and complex instruction data for large language models (LLMs) through a technique called attributed grounding. The approach combines a top-down attribution process, which connects real-world instructions to specific user contexts, with a bottom-up synthesis process that creates meaningful instructions from web documents. By leveraging this framework, the authors successfully produce a large dataset of 1 million synthesized instructions, named SynthQuestions, which significantly enhances the performance of LLMs on various benchmarks. The results indicate that as more web data is utilized, the effectiveness of the models continues to improve, showcasing the importance of rich and varied instruction data for model alignment.'}, 'zh': {'title': '通过属性基础生成复杂指令数据，提升语言模型性能', 'desc': '本文提出了一种通过属性基础生成多样化和复杂的指令数据的方法，以提高大型语言模型的性能。该方法结合了自上而下的归因过程和自下而上的合成过程，能够有效地从真实指令和网络文档中生成有意义的指令。通过这种框架，我们构建了一个包含100万个指令的数据集SynthQuestions，并在多个基准测试中取得了领先的表现。该研究表明，利用丰富的网络文档可以大规模收集复杂的指令，从而提升模型的对齐能力。'}}}, {'id': 'https://huggingface.co/papers/2506.07961', 'title': 'BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning\n  with Vision-Language Models', 'url': 'https://huggingface.co/papers/2506.07961', 'abstract': 'BridgeVLA is a 3D vision-language-action model that projects 3D inputs to 2D images and uses 2D heatmaps for efficient and effective action prediction, outperforming baselines in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, leveraging pre-trained vision-language models (VLMs) for building vision-language-action (VLA) models has emerged as a promising approach to effective robot manipulation learning. However, only few methods incorporate 3D signals into VLMs for action prediction, and they do not fully leverage the spatial structure inherent in 3D data, leading to low sample efficiency. In this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D inputs to multiple 2D images, ensuring input alignment with the VLM backbone, and (2) utilizes 2D heatmaps for action prediction, unifying the input and output spaces within a consistent 2D image space. In addition, we propose a scalable pre-training method that equips the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning. Extensive experiments show the proposed method is able to learn 3D manipulation efficiently and effectively. BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks. In RLBench, it improves the average success rate from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better performance in challenging generalization settings, boosting the average success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It generalizes robustly in multiple out-of-distribution settings, including visual disturbances and unseen instructions. Remarkably, it is able to achieve a success rate of 96.8% on 10+ tasks with only 3 trajectories per task, highlighting its extraordinary sample efficiency. Project Website:https://bridgevla.github.io/', 'score': 10, 'issue_id': 4324, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '3fcf8d6329af3962', 'authors': ['Peiyan Li', 'Yixiang Chen', 'Hongtao Wu', 'Xiao Ma', 'Xiangnan Wu', 'Yan Huang', 'Liang Wang', 'Tao Kong', 'Tieniu Tan'], 'affiliations': ['ByteDance Seed', 'CASIA', 'FiveAges', 'NJU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2506.07961.jpg', 'data': {'categories': ['#rl', '#3d', '#games', '#optimization', '#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'BridgeVLA: Эффективное обучение роботов через проекцию 3D в 2D', 'desc': 'BridgeVLA - это новая модель машинного обучения для роботизированных манипуляций, объединяющая 3D-зрение, язык и действия. Она проецирует 3D-входы на 2D-изображения и использует 2D-тепловые карты для эффективного прогнозирования действий. Модель превосходит существующие методы в различных тестах, включая симуляции и эксперименты с реальными роботами. BridgeVLA демонстрирует высокую эффективность обучения и способность к обобщению в нестандартных ситуациях.'}, 'en': {'title': 'BridgeVLA: Bridging 3D Vision and Action with 2D Heatmaps', 'desc': "BridgeVLA is a novel model that integrates 3D vision with language and action prediction by projecting 3D inputs into 2D images. This approach allows it to utilize 2D heatmaps for more efficient action prediction, enhancing the model's performance in robot manipulation tasks. The model is pre-trained to predict these heatmaps, which helps it learn effectively from fewer samples. Extensive testing shows that BridgeVLA significantly outperforms existing methods in various benchmarks, demonstrating its robustness and efficiency in real-world applications."}, 'zh': {'title': 'BridgeVLA：高效的3D视觉-语言-动作模型', 'desc': 'BridgeVLA是一种3D视觉-语言-动作模型，它将3D输入投影到2D图像，并利用2D热图进行高效的动作预测。该模型通过将3D信号整合到视觉-语言模型中，充分利用了3D数据的空间结构，从而提高了样本效率。我们提出了一种可扩展的预训练方法，使得视觉-语言模型能够在下游策略学习之前预测2D热图。实验结果表明，BridgeVLA在多个基准测试中超越了现有的最先进方法，展现了卓越的学习效率和效果。'}}}, {'id': 'https://huggingface.co/papers/2506.12450', 'title': 'Language Surgery in Multilingual Large Language Models', 'url': 'https://huggingface.co/papers/2506.12450', 'abstract': "Research confirms natural representation alignment in large language models and introduces Inference-Time Language Control to enhance cross-lingual performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and languages, revolutionizing natural language processing. This paper investigates the naturally emerging representation alignment in LLMs, particularly in the middle layers, and its implications for disentangling language-specific and language-agnostic information. We empirically confirm the existence of this alignment, analyze its behavior in comparison to explicitly designed alignment models, and demonstrate its potential for language-specific manipulation without semantic degradation. Building on these findings, we propose Inference-Time Language Control (ITLC), a novel method that leverages latent injection to enable precise cross-lingual language control and mitigate language confusion in LLMs. Our experiments highlight ITLC's strong cross-lingual control capabilities while preserving semantic integrity in target languages. Furthermore, we demonstrate its effectiveness in alleviating the cross-lingual language confusion problem, which persists even in current large-scale LLMs, leading to inconsistent language generation. This work advances our understanding of representation alignment in LLMs and introduces a practical solution for enhancing their cross-lingual performance.", 'score': 7, 'issue_id': 4325, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': '6c05e4a1a8b705dc', 'authors': ['Joanito Agili Lopo', 'Muhammad Ravi Shulthan Habibi', 'Tack Hwa Wong', 'Muhammad Ilham Ghozali', 'Fajri Koto', 'Genta Indra Winata', 'Peerat Limkonchotiwat', 'Alham Fikri Aji', 'Samuel Cahyawijaya'], 'affiliations': ['AI Singapore', 'Capital One', 'Cohere', 'Kreasof AI', 'MBZUAI', 'SEACrowd', 'Universitas Indonesia'], 'pdf_title_img': 'assets/pdf/title_img/2506.12450.jpg', 'data': {'categories': ['#alignment', '#machine_translation', '#inference', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Улучшение кросс-языковых возможностей LLM через контроль скрытых представлений', 'desc': 'Исследование подтверждает естественное выравнивание представлений в больших языковых моделях (LLM), особенно в средних слоях. Авторы предлагают метод Inference-Time Language Control (ITLC), который использует латентную инъекцию для точного межъязыкового контроля. ITLC позволяет улучшить кросс-языковую производительность LLM и уменьшить языковую путаницу. Эксперименты показывают эффективность ITLC в сохранении семантической целостности при переключении между языками.'}, 'en': {'title': 'Enhancing Cross-Lingual Performance with Natural Representation Alignment', 'desc': 'This paper explores how large language models (LLMs) naturally align their representations across different languages, particularly in their middle layers. It confirms that this alignment allows for the separation of language-specific and language-agnostic information, which can be manipulated without losing meaning. The authors introduce a new technique called Inference-Time Language Control (ITLC) that uses latent injection to improve control over language generation in cross-lingual contexts. Their experiments show that ITLC effectively reduces language confusion while maintaining semantic integrity, enhancing the overall performance of LLMs in multilingual tasks.'}, 'zh': {'title': '提升跨语言性能的推理时语言控制', 'desc': '本研究确认了大型语言模型（LLMs）中自然出现的表示对齐现象，特别是在中间层的表现。我们实证验证了这种对齐的存在，并分析了其与显式设计的对齐模型的行为比较。基于这些发现，我们提出了一种新方法——推理时语言控制（ITLC），它利用潜在注入技术实现精确的跨语言控制。实验结果表明，ITLC在保持目标语言语义完整性的同时，显著提升了跨语言控制能力，解决了当前大型LLMs中存在的语言混淆问题。'}}}, {'id': 'https://huggingface.co/papers/2506.06366', 'title': 'AI Agent Behavioral Science', 'url': 'https://huggingface.co/papers/2506.06366', 'abstract': 'A new field, AI Agent Behavioral Science, is proposed to systematically study the behaviors of AI agents in diverse contexts, emphasizing external factors and their interactions, and addressing responsible AI aspects.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled the development of AI agents that exhibit increasingly human-like behaviors, including planning, adaptation, and social dynamics across diverse, interactive, and open-ended scenarios. These behaviors are not solely the product of the internal architectures of the underlying models, but emerge from their integration into agentic systems operating within specific contexts, where environmental factors, social cues, and interaction feedbacks shape behavior over time. This evolution necessitates a new scientific perspective: AI Agent Behavioral Science. Rather than focusing only on internal mechanisms, this perspective emphasizes the systematic observation of behavior, design of interventions to test hypotheses, and theory-guided interpretation of how AI agents act, adapt, and interact over time. We systematize a growing body of research across individual agent, multi-agent, and human-agent interaction settings, and further demonstrate how this perspective informs responsible AI by treating fairness, safety, interpretability, accountability, and privacy as behavioral properties. By unifying recent findings and laying out future directions, we position AI Agent Behavioral Science as a necessary complement to traditional model-centric approaches, providing essential tools for understanding, evaluating, and governing the real-world behavior of increasingly autonomous AI systems.', 'score': 6, 'issue_id': 4326, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '683be64d015db51c', 'authors': ['Lin Chen', 'Yunke Zhang', 'Jie Feng', 'Haoye Chai', 'Honglin Zhang', 'Bingbing Fan', 'Yibo Ma', 'Shiyuan Zhang', 'Nian Li', 'Tianhui Liu', 'Nicholas Sukiennik', 'Keyu Zhao', 'Yu Li', 'Ziyi Liu', 'Fengli Xu', 'Yong Li'], 'affiliations': ['Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.06366.jpg', 'data': {'categories': ['#agi', '#healthcare', '#ethics', '#multimodal', '#agents', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'От модели к поведению: новый взгляд на изучение ИИ-агентов', 'desc': 'Предлагается новая область исследований - поведенческая наука ИИ-агентов. Она направлена на систематическое изучение поведения ИИ-агентов в различных контекстах, уделяя особое внимание внешним факторам и их взаимодействию. Исследования охватывают индивидуальных агентов, многоагентные системы и взаимодействие человека с агентами. Этот подход дополняет традиционные модельно-ориентированные методы и предоставляет инструменты для понимания, оценки и управления поведением автономных ИИ-систем в реальном мире.'}, 'en': {'title': 'Understanding AI Behavior: A New Scientific Approach', 'desc': "The paper introduces AI Agent Behavioral Science, a new field focused on studying the behaviors of AI agents in various contexts. It highlights that these behaviors arise not just from the AI's internal design but also from interactions with their environment and social dynamics. The approach emphasizes systematic observation, hypothesis testing, and theory-driven analysis to understand how AI agents adapt and interact over time. This perspective also addresses responsible AI considerations, such as fairness and accountability, making it a vital complement to traditional model-centric methods."}, 'zh': {'title': '探索人工智能代理的行为科学', 'desc': '本文提出了一个新的领域——人工智能代理行为科学，旨在系统研究人工智能代理在不同环境中的行为。随着大型语言模型的发展，AI代理展现出越来越人性化的行为，如规划、适应和社交动态。这些行为不仅源于模型的内部结构，还受到环境因素、社交线索和互动反馈的影响。该领域强调对行为的系统观察和干预设计，以促进对AI代理行为的理解和负责任的应用。'}}}, {'id': 'https://huggingface.co/papers/2506.09050', 'title': 'ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm\n  Engineering', 'url': 'https://huggingface.co/papers/2506.09050', 'abstract': 'ALE-Bench evaluates AI systems on score-based algorithmic programming contests drawn from AtCoder, focusing on long-term iterative problem-solving in domains like package-delivery routing, crew scheduling, factory production, and power-grid balancing.  \t\t\t\t\tAI-generated summary \t\t\t\t How well do AI systems perform in algorithm engineering for hard optimization problems in domains such as package-delivery routing, crew scheduling, factory production planning, and power-grid balancing? We introduce ALE-Bench, a new benchmark for evaluating AI systems on score-based algorithmic programming contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench presents optimization problems that are computationally hard and admit no known exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench encourages iterative solution refinement over long time horizons. Our software framework supports interactive agent architectures that leverage test-run feedback and visualizations. Our evaluation of frontier LLMs revealed that while they demonstrate high performance on specific problems, a notable gap remains compared to humans in terms of consistency across problems and long-horizon problem-solving capabilities. This highlights the need for this benchmark to foster future AI advancements.', 'score': 4, 'issue_id': 4328, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'f30a01d616f75ed3', 'authors': ['Yuki Imajuku', 'Kohki Horie', 'Yoichi Iwata', 'Kensho Aoki', 'Naohiro Takahashi', 'Takuya Akiba'], 'affiliations': ['AtCoder, Japan', 'Sakana AI, Japan', 'The University of Tokyo, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2506.09050.jpg', 'data': {'categories': ['#benchmark', '#agents', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'ALE-Bench: Испытание ИИ в алгоритмической оптимизации', 'desc': 'ALE-Bench - это новый эталонный тест для оценки систем искусственного интеллекта на основе алгоритмических соревнований по программированию. Он фокусируется на задачах оптимизации в таких областях, как маршрутизация доставки, планирование экипажей и балансировка электросетей. В отличие от кратковременных тестов на прохождение/непрохождение, ALE-Bench поощряет итеративное улучшение решений в течение длительного времени. Оценка современных языковых моделей показала, что хотя они демонстрируют высокую производительность на отдельных задачах, по сравнению с людьми остается заметный разрыв в последовательности решения задач и способности к долгосрочному решению проблем.'}, 'en': {'title': "Evaluating AI's Long-Term Problem-Solving with ALE-Bench", 'desc': 'ALE-Bench is a benchmark designed to assess AI systems on complex optimization problems derived from real-world algorithmic programming contests. It focuses on long-term iterative problem-solving in various domains, such as package delivery and factory production. The benchmark emphasizes the importance of refining solutions over extended periods, rather than just achieving quick pass/fail results. Our findings indicate that while advanced language models perform well on certain tasks, they still lag behind human consistency and long-term problem-solving abilities, underscoring the need for further development in AI.'}, 'zh': {'title': 'ALE-Bench：推动AI在复杂优化问题上的进步', 'desc': 'ALE-Bench是一个新的基准，用于评估人工智能系统在基于分数的算法编程竞赛中的表现，特别是在复杂的优化问题上。它关注于长期的迭代问题解决，涉及包裹投递、人员调度、工厂生产和电网平衡等领域。与短期的通过/不通过编码基准不同，ALE-Bench鼓励在较长时间内对解决方案进行细化。我们的评估显示，尽管前沿的大型语言模型在特定问题上表现良好，但在一致性和长期问题解决能力方面与人类相比仍存在显著差距。'}}}, {'id': 'https://huggingface.co/papers/2506.06454', 'title': 'LETS Forecast: Learning Embedology for Time Series Forecasting', 'url': 'https://huggingface.co/papers/2506.06454', 'abstract': "DeepEDM integrates empirical dynamic modeling with deep neural networks to learn latent spaces and approximate complex nonlinear dynamics for improved time series forecasting.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world time series are often governed by complex nonlinear dynamics. Understanding these underlying dynamics is crucial for precise future prediction. While deep learning has achieved major success in time series forecasting, many existing approaches do not explicitly model the dynamics. To bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear dynamical systems modeling with deep neural networks. Inspired by empirical dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel deep model that learns a latent space from time-delayed embeddings, and employs kernel regression to approximate the underlying dynamics, while leveraging efficient implementation of softmax attention and allowing for accurate prediction of future time steps. To evaluate our method, we conduct comprehensive experiments on synthetic data of nonlinear dynamical systems as well as real-world time series across domains. Our results show that DeepEDM is robust to input noise, and outperforms state-of-the-art methods in forecasting accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.", 'score': 3, 'issue_id': 4329, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '4d7af8be98618ff0', 'authors': ['Abrar Majeedi', 'Viswanatha Reddy Gajjala', 'Satya Sai Srinath Namburi GNVV', 'Nada Magdi Elkordi', 'Yin Li'], 'affiliations': ['Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison', 'Department of Computer Sciences, University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.06454.jpg', 'data': {'categories': ['#training', '#synthetic', '#optimization', '#data'], 'emoji': '🔮', 'ru': {'title': 'DeepEDM: Глубокое обучение встречается с нелинейной динамикой для точного прогнозирования', 'desc': 'DeepEDM - это новый фреймворк, объединяющий эмпирическое динамическое моделирование с глубокими нейронными сетями для прогнозирования временных рядов. Он обучает латентное пространство на основе вложений с временной задержкой и использует ядерную регрессию для аппроксимации нелинейной динамики. DeepEDM применяет эффективную реализацию механизма внимания softmax и превосходит современные методы по точности прогнозирования. Эксперименты показали устойчивость метода к входному шуму на синтетических и реальных данных из разных областей.'}, 'en': {'title': 'DeepEDM: Bridging Dynamics and Deep Learning for Better Forecasting', 'desc': 'DeepEDM is a novel framework that combines empirical dynamic modeling with deep neural networks to enhance time series forecasting. It effectively learns latent spaces from time-delayed embeddings, allowing it to capture complex nonlinear dynamics in data. By utilizing kernel regression and softmax attention, DeepEDM can accurately predict future time steps while being robust to input noise. Comprehensive experiments demonstrate that it outperforms existing state-of-the-art forecasting methods across various domains.'}, 'zh': {'title': '深度动态建模，精准时间预测', 'desc': 'DeepEDM是一种将经验动态建模与深度神经网络相结合的框架，旨在提高时间序列预测的准确性。该方法通过学习时间延迟嵌入的潜在空间，利用核回归来近似复杂的非线性动态。DeepEDM受到Takens定理的启发，能够有效地实现softmax注意力机制，从而准确预测未来的时间步。实验结果表明，DeepEDM在处理输入噪声时表现出色，并在预测准确性上超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2506.12189', 'title': "Supernova Event Dataset: Interpreting Large Language Model's Personality\n  through Critical Event Analysis", 'url': 'https://huggingface.co/papers/2506.12189', 'abstract': "The study evaluates various LLMs on diverse text tasks using a new dataset, revealing distinct personality traits and improving model interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications.", 'score': 2, 'issue_id': 4324, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '952c0d68aa23cbda', 'authors': ['Pranav Agarwal', 'Ioana Ciucă'], 'affiliations': ['Google Deep Research', 'Institute', 'Mila', 'Quebec AI', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12189.jpg', 'data': {'categories': ['#dataset', '#small_models', '#reasoning', '#long_context', '#interpretability', '#multimodal', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая личность искусственного интеллекта: новый подход к интерпретации языковых моделей', 'desc': "Исследование оценивает различные большие языковые модели (LLM) на разнообразных текстовых задачах с использованием нового набора данных, выявляя их отличительные личностные черты и улучшая интерпретируемость моделей. Авторы предлагают набор данных Supernova Event Dataset, содержащий разнообразные статьи, и используют его для оценки способности LLM извлекать и ранжировать ключевые события из текста. Анализ показывает, что разные модели демонстрируют различные подходы к рассуждению и анализу информации. Это исследование улучшает понимание 'личности' языковых моделей, делая их более удобными для широкого спектра приложений."}, 'en': {'title': 'Unveiling LLM Personalities for Better Interpretability', 'desc': 'This study investigates how different Large Language Models (LLMs) perform on various text tasks using a new dataset called the Supernova Event Dataset. The dataset includes a wide range of articles, allowing for the evaluation of LLMs in extracting and ranking key events, which requires complex reasoning and understanding of context. The research reveals distinct personality traits among the models, such as emotional reasoning in Orca 2 and strategic thinking in Qwen 2.5, enhancing our understanding of their decision-making processes. By using another LLM as a judge to assess these traits, the study improves the interpretability of models, making them more accessible for diverse applications.'}, 'zh': {'title': '揭示大型语言模型的个性特征', 'desc': '本研究评估了多种大型语言模型（LLMs）在不同文本任务上的表现，使用了一个新的数据集。通过分析模型的选择和分类事件，我们揭示了模型的个性特征，并提高了模型的可解释性。我们提出的超新星事件数据集包含多样的文章，帮助我们基准测试模型在提取和排序关键事件方面的能力。研究结果显示，不同模型在处理情感推理、战略分析和因果推理等方面表现出明显的个性差异。'}}}, {'id': 'https://huggingface.co/papers/2506.12953', 'title': 'Forecasting Time Series with LLMs via Patch-Based Prompting and\n  Decomposition', 'url': 'https://huggingface.co/papers/2506.12953', 'abstract': 'PatchInstruct enhances LLM forecasting quality through specialized prompting methods that include time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions.', 'score': 1, 'issue_id': 4324, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': 'fb2789e38592ff5a', 'authors': ['Mayank Bumb', 'Anshul Vemulapalli', 'Sri Harsha Vardhan Prasad Jella', 'Anish Gupta', 'An La', 'Ryan A. Rossi', 'Hongjie Chen', 'Franck Dernoncourt', 'Nesreen K. Ahmed', 'Yu Wang'], 'affiliations': ['Adobe', 'Dolby Labs', 'Intel', 'University of Massachusetts Amherst', 'University of Oregon'], 'pdf_title_img': 'assets/pdf/title_img/2506.12953.jpg', 'data': {'categories': ['#data', '#training', '#optimization'], 'emoji': '📈', 'ru': {'title': 'Точное прогнозирование временных рядов с помощью языковых моделей', 'desc': 'Статья представляет метод PatchInstruct для улучшения качества прогнозирования временных рядов с помощью больших языковых моделей (LLM). Метод использует специализированные техники промптинга, включая декомпозицию временных рядов, токенизацию на основе патчей и расширение данных с помощью похожих соседей. PatchInstruct позволяет LLM делать точные прогнозы без сложной архитектуры или масштабного дообучения. Это простой и гибкий подход, требующий минимальной предобработки данных.'}, 'en': {'title': 'Enhancing LLM Forecasting with Simple Prompting Techniques', 'desc': 'PatchInstruct is a method that improves the forecasting abilities of Large Language Models (LLMs) by using innovative prompting techniques. It incorporates time series decomposition to break down data into manageable parts, patch-based tokenization to efficiently handle input, and similarity-based neighbor augmentation to enhance predictions by considering related data points. This approach allows LLMs to perform time series forecasting without the need for extensive fine-tuning or complex architectures. Overall, PatchInstruct simplifies the process while boosting the accuracy of predictions in time series analysis.'}, 'zh': {'title': 'PatchInstruct：简化时间序列预测的有效方法', 'desc': 'PatchInstruct是一种增强大型语言模型（LLM）时间序列预测质量的方法。它通过专门的提示策略，如时间序列分解、基于补丁的标记化和相似性邻居增强，来实现这一目标。与以往需要大量微调的方法不同，PatchInstruct能够在不复杂重训练的情况下，灵活地进行时间序列预测。该方法保持了简单性，并且对数据的预处理要求最低。'}}}, {'id': 'https://huggingface.co/papers/2506.12623', 'title': 'MS4UI: A Dataset for Multi-modal Summarization of User Interface\n  Instructional Videos', 'url': 'https://huggingface.co/papers/2506.12623', 'abstract': 'A novel benchmark and dataset are proposed for multi-modal summarization of UI instructional videos, addressing the need for step-by-step executable instructions and key video frames.  \t\t\t\t\tAI-generated summary \t\t\t\t We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose a novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect a dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization.', 'score': 1, 'issue_id': 4324, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': 'ef83eb4ade9dc4bf', 'authors': ['Yuan Zang', 'Hao Tan', 'Seunghyun Yoon', 'Franck Dernoncourt', 'Jiuxiang Gu', 'Kushal Kafle', 'Chen Sun', 'Trung Bui'], 'affiliations': ['Adobe Research', 'Brown University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12623.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#video', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'Новый подход к сумматизации обучающих видео по UI', 'desc': 'Предложен новый бенчмарк и набор данных для мультимодальной сумматизации обучающих видео по пользовательским интерфейсам. Исследование направлено на создание пошаговых исполняемых инструкций и выделение ключевых кадров видео. Собран датасет из 2413 обучающих видео общей продолжительностью более 167 часов. Эксперименты показали, что современные методы мультимодальной сумматизации испытывают трудности с обработкой таких видео, что подчеркивает важность разработки новых подходов.'}, 'en': {'title': 'Enhancing Learning with UI Video Summarization', 'desc': 'This paper introduces a new benchmark and dataset specifically designed for multi-modal summarization of user interface (UI) instructional videos. The goal is to create efficient summaries that include step-by-step text instructions and key video frames, which are essential for effective learning. The authors highlight that existing benchmarks are inadequate for this purpose, as they focus on general video summarization rather than instructional content. Through extensive experiments on their dataset of 2,413 annotated UI instructional videos, they demonstrate that current multi-modal summarization techniques are not effective for this specific type of video, indicating a need for improved methods.'}, 'zh': {'title': '提升UI教学视频的多模态总结能力', 'desc': '本文提出了一种新的基准和数据集，用于多模态总结用户界面（UI）教学视频，旨在提供逐步可执行的指令和关键视频帧。现有的基准主要关注一般的语义级视频总结，无法满足教学视频中对逐步指令和插图的需求。我们收集了2413个UI教学视频的数据集，手动标注了视频分割、文本总结和视频总结，以便进行全面评估。实验结果表明，现有的多模态总结方法在UI视频总结上表现不佳，强调了开发新方法的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.12552', 'title': 'Profiling News Media for Factuality and Bias Using LLMs and the\n  Fact-Checking Methodology of Human Experts', 'url': 'https://huggingface.co/papers/2506.12552', 'abstract': 'A novel methodology using large language models with curated prompts improves predictions of media outlet factuality and political bias, validated through experiments and error analysis.  \t\t\t\t\tAI-generated summary \t\t\t\t In an age characterized by the proliferation of mis- and disinformation online, it is critical to empower readers to understand the content they are reading. Important efforts in this direction rely on manual or automatic fact-checking, which can be challenging for emerging claims with limited information. Such scenarios can be handled by assessing the reliability and the political bias of the source of the claim, i.e., characterizing entire news outlets rather than individual claims or articles. This is an important but understudied research direction. While prior work has looked into linguistic and social contexts, we do not analyze individual articles or information in social media. Instead, we propose a novel methodology that emulates the criteria that professional fact-checkers use to assess the factuality and political bias of an entire outlet. Specifically, we design a variety of prompts based on these criteria and elicit responses from large language models (LLMs), which we aggregate to make predictions. In addition to demonstrating sizable improvements over strong baselines via extensive experiments with multiple LLMs, we provide an in-depth error analysis of the effect of media popularity and region on model performance. Further, we conduct an ablation study to highlight the key components of our dataset that contribute to these improvements. To facilitate future research, we released our dataset and code at https://github.com/mbzuai-nlp/llm-media-profiling.', 'score': 1, 'issue_id': 4332, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': '6c852f01e3464d88', 'authors': ['Zain Muhammad Mujahid', 'Dilshod Azizov', 'Maha Tufail Agro', 'Preslav Nakov'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence, UAE', 'University of Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2506.12552.jpg', 'data': {'categories': ['#alignment', '#data', '#ethics', '#open_source', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'ИИ на страже достоверности: новый подход к оценке СМИ', 'desc': 'Статья представляет новую методологию использования больших языковых моделей (LLM) с специально подобранными промптами для улучшения прогнозирования фактической точности и политической предвзятости СМИ. Авторы предлагают подход, имитирующий критерии, используемые профессиональными фактчекерами для оценки целых новостных изданий, а не отдельных статей. Эксперименты показывают значительное улучшение по сравнению с сильными базовыми моделями, а также включают подробный анализ ошибок. Исследователи предоставляют набор данных и код для дальнейших исследований в этой области.'}, 'en': {'title': 'Empowering Readers: Assessing Media Factuality with LLMs', 'desc': 'This paper presents a new method that uses large language models (LLMs) to evaluate the factuality and political bias of media outlets. Instead of focusing on individual articles, the approach assesses the overall reliability of news sources by using curated prompts that mimic professional fact-checking criteria. The authors conducted experiments that showed significant improvements in prediction accuracy compared to existing methods, along with a detailed error analysis. They also released their dataset and code to support further research in this area.'}, 'zh': {'title': '利用大型语言模型提升媒体事实性与偏见预测', 'desc': '本文提出了一种新方法，利用大型语言模型和精心设计的提示来提高媒体来源的事实性和政治偏见预测。研究表明，评估整个新闻机构的可靠性比单独分析个别文章更有效。通过大量实验和错误分析，验证了该方法在多个大型语言模型上的显著改进。我们还发布了数据集和代码，以促进未来的研究。'}}}, {'id': 'https://huggingface.co/papers/2506.09968', 'title': 'SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification\n  and LLM Assistance', 'url': 'https://huggingface.co/papers/2506.09968', 'abstract': 'A gamified LLM-assisted system, SRLAgent, significantly improves self-regulated learning skills in college students through interactive, goal-setting, and real-time AI feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-regulated learning (SRL) is crucial for college students navigating increased academic demands and independence. Insufficient SRL skills can lead to disorganized study habits, low motivation, and poor time management, undermining learners ability to thrive in challenging environments. Through a formative study involving 59 college students, we identified key challenges students face in developing SRL skills, including difficulties with goal-setting, time management, and reflective learning. To address these challenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL skills through gamification and adaptive support from large language models (LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables students to engage in goal-setting, strategy execution, and self-reflection within an interactive game-based environment. The system offers real-time feedback and scaffolding powered by LLMs to support students independent study efforts. We evaluated SRLAgent using a between-subjects design, comparing it to a baseline system (SRL without Agent features) and a traditional multimedia learning condition. Results showed significant improvements in SRL skills within the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement compared to the baselines. This work highlights the value of embedding SRL scaffolding and real-time AI support within gamified environments, offering design implications for educational technologies that aim to promote deeper learning and metacognitive skill development.', 'score': 1, 'issue_id': 4330, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'ea72b7b36234ccbd', 'authors': ['Wentao Ge', 'Yuqing Sun', 'Ziyan Wang', 'Haoyue Zheng', 'Weiyang He', 'Piaohong Wang', 'Qianyu Zhu', 'Benyou Wang'], 'affiliations': ['City University of Hong Kong China', 'The Chinese University of Hong Kong, Shenzhen China'], 'pdf_title_img': 'assets/pdf/title_img/2506.09968.jpg', 'data': {'categories': ['#healthcare', '#games', '#multimodal', '#science', '#agents'], 'emoji': '🎓', 'ru': {'title': 'ИИ-ассистент для развития навыков самообучения', 'desc': 'Статья представляет SRLAgent - систему на основе больших языковых моделей (LLM), которая помогает студентам колледжей развивать навыки саморегулируемого обучения (SRL). SRLAgent использует геймификацию и адаптивную поддержку LLM для улучшения постановки целей, управления временем и рефлексивного обучения. Система основана на трехфазной модели SRL Циммермана и предоставляет интерактивную игровую среду с обратной связью в реальном времени. Результаты исследования показали значительное улучшение навыков SRL и вовлеченности студентов при использовании SRLAgent по сравнению с контрольными группами.'}, 'en': {'title': 'Empowering Students with Gamified AI for Self-Regulated Learning', 'desc': "The paper presents SRLAgent, a gamified system that enhances self-regulated learning (SRL) skills in college students by utilizing large language models (LLMs) for real-time feedback. It addresses common challenges students face, such as goal-setting and time management, by providing an interactive environment based on Zimmerman's SRL framework. A study with 59 participants demonstrated that SRLAgent significantly improved SRL skills and engagement compared to traditional learning methods. This research emphasizes the importance of integrating AI support and gamification in educational tools to foster independent learning and metacognitive development."}, 'zh': {'title': '游戏化系统提升大学生自我调节学习技能', 'desc': '本研究介绍了一种名为SRLAgent的系统，该系统通过游戏化和大型语言模型（LLM）的实时反馈，显著提升大学生的自我调节学习（SRL）技能。研究发现，许多学生在目标设定、时间管理和反思学习方面面临挑战。SRLAgent基于Zimmerman的三阶段SRL框架，帮助学生在互动游戏环境中进行目标设定、策略执行和自我反思。实验结果表明，使用SRLAgent的学生在SRL技能和参与度上均有显著提高。'}}}, {'id': 'https://huggingface.co/papers/2506.11115', 'title': 'Incorporating Domain Knowledge into Materials Tokenization', 'url': 'https://huggingface.co/papers/2506.11115', 'abstract': 'MATTER, a novel tokenization approach incorporating material knowledge, improves performance in scientific text processing tasks by maintaining structural and semantic material integrity.  \t\t\t\t\tAI-generated summary \t\t\t\t While language models are increasingly utilized in materials science, typical models rely on frequency-centric tokenization methods originally developed for natural language processing. However, these methods frequently produce excessive fragmentation and semantic loss, failing to maintain the structural and semantic integrity of material concepts. To address this issue, we propose MATTER, a novel tokenization approach that integrates material knowledge into tokenization. Based on MatDetector trained on our materials knowledge base and a re-ranking method prioritizing material concepts in token merging, MATTER maintains the structural integrity of identified material concepts and prevents fragmentation during tokenization, ensuring their semantic meaning remains intact. The experimental results demonstrate that MATTER outperforms existing tokenization methods, achieving an average performance gain of 4% and 2% in the generation and classification tasks, respectively. These results underscore the importance of domain knowledge for tokenization strategies in scientific text processing. Our code is available at https://github.com/yerimoh/MATTER', 'score': 1, 'issue_id': 4330, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'dfe8867cd5b71249', 'authors': ['Yerim Oh', 'Jun-Hyung Park', 'Junho Kim', 'SungHo Kim', 'SangKeun Lee'], 'affiliations': ['Department of Artificial Intelligence, Korea University', 'Department of Computer Science and Engineering, Korea University', 'Division of Language & AI, Hankuk University of Foreign Studies'], 'pdf_title_img': 'assets/pdf/title_img/2506.11115.jpg', 'data': {'categories': ['#optimization', '#data', '#dataset', '#multimodal', '#science'], 'emoji': '🧪', 'ru': {'title': 'MATTER: Токенизация с пониманием материаловедения', 'desc': 'MATTER - это новый подход к токенизации, который учитывает знания о материалах и улучшает обработку научных текстов. В отличие от стандартных методов токенизации, MATTER сохраняет структурную и семантическую целостность концепций материалов. Этот метод основан на модели MatDetector, обученной на базе знаний о материалах, и использует переранжирование для приоритизации концепций материалов при объединении токенов. Эксперименты показали, что MATTER превосходит существующие методы токенизации, улучшая результаты на 4% в задачах генерации и на 2% в задачах классификации.'}, 'en': {'title': 'MATTER: Tokenization that Understands Materials!', 'desc': 'The paper introduces MATTER, a new tokenization method designed specifically for scientific texts in materials science. Unlike traditional tokenization methods that often lead to loss of meaning and structure, MATTER incorporates material knowledge to preserve the integrity of material concepts. It utilizes a trained model called MatDetector and a re-ranking technique to effectively merge tokens while maintaining their semantic significance. Experimental results show that MATTER significantly improves performance in text generation and classification tasks compared to existing methods.'}, 'zh': {'title': 'MATTER：提升科学文本处理的分词新方法', 'desc': 'MATTER是一种新颖的分词方法，它结合了材料知识，旨在提高科学文本处理任务的性能。传统的分词方法往往基于频率，容易导致语义丢失和结构破坏，无法有效保持材料概念的完整性。MATTER通过使用训练好的MatDetector和优先考虑材料概念的重排序方法，确保在分词过程中保持材料概念的结构和语义完整。实验结果表明，MATTER在生成和分类任务中分别提高了4%和2%的性能，强调了领域知识在科学文本处理中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.13752', 'title': 'Steering LLM Thinking with Budget Guidance', 'url': 'https://huggingface.co/papers/2506.13752', 'abstract': 'Budget guidance is a method that steers LLM reasoning within a targeted budget without fine-tuning and achieves improved efficiency and performance on math benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, a simple yet effective method for steering the reasoning process of LLMs toward a target budget without requiring any LLM fine-tuning. Our approach introduces a lightweight predictor that models a Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in a soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to a 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance.', 'score': 0, 'issue_id': 4325, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'c3d9b714e91736d6', 'authors': ['Junyan Li', 'Wenshuo Zhao', 'Yang Zhang', 'Chuang Gan'], 'affiliations': ['MIT-IBM Watson AI Lab', 'UMass Amherst', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13752.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#reasoning', '#math'], 'emoji': '💡', 'ru': {'title': 'Эффективное управление рассуждениями ИИ в рамках бюджета', 'desc': "Метод 'бюджетного руководства' позволяет управлять рассуждениями языковых моделей в рамках заданного бюджета без дополнительного обучения. Он использует легковесный предиктор, моделирующий гамма-распределение оставшейся длины рассуждения при генерации следующего токена. Этот подход обеспечивает естественный контроль длины рассуждения и значительно повышает эффективность использования токенов на сложных математических тестах. Метод также демонстрирует обобщающую способность на более широкий спектр задач и проявляет эмерджентные свойства, такие как оценка сложности вопросов."}, 'en': {'title': 'Steering LLM Reasoning with Budget Guidance for Efficiency and Performance', 'desc': 'This paper introduces a method called budget guidance, which helps large language models (LLMs) reason effectively within a specified budget of thinking tokens. By using a lightweight predictor that models a Gamma distribution, the method controls the reasoning length during the generation of each token without needing to fine-tune the LLM. This approach not only improves efficiency but also enhances performance on math benchmarks, achieving significant accuracy gains while using fewer tokens. Additionally, budget guidance shows versatility across different tasks and can even estimate the difficulty of questions.'}, 'zh': {'title': '预算引导：高效推理的新方法', 'desc': '预算引导是一种方法，可以在不进行微调的情况下，引导大型语言模型（LLM）在目标预算内进行推理，从而提高效率和性能。该方法通过引入一个轻量级预测器，建模剩余思考长度的伽马分布，来控制推理长度。预算引导确保生成过程遵循指定的思考预算，同时在数学基准测试中显著提高了令牌效率。与基线方法相比，在紧张预算下，预算引导在MATH-500基准上实现了高达26%的准确率提升，同时仅使用全思考模型63%的思考令牌。'}}}, {'id': 'https://huggingface.co/papers/2506.13430', 'title': 'Uncertainty-Aware Remaining Lifespan Prediction from Images', 'url': 'https://huggingface.co/papers/2506.13430', 'abstract': 'Vision transformer models predict remaining lifespan from images with high accuracy and well-calibrated uncertainty estimates.  \t\t\t\t\tAI-generated summary \t\t\t\t Predicting mortality-related outcomes from images offers the prospect of accessible, noninvasive, and scalable health screening. We present a method that leverages pretrained vision transformer foundation models to estimate remaining lifespan from facial and whole-body images, alongside robust uncertainty quantification. We show that predictive uncertainty varies systematically with the true remaining lifespan, and that this uncertainty can be effectively modeled by learning a Gaussian distribution for each sample. Our approach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on an established Dataset, and further improves to 4.79 and 5.07 years MAE on two new, higher-quality datasets curated and published in this work. Importantly, our models provide well-calibrated uncertainty estimates, as demonstrated by a bucketed expected calibration error of 0.62 years. While not intended for clinical deployment, these results highlight the potential of extracting medically relevant signals from images. We make all code and datasets available to facilitate further research.', 'score': 0, 'issue_id': 4331, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'ce0b4c81ed1c657b', 'authors': ['Tristan Kenneweg', 'Philip Kenneweg', 'Barbara Hammer'], 'affiliations': ['University of Bielefeld'], 'pdf_title_img': 'assets/pdf/title_img/2506.13430.jpg', 'data': {'categories': ['#optimization', '#dataset', '#healthcare', '#science', '#open_source', '#cv'], 'emoji': '🔮', 'ru': {'title': 'ИИ предсказывает продолжительность жизни по фото', 'desc': 'Исследователи разработали метод, использующий предобученные модели Vision Transformer для оценки оставшейся продолжительности жизни по изображениям лица и тела. Модель достигает наилучшей точности предсказания со средней абсолютной ошибкой 4.79-7.48 лет на разных наборах данных. Важно, что модель предоставляет хорошо калиброванные оценки неопределенности. Хотя метод не предназначен для клинического применения, он демонстрирует потенциал извлечения медицински значимых сигналов из изображений.'}, 'en': {'title': 'Transforming Images into Lifespan Predictions with Uncertainty', 'desc': "This paper presents a method using vision transformer models to predict remaining lifespan from facial and whole-body images. The approach not only provides accurate lifespan estimates but also quantifies uncertainty in these predictions, which is crucial for understanding the reliability of the model's outputs. By learning a Gaussian distribution for each sample, the model effectively captures how uncertainty varies with the true remaining lifespan. The results show a significant improvement in prediction accuracy on new datasets, emphasizing the potential of using image analysis for health screening."}, 'zh': {'title': '从图像预测剩余寿命的创新方法', 'desc': '本文提出了一种利用预训练的视觉变换器模型，从面部和全身图像中预测剩余寿命的方法。该方法不仅具有高准确性，还能有效量化预测的不确定性。研究表明，预测的不确定性与真实的剩余寿命之间存在系统性的变化，并且可以通过为每个样本学习高斯分布来有效建模。我们的模型在多个数据集上实现了最先进的平均绝对误差，展示了从图像中提取医学相关信号的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.13172', 'title': 'Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging\n  Unsubstantiated Claims and Ambiguous Pronouns', 'url': 'https://huggingface.co/papers/2506.13172', 'abstract': "Structured workflow prompts improve hierarchical reasoning in LLMs for scholarly manuscript analysis, but their effectiveness varies with the model, task type, and context.  \t\t\t\t\tAI-generated summary \t\t\t\t We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks: identifying unsubstantiated claims in summaries (informational integrity) and flagging ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. In a summary-only setting, however, ChatGPT achieved a perfect (100%) success rate, while Gemini's performance was substantially degraded. Our findings suggest that structured prompting is a viable methodology for complex textual analysis but show that prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing.", 'score': 0, 'issue_id': 4328, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '298ec00caed4ffd4', 'authors': ['Evgeny Markhasin'], 'affiliations': ['Lobachevsky State University of Nizhny Novgorod'], 'pdf_title_img': 'assets/pdf/title_img/2506.13172.jpg', 'data': {'categories': ['#science', '#multimodal', '#training', '#reasoning', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Структурированные промпты улучшают анализ текста, но требуют индивидуального подхода', 'desc': 'Исследователи разработали структурированные рабочие процессы для улучшения иерархического рассуждения в больших языковых моделях (LLM) при анализе научных рукописей. Они оценили эффективность этих процессов на двух моделях (Gemini Pro 2.5 Pro и ChatGPT Plus o3) для задач выявления необоснованных утверждений и неоднозначных местоимений. Результаты показали, что эффективность структурированных промптов зависит от конкретной модели, типа задачи и контекста. Исследование подчеркивает необходимость тщательного тестирования для каждой конкретной модели и задачи.'}, 'en': {'title': 'Enhancing Scholarly Analysis with Structured Prompts in LLMs', 'desc': 'This paper explores how structured workflow prompts can enhance hierarchical reasoning in Large Language Models (LLMs) when analyzing scholarly manuscripts. The authors designed prompts to help models identify unsubstantiated claims and ambiguous pronoun references, which are crucial for maintaining informational integrity and linguistic clarity. They evaluated two advanced models, Gemini Pro 2.5 Pro and ChatGPT Plus o3, and found significant differences in their performance based on the task and context. The results indicate that while structured prompting can improve analysis, its effectiveness varies greatly depending on the model and the specific analytical task.'}, 'zh': {'title': '结构化提示提升学术分析中的层次推理', 'desc': '本研究探讨了结构化工作流程提示在大型语言模型（LLMs）中促进层次推理的效果，特别是在学术手稿分析中。我们设计了一系列概念验证的提示，旨在引导模型进行高水平的语义和语言分析，重点关注识别未证实的主张和模糊的代词引用。通过对两种前沿模型的系统评估，我们发现模型在不同任务和上下文中的表现差异显著，尤其是在处理句法角色时。研究结果表明，结构化提示是一种有效的复杂文本分析方法，但其效果受模型、任务类型和上下文的相互影响。'}}}, {'id': 'https://huggingface.co/papers/2506.12299', 'title': 'QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety', 'url': 'https://huggingface.co/papers/2506.12299', 'abstract': 'QGuard, a safety guard method using question prompting, effectively defends LLMs against harmful and multi-modal malicious prompts without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent advancements in Large Language Models(LLMs) have had a significant impact on a wide range of fields, from general domains to specialized areas. However, these advancements have also significantly increased the potential for malicious users to exploit harmful and jailbreak prompts for malicious attacks. Although there have been many efforts to prevent harmful prompts and jailbreak prompts, protecting LLMs from such malicious attacks remains an important and challenging task. In this paper, we propose QGuard, a simple yet effective safety guard method, that utilizes question prompting to block harmful prompts in a zero-shot manner. Our method can defend LLMs not only from text-based harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by diversifying and modifying guard questions, our approach remains robust against the latest harmful prompts without fine-tuning. Experimental results show that our model performs competitively on both text-only and multi-modal harmful datasets. Additionally, by providing an analysis of question prompting, we enable a white-box analysis of user inputs. We believe our method provides valuable insights for real-world LLM services in mitigating security risks associated with harmful prompts.', 'score': 0, 'issue_id': 4330, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': '52cafcd463738b45', 'authors': ['Taegyeong Lee', 'Jeonghwa Yoo', 'Hyoungseo Cho', 'Soo Yong Kim', 'Yunho Maeng'], 'affiliations': ['A.I.MATICS Inc.', 'Ewha Womans University', 'FnGuide Inc.', 'Safe Generative AI Lab, MODULABS'], 'pdf_title_img': 'assets/pdf/title_img/2506.12299.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#security', '#hallucinations'], 'emoji': '🛡️', 'ru': {'title': 'QGuard: Защита языковых моделей вопросами', 'desc': 'QGuard - это метод защиты больших языковых моделей (LLM) от вредоносных и мультимодальных атак с использованием вопросных промптов. Он работает без дополнительного обучения модели и эффективен как для текстовых, так и для мультимодальных вредоносных запросов. QGuard использует разнообразные защитные вопросы, что делает его устойчивым к новейшим вредоносным промптам. Экспериментальные результаты показывают конкурентоспособную эффективность метода на текстовых и мультимодальных наборах данных.'}, 'en': {'title': 'QGuard: Safeguarding LLMs with Smart Question Prompting', 'desc': 'QGuard is a novel safety guard method designed to protect Large Language Models (LLMs) from harmful and multi-modal malicious prompts without the need for fine-tuning. It employs question prompting to effectively block these harmful inputs in a zero-shot manner, making it versatile against various types of attacks. The method not only addresses text-based threats but also extends its defense to multi-modal prompt attacks, showcasing its robustness. Experimental results indicate that QGuard performs competitively across different datasets, providing valuable insights for enhancing the security of LLM services.'}, 'zh': {'title': 'QGuard：保护LLMs的安全防护新方法', 'desc': 'QGuard是一种使用问题提示的安全防护方法，能够有效防御大型语言模型（LLMs）对抗有害和多模态恶意提示，而无需进行微调。该方法通过零-shot的方式阻止有害提示，保护LLMs免受文本和多模态攻击。通过多样化和修改防护问题，QGuard在面对最新的有害提示时依然保持强大的鲁棒性。实验结果表明，该模型在文本和多模态有害数据集上表现出色，为实际的LLM服务提供了重要的安全风险缓解方案。'}}}, {'id': 'https://huggingface.co/papers/2506.12258', 'title': 'EgoPrivacy: What Your First-Person Camera Says About You?', 'url': 'https://huggingface.co/papers/2506.12258', 'abstract': "EgoPrivacy evaluates privacy risks in egocentric vision through a large-scale benchmark, revealing that foundation models can infer private information about camera wearers with high accuracy in zero-shot settings.  \t\t\t\t\tAI-generated summary \t\t\t\t While the rapid proliferation of wearable cameras has raised significant concerns about egocentric video privacy, prior work has largely overlooked the unique privacy threats posed to the camera wearer. This work investigates the core question: How much privacy information about the camera wearer can be inferred from their first-person view videos? We introduce EgoPrivacy, the first large-scale benchmark for the comprehensive evaluation of privacy risks in egocentric vision. EgoPrivacy covers three types of privacy (demographic, individual, and situational), defining seven tasks that aim to recover private information ranging from fine-grained (e.g., wearer's identity) to coarse-grained (e.g., age group). To further emphasize the privacy threats inherent to egocentric vision, we propose Retrieval-Augmented Attack, a novel attack strategy that leverages ego-to-exo retrieval from an external pool of exocentric videos to boost the effectiveness of demographic privacy attacks. An extensive comparison of the different attacks possible under all threat models is presented, showing that private information of the wearer is highly susceptible to leakage. For instance, our findings indicate that foundation models can effectively compromise wearer privacy even in zero-shot settings by recovering attributes such as identity, scene, gender, and race with 70-80% accuracy. Our code and data are available at https://github.com/williamium3000/ego-privacy.", 'score': 0, 'issue_id': 4332, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '80aaa11f21bae6d3', 'authors': ['Yijiang Li', 'Genpei Zhang', 'Jiacheng Cheng', 'Yi Li', 'Xiaojun Shan', 'Dashan Gao', 'Jiancheng Lyu', 'Yuan Li', 'Ning Bi', 'Nuno Vasconcelos'], 'affiliations': ['Qualcomm AI Research, an initiative of Qualcomm Technologies, Inc.', 'University of California, San Diego', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.12258.jpg', 'data': {'categories': ['#leakage', '#rag', '#ethics', '#benchmark'], 'emoji': '🕵️', 'ru': {'title': 'Эгоцентрическое зрение: скрытая угроза приватности', 'desc': 'Исследование EgoPrivacy оценивает риски конфиденциальности в эгоцентрическом зрении с помощью масштабного бенчмарка. Оно показывает, что фундаментальные модели могут с высокой точностью выводить личную информацию о носителях камер в режиме нулевого обучения. Работа охватывает три типа конфиденциальности: демографическую, индивидуальную и ситуационную, определяя семь задач для восстановления личной информации. Предложена новая стратегия атаки с использованием ретривала, которая повышает эффективность атак на демографическую конфиденциальность.'}, 'en': {'title': 'EgoPrivacy: Unveiling Hidden Risks in Wearable Camera Data', 'desc': 'EgoPrivacy is a benchmark designed to assess privacy risks associated with egocentric vision, particularly from wearable cameras. The study reveals that foundation models can accurately infer sensitive information about the camera wearer, such as identity and demographic details, even without prior training on specific data. It introduces a novel attack method called Retrieval-Augmented Attack, which enhances the effectiveness of privacy attacks by utilizing external video data. The results show that private information can be compromised with high accuracy, highlighting significant privacy concerns in the use of wearable cameras.'}, 'zh': {'title': 'EgoPrivacy：揭示自我中心视觉中的隐私风险', 'desc': 'EgoPrivacy是一个用于评估自我中心视觉隐私风险的大规模基准。研究表明，基础模型能够在零样本设置下高效推断摄像头佩戴者的私人信息。该研究定义了三种隐私类型，并提出了七个任务，以恢复从佩戴者身份到年龄组等不同层次的私人信息。此外，研究还提出了一种新颖的攻击策略，通过外部视频库增强人口统计隐私攻击的效果，显示出佩戴者的私人信息极易泄露。'}}}, {'id': 'https://huggingface.co/papers/2506.12148', 'title': "Hatevolution: What Static Benchmarks Don't Tell Us", 'url': 'https://huggingface.co/papers/2506.12148', 'abstract': 'Empirical evaluation reveals temporal misalignment in the robustness of language models on evolving hate speech benchmarks, highlighting the need for time-sensitive linguistic assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t Language changes over time, including in the hate speech domain, which evolves quickly following social dynamics and cultural shifts. While NLP research has investigated the impact of language evolution on model training and has proposed several solutions for it, its impact on model benchmarking remains under-explored. Yet, hate speech benchmarks play a crucial role to ensure model safety. In this paper, we empirically evaluate the robustness of 20 language models across two evolving hate speech experiments, and we show the temporal misalignment between static and time-sensitive evaluations. Our findings call for time-sensitive linguistic benchmarks in order to correctly and reliably evaluate language models in the hate speech domain.', 'score': 0, 'issue_id': 4333, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': 'f0e52e354d5e052b', 'authors': ['Chiara Di Bonaventura', 'Barbara McGillivray', 'Yulan He', 'Albert Meroño-Peñuela'], 'affiliations': ['Imperial College London', 'Kings College London'], 'pdf_title_img': 'assets/pdf/title_img/2506.12148.jpg', 'data': {'categories': ['#ethics', '#dataset', '#security', '#benchmark'], 'emoji': '⏳', 'ru': {'title': 'Время имеет значение: необходимость динамической оценки языковых моделей', 'desc': 'Исследование показывает временное несоответствие в устойчивости языковых моделей при оценке на эволюционирующих тестах по выявлению языка вражды. Авторы провели эмпирическую оценку 20 языковых моделей в двух экспериментах с изменяющимися во времени данными. Результаты выявили расхождение между статичными и учитывающими временной фактор методами оценки. Исследование подчеркивает необходимость разработки лингвистических тестов, чувствительных к временным изменениям, для корректной оценки языковых моделей в области выявления языка вражды.'}, 'en': {'title': 'Evolving Language, Evolving Benchmarks: Time-Sensitive Evaluations for Hate Speech Models', 'desc': "This paper investigates how language models perform on hate speech benchmarks that change over time. It highlights that as language evolves, especially in sensitive areas like hate speech, the effectiveness of static evaluations may not accurately reflect a model's robustness. The authors tested 20 different language models against two evolving hate speech datasets and found significant misalignment in their performance. The study emphasizes the importance of developing time-sensitive benchmarks to ensure that language models are evaluated accurately and safely in the context of evolving language."}, 'zh': {'title': '仇恨言论评估需与时俱进', 'desc': '本论文探讨了语言模型在不断变化的仇恨言论基准上的鲁棒性，发现了时间上的不一致性。随着社会动态和文化变迁，仇恨言论的语言也在不断演变。尽管自然语言处理研究已经关注语言演变对模型训练的影响，但对模型基准测试的影响仍然研究不足。我们建议在仇恨言论领域中采用时间敏感的语言基准，以便更准确地评估语言模型的安全性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (6)', '#agi (1)', '#alignment (5)', '#architecture (1)', '#audio', '#benchmark (15)', '#cv (1)', '#data (6)', '#dataset (12)', '#diffusion (1)', '#ethics (4)', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare (3)', '#inference (4)', '#interpretability (4)', '#leakage (1)', '#long_context (3)', '#low_resource', '#machine_translation (1)', '#math (2)', '#multilingual (1)', '#multimodal (13)', '#open_source (4)', '#optimization (11)', '#plp', '#rag (2)', '#reasoning (8)', '#rl (3)', '#rlhf', '#robotics', '#science (6)', '#security (2)', '#small_models (1)', '#story_generation', '#survey (2)', '#synthetic (3)', '#training (9)', '#transfer_learning', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-17 13:27',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-17 13:27')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-17 13:27')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    