
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 16 papers. June 17.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">17 июня</span> | <span id="title-articles-count">16 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-16.html">⬅️ <span id="prev-date">16.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-18.html">➡️ <span id="next-date">18.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'};
        let feedDateNext = {'ru': '18.06', 'en': '06/18', 'zh': '6月18日'};
        let feedDatePrev = {'ru': '16.06', 'en': '06/16', 'zh': '6月16日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.13585', 'title': 'MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention', 'url': 'https://huggingface.co/papers/2506.13585', 'abstract': "A hybrid-attention reasoning model called MiniMax-M1, featuring a Mixture-of-Experts architecture and lightning attention mechanism, is introduced for efficient long-input processing and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.", 'score': 111, 'issue_id': 4324, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '05163c188bd37051', 'authors': ['MiniMax', ':', 'Aili Chen', 'Aonian Li', 'Bangwei Gong', 'Binyang Jiang', 'Bo Fei', 'Bo Yang', 'Boji Shan', 'Changqing Yu', 'Chao Wang', 'Cheng Zhu', 'Chengjun Xiao', 'Chengyu Du', 'Chi Zhang', 'Chu Qiao', 'Chunhao Zhang', 'Chunhui Du', 'Congchao Guo', 'Da Chen', 'Deming Ding', 'Dianjun Sun', 'Dong Li', 'Enwei Jiao', 'Haigang Zhou', 'Haimo Zhang', 'Han Ding', 'Haohai Sun', 'Haoyu Feng', 'Huaiguang Cai', 'Haichao Zhu', 'Jian Sun', 'Jiaqi Zhuang', 'Jiaren Cai', 'Jiayuan Song', 'Jin Zhu', 'Jingyang Li', 'Jinhao Tian', 'Jinli Liu', 'Junhao Xu', 'Junjie Yan', 'Junteng Liu', 'Junxian He', 'Kaiyi Feng', 'Ke Yang', 'Kecheng Xiao', 'Le Han', 'Leyang Wang', 'Lianfei Yu', 'Liheng Feng', 'Lin Li', 'Lin Zheng', 'Linge Du', 'Lingyu Yang', 'Lunbin Zeng', 'Minghui Yu', 'Mingliang Tao', 'Mingyuan Chi', 'Mozhi Zhang', 'Mujie Lin', 'Nan Hu', 'Nongyu Di', 'Peng Gao', 'Pengfei Li', 'Pengyu Zhao', 'Qibing Ren', 'Qidi Xu', 'Qile Li', 'Qin Wang', 'Rong Tian', 'Ruitao Leng', 'Shaoxiang Chen', 'Shaoyu Chen', 'Shengmin Shi', 'Shitong Weng', 'Shuchang Guan', 'Shuqi Yu', 'Sichen Li', 'Songquan Zhu', 'Tengfei Li', 'Tianchi Cai', 'Tianrun Liang', 'Weiyu Cheng', 'Weize Kong', 'Wenkai Li', 'Xiancai Chen', 'Xiangjun Song', 'Xiao Luo', 'Xiao Su', 'Xiaobo Li', 'Xiaodong Han', 'Xinzhu Hou', 'Xuan Lu', 'Xun Zou', 'Xuyang Shen', 'Yan Gong', 'Yan Ma', 'Yang Wang', 'Yiqi Shi', 'Yiran Zhong', 'Yonghong Duan', 'Yongxiang Fu', 'Yongyi Hu', 'Yu Gao', 'Yuanxiang Fan', 'Yufeng Yang', 'Yuhao Li', 'Yulin Hu', 'Yunan Huang', 'Yunji Li', 'Yunzhi Xu', 'Yuxin Mao', 'Yuxuan Shi', 'Yuze Wenren', 'Zehan Li', 'Zelin Li', 'Zhanxu Tian', 'Zhengmao Zhu', 'Zhenhua Fan', 'Zhenzhen Wu', 'Zhichao Xu', 'Zhihang Yu', 'Zhiheng Lyu', 'Zhuo Jiang', 'Zibo Gao', 'Zijia Wu', 'Zijian Song', 'Zijun Sun'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.13585.jpg', 'data': {'categories': ['#rl', '#training', '#open_source', '#architecture', '#reasoning', '#long_context', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'MiniMax-M1: Гибридный ИИ для эффективной обработки сложных задач', 'desc': 'MiniMax-M1 - это гибридная модель рассуждений с архитектурой Mixture-of-Experts и механизмом молниеносного внимания. Модель поддерживает контекст длиной 1 миллион токенов и эффективно обрабатывает длинные входные данные. MiniMax-M1 обучена с помощью масштабного обучения с подкреплением на разнообразных задачах, включая реальные среды разработки программного обеспечения. Эксперименты показывают, что модель превосходит аналоги в сложных задачах программирования, использовании инструментов и работе с длинным контекстом.'}, 'en': {'title': 'Revolutionizing Long-Input Processing with MiniMax-M1', 'desc': 'MiniMax-M1 is a groundbreaking hybrid-attention reasoning model that utilizes a Mixture-of-Experts architecture and a lightning attention mechanism to efficiently handle long input sequences. With 456 billion parameters and the ability to process up to 1 million tokens, it significantly outperforms previous models in context length. The model is trained using a novel reinforcement learning algorithm called CISPO, which enhances training efficiency by clipping importance sampling weights. MiniMax-M1 demonstrates superior performance in complex tasks, particularly in software engineering and long-context applications, making it a valuable tool for various AI challenges.'}, 'zh': {'title': '高效长输入处理的混合注意力模型', 'desc': 'MiniMax-M1是一种混合注意力推理模型，采用混合专家架构和闪电注意力机制，旨在高效处理长输入和强化学习任务。该模型基于之前的MiniMax-Text-01模型，具有4560亿个参数，并支持高达100万个token的上下文长度。MiniMax-M1在复杂任务中表现出色，尤其是在软件工程和工具利用方面。我们还提出了一种新颖的强化学习算法CISPO，进一步提高了训练效率。'}}}, {'id': 'https://huggingface.co/papers/2506.11763', 'title': 'DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents', 'url': 'https://huggingface.co/papers/2506.11763', 'abstract': "DeepResearch Bench offers a benchmark framework to evaluate the capabilities of Deep Research Agents in terms of research quality and information retrieval accuracy across multiple fields.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.", 'score': 11, 'issue_id': 4324, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '197213635094ee83', 'authors': ['Mingxuan Du', 'Benfeng Xu', 'Chiwei Zhu', 'Xiaorui Wang', 'Zhendong Mao'], 'affiliations': ['MetastoneTechnology, Beijing, China', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.11763.jpg', 'data': {'categories': ['#open_source', '#science', '#agents', '#alignment', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Комплексная оценка ИИ-агентов для глубоких исследований', 'desc': 'DeepResearch Bench - это система оценки возможностей агентов глубоких исследований в области качества исследований и точности поиска информации. Бенчмарк включает 100 исследовательских задач уровня PhD по 22 различным областям, разработанных экспертами. Предложены две новые методологии оценки, хорошо согласующиеся с человеческим суждением: метод на основе эталонов и оценка возможностей поиска информации. DeepResearch Bench доступен в открытом доступе для ускорения разработки практических агентов на основе языковых моделей.'}, 'en': {'title': 'Benchmarking Deep Research Agents for Superior Research Quality', 'desc': "DeepResearch Bench is a new framework designed to evaluate the performance of Deep Research Agents (DRAs) in generating high-quality research outputs. It includes 100 PhD-level tasks across 22 fields, allowing for a comprehensive assessment of DRAs' capabilities in information retrieval and report synthesis. The framework introduces two innovative evaluation methods: one that uses reference-based criteria to judge report quality, and another that measures citation effectiveness and accuracy. By providing these tools, DeepResearch Bench aims to enhance the development of LLM-based agents and improve their research efficiency."}, 'zh': {'title': '评估深度研究代理的新基准', 'desc': 'DeepResearch Bench是一个基准框架，用于评估深度研究代理在研究质量和信息检索准确性方面的能力。该框架包含100个由领域专家精心设计的博士级研究任务，涵盖22个不同领域。我们提出了两种新方法来评估这些代理的能力，一种是基于参考的评估方法，另一种是评估信息检索和引用准确性的框架。通过开源DeepResearch Bench及其关键组件，我们希望加速基于大型语言模型的代理的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.13759', 'title': 'Discrete Diffusion in Large Language and Multimodal Models: A Survey', 'url': 'https://huggingface.co/papers/2506.13759', 'abstract': 'Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs) enable parallel generation and faster inference compared to autoregressive models through denoising-based strategies and full attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed.   The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025.   In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment.   Paper collection: https://github.com/LiQiiiii/DLLM-Survey', 'score': 10, 'issue_id': 4324, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '0a523ab9b7563360', 'authors': ['Runpeng Yu', 'Qi Li', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.13759.jpg', 'data': {'categories': ['#math', '#training', '#diffusion', '#inference', '#multimodal', '#survey'], 'emoji': '🧠', 'ru': {'title': 'Революция в языковом моделировании: дискретные диффузионные модели', 'desc': 'Статья представляет собой систематический обзор дискретных диффузионных языковых моделей (dLLMs) и мультимодальных языковых моделей (dMLLMs). В отличие от авторегрессионных моделей, dLLMs и dMLLMs используют параллельное декодирование и стратегию генерации на основе шумоподавления. Эти модели обеспечивают более быстрый вывод и лучшую контролируемость по сравнению с авторегрессионными аналогами. В работе рассматриваются математические основы, ключевые техники обучения и вывода, а также применения dLLMs и dMLLMs в различных областях.'}, 'en': {'title': 'Accelerating Language Generation with Discrete Diffusion Models', 'desc': 'This paper surveys Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs), highlighting their advantages over traditional autoregressive models. dLLMs and dMLLMs utilize a parallel decoding approach with full attention and denoising strategies, allowing for faster generation and improved output control. The paper reviews the historical development, mathematical foundations, and key techniques for training these models, as well as their applications in various domains. It also discusses the future potential of dLLMs and dMLLMs in advancing machine learning research and deployment.'}, 'zh': {'title': '离散扩散模型：加速生成与控制的未来', 'desc': '本文系统性地调查了离散扩散语言模型（dLLMs）和离散扩散多模态语言模型（dMLLMs）。与自回归模型不同，dLLMs和dMLLMs采用多标记并行解码的范式，利用全注意力机制和去噪生成策略，从而实现并行生成和更快的推理速度。这种新方法使得细粒度的输出控制和动态响应感知成为可能，这在自回归模型中是难以实现的。研究表明，dLLMs和dMLLMs在推理速度上可实现高达10倍的加速，且在多个领域的应用中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.12915', 'title': 'PersonaFeedback: A Large-scale Human-annotated Benchmark For\n  Personalization', 'url': 'https://huggingface.co/papers/2506.12915', 'abstract': "A new benchmark, PersonaFeedback, evaluates Large Language Models' ability to generate personalized responses given explicit user personas, revealing limitations in current systems.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid improvement in the general capabilities of LLMs, LLM personalization, i.e., how to build LLM systems that can generate personalized responses or services that are tailored to distinct user personas, has become an increasingly important research and engineering problem. However, unlike many new challenging benchmarks being released for evaluating the general/reasoning capabilities, the lack of high-quality benchmarks for evaluating LLM personalization greatly hinders progress in this field. To address this, we introduce PersonaFeedback, a new benchmark that directly evaluates LLMs' ability to provide personalized responses given pre-defined user personas and queries. Unlike existing benchmarks that require models to infer implicit user personas from historical interactions, PersonaFeedback decouples persona inference from personalization, focusing on evaluating the model's ability to generate responses tailored to explicit personas. PersonaFeedback consists of 8298 human-annotated test cases, which are categorized into easy, medium, and hard tiers based on the contextual complexity of the user personas and the difficulty in distinguishing subtle differences between two personalized responses. We conduct comprehensive evaluations across a wide range of models. The empirical results reveal that even state-of-the-art LLMs that can solve complex real-world reasoning tasks could fall short on the hard tier of PersonaFeedback where even human evaluators may find the distinctions challenging. Furthermore, we conduct an in-depth analysis of failure modes across various types of systems, demonstrating that the current retrieval-augmented framework should not be seen as a de facto solution for personalization tasks. All benchmark data, annotation protocols, and the evaluation pipeline will be publicly available to facilitate future research on LLM personalization.", 'score': 10, 'issue_id': 4325, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': '7f12645dbf1aa58d', 'authors': ['Meiling Tao', 'Chenghao Zhu', 'Dongyi Ding', 'Tiannan Wang', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['OPPO', 'South China Agricultural University', 'The Chinese University of Hong Kong, Shenzhen', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.12915.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#multimodal', '#benchmark'], 'emoji': '🎭', 'ru': {'title': 'PersonaFeedback: новый стандарт оценки персонализации языковых моделей', 'desc': 'Представлен новый бенчмарк PersonaFeedback для оценки способности больших языковых моделей (LLM) генерировать персонализированные ответы на основе явно заданных пользовательских персон. Бенчмарк состоит из 8298 аннотированных тестовых примеров, разделенных на легкие, средние и сложные уровни. Результаты показывают, что даже современные LLM, способные решать сложные задачи рассуждений, испытывают трудности на сложном уровне PersonaFeedback. Анализ выявил ограничения текущих подходов к персонализации LLM, включая retrieval-augmented фреймворки.'}, 'en': {'title': 'Enhancing Personalization in LLMs with PersonaFeedback', 'desc': 'The paper introduces PersonaFeedback, a new benchmark designed to assess the ability of Large Language Models (LLMs) to generate personalized responses based on explicit user personas. This benchmark addresses the gap in evaluating LLM personalization, which has been overlooked compared to general reasoning capabilities. PersonaFeedback includes 8,298 human-annotated test cases categorized by complexity, revealing that even advanced LLMs struggle with nuanced personalization tasks. The findings highlight the limitations of current models and emphasize the need for improved frameworks in LLM personalization.'}, 'zh': {'title': '个性化生成的新基准：PersonaFeedback', 'desc': '本文介绍了一个新的基准测试，名为PersonaFeedback，旨在评估大型语言模型（LLM）生成个性化响应的能力。随着LLM能力的快速提升，个性化生成已成为一个重要的研究问题，但缺乏高质量的基准测试限制了这一领域的进展。PersonaFeedback通过提供预定义的用户角色和查询，直接评估模型生成针对特定用户角色的响应能力。研究结果表明，即使是最先进的LLM在处理复杂的个性化任务时也可能表现不佳，强调了当前模型在个性化生成方面的局限性。'}}}, {'id': 'https://huggingface.co/papers/2506.08343', 'title': 'Wait, We Don\'t Need to "Wait"! Removing Thinking Tokens Improves\n  Reasoning Efficiency', 'url': 'https://huggingface.co/papers/2506.08343', 'abstract': 'NoWait suppresses explicit self-reflection tokens during inference to enhance efficiency in multimodal reasoning without reducing model utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning.', 'score': 9, 'issue_id': 4324, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'bc2b3e7cb2a8d002', 'authors': ['Chenlong Wang', 'Yuanning Feng', 'Dongping Chen', 'Zhaoyang Chu', 'Ranjay Krishna', 'Tianyi Zhou'], 'affiliations': ['University College London', 'University of Maryland', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.08343.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#inference', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение без лишних слов', 'desc': 'Исследование представляет метод NoWait, который подавляет токены явной саморефлексии в больших языковых моделях во время вывода. Это позволяет сократить длину цепочки рассуждений на 27-51% без ущерба для полезности модели. Эксперименты проводились на десяти бенчмарках, охватывающих задачи рассуждения с текстом, изображениями и видео. NoWait предлагает простое решение для повышения эффективности мультимодального рассуждения в ИИ-системах.'}, 'en': {'title': 'NoWait: Streamlining Multimodal Reasoning for Efficiency', 'desc': "The paper introduces NoWait, a method that improves the efficiency of multimodal reasoning models by suppressing explicit self-reflection tokens during inference. These tokens, like 'Wait' and 'Hmm', often lead to unnecessary verbosity and can slow down the reasoning process. By removing these tokens, NoWait significantly shortens the reasoning paths while maintaining the effectiveness of the model. The results from various benchmarks demonstrate that this approach can reduce the length of reasoning trajectories by up to 51%, making it a valuable enhancement for large reasoning models."}, 'zh': {'title': 'NoWait：提升多模态推理效率的创新方法', 'desc': 'NoWait是一种新方法，通过在推理过程中抑制显式自我反思的标记（如“等一下”和“嗯”），来提高多模态推理的效率，而不降低模型的实用性。研究表明，传统的推理模型在复杂推理时常常会出现过度思考，导致输出冗长且重复，影响效率。通过在十个基准测试中进行广泛实验，NoWait能够将思维链的长度减少27%到51%。因此，NoWait为高效且保持实用性的多模态推理提供了一种简单有效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2506.03968', 'title': 'From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding', 'url': 'https://huggingface.co/papers/2506.03968', 'abstract': 'The paper presents a method for generating diverse and complex instruction data for large language models using attributed grounding, achieving top performance on benchmarks with a large synthesized dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.', 'score': 8, 'issue_id': 4324, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '991991c3f686afa8', 'authors': ['Chiwei Zhu', 'Benfeng Xu', 'Xiaorui Wang', 'Zhendong Mao'], 'affiliations': ['Metastone Technology', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03968.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#alignment', '#data', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Синтез сложных инструкций для эффективного обучения языковых моделей', 'desc': 'Статья представляет метод генерации разнообразных и сложных инструкций для обучения больших языковых моделей (LLM) с использованием атрибутивного заземления. Авторы создали набор данных SynthQuestions, содержащий 1 миллион синтетических инструкций. Метод включает в себя нисходящий процесс атрибуции, который связывает реальные инструкции с конкретными пользователями, и восходящий процесс синтеза, использующий веб-документы для создания ситуаций и соответствующих инструкций. Модели, обученные на этом наборе данных, показали ведущие результаты на нескольких стандартных тестах.'}, 'en': {'title': 'Harnessing Attributed Grounding for Diverse Instruction Generation', 'desc': 'This paper introduces a novel method for generating diverse and complex instruction data for large language models (LLMs) through a technique called attributed grounding. The approach combines a top-down attribution process, which connects real-world instructions to specific user contexts, with a bottom-up synthesis process that creates meaningful instructions from web documents. By leveraging this framework, the authors successfully produce a large dataset of 1 million synthesized instructions, named SynthQuestions, which significantly enhances the performance of LLMs on various benchmarks. The results indicate that as more web data is utilized, the effectiveness of the models continues to improve, showcasing the importance of rich and varied instruction data for model alignment.'}, 'zh': {'title': '通过属性基础生成复杂指令数据，提升语言模型性能', 'desc': '本文提出了一种通过属性基础生成多样化和复杂的指令数据的方法，以提高大型语言模型的性能。该方法结合了自上而下的归因过程和自下而上的合成过程，能够有效地从真实指令和网络文档中生成有意义的指令。通过这种框架，我们构建了一个包含100万个指令的数据集SynthQuestions，并在多个基准测试中取得了领先的表现。该研究表明，利用丰富的网络文档可以大规模收集复杂的指令，从而提升模型的对齐能力。'}}}, {'id': 'https://huggingface.co/papers/2506.13654', 'title': 'Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning', 'url': 'https://huggingface.co/papers/2506.13654', 'abstract': 'Ego-R1, a reinforcement learning-based framework, uses a structured tool-augmented chain-of-thought process to reason over ultra-long egocentric videos, achieving better performance than existing methods by extending time coverage to a week.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e., in days and weeks) egocentric videos, which leverages a structured Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained via reinforcement learning (RL). Inspired by human problem-solving strategies, CoTT decomposes complex reasoning into modular steps, with the RL agent invoking specific tools, one per step, to iteratively and collaboratively answer sub-questions tackling such tasks as temporal retrieval and multi-modal understanding. We design a two-stage training paradigm involving supervised finetuning (SFT) of a pretrained language model using CoTT data and RL to enable our agent to dynamically propose step-by-step tools for long-range reasoning. To facilitate training, we construct a dataset called Ego-R1 Data, which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark, Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources. Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of understanding ultra-long egocentric videos, significantly extending the time coverage from few hours to a week.', 'score': 6, 'issue_id': 4326, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '52b4ddc8a62e646b', 'authors': ['Shulin Tian', 'Ruiqi Wang', 'Hongming Guo', 'Penghao Wu', 'Yuhao Dong', 'Xiuying Wang', 'Jingkang Yang', 'Hao Zhang', 'Hongyuan Zhu', 'Ziwei Liu'], 'affiliations': ['A*STAR, Singapore', 'S-Lab, Nanyang Technological University', 'Shanghai AI Lab', 'Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13654.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#training', '#multimodal', '#long_context', '#rl'], 'emoji': '🎥', 'ru': {'title': 'Ego-R1: Революция в анализе длительных эгоцентрических видео', 'desc': 'Ego-R1 - это новая система обработки сверхдлинных эгоцентрических видео, использующая структурированный процесс цепочки рассуждений с инструментами (Chain-of-Tool-Thought). Система применяет агента, обученного с помощью обучения с подкреплением, для декомпозиции сложных задач на модульные шаги. Ego-R1 использует двухэтапную парадигму обучения, включающую тонкую настройку предобученной языковой модели и обучение с подкреплением. Система значительно расширяет временной охват анализа видео с нескольких часов до недели, превосходя существующие методы.'}, 'en': {'title': 'Revolutionizing Video Understanding with Ego-R1', 'desc': 'Ego-R1 is a new framework designed to analyze ultra-long egocentric videos, which can last for days or even weeks. It employs a structured Chain-of-Tool-Thought (CoTT) process, allowing the system to break down complex reasoning tasks into manageable steps. The framework is powered by a reinforcement learning agent that learns to select and use specific tools for each step, enhancing its ability to answer questions about the video content. By training on a specially created dataset and evaluating on a new benchmark, Ego-R1 demonstrates improved performance in understanding long-duration videos compared to existing methods.'}, 'zh': {'title': 'Ego-R1：超长视频推理的新突破', 'desc': 'Ego-R1是一个基于强化学习的框架，旨在处理超长的自我中心视频。它采用了一种结构化的工具增强思维链（CoTT）过程，将复杂的推理分解为模块化步骤。通过强化学习训练的Ego-R1代理能够动态地提出逐步工具，以应对长时间范围内的推理任务。实验结果表明，Ego-R1在理解超长视频方面表现优异，时间覆盖范围从几小时扩展到一周。'}}}, {'id': 'https://huggingface.co/papers/2506.07961', 'title': 'BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning\n  with Vision-Language Models', 'url': 'https://huggingface.co/papers/2506.07961', 'abstract': 'BridgeVLA is a 3D vision-language-action model that projects 3D inputs to 2D images and uses 2D heatmaps for efficient and effective action prediction, outperforming baselines in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, leveraging pre-trained vision-language models (VLMs) for building vision-language-action (VLA) models has emerged as a promising approach to effective robot manipulation learning. However, only few methods incorporate 3D signals into VLMs for action prediction, and they do not fully leverage the spatial structure inherent in 3D data, leading to low sample efficiency. In this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D inputs to multiple 2D images, ensuring input alignment with the VLM backbone, and (2) utilizes 2D heatmaps for action prediction, unifying the input and output spaces within a consistent 2D image space. In addition, we propose a scalable pre-training method that equips the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning. Extensive experiments show the proposed method is able to learn 3D manipulation efficiently and effectively. BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks. In RLBench, it improves the average success rate from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better performance in challenging generalization settings, boosting the average success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It generalizes robustly in multiple out-of-distribution settings, including visual disturbances and unseen instructions. Remarkably, it is able to achieve a success rate of 96.8% on 10+ tasks with only 3 trajectories per task, highlighting its extraordinary sample efficiency. Project Website:https://bridgevla.github.io/', 'score': 6, 'issue_id': 4324, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '3fcf8d6329af3962', 'authors': ['Peiyan Li', 'Yixiang Chen', 'Hongtao Wu', 'Xiao Ma', 'Xiangnan Wu', 'Yan Huang', 'Liang Wang', 'Tao Kong', 'Tieniu Tan'], 'affiliations': ['ByteDance Seed', 'CASIA', 'FiveAges', 'NJU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2506.07961.jpg', 'data': {'categories': ['#rl', '#3d', '#games', '#optimization', '#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'BridgeVLA: Эффективное обучение роботов через проекцию 3D в 2D', 'desc': 'BridgeVLA - это новая модель машинного обучения для роботизированных манипуляций, объединяющая 3D-зрение, язык и действия. Она проецирует 3D-входы на 2D-изображения и использует 2D-тепловые карты для эффективного прогнозирования действий. Модель превосходит существующие методы в различных тестах, включая симуляции и эксперименты с реальными роботами. BridgeVLA демонстрирует высокую эффективность обучения и способность к обобщению в нестандартных ситуациях.'}, 'en': {'title': 'BridgeVLA: Bridging 3D Vision and Action with 2D Heatmaps', 'desc': "BridgeVLA is a novel model that integrates 3D vision with language and action prediction by projecting 3D inputs into 2D images. This approach allows it to utilize 2D heatmaps for more efficient action prediction, enhancing the model's performance in robot manipulation tasks. The model is pre-trained to predict these heatmaps, which helps it learn effectively from fewer samples. Extensive testing shows that BridgeVLA significantly outperforms existing methods in various benchmarks, demonstrating its robustness and efficiency in real-world applications."}, 'zh': {'title': 'BridgeVLA：高效的3D视觉-语言-动作模型', 'desc': 'BridgeVLA是一种3D视觉-语言-动作模型，它将3D输入投影到2D图像，并利用2D热图进行高效的动作预测。该模型通过将3D信号整合到视觉-语言模型中，充分利用了3D数据的空间结构，从而提高了样本效率。我们提出了一种可扩展的预训练方法，使得视觉-语言模型能够在下游策略学习之前预测2D热图。实验结果表明，BridgeVLA在多个基准测试中超越了现有的最先进方法，展现了卓越的学习效率和效果。'}}}, {'id': 'https://huggingface.co/papers/2506.13750', 'title': 'Test3R: Learning to Reconstruct 3D at Test Time', 'url': 'https://huggingface.co/papers/2506.13750', 'abstract': 'Test3R, a test-time learning technique for 3D reconstruction, enhances geometric accuracy by optimizing network consistency using self-supervised learning on image triplets.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense matching methods like DUSt3R regress pairwise pointmaps for 3D reconstruction. However, the reliance on pairwise prediction and the limited generalization capability inherently restrict the global geometric consistency. In this work, we introduce Test3R, a surprisingly simple test-time learning technique that significantly boosts geometric accuracy. Using image triplets (I_1,I_2,I_3), Test3R generates reconstructions from pairs (I_1,I_2) and (I_1,I_3). The core idea is to optimize the network at test time via a self-supervised objective: maximizing the geometric consistency between these two reconstructions relative to the common image I_1. This ensures the model produces cross-pair consistent outputs, regardless of the inputs. Extensive experiments demonstrate that our technique significantly outperforms previous state-of-the-art methods on the 3D reconstruction and multi-view depth estimation tasks. Moreover, it is universally applicable and nearly cost-free, making it easily applied to other models and implemented with minimal test-time training overhead and parameter footprint. Code is available at https://github.com/nopQAQ/Test3R.', 'score': 5, 'issue_id': 4324, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '68d521856e78273a', 'authors': ['Yuheng Yuan', 'Qiuhong Shen', 'Shizun Wang', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.13750.jpg', 'data': {'categories': ['#3d', '#training', '#optimization'], 'emoji': '🏛️', 'ru': {'title': 'Test3R: Повышение точности 3D-реконструкции через самообучение на тестовых данных', 'desc': 'Test3R - это новая техника обучения во время тестирования для 3D-реконструкции, которая улучшает геометрическую точность. Метод оптимизирует согласованность нейронной сети, используя самоконтролируемое обучение на триплетах изображений. Test3R генерирует реконструкции из пар изображений и максимизирует их геометрическую согласованность относительно общего изображения. Эксперименты показывают, что техника значительно превосходит предыдущие методы в задачах 3D-реконструкции и многоракурсной оценки глубины.'}, 'en': {'title': 'Boosting 3D Reconstruction Accuracy with Test3R', 'desc': "Test3R is a novel test-time learning approach designed to improve the accuracy of 3D reconstruction by leveraging self-supervised learning on image triplets. It addresses the limitations of traditional dense matching methods that rely on pairwise predictions, which can lead to inconsistencies in global geometry. By optimizing the network's output during testing, Test3R ensures that reconstructions from different image pairs maintain geometric consistency relative to a common reference image. This technique not only enhances performance on 3D reconstruction tasks but is also easy to implement and applicable to various models with minimal additional training requirements."}, 'zh': {'title': 'Test3R：提升3D重建精度的简单方法', 'desc': 'Test3R是一种用于3D重建的测试时学习技术，通过自监督学习优化网络一致性，从而提高几何精度。该方法利用图像三元组生成重建，确保不同图像对之间的一致性。Test3R的核心思想是在测试时最大化重建之间的几何一致性，确保模型输出在不同输入下保持一致。实验结果表明，Test3R在3D重建和多视角深度估计任务中显著优于现有的最先进方法，且适用性广泛，几乎不增加成本。'}}}, {'id': 'https://huggingface.co/papers/2506.10521', 'title': "Scientists' First Exam: Probing Cognitive Abilities of MLLM via\n  Perception, Understanding, and Reasoning", 'url': 'https://huggingface.co/papers/2506.10521', 'abstract': "Scientists' First Exam (SFE) benchmark assesses scientific cognitive capacities of Multimodal Large Language Models through perception, understanding, and comparative reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries.", 'score': 5, 'issue_id': 4325, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '3e2672b026127b5e', 'authors': ['Yuhao Zhou', 'Yiheng Wang', 'Xuming He', 'Ruoyao Xiao', 'Zhiwei Li', 'Qiantai Feng', 'Zijie Guo', 'Yuejin Yang', 'Hao Wu', 'Wenxuan Huang', 'Jiaqi Wei', 'Dan Si', 'Xiuqi Yao', 'Jia Bu', 'Haiwen Huang', 'Tianfan Fu', 'Shixiang Tang', 'Ben Fei', 'Dongzhan Zhou', 'Fenghua Ling', 'Yan Lu', 'Siqi Sun', 'Chenhui Li', 'Guanjie Zheng', 'Jiancheng Lv', 'Wenlong Zhang', 'Lei Bai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.10521.jpg', 'data': {'categories': ['#reasoning', '#science', '#multimodal', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Новый бенчмарк для оценки научного мышления ИИ', 'desc': "Учёные разработали новый бенчмарк под названием Scientists' First Exam (SFE) для оценки научных когнитивных способностей мультимодальных больших языковых моделей (MLLM). SFE оценивает восприятие, понимание и сравнительное рассуждение в научном контексте. Бенчмарк состоит из 830 экспертно проверенных пар вопросов-ответов по визуальным данным, охватывающих 66 мультимодальных задач в пяти важных научных дисциплинах. Эксперименты показали, что современные модели GPT-4 и InternVL-3 достигают лишь 34.08% и 26.52% соответственно на SFE, что указывает на значительный потенциал для улучшения MLLM в научной сфере."}, 'en': {'title': 'Enhancing Scientific Discovery with MLLMs: The SFE Benchmark', 'desc': "The Scientists' First Exam (SFE) benchmark evaluates the cognitive abilities of Multimodal Large Language Models (MLLMs) in scientific contexts. It focuses on three key areas: perception of scientific signals, understanding of scientific attributes, and comparative reasoning. The benchmark includes 830 expert-verified visual question-answering pairs across various multimodal tasks in five important scientific disciplines. Results show that leading models like GPT-o3 and InternVL-3 perform below expectations, indicating a need for improvement in their scientific reasoning capabilities."}, 'zh': {'title': '科学认知能力的新评估标准', 'desc': '科学家首次考试（SFE）基准测试评估多模态大型语言模型（MLLMs）的科学认知能力，主要通过感知、理解和比较推理三个方面进行评估。当前的科学基准主要关注MLLMs的知识理解能力，未能充分评估其感知和推理能力。SFE基准包含830个经过专家验证的视觉问答对，涵盖66个多模态任务，涉及五个高价值学科。实验结果显示，现有的最先进模型在SFE上的表现仍有很大提升空间，表明MLLMs在科学领域的应用潜力巨大。'}}}, {'id': 'https://huggingface.co/papers/2506.12953', 'title': 'Forecasting Time Series with LLMs via Patch-Based Prompting and\n  Decomposition', 'url': 'https://huggingface.co/papers/2506.12953', 'abstract': 'PatchInstruct enhances LLM forecasting quality through specialized prompting methods that include time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions.', 'score': 1, 'issue_id': 4324, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': 'fb2789e38592ff5a', 'authors': ['Mayank Bumb', 'Anshul Vemulapalli', 'Sri Harsha Vardhan Prasad Jella', 'Anish Gupta', 'An La', 'Ryan A. Rossi', 'Hongjie Chen', 'Franck Dernoncourt', 'Nesreen K. Ahmed', 'Yu Wang'], 'affiliations': ['Adobe', 'Dolby Labs', 'Intel', 'University of Massachusetts Amherst', 'University of Oregon'], 'pdf_title_img': 'assets/pdf/title_img/2506.12953.jpg', 'data': {'categories': ['#data', '#training', '#optimization'], 'emoji': '📈', 'ru': {'title': 'Точное прогнозирование временных рядов с помощью языковых моделей', 'desc': 'Статья представляет метод PatchInstruct для улучшения качества прогнозирования временных рядов с помощью больших языковых моделей (LLM). Метод использует специализированные техники промптинга, включая декомпозицию временных рядов, токенизацию на основе патчей и расширение данных с помощью похожих соседей. PatchInstruct позволяет LLM делать точные прогнозы без сложной архитектуры или масштабного дообучения. Это простой и гибкий подход, требующий минимальной предобработки данных.'}, 'en': {'title': 'Enhancing LLM Forecasting with Simple Prompting Techniques', 'desc': 'PatchInstruct is a method that improves the forecasting abilities of Large Language Models (LLMs) by using innovative prompting techniques. It incorporates time series decomposition to break down data into manageable parts, patch-based tokenization to efficiently handle input, and similarity-based neighbor augmentation to enhance predictions by considering related data points. This approach allows LLMs to perform time series forecasting without the need for extensive fine-tuning or complex architectures. Overall, PatchInstruct simplifies the process while boosting the accuracy of predictions in time series analysis.'}, 'zh': {'title': 'PatchInstruct：简化时间序列预测的有效方法', 'desc': 'PatchInstruct是一种增强大型语言模型（LLM）时间序列预测质量的方法。它通过专门的提示策略，如时间序列分解、基于补丁的标记化和相似性邻居增强，来实现这一目标。与以往需要大量微调的方法不同，PatchInstruct能够在不复杂重训练的情况下，灵活地进行时间序列预测。该方法保持了简单性，并且对数据的预处理要求最低。'}}}, {'id': 'https://huggingface.co/papers/2506.12623', 'title': 'MS4UI: A Dataset for Multi-modal Summarization of User Interface\n  Instructional Videos', 'url': 'https://huggingface.co/papers/2506.12623', 'abstract': 'A novel benchmark and dataset are proposed for multi-modal summarization of UI instructional videos, addressing the need for step-by-step executable instructions and key video frames.  \t\t\t\t\tAI-generated summary \t\t\t\t We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose a novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect a dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization.', 'score': 1, 'issue_id': 4324, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': 'ef83eb4ade9dc4bf', 'authors': ['Yuan Zang', 'Hao Tan', 'Seunghyun Yoon', 'Franck Dernoncourt', 'Jiuxiang Gu', 'Kushal Kafle', 'Chen Sun', 'Trung Bui'], 'affiliations': ['Adobe Research', 'Brown University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12623.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#video', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'Новый подход к сумматизации обучающих видео по UI', 'desc': 'Предложен новый бенчмарк и набор данных для мультимодальной сумматизации обучающих видео по пользовательским интерфейсам. Исследование направлено на создание пошаговых исполняемых инструкций и выделение ключевых кадров видео. Собран датасет из 2413 обучающих видео общей продолжительностью более 167 часов. Эксперименты показали, что современные методы мультимодальной сумматизации испытывают трудности с обработкой таких видео, что подчеркивает важность разработки новых подходов.'}, 'en': {'title': 'Enhancing Learning with UI Video Summarization', 'desc': 'This paper introduces a new benchmark and dataset specifically designed for multi-modal summarization of user interface (UI) instructional videos. The goal is to create efficient summaries that include step-by-step text instructions and key video frames, which are essential for effective learning. The authors highlight that existing benchmarks are inadequate for this purpose, as they focus on general video summarization rather than instructional content. Through extensive experiments on their dataset of 2,413 annotated UI instructional videos, they demonstrate that current multi-modal summarization techniques are not effective for this specific type of video, indicating a need for improved methods.'}, 'zh': {'title': '提升UI教学视频的多模态总结能力', 'desc': '本文提出了一种新的基准和数据集，用于多模态总结用户界面（UI）教学视频，旨在提供逐步可执行的指令和关键视频帧。现有的基准主要关注一般的语义级视频总结，无法满足教学视频中对逐步指令和插图的需求。我们收集了2413个UI教学视频的数据集，手动标注了视频分割、文本总结和视频总结，以便进行全面评估。实验结果表明，现有的多模态总结方法在UI视频总结上表现不佳，强调了开发新方法的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.12450', 'title': 'Language Surgery in Multilingual Large Language Models', 'url': 'https://huggingface.co/papers/2506.12450', 'abstract': "Research confirms natural representation alignment in large language models and introduces Inference-Time Language Control to enhance cross-lingual performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and languages, revolutionizing natural language processing. This paper investigates the naturally emerging representation alignment in LLMs, particularly in the middle layers, and its implications for disentangling language-specific and language-agnostic information. We empirically confirm the existence of this alignment, analyze its behavior in comparison to explicitly designed alignment models, and demonstrate its potential for language-specific manipulation without semantic degradation. Building on these findings, we propose Inference-Time Language Control (ITLC), a novel method that leverages latent injection to enable precise cross-lingual language control and mitigate language confusion in LLMs. Our experiments highlight ITLC's strong cross-lingual control capabilities while preserving semantic integrity in target languages. Furthermore, we demonstrate its effectiveness in alleviating the cross-lingual language confusion problem, which persists even in current large-scale LLMs, leading to inconsistent language generation. This work advances our understanding of representation alignment in LLMs and introduces a practical solution for enhancing their cross-lingual performance.", 'score': 1, 'issue_id': 4325, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': '6c05e4a1a8b705dc', 'authors': ['Joanito Agili Lopo', 'Muhammad Ravi Shulthan Habibi', 'Tack Hwa Wong', 'Muhammad Ilham Ghozali', 'Fajri Koto', 'Genta Indra Winata', 'Peerat Limkonchotiwat', 'Alham Fikri Aji', 'Samuel Cahyawijaya'], 'affiliations': ['AI Singapore', 'Capital One', 'Cohere', 'Kreasof AI', 'MBZUAI', 'SEACrowd', 'Universitas Indonesia'], 'pdf_title_img': 'assets/pdf/title_img/2506.12450.jpg', 'data': {'categories': ['#alignment', '#machine_translation', '#inference', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Улучшение кросс-языковых возможностей LLM через контроль скрытых представлений', 'desc': 'Исследование подтверждает естественное выравнивание представлений в больших языковых моделях (LLM), особенно в средних слоях. Авторы предлагают метод Inference-Time Language Control (ITLC), который использует латентную инъекцию для точного межъязыкового контроля. ITLC позволяет улучшить кросс-языковую производительность LLM и уменьшить языковую путаницу. Эксперименты показывают эффективность ITLC в сохранении семантической целостности при переключении между языками.'}, 'en': {'title': 'Enhancing Cross-Lingual Performance with Natural Representation Alignment', 'desc': 'This paper explores how large language models (LLMs) naturally align their representations across different languages, particularly in their middle layers. It confirms that this alignment allows for the separation of language-specific and language-agnostic information, which can be manipulated without losing meaning. The authors introduce a new technique called Inference-Time Language Control (ITLC) that uses latent injection to improve control over language generation in cross-lingual contexts. Their experiments show that ITLC effectively reduces language confusion while maintaining semantic integrity, enhancing the overall performance of LLMs in multilingual tasks.'}, 'zh': {'title': '提升跨语言性能的推理时语言控制', 'desc': '本研究确认了大型语言模型（LLMs）中自然出现的表示对齐现象，特别是在中间层的表现。我们实证验证了这种对齐的存在，并分析了其与显式设计的对齐模型的行为比较。基于这些发现，我们提出了一种新方法——推理时语言控制（ITLC），它利用潜在注入技术实现精确的跨语言控制。实验结果表明，ITLC在保持目标语言语义完整性的同时，显著提升了跨语言控制能力，解决了当前大型LLMs中存在的语言混淆问题。'}}}, {'id': 'https://huggingface.co/papers/2506.12189', 'title': "Supernova Event Dataset: Interpreting Large Language Model's Personality\n  through Critical Event Analysis", 'url': 'https://huggingface.co/papers/2506.12189', 'abstract': "The study evaluates various LLMs on diverse text tasks using a new dataset, revealing distinct personality traits and improving model interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications.", 'score': 1, 'issue_id': 4324, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '952c0d68aa23cbda', 'authors': ['Pranav Agarwal', 'Ioana Ciucă'], 'affiliations': ['Google Deep Research', 'Institute', 'Mila', 'Quebec AI', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12189.jpg', 'data': {'categories': ['#dataset', '#small_models', '#reasoning', '#long_context', '#interpretability', '#multimodal', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая личность искусственного интеллекта: новый подход к интерпретации языковых моделей', 'desc': "Исследование оценивает различные большие языковые модели (LLM) на разнообразных текстовых задачах с использованием нового набора данных, выявляя их отличительные личностные черты и улучшая интерпретируемость моделей. Авторы предлагают набор данных Supernova Event Dataset, содержащий разнообразные статьи, и используют его для оценки способности LLM извлекать и ранжировать ключевые события из текста. Анализ показывает, что разные модели демонстрируют различные подходы к рассуждению и анализу информации. Это исследование улучшает понимание 'личности' языковых моделей, делая их более удобными для широкого спектра приложений."}, 'en': {'title': 'Unveiling LLM Personalities for Better Interpretability', 'desc': 'This study investigates how different Large Language Models (LLMs) perform on various text tasks using a new dataset called the Supernova Event Dataset. The dataset includes a wide range of articles, allowing for the evaluation of LLMs in extracting and ranking key events, which requires complex reasoning and understanding of context. The research reveals distinct personality traits among the models, such as emotional reasoning in Orca 2 and strategic thinking in Qwen 2.5, enhancing our understanding of their decision-making processes. By using another LLM as a judge to assess these traits, the study improves the interpretability of models, making them more accessible for diverse applications.'}, 'zh': {'title': '揭示大型语言模型的个性特征', 'desc': '本研究评估了多种大型语言模型（LLMs）在不同文本任务上的表现，使用了一个新的数据集。通过分析模型的选择和分类事件，我们揭示了模型的个性特征，并提高了模型的可解释性。我们提出的超新星事件数据集包含多样的文章，帮助我们基准测试模型在提取和排序关键事件方面的能力。研究结果显示，不同模型在处理情感推理、战略分析和因果推理等方面表现出明显的个性差异。'}}}, {'id': 'https://huggingface.co/papers/2506.06366', 'title': 'AI Agent Behavioral Science', 'url': 'https://huggingface.co/papers/2506.06366', 'abstract': 'A new field, AI Agent Behavioral Science, is proposed to systematically study the behaviors of AI agents in diverse contexts, emphasizing external factors and their interactions, and addressing responsible AI aspects.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled the development of AI agents that exhibit increasingly human-like behaviors, including planning, adaptation, and social dynamics across diverse, interactive, and open-ended scenarios. These behaviors are not solely the product of the internal architectures of the underlying models, but emerge from their integration into agentic systems operating within specific contexts, where environmental factors, social cues, and interaction feedbacks shape behavior over time. This evolution necessitates a new scientific perspective: AI Agent Behavioral Science. Rather than focusing only on internal mechanisms, this perspective emphasizes the systematic observation of behavior, design of interventions to test hypotheses, and theory-guided interpretation of how AI agents act, adapt, and interact over time. We systematize a growing body of research across individual agent, multi-agent, and human-agent interaction settings, and further demonstrate how this perspective informs responsible AI by treating fairness, safety, interpretability, accountability, and privacy as behavioral properties. By unifying recent findings and laying out future directions, we position AI Agent Behavioral Science as a necessary complement to traditional model-centric approaches, providing essential tools for understanding, evaluating, and governing the real-world behavior of increasingly autonomous AI systems.', 'score': 1, 'issue_id': 4326, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '683be64d015db51c', 'authors': ['Lin Chen', 'Yunke Zhang', 'Jie Feng', 'Haoye Chai', 'Honglin Zhang', 'Bingbing Fan', 'Yibo Ma', 'Shiyuan Zhang', 'Nian Li', 'Tianhui Liu', 'Nicholas Sukiennik', 'Keyu Zhao', 'Yu Li', 'Ziyi Liu', 'Fengli Xu', 'Yong Li'], 'affiliations': ['Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.06366.jpg', 'data': {'categories': ['#agi', '#healthcare', '#ethics', '#multimodal', '#agents', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'От модели к поведению: новый взгляд на изучение ИИ-агентов', 'desc': 'Предлагается новая область исследований - поведенческая наука ИИ-агентов. Она направлена на систематическое изучение поведения ИИ-агентов в различных контекстах, уделяя особое внимание внешним факторам и их взаимодействию. Исследования охватывают индивидуальных агентов, многоагентные системы и взаимодействие человека с агентами. Этот подход дополняет традиционные модельно-ориентированные методы и предоставляет инструменты для понимания, оценки и управления поведением автономных ИИ-систем в реальном мире.'}, 'en': {'title': 'Understanding AI Behavior: A New Scientific Approach', 'desc': "The paper introduces AI Agent Behavioral Science, a new field focused on studying the behaviors of AI agents in various contexts. It highlights that these behaviors arise not just from the AI's internal design but also from interactions with their environment and social dynamics. The approach emphasizes systematic observation, hypothesis testing, and theory-driven analysis to understand how AI agents adapt and interact over time. This perspective also addresses responsible AI considerations, such as fairness and accountability, making it a vital complement to traditional model-centric methods."}, 'zh': {'title': '探索人工智能代理的行为科学', 'desc': '本文提出了一个新的领域——人工智能代理行为科学，旨在系统研究人工智能代理在不同环境中的行为。随着大型语言模型的发展，AI代理展现出越来越人性化的行为，如规划、适应和社交动态。这些行为不仅源于模型的内部结构，还受到环境因素、社交线索和互动反馈的影响。该领域强调对行为的系统观察和干预设计，以促进对AI代理行为的理解和负责任的应用。'}}}, {'id': 'https://huggingface.co/papers/2506.13752', 'title': 'Steering LLM Thinking with Budget Guidance', 'url': 'https://huggingface.co/papers/2506.13752', 'abstract': 'Budget guidance is a method that steers LLM reasoning within a targeted budget without fine-tuning and achieves improved efficiency and performance on math benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, a simple yet effective method for steering the reasoning process of LLMs toward a target budget without requiring any LLM fine-tuning. Our approach introduces a lightweight predictor that models a Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in a soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to a 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance.', 'score': 0, 'issue_id': 4325, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'c3d9b714e91736d6', 'authors': ['Junyan Li', 'Wenshuo Zhao', 'Yang Zhang', 'Chuang Gan'], 'affiliations': ['MIT-IBM Watson AI Lab', 'UMass Amherst', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13752.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#reasoning', '#math'], 'emoji': '💡', 'ru': {'title': 'Эффективное управление рассуждениями ИИ в рамках бюджета', 'desc': "Метод 'бюджетного руководства' позволяет управлять рассуждениями языковых моделей в рамках заданного бюджета без дополнительного обучения. Он использует легковесный предиктор, моделирующий гамма-распределение оставшейся длины рассуждения при генерации следующего токена. Этот подход обеспечивает естественный контроль длины рассуждения и значительно повышает эффективность использования токенов на сложных математических тестах. Метод также демонстрирует обобщающую способность на более широкий спектр задач и проявляет эмерджентные свойства, такие как оценка сложности вопросов."}, 'en': {'title': 'Steering LLM Reasoning with Budget Guidance for Efficiency and Performance', 'desc': 'This paper introduces a method called budget guidance, which helps large language models (LLMs) reason effectively within a specified budget of thinking tokens. By using a lightweight predictor that models a Gamma distribution, the method controls the reasoning length during the generation of each token without needing to fine-tune the LLM. This approach not only improves efficiency but also enhances performance on math benchmarks, achieving significant accuracy gains while using fewer tokens. Additionally, budget guidance shows versatility across different tasks and can even estimate the difficulty of questions.'}, 'zh': {'title': '预算引导：高效推理的新方法', 'desc': '预算引导是一种方法，可以在不进行微调的情况下，引导大型语言模型（LLM）在目标预算内进行推理，从而提高效率和性能。该方法通过引入一个轻量级预测器，建模剩余思考长度的伽马分布，来控制推理长度。预算引导确保生成过程遵循指定的思考预算，同时在数学基准测试中显著提高了令牌效率。与基线方法相比，在紧张预算下，预算引导在MATH-500基准上实现了高达26%的准确率提升，同时仅使用全思考模型63%的思考令牌。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (3)', '#agi (1)', '#alignment (4)', '#architecture (1)', '#audio', '#benchmark (9)', '#cv', '#data (2)', '#dataset (4)', '#diffusion (1)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (4)', '#interpretability (3)', '#leakage', '#long_context (3)', '#low_resource', '#machine_translation (1)', '#math (2)', '#multilingual (1)', '#multimodal (8)', '#open_source (2)', '#optimization (5)', '#plp', '#rag', '#reasoning (6)', '#rl (3)', '#rlhf', '#robotics', '#science (2)', '#security', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (6)', '#transfer_learning', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-17 04:21',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-17 04:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-17 04:21')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    