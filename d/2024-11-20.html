
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 16 papers. November 20.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            margin-top: 10px;
            margin-bottom: 10px;
            display: block;
            border-radius: 5px;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">20 ноября</span> | <span id="title-articles-count">16 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-11-19.html">⬅️ <span id="prev-date">19.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-11-21.html">➡️ <span id="next-date">21.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-11.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'};
        let feedDateNext = {'ru': '21.11', 'en': '11/21', 'zh': '11月21日'};
        let feedDatePrev = {'ru': '19.11', 'en': '11/19', 'zh': '11月19日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2411.11844', 'title': 'Generative World Explorer', 'url': 'https://huggingface.co/papers/2411.11844', 'abstract': 'Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state.In contrast, humans can imagine unseen parts of the world through a mental exploration and revise their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions, without necessitating the physical exploration of the world at all times. To achieve this human-like ability, we introduce the Generative World Explorer (Genex), an egocentric world exploration framework that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train Genex, we create a synthetic urban scene dataset, Genex-DB. Our experimental results demonstrate that (1) Genex can generate high-quality and consistent observations during long-horizon exploration of a large virtual physical world and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans.', 'score': 38, 'issue_id': 655, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 ноября', 'en': 'November 18', 'zh': '11月18日'}, 'hash': 'ad359ab18e626959', 'authors': ['Taiming Lu', 'Tianmin Shu', 'Alan Yuille', 'Daniel Khashabi', 'Jieneng Chen'], 'affiliations': ['Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2411.11844.jpg', 'data': {'categories': ['#agents', '#3d', '#synthetic', '#games', '#dataset', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Мысленное исследование мира для улучшения решений', 'desc': 'В статье рассматривается проблема планирования с частичным наблюдением в embodied AI. Авторы предлагают подход, который позволяет агенту мысленно исследовать 3D мир и обновлять свои представления о нём без физического взаимодействия. Для этого они разработали систему Generative World Explorer (Genex), которая использует синтетические данные для обучения. Эксперименты показали, что Genex может генерировать качественные наблюдения, которые помогают агенту принимать более обоснованные решения.'}, 'en': {'title': 'Imagine to Explore: Enhancing Decision-Making with Mental Simulations', 'desc': 'This paper addresses the challenge of planning in environments where an agent has incomplete information. Unlike traditional methods that rely on physical exploration, the authors propose a framework called Generative World Explorer (Genex) that enables agents to mentally simulate and explore their surroundings. By generating imagined observations, Genex allows agents to update their beliefs about the world without needing to physically navigate it. The results show that this approach leads to improved decision-making in complex environments, demonstrating the potential of mental exploration in embodied AI.'}, 'zh': {'title': '心理探索，智能决策的新方式', 'desc': '在具身人工智能中，部分观察的规划是一个重要挑战。大多数研究通过让智能体物理探索环境来更新对世界状态的信念，而我们提出的生成世界探索器（Genex）则允许智能体通过心理探索来想象未见的世界部分。Genex能够在大型3D世界中生成想象的观察，从而更新信念，帮助智能体在当前步骤做出更明智的决策。我们创建了一个合成城市场景数据集Genex-DB，并通过实验验证了Genex在长时间探索中的高质量观察生成能力。'}}}, {'id': 'https://huggingface.co/papers/2411.10640', 'title': 'BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices', 'url': 'https://huggingface.co/papers/2411.10640', 'abstract': 'The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile phones, as essential daily companions, represent the most effective and accessible deployment platform for MLLMs, enabling seamless integration into everyday tasks. However, deploying MLLMs on mobile phones presents challenges due to limitations in memory size and computational capability, making it difficult to achieve smooth and real-time processing without extensive optimization. In this paper, we present BlueLM-V-3B, an algorithm and system co-design approach specifically tailored for the efficient deployment of MLLMs on mobile platforms. To be specific, we redesign the dynamic resolution scheme adopted by mainstream MLLMs and implement system optimization for hardware-aware deployment to optimize model inference on mobile phones. BlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B features a language model with 2.7B parameters and a vision encoder with 400M parameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4 token/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight quantization. (3) Strong Performance: BlueLM-V-3B has attained the highest average score of 66.1 on the OpenCompass benchmark among models with leq 4B parameters and surpassed a series of models with much larger parameter sizes (e.g., MiniCPM-V-2.6, InternVL2-8B).', 'score': 30, 'issue_id': 652, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 ноября', 'en': 'November 16', 'zh': '11月16日'}, 'hash': '0366549d4347dfd2', 'authors': ['Xudong Lu', 'Yinghao Chen', 'Cheng Chen', 'Hui Tan', 'Boheng Chen', 'Yina Xie', 'Rui Hu', 'Guanxin Tan', 'Renshou Wu', 'Yan Hu', 'Yi Zeng', 'Lei Wu', 'Liuyang Bian', 'Zhaoxiong Wang', 'Long Liu', 'Yanzhou Yang', 'Han Xiao', 'Aojun Zhou', 'Yafei Wen', 'Xiaoxin Chen', 'Shuai Ren', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.10640.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#small_models', '#multimodal', '#inference', '#optimization'], 'emoji': '📱', 'ru': {'title': 'BlueLM-V-3B: Мощь больших языковых моделей в вашем кармане', 'desc': 'BlueLM-V-3B - это новый подход к развертыванию мультимодальных больших языковых моделей (MLLM) на мобильных устройствах. Он решает проблемы ограниченной памяти и вычислительной мощности смартфонов, оптимизируя алгоритм и систему для эффективной работы. Модель имеет компактный размер (3,1 млрд параметров), высокую скорость генерации (24,4 токена/с) и превосходную производительность по сравнению с более крупными моделями. BlueLM-V-3B демонстрирует потенциал для интеграции продвинутых ИИ-технологий в повседневную жизнь через мобильные устройства.'}, 'en': {'title': 'Efficient MLLMs for Mobile: BlueLM-V-3B Unleashed!', 'desc': 'This paper introduces BlueLM-V-3B, a multimodal large language model (MLLM) designed for efficient deployment on mobile devices. It addresses the challenges of limited memory and computational power by optimizing model inference through a redesigned dynamic resolution scheme and hardware-aware system optimizations. BlueLM-V-3B features a compact architecture with 2.7 billion parameters for the language model and 400 million for the vision encoder, ensuring fast processing speeds. The model achieves impressive performance metrics, outperforming larger models on benchmarks while maintaining a generation speed of 24.4 tokens per second.'}, 'zh': {'title': '高效部署多模态大语言模型的创新方案', 'desc': '这篇论文介绍了一种名为BlueLM-V-3B的多模态大语言模型（MLLM），旨在高效地在移动平台上部署。该模型具有2.7亿参数的语言模型和4亿参数的视觉编码器，能够在移动设备上实现快速生成。通过重新设计动态分辨率方案和进行硬件优化，BlueLM-V-3B在MediaTek Dimensity 9300处理器上达到了每秒24.4个标记的生成速度。该模型在OpenCompass基准测试中获得了66.1的最高平均分，超越了许多参数更大的模型。'}}}, {'id': 'https://huggingface.co/papers/2411.11504', 'title': 'Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering', 'url': 'https://huggingface.co/papers/2411.11504', 'abstract': 'The evolution of machine learning has increasingly prioritized the development of powerful models and more scalable supervision signals. However, the emergence of foundation models presents significant challenges in providing effective supervision signals necessary for further enhancing their capabilities. Consequently, there is an urgent need to explore novel supervision signals and technical approaches. In this paper, we propose verifier engineering, a novel post-training paradigm specifically designed for the era of foundation models. The core of verifier engineering involves leveraging a suite of automated verifiers to perform verification tasks and deliver meaningful feedback to foundation models. We systematically categorize the verifier engineering process into three essential stages: search, verify, and feedback, and provide a comprehensive review of state-of-the-art research developments within each stage. We believe that verifier engineering constitutes a fundamental pathway toward achieving Artificial General Intelligence.', 'score': 13, 'issue_id': 654, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 ноября', 'en': 'November 18', 'zh': '11月18日'}, 'hash': 'a48ecb5e8a1da0ae', 'authors': ['Xinyan Guan', 'Yanjiang Liu', 'Xinyu Lu', 'Boxi Cao', 'Ben He', 'Xianpei Han', 'Le Sun', 'Jie Lou', 'Bowen Yu', 'Yaojie Lu', 'Hongyu Lin'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2411.11504.jpg', 'data': {'categories': ['#rlhf', '#survey', '#training', '#agi'], 'emoji': '🔍', 'ru': {'title': 'Верификационная инженерия: новый путь к совершенствованию ИИ', 'desc': "В статье представлена концепция 'верификационной инженерии' - новой парадигмы для улучшения фундаментальных моделей машинного обучения. Авторы предлагают использовать автоматизированные верификаторы для проверки и обратной связи с моделями. Процесс разделен на три этапа: поиск, верификация и обратная связь. Исследователи считают, что этот подход может стать ключевым на пути к созданию искусственного общего интеллекта."}, 'en': {'title': 'Unlocking Foundation Models with Verifier Engineering', 'desc': 'This paper discusses the challenges of providing effective supervision signals for foundation models in machine learning. It introduces a new approach called verifier engineering, which uses automated verifiers to enhance the capabilities of these models. The process is divided into three stages: search, verify, and feedback, each aimed at improving model performance. The authors argue that this method is crucial for advancing towards Artificial General Intelligence.'}, 'zh': {'title': '验证器工程：迈向人工通用智能的新路径', 'desc': '本论文探讨了在基础模型时代，如何提供有效的监督信号以提升模型能力。我们提出了一种新的后训练范式——验证器工程，旨在利用自动化验证器进行验证任务，并为基础模型提供有意义的反馈。验证器工程的过程分为三个关键阶段：搜索、验证和反馈，并对每个阶段的最新研究进展进行了系统性回顾。我们认为，验证器工程是实现人工通用智能的重要途径。'}}}, {'id': 'https://huggingface.co/papers/2411.10836', 'title': 'AnimateAnything: Consistent and Controllable Animation for Video Generation', 'url': 'https://huggingface.co/papers/2411.10836', 'abstract': "We present a unified controllable video generation approach AnimateAnything that facilitates precise and consistent video manipulation across various conditions, including camera trajectories, text prompts, and user motion annotations. Specifically, we carefully design a multi-scale control feature fusion network to construct a common motion representation for different conditions. It explicitly converts all control information into frame-by-frame optical flows. Then we incorporate the optical flows as motion priors to guide final video generation. In addition, to reduce the flickering issues caused by large-scale motion, we propose a frequency-based stabilization module. It can enhance temporal coherence by ensuring the video's frequency domain consistency. Experiments demonstrate that our method outperforms the state-of-the-art approaches. For more details and videos, please refer to the webpage: https://yu-shaonian.github.io/Animate_Anything/.", 'score': 12, 'issue_id': 655, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 ноября', 'en': 'November 16', 'zh': '11月16日'}, 'hash': 'b979f7de4cf79a50', 'authors': ['Guojun Lei', 'Chi Wang', 'Hong Li', 'Rong Zhang', 'Yikai Wang', 'Weiwei Xu'], 'affiliations': ['Beihang University', 'State Key Lab of CAD&CG, Zhejiang University', 'Tsinghua University', 'Zhejiang Gongshang University'], 'pdf_title_img': 'assets/pdf/title_img/2411.10836.jpg', 'data': {'categories': ['#video'], 'emoji': '🎬', 'ru': {'title': 'Универсальная система для гибкой и точной генерации видео', 'desc': 'AnimateAnything - это унифицированный подход к управляемой генерации видео, позволяющий точно и последовательно манипулировать видео в различных условиях, включая траектории камеры, текстовые подсказки и пользовательские аннотации движения. Авторы разработали многомасштабную сеть слияния управляющих признаков для создания общего представления движения для различных условий. Метод преобразует всю управляющую информацию в покадровые оптические потоки и использует их в качестве априорной информации о движении для управления финальной генерацией видео. Для уменьшения мерцания при масштабных движениях предложен модуль стабилизации на основе частот, обеспечивающий согласованность видео в частотной области.'}, 'en': {'title': 'AnimateAnything: Mastering Video Generation with Precision Control', 'desc': 'The paper introduces AnimateAnything, a method for generating videos that allows for detailed control over various aspects like camera movement and user inputs. It uses a multi-scale control feature fusion network to create a unified motion representation that can adapt to different conditions. By converting control information into optical flows, the method guides the video generation process effectively. Additionally, a frequency-based stabilization module is implemented to minimize flickering and improve the smoothness of the final video output.'}, 'zh': {'title': '统一可控视频生成，精准操控每一帧', 'desc': '我们提出了一种统一的可控视频生成方法AnimateAnything，能够在不同条件下实现精确和一致的视频操控，包括相机轨迹、文本提示和用户运动注释。该方法设计了一个多尺度控制特征融合网络，以构建不同条件下的共同运动表示。它将所有控制信息显式转换为逐帧的光流，并将光流作为运动先验来指导最终的视频生成。此外，我们还提出了一种基于频率的稳定模块，以减少大规模运动引起的闪烁问题，从而增强视频的时间一致性。'}}}, {'id': 'https://huggingface.co/papers/2411.07641', 'title': 'Top-$nσ$: Not All Logits Are You Need', 'url': 'https://huggingface.co/papers/2411.07641', 'abstract': 'Large language models (LLMs) typically employ greedy decoding or low-temperature sampling for reasoning tasks, reflecting a perceived trade-off between diversity and accuracy. We challenge this convention by introducing top-nsigma, a novel sampling method that operates directly on pre-softmax logits by leveraging a statistical threshold. Our key insight is that logits naturally separate into a Gaussian-distributed noisy region and a distinct informative region, enabling efficient token filtering without complex probability manipulations. Unlike existing methods (e.g., top-p, min-p) that inadvertently include more noise tokens at higher temperatures, top-nsigma maintains a stable sampling space regardless of temperature scaling. We also provide a theoretical analysis of top-nsigma to better understand its behavior. The extensive experimental results across four reasoning-focused datasets demonstrate that our method not only outperforms existing sampling approaches but also surpasses greedy decoding, while maintaining consistent performance even at high temperatures.', 'score': 12, 'issue_id': 651, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': 'd3439bf0c336ac57', 'authors': ['Chenxia Tang', 'Jianchun Liu', 'Hongli Xu', 'Liusheng Huang'], 'affiliations': ['School of Computer Science and Technology, University of Science and Technology of China', 'Suzhou Institute for Advanced Research, University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2411.07641.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#reasoning'], 'emoji': '🎯', 'ru': {'title': 'Top-nsigma: эффективное сэмплирование для улучшения рассуждений языковых моделей', 'desc': 'В статье представлен новый метод сэмплирования для больших языковых моделей под названием top-nsigma. Этот метод работает напрямую с логитами перед софтмаксом, используя статистический порог для разделения шумовых и информативных токенов. Top-nsigma позволяет эффективно фильтровать токены без сложных манипуляций с вероятностями и сохраняет стабильное пространство сэмплирования независимо от температуры. Экспериментальные результаты показывают, что данный метод превосходит существующие подходы к сэмплированию и жадное декодирование на задачах рассуждения.'}, 'en': {'title': 'Top-NSigma: Enhancing Sampling for Better Reasoning in LLMs', 'desc': 'This paper introduces a new sampling method called top-nsigma for large language models (LLMs) that improves reasoning tasks. Unlike traditional methods that use greedy decoding or low-temperature sampling, top-nsigma filters tokens based on their pre-softmax logits, which are divided into informative and noisy regions. This approach allows for better token selection without the complications of probability adjustments, ensuring a stable sampling process across different temperature settings. Experimental results show that top-nsigma outperforms existing methods and maintains high performance even when using higher temperatures.'}, 'zh': {'title': '突破传统，提升推理性能的top-nsigma方法', 'desc': '本文提出了一种新的采样方法top-nsigma，旨在改善大语言模型在推理任务中的表现。该方法直接在预软最大值的logits上操作，通过统计阈值来进行有效的令牌过滤。我们的研究表明，logits可以自然地分为高斯分布的噪声区域和信息丰富的区域，从而避免了复杂的概率操作。实验结果显示，top-nsigma在多个推理数据集上超越了现有的采样方法和贪婪解码，且在高温度下仍能保持稳定的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.09944', 'title': 'SlimLM: An Efficient Small Language Model for On-Device Document Assistance', 'url': 'https://huggingface.co/papers/2411.09944', 'abstract': 'While small language models (SLMs) show promises for mobile deployment, their real-world performance and applications on smartphones remains underexplored. We present SlimLM, a series of SLMs optimized for document assistance tasks on mobile devices. Through extensive experiments on a Samsung Galaxy S24, we identify the optimal trade-offs between model size (ranging from 125M to 7B parameters), context length, and inference time for efficient on-device processing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on DocAssist, our constructed dataset for summarization, question answering and suggestion tasks. Our smallest model demonstrates efficient performance on S24, while larger variants offer enhanced capabilities within mobile constraints. We evaluate SlimLM against existing SLMs, showing comparable or superior performance and offering a benchmark for future research in on-device language models. We also provide an Android application, offering practical insights into SLM deployment. Our findings provide valuable insights and illuminate the capabilities of running advanced language models on high-end smartphones, potentially reducing server costs and enhancing privacy through on-device processing.', 'score': 10, 'issue_id': 655, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': 'ec53cf3813914219', 'authors': ['Thang M. Pham', 'Phat T. Nguyen', 'Seunghyun Yoon', 'Viet Dac Lai', 'Franck Dernoncourt', 'Trung Bui'], 'affiliations': ['Adobe Research', 'Auburn University', 'Georgia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2411.09944.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#small_models', '#open_source', '#dataset', '#benchmark'], 'emoji': '📱', 'ru': {'title': 'Эффективные языковые модели прямо в вашем кармане', 'desc': 'SlimLM - это серия малых языковых моделей, оптимизированных для задач помощи с документами на мобильных устройствах. Модели варьируются от 125 миллионов до 7 миллиардов параметров и оценивались на Samsung Galaxy S24. SlimLM предобучена на датасете SlimPajama-627B и дообучена на специально созданном наборе данных DocAssist для задач суммаризации, вопросно-ответных систем и генерации предложений. Исследование демонстрирует возможности запуска продвинутых языковых моделей на современных смартфонах, что потенциально снижает серверные затраты и повышает приватность за счет обработки на устройстве.'}, 'en': {'title': 'SlimLM: Efficient Language Models for Mobile Document Assistance', 'desc': 'This paper introduces SlimLM, a series of small language models designed specifically for mobile devices, focusing on document assistance tasks. The models are optimized for performance on smartphones, balancing size, context length, and inference time to ensure efficient on-device processing. SlimLM is pre-trained on a large dataset and fine-tuned for tasks like summarization and question answering, demonstrating effective performance even with the smallest model. The research highlights the potential of deploying advanced language models on smartphones, which can lower server costs and improve user privacy.'}, 'zh': {'title': 'SlimLM：移动设备上的高效语言模型', 'desc': '本论文介绍了一种名为SlimLM的小型语言模型系列，专为移动设备上的文档辅助任务优化。我们在三星Galaxy S24上进行了广泛实验，找到了模型大小、上下文长度和推理时间之间的最佳平衡，以实现高效的本地处理。SlimLM在SlimPajama-627B上进行预训练，并在我们构建的DocAssist数据集上进行微调，支持摘要、问答和建议等任务。我们的研究表明，SlimLM在移动设备上表现出色，能够降低服务器成本并增强隐私保护。'}}}, {'id': 'https://huggingface.co/papers/2411.10669', 'title': 'Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts', 'url': 'https://huggingface.co/papers/2411.10669', 'abstract': 'As the research of Multimodal Large Language Models (MLLMs) becomes popular, an advancing MLLM model is typically required to handle various textual and visual tasks (e.g., VQA, Detection, OCR, and ChartQA) simultaneously for real-world applications. However, due to the significant differences in representation and distribution among data from various tasks, simply mixing data of all tasks together leads to the well-known``multi-task conflict" issue, resulting in performance degradation across various tasks. To address this issue, we propose Awaker2.5-VL, a Mixture of Experts~(MoE) architecture suitable for MLLM, which acquires the multi-task capabilities through multiple sparsely activated experts. To speed up the training and inference of Awaker2.5-VL, each expert in our model is devised as a low-rank adaptation (LoRA) structure. Extensive experiments on multiple latest benchmarks demonstrate the effectiveness of Awaker2.5-VL. The code and model weight are released in our Project Page: https://github.com/MetabrainAGI/Awaker.', 'score': 8, 'issue_id': 654, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 ноября', 'en': 'November 16', 'zh': '11月16日'}, 'hash': 'f1319420ae85759e', 'authors': ['Jinqiang Long', 'Yanqi Dai', 'Guoxing Yang', 'Hongpeng Lin', 'Nanyi Fei', 'Yizhao Gao', 'Zhiwu Lu'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'Metabrain AGI Lab, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2411.10669.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#benchmark', '#optimization', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Awaker2.5-VL: Мультизадачная MLLM без конфликтов', 'desc': 'Статья представляет Awaker2.5-VL - новую архитектуру мультимодальной большой языковой модели (MLLM), основанную на принципе смеси экспертов (MoE). Модель решает проблему конфликта между задачами при обучении на разнородных данных. Каждый эксперт в системе реализован с использованием низкоранговой адаптации (LoRA) для ускорения обучения и вывода. Эксперименты показывают эффективность Awaker2.5-VL на современных бенчмарках.'}, 'en': {'title': 'Awaker2.5-VL: Mastering Multimodal Tasks with Expert Precision', 'desc': "This paper introduces Awaker2.5-VL, a new model designed to improve the performance of Multimodal Large Language Models (MLLMs) on various tasks like visual question answering and detection. The authors address the 'multi-task conflict' problem, which occurs when different tasks interfere with each other due to their diverse data representations. Awaker2.5-VL uses a Mixture of Experts (MoE) architecture, where only a subset of experts is activated for each task, allowing for better specialization and efficiency. Additionally, the model incorporates low-rank adaptation (LoRA) to enhance training speed and inference performance, showing promising results in extensive experiments."}, 'zh': {'title': '多模态任务的专家混合解决方案', 'desc': '本研究提出了一种名为Awaker2.5-VL的多模态大语言模型（MLLM），旨在同时处理文本和视觉任务，如视觉问答（VQA）、检测、光学字符识别（OCR）和图表问答（ChartQA）。为了克服多任务冲突问题，Awaker2.5-VL采用了专家混合（MoE）架构，通过多个稀疏激活的专家来实现多任务能力。每个专家被设计为低秩适应（LoRA）结构，以加速模型的训练和推理。大量实验结果表明，Awaker2.5-VL在多个最新基准测试中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2411.10510', 'title': 'SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.10510', 'abstract': 'Diffusion Transformers (DiT) have emerged as powerful generative models for various tasks, including image, video, and speech synthesis. However, their inference process remains computationally expensive due to the repeated evaluation of resource-intensive attention and feed-forward modules. To address this, we introduce SmoothCache, a model-agnostic inference acceleration technique for DiT architectures. SmoothCache leverages the observed high similarity between layer outputs across adjacent diffusion timesteps. By analyzing layer-wise representation errors from a small calibration set, SmoothCache adaptively caches and reuses key features during inference. Our experiments demonstrate that SmoothCache achieves 8% to 71% speed up while maintaining or even improving generation quality across diverse modalities. We showcase its effectiveness on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio, highlighting its potential to enable real-time applications and broaden the accessibility of powerful DiT models.', 'score': 8, 'issue_id': 654, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': '991f548fec1ec8c9', 'authors': ['Joseph Liu', 'Joshua Geddes', 'Ziyu Guo', 'Haomiao Jiang', 'Mahesh Kumar Nandwana'], 'affiliations': ['Queens University', 'Roblox'], 'pdf_title_img': 'assets/pdf/title_img/2411.10510.jpg', 'data': {'categories': ['#inference', '#multimodal', '#video', '#optimization', '#audio', '#diffusion'], 'emoji': '⚡', 'ru': {'title': 'SmoothCache: Быстрее и лучше с умным кэшированием для DiT', 'desc': 'Статья представляет SmoothCache - технику ускорения вывода для архитектур Diffusion Transformers (DiT). SmoothCache использует высокое сходство между выходными данными слоев на соседних временных шагах диффузии. Метод адаптивно кэширует и повторно использует ключевые признаки во время вывода, анализируя ошибки представления слоев на небольшом калибровочном наборе. Эксперименты показывают, что SmoothCache достигает ускорения от 8% до 71% при сохранении или даже улучшении качества генерации для различных модальностей.'}, 'en': {'title': 'Accelerating Diffusion Transformers with SmoothCache', 'desc': 'This paper presents SmoothCache, a technique designed to speed up the inference process of Diffusion Transformers (DiT), which are used for generating images, videos, and audio. The traditional inference method is slow because it requires many evaluations of complex attention and feed-forward layers. SmoothCache improves efficiency by caching and reusing similar outputs from adjacent diffusion timesteps, reducing the need for repeated calculations. Experiments show that this method can accelerate inference by 8% to 71% while maintaining or enhancing the quality of the generated content.'}, 'zh': {'title': 'SmoothCache：加速扩散变换器的推理过程', 'desc': '扩散变换器（DiT）是一种强大的生成模型，广泛应用于图像、视频和语音合成等任务。然而，它们的推理过程计算开销较大，因为需要重复评估资源密集型的注意力和前馈模块。为了解决这个问题，我们提出了SmoothCache，这是一种与模型无关的推理加速技术，利用相邻扩散时间步之间层输出的高度相似性。通过分析小型校准集中的层级表示误差，SmoothCache自适应地缓存和重用关键特征，从而在保持或提高生成质量的同时，实现了8%到71%的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2411.11767', 'title': 'Drowning in Documents: Consequences of Scaling Reranker Inference', 'url': 'https://huggingface.co/papers/2411.11767', 'abstract': 'Rerankers, typically cross-encoders, are often used to re-score the documents retrieved by cheaper initial IR systems. This is because, though expensive, rerankers are assumed to be more effective. We challenge this assumption by measuring reranker performance for full retrieval, not just re-scoring first-stage retrieval. Our experiments reveal a surprising trend: the best existing rerankers provide diminishing returns when scoring progressively more documents and actually degrade quality beyond a certain limit. In fact, in this setting, rerankers can frequently assign high scores to documents with no lexical or semantic overlap with the query. We hope that our findings will spur future research to improve reranking.', 'score': 8, 'issue_id': 652, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 ноября', 'en': 'November 18', 'zh': '11月18日'}, 'hash': '3fa06087787bc8d4', 'authors': ['Mathew Jacob', 'Erik Lindgren', 'Matei Zaharia', 'Michael Carbin', 'Omar Khattab', 'Andrew Drozdov'], 'affiliations': ['Databricks', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2411.11767.jpg', 'data': {'categories': ['#benchmark', '#data'], 'emoji': '🔍', 'ru': {'title': 'Неожиданные ограничения ранжировщиков в информационном поиске', 'desc': 'В статье исследуется эффективность ранжировщиков (rerankers) в информационном поиске. Авторы обнаружили, что при оценке большего количества документов качество ранжирования ухудшается. Более того, ранжировщики могут присваивать высокие оценки документам без лексического или семантического сходства с запросом. Исследование ставит под сомнение предположение о превосходстве ранжировщиков над более простыми системами поиска.'}, 'en': {'title': 'Rethinking Rerankers: Diminishing Returns in Document Scoring', 'desc': 'This paper investigates the effectiveness of rerankers, specifically cross-encoders, in the context of information retrieval (IR). Traditionally, rerankers are believed to enhance the quality of document scoring after an initial retrieval phase. However, the authors find that as more documents are scored, the performance of these rerankers diminishes and can even lead to poorer results. The study highlights that rerankers may assign high scores to irrelevant documents, suggesting a need for improved methods in reranking processes.'}, 'zh': {'title': '重排序器的有效性需重新审视', 'desc': '本文探讨了重排序器（通常是交叉编码器）在信息检索中的有效性。我们通过测量重排序器在完整检索中的表现，挑战了它们在初步检索后重新评分的假设。实验结果显示，现有的重排序器在评分越来越多的文档时，效果逐渐减弱，甚至在某个限度后质量下降。我们的发现希望能激励未来的研究，以改进重排序技术。'}}}, {'id': 'https://huggingface.co/papers/2411.11171', 'title': 'LLäMmlein: Compact and Competitive German-Only Language Models from Scratch', 'url': 'https://huggingface.co/papers/2411.11171', 'abstract': 'We create two German-only decoder models, LL\\"aMmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use. The model training involved several key steps, including extensive data preprocessing, the creation of a custom German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks. Throughout the training process, multiple checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor the models\' learning dynamics. Compared to state-of-the-art models on the SuperGLEBer benchmark, both LL\\"aMmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models\' quality scales with size as expected, but performance improvements on some tasks plateaued early, offering valuable insights into resource allocation for future model development.', 'score': 7, 'issue_id': 656, 'pub_date': '2024-11-17', 'pub_date_card': {'ru': '17 ноября', 'en': 'November 17', 'zh': '11月17日'}, 'hash': '3eea3fe6bac7ab51', 'authors': ['Jan Pfister', 'Julia Wunderle', 'Andreas Hotho'], 'affiliations': ['Data Science Chair Center for Artificial Intelligence and Data Science (CAIDAS) Julius-Maximilians-Universität Würzburg (JMU)'], 'pdf_title_img': 'assets/pdf/title_img/2411.11171.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#low_resource', '#open_source', '#training', '#multilingual'], 'emoji': '🇩🇪', 'ru': {'title': 'Немецкие языковые модели: от данных к декодеру', 'desc': 'Исследователи создали две модели декодера на немецком языке: LL"aMmlein 120M и 1B. Процесс включал обширную предобработку данных, создание специального немецкого токенизатора и обучение моделей. Модели оценивались на различных бенчмарках, включая SuperGLEBer, и показали конкурентоспособные результаты по сравнению с современными моделями аналогичного размера. Исследование предоставило ценные insights для будущей разработки языковых моделей.'}, 'en': {'title': 'Empowering German NLP with LL"aMmlein Models', 'desc': 'This paper presents two German-only decoder models, LL"aMmlein 120M and 1B, developed from scratch for the German NLP community. The training process included data preprocessing, a custom tokenizer, and evaluation against benchmarks like SuperGLEBer. The models demonstrated competitive performance, matching or exceeding state-of-the-art models of similar sizes. Insights from the training revealed that while model quality improves with size, some tasks show early performance plateaus, guiding future resource allocation in model development.'}, 'zh': {'title': '德语解码器模型的创新与共享', 'desc': '我们创建了两个仅支持德语的解码器模型，LL"aMmlein 120M和1B，并将其训练数据公开，供德语自然语言处理研究社区使用。模型训练包括多个关键步骤，如数据预处理、自定义德语分词器的创建、实际训练以及在各种基准上的最终模型评估。在训练过程中，我们保存并分析了多个检查点，使用SuperGLEBer基准监测模型的学习动态。与SuperGLEBer基准上的最先进模型相比，两个LL"aMmlein模型表现出竞争力，始终与相似参数大小的模型相匹配或超越。'}}}, {'id': 'https://huggingface.co/papers/2411.11045', 'title': 'StableV2V: Stablizing Shape Consistency in Video-to-Video Editing', 'url': 'https://huggingface.co/papers/2411.11045', 'abstract': 'Recent advancements of generative AI have significantly promoted content creation and editing, where prevailing studies further extend this exciting progress to video editing. In doing so, these studies mainly transfer the inherent motion patterns from the source videos to the edited ones, where results with inferior consistency to user prompts are often observed, due to the lack of particular alignments between the delivered motions and edited contents. To address this limitation, we present a shape-consistent video editing method, namely StableV2V, in this paper. Our method decomposes the entire editing pipeline into several sequential procedures, where it edits the first video frame, then establishes an alignment between the delivered motions and user prompts, and eventually propagates the edited contents to all other frames based on such alignment. Furthermore, we curate a testing benchmark, namely DAVIS-Edit, for a comprehensive evaluation of video editing, considering various types of prompts and difficulties. Experimental results and analyses illustrate the outperforming performance, visual consistency, and inference efficiency of our method compared to existing state-of-the-art studies.', 'score': 6, 'issue_id': 658, 'pub_date': '2024-11-17', 'pub_date_card': {'ru': '17 ноября', 'en': 'November 17', 'zh': '11月17日'}, 'hash': 'd4f742e7121f2322', 'authors': ['Chang Liu', 'Rui Li', 'Kaidong Zhang', 'Yunwei Lan', 'Dong Liu'], 'affiliations': ['University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2411.11045.jpg', 'data': {'categories': ['#optimization', '#games', '#video', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'StableV2V: редактирование видео с сохранением формы и согласованностью движения', 'desc': 'Статья представляет метод редактирования видео под названием StableV2V, который обеспечивает согласованность формы при редактировании. Метод разбивает процесс редактирования на несколько последовательных этапов, включая редактирование первого кадра и распространение изменений на остальные кадры. Авторы также создали тестовый набор данных DAVIS-Edit для оценки различных аспектов редактирования видео. Эксперименты показывают, что StableV2V превосходит существующие методы по качеству результатов, визуальной согласованности и эффективности вывода.'}, 'en': {'title': 'StableV2V: Aligning Motion with User Prompts for Consistent Video Editing', 'desc': 'This paper introduces StableV2V, a novel method for video editing that enhances the consistency between user prompts and the resulting video content. The approach involves breaking down the editing process into sequential steps, starting with the first frame and aligning the motion patterns with user instructions. By propagating the edited content across all frames based on this alignment, StableV2V achieves better visual coherence and performance. Additionally, the authors present a new benchmark, DAVIS-Edit, to evaluate the effectiveness of video editing methods under various conditions.'}, 'zh': {'title': '稳定一致的视频编辑方法', 'desc': '本文介绍了一种名为StableV2V的形状一致性视频编辑方法，旨在提高视频编辑的质量和一致性。该方法将整个编辑流程分解为多个顺序步骤，首先编辑第一帧视频，然后在用户提示和传递的运动之间建立对齐，最后根据这种对齐将编辑内容传播到其他帧。我们还创建了一个测试基准DAVIS-Edit，以全面评估视频编辑的效果，考虑了不同类型的提示和难度。实验结果表明，与现有的最先进研究相比，我们的方法在性能、视觉一致性和推理效率上均表现优越。'}}}, {'id': 'https://huggingface.co/papers/2411.09213', 'title': 'Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering', 'url': 'https://huggingface.co/papers/2411.09213', 'abstract': "Retrieval-augmented generation (RAG) has emerged as a promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates a completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of a reliable medical system. This paper addresses this gap by providing a comprehensive evaluation framework for medical question-answering (QA) systems in a RAG setting for these situations, including sufficiency, integration, and robustness. We introduce Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to four medical QA datasets for testing LLMs' ability to handle these specific scenarios. Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results reveals current models' limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs' reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain.", 'score': 6, 'issue_id': 655, 'pub_date': '2024-11-14', 'pub_date_card': {'ru': '14 ноября', 'en': 'November 14', 'zh': '11月14日'}, 'hash': 'e99e85d88963aa4c', 'authors': ['Nghia Trung Ngo', 'Chien Van Nguyen', 'Franck Dernoncourt', 'Thien Huu Nguyen'], 'affiliations': ['Adobe Research, USA', 'Department of Computer Science, University of Oregon, OR, USA'], 'pdf_title_img': 'assets/pdf/title_img/2411.09213.jpg', 'data': {'categories': ['#rag', '#survey', '#healthcare', '#open_source', '#benchmark', '#reasoning'], 'emoji': '🩺', 'ru': {'title': 'Новый бенчмарк для оценки надежности медицинских RAG-систем', 'desc': 'Статья представляет новый комплексный фреймворк оценки для медицинских систем вопросов и ответов, использующих retrieval-augmented generation (RAG). Авторы вводят бенчмарк MedRGB, который дополняет существующие наборы данных элементами для тестирования различных аспектов надежности системы. Проведены обширные эксперименты с коммерческими и открытыми языковыми моделями в разных условиях поиска. Результаты выявили ограниченные способности современных моделей справляться с шумом и дезинформацией в извлеченных документах.'}, 'en': {'title': 'Enhancing Medical QA with Robust Retrieval-Augmented Generation', 'desc': 'This paper introduces a new evaluation framework called Medical Retrieval-Augmented Generation Benchmark (MedRGB) to improve the performance of large language models (LLMs) in medical question-answering tasks. It highlights the need for accurate and trustworthy systems in the sensitive medical domain, addressing gaps in existing benchmarks that do not consider practical scenarios. The authors conduct extensive evaluations of both commercial and open-source LLMs, revealing their limitations in dealing with noise and misinformation in retrieved documents. The study also analyzes the reasoning processes of these models, providing insights for future improvements in retrieval-augmented generation systems for medical applications.'}, 'zh': {'title': '提升医疗问答系统的可靠性', 'desc': '检索增强生成（RAG）是一种有前景的方法，可以提高大型语言模型（LLMs）在知识密集型任务中的表现，尤其是在医疗领域。然而，医疗领域的敏感性要求系统必须完全准确和可信。现有的RAG基准主要关注标准的检索-回答设置，忽视了许多实际场景，这些场景对可靠医疗系统的关键方面进行了评估。本文提出了医疗检索增强生成基准（MedRGB），为四个医疗问答数据集提供了各种补充元素，以测试LLMs在特定场景下的处理能力。'}}}, {'id': 'https://huggingface.co/papers/2411.10499', 'title': 'FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on', 'url': 'https://huggingface.co/papers/2411.10499', 'abstract': 'Although image-based virtual try-on has made considerable progress, emerging approaches still encounter challenges in producing high-fidelity and robust fitting images across diverse scenarios. These methods often struggle with issues such as texture-aware maintenance and size-aware fitting, which hinder their overall effectiveness. To address these limitations, we propose a novel garment perception enhancement technique, termed FitDiT, designed for high-fidelity virtual try-on using Diffusion Transformers (DiT) allocating more parameters and attention to high-resolution features. First, to further improve texture-aware maintenance, we introduce a garment texture extractor that incorporates garment priors evolution to fine-tune garment feature, facilitating to better capture rich details such as stripes, patterns, and text. Additionally, we introduce frequency-domain learning by customizing a frequency distance loss to enhance high-frequency garment details. To tackle the size-aware fitting issue, we employ a dilated-relaxed mask strategy that adapts to the correct length of garments, preventing the generation of garments that fill the entire mask area during cross-category try-on. Equipped with the above design, FitDiT surpasses all baselines in both qualitative and quantitative evaluations. It excels in producing well-fitting garments with photorealistic and intricate details, while also achieving competitive inference times of 4.57 seconds for a single 1024x768 image after DiT structure slimming, outperforming existing methods.', 'score': 6, 'issue_id': 654, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': 'b142b4be26ef6147', 'authors': ['Boyuan Jiang', 'Xiaobin Hu', 'Donghao Luo', 'Qingdong He', 'Chengming Xu', 'Jinlong Peng', 'Jiangning Zhang', 'Chengjie Wang', 'Yunsheng Wu', 'Yanwei Fu'], 'affiliations': ['Fudan University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2411.10499.jpg', 'data': {'categories': ['#cv', '#optimization', '#3d', '#diffusion'], 'emoji': '👚', 'ru': {'title': 'FitDiT: Высококачественная виртуальная примерка с сохранением деталей', 'desc': 'Статья представляет новый метод виртуальной примерки одежды под названием FitDiT, основанный на диффузионных трансформерах. FitDiT улучшает сохранение текстур одежды с помощью экстрактора текстур и обучения в частотной области. Для решения проблемы подгонки размера используется стратегия расширенной маски. Метод превосходит существующие подходы по качеству и реалистичности генерируемых изображений.'}, 'en': {'title': 'FitDiT: Revolutionizing Virtual Try-On with High-Fidelity Garment Perception', 'desc': 'This paper presents FitDiT, a new technique for improving virtual try-on systems using Diffusion Transformers. It addresses challenges in producing realistic and well-fitting images by enhancing garment perception through a specialized texture extractor and frequency-domain learning. The method also incorporates a dilated-relaxed mask strategy to ensure garments fit correctly without oversizing. FitDiT demonstrates superior performance in generating high-fidelity images with intricate details while maintaining efficient processing times.'}, 'zh': {'title': 'FitDiT：高保真虚拟试穿的新突破', 'desc': '本文提出了一种新的服装感知增强技术，称为FitDiT，旨在提高虚拟试穿的高保真度。该方法利用扩散变换器（DiT）分配更多参数和注意力于高分辨率特征，以解决纹理感知维护和尺寸感知适配的问题。我们引入了服装纹理提取器和频域学习，增强了服装细节的捕捉能力，并采用扩张放松掩码策略来适应服装的正确长度。FitDiT在定性和定量评估中均超越了所有基线，能够生成具有真实感和复杂细节的合身服装。'}}}, {'id': 'https://huggingface.co/papers/2411.11024', 'title': 'VeGaS: Video Gaussian Splatting', 'url': 'https://huggingface.co/papers/2411.11024', 'abstract': 'Implicit Neural Representations (INRs) employ neural networks to approximate discrete data as continuous functions. In the context of video data, such models can be utilized to transform the coordinates of pixel locations along with frame occurrence times (or indices) into RGB color values. Although INRs facilitate effective compression, they are unsuitable for editing purposes. One potential solution is to use a 3D Gaussian Splatting (3DGS) based model, such as the Video Gaussian Representation (VGR), which is capable of encoding video as a multitude of 3D Gaussians and is applicable for numerous video processing operations, including editing. Nevertheless, in this case, the capacity for modification is constrained to a limited set of basic transformations. To address this issue, we introduce the Video Gaussian Splatting (VeGaS) model, which enables realistic modifications of video data. To construct VeGaS, we propose a novel family of Folded-Gaussian distributions designed to capture nonlinear dynamics in a video stream and model consecutive frames by 2D Gaussians obtained as respective conditional distributions. Our experiments demonstrate that VeGaS outperforms state-of-the-art solutions in frame reconstruction tasks and allows realistic modifications of video data. The code is available at: https://github.com/gmum/VeGaS.', 'score': 5, 'issue_id': 661, 'pub_date': '2024-11-17', 'pub_date_card': {'ru': '17 ноября', 'en': 'November 17', 'zh': '11月17日'}, 'hash': '0c6ed02c1597f4ff', 'authors': ['Weronika Smolak-Dyżewska', 'Dawid Malarz', 'Kornel Howil', 'Jan Kaczmarczyk', 'Marcin Mazur', 'Przemysław Spurek'], 'affiliations': ['Jagiellonian University Faculty of Mathematics and Computer Science'], 'pdf_title_img': 'assets/pdf/title_img/2411.11024.jpg', 'data': {'categories': ['#video', '#optimization', '#3d'], 'emoji': '🎬', 'ru': {'title': 'VeGaS: Революция в редактировании видео с помощью гауссовых распределений', 'desc': 'Статья представляет новую модель под названием Video Gaussian Splatting (VeGaS) для реалистичного редактирования видео. VeGaS использует семейство сложенных гауссовых распределений для захвата нелинейной динамики в видеопотоке. Модель превосходит современные решения в задачах реконструкции кадров. VeGaS позволяет проводить реалистичные модификации видеоданных, преодолевая ограничения предыдущих подходов.'}, 'en': {'title': 'VeGaS: Revolutionizing Video Editing with Gaussian Splatting', 'desc': 'Implicit Neural Representations (INRs) use neural networks to represent discrete data as continuous functions, particularly in video processing. They convert pixel coordinates and frame indices into RGB values, enabling effective data compression but limiting editing capabilities. The Video Gaussian Splatting (VeGaS) model enhances this by utilizing a new family of Folded-Gaussian distributions, allowing for realistic video modifications while capturing nonlinear dynamics. Our experiments show that VeGaS surpasses existing methods in frame reconstruction and supports a wider range of editing operations.'}, 'zh': {'title': '视频数据的真实修改新方法', 'desc': '隐式神经表示（INRs）使用神经网络将离散数据近似为连续函数。在视频数据中，这种模型可以将像素位置的坐标和帧出现时间转换为RGB颜色值。虽然INRs在压缩方面表现出色，但不适合编辑。我们提出的视频高斯点云（VeGaS）模型，利用新型的折叠高斯分布，能够实现视频数据的真实修改，并在帧重建任务中超越了现有的最先进解决方案。'}}}, {'id': 'https://huggingface.co/papers/2411.09661', 'title': 'Adaptive Decoding via Latent Preference Optimization', 'url': 'https://huggingface.co/papers/2411.09661', 'abstract': 'During language model decoding, it is known that using higher temperature sampling gives more creative responses, while lower temperatures are more factually accurate. However, such models are commonly applied to general instruction following, which involves both creative and fact seeking tasks, using a single fixed temperature across all examples and tokens. In this work, we introduce Adaptive Decoding, a layer added to the model to select the sampling temperature dynamically at inference time, at either the token or example level, in order to optimize performance. To learn its parameters we introduce Latent Preference Optimization (LPO) a general approach to train discrete latent variables such as choices of temperature. Our method outperforms all fixed decoding temperatures across a range of tasks that require different temperatures, including UltraFeedback, Creative Story Writing, and GSM8K.', 'score': 5, 'issue_id': 659, 'pub_date': '2024-11-14', 'pub_date_card': {'ru': '14 ноября', 'en': 'November 14', 'zh': '11月14日'}, 'hash': '4bb5fff16280c4bc', 'authors': ['Shehzaad Dhuliawala', 'Ilia Kulikov', 'Ping Yu', 'Asli Celikyilmaz', 'Jason Weston', 'Sainbayar Sukhbaatar', 'Jack Lanchantin'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.09661.jpg', 'data': {'categories': ['#training', '#optimization', '#story_generation', '#inference'], 'emoji': '🌡️', 'ru': {'title': 'Адаптивное декодирование: умный выбор температуры для языковых моделей', 'desc': 'Статья представляет новый метод адаптивного декодирования для языковых моделей, который динамически выбирает температуру сэмплирования во время вывода. Авторы вводят технику оптимизации латентных предпочтений (LPO) для обучения дискретных латентных переменных, таких как выбор температуры. Метод превосходит фиксированные температуры декодирования на ряде задач, требующих различных температур. Эксперименты проводились на датасетах UltraFeedback, Creative Story Writing и GSM8K.'}, 'en': {'title': 'Dynamic Temperature for Optimal Language Model Responses', 'desc': "This paper presents a new method called Adaptive Decoding, which allows language models to adjust their sampling temperature dynamically during inference. By varying the temperature, the model can balance creativity and factual accuracy based on the specific task at hand. The authors introduce Latent Preference Optimization (LPO) to effectively train the model's temperature selection process. Their approach shows improved performance over traditional fixed temperature methods across various tasks, demonstrating its versatility and effectiveness."}, 'zh': {'title': '自适应解码：动态选择温度优化模型表现', 'desc': '在语言模型解码过程中，使用较高的温度采样可以产生更具创意的响应，而较低的温度则更准确。本文提出了一种自适应解码方法，通过在推理时动态选择采样温度，优化模型在不同任务中的表现。我们引入了潜在偏好优化（LPO）来训练温度选择的离散潜变量。实验结果表明，我们的方法在需要不同温度的多种任务中表现优于固定解码温度。'}}}, {'id': 'https://huggingface.co/papers/2411.10168', 'title': "Evaluating the role of `Constitutions' for learning from AI feedback", 'url': 'https://huggingface.co/papers/2411.10168', 'abstract': "The growing capabilities of large language models (LLMs) have led to their use as substitutes for human feedback for training and assessing other LLMs. These methods often rely on `constitutions', written guidelines which a critic model uses to provide feedback and improve generations. We investigate how the choice of constitution affects feedback quality by using four different constitutions to improve patient-centered communication in medical interviews. In pairwise comparisons conducted by 215 human raters, we found that detailed constitutions led to better results regarding emotive qualities. However, none of the constitutions outperformed the baseline in learning more practically-oriented skills related to information gathering and provision. Our findings indicate that while detailed constitutions should be prioritised, there are possible limitations to the effectiveness of AI feedback as a reward signal in certain areas.", 'score': 3, 'issue_id': 661, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': '491ed277e2d6a217', 'authors': ['Saskia Redgate', 'Andrew M. Bean', 'Adam Mahdi'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2411.10168.jpg', 'data': {'categories': ['#rlhf', '#interpretability', '#alignment', '#healthcare'], 'emoji': '🤖', 'ru': {'title': 'Конституции LLM: возможности и ограничения в медицинской коммуникации', 'desc': "Исследование посвящено использованию больших языковых моделей (LLM) для обучения и оценки других LLM вместо человеческой обратной связи. Авторы изучали влияние выбора 'конституции' (письменных инструкций) на качество обратной связи в контексте улучшения коммуникации между врачом и пациентом. Результаты показали, что подробные конституции лучше справляются с эмоциональными аспектами, но не превосходят базовую модель в практических навыках сбора и предоставления информации. Исследование указывает на потенциальные ограничения эффективности обратной связи от ИИ в определенных областях."}, 'en': {'title': 'Enhancing AI Feedback with Detailed Guidelines', 'desc': "This paper explores how large language models (LLMs) can be trained and evaluated using human-like feedback, specifically through the use of 'constitutions'—guidelines that help a critic model assess and improve LLM outputs. The study tests four different constitutions to enhance patient-centered communication during medical interviews. Results from 215 human raters show that more detailed constitutions improve the emotive qualities of the communication, but they do not significantly enhance practical skills like information gathering. The findings suggest that while detailed guidelines are beneficial, there are limitations to using AI feedback as a reward signal for certain competencies."}, 'zh': {'title': '详细指导原则提升情感质量', 'desc': '本文探讨了大型语言模型（LLMs）在训练和评估其他LLMs时，如何作为人类反馈的替代品。研究中使用了四种不同的指导原则（constitution），以改善医疗访谈中的以患者为中心的沟通。通过215名人类评审者的对比实验，我们发现详细的指导原则在情感质量方面的反馈效果更好。然而，在信息收集和提供等实际技能的学习上，没有任何指导原则超越了基线表现。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (1)', '#agi (1)', '#alignment (1)', '#architecture (2)', '#audio (1)', '#benchmark (7)', '#cv (1)', '#data (2)', '#dataset (4)', '#diffusion (2)', '#ethics', '#games (2)', '#graphs', '#hallucinations', '#healthcare (2)', '#inference (4)', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (3)', '#open_source (4)', '#optimization (9)', '#plp', '#rag (1)', '#reasoning (2)', '#rl', '#rlhf (2)', '#robotics', '#science', '#security', '#small_models (2)', '#story_generation (1)', '#survey (2)', '#synthetic (1)', '#training (6)', '#transfer_learning', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <img class="article-pdf-title-img" src="https://hfday.ru/${item['pdf_title_img']}" />
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-11-20 02:17',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-11-20 02:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-11-20 02:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    