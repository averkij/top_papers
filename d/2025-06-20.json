{
    "date": {
        "ru": "20 июня",
        "en": "June 20",
        "zh": "6月20日"
    },
    "time_utc": "2025-06-20 14:11",
    "weekday": 4,
    "issue_id": 4405,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.14965",
            "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective",
            "url": "https://huggingface.co/papers/2506.14965",
            "abstract": "Guru, a diverse RL reasoning corpus, highlights domain-specific training needs and demonstrates improved performance in complex tasks for RL-enhanced LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domains--Math, Code, Science, Logic, Simulation, and Tabular--each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: https://github.com/LLM360/Reasoning360",
            "score": 10,
            "issue_id": 4399,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 июня",
                "en": "June 17",
                "zh": "6月17日"
            },
            "hash": "ecb3c8bbf20f8213",
            "authors": [
                "Zhoujun Cheng",
                "Shibo Hao",
                "Tianyang Liu",
                "Fan Zhou",
                "Yutao Xie",
                "Feng Yao",
                "Yuexin Bian",
                "Yonghao Zhuang",
                "Nilabjo Dey",
                "Yuheng Zha",
                "Yi Gu",
                "Kun Zhou",
                "Yuqi Wang",
                "Yuan Li",
                "Richard Fan",
                "Jianshu She",
                "Chengqian Gao",
                "Abulhair Saparov",
                "Haonan Li",
                "Taylor W. Killian",
                "Mikhail Yurochkin",
                "Zhengzhong Liu",
                "Eric P. Xing",
                "Zhiting Hu"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "MBZUAI",
                "Purdue University",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14965.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#open_source",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Guru: Расширение горизонтов рассуждений LLM с помощью обучения с подкреплением",
                    "desc": "Представлен корпус Guru для обучения с подкреплением (RL) в различных областях рассуждений для больших языковых моделей (LLM). Исследование показывает, что эффективность RL варьируется в зависимости от предметной области, причем некоторые области требуют специфического обучения. Разработаны модели Guru-7B и Guru-32B, демонстрирующие улучшенную производительность в сложных задачах рассуждения. Результаты указывают на потенциал RL не только для извлечения существующих знаний, но и для приобретения новых навыков в LLM."
                },
                "en": {
                    "title": "Unlocking Diverse Reasoning with Guru: A Reinforcement Learning Revolution",
                    "desc": "The paper introduces Guru, a comprehensive reinforcement learning (RL) reasoning corpus designed to enhance the reasoning capabilities of large language models (LLMs) across various domains. It addresses the challenge of limited RL reward signals by providing 92,000 examples in six distinct reasoning areas, ensuring reliable and effective training. The findings reveal that while some domains benefit from cross-domain training, others require specific in-domain training to improve performance, indicating that RL can lead to real skill development. The authors present two models, Guru-7B and Guru-32B, which achieve state-of-the-art results in RL training, outperforming existing models and enhancing their base performance on complex reasoning tasks."
                },
                "zh": {
                    "title": "Guru：提升RL在推理中的应用",
                    "desc": "本文介绍了Guru，一个多样化的强化学习（RL）推理语料库，强调了特定领域训练的需求，并展示了在复杂任务中对增强型大型语言模型（LLM）性能的提升。Guru包含92,000个可验证的示例，涵盖数学、代码、科学、逻辑、仿真和表格等六个推理领域，确保了RL训练的可靠性和有效性。研究表明，RL在不同领域的表现差异显著，某些领域（如数学和代码）可以通过跨领域训练获得更好的效果，而其他领域（如逻辑和仿真）则需要在特定领域内进行训练。最终，Guru-7B和Guru-32B模型在多个推理任务中表现出色，超越了现有的最佳基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.09827",
            "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion Detection",
            "url": "https://huggingface.co/papers/2506.09827",
            "abstract": "EmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration.",
            "score": 5,
            "issue_id": 4400,
            "pub_date": "2025-06-11",
            "pub_date_card": {
                "ru": "11 июня",
                "en": "June 11",
                "zh": "6月11日"
            },
            "hash": "38fbc70bf2ef023b",
            "authors": [
                "Christoph Schuhmann",
                "Robert Kaczmarczyk",
                "Gollam Rabby",
                "Felix Friedrich",
                "Maurice Kraus",
                "Kourosh Nadi",
                "Huu Nguyen",
                "Kristian Kersting",
                "Sören Auer"
            ],
            "affiliations": [
                "Centre for Cognitive Science",
                "DFKI",
                "Hessian.AI",
                "L3S Research Center Leibniz University of Hannover",
                "LAION e.V.",
                "Ontocord",
                "TIBLeibniz Information Centre for Science and Technology",
                "TU Darmstadt",
                "Technical University of Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.09827.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#science",
                    "#synthetic",
                    "#data",
                    "#audio"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Точное распознавание эмоций в речи с помощью синтетических данных",
                    "desc": "EmoNet-Voice - это новый ресурс для распознавания эмоций в речи, включающий крупномасштабный датасет для предобучения и набор данных для бенчмаркинга. Он предлагает оценку 40 эмоциональных категорий с различными уровнями интенсивности, используя синтетические аудиофрагменты, сгенерированные с помощью современных технологий синтеза речи. Датасет прошел тщательную валидацию экспертами-психологами и обеспечивает конфиденциальность данных. Авторы также представили модели Empathic Insight Voice, демонстрирующие высокое согласие с оценками экспертов в задаче распознавания эмоций в речи."
                },
                "en": {
                    "title": "EmoNet-Voice: Revolutionizing Speech Emotion Recognition with Synthetic Audio",
                    "desc": "EmoNet-Voice is a new resource that enhances speech emotion recognition (SER) by providing extensive pre-training and benchmark datasets. It includes EmoNet-Voice Big, which features over 4,500 hours of synthetic audio across multiple voices and languages, allowing for fine-grained emotion evaluation across 40 distinct categories. The dataset is designed to address privacy concerns and emotional granularity limitations found in existing SER datasets by using AI-generated audio that simulates real emotional expressions. Additionally, the paper introduces Empathic Insight Voice models that demonstrate high agreement with human expert evaluations, revealing insights into the detection of various emotional intensities."
                },
                "zh": {
                    "title": "EmoNet-Voice：细粒度语音情感识别的新标准",
                    "desc": "EmoNet-Voice是一个新的语音情感识别资源，提供了大规模的预训练和基准数据集。它包含超过4500小时的语音数据，涵盖11种声音、40种情感和4种语言，能够进行细粒度的情感评估。通过合成的音频片段，EmoNet-Voice模拟了演员表现特定情感的场景，并经过心理学专家的严格验证。该资源为情感识别模型设定了新的标准，尤其在高唤醒情感（如愤怒）与低唤醒状态（如专注）之间的检测上表现出显著差异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15154",
            "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
            "url": "https://huggingface.co/papers/2506.15154",
            "abstract": "SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.  \t\t\t\t\tAI-generated summary \t\t\t\t Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions.",
            "score": 3,
            "issue_id": 4397,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 июня",
                "en": "June 18",
                "zh": "6月18日"
            },
            "hash": "f2b380f0491c0add",
            "authors": [
                "Anuradha Chopra",
                "Abhinaba Roy",
                "Dorien Herremans"
            ],
            "affiliations": [
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15154.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#games",
                    "#multimodal",
                    "#architecture",
                    "#science",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "SonicVerse: Понимание музыки через многозадачное обучение",
                    "desc": "SonicVerse - это мультизадачная модель для создания описаний музыки, которая объединяет генерацию подписей с определением музыкальных характеристик. Модель использует проекционную архитектуру, преобразующую аудиовход в языковые токены и одновременно определяющую музыкальные особенности через вспомогательные выходы. Для обучения модели был расширен датасет MusicBench с помощью аннотаций музыкальных характеристик, полученных с использованием MIRFLEX. Экспериментальные результаты показывают, что включение музыкальных характеристик улучшает качество и детализацию генерируемых описаний."
                },
                "en": {
                    "title": "Enhancing Music Descriptions with SonicVerse",
                    "desc": "SonicVerse is a multi-task music captioning model that improves the quality of music descriptions by integrating audio feature detection. It captures both low-level acoustic details and high-level musical attributes through a projection-based architecture. This model generates detailed captions for short music pieces and can create time-informed descriptions for longer compositions by using a large-language model. By enhancing the training dataset with music features, SonicVerse demonstrates improved caption quality and detail in its outputs."
                },
                "zh": {
                    "title": "SonicVerse：提升音乐字幕质量的多任务模型",
                    "desc": "SonicVerse是一种多任务音乐字幕生成模型，结合了音频特征检测以提高字幕质量。该模型通过关键音调检测、声乐检测等辅助任务，捕捉音乐的低级声学细节和高级音乐属性。其关键贡献在于采用基于投影的架构，将音频输入转换为语言标记，同时通过专用辅助头检测音乐特征。实验结果表明，这种特征的结合显著提升了生成字幕的质量和细节。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14837",
            "title": "Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction",
            "url": "https://huggingface.co/papers/2506.14837",
            "abstract": "ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, multimodal large language models (MLLMs) have attracted increasing research attention due to their powerful visual understanding capabilities. While they have achieved impressive results on various vision tasks, their performance on chart-to-code generation remains suboptimal. This task requires MLLMs to generate executable code that can reproduce a given chart, demanding not only precise visual understanding but also accurate translation of visual elements into structured code. Directly prompting MLLMs to perform this complex task often yields unsatisfactory results. To address this challenge, we propose {ChartIR}, an iterative refinement method based on structured instruction. First, we distinguish two tasks: visual understanding and code translation. To accomplish the visual understanding component, we design two types of structured instructions: description and difference. The description instruction captures the visual elements of the reference chart, while the difference instruction characterizes the discrepancies between the reference chart and the generated chart. These instructions effectively transform visual features into language representations, thereby facilitating the subsequent code translation process. Second, we decompose the overall chart generation pipeline into two stages: initial code generation and iterative refinement, enabling progressive enhancement of the final output. Experimental results show that, compared to other method, our method achieves superior performance on both the open-source model Qwen2-VL and the closed-source model GPT-4o.",
            "score": 1,
            "issue_id": 4396,
            "pub_date": "2025-06-15",
            "pub_date_card": {
                "ru": "15 июня",
                "en": "June 15",
                "zh": "6月15日"
            },
            "hash": "3172095671c65e03",
            "authors": [
                "Chengzhi Xu",
                "Yuyang Wang",
                "Lai Wei",
                "Lichao Sun",
                "Weiran Huang"
            ],
            "affiliations": [
                "Lehigh University",
                "MIFA Lab, Shanghai Jiao Tong University",
                "Shanghai Innovation Institute",
                "State Key Laboratory of General Artificial Intelligence, BIGAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14837.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#interpretability",
                    "#optimization",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Точная генерация кода графиков с помощью структурированных инструкций и итеративного уточнения",
                    "desc": "ChartIR - это метод итеративного уточнения для улучшения производительности мультимодальных больших языковых моделей (MLLM) в задаче генерации кода по изображению графика. Метод разделяет задачи визуального понимания и перевода в код, используя структурированные инструкции для описания и сравнения графиков. ChartIR применяет двухэтапный подход: начальная генерация кода и итеративное уточнение. Эксперименты показали превосходство ChartIR над другими методами на моделях Qwen2-VL и GPT-4."
                },
                "en": {
                    "title": "ChartIR: Refining Code Generation from Charts with Structured Instructions",
                    "desc": "ChartIR is a novel approach that enhances the performance of multimodal large language models (MLLMs) in generating code from charts by separating the tasks of visual understanding and code translation. It employs structured instructions to guide the model in accurately interpreting visual elements and translating them into executable code. The method involves two main stages: initial code generation followed by iterative refinement, which allows for progressive improvements in the output. Experimental results demonstrate that ChartIR significantly outperforms existing methods on both open-source and closed-source models."
                },
                "zh": {
                    "title": "ChartIR：提升图表到代码生成的智能方法",
                    "desc": "ChartIR是一种通过结构化指令和迭代优化来提升多模态大语言模型（MLLM）在图表到代码生成任务中的表现的方法。该方法将视觉理解和代码翻译任务分开，首先通过描述和差异两种结构化指令来捕捉图表的视觉元素。接着，ChartIR将整体图表生成流程分为初始代码生成和迭代优化两个阶段，从而逐步提升最终输出的质量。实验结果表明，与其他方法相比，ChartIR在开源模型Qwen2-VL和闭源模型GPT-4o上均表现出更优的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-06-19.html",
    "link_next": "2025-06-23.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "19.06",
        "en": "06/19",
        "zh": "6月19日"
    },
    "short_date_next": {
        "ru": "23.06",
        "en": "06/23",
        "zh": "6月23日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 2,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    }
}