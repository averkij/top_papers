{
    "date": {
        "ru": "21 Ğ¼Ğ°Ñ",
        "en": "May 21",
        "zh": "5æœˆ21æ—¥"
    },
    "time_utc": "2025-05-21 00:55",
    "weekday": 2,
    "issue_id": 3867,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.11820",
            "title": "Chain-of-Model Learning for Language Model",
            "url": "https://huggingface.co/papers/2505.11820",
            "abstract": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the concept of Chain-of-Representation (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains) at the hidden dimension level. In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise Chain-of-Language-Model (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a KV sharing mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models. Our code will be released in the future at: https://github.com/microsoft/CoLM.",
            "score": 68,
            "issue_id": 3848,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ",
                "en": "May 17",
                "zh": "5æœˆ17æ—¥"
            },
            "hash": "2e8115f0fe78856b",
            "authors": [
                "Kaitao Song",
                "Xiaohua Wang",
                "Xu Tan",
                "Huiqiang Jiang",
                "Chengruidong Zhang",
                "Yongliang Shen",
                "Cen LU",
                "Zihao Li",
                "Zifan Song",
                "Caihua Shan",
                "Yansen Wang",
                "Kan Ren",
                "Xiaoqing Zheng",
                "Tao Qin",
                "Yuqing Yang",
                "Dongsheng Li",
                "Lili Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "Microsoft Research",
                "ShanghaiTech University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11820.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#inference",
                    "#agi",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "Ğ¦ĞµĞ¿Ğ½Ğ°Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Chain-of-Model (CoM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Chain-of-Representation (CoR), Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Chain-of-Language-Model (CoLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ¸Ğ´ĞµÑ CoM Ğ² ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ Transformer. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ CoLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Transformer, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Scaling Language Models with Chain-of-Model Efficiency",
                    "desc": "This paper introduces a new learning approach called Chain-of-Model (CoM), which enhances model training efficiency by incorporating causal relationships into the hidden states of each layer. It presents the Chain-of-Representation (CoR) concept, where hidden states are formed from multiple sub-representations, allowing each layer to only access its preceding chains. The CoM framework enables models to scale up by adding more chains, providing flexibility in deploying various sub-models of different sizes. The Chain-of-Language-Model (CoLM) and its variant CoLM-Air further optimize Transformer architectures by sharing key-value pairs across chains, resulting in improved performance and adaptability for language models."
                },
                "zh": {
                    "title": "é“¾å¼æ¨¡å‹ï¼šçµæ´»é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹æ–°èŒƒå¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºé“¾å¼æ¨¡å‹ï¼ˆCoMï¼‰ï¼Œå®ƒå°†å› æœå…³ç³»èå…¥æ¯ä¸€å±‚çš„éšè—çŠ¶æ€ï¼Œä»¥é“¾å¼ç»“æ„æé«˜æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œæ¨ç†çš„çµæ´»æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†é“¾å¼è¡¨ç¤ºï¼ˆCoRï¼‰çš„æ¦‚å¿µï¼Œå°†æ¯ä¸€å±‚çš„éšè—çŠ¶æ€è¡¨ç¤ºä¸ºå¤šä¸ªå­è¡¨ç¤ºçš„ç»„åˆï¼ˆå³é“¾ï¼‰ã€‚åœ¨æ¯ä¸€å±‚ä¸­ï¼Œè¾“å‡ºè¡¨ç¤ºçš„æ¯ä¸ªé“¾åªèƒ½æŸ¥çœ‹è¾“å…¥è¡¨ç¤ºä¸­æ‰€æœ‰å‰é¢çš„é“¾ï¼Œä»è€Œä½¿å¾—åŸºäºCoMæ¡†æ¶æ„å»ºçš„æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å¢åŠ é“¾çš„æ•°é‡é€æ­¥æ‰©å¤§æ¨¡å‹è§„æ¨¡ï¼Œå¹¶æä¾›ä¸åŒå¤§å°çš„å­æ¨¡å‹ä»¥å®ç°çµæ´»æ¨ç†ã€‚åŸºäºè¿™ä¸€åŸç†ï¼Œæˆ‘ä»¬è®¾è®¡äº†é“¾å¼è¯­è¨€æ¨¡å‹ï¼ˆCoLMï¼‰ï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†CoLM-Airï¼Œé€šè¿‡å¼•å…¥é”®å€¼å…±äº«æœºåˆ¶ï¼Œè®¡ç®—ç¬¬ä¸€ä¸ªé“¾ä¸­çš„æ‰€æœ‰é”®å’Œå€¼ï¼Œç„¶ååœ¨æ‰€æœ‰é“¾ä¹‹é—´å…±äº«ï¼Œä»è€Œå±•ç¤ºäº†é¢å¤–çš„å¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13417",
            "title": "AdaptThink: Reasoning Models Can Learn When to Think",
            "url": "https://huggingface.co/papers/2505.13417",
            "abstract": "Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https://github.com/THU-KEG/AdaptThink.",
            "score": 56,
            "issue_id": 3845,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "edd33223d8d833a7",
            "authors": [
                "Jiajie Zhang",
                "Nianyi Lin",
                "Lei Hou",
                "Ling Feng",
                "Juanzi Li"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13417.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ AdaptThink, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AdaptThink Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ» ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DeepSeek-R1-Distill-Qwen-1.5B Ğ½Ğ° 53% Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ» ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 2.4%."
                },
                "en": {
                    "title": "Optimize Reasoning with Adaptive Thinking Modes!",
                    "desc": "This paper introduces AdaptThink, a reinforcement learning algorithm designed to optimize reasoning models by allowing them to choose between two thinking modes: NoThinking and traditional thinking. NoThinking enables models to skip lengthy reasoning processes for simpler tasks, improving efficiency without sacrificing performance. AdaptThink employs a constrained optimization objective to encourage the use of NoThinking while maintaining overall accuracy, and it uses importance sampling to balance training between both modes. The results show that AdaptThink significantly reduces inference costs and enhances performance on math tasks, demonstrating the effectiveness of adaptive thinking-mode selection."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”æ€è€ƒæ¨¡å¼é€‰æ‹©ï¼Œæå‡æ¨ç†æ•ˆç‡ä¸è´¨é‡",
                    "desc": "æœ€è¿‘ï¼Œå¤§å‹æ¨ç†æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å†—é•¿çš„æ€è€ƒè¿‡ç¨‹æ˜¾è‘—å¢åŠ äº†æ¨ç†å¼€é”€ï¼Œå¯¼è‡´æ•ˆç‡æˆä¸ºç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºNoThinkingçš„æ–¹æ³•ï¼Œé¼“åŠ±æ¨ç†æ¨¡å‹è·³è¿‡æ€è€ƒï¼Œç›´æ¥ç”Ÿæˆæœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºç›¸å¯¹ç®€å•çš„ä»»åŠ¡ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†AdaptThinkï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æ ¹æ®é—®é¢˜éš¾åº¦è‡ªé€‚åº”é€‰æ‹©æœ€ä½³æ€è€ƒæ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒAdaptThinkæ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11896",
            "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.11896",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and decreased average response tokens by 69.06%, while maintaining high performance on complex tasks.",
            "score": 45,
            "issue_id": 3846,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ",
                "en": "May 17",
                "zh": "5æœˆ17æ—¥"
            },
            "hash": "bdc79864df7cbd51",
            "authors": [
                "Chenwei Lou",
                "Zewei Sun",
                "Xinnian Liang",
                "Meng Qu",
                "Wei Shen",
                "Wenqi Wang",
                "Yuntao Li",
                "Qingping Yang",
                "Shuangzhi Wu"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11896.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rlhf",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "AdaCoT: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "AdaCoT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought, CoT). Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Proximal Policy Optimization (PPO), AdaCoT Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ CoT. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²ĞºĞ»Ğ°Ğ´Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Selective Loss Masking (SLM), Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AdaCoT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ CoT Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Adaptive Reasoning for Efficient Language Models",
                    "desc": "This paper presents AdaCoT, a new framework that improves the efficiency of Large Language Models (LLMs) by adaptively deciding when to use Chain-of-Thought (CoT) prompting. Traditional CoT prompting can be computationally expensive, especially for simpler queries, but AdaCoT optimizes this by framing the decision to use CoT as a Pareto optimization problem. The authors employ reinforcement learning, specifically Proximal Policy Optimization (PPO), to dynamically adjust when CoT is triggered based on the complexity of the input. Their approach includes a technique called Selective Loss Masking (SLM) to ensure stable training, resulting in significant reductions in CoT usage while maintaining high performance on complex tasks."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”é“¾å¼æ¨ç†ï¼Œæå‡æ•ˆç‡ä¸æ€§èƒ½",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†AdaCoTï¼ˆè‡ªé€‚åº”é“¾å¼æ¨ç†ï¼‰ï¼Œå®ƒå…è®¸æ¨¡å‹æ ¹æ®è¾“å…¥çš„å¤æ‚æ€§è‡ªé€‚åº”åœ°å†³å®šæ˜¯å¦ä½¿ç”¨é“¾å¼æ¨ç†ã€‚æˆ‘ä»¬å°†è‡ªé€‚åº”æ¨ç†è§†ä¸ºä¸€ä¸ªå¸•ç´¯æ‰˜ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨å¹³è¡¡æ¨¡å‹æ€§èƒ½ä¸é“¾å¼æ¨ç†çš„è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaCoTåœ¨ä¸éœ€è¦å¤æ‚æ¨ç†çš„æŸ¥è¯¢ä¸­æ˜¾è‘—å‡å°‘äº†é“¾å¼æ¨ç†çš„ä½¿ç”¨ï¼Œæå‡äº†æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11254",
            "title": "Delta Attention: Fast and Accurate Sparse Attention Inference by Delta\n  Correction",
            "url": "https://huggingface.co/papers/2505.11254",
            "abstract": "The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills.",
            "score": 35,
            "issue_id": 3847,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "2aba31b686859e82",
            "authors": [
                "Jeffrey Willette",
                "Heejun Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11254.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#architecture",
                    "#inference",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ´Ğ²Ğ¸Ğ³ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ¾Ñ‚ ÑĞ´Ğ²Ğ¸Ğ³, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ… Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸."
                },
                "en": {
                    "title": "Boosting Sparse Attention: Aligning Outputs for Enhanced Performance",
                    "desc": "This paper addresses the inefficiencies of the attention mechanism in transformers, which typically has a quadratic complexity that increases inference costs for long sequences. It highlights that while sparse attention methods can reduce computation, they often lead to performance degradation due to a distributional shift in attention outputs. The authors propose a novel procedure to correct this shift, aligning sparse attention outputs more closely with those of traditional quadratic attention. Their method significantly improves performance, achieving an average increase of 36 percentage points while maintaining high sparsity and speed, making it much faster than existing methods."
                },
                "zh": {
                    "title": "ç¨€ç–æ³¨æ„åŠ›çš„åˆ†å¸ƒä¿®æ­£ï¼Œæå‡æ€§èƒ½ä¸æ•ˆç‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å˜æ¢å™¨çš„æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„è®¡ç®—å¤æ‚åº¦é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬é«˜å’Œå»¶è¿Ÿå¤§ã€‚å°½ç®¡æ³¨æ„åŠ›çŸ©é˜µé€šå¸¸æ˜¯ç¨€ç–çš„ï¼Œä½†ç¨€ç–æ³¨æ„åŠ›æ¨ç†æ–¹æ³•åœ¨å‡å°‘è®¡ç®—è´Ÿæ‹…çš„åŒæ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ€§èƒ½ä¸‹é™çš„ä¸€ä¸ªåŸå› æ˜¯ç¨€ç–è®¡ç®—å¼•èµ·äº†æ³¨æ„åŠ›è¾“å‡ºçš„åˆ†å¸ƒåç§»ï¼Œè¿™ä½¿å¾—è§£ç æ—¶çš„æŸ¥è¯¢ä¸é¢„å¡«é˜¶æ®µçš„é”®å¯¹é½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ä¿®æ­£æ–¹æ³•ï¼Œä½¿ç¨€ç–æ³¨æ„åŠ›è¾“å‡ºçš„åˆ†å¸ƒæ›´æ¥è¿‘äºäºŒæ¬¡æ³¨æ„åŠ›ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13227",
            "title": "Scaling Computer-Use Grounding via User Interface Decomposition and\n  Synthesis",
            "url": "https://huggingface.co/papers/2505.13227",
            "abstract": "Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.",
            "score": 34,
            "issue_id": 3849,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "fe01e0daca57b031",
            "authors": [
                "Tianbao Xie",
                "Jiaqi Deng",
                "Xiaochuan Li",
                "Junlin Yang",
                "Haoyuan Wu",
                "Jixuan Chen",
                "Wenjing Hu",
                "Xinyuan Wang",
                "Yuhui Xu",
                "Zekun Wang",
                "Yiheng Xu",
                "Junli Wang",
                "Doyen Sahoo",
                "Tao Yu",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13227.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#graphs",
                    "#agents",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº OSWorld-G Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Jedi, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Jedi, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ OSWorld-G. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Enhancing GUI Grounding with Comprehensive Datasets and Models",
                    "desc": "This paper addresses the challenge of GUI grounding, which is the process of translating natural language commands into actions on graphical user interfaces. Current benchmarks are limited as they only focus on simple tasks, neglecting the complexities of real-world interactions that require understanding of software context and layout. The authors introduce OSWorld-G, a new benchmark with 564 detailed samples and the Jedi dataset, which contains 4 million examples to improve grounding tasks. Their findings show that using the Jedi dataset significantly enhances the performance of multi-scale models in executing complex computer tasks, demonstrating the importance of specialized data for effective grounding."
                },
                "zh": {
                    "title": "æå‡è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„åŸºç¡€èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰åŸºç¡€çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰åŸºå‡†æµ‹è¯•è¿‡äºç®€åŒ–ï¼Œæ— æ³•åæ˜ çœŸå®ä¸–ç•Œçš„å¤æ‚äº¤äº’ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†OSWorld-GåŸºå‡†ï¼ŒåŒ…å«564ä¸ªç²¾ç»†æ³¨é‡Šçš„æ ·æœ¬ï¼Œæ¶µç›–æ–‡æœ¬åŒ¹é…ã€å…ƒç´ è¯†åˆ«ã€å¸ƒå±€ç†è§£å’Œç²¾ç¡®æ“ä½œç­‰å¤šç§ä»»åŠ¡ç±»å‹ã€‚æ­¤å¤–ï¼Œä½œè€…åˆæˆå¹¶å‘å¸ƒäº†æœ€å¤§çš„è®¡ç®—æœºä½¿ç”¨åŸºç¡€æ•°æ®é›†Jediï¼ŒåŒ…å«400ä¸‡ä¸ªç¤ºä¾‹ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚è®¡ç®—æœºä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡è¯¦ç»†çš„æ¶ˆèç ”ç©¶ï¼Œè®ºæ–‡è¿˜è¯†åˆ«äº†å½±å“åŸºç¡€æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œå¹¶éªŒè¯äº†ä¸åŒç•Œé¢å…ƒç´ çš„ä¸“é—¨æ•°æ®ç»“åˆèƒ½å¤Ÿå®ç°å¯¹æ–°ç•Œé¢çš„ç»„åˆæ³›åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13389",
            "title": "Faster Video Diffusion with Trainable Sparse Attention",
            "url": "https://huggingface.co/papers/2505.13389",
            "abstract": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on a small subset of positions. We turn this observation into VSA, a trainable, hardware-efficient sparse attention that replaces full attention at both training and inference. In VSA, a lightweight coarse stage pools tokens into tiles and identifies high-weight critical tokens; a fine stage computes token-level attention only inside those tiles subjecting to block computing layout to ensure hard efficiency. This leads to a single differentiable kernel that trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of FlashAttention3 MFU. We perform a large sweep of ablation studies and scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA reaches a Pareto point that cuts training FLOPS by 2.53times with no drop in diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention time by 6times and lowers end-to-end generation time from 31s to 18s with comparable quality. These results establish trainable sparse attention as a practical alternative to full attention and a key enabler for further scaling of video diffusion models.",
            "score": 26,
            "issue_id": 3850,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "33a48c202961951b",
            "authors": [
                "Peiyuan Zhang",
                "Haofeng Huang",
                "Yongqi Chen",
                "Will Lin",
                "Zhengzhong Liu",
                "Ion Stoica",
                "Eric P. Xing",
                "Hao Zhang"
            ],
            "affiliations": [
                "MBZUAI",
                "UC Berkeley",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13389.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#diffusion",
                    "#training",
                    "#open_source",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (VSA) Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². VSA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ³Ñ€ÑƒĞ±Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ´Ğ¾ ĞºĞ¾Ğ½Ñ†Ğ°, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ 85% ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VSA ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ² 2,53 Ñ€Ğ°Ğ·Ğ° Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² 6 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Efficient Attention for Scalable Video Diffusion",
                    "desc": "This paper introduces VSA, a novel trainable sparse attention mechanism designed to improve the efficiency of video diffusion transformers (DiTs). By focusing on a small subset of critical tokens, VSA reduces the computational burden associated with traditional quadratic attention. The method consists of a coarse stage that pools tokens and identifies important ones, followed by a fine stage that computes attention only within these selected tokens. The results demonstrate that VSA significantly decreases training FLOPS while maintaining performance, making it a viable alternative for scaling video diffusion models."
                },
                "zh": {
                    "title": "å¯è®­ç»ƒç¨€ç–æ³¨æ„åŠ›ï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–°é€‰æ‹©",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºVSAçš„å¯è®­ç»ƒç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰åœ¨å¤„ç†3Dæ³¨æ„åŠ›æ—¶çš„è®¡ç®—é™åˆ¶ã€‚VSAé€šè¿‡å°†æ³¨æ„åŠ›è®¡ç®—åˆ†ä¸ºç²—ç•¥é˜¶æ®µå’Œç²¾ç»†é˜¶æ®µï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆçš„è®­ç»ƒæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVSAåœ¨ä¸é™ä½æ‰©æ•£æŸå¤±çš„æƒ…å†µä¸‹ï¼Œå°†è®­ç»ƒçš„FLOPSå‡å°‘äº†2.53å€ï¼Œå¹¶ä¸”åœ¨å¼€æºæ¨¡å‹Wan-2.1ä¸Šå®ç°äº†6å€çš„æ³¨æ„åŠ›è®¡ç®—åŠ é€Ÿã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œå¯è®­ç»ƒçš„ç¨€ç–æ³¨æ„åŠ›æ˜¯å…¨æ³¨æ„åŠ›çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿›ä¸€æ­¥æ‰©å±•æä¾›äº†å…³é”®æ”¯æŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13379",
            "title": "Thinkless: LLM Learns When to Think",
            "url": "https://huggingface.co/papers/2505.13379",
            "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at https://github.com/VainF/Thinkless",
            "score": 25,
            "issue_id": 3846,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "d41117eabc11e5c3",
            "authors": [
                "Gongfan Fang",
                "Xinyin Ma",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13379.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Thinkless - Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´Ğ²Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°: <short> Ğ´Ğ»Ñ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ <think> Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ DeGRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ñ†ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Thinkless ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 50-90%, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Thinkless: Smart Reasoning for Efficient Language Models",
                    "desc": "This paper introduces Thinkless, a framework designed to enhance the efficiency of Reasoning Language Models (RLMs) by enabling them to choose between short-form and long-form reasoning based on task complexity. The framework utilizes reinforcement learning and two control tokens, <short> for brief answers and <think> for detailed reasoning, to guide the model's response strategy. A novel algorithm called Decoupled Group Relative Policy Optimization (DeGRPO) is employed to separate the learning objectives, allowing for better control over reasoning mode selection and response accuracy. The results show that Thinkless can significantly reduce the reliance on long-chain reasoning, improving computational efficiency while maintaining performance on various benchmarks."
                },
                "zh": {
                    "title": "è®©æ¨¡å‹å­¦ä¼šä½•æ—¶æ€è€ƒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºThinklessçš„å¯å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡å¤æ‚æ€§å’Œè‡ªèº«èƒ½åŠ›è‡ªé€‚åº”é€‰æ‹©çŸ­æœŸæˆ–é•¿æœŸæ¨ç†ã€‚Thinklessä½¿ç”¨ä¸¤ä¸ªæ§åˆ¶æ ‡è®°<short>å’Œ<think>æ¥åˆ†åˆ«è¡¨ç¤ºç®€æ´å›ç­”å’Œè¯¦ç»†æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒThinklessèƒ½å¤Ÿå°†é•¿æœŸæ¨ç†çš„ä½¿ç”¨å‡å°‘50%è‡³90%ï¼Œæ˜¾è‘—æå‡æ¨ç†è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13308",
            "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space",
            "url": "https://huggingface.co/papers/2505.13308",
            "abstract": "Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.",
            "score": 23,
            "issue_id": 3851,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "bd9e494dc68db18c",
            "authors": [
                "Hengli Li",
                "Chenxi Li",
                "Tong Wu",
                "Xuekai Zhu",
                "Yuxuan Wang",
                "Zhaoxin Yu",
                "Eric Hanchen Jiang",
                "Song-Chun Zhu",
                "Zixia Jia",
                "Ying Nian Wu",
                "Zilong Zheng"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University",
                "Institute for Artificial Intelligence, Peking University",
                "Institute of Automation, Chinese Academy of Sciences",
                "NLCo Lab, Beijing Institute for General Artificial Intelligence",
                "Shanghai Jiao Tong University",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13308.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#agi"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LatentSeek: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñƒ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LatentSeek - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. LatentSeek Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ÑƒÑÑÑŒ ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑÑŒ Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. LatentSeek Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğµ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with LatentSeek",
                    "desc": "This paper addresses the challenges of reasoning in Large Language Models (LLMs) as they strive for Artificial General Intelligence (AGI). It introduces LatentSeek, a framework that improves reasoning by adapting the model's latent space during test time, rather than updating parameters. By using policy gradient methods, LatentSeek iteratively refines latent representations based on self-generated rewards, leading to enhanced performance on reasoning tasks. The results show that LatentSeek outperforms existing methods and is efficient, converging quickly while still benefiting from additional iterations."
                },
                "zh": {
                    "title": "LatentSeekï¼šæå‡LLMæ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è¿½æ±‚é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰æ—¶ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶LatentSeekï¼Œé€šè¿‡åœ¨æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæµ‹è¯•æ—¶å®ä¾‹çº§é€‚åº”ï¼ˆTTIAï¼‰ï¼Œæ¥å¢å¼ºLLMsçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒLatentSeekåˆ©ç”¨ç­–ç•¥æ¢¯åº¦è¿­ä»£æ›´æ–°æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶é€šè¿‡è‡ªç”Ÿæˆçš„å¥–åŠ±ä¿¡å·è¿›è¡ŒæŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLatentSeekåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæµ‹è¯•æ—¶æ‰©å±•çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12082",
            "title": "Model Merging in Pre-training of Large Language Models",
            "url": "https://huggingface.co/papers/2505.12082",
            "abstract": "Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.",
            "score": 23,
            "issue_id": 3854,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ",
                "en": "May 17",
                "zh": "5æœˆ17æ—¥"
            },
            "hash": "7f25885def33b040",
            "authors": [
                "Yunshui Li",
                "Yiyuan Ma",
                "Shen Yan",
                "Chaoyi Zhang",
                "Jing Liu",
                "Jianqiao Lu",
                "Ziwen Xu",
                "Mengzhao Chen",
                "Minrui Wang",
                "Shiyi Zhan",
                "Jin Ma",
                "Xunhao Lai",
                "Yao Luo",
                "Xingyan Bin",
                "Hongbin Ren",
                "Mingji Han",
                "Wenhao Hao",
                "Bairen Yi",
                "LingJun Liu",
                "Bole Ma",
                "Xiaoying Jia",
                "Zhou Xun",
                "Liang Xiang",
                "Yonghui Wu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.12082.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Mixture-of-Experts, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ´ĞµĞ¸ Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ñ… ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing Language Models through Effective Model Merging",
                    "desc": "This paper explores the technique of model merging to improve large language models during their pre-training phase. The authors conduct extensive experiments on various architectures, including dense models and Mixture-of-Experts (MoE), to assess the impact of merging checkpoints. They find that using constant learning rates during merging not only enhances model performance but also allows for better predictions of training behavior. The study provides valuable insights and practical guidelines for the open-source community to implement effective model merging strategies, ultimately reducing training costs and improving efficiency."
                },
                "zh": {
                    "title": "æ¨¡å‹åˆå¹¶ï¼šæå‡é¢„è®­ç»ƒæ•ˆç‡çš„æ–°æ–¹æ³•",
                    "desc": "æ¨¡å‹åˆå¹¶æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æŠ€æœ¯ï¼Œå¯ä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½†åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒä¸­çš„åº”ç”¨ä»ç„¶ç›¸å¯¹æœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡å…¨é¢ç ”ç©¶äº†åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ¨¡å‹åˆå¹¶æŠ€æœ¯ã€‚é€šè¿‡å¯¹æ•°ç™¾ä¸‡åˆ°è¶…è¿‡1000äº¿å‚æ•°çš„å¯†é›†å’Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„è¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨æ’å®šå­¦ä¹ ç‡è®­ç»ƒçš„æ£€æŸ¥ç‚¹åˆå¹¶ä¸ä»…æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œè¿˜èƒ½å‡†ç¡®é¢„æµ‹é€€ç«è¡Œä¸ºã€‚è¿™äº›æ”¹è¿›ä½¿å¾—æ¨¡å‹å¼€å‘æ›´åŠ é«˜æ•ˆï¼Œå¹¶æ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13427",
            "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable\n  Step-Level Supervision",
            "url": "https://huggingface.co/papers/2505.13427",
            "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning, often producing logically inconsistent or partially correct solutions. A key limitation lies in the lack of fine-grained supervision over intermediate reasoning steps. To address this, we propose MM-PRM, a process reward model trained within a fully automated, scalable framework. We first build MM-Policy, a strong multimodal model trained on diverse mathematical reasoning data. Then, we construct MM-K12, a curated dataset of 10,000 multimodal math problems with verifiable answers, which serves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based pipeline, we generate over 700k step-level annotations without human labeling. The resulting PRM is used to score candidate reasoning paths in the Best-of-N inference setup and achieves significant improvements across both in-domain (MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.) benchmarks. Further analysis confirms the effectiveness of soft labels, smaller learning rates, and path diversity in optimizing PRM performance. MM-PRM demonstrates that process supervision is a powerful tool for enhancing the logical robustness of multimodal reasoning systems. We release all our codes and data at https://github.com/ModalMinds/MM-PRM.",
            "score": 20,
            "issue_id": 3847,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "22362149c9b7b5ae",
            "authors": [
                "Lingxiao Du",
                "Fanqing Meng",
                "Zongkai Liu",
                "Zhixiang Zhou",
                "Ping Luo",
                "Qiaosheng Zhang",
                "Wenqi Shao"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13427.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#multimodal",
                    "#math",
                    "#training",
                    "#open_source",
                    "#benchmark",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MM-PRM, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ°ÑÑÑ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MM-K12 Ñ 10 000 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 700 Ñ‚Ñ‹ÑÑÑ‡ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ MM-PRM Ğ² ÑÑ…ĞµĞ¼Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Best-of-N Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ MM-K12, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑĞ³ĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº, Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with Process Supervision",
                    "desc": "This paper introduces MM-PRM, a novel process reward model designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs) in solving complex math problems. The authors highlight that MLLMs often struggle with multi-step reasoning due to insufficient supervision of intermediate steps. To overcome this, they create a strong multimodal model called MM-Policy and a new dataset, MM-K12, containing 10,000 multimodal math problems. By employing a Monte Carlo Tree Search method, they generate over 700,000 annotations to train the PRM, which significantly enhances the logical consistency of reasoning in various benchmarks."
                },
                "zh": {
                    "title": "è¿‡ç¨‹ç›‘ç£æå‡å¤šæ¨¡æ€æ¨ç†çš„é€»è¾‘ç¨³å¥æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆMM-PRMï¼‰ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šæ­¥éª¤æ¨ç†ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç¼ºä¹ç»†ç²’åº¦çš„ç›‘ç£ï¼Œå¯¼è‡´é€»è¾‘ä¸ä¸€è‡´æˆ–éƒ¨åˆ†æ­£ç¡®çš„ç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªå¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹MM-Policyï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«10,000ä¸ªå¯éªŒè¯ç­”æ¡ˆçš„å¤šæ¨¡æ€æ•°å­¦é—®é¢˜æ•°æ®é›†MM-K12ã€‚é€šè¿‡æ— äººå·¥æ ‡æ³¨çš„æ–¹å¼ç”Ÿæˆè¶…è¿‡70ä¸‡æ¡æ­¥éª¤çº§æ³¨é‡Šï¼ŒMM-PRMåœ¨æ¨ç†è·¯å¾„è¯„åˆ†ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼Œè¯æ˜äº†è¿‡ç¨‹ç›‘ç£åœ¨å¢å¼ºå¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿé€»è¾‘ç¨³å¥æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13215",
            "title": "Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation",
            "url": "https://huggingface.co/papers/2505.13215",
            "abstract": "Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and temporal variations. However, existing methods suffer from substantial computational and memory overhead due to the redundant allocation of 4D Gaussians to static regions, which can also degrade image quality. In this work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework that adaptively represents static regions with 3D Gaussians while reserving 4D Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian representation and iteratively converts temporally invariant Gaussians into 3D, significantly reducing the number of parameters and improving computational efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation, capturing complex motions with high fidelity. Our approach achieves significantly faster training times compared to baseline 4D Gaussian Splatting methods while maintaining or improving the visual quality.",
            "score": 20,
            "issue_id": 3846,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "0ab00a261298ad44",
            "authors": [
                "Seungjun Oh",
                "Younggeun Lee",
                "Hyejin Jeon",
                "Eunbyung Park"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Sungkyunkwan University",
                "Department of Artificial Intelligence, Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13215.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ 3D-4D Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ 3D-4DGS Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½. ĞĞ½ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ 3D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ 4D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ 4D Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Efficient 3D-4D Scene Reconstruction with Hybrid Gaussian Splatting",
                    "desc": "This paper presents a new method called hybrid 3D-4D Gaussian Splatting (3D-4DGS) for dynamic 3D scene reconstruction. It combines 3D and 4D Gaussian representations to efficiently model static and dynamic elements in a scene. By converting static regions to 3D Gaussians, the method reduces computational load and memory usage while preserving the quality of dynamic elements with 4D Gaussians. The results show that 3D-4DGS achieves faster training times and improved visual quality compared to traditional 4D Gaussian Splatting techniques."
                },
                "zh": {
                    "title": "é«˜æ•ˆçš„åŠ¨æ€3Dåœºæ™¯é‡å»ºæ–°æ–¹æ³•",
                    "desc": "æœ€è¿‘åŠ¨æ€3Dåœºæ™¯é‡å»ºçš„è¿›å±•æ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ•ˆæœï¼Œèƒ½å¤Ÿå®ç°é«˜ä¿çœŸåº¦çš„3Dæ–°è§†å›¾åˆæˆï¼Œå¹¶æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼Œ4Dé«˜æ–¯ç‚¹äº‘ï¼ˆ4DGSï¼‰å› å…¶èƒ½å¤Ÿå»ºæ¨¡é«˜ä¿çœŸçš„ç©ºé—´å’Œæ—¶é—´å˜åŒ–è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨é™æ€åŒºåŸŸå†—ä½™åˆ†é…4Dé«˜æ–¯æ—¶ï¼Œå¯¼è‡´äº†æ˜¾è‘—çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ï¼Œå¹¶å¯èƒ½é™ä½å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆ3D-4Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3D-4DGSï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°ç”¨3Dé«˜æ–¯è¡¨ç¤ºé™æ€åŒºåŸŸï¼ŒåŒæ—¶ä¸ºåŠ¨æ€å…ƒç´ ä¿ç•™4Dé«˜æ–¯ï¼Œä»è€Œæ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12805",
            "title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with\n  LoRA",
            "url": "https://huggingface.co/papers/2505.12805",
            "abstract": "Low-Rank Adaptation (LoRA), which introduces a product of two trainable low-rank matrices into frozen pre-trained weights, is widely used for efficient fine-tuning of language models in federated learning (FL). However, when combined with differentially private stochastic gradient descent (DP-SGD), LoRA faces substantial noise amplification: DP-SGD perturbs per-sample gradients, and the matrix multiplication of the LoRA update (BA) intensifies this effect. Freezing one matrix (e.g., A) reduces the noise but restricts model expressiveness, often resulting in suboptimal adaptation. To address this, we propose FedSVD, a simple yet effective method that introduces a global reparameterization based on singular value decomposition (SVD). In our approach, each client optimizes only the B matrix and transmits it to the server. The server aggregates the B matrices, computes the product BA using the previous A, and refactorizes the result via SVD. This yields a new adaptive A composed of the orthonormal right singular vectors of BA, and an updated B containing the remaining SVD components. This reparameterization avoids quadratic noise amplification, while allowing A to better capture the principal directions of the aggregate updates. Moreover, the orthonormal structure of A bounds the gradient norms of B and preserves more signal under DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD consistently improves stability and performance across a variety of privacy settings and benchmarks, outperforming relevant baselines under both private and non-private regimes.",
            "score": 20,
            "issue_id": 3848,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "41ef4fd84db0c7fb",
            "authors": [
                "Seanie Lee",
                "Sangwoo Park",
                "Dong Bok Lee",
                "Dominik Wagner",
                "Haebin Seong",
                "Tobias Bocklet",
                "Juho Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST",
                "Technische Hochschule NÃ¼rnberg Georg Simon Ohm"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12805.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#benchmark",
                    "#security",
                    "#optimization"
                ],
                "emoji": "ğŸ”’",
                "ru": {
                    "title": "FedSVD: Ğ—Ğ°Ñ‰Ğ¸Ñ‰ĞµĞ½Ğ½Ğ¾Ğµ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ FedSVD Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ. FedSVD Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ Low-Rank Adaptation (LoRA) Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ DP-SGD. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ (SVD) Ğ´Ğ»Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ°. FedSVD Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Federated Learning with FedSVD: A Noise-Resilient Approach",
                    "desc": "This paper presents FedSVD, a novel method that enhances the fine-tuning of language models in federated learning while addressing the challenges posed by noise amplification in differentially private stochastic gradient descent (DP-SGD). By utilizing singular value decomposition (SVD), FedSVD allows clients to optimize only one matrix (B) and send it to a central server, which then aggregates these updates to improve model adaptation. This approach mitigates the noise amplification that occurs when combining LoRA with DP-SGD, ensuring that the model remains expressive without compromising privacy. The results demonstrate that FedSVD improves both stability and performance across various privacy settings, outperforming existing methods."
                },
                "zh": {
                    "title": "FedSVDï¼šä¼˜åŒ–è”é‚¦å­¦ä¹ ä¸­çš„ä½ç§©é€‚åº”",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFedSVDçš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åœ¨è”é‚¦å­¦ä¹ ä¸­ä¸å·®åˆ†éšç§éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆDP-SGDï¼‰ç»“åˆæ—¶çš„å™ªå£°æ”¾å¤§é—®é¢˜ã€‚é€šè¿‡å¼•å…¥åŸºäºå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰çš„å…¨å±€é‡å‚æ•°åŒ–ï¼ŒFedSVDå…è®¸æ¯ä¸ªå®¢æˆ·ç«¯ä»…ä¼˜åŒ–BçŸ©é˜µå¹¶å°†å…¶ä¼ è¾“åˆ°æœåŠ¡å™¨ã€‚æœåŠ¡å™¨èšåˆBçŸ©é˜µï¼Œè®¡ç®—BAçš„ä¹˜ç§¯ï¼Œå¹¶é€šè¿‡SVDé‡æ–°å› å¼åˆ†è§£ï¼Œä»è€Œç”Ÿæˆæ–°çš„é€‚åº”æ€§Aå’Œæ›´æ–°åçš„Bã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå‡å°‘äº†å™ªå£°æ”¾å¤§ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ï¼Œå°¤å…¶åœ¨éšç§è®¾ç½®ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12504",
            "title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language\n  Models",
            "url": "https://huggingface.co/papers/2505.12504",
            "abstract": "Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates and improper clipping can lead to training collapse. To address this issue, we propose Clipped Policy Gradient Optimization with Policy Drift (CPGD), a novel algorithm designed to stabilize policy learning in LMs. CPGD introduces a policy drift constraint based on KL divergence to dynamically regularize policy updates, and leverages a clip mechanism on the logarithm of the ratio to prevent excessive policy updates. We provide theoretical justification for CPGD and demonstrate through empirical analysis that it mitigates the instability observed in prior approaches. Furthermore, we show that CPGD significantly improves performance while maintaining training stability. Our implementation balances theoretical rigor with practical usability, offering a robust alternative for RL in the post-training of LMs. We release our code at https://github.com/ModalMinds/MM-EUREKA.",
            "score": 20,
            "issue_id": 3847,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "f8b07da7e5e43f1e",
            "authors": [
                "Zongkai Liu",
                "Fanqing Meng",
                "Lingxiao Du",
                "Zhixiang Zhou",
                "Chao Yu",
                "Wenqi Shao",
                "Qiaosheng Zhang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "Sun Yat-Sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12504.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ CPGD Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. CPGD Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ñ€ĞµĞ¹Ñ„ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑ‚ CPGD Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼ÑƒÑ Ğ² Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Stabilizing Reinforcement Learning with CPGD",
                    "desc": "This paper introduces Clipped Policy Gradient Optimization with Policy Drift (CPGD), a new algorithm aimed at improving the stability of reinforcement learning in language models. Traditional methods often face issues like training collapse due to large policy updates and improper clipping. CPGD addresses these challenges by using a KL divergence-based policy drift constraint to regulate updates and a clipping mechanism to limit excessive changes. The authors provide theoretical support for CPGD and demonstrate its effectiveness in enhancing performance while ensuring stable training."
                },
                "zh": {
                    "title": "ç¨³å®šå¼ºåŒ–å­¦ä¹ ï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ€è¿‘ï¼ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„RLæ–¹æ³•å¦‚GRPOã€REINFORCE++å’ŒRLOOå¸¸å¸¸é¢ä¸´è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°ç®—æ³•â€”â€”å¸¦æœ‰ç­–ç•¥æ¼‚ç§»çš„å‰ªåˆ‡ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼ˆCPGDï¼‰ï¼Œæ—¨åœ¨ç¨³å®šè¯­è¨€æ¨¡å‹ä¸­çš„ç­–ç•¥å­¦ä¹ ã€‚CPGDé€šè¿‡åŸºäºKLæ•£åº¦çš„ç­–ç•¥æ¼‚ç§»çº¦æŸåŠ¨æ€åœ°è§„èŒƒç­–ç•¥æ›´æ–°ï¼Œå¹¶åˆ©ç”¨å¯¹æ•°æ¯”ç‡çš„å‰ªåˆ‡æœºåˆ¶é˜²æ­¢è¿‡å¤§çš„ç­–ç•¥æ›´æ–°ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æå’Œå®è¯ç»“æœè¡¨æ˜ï¼ŒCPGDä¸ä»…ç¼“è§£äº†ä¹‹å‰æ–¹æ³•çš„è®­ç»ƒä¸ç¨³å®šæ€§ï¼Œè¿˜æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12992",
            "title": "Fractured Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2505.12992",
            "abstract": "Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning.",
            "score": 16,
            "issue_id": 3849,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "464c0b3fac842217",
            "authors": [
                "Baohao Liao",
                "Hanze Dong",
                "Yuhui Xu",
                "Doyen Sahoo",
                "Christof Monz",
                "Junnan Li",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "University of Amsterdam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12992.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Fractured Sampling Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¸Ğ´ĞµĞµ ÑƒÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought, CoT) Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ CoT Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ¾ÑÑĞ¼: ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° ÑƒÑĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Fractured Sampling Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Efficient Reasoning with Fractured Sampling",
                    "desc": "This paper presents a new method called Fractured Sampling that enhances the reasoning abilities of large language models (LLMs) during inference without the need for retraining. It builds on the concept of Chain-of-Thought (CoT) prompting, which improves accuracy by generating intermediate reasoning steps, but often at a high token cost. The authors demonstrate that a truncated version of CoT can achieve similar results with significantly fewer tokens. By exploring different ways to balance reasoning depth and the number of solutions, Fractured Sampling offers a more efficient approach that improves accuracy while reducing computational costs."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ï¼šFractured Samplingçš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æ¨ç†æ—¶é—´ç¼©æ”¾æŠ€æœ¯ï¼Œç§°ä¸ºFractured Samplingï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æˆªæ–­é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆæ—¶å‡å°‘äº†æ‰€éœ€çš„tokenæ•°é‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å®Œæ•´CoTç›¸ä¼¼çš„å‡†ç¡®æ€§ã€‚Fractured Samplingåœ¨æ¨ç†è½¨è¿¹æ•°é‡ã€æ¯æ¡è½¨è¿¹çš„æœ€ç»ˆè§£å†³æ–¹æ¡ˆæ•°é‡å’Œæ¨ç†æ·±åº¦ç­‰ä¸‰ä¸ªç»´åº¦ä¸Šè¿›è¡Œæ’å€¼ï¼Œä»è€Œä¼˜åŒ–äº†å‡†ç¡®æ€§ä¸æˆæœ¬çš„å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜æ•ˆå’Œå¯æ‰©å±•çš„LLMæ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13444",
            "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2505.13444",
            "abstract": "Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.",
            "score": 15,
            "issue_id": 3848,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "7bd36c8068fb640c",
            "authors": [
                "Liyan Tang",
                "Grace Kim",
                "Xinyu Zhao",
                "Thom Lake",
                "Wenxuan Ding",
                "Fangcong Yin",
                "Prasann Singhal",
                "Manya Wadhwa",
                "Zeyu Leo Liu",
                "Zayne Sprague",
                "Ramya Namuduri",
                "Bodun Hu",
                "Juan Diego Rodriguez",
                "Puyuan Peng",
                "Greg Durrett"
            ],
            "affiliations": [
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13444.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#interpretability",
                    "#cv",
                    "#benchmark",
                    "#synthetic",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ChartMuseum Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ»ÑĞ´ÑĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ChartMuseum ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Bridging the Gap in Chart Understanding: Visual vs. Textual Reasoning",
                    "desc": "This paper addresses the challenges faced by large vision-language models (LVLMs) in understanding charts, highlighting their struggle with visual reasoning compared to textual reasoning. The authors present a case study using a synthetic dataset that shows a significant drop in model performance as visual complexity increases, while human performance remains stable. They introduce ChartMuseum, a new benchmark for Chart Question Answering (QA) that includes 1,162 expert-annotated questions designed to test both visual and textual reasoning. The results reveal a substantial performance gap between humans and models, with the best model achieving only 63% accuracy compared to 93% for humans, particularly struggling with questions that require visual reasoning."
                },
                "zh": {
                    "title": "å›¾è¡¨ç†è§£ï¼šäººç±»ä¸æ¨¡å‹çš„å·®è·",
                    "desc": "å›¾è¡¨ç†è§£å¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒéœ€è¦å¤æ‚çš„æ–‡æœ¬å’Œè§†è§‰æ¨ç†èƒ½åŠ›çš„ç»“åˆã€‚å½“å‰çš„LVLMåœ¨è¿™äº›æŠ€èƒ½ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„ä¸å¹³è¡¡ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªåˆæˆæ•°æ®é›†è¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ï¼Œå‘ç°éšç€è§†è§‰å¤æ‚æ€§çš„å¢åŠ ï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè€Œäººç±»çš„è¡¨ç°åˆ™ä¿æŒç¨³å®šã€‚æˆ‘ä»¬å¼•å…¥äº†ChartMuseumï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å›¾è¡¨é—®ç­”åŸºå‡†ï¼ŒåŒ…å«1162ä¸ªä¸“å®¶æ³¨é‡Šçš„é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°å¤æ‚çš„è§†è§‰å’Œæ–‡æœ¬æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11932",
            "title": "Neuro-Symbolic Query Compiler",
            "url": "https://huggingface.co/papers/2505.11932",
            "abstract": "Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar rules and compiler design, to bridge this gap. It theoretically designs a minimal yet sufficient Backus-Naur Form (BNF) grammar G[q] to formalize complex queries. Unlike previous methods, this grammar maintains completeness while minimizing redundancy. Based on this, QCompiler includes a Query Expression Translator, a Lexical Syntax Parser, and a Recursive Descent Processor to compile queries into Abstract Syntax Trees (ASTs) for execution. The atomicity of the sub-queries in the leaf nodes ensures more precise document retrieval and response generation, significantly improving the RAG system's ability to address complex queries.",
            "score": 14,
            "issue_id": 3846,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ",
                "en": "May 17",
                "zh": "5æœˆ17æ—¥"
            },
            "hash": "9445be4eff7e4edc",
            "authors": [
                "Yuyao Zhang",
                "Zhicheng Dou",
                "Xiaoxi Li",
                "Jiajie Jin",
                "Yongkang Wu",
                "Zhonghua Li",
                "Qi Ye",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Huawei Poisson Lab",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11932.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rag",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…",
                    "desc": "QCompiler - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾-ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºÑƒ BNF Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ€ĞµĞ²ÑŒÑ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸. QCompiler Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‡Ğ¸Ğº Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€."
                },
                "en": {
                    "title": "Enhancing Query Understanding in RAG Systems with QCompiler",
                    "desc": "This paper introduces QCompiler, a neuro-symbolic framework designed to enhance the understanding of complex search queries in Retrieval-Augmented Generation (RAG) systems. It utilizes a specially designed Backus-Naur Form (BNF) grammar to formalize these queries, ensuring that they are both complete and free of unnecessary complexity. QCompiler operates through a series of components, including a Query Expression Translator and a Lexical Syntax Parser, which work together to convert queries into Abstract Syntax Trees (ASTs). By focusing on the atomicity of sub-queries, QCompiler improves the accuracy of document retrieval and response generation for intricate queries."
                },
                "zh": {
                    "title": "æå‡RAGç³»ç»Ÿçš„å¤æ‚æŸ¥è¯¢è¯†åˆ«èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºQCompilerçš„ç¥ç»ç¬¦å·æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå¯¹å¤æ‚æŸ¥è¯¢çš„è¯†åˆ«èƒ½åŠ›ã€‚QCompileråŸºäºè¯­è¨€è¯­æ³•è§„åˆ™å’Œç¼–è¯‘å™¨è®¾è®¡ï¼Œè®¾è®¡äº†ä¸€ç§æœ€å°ä½†è¶³å¤Ÿçš„å·´ç§‘æ–¯-è¯ºå°”å½¢å¼ï¼ˆBNFï¼‰è¯­æ³•G[q]ï¼Œä»¥å½¢å¼åŒ–å¤æ‚æŸ¥è¯¢ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼Œè¿™ç§è¯­æ³•åœ¨ä¿æŒå®Œæ•´æ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†å†—ä½™ã€‚é€šè¿‡å°†æŸ¥è¯¢ç¼–è¯‘æˆæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰ï¼ŒQCompilerèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°æ£€ç´¢æ–‡æ¡£å¹¶ç”Ÿæˆå“åº”ï¼Œä»è€Œæ˜¾è‘—æå‡RAGç³»ç»Ÿå¤„ç†å¤æ‚æŸ¥è¯¢çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12346",
            "title": "SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy\n  Optimization",
            "url": "https://huggingface.co/papers/2505.12346",
            "abstract": "Large language models (LLMs) exhibit varying levels of confidence across input prompts (questions): some lead to consistent, semantically similar answers, while others yield diverse or contradictory outputs. This variation reflects LLM's uncertainty about the input prompt, a signal of how confidently the model understands a given problem. However, vanilla Group Relative Policy Optimization (GRPO) treats all prompts equally during policy updates, ignoring this important information about the model's knowledge boundaries. To address this limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which explicitly measures LLMs' uncertainty of the input prompts semantic entropy. Semantic entropy measures the diversity of meaning in multiple generated answers given a prompt and uses this to modulate the magnitude of policy updates. This uncertainty-aware training mechanism enables dynamic adjustment of policy update magnitudes based on question uncertainty. It allows more conservative updates on high-uncertainty questions while maintaining the original learning signal on confident ones. Experimental results on five mathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva 34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new state-of-the-art performance in average accuracy, validating the effectiveness of uncertainty-aware policy optimization.",
            "score": 13,
            "issue_id": 3857,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "fbe6291cd68e8efb",
            "authors": [
                "Minghan Chen",
                "Guikun Chen",
                "Wenguan Wang",
                "Yi Yang"
            ],
            "affiliations": [
                "ReLER Lab, CCAI, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12346.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#math",
                    "#benchmark",
                    "#rl",
                    "#hallucinations"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜: ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SEED-GRPO. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². SEED-GRPO Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñƒ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ½ÑĞµÑ€Ğ²Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SEED-GRPO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ³Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Enhancing LLM Training with Uncertainty Awareness",
                    "desc": "This paper introduces SEED-GRPO, an advanced method for training large language models (LLMs) that takes into account the model's uncertainty regarding input prompts. Traditional Group Relative Policy Optimization (GRPO) treats all prompts the same, which can overlook important variations in the model's confidence. SEED-GRPO incorporates semantic entropy, a measure of the diversity of answers generated for a prompt, to adjust how much the model learns from different questions. By applying this uncertainty-aware approach, the model can make more cautious updates for prompts it finds challenging, leading to improved performance on mathematical reasoning tasks."
                },
                "zh": {
                    "title": "åŸºäºä¸ç¡®å®šæ€§çš„ç­–ç•¥ä¼˜åŒ–æ–°æ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†è¾“å…¥æç¤ºæ—¶è¡¨ç°å‡ºä¸åŒçš„ä¿¡å¿ƒæ°´å¹³ï¼Œæœ‰äº›æç¤ºäº§ç”Ÿä¸€è‡´ä¸”è¯­ä¹‰ç›¸ä¼¼çš„ç­”æ¡ˆï¼Œè€Œå¦ä¸€äº›åˆ™äº§ç”Ÿå¤šæ ·æˆ–çŸ›ç›¾çš„è¾“å‡ºã€‚è¿™ç§å˜åŒ–åæ˜ äº†æ¨¡å‹å¯¹è¾“å…¥æç¤ºçš„ä¸ç¡®å®šæ€§ï¼Œè¡¨æ˜æ¨¡å‹å¯¹ç‰¹å®šé—®é¢˜çš„ç†è§£ç¨‹åº¦ã€‚ä¼ ç»Ÿçš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨ç­–ç•¥æ›´æ–°æ—¶å¯¹æ‰€æœ‰æç¤ºä¸€è§†åŒä»ï¼Œå¿½ç•¥äº†æ¨¡å‹çŸ¥è¯†è¾¹ç•Œçš„é‡è¦ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SEED-GRPOï¼ˆè¯­ä¹‰ç†µå¢å¼ºçš„GRPOï¼‰ï¼Œå®ƒé€šè¿‡æµ‹é‡è¾“å…¥æç¤ºçš„è¯­ä¹‰ç†µæ¥æ˜¾å¼è€ƒè™‘LLMsçš„ä¸ç¡®å®šæ€§ï¼Œä»è€ŒåŠ¨æ€è°ƒæ•´ç­–ç•¥æ›´æ–°çš„å¹…åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12081",
            "title": "VisionReasoner: Unified Visual Perception and Reasoning via\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.12081",
            "abstract": "Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, a unified framework capable of reasoning and solving multiple visual perception tasks within a shared model. Specifically, by designing novel multi-object cognitive learning strategies and systematic task reformulation, VisionReasoner enhances its reasoning capabilities to analyze visual inputs, and addresses diverse perception tasks in a unified framework. The model generates a structured reasoning process before delivering the desired outputs responding to user queries. To rigorously assess unified visual perception capabilities, we evaluate VisionReasoner on ten diverse tasks spanning three critical domains: detection, segmentation, and counting. Experimental results show that VisionReasoner achieves superior performance as a unified model, outperforming Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg (segmentation), and 15.3% on CountBench (counting).",
            "score": 13,
            "issue_id": 3845,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ",
                "en": "May 17",
                "zh": "5æœˆ17æ—¥"
            },
            "hash": "9b7953f88ae7653d",
            "authors": [
                "Yuqi Liu",
                "Tianyuan Qu",
                "Zhisheng Zhong",
                "Bohao Peng",
                "Shu Liu",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "SmartMore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12081.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ VisionReasoner - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. VisionReasoner Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ²Ñ‹Ğ´Ğ°Ñ‡ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ VisionReasoner Ğ½Ğ°Ğ´ Qwen2.5VL Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "VisionReasoner: Unifying Visual Perception with Advanced Reasoning",
                    "desc": "This paper presents VisionReasoner, a unified framework designed to enhance visual perception tasks through advanced reasoning capabilities. It employs innovative multi-object cognitive learning strategies and reformulates tasks systematically to improve its performance across various visual challenges. The model processes visual inputs in a structured manner, allowing it to effectively respond to user queries. Evaluation results demonstrate that VisionReasoner significantly outperforms existing models in detection, segmentation, and counting tasks, showcasing its effectiveness as a comprehensive solution for visual perception."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è§†è§‰æ„ŸçŸ¥çš„æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVisionReasonerçš„ç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å…±äº«æ¨¡å‹ä¸­å¤„ç†å¤šç§è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ã€‚é€šè¿‡è®¾è®¡æ–°é¢–çš„å¤šå¯¹è±¡è®¤çŸ¥å­¦ä¹ ç­–ç•¥å’Œç³»ç»Ÿçš„ä»»åŠ¡é‡æ„ï¼ŒVisionReasonerå¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›ï¼Œä»¥åˆ†æè§†è§‰è¾“å…¥å¹¶è§£å†³å¤šæ ·çš„æ„ŸçŸ¥ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åœ¨ç”Ÿæˆæ‰€éœ€è¾“å‡ºä¹‹å‰ï¼Œä¼šå…ˆè¿›è¡Œç»“æ„åŒ–çš„æ¨ç†è¿‡ç¨‹ï¼Œä»¥å“åº”ç”¨æˆ·æŸ¥è¯¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisionReasoneråœ¨æ£€æµ‹ã€åˆ†å‰²å’Œè®¡æ•°ç­‰ä¸‰ä¸ªå…³é”®é¢†åŸŸçš„åä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†Qwen2.5VLã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07704",
            "title": "Through the Looking Glass: Common Sense Consistency Evaluation of Weird\n  Images",
            "url": "https://huggingface.co/papers/2505.07704",
            "abstract": "Measuring how real images look is a complex task in artificial intelligence research. For example, an image of a boy with a vacuum cleaner in a desert violates common sense. We introduce a novel method, which we call Through the Looking Glass (TLG), to assess image common sense consistency using Large Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging LVLMs to extract atomic facts from these images, we obtain a mix of accurate facts. We proceed by fine-tuning a compact attention-pooling classifier over encoded atomic facts. Our TLG has achieved a new state-of-the-art performance on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning component.",
            "score": 13,
            "issue_id": 3862,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 Ğ¼Ğ°Ñ",
                "en": "May 12",
                "zh": "5æœˆ12æ—¥"
            },
            "hash": "9fc123da51142c1e",
            "authors": [
                "Elisei Rykov",
                "Kseniia Petrushina",
                "Kseniia Titova",
                "Anton Razzhigaev",
                "Alexander Panchenko",
                "Vasily Konovalov"
            ],
            "affiliations": [
                "AIRI",
                "MTS AI",
                "Moscow Institute of Physics and Technology",
                "Skoltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07704.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#interpretability",
                    "#cv",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ» Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Through the Looking Glass (TLG). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LVLMs) Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ—Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ñ Ğ¿ÑƒĞ»Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. TLG Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… WHOOPS! Ğ¸ WEIRD."
                },
                "en": {
                    "title": "Assessing Image Realism with Through the Looking Glass (TLG)",
                    "desc": "This paper presents a new method called Through the Looking Glass (TLG) for evaluating the common sense consistency of images using Large Vision-Language Models (LVLMs). The approach involves extracting atomic facts from images with a Transformer-based encoder, which helps in understanding the context of the images better. By fine-tuning a compact attention-pooling classifier on these encoded facts, TLG improves the accuracy of image assessments. The method has set a new benchmark in performance on the WHOOPS! and WEIRD datasets, demonstrating its effectiveness in measuring image realism."
                },
                "zh": {
                    "title": "é€è¿‡é•œå­ï¼šå›¾åƒå¸¸è¯†ä¸€è‡´æ€§çš„æ–°æ–¹æ³•",
                    "desc": "åœ¨äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­ï¼Œè¯„ä¼°çœŸå®å›¾åƒçš„å¤–è§‚æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºé€è¿‡é•œå­ï¼ˆTLGï¼‰ï¼Œç”¨äºè¯„ä¼°å›¾åƒçš„ä¸€è‡´æ€§ä¸å¸¸è¯†ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æå–å›¾åƒä¸­çš„åŸºæœ¬äº‹å®ï¼Œå¹¶é€šè¿‡å¯¹è¿™äº›äº‹å®è¿›è¡Œç¼–ç æ¥è·å¾—å‡†ç¡®çš„ä¿¡æ¯ã€‚æˆ‘ä»¬çš„TLGåœ¨WHOOPS!å’ŒWEIRDæ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨äº†ç´§å‡‘çš„å¾®è°ƒç»„ä»¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13180",
            "title": "ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2505.13180",
            "abstract": "Integrating Large Language Models with symbolic planners is a promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VLM-grounded symbolic approaches and methods that plan directly with a VLM has been hindered by a lack of common environments, evaluation protocols and model coverage. We introduce ViPlan, the first open-source benchmark for Visual Planning with symbolic predicates and VLMs. ViPlan features a series of increasingly challenging tasks in two domains: a visual variant of the classic Blocksworld planning problem and a simulated household robotics environment. We benchmark nine open-source VLM families across multiple sizes, along with selected closed models, evaluating both VLM-grounded symbolic planning and using the models directly to propose actions. We find symbolic planning to outperform direct VLM planning in Blocksworld, where accurate image grounding is crucial, whereas the opposite is true in the household robotics tasks, where commonsense knowledge and the ability to recover from errors are beneficial. Finally, we show that across most models and methods, there is no significant benefit to using Chain-of-Thought prompting, suggesting that current VLMs still struggle with visual reasoning.",
            "score": 11,
            "issue_id": 3851,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "333ba599d7e29ff8",
            "authors": [
                "Matteo Merler",
                "Nicola Dainese",
                "Minttu Alakuijala",
                "Giovanni Bonetta",
                "Pietro Ferrazzi",
                "Yu Tian",
                "Bernardo Magnini",
                "Pekka Marttinen"
            ],
            "affiliations": [
                "Department of Computer Science, Aalto University",
                "Department of Mathematics, UniversitÃ  degli Studi di Padova",
                "Fondazione Bruno Kessler"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13180.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#cv",
                    "#games",
                    "#reasoning",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ViPlan: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ViPlan - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLM Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VLM Ğ² Ğ´Ğ²ÑƒÑ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Blocksworld Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½ĞµĞ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ VLM Ğ² Blocksworld, Ğ³Ğ´Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ° Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½ĞµĞ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Chain-of-Thought Ğ½Ğµ Ğ´Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "ViPlan: Bridging Symbolic Planning and Vision-Language Models for Better Visual Reasoning",
                    "desc": "This paper discusses the integration of Large Language Models (LLMs) with symbolic planners to create more reliable and understandable plans in visual tasks. It introduces ViPlan, an open-source benchmark designed to evaluate various planning methods using Vision-Language Models (VLMs) in two distinct environments: Blocksworld and household robotics. The study compares the performance of symbolic planning against direct VLM planning, revealing that symbolic methods excel in scenarios requiring precise image grounding, while VLMs perform better in tasks that demand commonsense reasoning. Additionally, the findings indicate that current VLMs do not significantly benefit from Chain-of-Thought prompting, highlighting ongoing challenges in visual reasoning capabilities."
                },
                "zh": {
                    "title": "è§†è§‰è§„åˆ’çš„æ–°åŸºå‡†ï¼šViPlan",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ViPlanï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè§†è§‰è§„åˆ’çš„å¼€æºåŸºå‡†ï¼Œç»“åˆäº†ç¬¦å·è§„åˆ’å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç³»åˆ—é€æ¸å¢åŠ éš¾åº¦çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç»å…¸çš„Blocksworldè§„åˆ’é—®é¢˜å’Œæ¨¡æ‹Ÿå®¶åº­æœºå™¨äººç¯å¢ƒã€‚é€šè¿‡å¯¹ä¹ä¸ªå¼€æºVLMæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°ç¬¦å·è§„åˆ’åœ¨Blocksworldä¸­è¡¨ç°ä¼˜äºç›´æ¥ä½¿ç”¨VLMè¿›è¡Œè§„åˆ’ï¼Œè€Œåœ¨å®¶åº­æœºå™¨äººä»»åŠ¡ä¸­åˆ™ç›¸åã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„VLMåœ¨è§†è§‰æ¨ç†æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œä½¿ç”¨Chain-of-Thoughtæç¤ºå¹¶æ²¡æœ‰æ˜¾è‘—æé«˜æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11855",
            "title": "When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification\n  of Scientific Research",
            "url": "https://huggingface.co/papers/2505.11855",
            "abstract": "Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore a complementary application: using LLMs as verifiers to automate the academic verification of scientific manuscripts. To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.",
            "score": 8,
            "issue_id": 3849,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ",
                "en": "May 17",
                "zh": "5æœˆ17æ—¥"
            },
            "hash": "8432e529923dacc5",
            "authors": [
                "Guijin Son",
                "Jiwoo Hong",
                "Honglu Fan",
                "Heejeong Nam",
                "Hyunwoo Ko",
                "Seungwon Lim",
                "Jinyeop Song",
                "Jinha Choi",
                "GonÃ§alo Paulo",
                "Youngjae Yu",
                "Stella Biderman"
            ],
            "affiliations": [
                "Boeing Korea",
                "EleutherAI",
                "KAIST",
                "MIT",
                "OneLineAI",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11855.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#science",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğ±Ñ‹Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SPOT - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 83 Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ñ 91 Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¾Ğ¹, Ğ¿Ñ€Ğ¸Ğ²ĞµĞ´ÑˆĞµĞ¹ Ğº Ğ¾Ğ¿ĞµÑ‡Ğ°Ñ‚ĞºĞ°Ğ¼ Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 21.1% Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ¸ 6.1% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ½ĞµĞ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½ĞµĞ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ÑÑ‚ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ±Ğ»ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ LLM Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ˜Ğ˜-assisted Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚."
                },
                "en": {
                    "title": "Bridging the Gap: LLMs as Verifiers in Scientific Discovery",
                    "desc": "This paper investigates the use of large language models (LLMs) as tools for verifying scientific manuscripts, rather than just generating content. The authors introduce a dataset called SPOT, which includes published papers with significant errors that could lead to errata or retraction. They evaluate various state-of-the-art LLMs on this dataset and find that their performance is lacking, with low recall and precision rates. The study concludes that current LLMs are not yet reliable enough for academic verification, as they often make errors similar to those of novice students."
                },
                "zh": {
                    "title": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å­¦æœ¯éªŒè¯ä¸­çš„æŒ‘æˆ˜",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å­¦æœ¯éªŒè¯ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºSPOTçš„æ•°æ®é›†ï¼ŒåŒ…å«83ç¯‡å·²å‘è¡¨è®ºæ–‡å’Œ91ä¸ªæ˜¾è‘—é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰æœ€å…ˆè¿›çš„LLMsåœ¨è¯†åˆ«é”™è¯¯æ–¹é¢çš„è¡¨ç°ä¸ä½³ï¼Œæœ€é«˜å¬å›ç‡ä»…ä¸º21.1%ï¼Œç²¾ç¡®ç‡ä¸º6.1%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„ç½®ä¿¡åº¦æ™®éè¾ƒä½ï¼Œä¸”åœ¨å¤šæ¬¡ç‹¬ç«‹æµ‹è¯•ä¸­ï¼Œæ¨¡å‹å¾ˆå°‘èƒ½é‡æ–°å‘ç°ç›¸åŒçš„é”™è¯¯ã€‚é€šè¿‡ä¸é¢†åŸŸä¸“å®¶çš„å®šæ€§åˆ†æï¼Œå‘ç°å³ä½¿æ˜¯æœ€å¼ºçš„æ¨¡å‹ä¹Ÿä¼šçŠ¯ç±»ä¼¼å­¦ç”Ÿçº§åˆ«çš„è¯¯è§£é”™è¯¯ï¼Œæ˜¾ç¤ºå‡ºå½“å‰LLMsåœ¨å¯é çš„å­¦æœ¯éªŒè¯ä¸­å­˜åœ¨æ˜¾è‘—å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12849",
            "title": "Accelerate TarFlow Sampling with GS-Jacobi Iteration",
            "url": "https://huggingface.co/papers/2505.12849",
            "abstract": "Image generation models have achieved widespread applications. As an instance, the TarFlow model combines the transformer architecture with Normalizing Flow models, achieving state-of-the-art results on multiple benchmarks. However, due to the causal form of attention requiring sequential computation, TarFlow's sampling process is extremely slow. In this paper, we demonstrate that through a series of optimization strategies, TarFlow sampling can be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as GS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow model have varying importance: a small number of blocks play a major role in image generation tasks, while other blocks contribute relatively little; some blocks are sensitive to initial values and prone to numerical overflow, while others are relatively robust. Based on these two characteristics, we propose the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM is used to identify whether a TarFlow block is \"simple\" (converges in few iterations) or \"tough\" (requires more iterations); IGM is used to evaluate whether the initial value of the iteration is good. Experiments on four TarFlow models demonstrate that GS-Jacobi sampling can significantly enhance sampling efficiency while maintaining the quality of generated images (measured by FID), achieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in Img64uncond, and 2.51x in Img64cond without degrading FID scores or sample quality. Code and checkpoints are accessible on https://github.com/encoreus/GS-Jacobi_for_TarFlow",
            "score": 7,
            "issue_id": 3846,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "191f5a409cc6b32e",
            "authors": [
                "Ben Liu",
                "Zhen Qin"
            ],
            "affiliations": [
                "TapTap, Shanghai, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12849.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#open_source",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² TarFlow Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ TarFlow Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ“Ğ°ÑƒÑÑĞ°-Ğ—ĞµĞ¹Ğ´ĞµĞ»Ñ-Ğ¯ĞºĞ¾Ğ±Ğ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´Ğ²Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸: Convergence Ranking Metric (CRM) Ğ¸ Initial Guessing Metric (IGM). CRM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² TarFlow, Ğ° IGM Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (Ğ´Ğ¾ 5.32 Ñ€Ğ°Ğ·) Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Accelerating TarFlow: Faster Sampling without Quality Loss",
                    "desc": "This paper presents an optimization strategy for the TarFlow model, which combines transformer architecture with Normalizing Flow for image generation. The authors introduce the Gauss-Seidel-Jacobi (GS-Jacobi) iteration method to accelerate the slow sampling process of TarFlow. They identify that certain blocks within the model are more critical for image generation and propose metrics to evaluate their performance: the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM). Experimental results show that the GS-Jacobi method significantly improves sampling speed while preserving image quality, achieving notable speed-ups across various benchmarks."
                },
                "zh": {
                    "title": "åŠ é€ŸTarFlowé‡‡æ ·ï¼Œæå‡å›¾åƒç”Ÿæˆæ•ˆç‡",
                    "desc": "å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸ªåº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚TarFlowæ¨¡å‹ç»“åˆäº†å˜æ¢å™¨æ¶æ„å’Œå½’ä¸€åŒ–æµæ¨¡å‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ç„¶è€Œï¼Œç”±äºå› æœæ³¨æ„åŠ›çš„é¡ºåºè®¡ç®—ï¼ŒTarFlowçš„é‡‡æ ·è¿‡ç¨‹éå¸¸ç¼“æ…¢ã€‚æœ¬æ–‡é€šè¿‡ä¼˜åŒ–ç­–ç•¥ï¼Œåˆ©ç”¨é«˜æ–¯-èµ›å¾·å°”-é›…å¯æ¯”è¿­ä»£æ–¹æ³•æ˜¾è‘—åŠ é€Ÿäº†TarFlowçš„é‡‡æ ·è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13388",
            "title": "R3: Robust Rubric-Agnostic Reward Models",
            "url": "https://huggingface.co/papers/2505.13388",
            "abstract": "Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce R3, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. R3 enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3",
            "score": 6,
            "issue_id": 3860,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "2589c2eb838a62f9",
            "authors": [
                "David Anugraha",
                "Zilu Tang",
                "Lester James V. Miranda",
                "Hanyang Zhao",
                "Mohammad Rifqi Farhansyah",
                "Garry Kuwanto",
                "Derry Wijaya",
                "Genta Indra Winata"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Boston University",
                "Capital One",
                "Columbia University",
                "Institut Teknologi Bandung",
                "Monash Indonesia",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13388.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#optimization",
                    "#benchmark",
                    "#alignment",
                    "#interpretability",
                    "#rlhf"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "R3: ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "R3 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. R3 Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡Ğ½Ğ° Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "R3: A New Era in Reward Modeling for Language Alignment",
                    "desc": "This paper presents R3, a new framework for reward modeling that aims to improve the alignment of language model outputs with human preferences. Unlike traditional reward models that focus on narrow objectives, R3 is designed to be rubric-agnostic and generalizable, allowing it to adapt to various evaluation criteria. It also enhances interpretability by providing reasoned score assignments, making it easier to understand how scores are derived. Overall, R3 promotes a more transparent and flexible approach to evaluating language models, ensuring they align better with diverse human values."
                },
                "zh": {
                    "title": "R3ï¼šæå‡è¯­è¨€æ¨¡å‹å¯¹äººç±»ä»·å€¼çš„å¯¹é½èƒ½åŠ›",
                    "desc": "å¥–åŠ±æ¨¡å‹åœ¨å°†è¯­è¨€æ¨¡å‹çš„è¾“å‡ºä¸äººç±»åå¥½å¯¹é½ä¸­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹å¯æ§æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¿™äº›æ¨¡å‹é€šå¸¸é’ˆå¯¹ç‹­çª„çš„ç›®æ ‡è¿›è¡Œä¼˜åŒ–ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ›´å¹¿æ³›ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„æ ‡é‡è¾“å‡ºåœ¨æ²¡æœ‰ä¸Šä¸‹æ–‡æ¨ç†çš„æƒ…å†µä¸‹éš¾ä»¥è§£é‡Šã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†R3ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œå…·æœ‰ä¸è¯„åˆ†æ ‡å‡†æ— å…³ã€è·¨è¯„ä¼°ç»´åº¦çš„å¯æ¨å¹¿æ€§ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„æ¨ç†è¯„åˆ†åˆ†é…ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12058",
            "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset\n  Generation & Smoke-Tests for Continuous LLM Evaluation",
            "url": "https://huggingface.co/papers/2505.12058",
            "abstract": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost. Born out of the tight feedback-loop demands building the Comet Opik prompt-optimization SDK, where waiting on heavyweight benchmarks breaks developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with a tiny synthetic-data generator pypi package built on provider-agnostic LiteLLM. The generator lets practitioners mint their own tiny packs in any language, domain, or difficulty, while ten ready-made packs already cover Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian, Spanish, and Turkish. Every dataset ships with Croissant metadata and plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so teams can drop deterministic micro-benchmarks directly into pull-request gates, prompt-engineering loops, and production dashboards without touching GPU budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet reliably flags prompt-template errors, tokenizer drift, and fine-tuning side-effects long before full-scale suites like MMLU or BIG-Bench would finish configuring. The entire framework is released to accelerate continuous, resource-efficient quality assurance across the generative-AI ecosystem.",
            "score": 6,
            "issue_id": 3853,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ",
                "en": "May 17",
                "zh": "5æœˆ17æ—¥"
            },
            "hash": "db0c871b64bd6d89",
            "authors": [
                "Vincent Koc"
            ],
            "affiliations": [
                "Comet ML, Inc. New York, NY, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12058.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#synthetic",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞœĞ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ…",
                    "desc": "TQB++ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¸Ğ· 52 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. TQB++ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·ĞµĞ½ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸."
                },
                "en": {
                    "title": "Quick and Efficient Testing for Language Models",
                    "desc": "Tiny QA Benchmark++ (TQB++) is a lightweight, multilingual testing suite designed for large-language-model (LLM) pipelines, providing a quick and cost-effective way to ensure model quality. It includes a small set of gold-standard tests and a synthetic data generator that allows users to create custom test packs in various languages and domains. TQB++ integrates easily with existing tools, enabling developers to incorporate micro-benchmarks into their workflows without significant resource expenditure. This framework aims to enhance continuous quality assurance in generative AI by quickly identifying issues like prompt errors and tokenizer drift before more extensive testing is conducted."
                },
                "zh": {
                    "title": "è½»é‡çº§å¤šè¯­è¨€æµ‹è¯•ï¼Œæå‡AIè´¨é‡ä¿éšœ",
                    "desc": "Tiny QA Benchmark++ï¼ˆTQB++ï¼‰æ˜¯ä¸€ä¸ªè¶…è½»é‡çº§çš„å¤šè¯­è¨€æµ‹è¯•å¥—ä»¶ï¼Œæ—¨åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›å¿«é€Ÿçš„å•å…ƒæµ‹è¯•æ•°æ®é›†ï¼Œè¿è¡Œæ—¶é—´çŸ­ä¸”æˆæœ¬ä½ã€‚å®ƒç»“åˆäº†ä¸€ä¸ª52é¡¹çš„è‹±è¯­é‡‘æ ‡å‡†é›†å’Œä¸€ä¸ªåŸºäºLiteLLMçš„åˆæˆæ•°æ®ç”Ÿæˆå™¨ï¼Œå…è®¸ç”¨æˆ·ç”Ÿæˆé€‚åˆä»»ä½•è¯­è¨€ã€é¢†åŸŸæˆ–éš¾åº¦çš„å°æ•°æ®åŒ…ã€‚TQB++æä¾›äº†åä¸ªç°æˆçš„æ•°æ®åŒ…ï¼Œè¦†ç›–å¤šç§è¯­è¨€ï¼Œå¹¶ä¸”æ¯ä¸ªæ•°æ®é›†éƒ½é™„å¸¦äº†å…ƒæ•°æ®å’Œå¯ç›´æ¥ä½¿ç”¨çš„æ–‡ä»¶ï¼Œæ–¹ä¾¿é›†æˆåˆ°ç°æœ‰çš„å¼€å‘æµç¨‹ä¸­ã€‚è¿™ä¸ªæ¡†æ¶çš„è®¾è®¡æ—¨åœ¨åŠ é€Ÿç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿä¸­çš„æŒç»­ã€èµ„æºé«˜æ•ˆçš„è´¨é‡ä¿è¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13437",
            "title": "FinePhys: Fine-grained Human Action Generation by Explicitly\n  Incorporating Physical Laws for Effective Skeletal Guidance",
            "url": "https://huggingface.co/papers/2505.13437",
            "abstract": "Despite significant advances in video generation, synthesizing physically plausible human actions remains a persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as \"switch leap with 0.5 turn\" poses substantial difficulties for current methods, often yielding unsatisfactory results. To bridge this gap, we propose FinePhys, a Fine-grained human action generation framework that incorporates Physics to obtain effective skeletal guidance. Specifically, FinePhys first estimates 2D poses in an online manner and then performs 2D-to-3D dimension lifting via in-context learning. To mitigate the instability and limited interpretability of purely data-driven 3D poses, we further introduce a physics-based motion re-estimation module governed by Euler-Lagrange equations, calculating joint accelerations via bidirectional temporal updating. The physically predicted 3D poses are then fused with data-driven ones, offering multi-scale 2D heatmap guidance for the diffusion process. Evaluated on three fine-grained action subsets from FineGym (FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms competitive baselines. Comprehensive qualitative results further demonstrate FinePhys's ability to generate more natural and plausible fine-grained human actions.",
            "score": 4,
            "issue_id": 3847,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "28a08dbfb09c6639",
            "authors": [
                "Dian Shao",
                "Mingfei Shi",
                "Shengda Xu",
                "Haodong Chen",
                "Yongle Huang",
                "Binglu Wang"
            ],
            "affiliations": [
                "School of Astronautics, Northwestern Polytechnical University, Xian, China",
                "School of Automation, Northwestern Polytechnical University, Xian, China",
                "School of Software, Northwestern Polytechnical University, Xian, China",
                "Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13437.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¤¸",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FinePhys - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ 2D Ğ¿Ğ¾Ğ·Ñ‹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² 3D Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ”Ğ°Ğ»ĞµĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ­Ğ¹Ğ»ĞµÑ€Ğ°-Ğ›Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 3D Ğ¿Ğ¾Ğ·. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ñ‹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… 2D Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ²Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Bridging Physics and Data for Realistic Human Action Generation",
                    "desc": "This paper presents FinePhys, a framework designed to improve the generation of fine-grained human actions in videos by integrating physics-based modeling. It addresses the challenges of synthesizing complex movements, such as gymnastics routines, by first estimating 2D poses and then converting them to 3D using in-context learning. To enhance the stability and interpretability of the generated poses, FinePhys employs a motion re-estimation module based on Euler-Lagrange equations, which calculates joint accelerations through bidirectional temporal updates. The combination of physics-based predictions with data-driven approaches results in more realistic and coherent human actions, as demonstrated by superior performance on benchmark datasets."
                },
                "zh": {
                    "title": "FinePhysï¼šç‰©ç†é©±åŠ¨çš„ç»†ç²’åº¦äººç±»åŠ¨ä½œç”Ÿæˆæ¡†æ¶",
                    "desc": "å°½ç®¡è§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åˆæˆç‰©ç†ä¸Šåˆç†çš„äººç±»åŠ¨ä½œä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å»ºæ¨¡ç»†ç²’åº¦è¯­ä¹‰å’Œå¤æ‚æ—¶é—´åŠ¨æ€æ–¹é¢ã€‚æœ¬æ–‡æå‡ºäº†FinePhysï¼Œä¸€ä¸ªç»†ç²’åº¦äººç±»åŠ¨ä½œç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆç‰©ç†å­¦ä»¥è·å¾—æœ‰æ•ˆçš„éª¨éª¼æŒ‡å¯¼ã€‚FinePhysé¦–å…ˆä»¥åœ¨çº¿æ–¹å¼ä¼°è®¡2Då§¿åŠ¿ï¼Œç„¶åé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œ2Dåˆ°3Dçš„ç»´åº¦æå‡ã€‚é€šè¿‡å¼•å…¥åŸºäºç‰©ç†çš„è¿åŠ¨é‡æ–°ä¼°è®¡æ¨¡å—ï¼ŒFinePhysèƒ½å¤Ÿç”Ÿæˆæ›´è‡ªç„¶å’Œåˆç†çš„ç»†ç²’åº¦äººç±»åŠ¨ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10238",
            "title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation",
            "url": "https://huggingface.co/papers/2505.10238",
            "abstract": "Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks a significant step forward in this field and opens a new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are on: https://github.com/DINGYANB/MTVCrafter.",
            "score": 4,
            "issue_id": 3851,
            "pub_date": "2025-05-15",
            "pub_date_card": {
                "ru": "15 Ğ¼Ğ°Ñ",
                "en": "May 15",
                "zh": "5æœˆ15æ—¥"
            },
            "hash": "5d8979fc79f9bd4e",
            "authors": [
                "Yanbo Ding",
                "Xirui Hu",
                "Zhizhi Guo",
                "Yali Wang"
            ],
            "affiliations": [
                "chinatelecom.cn",
                "siat.ac.cn",
                "xjtu.edu.cn"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10238.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ•º",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°: Ğ¾Ñ‚ 2D Ğº 4D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "MTVCrafter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 4D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ·. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ 4DMoT Ğ´Ğ»Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ MV-DiT Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. MTVCrafter Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FID-VID Ğ½Ğ° 65% Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Revolutionizing Human Animation with 4D Motion Tokens",
                    "desc": "This paper presents MTVCrafter, a novel framework for human image animation that utilizes raw 3D motion sequences instead of traditional 2D-rendered pose images. By introducing 4DMoT, the framework quantizes 3D motion into 4D motion tokens, which provide enhanced spatio-temporal cues and greater flexibility in animation control. Additionally, MV-DiT employs motion-aware attention mechanisms to effectively utilize these motion tokens, allowing for more expressive human image generation in complex 3D environments. The results demonstrate that MTVCrafter achieves state-of-the-art performance, significantly improving generalization across various character types and styles."
                },
                "zh": {
                    "title": "å¼€åˆ›4Dè¿åŠ¨æ ‡è®°çš„äººåƒåŠ¨ç”»æ–°æ–¹å‘",
                    "desc": "äººåƒåŠ¨ç”»æŠ€æœ¯åœ¨æ•°å­—äººç±»åº”ç”¨ä¸­è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äº2Dæ¸²æŸ“çš„å§¿æ€å›¾åƒï¼Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›å¹¶ä¸¢å¤±äº†é‡è¦çš„3Dä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MTVCrafteræ¡†æ¶ï¼Œå®ƒç›´æ¥å»ºæ¨¡åŸå§‹çš„3Dè¿åŠ¨åºåˆ—ï¼ˆå³4Dè¿åŠ¨ï¼‰ï¼Œå¹¶å¼•å…¥äº†4DMoTï¼ˆ4Dè¿åŠ¨æ ‡è®°å™¨ï¼‰å°†3Dè¿åŠ¨åºåˆ—é‡åŒ–ä¸º4Dè¿åŠ¨æ ‡è®°ã€‚ä¸2Dæ¸²æŸ“çš„å§¿æ€å›¾åƒç›¸æ¯”ï¼Œ4Dè¿åŠ¨æ ‡è®°æä¾›äº†æ›´å¼ºçš„æ—¶ç©ºçº¿ç´¢ï¼Œé¿å…äº†å§¿æ€å›¾åƒä¸è§’è‰²ä¹‹é—´ä¸¥æ ¼çš„åƒç´ çº§å¯¹é½ï¼Œä»è€Œå®ç°äº†æ›´çµæ´»å’Œè§£è€¦çš„æ§åˆ¶ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMTVCrafteråœ¨å¤šç§é£æ ¼å’Œåœºæ™¯ä¸‹å¯¹ä¸åŒçš„å¼€æ”¾ä¸–ç•Œè§’è‰²å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12996",
            "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.12996",
            "abstract": "In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance.",
            "score": 3,
            "issue_id": 3850,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "56ff8af5ab05144f",
            "authors": [
                "Jiaan Wang",
                "Fandong Meng",
                "Jie Zhou"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI, Tencent Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12996.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multilingual",
                    "#rl",
                    "#low_resource",
                    "#machine_translation"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ: Ğ¾Ñ‚ Ğ¾Ğ´Ğ½Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ´Ğ»Ñ 11 ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² 90 Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Multilingual Translation with Advanced Reward Modeling",
                    "desc": "This paper discusses advancements in large reasoning models (LRMs) for neural machine translation (MT), particularly focusing on improving translation capabilities through reinforcement learning (RL). The authors introduce a novel reward modeling method that evaluates translation quality by comparing outputs from a policy MT model against a powerful LRM, DeepSeek-R1-671B. Their approach not only enhances performance in literary translation but also extends to 11 languages, achieving state-of-the-art results and surpassing existing LRMs. The lightweight reward modeling allows for effective transfer of translation skills across multiple languages, demonstrating significant improvements in multilingual MT performance."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¤šè¯­è¨€ç¿»è¯‘æ–°çªç ´",
                    "desc": "è¿‘å¹´æ¥ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚OpenAI-o1å’ŒDeepSeek-R1åœ¨å¤æ‚é—®é¢˜ä¸Šå±•ç°äº†å‡ºè‰²çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢ã€‚ä¸€äº›å¼€åˆ›æ€§ç ”ç©¶å°è¯•å°†LRMsçš„æˆåŠŸåº”ç”¨äºç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ„å»ºå…·æœ‰æ·±åº¦æ¨ç†èƒ½åŠ›çš„MTæ¨¡å‹ã€‚å°½ç®¡å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†è¿™äº›ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é«˜èµ„æºè¯­è¨€ä¸Šï¼Œå¦‚è‹±è¯­å’Œä¸­æ–‡ï¼Œå…¶ä»–è¯­è¨€çš„è¡¨ç°ä»ä¸æ˜ç¡®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡ä¸å¼ºå¤§çš„LRMï¼ˆå¦‚DeepSeek-R1-671Bï¼‰æ¯”è¾ƒç¿»è¯‘ç»“æœï¼Œä¸ºMTæ¨¡å‹æä¾›å¥–åŠ±ï¼Œä»è€Œå……åˆ†å‘æŒ¥å¼ºåŒ–å­¦ä¹ çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12120",
            "title": "HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for\n  Computational Pathology",
            "url": "https://huggingface.co/papers/2505.12120",
            "abstract": "Recent advancements in Digital Pathology (DP), particularly through artificial intelligence and Foundation Models, have underscored the importance of large-scale, diverse, and richly annotated datasets. Despite their critical role, publicly available Whole Slide Image (WSI) datasets often lack sufficient scale, tissue diversity, and comprehensive clinical metadata, limiting the robustness and generalizability of AI models. In response, we introduce the HISTAI dataset, a large, multimodal, open-access WSI collection comprising over 60,000 slides from various tissue types. Each case in the HISTAI dataset is accompanied by extensive clinical metadata, including diagnosis, demographic information, detailed pathological annotations, and standardized diagnostic coding. The dataset aims to fill gaps identified in existing resources, promoting innovation, reproducibility, and the development of clinically relevant computational pathology solutions. The dataset can be accessed at https://github.com/HistAI/HISTAI.",
            "score": 3,
            "issue_id": 3855,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ",
                "en": "May 17",
                "zh": "5æœˆ17æ—¥"
            },
            "hash": "4774ecdbd93d4a7b",
            "authors": [
                "Dmitry Nechaev",
                "Alexey Pchelnikov",
                "Ekaterina Ivanova"
            ],
            "affiliations": [
                "HistAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12120.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#healthcare",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "HISTAI: ĞšÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… HISTAI Ğ´Ğ»Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 60 000 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑÑ€ĞµĞ·Ğ¾Ğ² Ñ‚ĞºĞ°Ğ½ĞµĞ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… HISTAI Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑƒÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ… Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Empowering AI in Pathology with the HISTAI Dataset",
                    "desc": "This paper presents the HISTAI dataset, a significant advancement in Digital Pathology that addresses the limitations of existing Whole Slide Image (WSI) datasets. It comprises over 60,000 slides from diverse tissue types, ensuring a large-scale and multimodal resource for training AI models. Each slide is enriched with comprehensive clinical metadata, including diagnosis and detailed annotations, which enhances the dataset's utility for research and clinical applications. By providing this open-access resource, the authors aim to foster innovation and improve the robustness of AI solutions in computational pathology."
                },
                "zh": {
                    "title": "HISTAIæ•°æ®é›†ï¼šæ¨åŠ¨æ•°å­—ç—…ç†å­¦çš„åˆ›æ–°ä¸å‘å±•",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†HISTAIæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å‹çš„å¤šæ¨¡æ€å¼€æ”¾è·å–çš„å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰é›†åˆï¼ŒåŒ…å«è¶…è¿‡60,000å¼ æ¥è‡ªä¸åŒç»„ç»‡ç±»å‹çš„å¹»ç¯ç‰‡ã€‚è¯¥æ•°æ®é›†é…å¤‡äº†ä¸°å¯Œçš„ä¸´åºŠå…ƒæ•°æ®ï¼ŒåŒ…æ‹¬è¯Šæ–­ã€äººå£ç»Ÿè®¡ä¿¡æ¯ã€è¯¦ç»†çš„ç—…ç†æ³¨é‡Šå’Œæ ‡å‡†åŒ–çš„è¯Šæ–­ç¼–ç ã€‚HISTAIæ•°æ®é›†æ—¨åœ¨å¡«è¡¥ç°æœ‰èµ„æºä¸­çš„ç©ºç™½ï¼Œä¿ƒè¿›åˆ›æ–°ã€å¯é‡å¤æ€§ä»¥åŠä¸´åºŠç›¸å…³è®¡ç®—ç—…ç†è§£å†³æ–¹æ¡ˆçš„å‘å±•ã€‚é€šè¿‡æä¾›å¤šæ ·åŒ–å’Œä¸°å¯Œæ³¨é‡Šçš„æ•°æ®ï¼ŒHISTAIå°†å¢å¼ºäººå·¥æ™ºèƒ½æ¨¡å‹çš„é²æ£’æ€§å’Œæ™®é€‚æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11497",
            "title": "QVGen: Pushing the Limit of Quantized Video Generative Models",
            "url": "https://huggingface.co/papers/2505.11497",
            "abstract": "Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules (Phi) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of Phi, we propose a rank-decay strategy that progressively eliminates Phi. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization gamma to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across 4 state-of-the-art (SOTA) video DMs, with parameter sizes ranging from 1.3B sim14B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of +25.28 in Dynamic Degree and +8.43 in Scene Consistency on VBench.",
            "score": 3,
            "issue_id": 3851,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "48866ba1d6ad838b",
            "authors": [
                "Yushi Huang",
                "Ruihao Gong",
                "Jing Liu",
                "Yifu Ding",
                "Chengtao Lv",
                "Haotong Qin",
                "Jun Zhang"
            ],
            "affiliations": [
                "Beihang University",
                "ETH ZÃ¼rich",
                "Hong Kong University of Science and Technology",
                "Monash University",
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11497.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#inference",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "QVGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ QVGen Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ 4-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ñ… Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Efficient Video Synthesis with Low-Bit Quantization",
                    "desc": "This paper introduces QVGen, a new framework for quantization-aware training (QAT) specifically designed for video diffusion models (DMs). It addresses the challenges of high computational and memory requirements by enabling efficient video synthesis at extremely low-bit quantization levels, such as 4-bit. The authors demonstrate that reducing the gradient norm is crucial for effective convergence in QAT and propose auxiliary modules to minimize quantization errors. Additionally, they implement a rank-decay strategy to eliminate inference overhead while maintaining performance, achieving significant improvements over existing methods in video quality metrics."
                },
                "zh": {
                    "title": "é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼Œæå‡è§†é¢‘åˆæˆæ•ˆç‡ï¼",
                    "desc": "è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨é«˜è´¨é‡è§†é¢‘åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶è®¡ç®—å’Œå†…å­˜éœ€æ±‚é«˜ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ¡†æ¶QVGenï¼Œæ—¨åœ¨åœ¨æä½ä½é‡åŒ–ï¼ˆå¦‚4ä½æˆ–æ›´ä½ï¼‰ä¸‹å®ç°é«˜æ€§èƒ½å’Œé«˜æ•ˆæ¨ç†ã€‚æˆ‘ä»¬é€šè¿‡ç†è®ºåˆ†æè¡¨æ˜ï¼Œé™ä½æ¢¯åº¦èŒƒæ•°å¯¹QATçš„æ”¶æ•›è‡³å…³é‡è¦ï¼Œå¹¶å¼•å…¥è¾…åŠ©æ¨¡å—ï¼ˆPhiï¼‰æ¥å‡å°é‡åŒ–è¯¯å·®ï¼Œä»è€Œæ˜¾è‘—æé«˜æ”¶æ•›æ€§ã€‚é€šè¿‡é€æ­¥æ¶ˆé™¤Phiçš„æ¨ç†å¼€é”€ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜QVGenåœ¨4ä½è®¾ç½®ä¸‹é¦–æ¬¡å®ç°äº†ä¸å…¨ç²¾åº¦ç›¸å½“çš„è´¨é‡ï¼Œå¹¶æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11484",
            "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2505.11484",
            "abstract": "Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.",
            "score": 3,
            "issue_id": 3851,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "4a502c14d07f6e6c",
            "authors": [
                "Yige Xu",
                "Xu Guo",
                "Zhiwei Zeng",
                "Chunyan Miao"
            ],
            "affiliations": [
                "Alibaba-NTU Global e-Sustainability CorpLab (ANGEL)",
                "College of Computing and Data Science, Nanyang Technological University, Singapore",
                "Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly",
                "KTH Royal Institute of Technology, Sweden"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11484.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#benchmark",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ñ‹ÑĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SoftCoT++, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², SoftCoT++ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¼ÑĞ³ĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ñ‹ÑĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SoftCoT++ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ SoftCoT Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ÑÑ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Reasoning with Diverse Latent Thoughts",
                    "desc": "This paper introduces SoftCoT++, an advanced method for Test-Time Scaling (TTS) that enhances reasoning performance in machine learning models. Unlike traditional TTS methods that work with discrete tokens, SoftCoT++ leverages continuous latent space to improve the quality of reasoning by allowing for diverse exploration of thought paths. By perturbing latent thoughts with specialized initial tokens and using contrastive learning, the method promotes diversity in soft thought representations. Experimental results show that SoftCoT++ significantly outperforms previous methods, demonstrating its effectiveness across various reasoning benchmarks and compatibility with existing scaling techniques."
                },
                "zh": {
                    "title": "å¤šæ ·åŒ–æ€ç»´è·¯å¾„çš„æ¢ç´¢æ–°æ–¹æ³•",
                    "desc": "æµ‹è¯•æ—¶æ‰©å±•ï¼ˆTTSï¼‰æ˜¯ä¸€ç§åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡åˆ†é…é¢å¤–è®¡ç®—æ¥æé«˜æ¨ç†æ€§èƒ½çš„æ–¹æ³•ï¼Œè€Œä¸æ”¹å˜æ¨¡å‹å‚æ•°ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ€è€ƒå¯ä»¥è¿›ä¸€æ­¥æå‡æ¨ç†æ€§èƒ½ã€‚ä¸ç¦»æ•£è§£ç ä¸åŒï¼Œè¿ç»­ç©ºé—´ä¸­çš„æ½œåœ¨è¡¨ç¤ºæ˜¯å›ºå®šçš„ï¼Œè¿™é™åˆ¶äº†å¤šæ ·åŒ–çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SoftCoT++ï¼Œé€šè¿‡æ‰°åŠ¨æ½œåœ¨æ€ç»´å¹¶åº”ç”¨å¯¹æ¯”å­¦ä¹ æ¥ä¿ƒè¿›æ€ç»´è·¯å¾„çš„å¤šæ ·æ€§ï¼Œä»è€Œæ‰©å±•äº†SoftCoTåœ¨æµ‹è¯•æ—¶æ‰©å±•èŒƒå¼ä¸­çš„åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13840",
            "title": "EfficientLLM: Efficiency in Large Language Models",
            "url": "https://huggingface.co/papers/2505.13840",
            "abstract": "Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first comprehensive empirical study evaluating efficiency techniques for LLMs at scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our study systematically explores three key axes: (1) architecture pretraining (efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts (MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and (3) inference (quantization methods: int4, float16). We define six fine-grained metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy Consumption, Compression Rate) to capture hardware saturation, latency-throughput balance, and carbon cost. Evaluating over 100 model-technique pairs (0.5B-72B parameters), we derive three core insights: (i) Efficiency involves quantifiable trade-offs: no single method is universally optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by 40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5% accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal memory-latency trade-offs for constrained devices, MLA achieves lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency only beyond 14B parameters. (iii) Techniques generalize across modalities: we extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides essential guidance for researchers and engineers navigating the efficiency-performance landscape of next-generation foundation models.",
            "score": 2,
            "issue_id": 3867,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "01e23584249e610e",
            "authors": [
                "Zhengqing Yuan",
                "Weixiang Sun",
                "Yixin Liu",
                "Huichi Zhou",
                "Rong Zhou",
                "Yiyang Li",
                "Zheyuan Zhang",
                "Wei Song",
                "Yue Huang",
                "Haolong Jia",
                "Keerthiram Murugesan",
                "Yu Wang",
                "Lifang He",
                "Jianfeng Gao",
                "Lichao Sun",
                "Yanfang Ye"
            ],
            "affiliations": [
                "Imperial College London",
                "International Business Machines Corporation (IBM)",
                "Lehigh University",
                "Microsoft Research",
                "Rutgers University",
                "University of Illinois Chicago",
                "University of Notre Dame"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13840.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#open_source",
                    "#multimodal",
                    "#inference",
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "EfficientLLM: ĞšĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EfficientLLM - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑˆĞµÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ³Ğ»ĞµÑ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ğ¿Ğ°Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ² Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. EfficientLLM Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Optimizing Efficiency in Large Language Models",
                    "desc": "This paper presents EfficientLLM, a benchmark designed to evaluate the efficiency of Large Language Models (LLMs) at scale. It systematically investigates various techniques across architecture pretraining, fine-tuning, and inference, using a comprehensive set of metrics to assess performance. The study reveals that efficiency techniques involve trade-offs that vary by task and model size, highlighting that no single method is best for all scenarios. Additionally, the findings indicate that these efficiency techniques can be applied across different model types, including vision and vision-language models, providing valuable resources for future research."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨¡å‹ï¼ŒèŠ‚èƒ½é™è€—ï¼",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŠ€æœ¯ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶ä¸æ–­å¢é•¿çš„å‚æ•°æ•°é‡å’Œä¸Šä¸‹æ–‡çª—å£å¯¼è‡´äº†é«˜æ˜‚çš„è®¡ç®—ã€èƒ½æºå’Œç»æµæˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†EfficientLLMï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œé¦–æ¬¡å…¨é¢è¯„ä¼°äº†å¤§è§„æ¨¡LLMsçš„æ•ˆç‡æŠ€æœ¯ã€‚ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†æ¶æ„é¢„è®­ç»ƒã€å¾®è°ƒå’Œæ¨ç†ç­‰ä¸‰ä¸ªå…³é”®æ–¹é¢ï¼Œå¹¶å®šä¹‰äº†å…­ä¸ªç»†è‡´çš„æŒ‡æ ‡æ¥æ•æ‰ç¡¬ä»¶é¥±å’Œåº¦ã€å»¶è¿Ÿ-ååé‡å¹³è¡¡å’Œç¢³æˆæœ¬ã€‚é€šè¿‡å¯¹100å¤šä¸ªæ¨¡å‹-æŠ€æœ¯å¯¹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å¾—å‡ºäº†æ•ˆç‡æ¶‰åŠå¯é‡åŒ–æƒè¡¡ã€æœ€ä¼˜è§£ä¾èµ–äºä»»åŠ¡å’Œè§„æ¨¡ä»¥åŠæŠ€æœ¯åœ¨ä¸åŒæ¨¡æ€é—´çš„é€šç”¨æ€§ç­‰ä¸‰å¤§æ ¸å¿ƒè§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12872",
            "title": "From Grunts to Grammar: Emergent Language from Cooperative Foraging",
            "url": "https://huggingface.co/papers/2505.12872",
            "abstract": "Early cavemen relied on gestures, vocalizations, and simple signals to coordinate, plan, avoid predators, and share resources. Today, humans collaborate using complex languages to achieve remarkable results. What drives this evolution in communication? How does language emerge, adapt, and become vital for teamwork? Understanding the origins of language remains a challenge. A leading hypothesis in linguistics and anthropology posits that language evolved to meet the ecological and social demands of early human cooperation. Language did not arise in isolation, but through shared survival goals. Inspired by this view, we investigate the emergence of language in multi-agent Foraging Games. These environments are designed to reflect the cognitive and ecological constraints believed to have influenced the evolution of communication. Agents operate in a shared grid world with only partial knowledge about other agents and the environment, and must coordinate to complete games like picking up high-value targets or executing temporally ordered actions. Using end-to-end deep reinforcement learning, agents learn both actions and communication strategies from scratch. We find that agents develop communication protocols with hallmark features of natural language: arbitrariness, interchangeability, displacement, cultural transmission, and compositionality. We quantify each property and analyze how different factors, such as population size and temporal dependencies, shape specific aspects of the emergent language. Our framework serves as a platform for studying how language can evolve from partial observability, temporal reasoning, and cooperative goals in embodied multi-agent settings. We will release all data, code, and models publicly.",
            "score": 2,
            "issue_id": 3853,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "cb62284e65b8c471",
            "authors": [
                "Maytus Piriyajitakonkij",
                "Rujikorn Charakorn",
                "Weicheng Tao",
                "Wei Pan",
                "Mingfei Sun",
                "Cheston Tan",
                "Mengmi Zhang"
            ],
            "affiliations": [
                "Centre for Frontier AI Research (CFAR), ASTAR, Singapore",
                "College of Computing and Data Science, Nanyang Technological University, Singapore",
                "Department of Computer Science, The University of Manchester, United Kingdom",
                "Institute for Infocomm Research (I2R), ASTAR, Singapore",
                "Sakana AI, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12872.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#agents",
                    "#reasoning",
                    "#open_source",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€ Ğ¿Ğ¾ Ğ´Ğ¾Ğ±Ñ‹Ñ‡Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ€ĞµĞ´Ñƒ, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰ÑƒÑ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ²Ğ»Ğ¸ÑĞ²ÑˆĞ¸Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñƒ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ğ»ÑĞ´ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°: Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ, ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ, ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Evolving Language Through Cooperative Learning in Multi-Agent Systems",
                    "desc": "This paper explores how language can emerge in multi-agent systems through the lens of Foraging Games, which simulate the ecological and social conditions of early human cooperation. Using end-to-end deep reinforcement learning, agents learn to communicate and coordinate their actions in a shared environment with limited information. The study reveals that these agents develop communication protocols that exhibit characteristics similar to natural language, such as arbitrariness and compositionality. By analyzing how factors like population size and temporal dependencies influence language features, the research provides insights into the evolutionary processes of communication."
                },
                "zh": {
                    "title": "è¯­è¨€çš„æ¼”åŒ–ï¼šä»åˆä½œåˆ°æ²Ÿé€šçš„æ—…ç¨‹",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è¯­è¨€å¦‚ä½•åœ¨å¤šæ™ºèƒ½ä½“è§…é£Ÿæ¸¸æˆä¸­å‡ºç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ™ºèƒ½ä½“åœ¨å…±äº«çš„ç¯å¢ƒä¸­ï¼Œé€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ å­¦ä¹ è¡ŒåŠ¨å’Œæ²Ÿé€šç­–ç•¥ï¼Œé€æ¸å‘å±•å‡ºå…·æœ‰è‡ªç„¶è¯­è¨€ç‰¹å¾çš„æ²Ÿé€šåè®®ã€‚è®ºæ–‡é‡åŒ–äº†è¯­è¨€çš„ä¸åŒç‰¹æ€§ï¼Œå¦‚ä»»æ„æ€§ã€å¯äº’æ¢æ€§å’Œæ–‡åŒ–ä¼ é€’ç­‰ï¼Œå¹¶åˆ†æäº†äººå£è§„æ¨¡å’Œæ—¶é—´ä¾èµ–æ€§ç­‰å› ç´ å¦‚ä½•å½±å“è¯­è¨€çš„æ¼”å˜ã€‚è¯¥æ¡†æ¶ä¸ºç ”ç©¶è¯­è¨€å¦‚ä½•åœ¨åˆä½œç›®æ ‡å’Œéƒ¨åˆ†å¯è§‚å¯Ÿæ€§ä¸­æ¼”åŒ–æä¾›äº†å¹³å°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11733",
            "title": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from\n  clinical case reports",
            "url": "https://huggingface.co/papers/2505.11733",
            "abstract": "Doctors and patients alike increasingly use Large Language Models (LLMs) to diagnose clinical cases. However, unlike domains such as math or coding, where correctness can be objectively defined by the final answer, medical diagnosis requires both the outcome and the reasoning process to be accurate. Currently, widely used medical benchmarks like MedQA and MMLU assess only accuracy in the final answer, overlooking the quality and faithfulness of the clinical reasoning process. To address this limitation, we introduce MedCaseReasoning, the first open-access dataset for evaluating LLMs on their ability to align with clinician-authored diagnostic reasoning. The dataset includes 14,489 diagnostic question-and-answer cases, each paired with detailed reasoning statements derived from open-access medical case reports. We evaluate state-of-the-art reasoning LLMs on MedCaseReasoning and find significant shortcomings in their diagnoses and reasoning: for instance, the top-performing open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy and mentions only 64% of the clinician reasoning statements (recall). However, we demonstrate that fine-tuning LLMs on the reasoning traces derived from MedCaseReasoning significantly improves diagnostic accuracy and clinical reasoning recall by an average relative gain of 29% and 41%, respectively. The open-source dataset, code, and models are available at https://github.com/kevinwu23/Stanford-MedCaseReasoning.",
            "score": 2,
            "issue_id": 3864,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "0eaea70de66597a0",
            "authors": [
                "Kevin Wu",
                "Eric Wu",
                "Rahul Thapa",
                "Kevin Wei",
                "Angela Zhang",
                "Arvind Suresh",
                "Jacqueline J. Tao",
                "Min Woo Sun",
                "Alejandro Lozano",
                "James Zou"
            ],
            "affiliations": [
                "Stanford University",
                "University of California, San Francisco",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11733.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#training",
                    "#benchmark",
                    "#alignment",
                    "#data",
                    "#healthcare",
                    "#reasoning"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MedCaseReasoning Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞµ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 14 Ñ‚Ñ‹ÑÑÑ‡ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°Ñ…. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ² Ğ¸Ñ… Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MedCaseReasoning Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Medical Diagnosis with Reasoning Evaluation",
                    "desc": "This paper addresses the limitations of current medical benchmarks that only evaluate the final accuracy of Large Language Models (LLMs) in clinical diagnosis. It introduces MedCaseReasoning, a new dataset designed to assess both the accuracy of diagnoses and the quality of reasoning behind them, featuring 14,489 cases with clinician-authored explanations. The authors evaluate existing LLMs and find that they struggle with both diagnostic accuracy and recalling reasoning statements. By fine-tuning these models on the new dataset, they achieve significant improvements in both diagnostic accuracy and reasoning recall, demonstrating the importance of evaluating the reasoning process in medical AI applications."
                },
                "zh": {
                    "title": "æå‡åŒ»å­¦è¯Šæ–­çš„æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†MedCaseReasoningæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„åŒ»å­¦åŸºå‡†ä¸åŒï¼Œè¯¥æ•°æ®é›†ä¸ä»…å…³æ³¨æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œè¿˜é‡è§†ä¸´åºŠæ¨ç†è¿‡ç¨‹çš„è´¨é‡å’Œå¯ä¿¡åº¦ã€‚æˆ‘ä»¬å‘ç°å½“å‰æœ€å…ˆè¿›çš„LLMsåœ¨è¯Šæ–­å’Œæ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å‡†ç¡®æ€§å’Œæ¨ç†å›å¿†ç‡ä¸Šã€‚é€šè¿‡å¯¹LLMsè¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨MedCaseReasoningçš„æ•°æ®ï¼Œè¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†å›å¿†ç‡åˆ†åˆ«æé«˜äº†29%å’Œ41%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11475",
            "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across\n  Diverse Tasks and Languages",
            "url": "https://huggingface.co/papers/2505.11475",
            "abstract": "Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): https://huggingface.co/datasets/nvidia/HelpSteer3#preference",
            "score": 2,
            "issue_id": 3854,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "82282826ff02b787",
            "authors": [
                "Zhilin Wang",
                "Jiaqi Zeng",
                "Olivier Delalleau",
                "Hoo-Chang Shin",
                "Felipe Soares",
                "Alexander Bukharin",
                "Ellie Evans",
                "Yi Dong",
                "Oleksii Kuchaiev"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11475.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rlhf",
                    "#multilingual",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "HelpSteer3-Preference: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ RLHF",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ HelpSteer3-Preference - Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF). ĞĞ°Ğ±Ğ¾Ñ€ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 40 000 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ STEM, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ HelpSteer3-Preference, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (Reward Models), Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ÑˆĞ¸Ğµ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… RM-Bench Ğ¸ JudgeBench. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RLHF."
                },
                "en": {
                    "title": "Enhancing Language Models with High-Quality Preference Data",
                    "desc": "This paper presents HelpSteer3-Preference, a new dataset designed to improve the training of language models using Reinforcement Learning from Human Feedback (RLHF). It contains over 40,000 high-quality, human-annotated preference samples that cover a wide range of real-world applications, including STEM and coding tasks. The dataset has been shown to significantly enhance the performance of Reward Models (RMs), achieving top scores on benchmark tests. Additionally, the paper discusses how this dataset can be utilized to train Generative RMs and align policy models with RLHF techniques."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹çš„åå¥½æ•°æ®é›†",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†HelpSteer3-Preferenceï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„äººç±»æ ‡æ³¨åå¥½æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡40,000ä¸ªæ ·æœ¬ï¼Œæ—¨åœ¨æå‡é€šç”¨é¢†åŸŸçš„è¯­è¨€æ¨¡å‹è®­ç»ƒã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å¤šç§çœŸå®ä¸–ç•Œåº”ç”¨ï¼ŒåŒ…æ‹¬STEMã€ç¼–ç¨‹å’Œå¤šè¯­è¨€åœºæ™¯ï¼Œå…·æœ‰å¤šæ ·æ€§å’Œé«˜è´¨é‡ã€‚é€šè¿‡ä½¿ç”¨HelpSteer3-Preferenceï¼Œæˆ‘ä»¬è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹åœ¨RM-Benchå’ŒJudgeBenchä¸Šå–å¾—äº†ä¼˜å¼‚çš„è¡¨ç°ï¼Œæ˜¾è‘—æé«˜äº†ä¹‹å‰æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¯¥æ•°æ®é›†è®­ç»ƒç”Ÿæˆæ€§å¥–åŠ±æ¨¡å‹ï¼Œå¹¶å°†ç­–ç•¥æ¨¡å‹ä¸äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10420",
            "title": "Learned Lightweight Smartphone ISP with Unpaired Data",
            "url": "https://huggingface.co/papers/2505.10420",
            "abstract": "The Image Signal Processor (ISP) is a fundamental component in modern smartphone cameras responsible for conversion of RAW sensor image data to RGB images with a strong focus on perceptual quality. Recent work highlights the potential of deep learning approaches and their ability to capture details with a quality increasingly close to that of professional cameras. A difficult and costly step when developing a learned ISP is the acquisition of pixel-wise aligned paired data that maps the raw captured by a smartphone camera sensor to high-quality reference images. In this work, we address this challenge by proposing a novel training method for a learnable ISP that eliminates the need for direct correspondences between raw images and ground-truth data with matching content. Our unpaired approach employs a multi-term loss function guided by adversarial training with multiple discriminators processing feature maps from pre-trained networks to maintain content structure while learning color and texture characteristics from the target RGB dataset. Using lightweight neural network architectures suitable for mobile devices as backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm UltraISP datasets. Compared to paired training methods, our unpaired learning strategy shows strong potential and achieves high fidelity across multiple evaluation metrics. The code and pre-trained models are available at https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .",
            "score": 2,
            "issue_id": 3855,
            "pub_date": "2025-05-15",
            "pub_date_card": {
                "ru": "15 Ğ¼Ğ°Ñ",
                "en": "May 15",
                "zh": "5æœˆ15æ—¥"
            },
            "hash": "0ba36884ee806c6f",
            "authors": [
                "Andrei Arhire",
                "Radu Timofte"
            ],
            "affiliations": [
                "Computer Vision Lab, CAIDAS & IFI University of Wurzburg",
                "Faculty of Computer Science Alexandru Ioan Cuza University of Iasi"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10420.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ„Ğ¾Ñ‚Ğ¾ Ğ±ĞµĞ· Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Image Signal Processor (ISP) Ğ´Ğ»Ñ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµÑ‚ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ†Ğ²ĞµÑ‚Ñƒ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Zurich RAW to RGB Ğ¸ Fujifilm UltraISP Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ»ĞµĞ³ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Smartphone Imaging with Unpaired Learning!",
                    "desc": "This paper presents a new method for training an Image Signal Processor (ISP) using unpaired data, which means it does not require exact matches between raw images and high-quality reference images. The authors utilize a multi-term loss function and adversarial training with multiple discriminators to effectively learn color and texture characteristics while preserving content structure. Their approach is designed for lightweight neural networks, making it suitable for mobile devices. The results demonstrate that this unpaired learning strategy outperforms traditional paired methods in terms of image quality and fidelity."
                },
                "zh": {
                    "title": "æ— é…å¯¹æ•°æ®çš„å­¦ä¹ å‹å›¾åƒä¿¡å·å¤„ç†å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºå­¦ä¹ å›¾åƒä¿¡å·å¤„ç†å™¨ï¼ˆISPï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•ä¸­éœ€è¦æˆå¯¹çš„åŸå§‹å›¾åƒå’Œé«˜è´¨é‡å‚è€ƒå›¾åƒçš„é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨æ— é…å¯¹çš„å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒå’Œå¤šé‡åˆ¤åˆ«å™¨æ¥ä¿æŒå†…å®¹ç»“æ„ï¼ŒåŒæ—¶å­¦ä¹ ç›®æ ‡RGBæ•°æ®é›†çš„é¢œè‰²å’Œçº¹ç†ç‰¹å¾ã€‚ä½¿ç”¨é€‚åˆç§»åŠ¨è®¾å¤‡çš„è½»é‡çº§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºå‡ºä¸æˆå¯¹è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„ä¿çœŸåº¦å’Œæ½œåŠ›ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨æŒ‡å®šé“¾æ¥è·å–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13181",
            "title": "Efficient Speech Language Modeling via Energy Distance in Continuous\n  Latent Space",
            "url": "https://huggingface.co/papers/2505.13181",
            "abstract": "We introduce SLED, an alternative approach to speech language modeling by encoding speech waveforms into sequences of continuous latent representations and modeling them autoregressively using an energy distance objective. The energy distance offers an analytical measure of the distributional gap by contrasting simulated and target samples, enabling efficient training to capture the underlying continuous autoregressive distribution. By bypassing reliance on residual vector quantization, SLED avoids discretization errors and eliminates the need for the complicated hierarchical architectures common in existing speech language models. It simplifies the overall modeling pipeline while preserving the richness of speech information and maintaining inference efficiency. Empirical results demonstrate that SLED achieves strong performance in both zero-shot and streaming speech synthesis, showing its potential for broader applications in general-purpose speech language models.",
            "score": 1,
            "issue_id": 3867,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "5121a5b0ecc79bb1",
            "authors": [
                "Zhengrui Ma",
                "Yang Feng",
                "Chenze Shao",
                "Fandong Meng",
                "Jie Zhou",
                "Min Zhang"
            ],
            "affiliations": [
                "Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences",
                "Pattern Recognition Center, WeChat AI, Tencent Inc",
                "School of Future Science and Engineering, Soochow University",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13181.jpg",
            "data": {
                "categories": [
                    "#audio"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "SLED: ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "SLED - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ»Ğ½Ñ‹ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€ĞµÑ‡Ğ¸. SLED ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ SLED Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ."
                },
                "en": {
                    "title": "SLED: Simplifying Speech Modeling with Continuous Representations",
                    "desc": "SLED is a new method for speech language modeling that transforms speech waveforms into continuous latent representations. It uses an autoregressive approach combined with an energy distance objective to effectively measure and minimize the differences between simulated and actual speech samples. This method avoids common issues found in traditional models, such as discretization errors and complex architectures, leading to a simpler and more efficient modeling process. The results show that SLED performs well in generating speech, even in zero-shot and streaming scenarios, indicating its versatility for various speech applications."
                },
                "zh": {
                    "title": "SLEDï¼šç®€åŒ–è¯­éŸ³å»ºæ¨¡çš„æ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºSLEDçš„è¯­éŸ³è¯­è¨€å»ºæ¨¡æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡å°†è¯­éŸ³æ³¢å½¢ç¼–ç ä¸ºè¿ç»­æ½œåœ¨è¡¨ç¤ºåºåˆ—ï¼Œå¹¶ä½¿ç”¨èƒ½é‡è·ç¦»ç›®æ ‡è¿›è¡Œè‡ªå›å½’å»ºæ¨¡ã€‚èƒ½é‡è·ç¦»æä¾›äº†ä¸€ç§åˆ†ææ€§åº¦é‡ï¼Œç”¨äºå¯¹æ¯”æ¨¡æ‹Ÿæ ·æœ¬å’Œç›®æ ‡æ ·æœ¬ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ï¼Œä»è€Œå®ç°é«˜æ•ˆè®­ç»ƒï¼Œæ•æ‰æ½œåœ¨çš„è¿ç»­è‡ªå›å½’åˆ†å¸ƒã€‚SLEDé¿å…äº†å¯¹æ®‹å·®å‘é‡é‡åŒ–çš„ä¾èµ–ï¼Œæ¶ˆé™¤äº†ç¦»æ•£åŒ–è¯¯å·®ï¼Œå¹¶ç®€åŒ–äº†ç°æœ‰è¯­éŸ³è¯­è¨€æ¨¡å‹ä¸­å¸¸è§çš„å¤æ‚å±‚æ¬¡ç»“æ„ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒSLEDåœ¨é›¶æ ·æœ¬å’Œæµå¼è¯­éŸ³åˆæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨é€šç”¨è¯­éŸ³è¯­è¨€æ¨¡å‹ä¸­çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12781",
            "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation\n  through Low-Rank Clone",
            "url": "https://huggingface.co/papers/2505.12781",
            "abstract": "Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.",
            "score": 1,
            "issue_id": 3860,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "1ba09e10a601b724",
            "authors": [
                "Jitai Hao",
                "Qiang Huang",
                "Hao Liu",
                "Xinyan Xiao",
                "Zhaochun Ren",
                "Jun Yu"
            ],
            "affiliations": [
                "Baidu Inc.",
                "Harbin Institute of Technology, Shenzhen",
                "Leiden University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12781.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#transfer_learning",
                    "#training",
                    "#small_models"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Low-Rank Clone (LRC). LRC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸Ğ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑĞ¼Ğ¸, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LRC Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 20 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 1000 Ñ€Ğ°Ğ·."
                },
                "en": {
                    "title": "Efficient Training of Small Language Models with Low-Rank Clone",
                    "desc": "This paper presents a new method called Low-Rank Clone (LRC) for training Small Language Models (SLMs) efficiently. LRC addresses three main challenges in model training: reducing information loss from hard pruning, improving the alignment of representations, and better utilizing activations from Feed-Forward Networks. By using low-rank projection matrices, LRC allows for soft pruning and aligns student activations with those of a teacher model, enhancing knowledge transfer. The results show that LRC achieves high performance comparable to larger models while significantly reducing the amount of training data needed."
                },
                "zh": {
                    "title": "ä½ç§©å…‹éš†ï¼šé«˜æ•ˆè®­ç»ƒå°å‹è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºä½ç§©å…‹éš†ï¼ˆLow-Rank Clone, LRCï¼‰çš„é«˜æ•ˆé¢„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è®­ç»ƒå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰æ—¶é¢ä¸´çš„ä¿¡æ¯æŸå¤±ã€è¡¨ç¤ºå¯¹é½æ•ˆç‡ä½å’Œæ¿€æ´»åˆ©ç”¨ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚LRCé€šè¿‡æ„å»ºä½ç§©æŠ•å½±çŸ©é˜µï¼Œå®ç°äº†æ•™å¸ˆæ¨¡å‹æƒé‡çš„è½¯ä¿®å‰ªå’Œå­¦ç”Ÿæ¨¡å‹æ¿€æ´»çš„å¯¹é½ï¼Œç‰¹åˆ«æ˜¯å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰çš„ä¿¡å·ã€‚è¯¥æ–¹æ³•æœ€å¤§åŒ–äº†çŸ¥è¯†è½¬ç§»çš„æ•ˆç‡ï¼Œæ¶ˆé™¤äº†å¯¹æ˜¾å¼å¯¹é½æ¨¡å—çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLRCåœ¨ä½¿ç”¨ä»…20Bæ ‡è®°çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤ŸåŒ¹é…æˆ–è¶…è¶Šåœ¨ä¸‡äº¿æ ‡è®°ä¸Šè®­ç»ƒçš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œè¾¾åˆ°äº†è¶…è¿‡1000å€çš„è®­ç»ƒæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12257",
            "title": "LLM Context Conditioning and PWP Prompting for Multimodal Validation of\n  Chemical Formulas",
            "url": "https://huggingface.co/papers/2505.12257",
            "abstract": "Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. This exploratory proof-of-concept (PoC) study investigates structured LLM context conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a methodological strategy to modulate this LLM behavior at inference time. The approach is designed to enhance the reliability of readily available, general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for precise validation tasks, crucially relying only on their standard chat interfaces without API access or model modifications. To explore this methodology, we focused on validating chemical formulas within a single, complex test paper with known textual and image-based errors. Several prompting strategies were evaluated: while basic prompts proved unreliable, an approach adapting PWP structures to rigorously condition the LLM's analytical mindset appeared to improve textual error identification with both models. Notably, this method also guided Gemini 2.5 Pro to repeatedly identify a subtle image-based formula error previously overlooked during manual review, a task where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight specific LLM operational modes that impede detail-oriented validation and suggest that PWP-informed context conditioning offers a promising and highly accessible technique for developing more robust LLM-driven analytical workflows, particularly for tasks requiring meticulous error detection in scientific and technical documents. Extensive validation beyond this limited PoC is necessary to ascertain broader applicability.",
            "score": 1,
            "issue_id": 3849,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "74901d316cc1d6cb",
            "authors": [
                "Evgeny Markhasin"
            ],
            "affiliations": [
                "Lobachevsky State University of Nizhny Novgorod"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12257.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#multimodal",
                    "#interpretability",
                    "#inference"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (PWP) Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… LLM Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ PWP-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ» Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemini 2.5 Pro Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ² Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğµ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾-Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing LLM Accuracy in Scientific Error Detection with PWP",
                    "desc": "This paper explores how to improve Large Language Models (LLMs) in identifying subtle errors in complex scientific documents, especially those with images and formulas. It introduces a method called Persistent Workflow Prompting (PWP) to better condition LLMs during their analysis, enhancing their ability to detect inaccuracies. The study specifically tests this approach on Gemini 2.5 Pro and ChatGPT Plus o3, showing that PWP can significantly improve error detection compared to basic prompting strategies. The findings suggest that this method could lead to more reliable LLMs for validating technical content, although further research is needed to confirm its effectiveness across different contexts."
                },
                "zh": {
                    "title": "æå‡LLMåœ¨ç§‘å­¦æ–‡æ¡£ä¸­çš„é”™è¯¯è¯†åˆ«èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡æ¡ä»¶æ¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ç§‘å­¦å’ŒæŠ€æœ¯æ–‡æ¡£ä¸­çš„é”™è¯¯è¯†åˆ«èƒ½åŠ›ã€‚ç ”ç©¶é‡‡ç”¨äº†æŒä¹…å·¥ä½œæç¤ºï¼ˆPWPï¼‰åŸåˆ™ï¼Œæ—¨åœ¨æé«˜LLMsåœ¨æ¨ç†æ—¶çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨éªŒè¯åŒ–å­¦å…¬å¼æ—¶ã€‚é€šè¿‡å¯¹ä¸åŒæç¤ºç­–ç•¥çš„è¯„ä¼°ï¼Œå‘ç°é€‚åº”PWPç»“æ„çš„æç¤ºèƒ½å¤Ÿæœ‰æ•ˆæé«˜æ–‡æœ¬é”™è¯¯çš„è¯†åˆ«ç‡ï¼Œå¹¶æˆåŠŸå‘ç°äº†ä¹‹å‰æœªè¢«æ‰‹åŠ¨å®¡æŸ¥è¯†åˆ«çš„å›¾åƒå…¬å¼é”™è¯¯ã€‚è¯¥æ–¹æ³•ä¸ºå¼€å‘æ›´å¼ºå¤§çš„LLMé©±åŠ¨åˆ†æå·¥ä½œæµæä¾›äº†æœ‰å¸Œæœ›çš„æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç»†è‡´é”™è¯¯æ£€æµ‹çš„ä»»åŠ¡ä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11988",
            "title": "TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique\n  Annotation in Cyber Threat Intelligence Text",
            "url": "https://huggingface.co/papers/2505.11988",
            "abstract": "Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations, such as custom hard-negative mining and denoising, resources rarely available in specialized domains.   We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal text-technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques.   Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights.",
            "score": 1,
            "issue_id": 3850,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ",
                "en": "May 17",
                "zh": "5æœˆ17æ—¥"
            },
            "hash": "95b404534e69c826",
            "authors": [
                "Ahmed Lekssays",
                "Utsav Shukla",
                "Husrev Taha Sencar",
                "Md Rizwan Parvez"
            ],
            "affiliations": [
                "Independent Researcher",
                "Qatar Computing Research Institute, Doha, Qatar"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11988.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#data",
                    "#hallucinations",
                    "#rag",
                    "#benchmark"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "TechniqueRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¸ĞºĞ¾Ğ² Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². TechniqueRAG Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Cyber Defense with TechniqueRAG: Precision without Excessive Resources",
                    "desc": "This paper introduces TechniqueRAG, a novel framework designed to improve the identification of adversarial techniques in cybersecurity texts. It combines retrieval-augmented generation (RAG) with instruction-tuned large language models (LLMs) to enhance domain specificity while minimizing the need for extensive labeled datasets. By fine-tuning only the generation component and employing zero-shot LLM re-ranking, TechniqueRAG improves the quality of retrieved candidates, ensuring they are more relevant to the specific domain of adversarial techniques. The results show that TechniqueRAG outperforms existing methods, achieving high accuracy without the heavy resource demands typically associated with task-specific optimizations."
                },
                "zh": {
                    "title": "æå‡å®‰å…¨æ–‡æœ¬å¯¹æŠ—æŠ€æœ¯è¯†åˆ«çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTechniqueRAGçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¯¹å®‰å…¨æ–‡æœ¬ä¸­å¯¹æŠ—æ€§æŠ€æœ¯çš„è¯†åˆ«èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç°æˆçš„æ£€ç´¢å™¨ã€ç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæœ€å°çš„æ–‡æœ¬-æŠ€æœ¯å¯¹ï¼Œè§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚é€šè¿‡ä»…å¯¹ç”Ÿæˆç»„ä»¶è¿›è¡Œå¾®è°ƒï¼ŒTechniqueRAGé¿å…äº†å¯¹èµ„æºå¯†é›†å‹æ£€ç´¢è®­ç»ƒçš„ä¾èµ–ï¼ŒåŒæ—¶é€šè¿‡é›¶-shot LLMé‡æ’åºæé«˜äº†æ£€ç´¢è´¨é‡å’Œé¢†åŸŸç‰¹å¼‚æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTechniqueRAGåœ¨å¤šä¸ªå®‰å…¨åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ— éœ€å¤§é‡ç‰¹å®šä»»åŠ¡çš„ä¼˜åŒ–æˆ–æ ‡è®°æ•°æ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10831",
            "title": "Creating General User Models from Computer Use",
            "url": "https://huggingface.co/papers/2505.10831",
            "abstract": "Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture that user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.",
            "score": 1,
            "issue_id": 3858,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "b70f4a83c98a9326",
            "authors": [
                "Omar Shaikh",
                "Shardul Sapkota",
                "Shan Rizvi",
                "Eric Horvitz",
                "Joon Sung Park",
                "Diyi Yang",
                "Michael S. Bernstein"
            ],
            "affiliations": [
                "Independent",
                "Microsoft Research",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10831.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#agents",
                    "#multimodal",
                    "#agi",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "GUM: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ (GUM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ñ Ğ·Ğ° ĞµĞ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼. GUM Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¾ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞµ Ğº ÑĞ²Ğ°Ğ´ÑŒĞ±Ğµ Ğ¸Ğ»Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ½Ğ°Ğ´ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. GUM Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ¼Ğ½Ñ‹Ğµ Ñ‡Ğ°Ñ‚-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ñ‹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ĞĞ¡ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğº Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Empowering Technology to Anticipate User Needs",
                    "desc": "This paper introduces a General User Model (GUM) that learns about users by observing their interactions with computers, using unstructured data like device screenshots. GUMs create confidence-weighted propositions to capture user preferences and behaviors, allowing for flexible reasoning about user needs. The architecture can infer context from multimodal observations, enabling proactive assistants that can suggest actions without explicit user requests. Overall, GUMs enhance human-computer interaction by providing a more comprehensive understanding of user behavior and preferences across different applications."
                },
                "zh": {
                    "title": "é€šç”¨ç”¨æˆ·æ¨¡å‹ï¼šæ™ºèƒ½äººæœºäº¤äº’çš„æ–°æœªæ¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨ç”¨æˆ·æ¨¡å‹ï¼ˆGUMï¼‰ï¼Œæ—¨åœ¨é€šè¿‡è§‚å¯Ÿç”¨æˆ·ä¸è®¡ç®—æœºçš„äº’åŠ¨æ¥å­¦ä¹ ç”¨æˆ·çš„åå¥½å’Œä¹ æƒ¯ã€‚GUMèƒ½å¤Ÿå¤„ç†éç»“æ„åŒ–çš„ç”¨æˆ·è§‚å¯Ÿæ•°æ®ï¼ˆå¦‚è®¾å¤‡æˆªå›¾ï¼‰ï¼Œå¹¶æ„å»ºå‡ºåæ˜ ç”¨æˆ·çŸ¥è¯†å’Œåå¥½çš„ç½®ä¿¡åŠ æƒå‘½é¢˜ã€‚é€šè¿‡å¤šæ¨¡æ€è§‚å¯Ÿï¼ŒGUMå¯ä»¥æ¨æ–­ç”¨æˆ·çš„éœ€æ±‚ï¼Œå¹¶æ ¹æ®ä¸Šä¸‹æ–‡æ£€ç´¢ç›¸å…³å‘½é¢˜ï¼ŒæŒç»­ä¿®æ­£å·²æœ‰å‘½é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºGUMçš„åŠ©æ‰‹èƒ½å¤Ÿä¸»åŠ¨è¯†åˆ«ç”¨æˆ·éœ€æ±‚å¹¶æ‰§è¡Œæœ‰ç”¨çš„å»ºè®®ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„äººæœºäº¤äº’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03332",
            "title": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning",
            "url": "https://huggingface.co/papers/2505.03332",
            "abstract": "Critical peer review of scientific manuscripts presents a significant challenge for Large Language Models (LLMs), partly due to data limitations and the complexity of expert reasoning. This report introduces Persistent Workflow Prompting (PWP), a potentially broadly applicable prompt engineering methodology designed to bridge this gap using standard LLM chat interfaces (zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical analysis of experimental chemistry manuscripts, featuring a hierarchical, modular architecture (structured via Markdown) that defines detailed analysis workflows. We develop this PWP prompt through iterative application of meta-prompting techniques and meta-reasoning aimed at systematically codifying expert review workflows, including tacit knowledge. Submitted once at the start of a session, this PWP prompt equips the LLM with persistent workflows triggered by subsequent queries, guiding modern reasoning LLMs through systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM identifying major methodological flaws in a test case while mitigating LLM input bias and performing complex tasks, including distinguishing claims from evidence, integrating text/photo/figure analysis to infer parameters, executing quantitative feasibility checks, comparing estimates against claims, and assessing a priori plausibility. To ensure transparency and facilitate replication, we provide full prompts, detailed demonstration analyses, and logs of interactive chats as supplementary resources. Beyond the specific application, this work offers insights into the meta-development process itself, highlighting the potential of PWP, informed by detailed workflow formalization, to enable sophisticated analysis using readily available LLMs for complex scientific tasks.",
            "score": 1,
            "issue_id": 3849,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 Ğ¼Ğ°Ñ",
                "en": "May 6",
                "zh": "5æœˆ6æ—¥"
            },
            "hash": "9c52936c2b9a7443",
            "authors": [
                "Evgeny Markhasin"
            ],
            "affiliations": [
                "Lobachevsky State University of Nizhny Novgorod"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03332.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#data",
                    "#science",
                    "#multimodal",
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "PWP: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Persistent Workflow Prompting (PWP) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞµĞ¹. PWP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµÑÑĞ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ°-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¼ĞµÑ‚Ğ°-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PWP-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ LLM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."
                },
                "en": {
                    "title": "Empowering LLMs for Expert Scientific Review with Persistent Workflow Prompting",
                    "desc": "This paper addresses the challenges faced by Large Language Models (LLMs) in performing critical peer reviews of scientific manuscripts, particularly due to data limitations and the intricacies of expert reasoning. It introduces a new methodology called Persistent Workflow Prompting (PWP), which allows users to create structured prompts that guide LLMs through detailed analysis workflows without needing coding skills. The PWP framework is designed to help LLMs systematically evaluate scientific content by integrating various forms of data, such as text and images, to identify flaws and assess claims. The authors demonstrate the effectiveness of PWP in analyzing experimental chemistry manuscripts, showcasing its ability to enhance LLM performance in complex scientific evaluations."
                },
                "zh": {
                    "title": "æŒä¹…å·¥ä½œæµæç¤ºï¼šæå‡ç§‘å­¦è¯„å®¡çš„æ™ºèƒ½åŒ–",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æç¤ºå·¥ç¨‹æ–¹æ³•ï¼Œç§°ä¸ºæŒä¹…å·¥ä½œæµæç¤ºï¼ˆPWPï¼‰ï¼Œæ—¨åœ¨å¸®åŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œç§‘å­¦æ‰‹ç¨¿çš„æ‰¹åˆ¤æ€§åŒè¡Œè¯„å®¡ã€‚PWPé€šè¿‡æ ‡å‡†çš„LLMèŠå¤©ç•Œé¢ï¼Œä½¿ç”¨åˆ†å±‚æ¨¡å—åŒ–çš„æ¶æ„ï¼Œå®šä¹‰è¯¦ç»†çš„åˆ†æå·¥ä½œæµç¨‹ï¼Œèƒ½å¤Ÿç³»ç»ŸåŒ–åœ°ç¼–ç ä¸“å®¶è¯„å®¡çš„å·¥ä½œæµç¨‹ã€‚é€šè¿‡è¿­ä»£çš„å…ƒæç¤ºæŠ€æœ¯å’Œå…ƒæ¨ç†ï¼ŒPWPèƒ½å¤Ÿå¼•å¯¼LLMè¿›è¡Œå¤šæ¨¡æ€è¯„ä¼°ï¼Œè¯†åˆ«å®éªŒåŒ–å­¦æ‰‹ç¨¿ä¸­çš„ä¸»è¦æ–¹æ³•è®ºç¼ºé™·ã€‚è¯¥æ–¹æ³•ä¸ä»…æä¾›äº†å…·ä½“åº”ç”¨çš„ç¤ºä¾‹ï¼Œè¿˜å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç°æœ‰çš„LLMè¿›è¡Œå¤æ‚ç§‘å­¦ä»»åŠ¡çš„æ·±å…¥åˆ†æã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12973",
            "title": "Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models",
            "url": "https://huggingface.co/papers/2505.12973",
            "abstract": "Homograph disambiguation remains a significant challenge in grapheme-to-phoneme (G2P) conversion, especially for low-resource languages. This challenge is twofold: (1) creating balanced and comprehensive homograph datasets is labor-intensive and costly, and (2) specific disambiguation strategies introduce additional latency, making them unsuitable for real-time applications such as screen readers and other accessibility tools. In this paper, we address both issues. First, we propose a semi-automated pipeline for constructing homograph-focused datasets, introduce the HomoRich dataset generated through this pipeline, and demonstrate its effectiveness by applying it to enhance a state-of-the-art deep learning-based G2P system for Persian. Second, we advocate for a paradigm shift - utilizing rich offline datasets to inform the development of fast, rule-based methods suitable for latency-sensitive accessibility applications like screen readers. To this end, we improve one of the most well-known rule-based G2P systems, eSpeak, into a fast homograph-aware version, HomoFast eSpeak. Our results show an approximate 30% improvement in homograph disambiguation accuracy for the deep learning-based and eSpeak systems.",
            "score": 0,
            "issue_id": 3854,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "e144d477e8eb1cab",
            "authors": [
                "Mahta Fetrat Qharabagh",
                "Zahra Dehghanian",
                "Hamid R. Rabiee"
            ],
            "affiliations": [
                "Dep. of Computer Engineering, Sharif University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12973.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#healthcare",
                    "#low_resource",
                    "#dataset"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ´Ğ»Ñ G2P ĞºĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ² Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„ĞµĞ¼ Ğ² Ñ„Ğ¾Ğ½ĞµĞ¼Ñ‹ (G2P) Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»ÑƒĞ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ Ğ¾Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ HomoRich. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ G2P Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¸Ğ´ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ eSpeak Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¾Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² - HomoFast eSpeak, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 30%."
                },
                "en": {
                    "title": "Enhancing G2P with Efficient Homograph Disambiguation",
                    "desc": "This paper tackles the problem of homograph disambiguation in grapheme-to-phoneme (G2P) conversion, particularly for low-resource languages. It introduces a semi-automated method to create homograph datasets, exemplified by the new HomoRich dataset, which enhances a deep learning G2P system for Persian. Additionally, the authors propose a shift towards using comprehensive offline datasets to develop efficient, rule-based G2P methods that are suitable for real-time applications. The improved eSpeak system, named HomoFast eSpeak, demonstrates a significant increase in disambiguation accuracy, making it more effective for accessibility tools."
                },
                "zh": {
                    "title": "æå‡åŒå½¢å¼‚ä¹‰è¯æ¶ˆæ­§çš„æ™ºèƒ½è§£å†³æ–¹æ¡ˆ",
                    "desc": "åŒå½¢å¼‚ä¹‰è¯æ¶ˆæ­§åœ¨å›¾å½¢åˆ°éŸ³ç´ ï¼ˆG2Pï¼‰è½¬æ¢ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºåŒ®ä¹çš„è¯­è¨€ä¸­ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠè‡ªåŠ¨åŒ–çš„æµç¨‹æ¥æ„å»ºåŒå½¢å¼‚ä¹‰è¯æ•°æ®é›†ï¼Œå¹¶ç”Ÿæˆäº†HomoRichæ•°æ®é›†ï¼Œä»¥å¢å¼ºæ³¢æ–¯è¯­çš„æ·±åº¦å­¦ä¹ G2Pç³»ç»Ÿã€‚æˆ‘ä»¬è¿˜å€¡å¯¼åˆ©ç”¨ä¸°å¯Œçš„ç¦»çº¿æ•°æ®é›†æ¥å¼€å‘é€‚åˆå®æ—¶åº”ç”¨çš„å¿«é€Ÿè§„åˆ™åŸºç¡€æ–¹æ³•ã€‚é€šè¿‡æ”¹è¿›è‘—åçš„è§„åˆ™åŸºç¡€G2Pç³»ç»ŸeSpeakï¼Œæˆ‘ä»¬çš„HomoFast eSpeakç‰ˆæœ¬åœ¨åŒå½¢å¼‚ä¹‰è¯æ¶ˆæ­§å‡†ç¡®æ€§ä¸Šæé«˜äº†çº¦30%ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-20.html",
    "link_next": "2025-05-22.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "20.05",
        "en": "05/20",
        "zh": "5æœˆ20æ—¥"
    },
    "short_date_next": {
        "ru": "22.05",
        "en": "05/22",
        "zh": "5æœˆ22æ—¥"
    },
    "categories": {
        "#dataset": 11,
        "#data": 10,
        "#benchmark": 18,
        "#agents": 4,
        "#cv": 6,
        "#rl": 7,
        "#rlhf": 4,
        "#rag": 2,
        "#plp": 0,
        "#inference": 8,
        "#3d": 3,
        "#audio": 1,
        "#video": 5,
        "#multimodal": 9,
        "#math": 3,
        "#multilingual": 3,
        "#architecture": 11,
        "#healthcare": 3,
        "#training": 19,
        "#robotics": 0,
        "#agi": 3,
        "#games": 2,
        "#interpretability": 6,
        "#reasoning": 18,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 21,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 15,
        "#small_models": 1,
        "#science": 3,
        "#low_resource": 2
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºChain-of-Model (CoM)ã€‚å®ƒå°†å› æœå…³ç³»å¼•å…¥æ¯å±‚çš„éšè—çŠ¶æ€ï¼Œå½¢æˆé“¾å¼ç»“æ„ï¼Œæé«˜äº†æ¨¡å‹è®­ç»ƒçš„æ‰©å±•æ•ˆç‡å’Œéƒ¨ç½²çš„çµæ´»æ€§ã€‚ä½œè€…å¼•å…¥äº†Chain-of-Representation (CoR)çš„æ¦‚å¿µï¼Œå°†æ¯å±‚çš„éšè—çŠ¶æ€è¡¨ç¤ºä¸ºå¤šä¸ªå­è¡¨ç¤ºï¼ˆå³é“¾ï¼‰çš„ç»„åˆã€‚æ¯å±‚ä¸­ï¼Œæ¯ä¸ªé“¾åªèƒ½æŸ¥çœ‹è¾“å…¥è¡¨ç¤ºä¸­çš„æ‰€æœ‰å‰åºé“¾ã€‚å› æ­¤ï¼ŒåŸºäºCoMæ¡†æ¶çš„æ¨¡å‹å¯ä»¥é€šè¿‡å¢åŠ é“¾æ¥é€æ­¥æ‰©å±•æ¨¡å‹å¤§å°ï¼Œå¹¶é€šè¿‡ä½¿ç”¨ä¸åŒçš„é“¾æ•°é‡æä¾›å¤šä¸ªä¸åŒå¤§å°çš„å­æ¨¡å‹è¿›è¡Œå¼¹æ€§æ¨ç†ã€‚åŸºäºè¿™ä¸€åŸåˆ™ï¼Œä½œè€…è®¾è®¡äº†Chain-of-Language-Model (CoLM)ï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†KVå…±äº«æœºåˆ¶çš„CoLM-Airï¼Œä»¥å®ç°æ›´å¤šçš„æ‰©å±•åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLMç³»åˆ—æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸æ ‡å‡†Transformerç›¸å½“ï¼ŒåŒæ—¶æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚",
        "title": "Chain-of-Model Learning for Language Model",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÃ­chÅ« le yÄ«zhÇ’ng xÄ«n de xuÃ©xÃ­ fÃ nshÃ¬, chÄ“ngwÃ©i Chain-of-Model (CoM). TÄ jiÄng yÄ«nguÇ’ guÄnxÃ¬ yÇnrÃ¹ mÄ›i cÃ©ng de yÇncÃ¡ng zhuÃ ngtÃ i, xÃ­ngchÃ©ng liÃ nshÃ¬ jiÃ©gÃ²u, tÄ«gÄo le mÃ³xÃ­ng xÃ¹nliÃ n de kuÃ²zhÇn xiÃ olÇœ hÃ© bÃ¹shÇ” de lÃ­nghuÃ³xÃ¬ng. ZuÃ²zhÄ› yÇnrÃ¹ le Chain-of-Representation (CoR) de gÃ iniÃ n, jiÄng mÄ›i cÃ©ng de yÇncÃ¡ng zhuÃ ngtÃ i biÇoshÃ¬ wÃ©i duÅgÃ¨ zÇ biÇoshÃ¬ (jiÄ“ liÃ n) de zÇ”hÃ©. MÄ›i cÃ©ng zhÅng, mÄ›i gÃ¨ liÃ n zhÇnÃ©ng chÃ¡ kÃ n shÅ«rÃ¹ biÇoshÃ¬ zhÅng de suÇ’yÇ’u qiÃ¡nxÃ¹ liÃ n. YÄ«ncÇ, jÄ«yÃº CoM kuÃ ngjiÃ  de mÃ³xÃ­ng kÄ›yÇ tÅngguÃ² zÄ“ngjiÄ liÃ n lÃ¡i zhÃºbÃ¹ kuÃ²zhÇn mÃ³xÃ­ng dÃ xÃ¬ng, bÃ¬ng tÅngguÃ² shÇyÃ²ng bÃ¹tÃ³ng de liÃ n shÃ¹liÃ ng tÃ­gÅng duÅgÃ¨ bÃ¹tÃ³ng dÃ xÃ¬ng de zÇ mÃ³xÃ­ng jÃ¬nxÃ­ng tÃ¡nxÃ¬ng tuÄ«lÇ. JÄ«yÃº zhÃ¨ yÄ« yuÃ¡nzÃ©, zuÃ²zhÄ› shÃ¨jÃ¬ le Chain-of-Language-Model (CoLM), bÃ¬ng jÃ¬n yÄ«bÃ¹ yÇnrÃ¹ le KV gÃ²ngxiÇng jÄ«zhÃ¬ de CoLM-Air, yÇ shÃ­xiÃ n gÃ¨ng duÅ de kuÃ²zhÇn gÅngnÃ©ng. ShÃ­yÃ n jiÃ©guÇ’ biÇomÃ­ng, CoLM xÃ¬liÃ¨ mÃ³xÃ­ng zÃ i xÃ¬ngnÃ©ng shÃ ng yÇ” biÄozhÇ”n Transformer xiÄngdÄng, tÃ³ngshÃ­ tÃ­gÅng le gÃ¨ng dÃ  de lÃ­nghuÃ³xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"èŒƒå¼\", \"pinyin\": \"fÃ n shÃ¬\", \"trans\": \"paradigm\"},\n    {\"word\": \"Chain-of-Model\", \"pinyin\": \"ChÃ¨in-Ã²f-MÃ³del\", \"trans\": \"Chain-of-Model\"},\n    {\"word\": \"å› æœå…³ç³»\", \"pinyin\": \"yÄ«n guÇ’ guÄn xÃ¬\", \"trans\": \"causal relationship\"},\n    {\"word\": \"éšè—çŠ¶æ€\", \"pinyin\": \"yÇn cÃ¡ng zhuÃ ng tÃ i\", \"trans\": \"hidden state\"},\n    {\"word\": \"é“¾å¼ç»“æ„\", \"pinyin\": \"liÃ n shÃ¬ jiÃ©gÃ²u\", \"trans\": \"chain structure\"},\n    {\"word\": \"æ‰©å±•æ•ˆç‡\", \"pinyin\": \"kuÃ² zhÇn xiÃ o lÇœ\", \"trans\": \"scalability\"},\n    {\"word\": \"éƒ¨ç½²\", \"pinyin\": \"bÃ¹ shÇ”\", \"trans\": \"deployment\"},\n    {\"word\": \"çµæ´»æ€§\", \"pinyin\": \"lÃ­ng huÃ³ xÃ¬ng\", \"trans\": \"flexibility\"},\n    {\"word\": \"Chain-of-Representation\", \"pinyin\": \"ChÃ¨in-Ã²f-RÄ›prizen tÃ©i shÄ“n\", \"trans\": \"Chain-of-Representation\"},\n    {\"word\": \"å­è¡¨ç¤º\", \"pinyin\": \"zÇ biÇo shÃ¬\", \"trans\": \"sub-representation\"},\n    {\"word\": \"ç»„åˆ\", \"pinyin\": \"zÇ” hÃ©\", \"trans\": \"combination\"},\n    {\"word\": \"å‰åºé“¾\", \"pinyin\": \"qiÃ¡n xÃ¹ liÃ n\", \"trans\": \"preceding chain\"},\n    {\"word\": \"å¼¹æ€§æ¨ç†\", \"pinyin\": \"tÃ¡n xÃ¬ng tuÄ« lÇ\", \"trans\": \"elastic inference\"},\n    {\"word\": \"Chain-of-Language-Model\", \"pinyin\": \"ChÃ¨in-Ã²f-LÃ¡nggÃ¹ MÃ³del\", \"trans\": \"Chain-of-Language-Model\"},\n    {\"word\": \"KVå…±äº«æœºåˆ¶\", \"pinyin\": \"KV gÃ²ng xiÇng jÄ« zhÃ¬\", \"trans\": \"KV sharing mechanism\"},\n    {\"word\": \"CoLM-Air\", \"pinyin\": \"CoLM-Ã‰ir\", \"trans\": \"CoLM-Air\"},\n    {\"word\": \"æ‰©å±•åŠŸèƒ½\", \"pinyin\": \"kuÃ² zhÇn gÅng nÃ©ng\", \"trans\": \"extended functionality\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"TÃ¨inshÃ¨in fÅmÄ›i\", \"trans\": \"Transformer\"}\n]",
        "trans": "This article proposes a new learning paradigm called Chain-of-Model (CoM). It introduces causality into the hidden states of each layer, forming a chain-like structure that enhances the scalability of model training and the flexibility of deployment. The authors introduce the concept of Chain-of-Representation (CoR), representing the hidden states of each layer as a combination of multiple sub-representations (i.e., chains). Within each layer, each chain can only view all preceding chains in the input representation. Therefore, models based on the CoM framework can incrementally scale the model size by adding chains and provide multiple sub-models of different sizes for elastic inference by using different numbers of chains. Based on this principle, the authors designed Chain-of-Language-Model (CoLM) and further introduced CoLM-Air with a KV sharing mechanism to achieve more scalable functionalities. Experimental results show that the CoLM series models perform comparably to standard Transformers while offering greater flexibility.",
        "update_ts": "2025-05-20 09:13"
    }
}