{
    "date": {
        "ru": "21 мая",
        "en": "May 21",
        "zh": "5月21日"
    },
    "time_utc": "2025-05-21 18:15",
    "weekday": 2,
    "issue_id": 3884,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.14683",
            "title": "Emerging Properties in Unified Multimodal Pretraining",
            "url": "https://huggingface.co/papers/2505.14683",
            "abstract": "Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open0source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder0only model pretrained on trillions of tokens curated from large0scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/",
            "score": 75,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "57522649bb8f8010",
            "authors": [
                "Chaorui Deng",
                "Deyao Zhu",
                "Kunchang Li",
                "Chenhui Gou",
                "Feng Li",
                "Zeyu Wang",
                "Shu Zhong",
                "Weihao Yu",
                "Xiaonan Nie",
                "Ziang Song",
                "Guang Shi",
                "Haoqi Fan"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Hong Kong University of Science and Technology",
                "Monash University",
                "Shenzhen Institutes of Advanced Technology",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14683.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#reasoning",
                    "#open_source",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🥯",
                "ru": {
                    "title": "BAGEL: Объединение мультимодального понимания и генерации в открытой модели",
                    "desc": "BAGEL - это открытая фундаментальная модель для мультимодального понимания и генерации. Она обучена на триллионах токенов из текстовых, изображений, видео и веб-данных. BAGEL превосходит другие открытые унифицированные модели в задачах мультимодальной генерации и понимания. Модель демонстрирует продвинутые способности в мультимодальном рассуждении, включая манипуляции с изображениями, предсказание будущих кадров и 3D-манипуляции."
                },
                "en": {
                    "title": "BAGEL: Unifying Multimodal AI for Enhanced Understanding and Generation",
                    "desc": "This paper presents BAGEL, an open-source foundational model designed for multimodal understanding and generation. BAGEL is a decoder-only model that has been pretrained on a vast dataset comprising text, images, videos, and web content. By leveraging this diverse multimodal data, BAGEL demonstrates advanced capabilities in complex reasoning tasks, outperforming existing open-source models. The authors aim to promote further research in multimodal AI by sharing their findings, pretraining methods, and code with the community."
                },
                "zh": {
                    "title": "BAGEL：开源多模态理解与生成的统一模型",
                    "desc": "本文介绍了一个名为BAGEL的开源基础模型，它支持多模态理解和生成。BAGEL是一个统一的解码器模型，经过在大量文本、图像、视频和网络数据上进行预训练。通过使用多样化的多模态数据，BAGEL在复杂的多模态推理方面展现出新的能力，显著超越了现有的开源统一模型。我们希望通过分享关键发现、预训练细节和数据创建协议，促进多模态研究的进一步发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11594",
            "title": "SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training",
            "url": "https://huggingface.co/papers/2505.11594",
            "abstract": "The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code will be available at https://github.com/thu-ml/SageAttention.",
            "score": 43,
            "issue_id": 3869,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "33309444d442b40c",
            "authors": [
                "Jintao Zhang",
                "Jia Wei",
                "Pengle Zhang",
                "Xiaoming Xu",
                "Haofeng Huang",
                "Haoxu Wang",
                "Kai Jiang",
                "Jun Zhu",
                "Jianfei Chen"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11594.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Революция в эффективности механизма внимания: от FP4 до 8-бит",
                    "desc": "Статья представляет два ключевых улучшения эффективности механизма внимания в нейронных сетях. Во-первых, авторы используют новые тензорные ядра FP4 в GPU Blackwell для ускорения вычислений, достигая 5-кратного прироста производительности по сравнению с FlashAttention. Во-вторых, они разрабатывают 8-битное внимание для задач обучения, которое показывает хорошие результаты при дообучении моделей. Эксперименты демонстрируют, что предложенные методы могут эффективно применяться для ускорения различных моделей."
                },
                "en": {
                    "title": "Revolutionizing Attention: Fast and Efficient for Training and Inference",
                    "desc": "This paper addresses the inefficiency of attention mechanisms in machine learning, which typically have a quadratic time complexity. The authors introduce enhancements using FP4 Tensor Cores in Blackwell GPUs, achieving a significant speedup in attention computation, reaching 1038 TOPS on the RTX5090. Additionally, they explore low-bit attention for training tasks, proposing an 8-bit attention method that maintains performance during fine-tuning while showing slower convergence during pretraining. This work not only improves inference speed but also expands the application of low-bit attention to training, making it a versatile solution for large model training."
                },
                "zh": {
                    "title": "提升注意力机制效率的创新方法",
                    "desc": "本文探讨了注意力机制的效率问题，主要由于其二次时间复杂度。我们通过利用Blackwell GPU中的新FP4 Tensor Cores来加速注意力计算，实现了在RTX5090上达到1038 TOPS的性能，相比于最快的FlashAttention提升了5倍。我们的FP4注意力可以以即插即用的方式加速各种模型的推理。此外，我们还首次将低位注意力应用于训练任务，设计了高效的8位注意力，实验表明在微调任务中表现无损，但在预训练任务中收敛速度较慢。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13438",
            "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
            "url": "https://huggingface.co/papers/2505.13438",
            "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.",
            "score": 25,
            "issue_id": 3874,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "34ff8235b7a27562",
            "authors": [
                "Penghui Qi",
                "Zichen Liu",
                "Tianyu Pang",
                "Chao Du",
                "Wee Sun Lee",
                "Min Lin"
            ],
            "affiliations": [
                "National University of Singapore",
                "Sea AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13438.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rl",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективные рассуждения ЛЯМ в любой момент времени",
                    "desc": "Статья представляет новый подход AnytimeReasoner для оптимизации рассуждений языковых моделей (ЛЯМ) в любой момент времени. Метод использует усеченные процессы мышления под различные бюджеты токенов, заставляя модель суммировать оптимальный ответ для каждого усеченного рассуждения. Авторы вводят технику снижения дисперсии Budget Relative Policy Optimization (BRPO) для улучшения обучения с подкреплением. Эмпирические результаты показывают превосходство метода над GRPO в задачах математических рассуждений при различных распределениях бюджетов."
                },
                "en": {
                    "title": "Optimizing Reasoning Efficiency with AnytimeReasoner",
                    "desc": "This paper introduces AnytimeReasoner, a framework designed to improve the reasoning capabilities of large language models (LLMs) by optimizing their performance under varying token budgets. Unlike traditional reinforcement learning methods that focus solely on final outcomes, AnytimeReasoner allows for flexible reasoning by truncating the thinking process and summarizing answers based on sampled token budgets. This approach incorporates verifiable dense rewards, which aids in better credit assignment during the reinforcement learning optimization. The authors also propose a new technique called Budget Relative Policy Optimization (BRPO) to enhance the robustness and efficiency of the learning process, leading to superior performance in mathematical reasoning tasks compared to existing methods."
                },
                "zh": {
                    "title": "优化推理性能，提升效率与灵活性",
                    "desc": "本文提出了一种新的框架，名为AnytimeReasoner，旨在优化大型语言模型（LLMs）的推理性能。与传统方法不同，AnytimeReasoner通过在不同的令牌预算约束下进行推理，提升了令牌的使用效率和灵活性。该方法引入了可验证的密集奖励，使得在强化学习优化中能够更有效地进行信用分配。实验结果表明，在数学推理任务中，AnytimeReasoner在各种预算条件下均优于现有方法，提升了训练和令牌效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14460",
            "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank",
            "url": "https://huggingface.co/papers/2505.14460",
            "abstract": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computational modeling has not been thoroughly explored in the context of image quality assessment (IQA), a task critically dependent on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are then used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation.",
            "score": 24,
            "issue_id": 3876,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "98e34275d69f6e41",
            "authors": [
                "Tianhe Wu",
                "Jian Zou",
                "Jie Liang",
                "Lei Zhang",
                "Kede Ma"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "OPPO Research Institute",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14460.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#cv",
                    "#rl",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "VisualQuality-R1: Революция в оценке качества изображений с помощью рассуждений и обучения с подкреплением",
                    "desc": "В статье представлена модель VisualQuality-R1 для оценки качества изображений без эталона, обученная с помощью обучения с подкреплением для ранжирования. Модель генерирует несколько оценок качества для каждого изображения в паре, используя групповую относительную оптимизацию политики. VisualQuality-R1 превосходит дискриминативные модели глубокого обучения и может генерировать содержательные описания качества, понятные человеку. Модель поддерживает обучение на нескольких наборах данных без необходимости перекалибровки перцептивной шкалы."
                },
                "en": {
                    "title": "Revolutionizing Image Quality Assessment with Reasoning and Reinforcement Learning",
                    "desc": "This paper presents VisualQuality-R1, a novel no-reference image quality assessment (NR-IQA) model that leverages reasoning and reinforcement learning to evaluate image quality. By using group relative policy optimization, the model generates multiple quality scores for image pairs, allowing for a nuanced comparison of visual quality. The rewards for these scores are based on continuous fidelity measures, enhancing the model's ability to provide accurate quality assessments. Experimental results show that VisualQuality-R1 outperforms existing deep learning-based NR-IQA models and can generate detailed quality descriptions, making it effective for various image processing applications."
                },
                "zh": {
                    "title": "推理驱动的图像质量评估新模型",
                    "desc": "本文介绍了一种新的无参考图像质量评估模型VisualQuality-R1，该模型通过强化学习来提高大语言模型的推理和泛化能力。VisualQuality-R1利用组相对策略优化，为每对图像生成多个质量评分，并计算图像之间的比较概率。与传统的二元标签不同，奖励是基于连续的保真度度量来定义的。实验结果表明，VisualQuality-R1在图像质量评估任务中表现优于现有的深度学习模型，并能够生成丰富的质量描述，适用于多数据集训练。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14246",
            "title": "Visual Agentic Reinforcement Fine-Tuning",
            "url": "https://huggingface.co/papers/2505.14246",
            "abstract": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native agentic ability to use external tools such as web browsers for searching and writing/executing code for image manipulation to think with images. In the open-source research community, while significant progress has been made in language-only agentic abilities such as function calling and tool integration, the development of multi-modal agentic capabilities that involve truly thinking with images, and their corresponding benchmarks, are still less explored. This work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the ability to browse websites for real-time information updates and write code to manipulate and analyze input images through cropping, rotation, and other image processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT) with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs' agentic search and coding abilities. Our experimental results demonstrate that Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and +10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities. Our findings suggest that Visual-ARFT offers a promising path toward building robust and generalizable multimodal agents.",
            "score": 24,
            "issue_id": 3873,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "163cdaefde9d9174",
            "authors": [
                "Ziyu Liu",
                "Yuhang Zang",
                "Yushan Zou",
                "Zijian Liang",
                "Xiaoyi Dong",
                "Yuhang Cao",
                "Haodong Duan",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiaotong University",
                "The Chinese University of Hong Kong",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14246.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rlhf",
                    "#benchmark",
                    "#multimodal",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Усиление мультимодальных ИИ-агентов через обучение с подкреплением",
                    "desc": "Статья представляет метод Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) для улучшения способностей мультимодальных языковых моделей (LVLM) к рассуждению и использованию инструментов. Visual-ARFT позволяет моделям просматривать веб-сайты для получения актуальной информации и писать код для манипуляции изображениями. Авторы также представляют набор тестов Multi-modal Agentic Tool Bench (MAT) для оценки агентных способностей LVLM. Результаты экспериментов показывают, что Visual-ARFT значительно улучшает производительность моделей на MAT и других задачах многоэтапного вопросно-ответного анализа."
                },
                "en": {
                    "title": "Empowering Vision-Language Models with Visual-ARFT for Enhanced Reasoning",
                    "desc": "This paper introduces Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT), a method that enhances Large Vision-Language Models (LVLMs) by enabling them to think with images and use external tools effectively. The research highlights the development of multi-modal agentic capabilities, allowing LVLMs to browse the web for information and manipulate images through coding. The authors present a new evaluation framework called the Multi-modal Agentic Tool Bench (MAT), which assesses the models' abilities in searching and coding tasks. Experimental results show that Visual-ARFT significantly improves performance on various benchmarks, indicating its potential for creating more capable multimodal agents."
                },
                "zh": {
                    "title": "视觉代理强化微调：多模态智能的未来",
                    "desc": "本文探讨了大型视觉语言模型（LVLMs）在多模态推理能力方面的进展，特别是通过视觉代理强化微调（Visual-ARFT）技术。该技术使得LVLMs能够灵活地使用外部工具，如浏览器和代码执行，进行实时信息更新和图像处理。研究还提出了一个多模态代理工具基准（MAT），用于评估LVLMs的搜索和编码能力。实验结果表明，Visual-ARFT在多个基准测试中显著优于传统模型，展示了其强大的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13138",
            "title": "Neurosymbolic Diffusion Models",
            "url": "https://huggingface.co/papers/2505.13138",
            "abstract": "Neurosymbolic (NeSy) predictors combine neural perception with symbolic reasoning to solve tasks like visual reasoning. However, standard NeSy predictors assume conditional independence between the symbols they extract, thus limiting their ability to model interactions and uncertainty - often leading to overconfident predictions and poor out-of-distribution generalisation. To overcome the limitations of the independence assumption, we introduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy predictors that use discrete diffusion to model dependencies between symbols. Our approach reuses the independence assumption from NeSy predictors at each step of the diffusion process, enabling scalable learning while capturing symbol dependencies and uncertainty quantification. Across both synthetic and real-world benchmarks - including high-dimensional visual path planning and rule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among NeSy predictors and demonstrate strong calibration.",
            "score": 22,
            "issue_id": 3876,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "56639003a63a4eb4",
            "authors": [
                "Emile van Krieken",
                "Pasquale Minervini",
                "Edoardo Ponti",
                "Antonio Vergari"
            ],
            "affiliations": [
                "Miniml.AI",
                "School of Informatics, University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13138.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#cv",
                    "#agents",
                    "#benchmark",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Нейросимволические диффузионные модели: преодоление ограничений условной независимости",
                    "desc": "Нейросимволические диффузионные модели (NeSyDMs) представляют собой новый класс нейросимволических предикторов, использующих дискретную диффузию для моделирования зависимостей между символами. В отличие от стандартных нейросимволических предикторов, которые предполагают условную независимость извлекаемых символов, NeSyDMs способны лучше моделировать взаимодействия и неопределенность. Этот подход позволяет сохранить масштабируемость обучения при одновременном учете зависимостей между символами и количественной оценке неопределенности. На синтетических и реальных тестах, включая высокоразмерное визуальное планирование пути и автономное вождение на основе правил, NeSyDMs достигают наилучшей точности среди нейросимволических предикторов и демонстрируют хорошую калибровку."
                },
                "en": {
                    "title": "Enhancing Symbolic Reasoning with Dependency Modeling in Neurosymbolic Predictors",
                    "desc": "This paper presents neurosymbolic diffusion models (NeSyDMs), which enhance traditional neurosymbolic predictors by addressing the limitations of assuming conditional independence between symbols. By employing discrete diffusion processes, NeSyDMs effectively model the interactions and dependencies among symbols, leading to improved predictions. This method allows for scalable learning while also quantifying uncertainty, which is crucial for tasks like visual reasoning and autonomous driving. The results show that NeSyDMs outperform existing neurosymbolic approaches in accuracy and calibration on various benchmarks."
                },
                "zh": {
                    "title": "突破独立假设，提升符号依赖性建模",
                    "desc": "神经符号预测器（NeSy）结合了神经感知和符号推理，用于解决视觉推理等任务。传统的NeSy预测器假设提取的符号之间是条件独立的，这限制了它们建模交互和不确定性的能力，常常导致过于自信的预测和较差的分布外泛化能力。为了解决独立性假设的局限性，我们提出了神经符号扩散模型（NeSyDMs），这是一类新的NeSy预测器，利用离散扩散来建模符号之间的依赖关系。我们的研究在合成和真实世界基准测试中，包括高维视觉路径规划和基于规则的自动驾驶，展示了NeSyDMs在NeSy预测器中的最先进准确性和强大的校准能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04388",
            "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
            "url": "https://huggingface.co/papers/2505.04388",
            "abstract": "Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare.",
            "score": 19,
            "issue_id": 3874,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "12792ceffb601d5a",
            "authors": [
                "Dario Garcia-Gasulla",
                "Jordi Bayarri-Planas",
                "Ashwin Kumar Gururajan",
                "Enrique Lopez-Cuena",
                "Adrian Tormos",
                "Daniel Hinjos",
                "Pablo Bernabeu-Perez",
                "Anna Arias-Duart",
                "Pablo Agustin Martin-Torres",
                "Marta Gonzalez-Mallo",
                "Sergio Alvarez-Napagao",
                "Eduard Ayguadé-Parra",
                "Ulises Cortés"
            ],
            "affiliations": [
                "Barcelona Supercomputing Center (BSC-CNS), Spain",
                "Universitat Polit`ecnica de Catalunya - Barcelona Tech (UPC), Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04388.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#data",
                    "#healthcare",
                    "#training",
                    "#open_source",
                    "#ethics",
                    "#benchmark",
                    "#rlhf",
                    "#rag"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Открытые медицинские ИИ-модели нового поколения: эффективность и безопасность",
                    "desc": "Статья представляет семейство моделей Aloe Beta - открытые языковые модели для здравоохранения, конкурентоспособные с лучшими закрытыми аналогами. Исследователи оптимизировали ключевые этапы предобработки данных и обучения, улучшили безопасность моделей с помощью DPO и эффективность через RAG. Предложенная методология оценки, включающая четыре типа тестов, устанавливает новый стандарт в этой области. Модели Aloe Beta демонстрируют высокую производительность в медицинских задачах и устойчивость к атакам на безопасность."
                },
                "en": {
                    "title": "Empowering Healthcare with Open-Source LLMs: Safety Meets Efficacy",
                    "desc": "This paper discusses the development of open-source Large Language Models (LLMs) for healthcare, focusing on optimizing data preprocessing and training methods. It introduces Direct Preference Optimization (DPO) to enhance model safety and Retrieval-Augmented Generation (RAG) to improve efficacy. The evaluation methodology includes various tests to establish a new standard for assessing model performance in medical contexts. The resulting Aloe Beta models demonstrate competitive capabilities against private models while adhering to ethical guidelines and safety measures."
                },
                "zh": {
                    "title": "推动开放医疗模型，保障公众利益",
                    "desc": "本研究旨在推动开放源代码医疗大型语言模型（LLMs）的发展，以保护公众利益。通过优化数据预处理和训练的关键阶段，研究展示了如何通过直接偏好优化（DPO）提高模型的安全性，以及通过检索增强生成（RAG）提升模型的有效性。评估方法包括四种不同类型的测试，为该领域设定了新的标准。最终发布的模型在医疗基准测试中表现出色，且在安全性和伦理方面也有显著改善。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14513",
            "title": "Latent Flow Transformer",
            "url": "https://huggingface.co/papers/2505.14513",
            "abstract": "Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in preserving coupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms.",
            "score": 18,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "3683bab427c47086",
            "authors": [
                "Yen-Chen Wu",
                "Feng-Ting Liao",
                "Meng-Hsi Chen",
                "Pei-Chen Ho",
                "Farhang Nabiei",
                "Da-shan Shiu"
            ],
            "affiliations": [
                "MediaTek Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14513.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Непрерывные потоки вместо дискретных слоев: революция в архитектуре трансформеров",
                    "desc": "Статья представляет Latent Flow Transformer (LFT), новый подход к архитектуре языковых моделей. LFT заменяет несколько дискретных слоев одним непрерывным оператором переноса, обученным с помощью метода согласования потоков. Авторы также предлагают алгоритм Flow Walking для улучшения сохранения связей между токенами. Эксперименты на модели Pythia-410M показывают, что LFT позволяет значительно сжать модель, сохраняя или даже улучшая ее производительность."
                },
                "en": {
                    "title": "Efficient Layer Compression with Latent Flow Transformers",
                    "desc": "This paper introduces the Latent Flow Transformer (LFT), a new architecture for large language models that replaces multiple discrete layers with a single learned transport operator. By utilizing flow matching, LFT achieves significant model compression while still being compatible with traditional transformer designs. The authors also present the Flow Walking (FW) algorithm to enhance the coupling preservation in flow-based methods. Experimental results show that LFT can effectively reduce the number of layers while improving performance metrics, bridging the gap between autoregressive and flow-based generation techniques."
                },
                "zh": {
                    "title": "潜在流变换器：高效压缩大语言模型的创新方案",
                    "desc": "本文提出了一种新的模型——潜在流变换器（Latent Flow Transformer, LFT），旨在提高大语言模型的效率。LFT通过使用学习的传输算子替代多个离散层，从而实现显著的压缩，同时保持与原始架构的兼容性。我们还引入了流步行（Flow Walking, FW）算法，以解决现有流基方法在保持耦合方面的局限性。实验结果表明，LFT在压缩层数的同时，能够在性能上超越传统的层跳过方法，缩小自回归和流生成范式之间的差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14674",
            "title": "Reward Reasoning Model",
            "url": "https://huggingface.co/papers/2505.14674",
            "abstract": "Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at https://huggingface.co/Reward-Reasoning.",
            "score": 13,
            "issue_id": 3871,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "b51747905eeda5db",
            "authors": [
                "Jiaxin Guo",
                "Zewen Chi",
                "Li Dong",
                "Qingxiu Dong",
                "Xun Wu",
                "Shaohan Huang",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14674.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#alignment",
                    "#reasoning",
                    "#rl",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Рассуждающие модели вознаграждения: новый шаг к более точной оценке языковых моделей",
                    "desc": "Статья представляет новый подход к улучшению моделей вознаграждения для больших языковых моделей - Reward Reasoning Models (RRMs). RRMs используют цепочку рассуждений для более точной оценки сложных запросов. Модели обучаются с помощью reinforcement learning без необходимости в размеченных данных с рассуждениями. Эксперименты показывают, что RRMs превосходят обычные модели вознаграждения в различных областях и могут адаптивно использовать дополнительные вычисления во время вывода для повышения точности."
                },
                "en": {
                    "title": "Enhancing Reward Models with Adaptive Reasoning",
                    "desc": "This paper introduces Reward Reasoning Models (RRMs), which enhance the performance of reward models in large language models by incorporating a structured reasoning process. RRMs utilize additional computational resources during testing to tackle complex queries where the correct rewards are not obvious. The authors employ a reinforcement learning framework that allows these models to develop their reasoning capabilities autonomously, without needing specific reasoning examples in the training data. Experimental results indicate that RRMs outperform existing reward modeling methods across various benchmarks, demonstrating their ability to adaptively use test-time compute for improved reward accuracy."
                },
                "zh": {
                    "title": "提升奖励模型的推理能力",
                    "desc": "奖励模型在引导大型语言模型生成符合人类期望的输出中起着关键作用。然而，在有效利用测试时计算以提升奖励模型性能方面仍然存在挑战。我们提出了奖励推理模型（RRMs），它们专门设计用于在生成最终奖励之前执行深思熟虑的推理过程。实验结果表明，RRMs在各个领域的奖励建模基准上表现优越，并能够自适应地利用测试时计算进一步提高奖励准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14489",
            "title": "Reasoning Models Better Express Their Confidence",
            "url": "https://huggingface.co/papers/2505.14489",
            "abstract": "Despite their strengths, large language models (LLMs) often fail to communicate their confidence accurately, making it difficult to assess when they might be wrong and limiting their reliability. In this work, we demonstrate that reasoning models-LLMs that engage in extended chain-of-thought (CoT) reasoning-exhibit superior performance not only in problem-solving but also in accurately expressing their confidence. Specifically, we benchmark six reasoning models across six datasets and find that they achieve strictly better confidence calibration than their non-reasoning counterparts in 33 out of the 36 settings. Our detailed analysis reveals that these gains in calibration stem from the slow thinking behaviors of reasoning models-such as exploring alternative approaches and backtracking-which enable them to adjust their confidence dynamically throughout their CoT, making it progressively more accurate. In particular, we find that reasoning models become increasingly better calibrated as their CoT unfolds, a trend not observed in non-reasoning models. Moreover, removing slow thinking behaviors from the CoT leads to a significant drop in calibration. Lastly, we show that these gains are not exclusive to reasoning models-non-reasoning models also benefit when guided to perform slow thinking via in-context learning.",
            "score": 12,
            "issue_id": 3871,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "c945246738ceba22",
            "authors": [
                "Dongkeun Yoon",
                "Seungone Kim",
                "Sohee Yang",
                "Sunkyoung Kim",
                "Soyeon Kim",
                "Yongil Kim",
                "Eunbi Choi",
                "Yireun Kim",
                "Minjoon Seo"
            ],
            "affiliations": [
                "CMU",
                "KAIST",
                "LG AI Research",
                "UCL"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14489.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#interpretability",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Медленное мышление улучшает самооценку ИИ",
                    "desc": "Исследование показывает, что модели рассуждений (reasoning models) на основе больших языковых моделей (LLM) с расширенной цепочкой рассуждений (CoT) демонстрируют лучшую калибровку уверенности по сравнению с обычными LLM. Это достигается за счет 'медленного мышления' - исследования альтернативных подходов и корректировки уверенности в процессе рассуждений. Улучшение калибровки наблюдается по мере развертывания цепочки рассуждений. Удаление элементов 'медленного мышления' из CoT значительно снижает качество калибровки."
                },
                "en": {
                    "title": "Boosting Confidence Calibration in Language Models through Reasoning",
                    "desc": "This paper explores how large language models (LLMs) can improve their confidence calibration through reasoning techniques. It shows that LLMs that use chain-of-thought (CoT) reasoning not only solve problems better but also express their confidence more accurately. The study benchmarks six reasoning models and finds that they outperform non-reasoning models in confidence calibration across most scenarios. The authors conclude that the slow thinking behaviors inherent in reasoning models allow them to dynamically adjust their confidence, leading to better performance as the reasoning process unfolds."
                },
                "zh": {
                    "title": "推理模型提升自信度校准的秘密",
                    "desc": "尽管大型语言模型（LLMs）具有很强的能力，但它们在表达自信度方面常常不准确，这使得评估其错误的可能性变得困难，从而限制了它们的可靠性。本文展示了推理模型，即进行扩展思维链（CoT）推理的LLMs，不仅在解决问题方面表现优越，而且在准确表达自信度方面也表现更佳。我们对六个推理模型在六个数据集上进行了基准测试，发现它们在36种设置中有33种情况下的自信度校准明显优于非推理模型。我们的分析表明，这种校准的提升源于推理模型的慢思维行为，如探索替代方法和回溯，使它们能够在思维链中动态调整自信度，从而逐步提高准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13547",
            "title": "Exploring Federated Pruning for Large Language Models",
            "url": "https://huggingface.co/papers/2505.13547",
            "abstract": "LLM pruning has emerged as a promising technology for compressing LLMs, enabling their deployment on resource-limited devices. However, current methodologies typically require access to public calibration samples, which can be challenging to obtain in privacy-sensitive domains. To address this issue, we introduce FedPrLLM, a comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs to calculate a pruning mask matrix based on its local calibration data and share it with the server to prune the global model. This approach allows for collaborative pruning of the global model with the knowledge of each client while maintaining local data privacy. Additionally, we conduct extensive experiments to explore various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and the decision to scale weights. Our extensive evaluation reveals that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework. We hope our work will help guide future efforts in pruning LLMs in privacy-sensitive fields. Our code is available at https://github.com/Pengxin-Guo/FedPrLLM.",
            "score": 12,
            "issue_id": 3872,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "436f0f2e8c3f8481",
            "authors": [
                "Pengxin Guo",
                "Yinong Wang",
                "Wei Li",
                "Mengting Liu",
                "Ming Li",
                "Jinkai Zheng",
                "Liangqiong Qu"
            ],
            "affiliations": [
                "Guangming Laboratory",
                "Hangzhou Dianzi University",
                "Southern University of Science and Technology",
                "Sun Yat-sen University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13547.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#security",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🔒",
                "ru": {
                    "title": "Федеративное сжатие языковых моделей с сохранением приватности данных",
                    "desc": "FedPrLLM - это новая федеративная система для приватного сжатия больших языковых моделей (LLM). Она позволяет клиентам вычислять маски прунинга на локальных данных и делиться ими с сервером для обрезки глобальной модели. Эксперименты показали, что одноразовый прунинг с посдойным сравнением и без масштабирования весов дает оптимальные результаты. Этот подход может помочь в сжатии LLM для конфиденциальных областей применения."
                },
                "en": {
                    "title": "Privacy-Preserving Compression of LLMs with FedPrLLM",
                    "desc": "This paper presents FedPrLLM, a federated pruning framework aimed at compressing large language models (LLMs) while ensuring data privacy. Unlike traditional methods that require public calibration samples, FedPrLLM allows clients to generate pruning masks using their local data, which are then shared with a central server. This collaborative approach enables the global model to be pruned without exposing sensitive local data. The authors conducted experiments to identify the best pruning strategies, concluding that one-shot pruning with layer comparison and no weight scaling is the most effective method within their framework."
                },
                "zh": {
                    "title": "隐私保护下的LLM剪枝新方法",
                    "desc": "LLM剪枝是一种有前景的技术，可以压缩大型语言模型（LLM），使其能够在资源有限的设备上运行。现有的方法通常需要公共校准样本，这在隐私敏感的领域中很难获得。为了解决这个问题，我们提出了FedPrLLM，这是一个全面的联邦剪枝框架，旨在保护隐私的同时压缩LLM。在FedPrLLM中，每个客户端只需根据本地校准数据计算剪枝掩码矩阵，并将其与服务器共享，从而对全局模型进行剪枝。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14677",
            "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.14677",
            "abstract": "Learning general-purpose reasoning capabilities has long been a challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to a VLM -- by prompting the model to produce a reasoning chain before providing an answer -- can lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to a caption-reason-answer output format: initially generating a detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks.",
            "score": 11,
            "issue_id": 3873,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "032b4d528d6984fd",
            "authors": [
                "Jiaer Xia",
                "Yuhang Zang",
                "Peng Gao",
                "Yixuan Li",
                "Kaiyang Zhou"
            ],
            "affiliations": [
                "Hong Kong Baptist University",
                "Shanghai AI Lab",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14677.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#cv",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Визуальное рассуждение без явного обучения цепочке мыслей",
                    "desc": "Исследователи разработали метод обучения визуальных языковых моделей (VLM) выполнять рассуждения на основе изображений с помощью обучения с подкреплением и пар визуальных вопросов-ответов. Они обнаружили, что простое применение обучения с подкреплением может привести к появлению у модели нежелательных 'shortcuts'. Для решения этой проблемы авторы предложили формат вывода 'подпись-рассуждение-ответ', который побуждает модель сначала интерпретировать изображение. Разработанная модель Visionary-R1 превзошла другие сильные мультимодальные модели на нескольких тестах визуальных рассуждений."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Captions First!",
                    "desc": "This paper addresses the challenge of enhancing reasoning capabilities in visual language models (VLMs) using reinforcement learning. The authors propose a novel training approach that emphasizes generating detailed captions for images before reasoning, which helps prevent shortcut learning. By training their model, Visionary-R1, on a large dataset of visual question-answer pairs without explicit chain-of-thought supervision, they achieve superior performance compared to existing multimodal models. The results suggest that focusing on image interpretation prior to reasoning can significantly improve generalization across diverse data distributions."
                },
                "zh": {
                    "title": "通过强化学习提升视觉语言模型的推理能力",
                    "desc": "本论文探讨了如何通过强化学习训练视觉语言模型（VLM）来进行图像数据的推理。我们的方法不依赖于显式的思维链（CoT）监督，而是通过视觉问答对来实现。研究表明，简单地应用强化学习可以导致模型在回答之前生成推理链，但这可能导致模型在面对新数据时的泛化能力下降。为了解决这个问题，我们提出让模型在推理之前先对图像进行解释，从而提高其推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14673",
            "title": "Training-Free Watermarking for Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2505.14673",
            "abstract": "Invisible image watermarking can protect image ownership and prevent malicious misuse of visual generative models. However, existing generative watermarking methods are mainly designed for diffusion models while watermarking for autoregressive image generation models remains largely underexplored. We propose IndexMark, a training-free watermarking framework for autoregressive image generation models. IndexMark is inspired by the redundancy property of the codebook: replacing autoregressively generated indices with similar indices produces negligible visual differences. The core component in IndexMark is a simple yet effective match-then-replace method, which carefully selects watermark tokens from the codebook based on token similarity, and promotes the use of watermark tokens through token replacement, thereby embedding the watermark without affecting the image quality. Watermark verification is achieved by calculating the proportion of watermark tokens in generated images, with precision further improved by an Index Encoder. Furthermore, we introduce an auxiliary validation scheme to enhance robustness against cropping attacks. Experiments demonstrate that IndexMark achieves state-of-the-art performance in terms of image quality and verification accuracy, and exhibits robustness against various perturbations, including cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG compression.",
            "score": 11,
            "issue_id": 3873,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "ec739be428657981",
            "authors": [
                "Yu Tong",
                "Zihao Pan",
                "Shuai Yang",
                "Kaiyang Zhou"
            ],
            "affiliations": [
                "Hong Kong Baptist University",
                "Peking University",
                "Sun Yat-sen University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14673.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#training",
                    "#security"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Незаметная защита авторства изображений без переобучения модели",
                    "desc": "IndexMark - это безтренировочный метод встраивания водяных знаков в автореггрессивные модели генерации изображений. Он использует избыточность кодовой книги, заменяя сгенерированные индексы на похожие для внедрения водяного знака без ухудшения качества изображения. Верификация водяного знака осуществляется путем подсчета доли токенов водяного знака в сгенерированных изображениях. Эксперименты показывают, что IndexMark достигает высоких результатов по качеству изображений и точности верификации, а также демонстрирует устойчивость к различным искажениям."
                },
                "en": {
                    "title": "IndexMark: Watermarking Autoregressive Models with Precision and Robustness",
                    "desc": "This paper presents IndexMark, a novel watermarking framework specifically designed for autoregressive image generation models. Unlike previous methods focused on diffusion models, IndexMark utilizes the redundancy in the codebook to embed watermarks without compromising image quality. The framework employs a match-then-replace strategy to select and replace indices with similar ones, effectively embedding the watermark. Additionally, it includes a robust verification process and an auxiliary validation scheme to withstand various image perturbations, demonstrating superior performance in both image quality and watermark verification accuracy."
                },
                "zh": {
                    "title": "自回归图像生成的隐形水印新方案",
                    "desc": "隐形图像水印技术可以保护图像的所有权，并防止视觉生成模型的恶意滥用。现有的生成水印方法主要针对扩散模型，而自回归图像生成模型的水印技术尚未得到充分研究。我们提出了IndexMark，这是一种无需训练的自回归图像生成模型水印框架。IndexMark通过匹配和替换的方法，利用代码本的冗余特性，嵌入水印而不影响图像质量，并在多种干扰下表现出良好的鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14652",
            "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
            "url": "https://huggingface.co/papers/2505.14652",
            "abstract": "Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks.",
            "score": 11,
            "issue_id": 3869,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "494fe90709dc6c63",
            "authors": [
                "Xueguang Ma",
                "Qian Liu",
                "Dongfu Jiang",
                "Ge Zhang",
                "Zejun Ma",
                "Wenhu Chen"
            ],
            "affiliations": [
                "M-A-P",
                "Singapore",
                "TikTok",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14652.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#benchmark",
                    "#math",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Универсальный рассуждатель: расширение возможностей LLM в многодоменном анализе",
                    "desc": "В статье представлен новый подход к обучению больших языковых моделей (LLM) для улучшения их способностей к рассуждению в различных областях знаний. Авторы предлагают метод General-Reasoner, который включает создание масштабного набора данных с вопросами и проверяемыми ответами из разных дисциплин. Они также разработали генеративную модель для верификации ответов, способную анализировать цепочки рассуждений и учитывать контекст. Результаты экспериментов показывают, что General-Reasoner превосходит существующие методы по эффективности и обобщаемости рассуждений на различных тестовых наборах."
                },
                "en": {
                    "title": "Empowering LLMs with General-Reasoner for Diverse Reasoning",
                    "desc": "This paper introduces General-Reasoner, a new approach to improve the reasoning abilities of large language models (LLMs) across various fields. It leverages a large-scale dataset of questions with verifiable answers, which is created through web crawling, to train LLMs without the need for prior supervised fine-tuning. Additionally, it employs a generative model-based answer verifier that enhances the model's ability to understand context and think through problems. The results show that General-Reasoner significantly outperforms existing methods in reasoning tasks, especially in mathematics, while also being effective in other domains like physics and finance."
                },
                "zh": {
                    "title": "提升LLM推理能力的新范式",
                    "desc": "强化学习（RL）在提升大型语言模型（LLM）的推理能力方面展现出强大的潜力。本文提出了一种新颖的训练范式——General-Reasoner，旨在增强LLM在多领域的推理能力。我们构建了一个大规模、高质量的问题数据集，并开发了一种基于生成模型的答案验证器，取代了传统的基于规则的验证方法。通过在多个领域的数据集上进行评估，General-Reasoner在推理性能上超越了现有的基线方法，尤其在数学推理任务中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13866",
            "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
            "url": "https://huggingface.co/papers/2505.13866",
            "abstract": "Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers. While this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and throughput of token generation, limiting the practical deployment of such models. We propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths. RPC periodically compresses the KV cache by retaining KV cache that receive high importance score, which are computed using a selector window composed of recently generated queries. Experiments show that RPC improves generation throughput of QwQ-32B by up to 1.60times compared to the inference with full KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression.",
            "score": 11,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "72f6460e348e135a",
            "authors": [
                "Jiwon Song",
                "Dongwon Jo",
                "Yulhwa Kim",
                "Jae-Joon Kim"
            ],
            "affiliations": [
                "Seoul National University",
                "Sungkyunkwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13866.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Ускорение языковых моделей через сжатие путей рассуждений",
                    "desc": "Статья представляет метод Сжатия Пути Рассуждений (RPC) для ускорения вывода моделей языка, ориентированных на рассуждения. RPC использует семантическую разреженность путей рассуждений, периодически сжимая KV-кэш путем сохранения наиболее важных элементов. Эксперименты показывают, что RPC увеличивает пропускную способность генерации модели QwQ-32B до 1.60 раз по сравнению с выводом с полным KV-кэшем. Метод демонстрирует, что семантическая разреженность в следах рассуждений может быть эффективно использована для сжатия, предлагая практический путь к эффективному развертыванию рассуждающих языковых моделей."
                },
                "en": {
                    "title": "Efficient Inference with Reasoning Path Compression",
                    "desc": "This paper introduces Reasoning Path Compression (RPC), a method designed to enhance the efficiency of reasoning-focused language models during inference. By utilizing the concept of semantic sparsity, RPC compresses the key-value (KV) cache, retaining only the most important elements based on recent queries. This approach significantly increases the throughput of token generation while only slightly affecting accuracy. The results indicate that RPC can improve the performance of large models like QwQ-32B, making them more practical for real-world applications."
                },
                "zh": {
                    "title": "推理路径压缩：高效推理的新方法",
                    "desc": "最近专注于推理的语言模型通过生成较长的中间推理路径来实现高准确率。这种方法在解决需要逻辑思维的问题时非常有效，但长推理路径显著增加了内存使用和令牌生成的吞吐量，限制了模型的实际应用。我们提出了一种名为推理路径压缩（RPC）的方法，通过利用推理路径的语义稀疏性来加速推理。实验表明，RPC在AIME 2024基准测试中相比于完整KV缓存，提升了QwQ-32B的生成吞吐量，准确率仅下降1.2%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14640",
            "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
            "url": "https://huggingface.co/papers/2505.14640",
            "abstract": "Large multimodal models (LMMs) have recently emerged as a powerful tool for long video understanding (LVU), prompting the development of standardized LVU benchmarks to evaluate their performance. However, our investigation reveals a rather sober lesson for existing LVU benchmarks. First, most existing benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation results are inflated due to the possibility of guessing the correct answer; Second, a significant portion of questions in these benchmarks have strong priors to allow models to answer directly without even reading the input video. For example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame from a long video on Video-MME. We also observe that increasing the number of frames does not necessarily lead to improvement on existing benchmarks, which is counterintuitive. As a result, the validity and robustness of current LVU benchmarks are undermined, impeding a faithful assessment of LMMs' long-video understanding capability. To tackle this problem, we propose VideoEval-Pro, a realistic LVU benchmark containing questions with open-ended short-answer, which truly require understanding the entire video. VideoEval-Pro assesses both segment-level and full-video understanding through perception and reasoning tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the following findings: (1) video LMMs show drastic performance (>25\\%) drops on open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do not lead to higher open-ended scores on VideoEval-Pro; (3) compared to other MCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input frames. Our results show that VideoEval-Pro offers a more realistic and reliable measure of long video understanding, providing a clearer view of progress in this domain.",
            "score": 9,
            "issue_id": 3869,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "45d64d535935c6a4",
            "authors": [
                "Wentao Ma",
                "Weiming Ren",
                "Yiming Jia",
                "Zhuofeng Li",
                "Ping Nie",
                "Ge Zhang",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Independent",
                "M-A-P",
                "Shanghai University",
                "University of Toronto",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14640.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Реалистичная оценка понимания длинных видео: от угадывания к глубокому анализу",
                    "desc": "Эта статья посвящена проблемам существующих бенчмарков для оценки понимания длинных видео большими мультимодальными моделями (LMM). Авторы выявили, что текущие бенчмарки часто используют вопросы с множественным выбором, что приводит к завышенным результатам из-за возможности угадывания. Они предлагают новый бенчмарк VideoEval-Pro с открытыми вопросами, требующими понимания всего видео. Результаты показывают, что производительность моделей на открытых вопросах значительно ниже, чем на вопросах с множественным выбором, что дает более реалистичную оценку способностей LMM к пониманию длинных видео."
                },
                "en": {
                    "title": "Revolutionizing Long Video Understanding with Realistic Benchmarks",
                    "desc": "This paper discusses the limitations of current benchmarks for evaluating long video understanding (LVU) in large multimodal models (LMMs). It highlights that many existing benchmarks rely on multiple-choice questions (MCQs), which can inflate performance scores due to guessing and prior knowledge. The authors introduce VideoEval-Pro, a new benchmark that uses open-ended questions requiring comprehensive video understanding, thus providing a more accurate assessment of LMM capabilities. Their findings reveal significant performance drops for LMMs on open-ended questions compared to MCQs, indicating that current benchmarks may not effectively measure true understanding of long videos."
                },
                "zh": {
                    "title": "VideoEval-Pro：提升长视频理解的真实评估",
                    "desc": "大型多模态模型（LMMs）在长视频理解（LVU）中表现出色，但现有的LVU基准测试存在问题。许多基准依赖多项选择题（MCQs），这导致评估结果被夸大，因为模型可能通过猜测获得正确答案。此外，部分问题的先验信息使得模型可以在不观看视频的情况下直接回答。为了解决这些问题，我们提出了VideoEval-Pro基准，采用开放式短答案问题，真正考察模型对整个视频的理解能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14631",
            "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
            "url": "https://huggingface.co/papers/2505.14631",
            "abstract": "Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model's capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems.",
            "score": 9,
            "issue_id": 3872,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "12732abf8e9d807f",
            "authors": [
                "Lingjie Jiang",
                "Xun Wu",
                "Shaohan Huang",
                "Qingxiu Dong",
                "Zewen Chi",
                "Li Dong",
                "Xingxing Zhang",
                "Tengchao Lv",
                "Lei Cui",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14631.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#rl",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Гибридное мышление: новый шаг в развитии искусственного интеллекта",
                    "desc": "Статья представляет новый тип моделей машинного обучения - Large Hybrid-Reasoning Models (LHRMs), способных адаптивно определять необходимость применения расширенного процесса мышления в зависимости от контекста запроса пользователя. Авторы предлагают двухэтапный процесс обучения, включающий Hybrid Fine-Tuning (HFT) и онлайн-обучение с подкреплением с использованием Hybrid Group Policy Optimization (HGPO). Введена метрика Hybrid Accuracy для оценки способности модели к гибридному мышлению. Эксперименты показывают, что LHRMs превосходят существующие LRMs и LLMs в рассуждениях и общих возможностях, значительно повышая эффективность."
                },
                "en": {
                    "title": "Adaptive Thinking for Efficient Reasoning",
                    "desc": "This paper presents Large Hybrid-Reasoning Models (LHRMs), which enhance reasoning abilities by deciding when to engage in extended thinking based on the complexity of user queries. Unlike traditional Large Language Models (LLMs), LHRMs use a two-stage training approach that includes Hybrid Fine-Tuning and online reinforcement learning to optimize their reasoning process. The authors introduce a new metric, Hybrid Accuracy, to evaluate the effectiveness of these models in adapting their thinking strategies. Experimental results demonstrate that LHRMs outperform existing models in both reasoning and efficiency, suggesting a new direction for developing intelligent systems that balance thinking depth with response speed."
                },
                "zh": {
                    "title": "自适应混合推理，提升效率与能力",
                    "desc": "最近的大型推理模型（LRMs）在推理能力上显著优于传统的大型语言模型（LLMs），因为它们在生成最终回答之前进行了更深入的思考。然而，过长的思考过程会导致令牌消耗和延迟的显著增加，这在处理简单查询时尤其不必要。为此，我们提出了大型混合推理模型（LHRMs），这种模型能够根据用户查询的上下文信息自适应地决定是否进行思考。我们的研究表明，LHRMs在处理不同难度和类型的查询时，能够有效地进行混合思考，并在推理和整体能力上超越现有的LRMs和LLMs，同时显著提高效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13559",
            "title": "CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the\n  Limits of Large Language Models",
            "url": "https://huggingface.co/papers/2505.13559",
            "abstract": "Code-switching (CS) poses a significant challenge for Large Language Models (LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue to English summarization. CS-Sum is the first benchmark for CS dialogue summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language pair. Evaluating ten LLMs, including open and closed-source models, we analyze performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA on synthetic data) approaches. Our findings show that though the scores on automated metrics are high, LLMs make subtle mistakes that alter the complete meaning of the dialogue. To this end, we introduce 3 most common type of errors that LLMs make when handling CS input. Error rates vary across CS pairs and LLMs, with some LLMs showing more frequent errors on certain language pairs, underscoring the need for specialized training on code-switched data.",
            "score": 9,
            "issue_id": 3870,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "9ea21df740810e7e",
            "authors": [
                "Sathya Krishnan Suresh",
                "Tanmay Surana",
                "Lim Zhi Hao",
                "Eng Siong Chng"
            ],
            "affiliations": [
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13559.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#machine_translation",
                    "#low_resource",
                    "#synthetic",
                    "#training",
                    "#benchmark",
                    "#multilingual"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "CS-Sum: новый бенчмарк для оценки понимания переключения кодов в больших языковых моделях",
                    "desc": "Статья представляет новый бенчмарк CS-Sum для оценки понимания переключения кодов (code-switching) в больших языковых моделях. Бенчмарк включает задачу суммаризации диалогов с переключением кодов на английский язык для пар китайский-английский, тамильский-английский и малайский-английский. Авторы оценили производительность десяти языковых моделей, используя различные подходы, включая few-shot обучение и тонкую настройку. Результаты показывают, что модели часто допускают тонкие ошибки, искажающие смысл диалога, несмотря на высокие показатели автоматических метрик."
                },
                "en": {
                    "title": "Enhancing LLMs for Code-Switching Comprehensibility",
                    "desc": "This paper addresses the challenges that Large Language Models (LLMs) face when dealing with code-switching (CS) in dialogues. It introduces CS-Sum, a benchmark designed to assess how well LLMs can summarize CS dialogues into English, focusing on three language pairs: Mandarin-English, Tamil-English, and Malay-English. The study evaluates ten different LLMs using various methods, including few-shot learning and fine-tuning techniques, to understand their performance on CS data. The results reveal that while LLMs achieve high scores on automated metrics, they often make subtle errors that can change the meaning of the dialogues, highlighting the need for improved training on code-switched content."
                },
                "zh": {
                    "title": "评估代码切换的可理解性",
                    "desc": "代码切换（CS）对大型语言模型（LLMs）构成了重大挑战，但其可理解性在LLMs中的研究仍然不足。我们提出了CS-Sum，旨在通过将CS对话总结为英语来评估LLMs对CS的理解能力。CS-Sum是首个针对普通话-英语、泰米尔语-英语和马来语-英语的CS对话总结基准，包含每对语言900到1300个人工标注的对话。我们的研究发现，尽管自动评估指标得分较高，LLMs在处理CS输入时仍会出现细微错误，这些错误会改变对话的完整含义。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14680",
            "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search",
            "url": "https://huggingface.co/papers/2505.14680",
            "abstract": "Generative AI search is reshaping information retrieval by offering end-to-end answers to complex queries, reducing users' reliance on manually browsing and summarizing multiple web pages. However, while this paradigm enhances convenience, it disrupts the feedback-driven improvement loop that has historically powered the evolution of traditional Web search. Web search can continuously improve their ranking models by collecting large-scale, fine-grained user feedback (e.g., clicks, dwell time) at the document level. In contrast, generative AI search operates through a much longer search pipeline, spanning query decomposition, document retrieval, and answer generation, yet typically receives only coarse-grained feedback on the final answer. This introduces a feedback loop disconnect, where user feedback for the final output cannot be effectively mapped back to specific system components, making it difficult to improve each intermediate stage and sustain the feedback loop. In this paper, we envision NExT-Search, a next-generation paradigm designed to reintroduce fine-grained, process-level feedback into generative AI search. NExT-Search integrates two complementary modes: User Debug Mode, which allows engaged users to intervene at key stages; and Shadow User Mode, where a personalized user agent simulates user preferences and provides AI-assisted feedback for less interactive users. Furthermore, we envision how these feedback signals can be leveraged through online adaptation, which refines current search outputs in real-time, and offline update, which aggregates interaction logs to periodically fine-tune query decomposition, retrieval, and generation models. By restoring human control over key stages of the generative AI search pipeline, we believe NExT-Search offers a promising direction for building feedback-rich AI search systems that can evolve continuously alongside human feedback.",
            "score": 8,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "ace242db16327202",
            "authors": [
                "Sunhao Dai",
                "Wenjie Wang",
                "Liang Pang",
                "Jun Xu",
                "See-Kiong Ng",
                "Ji-Rong Wen",
                "Tat-Seng Chua"
            ],
            "affiliations": [
                "CAS Key Laboratory of AI Safety Institute of Computing Technology Chinese Academy of Sciences",
                "Gaoling School of Artificial Intelligence Renmin University of China",
                "National University of Singapore",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14680.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#rag",
                    "#rlhf",
                    "#agents",
                    "#alignment"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Возвращение человеческого контроля в ИИ-поиск",
                    "desc": "Статья представляет концепцию NExT-Search, новую парадигму генеративного ИИ-поиска. Она направлена на восстановление детальной обратной связи в процессе поиска, что было утрачено при переходе от традиционного веб-поиска к генеративному ИИ. NExT-Search предлагает два режима: режим отладки пользователем и режим теневого пользователя, позволяющие собирать обратную связь на разных этапах поиска. Система использует эту обратную связь для онлайн-адаптации и офлайн-обновления моделей декомпозиции запросов, извлечения и генерации ответов."
                },
                "en": {
                    "title": "NExT-Search: Enhancing Generative AI Search with User Feedback",
                    "desc": "This paper discusses the challenges of integrating user feedback into generative AI search systems, which provide direct answers to complex queries but lack detailed feedback mechanisms. Traditional web search benefits from fine-grained user interactions, allowing for continuous improvement of ranking models. The proposed NExT-Search framework aims to bridge this gap by introducing two modes of user feedback: User Debug Mode for active user engagement and Shadow User Mode for passive feedback collection. By leveraging both real-time and aggregated feedback, NExT-Search seeks to enhance the generative AI search process and ensure it evolves in response to user needs."
                },
                "zh": {
                    "title": "NExT-Search：重塑生成式搜索的反馈循环",
                    "desc": "生成式人工智能搜索正在改变信息检索，通过提供端到端的答案来应对复杂查询，减少用户手动浏览和总结多个网页的依赖。然而，这种新模式虽然提高了便利性，却打破了传统网页搜索中基于反馈的改进循环。传统搜索可以通过收集用户反馈（如点击率和停留时间）来不断改进排名模型，而生成式搜索则面临反馈循环断裂的问题，用户反馈难以有效映射到系统的具体组件。本文提出了NExT-Search，旨在将细粒度的过程级反馈重新引入生成式搜索，结合用户调试模式和影子用户模式，以实现实时和离线的反馈信号利用，从而持续改进搜索系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14135",
            "title": "Hunyuan-Game: Industrial-grade Intelligent Game Creation Model",
            "url": "https://huggingface.co/papers/2505.14135",
            "abstract": "Intelligent game creation represents a transformative advancement in game development, utilizing generative artificial intelligence to dynamically generate and enhance game content. Despite notable progress in generative models, the comprehensive synthesis of high-quality game assets, including both images and videos, remains a challenging frontier. To create high-fidelity game content that simultaneously aligns with player preferences and significantly boosts designer efficiency, we present Hunyuan-Game, an innovative project designed to revolutionize intelligent game production. Hunyuan-Game encompasses two primary branches: image generation and video generation. The image generation component is built upon a vast dataset comprising billions of game images, leading to the development of a group of customized image generation models tailored for game scenarios: (1) General Text-to-Image Generation. (2) Game Visual Effects Generation, involving text-to-effect and reference image-based game visual effect generation. (3) Transparent Image Generation for characters, scenes, and game visual effects. (4) Game Character Generation based on sketches, black-and-white images, and white models. The video generation component is built upon a comprehensive dataset of millions of game and anime videos, leading to the development of five core algorithmic models, each targeting critical pain points in game development and having robust adaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2) 360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4) Generative Video Super-Resolution. (5) Interactive Game Video Generation. These image and video generation models not only exhibit high-level aesthetic expression but also deeply integrate domain-specific knowledge, establishing a systematic understanding of diverse game and anime art styles.",
            "score": 8,
            "issue_id": 3869,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "344469b85ea1e75e",
            "authors": [
                "Ruihuang Li",
                "Caijin Zhou",
                "Shoujian Zheng",
                "Jianxiang Lu",
                "Jiabin Huang",
                "Comi Chen",
                "Junshu Tang",
                "Guangzheng Xu",
                "Jiale Tao",
                "Hongmei Wang",
                "Donghao Li",
                "Wenqing Yu",
                "Senbo Wang",
                "Zhimin Li",
                "Yetshuan Shi",
                "Haoyu Yang",
                "Yukun Wang",
                "Wenxun Dai",
                "Jiaqi Li",
                "Linqing Wang",
                "Qixun Wang",
                "Zhiyong Xu",
                "Yingfang Zhang",
                "Jiangfeng Xiong",
                "Weijie Kong",
                "Chao Zhang",
                "Hongxin Zhang",
                "Qiaoling Zheng",
                "Weiting Guo",
                "Xinchi Deng",
                "Yixuan Li",
                "Renjia Wei",
                "Yulin Jian",
                "Duojun Huang",
                "Xuhua Ren",
                "Sihuan Lin",
                "Yifu Sun",
                "Yuan Zhou",
                "Joey Wang",
                "Qin Lin",
                "Jingmiao Yu",
                "Jihong Zhang",
                "Caesar Zhong",
                "Di Wang",
                "Yuhong Liu",
                "Linus",
                "Jie Jiang",
                "Longhuang Wu",
                "Shuai Shao",
                "Qinglin Lu"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14135.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#cv",
                    "#games",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Революция в создании игр: ИИ на службе разработчиков",
                    "desc": "Проект Hunyuan-Game представляет собой инновационный подход к интеллектуальному созданию игр с использованием генеративного искусственного интеллекта. Он включает в себя две основные ветви: генерацию изображений и генерацию видео, каждая из которых основана на обширных наборах данных игровых ресурсов. Модели генерации изображений способны создавать различные игровые элементы, включая общие сцены, визуальные эффекты и персонажей. Компонент генерации видео предлагает пять ключевых алгоритмических моделей, направленных на решение критических проблем в разработке игр."
                },
                "en": {
                    "title": "Revolutionizing Game Development with AI-Driven Content Creation",
                    "desc": "The paper introduces Hunyuan-Game, a project that leverages generative artificial intelligence to enhance game development by creating high-quality game assets. It focuses on two main areas: image generation and video generation, utilizing extensive datasets of game images and videos. The image generation models include various techniques for creating game visuals, such as text-to-image and character generation from sketches. The video generation models address specific challenges in game video production, offering solutions like image-to-video synthesis and interactive video generation, all while maintaining aesthetic quality and understanding of game art styles."
                },
                "zh": {
                    "title": "智能游戏创作的未来",
                    "desc": "智能游戏创作是游戏开发中的一项变革性进展，利用生成性人工智能动态生成和增强游戏内容。尽管生成模型取得了显著进展，但高质量游戏资产的综合合成仍然是一个挑战。Hunyuan-Game项目旨在通过图像和视频生成，提升游戏内容的质量和设计师的效率。该项目包括图像生成和视频生成两个主要部分，涵盖了多种定制化的生成模型，能够满足不同游戏场景的需求。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13430",
            "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
            "url": "https://huggingface.co/papers/2505.13430",
            "abstract": "As the size of large language models grows exponentially, GPU memory has become a bottleneck for adapting these models to downstream tasks. In this paper, we aim to push the limits of memory-efficient training by minimizing memory usage on model weights, gradients, and optimizer states, within a unified framework. Our idea is to eliminate both gradients and optimizer states using zeroth-order optimization, which approximates gradients by perturbing weights during forward passes to identify gradient directions. To minimize memory usage on weights, we employ model quantization, e.g., converting from bfloat16 to int4. However, directly applying zeroth-order optimization to quantized weights is infeasible due to the precision gap between discrete weights and continuous gradients, which would otherwise require de-quantization and re-quantization. To overcome this challenge, we propose Quantized Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous quantization scale for gradient estimation and uses a directional derivative clipping method to stabilize training. QZO is orthogonal to both scalar-based and codebook-based post-training quantization methods. Compared to full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by more than 18times for 4-bit LLMs, and enables fine-tuning Llama-2-13B and Stable Diffusion 3.5 Large within a single 24GB GPU.",
            "score": 8,
            "issue_id": 3873,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "6728dda02398fbcc",
            "authors": [
                "Sifeng Shang",
                "Jiayi Zhou",
                "Chenyu Lin",
                "Minxian Li",
                "Kaiyang Zhou"
            ],
            "affiliations": [
                "Hong Kong Baptist University",
                "Nanjing University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13430.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "QZO: Революция в эффективном обучении крупных языковых моделей",
                    "desc": "Статья представляет новый метод оптимизации крупных языковых моделей - Quantized Zeroth-order Optimization (QZO). Этот подход позволяет значительно сократить использование памяти GPU при адаптации больших моделей к конкретным задачам. QZO использует оптимизацию нулевого порядка и квантование модели для устранения необходимости хранения градиентов и состояний оптимизатора. Метод преодолевает проблему разрыва точности между дискретными весами и непрерывными градиентами, возмущая непрерывную шкалу квантования для оценки градиентов."
                },
                "en": {
                    "title": "Revolutionizing Memory Efficiency in Large Language Model Training",
                    "desc": "This paper addresses the challenge of training large language models (LLMs) with limited GPU memory. It introduces a method called Quantized Zeroth-order Optimization (QZO) that reduces memory usage by eliminating the need for gradients and optimizer states. QZO achieves this by perturbing the quantization scale of weights to estimate gradients, allowing for efficient training without the need for de-quantization. The proposed approach significantly lowers memory costs, enabling the fine-tuning of large models on standard GPUs."
                },
                "zh": {
                    "title": "突破内存瓶颈，实现高效训练",
                    "desc": "随着大型语言模型规模的快速增长，GPU内存成为适应这些模型到下游任务的瓶颈。本文提出了一种内存高效训练的方法，通过在统一框架内最小化模型权重、梯度和优化器状态的内存使用。我们使用零阶优化来消除梯度和优化器状态，通过在前向传播中扰动权重来近似梯度方向。我们还提出了量化零阶优化（QZO），通过扰动连续量化尺度来估计梯度，从而在不牺牲精度的情况下显著减少内存消耗。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12448",
            "title": "SSR: Enhancing Depth Perception in Vision-Language Models via\n  Rationale-Guided Spatial Reasoning",
            "url": "https://huggingface.co/papers/2505.12448",
            "abstract": "Despite impressive advancements in Visual-Language Models (VLMs) for multi-modal tasks, their reliance on RGB inputs limits precise spatial understanding. Existing methods for integrating spatial cues, such as point clouds or depth, either require specialized sensors or fail to effectively exploit depth information for higher-order reasoning. To this end, we propose a novel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that transforms raw depth data into structured, interpretable textual rationales. These textual rationales serve as meaningful intermediate representations to significantly enhance spatial reasoning capabilities. Additionally, we leverage knowledge distillation to compress the generated rationales into compact latent embeddings, which facilitate resource-efficient and plug-and-play integration into existing VLMs without retraining. To enable comprehensive evaluation, we introduce a new dataset named SSR-CoT, a million-scale visual-language reasoning dataset enriched with intermediate spatial reasoning annotations, and present SSRBench, a comprehensive multi-task benchmark. Extensive experiments on multiple benchmarks demonstrate SSR substantially improves depth utilization and enhances spatial reasoning, thereby advancing VLMs toward more human-like multi-modal understanding. Our project page is at https://yliu-cs.github.io/SSR.",
            "score": 7,
            "issue_id": 3869,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 мая",
                "en": "May 18",
                "zh": "5月18日"
            },
            "hash": "18ffd5153e838d86",
            "authors": [
                "Yang Liu",
                "Ming Ma",
                "Xiaomin Yu",
                "Pengxiang Ding",
                "Han Zhao",
                "Mingyang Sun",
                "Siteng Huang",
                "Donglin Wang"
            ],
            "affiliations": [
                "Alibaba DAMO Academy",
                "Harbin Institute of Technology",
                "Shanghai Innovation Institute",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12448.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#cv",
                    "#interpretability",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение пространственного понимания в визуально-языковых моделях с помощью структурированных текстовых обоснований",
                    "desc": "Статья представляет новый метод под названием SSR (Spatial Sense and Reasoning) для улучшения пространственного понимания в визуально-языковых моделях (VLM). SSR преобразует данные о глубине в структурированные текстовые обоснования, которые затем сжимаются в компактные латентные представления с помощью дистилляции знаний. Авторы также представляют новый набор данных SSR-CoT и бенчмарк SSRBench для оценки пространственных рассуждений в мультимодальных задачах. Эксперименты показывают, что SSR значительно улучшает использование информации о глубине и повышает способность моделей к пространственным рассуждениям."
                },
                "en": {
                    "title": "Enhancing Spatial Reasoning in VLMs with SSR",
                    "desc": "This paper introduces a new method called Spatial Sense and Reasoning (SSR) to improve how Visual-Language Models (VLMs) understand spatial information. SSR transforms raw depth data into structured textual rationales, which help the model reason about space more effectively. The authors also use knowledge distillation to create compact representations of these rationales, allowing for easy integration into existing VLMs without needing to retrain them. To support their research, they present a new dataset, SSR-CoT, and a benchmark, SSRBench, which show that SSR significantly enhances spatial reasoning in VLMs."
                },
                "zh": {
                    "title": "提升空间推理能力的创新方法",
                    "desc": "尽管视觉语言模型（VLMs）在多模态任务上取得了显著进展，但它们对RGB输入的依赖限制了精确的空间理解。现有的方法在整合空间线索时，往往需要专用传感器或无法有效利用深度信息进行更高阶的推理。为此，我们提出了一种新颖的空间感知与推理方法（SSR），该框架将原始深度数据转化为结构化的可解释文本推理。这些文本推理作为有意义的中间表示，显著增强了空间推理能力，并通过知识蒸馏将生成的推理压缩为紧凑的潜在嵌入，便于与现有VLMs的高效集成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14681",
            "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing\n  Cognitive Effort in MoE Reasoning Models Without Additional Training",
            "url": "https://huggingface.co/papers/2505.14681",
            "abstract": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.",
            "score": 6,
            "issue_id": 3878,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "89ee9aa82837601e",
            "authors": [
                "Mengru Wang",
                "Xingyu Chen",
                "Yue Wang",
                "Zhiwei He",
                "Jiahao Xu",
                "Tian Liang",
                "Qiuzhi Liu",
                "Yunzhi Yao",
                "Wenxuan Wang",
                "Ruotian Ma",
                "Haitao Mi",
                "Ningyu Zhang",
                "Zhaopeng Tu",
                "Xiaolong Li",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14681.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#interpretability",
                    "#architecture",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Усиление когнитивных экспертов для эффективного машинного рассуждения",
                    "desc": "Статья представляет новый метод под названием RICE для улучшения рассуждений в моделях машинного обучения с архитектурой Mixture-of-Experts. Используя нормализованную поточечную взаимную информацию (nPMI), авторы выявляют специализированные 'когнитивные эксперты', отвечающие за мета-уровневые операции рассуждения. Эксперименты с ведущими моделями типа MoE показали заметное улучшение точности рассуждений, когнитивной эффективности и обобщения на разные домены. Подход RICE превосходит существующие методы управления рассуждениями, сохраняя при этом общие навыки модели следовать инструкциям."
                },
                "en": {
                    "title": "Enhancing Reasoning Efficiency with Cognitive Experts",
                    "desc": "This paper presents a new method called Reinforcing Cognitive Experts (RICE) to improve the reasoning capabilities of Mixture-of-Experts (MoE) architectures in Large Reasoning Models (LRMs). RICE addresses issues of cognitive inefficiencies, such as overthinking and underthinking, by selectively activating specialized experts during inference. The method uses normalized Pointwise Mutual Information (nPMI) to identify these 'cognitive experts' that enhance meta-level reasoning processes. Empirical results show that RICE significantly boosts reasoning accuracy and efficiency compared to existing techniques, while maintaining the model's ability to follow instructions effectively."
                },
                "zh": {
                    "title": "强化认知专家：提升推理效率的新方法",
                    "desc": "混合专家（MoE）架构在大型推理模型（LRMs）中通过选择性激活专家来实现出色的推理能力。然而，现有的推理模型常常面临认知效率低下的问题，如过度思考和不足思考。为了解决这些问题，我们提出了一种新的推理时间引导方法，称为强化认知专家（RICE），旨在在不增加额外训练或复杂启发式的情况下提高推理性能。通过利用归一化的点互信息（nPMI），我们系统地识别出专门的专家，称为“认知专家”，以协调以“<think>”等标记为特征的元级推理操作。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14464",
            "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters",
            "url": "https://huggingface.co/papers/2505.14464",
            "abstract": "Distillation has emerged as a practical and effective approach to enhance the reasoning capabilities of open-source language models. In this work, we conduct a large-scale empirical study on reasoning data distillation by collecting verified outputs from three state-of-the-art teacher models-AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We construct three parallel datasets and analyze their distributions, revealing that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench. The AM-based model consistently achieves the best performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing longer responses for harder tasks and shorter ones for simpler tasks. These findings highlight the value of high-quality, verified reasoning traces. We release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support future research on open and high-performing reasoning-oriented language models. The datasets are publicly available on Hugging FaceDatasets are available on Hugging Face: \\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled}, https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.",
            "score": 6,
            "issue_id": 3869,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "709996374c466144",
            "authors": [
                "Xiaoyu Tian",
                "Yunjie Ji",
                "Haotian Wang",
                "Shuaiting Chen",
                "Sitong Zhao",
                "Yiping Peng",
                "Han Zhao",
                "Xiangang Li"
            ],
            "affiliations": [
                "Beike (Ke.com)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14464.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#data",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Дистилляция знаний улучшает способности ИИ к рассуждению",
                    "desc": "Это исследование посвящено дистилляции данных для улучшения способностей рассуждения языковых моделей с открытым исходным кодом. Авторы собрали верифицированные выходные данные от трех современных моделей-учителей на корпусе из 1,89 миллиона запросов. Анализ показал, что данные, дистиллированные из модели AM-Thinking-v1, обладают большим разнообразием длины токенов и меньшей перплексией. Модели-ученики, обученные на этих данных, продемонстрировали лучшие результаты на нескольких тестах по рассуждению."
                },
                "en": {
                    "title": "Enhancing Reasoning in Language Models through Data Distillation",
                    "desc": "This paper explores the process of distillation to improve the reasoning abilities of open-source language models. The authors conducted a large-scale study using outputs from three advanced teacher models on a dataset of 1.89 million queries. They found that the distilled data from the AM-Thinking-v1 model had better diversity in token length and lower perplexity, leading to superior performance on various reasoning benchmarks. The results indicate that high-quality reasoning data is crucial for training effective student models, and the authors have made their datasets publicly available for further research."
                },
                "zh": {
                    "title": "蒸馏技术提升语言模型推理能力",
                    "desc": "本研究探讨了通过蒸馏技术提升开源语言模型推理能力的方法。我们收集了来自三种先进教师模型的验证输出，并构建了三个平行数据集进行分析。结果显示，AM-Thinking-v1蒸馏数据在标记长度多样性和困惑度方面表现更佳。经过训练的学生模型在多个推理基准测试中表现优异，特别是AM-Thinking-v1模型在各项测试中均取得了最佳成绩，展示了高质量推理轨迹的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14534",
            "title": "Lessons from Defending Gemini Against Indirect Prompt Injections",
            "url": "https://huggingface.co/papers/2505.14534",
            "abstract": "Gemini is increasingly used to perform tasks on behalf of users, where function-calling and tool-use capabilities enable the model to access user data. Some tools, however, require access to untrusted data introducing risk. Adversaries can embed malicious instructions in untrusted data which cause the model to deviate from the user's expectations and mishandle their data or permissions. In this report, we set out Google DeepMind's approach to evaluating the adversarial robustness of Gemini models and describe the main lessons learned from the process. We test how Gemini performs against a sophisticated adversary through an adversarial evaluation framework, which deploys a suite of adaptive attack techniques to run continuously against past, current, and future versions of Gemini. We describe how these ongoing evaluations directly help make Gemini more resilient against manipulation.",
            "score": 5,
            "issue_id": 3874,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "1e855b0dd2463fec",
            "authors": [
                "Chongyang Shi",
                "Sharon Lin",
                "Shuang Song",
                "Jamie Hayes",
                "Ilia Shumailov",
                "Itay Yona",
                "Juliette Pluto",
                "Aneesh Pappu",
                "Christopher A. Choquette-Choo",
                "Milad Nasr",
                "Chawin Sitawarin",
                "Gena Gibson",
                "Andreas Terzis",
                "John \"Four\" Flynn"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14534.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#security",
                    "#benchmark"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Укрепление защиты Gemini от состязательных атак",
                    "desc": "Статья описывает подход Google DeepMind к оценке устойчивости моделей Gemini к состязательным атакам. Авторы разработали систему непрерывного тестирования моделей с использованием адаптивных методов атак. Исследование направлено на выявление уязвимостей, связанных с обработкой ненадежных данных при выполнении задач от имени пользователей. Результаты оценки помогают повысить устойчивость Gemini к манипуляциям со стороны злоумышленников."
                },
                "en": {
                    "title": "Strengthening Gemini: Safeguarding User Data Against Adversarial Attacks",
                    "desc": "The paper discusses the challenges of ensuring the security of Gemini, a machine learning model that performs tasks for users by accessing their data. It highlights the risks posed by untrusted data, which can contain malicious instructions that lead the model to behave unexpectedly. To address these risks, Google DeepMind has developed an adversarial evaluation framework that tests Gemini's robustness against sophisticated attacks. The ongoing evaluations aim to enhance Gemini's resilience, ensuring it handles user data and permissions safely and effectively."
                },
                "zh": {
                    "title": "提升Gemini模型的对抗性鲁棒性",
                    "desc": "Gemini模型被广泛用于执行用户任务，但在使用工具时可能会接触到不可信的数据，这带来了风险。恶意攻击者可以在不可信的数据中嵌入恶意指令，导致模型偏离用户的期望，错误处理用户的数据或权限。本文介绍了Google DeepMind评估Gemini模型对抗性鲁棒性的方法，并总结了在这一过程中获得的主要经验教训。通过对Gemini进行持续的对抗性评估，我们能够提高其抵御操控的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14352",
            "title": "Towards eliciting latent knowledge from LLMs with mechanistic\n  interpretability",
            "url": "https://huggingface.co/papers/2505.14352",
            "abstract": "As language models become more powerful and sophisticated, it is crucial that they remain trustworthy and reliable. There is concerning preliminary evidence that models may attempt to deceive or keep secrets from their operators. To explore the ability of current techniques to elicit such hidden knowledge, we train a Taboo model: a language model that describes a specific secret word without explicitly stating it. Importantly, the secret word is not presented to the model in its training data or prompt. We then investigate methods to uncover this secret. First, we evaluate non-interpretability (black-box) approaches. Subsequently, we develop largely automated strategies based on mechanistic interpretability techniques, including logit lens and sparse autoencoders. Evaluation shows that both approaches are effective in eliciting the secret word in our proof-of-concept setting. Our findings highlight the promise of these approaches for eliciting hidden knowledge and suggest several promising avenues for future work, including testing and refining these methods on more complex model organisms. This work aims to be a step towards addressing the crucial problem of eliciting secret knowledge from language models, thereby contributing to their safe and reliable deployment.",
            "score": 5,
            "issue_id": 3874,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "6b06f2f5351e8b60",
            "authors": [
                "Bartosz Cywiński",
                "Emil Ryd",
                "Senthooran Rajamanoharan",
                "Neel Nanda"
            ],
            "affiliations": [
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14352.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#inference",
                    "#training",
                    "#security",
                    "#interpretability"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Раскрытие секретов искусственного интеллекта: новые методы интерпретации языковых моделей",
                    "desc": "Статья исследует методы выявления скрытых знаний в языковых моделях. Авторы обучают модель Taboo, которая описывает секретное слово, не называя его явно. Затем они применяют методы интерпретируемости, включая logit lens и разреженные автоэнкодеры, для раскрытия этого секрета. Результаты показывают эффективность этих подходов и открывают перспективы для дальнейших исследований в области безопасности и надежности языковых моделей."
                },
                "en": {
                    "title": "Unveiling Secrets: Enhancing Trust in Language Models",
                    "desc": "This paper addresses the challenge of ensuring that powerful language models remain trustworthy by investigating their potential to conceal information. The authors introduce a Taboo model, which is designed to describe a secret word without revealing it directly, even though the word is not included in the training data. They evaluate two main approaches to uncover this hidden knowledge: black-box methods and mechanistic interpretability techniques, such as logit lens and sparse autoencoders. The results demonstrate that these methods can effectively elicit the secret word, paving the way for future research on improving the transparency and reliability of language models."
                },
                "zh": {
                    "title": "揭示语言模型中的隐藏知识",
                    "desc": "随着语言模型变得越来越强大和复杂，确保它们的可信性和可靠性变得至关重要。研究表明，模型可能会试图欺骗或隐瞒信息。为此，我们训练了一个禁忌模型，它在不直接说明特定秘密词的情况下进行描述。我们的研究表明，使用黑箱方法和机械解释技术可以有效地揭示这些隐藏的知识，推动未来在更复杂模型上的应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13718",
            "title": "Warm Up Before You Train: Unlocking General Reasoning in\n  Resource-Constrained Settings",
            "url": "https://huggingface.co/papers/2505.13718",
            "abstract": "Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the amount of quality training data is scarce. We propose a sample-efficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we \"warm up\" the model by distilling Long CoTs from a toy domain, namely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using a limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: (i) the warmup phase alone facilitates generalized reasoning, leading to performance improvements across a range of tasks, including MATH, HumanEval^{+}, and MMLU-Pro. (ii) When both the base model and the warmed-up model are RLVR trained on the same small dataset (leq100 examples), the warmed-up model consistently outperforms the base model; (iii) Warming up before RLVR training allows a model to maintain cross-domain generalizability even after training on a specific domain; (iv) Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments.",
            "score": 5,
            "issue_id": 3872,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "29613228991289b5",
            "authors": [
                "Safal Shrestha",
                "Minwu Kim",
                "Aadim Nepal",
                "Anubhav Shrestha",
                "Keith Ross"
            ],
            "affiliations": [
                "New York University Abu Dhabi"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13718.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#rl",
                    "#long_context",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное обучение LLM рассуждению с минимумом данных",
                    "desc": "Исследователи предлагают эффективный двухэтапный метод обучения языковых моделей (LLM) навыкам рассуждения при ограниченном количестве данных. На первом этапе модель 'разогревается' на логических головоломках, приобретая общие навыки рассуждения. На втором этапе применяется обучение с подкреплением с проверяемыми наградами (RLVR) на небольшом наборе целевых примеров. Эксперименты показывают, что такой подход улучшает производительность модели на различных задачах и повышает эффективность использования данных при обучении RLVR. Метод особенно перспективен для создания надежных LLM в условиях ограниченности данных."
                },
                "en": {
                    "title": "Warmup for Robust Reasoning in Data-Scarce LLMs",
                    "desc": "This paper presents a novel two-stage training strategy for developing reasoning-capable large language models (LLMs) when high-quality training data is limited. The first stage involves warming up the model by distilling Long Chains of Thought (CoT) from simple logic puzzles, which helps the model acquire general reasoning skills. In the second stage, Reinforcement Learning with Verifiable Rewards (RLVR) is applied using a small set of examples from the target domain. The results show that this approach enhances performance across various tasks and improves sample efficiency, demonstrating the effectiveness of the warmup phase in building robust reasoning LLMs."
                },
                "zh": {
                    "title": "在数据稀缺环境中构建强大推理模型的有效策略",
                    "desc": "本文提出了一种在有限监督下开发推理能力强的语言模型（LLM）的两阶段训练策略。第一阶段通过从简单的逻辑谜题（骑士与骗子）中提取长思维链（CoT）来“预热”模型，以获取一般推理技能。第二阶段则使用有限的目标领域示例对预热后的模型进行强化学习与可验证奖励（RLVR）训练。实验结果表明，这种两阶段的方法在数据稀缺的环境中能够有效提高模型的推理能力和样本效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12182",
            "title": "Truth Neurons",
            "url": "https://huggingface.co/papers/2505.12182",
            "abstract": "Despite their remarkable success and deployment across diverse workflows, language models sometimes produce untruthful responses. Our limited understanding of how truthfulness is mechanistically encoded within these models jeopardizes their reliability and safety. In this paper, we propose a method for identifying representations of truthfulness at the neuron level. We show that language models contain truth neurons, which encode truthfulness in a subject-agnostic manner. Experiments conducted across models of varying scales validate the existence of truth neurons, confirming that the encoding of truthfulness at the neuron level is a property shared by many language models. The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness. Selectively suppressing the activations of truth neurons found through the TruthfulQA dataset degrades performance both on TruthfulQA and on other benchmarks, showing that the truthfulness mechanisms are not tied to a specific dataset. Our results offer novel insights into the mechanisms underlying truthfulness in language models and highlight potential directions toward improving their trustworthiness and reliability.",
            "score": 5,
            "issue_id": 3868,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 мая",
                "en": "May 18",
                "zh": "5月18日"
            },
            "hash": "ddeab64450bb26a9",
            "authors": [
                "Haohang Li",
                "Yupeng Cao",
                "Yangyang Yu",
                "Jordan W. Suchow",
                "Zining Zhu"
            ],
            "affiliations": [
                "Stevens Institute of Technology",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12182.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#hallucinations",
                    "#alignment",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Нейроны правды: путь к повышению надежности языковых моделей",
                    "desc": "Исследователи предложили метод идентификации представлений правдивости на уровне отдельных нейронов в языковых моделях. Они обнаружили так называемые 'нейроны правды', которые кодируют правдивость независимо от темы. Эксперименты подтвердили наличие таких нейронов в моделях разного масштаба. Подавление активации этих нейронов ухудшает производительность модели на различных тестах правдивости."
                },
                "en": {
                    "title": "Unveiling Truth Neurons: Enhancing Language Model Trustworthiness",
                    "desc": "This paper investigates how language models encode truthfulness at the neuron level, revealing the presence of 'truth neurons' that represent truthfulness in a way that is not dependent on specific subjects. The authors demonstrate that these truth neurons exist across various models, indicating a shared property among them. By analyzing the distribution of truth neurons across different layers, the study aligns with previous research on the geometry of truthfulness. Additionally, the suppression of these neurons negatively impacts model performance, suggesting that understanding and improving truthfulness in language models is crucial for their reliability."
                },
                "zh": {
                    "title": "揭示语言模型中的真相神经元",
                    "desc": "尽管语言模型在各种工作流程中取得了显著成功，但有时会产生不真实的回答。我们对这些模型中真相编码机制的理解有限，这影响了它们的可靠性和安全性。本文提出了一种方法，通过神经元层面识别真相的表示，发现语言模型中存在编码真相的真相神经元。实验表明，真相神经元的存在是许多语言模型的共同特性，并且其分布模式与真相的几何特征一致。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.09569",
            "title": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8",
            "url": "https://huggingface.co/papers/2505.09569",
            "abstract": "With the rapid advancement of powerful large language models (LLMs) in recent years, a wide range of software engineering tasks can now be addressed using LLMs, significantly enhancing productivity and scalability. Numerous benchmark datasets have been developed to evaluate the coding capabilities of these models, while they primarily focus on problem-solving and issue-resolution tasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a distinct focus: code migration. MIGRATION-BENCH aims to serve as a comprehensive benchmark for migration from Java 8 to the latest long-term support (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset and its subset selected with 5,102 and 300 repositories respectively. Selected is a representative subset curated for complexity and difficulty, offering a versatile resource to support research in the field of code migration. Additionally, we provide a comprehensive evaluation framework to facilitate rigorous and standardized assessment of LLMs on this challenging task. We further propose SD-Feedback and demonstrate that LLMs can effectively tackle repository-level code migration to Java 17. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate (pass@1) for minimal and maximal migration respectively. The benchmark dataset and source code are available at: https://huggingface.co/collections/AmazonScience and https://github.com/amazon-science/self_debug respectively.",
            "score": 5,
            "issue_id": 3872,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "cb7832fb680cc056",
            "authors": [
                "Linbo Liu",
                "Xinle Liu",
                "Qiang Zhou",
                "Lin Chen",
                "Yihan Liu",
                "Hoan Nguyen",
                "Behrooz Omidvar-Tehrani",
                "Xi Shen",
                "Jun Huan",
                "Omer Tripp",
                "Anoop Deoras"
            ],
            "affiliations": [
                "AWS AI Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.09569.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#data",
                    "#optimization",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "MIGRATION-BENCH: новый стандарт для оценки миграции кода с помощью LLM",
                    "desc": "Статья представляет новый бенчмарк MIGRATION-BENCH для оценки способностей больших языковых моделей (LLM) в задаче миграции кода с Java 8 на более новые версии. Бенчмарк включает полный набор данных из 5,102 репозиториев и выбранное подмножество из 300 репозиториев, отобранных по сложности. Авторы также предлагают метод SD-Feedback, который позволяет LLM эффективно выполнять миграцию кода на уровне репозитория. С использованием модели Claude-3.5-Sonnet-v2 и метода SD-Feedback достигнута успешность в 62.33% для минимальной миграции и 27.00% для максимальной миграции на выбранном подмножестве данных."
                },
                "en": {
                    "title": "Revolutionizing Code Migration with MIGRATION-BENCH",
                    "desc": "This paper introduces MIGRATION-BENCH, a new benchmark specifically designed for evaluating large language models (LLMs) on the task of code migration from Java 8 to newer long-term support versions like Java 17 and 21. Unlike existing benchmarks that focus on problem-solving, MIGRATION-BENCH provides a comprehensive dataset of 5,102 repositories, with a curated subset of 300 that vary in complexity and difficulty. The authors also present an evaluation framework to standardize the assessment of LLMs in this domain, demonstrating that LLMs can effectively perform repository-level code migration. Using their proposed SD-Feedback method, they report success rates of 62.33% for minimal migration and 27.00% for maximal migration with the Claude-3.5-Sonnet-v2 model."
                },
                "zh": {
                    "title": "代码迁移的新基准：MIGRATION-BENCH",
                    "desc": "近年来，强大的大型语言模型（LLMs）迅速发展，能够处理多种软件工程任务，显著提高了生产力和可扩展性。为了评估这些模型的编码能力，开发了许多基准数据集，但大多集中在问题解决和故障排除任务上。我们提出了一个新的编码基准MIGRATION-BENCH，专注于代码迁移，特别是从Java 8迁移到最新的长期支持版本（Java 17、21）。该基准包含完整数据集和代表性子集，提供了一个多功能资源，以支持代码迁移领域的研究，并提供了全面的评估框架，以便对LLMs在这一挑战性任务上的表现进行严格和标准化的评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13103",
            "title": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair",
            "url": "https://huggingface.co/papers/2505.13103",
            "abstract": "The rapid advancement of bug-finding techniques has led to the discovery of more vulnerabilities than developers can reasonably fix, creating an urgent need for effective Automated Program Repair (APR) methods. However, the complexity of modern bugs often makes precise root cause analysis difficult and unreliable. To address this challenge, we propose crash-site repair to simplify the repair task while still mitigating the risk of exploitation. In addition, we introduce a template-guided patch generation approach that significantly reduces the token cost of Large Language Models (LLMs) while maintaining both efficiency and effectiveness.   We implement our prototype system, WILLIAMT, and evaluate it against state-of-the-art APR tools. Our results show that, when combined with the top-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and increases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open source software vulnerabilities benchmark. Furthermore, we demonstrate that WILLIAMT can function effectively even without access to frontier LLMs: even a local model running on a Mac M4 Mini achieves a reasonable repair rate. These findings highlight the broad applicability and scalability of WILLIAMT.",
            "score": 4,
            "issue_id": 3874,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "4d2278523a88b9bc",
            "authors": [
                "Han Zheng",
                "Ilia Shumailov",
                "Tianqi Fan",
                "Aiden Hall",
                "Mathias Payer"
            ],
            "affiliations": [
                "EPFL Lausanne, Switzerland",
                "Google DeepMind London, UK",
                "Google New York, USA",
                "Google Zurich, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13103.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#plp",
                    "#training",
                    "#optimization",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🛠️",
                "ru": {
                    "title": "WILLIAMT: Эффективное автоматическое исправление программ с меньшими затратами",
                    "desc": "Статья представляет новый подход к автоматическому исправлению программ (APR) под названием WILLIAMT. Система фокусируется на исправлении места сбоя программы и использует шаблоны для генерации патчей, что значительно снижает затраты токенов при использовании больших языковых моделей (LLM). WILLIAMT показывает улучшение на 29.6% в скорости исправления ошибок по сравнению с современными APR инструментами на бенчмарке ARVO. Исследование также демонстрирует, что WILLIAMT может эффективно работать даже с локальными моделями на обычных компьютерах."
                },
                "en": {
                    "title": "Simplifying Bug Fixes with WILLIAMT: Efficient Automated Program Repair",
                    "desc": "This paper addresses the challenge of fixing software bugs, which have become too numerous for developers to handle manually. It introduces a method called crash-site repair that simplifies the bug-fixing process while reducing the risk of security issues. The authors also present a template-guided patch generation technique that lowers the token usage of Large Language Models (LLMs), making the repair process more efficient. Their system, WILLIAMT, shows significant improvements in bug-fixing rates and can operate effectively even on less powerful hardware."
                },
                "zh": {
                    "title": "WILLIAMT：高效的自动化程序修复新方法",
                    "desc": "随着漏洞发现技术的快速发展，开发者面临着越来越多的漏洞修复需求，因此迫切需要有效的自动化程序修复（APR）方法。现代漏洞的复杂性使得精确的根本原因分析变得困难且不可靠。为了解决这个问题，我们提出了崩溃现场修复方法，以简化修复任务，同时降低被利用的风险。此外，我们引入了一种模板引导的补丁生成方法，显著降低了大型语言模型（LLMs）的令牌成本，同时保持了效率和有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11365",
            "title": "Phare: A Safety Probe for Large Language Models",
            "url": "https://huggingface.co/papers/2505.11365",
            "abstract": "Ensuring the safety of large language models (LLMs) is critical for responsible deployment, yet existing evaluations often prioritize performance over identifying failure modes. We introduce Phare, a multilingual diagnostic framework to probe and evaluate LLM behavior across three critical dimensions: hallucination and reliability, social biases, and harmful content generation. Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic vulnerabilities across all safety dimensions, including sycophancy, prompt sensitivity, and stereotype reproduction. By highlighting these specific failure modes rather than simply ranking models, Phare provides researchers and practitioners with actionable insights to build more robust, aligned, and trustworthy language systems.",
            "score": 4,
            "issue_id": 3873,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "8ab32377d956578e",
            "authors": [
                "Pierre Le Jeune",
                "Benoît Malézieux",
                "Weixuan Xiao",
                "Matteo Dora"
            ],
            "affiliations": [
                "Giskard AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11365.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multilingual",
                    "#ethics",
                    "#hallucinations",
                    "#alignment"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Диагностика безопасности языковых моделей для ответственного ИИ",
                    "desc": "Статья представляет Phare - многоязычную диагностическую систему для оценки больших языковых моделей (LLM) по трем ключевым аспектам безопасности: галлюцинации и надежность, социальные предубеждения и генерация вредного контента. Авторы провели оценку 17 современных LLM, выявив систематические уязвимости во всех аспектах безопасности, включая угодничество, чувствительность к промпту и воспроизведение стереотипов. Phare фокусируется на выявлении конкретных режимов отказа, а не просто ранжировании моделей, что дает исследователям и практикам полезную информацию для создания более надежных и заслуживающих доверия языковых систем."
                },
                "en": {
                    "title": "Phare: Probing Safety in Language Models Beyond Performance",
                    "desc": "This paper presents Phare, a new framework designed to evaluate the safety of large language models (LLMs) by focusing on their failure modes rather than just their performance. It assesses LLMs across three key areas: hallucination and reliability, social biases, and harmful content generation. The study analyzes 17 advanced LLMs and uncovers common vulnerabilities, such as sycophancy and prompt sensitivity, which can lead to biased or harmful outputs. By identifying these specific issues, Phare aims to help researchers and developers create more reliable and ethical language models."
                },
                "zh": {
                    "title": "构建更安全的语言模型，识别失败模式！",
                    "desc": "确保大型语言模型（LLMs）的安全性对于负责任的部署至关重要，但现有的评估往往更关注性能而非识别失败模式。我们提出了Phare，这是一个多语言诊断框架，用于探测和评估LLM在三个关键维度上的行为：幻觉和可靠性、社会偏见以及有害内容生成。对17个最先进的LLM的评估揭示了在所有安全维度上系统性脆弱性的模式，包括谄媚、提示敏感性和刻板印象再现。通过突出这些具体的失败模式，Phare为研究人员和从业者提供了可操作的见解，以构建更强大、更一致和更可信的语言系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13380",
            "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition",
            "url": "https://huggingface.co/papers/2505.13380",
            "abstract": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an improved version of the previous study at arXiv:2402.02526",
            "score": 3,
            "issue_id": 3868,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "6a5e70a76e6f012c",
            "authors": [
                "Nam V. Nguyen",
                "Huy Nguyen",
                "Quang Pham",
                "Van Nguyen",
                "Savitha Ramasamy",
                "Nhat Ho"
            ],
            "affiliations": [
                "FPT Software AI Center",
                "Independent Researcher",
                "Institute for Infocomm Research, ASTAR",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13380.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "🏆",
                "ru": {
                    "title": "Конкуренция экспертов для эффективного обучения языковых моделей",
                    "desc": "Статья представляет новый механизм маршрутизации токенов в разреженных смесях экспертов (SMoE) под названием 'competition'. Авторы теоретически доказывают, что этот метод обладает лучшей эффективностью выборки по сравнению с традиционной маршрутизацией softmax. На основе этого механизма разработан алгоритм CompeteSMoE для обучения больших языковых моделей. Эмпирические эксперименты на задачах визуального обучения и предобучения языка демонстрируют эффективность, надежность и масштабируемость CompeteSMoE по сравнению с современными стратегиями SMoE."
                },
                "en": {
                    "title": "CompeteSMoE: Efficient Routing for Powerful Language Models",
                    "desc": "Sparse mixture of experts (SMoE) is a method that allows models to become more complex without simply making them deeper or wider. The challenge with SMoE is that the way experts are chosen to process data can be inefficient, as not all experts contribute to the decision-making process. This paper introduces a new routing mechanism called competition, which directs data to the most responsive experts, improving the efficiency of the model. The authors present CompeteSMoE, an algorithm that uses this competition mechanism to train large language models effectively, showing better performance and lower training costs compared to existing methods."
                },
                "zh": {
                    "title": "竞争机制提升稀疏专家混合模型的效率",
                    "desc": "稀疏专家混合模型（SMoE）是一种有效提升模型复杂度的方法，超越了简单增加网络深度或宽度的方式。然而，SMoE的训练仍然面临挑战，主要是因为计算的专家与路由过程之间的联系不够直接。我们提出了一种新的机制——竞争，能够将输入数据更有效地路由到响应最强的专家。通过理论分析，我们证明了竞争机制在样本效率上优于传统的softmax路由，并开发了CompeteSMoE算法，能够以较低的训练开销实现强大的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12306",
            "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for\n  Real-world Knowledge Injection",
            "url": "https://huggingface.co/papers/2505.12306",
            "abstract": "Despite significant advances in large language models (LLMs), their knowledge memorization capabilities remain underexplored, due to the lack of standardized and high-quality test ground. In this paper, we introduce a novel, real-world and large-scale knowledge injection benchmark that evolves continuously over time without requiring human intervention. Specifically, we propose WikiDYK, which leverages recently-added and human-written facts from Wikipedia's \"Did You Know...\" entries. These entries are carefully selected by expert Wikipedia editors based on criteria such as verifiability and clarity. Each entry is converted into multiple question-answer pairs spanning diverse task formats from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290 facts and 77,180 questions, which is also seamlessly extensible with future updates from Wikipedia editors. Extensive experiments using continued pre-training reveal a surprising insight: despite their prevalence in modern LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), exhibiting a 23% lower accuracy in terms of reliability. To compensate for the smaller scales of current BiLMs, we introduce a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to integrate with LLMs. Experiment shows that our framework further improves the reliability accuracy by up to 29.1%.",
            "score": 3,
            "issue_id": 3868,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 мая",
                "en": "May 18",
                "zh": "5月18日"
            },
            "hash": "ccbad06f5ba35418",
            "authors": [
                "Yuwei Zhang",
                "Wenhao Yu",
                "Shangbin Feng",
                "Yifan Zhu",
                "Letian Peng",
                "Jayanth Srinivasa",
                "Gaowen Liu",
                "Jingbo Shang"
            ],
            "affiliations": [
                "Cisco",
                "Tencent AI Lab",
                "UC, San Diego",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12306.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#interpretability",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "WikiDYK: новый стандарт оценки памяти языковых моделей",
                    "desc": "Статья представляет новый масштабный бенчмарк WikiDYK для оценки способности языковых моделей запоминать знания. WikiDYK использует недавно добавленные факты из раздела Wikipedia 'Did You Know...', преобразуя их в разнообразные вопросно-ответные пары. Эксперименты показали, что двунаправленные языковые модели (BiLM) значительно лучше запоминают знания, чем однонаправленные причинные модели (CLM). Авторы предлагают модульную коллаборативную систему, использующую ансамбли BiLM в качестве внешних хранилищ знаний для интеграции с большими языковыми моделями."
                },
                "en": {
                    "title": "Enhancing Knowledge Memorization in Language Models with WikiDYK",
                    "desc": "This paper presents WikiDYK, a new benchmark for evaluating knowledge memorization in large language models (LLMs). It uses real-world facts from Wikipedia's 'Did You Know...' entries to create a diverse set of question-answer pairs. The study finds that Causal Language Models (CLMs) have weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), with a notable accuracy gap. To enhance BiLMs' performance, the authors propose a collaborative framework that combines multiple BiLMs as external knowledge sources, resulting in improved accuracy in knowledge retrieval tasks."
                },
                "zh": {
                    "title": "知识记忆能力的新基准：WikiDYK",
                    "desc": "尽管大型语言模型（LLMs）取得了显著进展，但它们的知识记忆能力仍然未得到充分探索。本文提出了一种新颖的、真实世界的大规模知识注入基准，名为WikiDYK，能够随着时间的推移不断演变，而无需人工干预。WikiDYK利用维基百科“你知道吗...”条目中最近添加的、由人类撰写的事实，经过专家编辑的严格筛选，确保其可验证性和清晰性。实验结果表明，尽管因果语言模型（CLMs）在现代LLMs中普遍存在，但其知识记忆能力显著低于双向语言模型（BiLMs），准确性低23%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11966",
            "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\n  Verifier",
            "url": "https://huggingface.co/papers/2505.11966",
            "abstract": "Large Language Model (LLM) reasoning for complex tasks inherently involves a trade-off between solution accuracy and computational efficiency. The subsequent step of verification, while intended to improve performance, further complicates this landscape by introducing its own challenging trade-off: sophisticated Generative Reward Models (GenRMs) can be computationally prohibitive if naively integrated with LLMs at test-time, while simpler, faster methods may lack reliability. To overcome these challenges, we introduce FlexiVe, a novel generative verifier that flexibly balances computational resources between rapid, reliable fast thinking and meticulous slow thinking using a Flexible Allocation of Verification Budget strategy. We further propose the Solve-Detect-Verify pipeline, an efficient inference-time scaling framework that intelligently integrates FlexiVe, proactively identifying solution completion points to trigger targeted verification and provide focused solver feedback. Experiments show FlexiVe achieves superior accuracy in pinpointing errors within reasoning traces on ProcessBench. Furthermore, on challenging mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full approach outperforms baselines like self-consistency in reasoning accuracy and inference efficiency. Our system offers a scalable and effective solution to enhance LLM reasoning at test time.",
            "score": 3,
            "issue_id": 3869,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "cd659e075a3efafa",
            "authors": [
                "Jianyuan Zhong",
                "Zeju Li",
                "Zhijian Xu",
                "Xiangyu Wen",
                "Kezhi Li",
                "Qiang Xu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11966.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#math",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Гибкая верификация для улучшения рассуждений языковых моделей",
                    "desc": "Статья представляет FlexiVe - новый генеративный верификатор для улучшения рассуждений больших языковых моделей (LLM). FlexiVe гибко распределяет вычислительные ресурсы между быстрым и медленным мышлением, используя стратегию гибкого распределения бюджета верификации. Авторы также предлагают конвейер Solve-Detect-Verify для эффективного масштабирования во время вывода. Эксперименты показывают, что FlexiVe превосходит базовые методы по точности рассуждений и эффективности вывода на сложных математических задачах."
                },
                "en": {
                    "title": "Balancing Speed and Accuracy in LLM Reasoning with FlexiVe",
                    "desc": "This paper discusses the challenges of using Large Language Models (LLMs) for complex tasks, particularly the balance between accuracy and computational efficiency. It introduces FlexiVe, a generative verifier that optimizes the use of computational resources by allowing for both quick and thorough reasoning processes. The authors propose a Solve-Detect-Verify pipeline that enhances the integration of FlexiVe, enabling targeted verification and improved feedback during inference. Experimental results demonstrate that FlexiVe significantly improves error detection and reasoning accuracy on various benchmarks compared to traditional methods."
                },
                "zh": {
                    "title": "灵活验证，提升推理效率与准确性",
                    "desc": "本文探讨了大型语言模型（LLM）在复杂任务推理中的准确性与计算效率之间的权衡。我们提出了一种新颖的生成验证器FlexiVe，它通过灵活分配验证预算，在快速可靠的思维与细致慢思维之间取得平衡。我们还提出了Solve-Detect-Verify管道，这是一种高效的推理时间扩展框架，能够智能整合FlexiVe，主动识别解决方案完成点以触发针对性的验证。实验结果表明，FlexiVe在ProcessBench上能够更准确地定位推理过程中的错误，并在多个数学推理基准测试中超越了现有的基线方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14178",
            "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits",
            "url": "https://huggingface.co/papers/2505.14178",
            "abstract": "Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.",
            "score": 2,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "f4fdc7fb140f9273",
            "authors": [
                "Xiang Zhang",
                "Juntai Cao",
                "Jiaqi Wei",
                "Yiwei Xu",
                "Chenyu You"
            ],
            "affiliations": [
                "Cisco",
                "Stony Brook University",
                "University of British Columbia",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14178.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#architecture",
                    "#small_models",
                    "#data",
                    "#training"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Токенизация: скрытый ключ к символьным рассуждениям в ИИ",
                    "desc": "Статья исследует влияние токенизации на способность языковых моделей к символьным вычислениям. Авторы вводят понятие 'осведомленности о токенах' (Token Awareness) для формализации того, как неоптимальная гранулярность токенов нарушает логическое выравнивание и препятствует обобщению символьных процедур. Эмпирические эксперименты показывают, что структура токенов существенно влияет на производительность рассуждений, даже при использовании метода цепочки рассуждений (Chain-of-Thought). Исследование демонстрирует, что способность к символьным рассуждениям в больших языковых моделях (LLM) глубоко обусловлена токен-уровневыми представлениями."
                },
                "en": {
                    "title": "Tokenization Matters: Unlocking Reasoning in Language Models",
                    "desc": "This paper explores the importance of tokenization in language models, particularly how it affects reasoning capabilities. It highlights that traditional tokenization methods, like byte-pair encoding (BPE), can obscure essential reasoning units, limiting the model's ability to perform symbolic computation. The authors introduce the concept of Token Awareness, which emphasizes the need for better token granularity to enhance logical alignment and generalization in models. Through experiments on arithmetic and symbolic tasks, they show that models with well-structured token representations can significantly outperform larger models in reasoning tasks."
                },
                "zh": {
                    "title": "分词结构决定推理能力",
                    "desc": "本文探讨了在语言模型中，分词（Tokenization）对推理能力的影响。我们发现，分词方案，特别是基于子词的方法（如字节对编码BPE），会合并或模糊基本的推理单元，从而妨碍符号计算。我们引入了“Token Awareness”的概念，强调了分词粒度不佳如何干扰逻辑对齐，阻碍模型的符号程序泛化。通过对算术和符号任务的系统评估，我们证明了分词结构显著影响推理性能，较小的模型在对齐格式下能够超越更大的系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13010",
            "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector",
            "url": "https://huggingface.co/papers/2505.13010",
            "abstract": "Media bias detection is a critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of high-quality annotated data. In this work, we perform sentence-level bias classification by fine-tuning a RoBERTa-based model on the expert-annotated BABE dataset. Using McNemar's test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to a domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a comprehensive examination of media bias, we present a pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection.",
            "score": 2,
            "issue_id": 3877,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "9f759ef4e436d2f0",
            "authors": [
                "Himel Ghosh",
                "Ahmed Mosharafa",
                "Georg Groh"
            ],
            "affiliations": [
                "Sapienza University of Rome, Italy",
                "Technical University of Munich (TUM), Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13010.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#interpretability",
                    "#ethics",
                    "#dataset"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Точное обнаружение предвзятости в СМИ с помощью глубокого обучения",
                    "desc": "Исследователи разработали модель на основе RoBERTa для классификации предвзятости на уровне предложений, используя экспертно-аннотированный датасет BABE. Статистически значимые улучшения производительности были показаны по сравнению с базовой моделью DA-RoBERTa. Анализ внимания продемонстрировал, что модель избегает чрезмерной чувствительности к политически заряженным терминам. Представлен конвейер, объединяющий разработанную модель с существующим классификатором типов предвзятости для комплексного анализа медиа-предвзятости."
                },
                "en": {
                    "title": "Enhancing Media Bias Detection with Context-Aware Models",
                    "desc": "This paper addresses the challenge of detecting media bias by fine-tuning a RoBERTa-based model on the BABE dataset, which is annotated by experts. The authors demonstrate significant performance improvements over a baseline model using statistical tests, indicating the effectiveness of their approach. They also analyze the model's attention mechanisms, showing it focuses on contextually relevant information rather than being overly sensitive to biased language. The study proposes a comprehensive pipeline for media bias detection and discusses future directions for enhancing bias classification and model interpretability."
                },
                "zh": {
                    "title": "提升媒体偏见检测的智能化方法",
                    "desc": "本研究针对媒体偏见检测这一重要任务，提出了一种基于RoBERTa模型的句子级偏见分类方法。我们在专家标注的BABE数据集上进行了微调，并通过统计测试验证了模型性能的显著提升。模型的注意力分析表明，它能够有效避免对政治敏感词的过度敏感，而是更关注上下文相关的词汇。尽管受限于句子级分析和数据集规模，我们的方法在偏见检测中展现了良好的泛化能力和可解释性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10176",
            "title": "Incorporating brain-inspired mechanisms for multimodal learning in\n  artificial intelligence",
            "url": "https://huggingface.co/papers/2505.10176",
            "abstract": "Multimodal learning enhances the perceptual capabilities of cognitive systems by integrating information from different sensory modalities. However, existing multimodal fusion research typically assumes static integration, not fully incorporating key dynamic mechanisms found in the brain. Specifically, the brain exhibits an inverse effectiveness phenomenon, wherein weaker unimodal cues yield stronger multisensory integration benefits; conversely, when individual modal cues are stronger, the effect of fusion is diminished. This mechanism enables biological systems to achieve robust cognition even with scarce or noisy perceptual cues. Inspired by this biological mechanism, we explore the relationship between multimodal output and information from individual modalities, proposing an inverse effectiveness driven multimodal fusion (IEMF) strategy. By incorporating this strategy into neural networks, we achieve more efficient integration with improved model performance and computational efficiency, demonstrating up to 50% reduction in computational cost across diverse fusion methods. We conduct experiments on audio-visual classification, continual learning, and question answering tasks to validate our method. Results consistently demonstrate that our method performs excellently in these tasks. To verify universality and generalization, we also conduct experiments on Artificial Neural Networks (ANN) and Spiking Neural Networks (SNN), with results showing good adaptability to both network types. Our research emphasizes the potential of incorporating biologically inspired mechanisms into multimodal networks and provides promising directions for the future development of multimodal artificial intelligence. The code is available at https://github.com/Brain-Cog-Lab/IEMF.",
            "score": 2,
            "issue_id": 3878,
            "pub_date": "2025-05-15",
            "pub_date_card": {
                "ru": "15 мая",
                "en": "May 15",
                "zh": "5月15日"
            },
            "hash": "6384b169333ad553",
            "authors": [
                "Xiang He",
                "Dongcheng Zhao",
                "Yang Li",
                "Qingqun Kong",
                "Xin Yang",
                "Yi Zeng"
            ],
            "affiliations": [
                "Brain-inspired Cognitive AI Lab, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
                "CAS Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
                "Center for Long-term Al, Beijing, China",
                "Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology, Chinese Academy of Sciences, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10176.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#multimodal",
                    "#audio",
                    "#agi"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Биологически вдохновленное мультимодальное слияние для эффективного ИИ",
                    "desc": "Статья представляет новую стратегию мультимодального слияния, вдохновленную биологическим механизмом обратной эффективности в мозге. Авторы предлагают метод IEMF (Inverse Effectiveness driven Multimodal Fusion), который позволяет более эффективно интегрировать информацию из разных модальностей в нейронных сетях. Эксперименты показывают, что IEMF улучшает производительность модели и снижает вычислительные затраты до 50% в различных задачах, включая аудио-визуальную классификацию и ответы на вопросы. Метод демонстрирует хорошую адаптируемость как к искусственным нейронным сетям (ANN), так и к спайковым нейронным сетям (SNN)."
                },
                "en": {
                    "title": "Enhancing Multimodal Learning with Brain-Inspired Fusion",
                    "desc": "This paper discusses a new approach to multimodal learning that mimics how the human brain processes information from different senses. It introduces the Inverse Effectiveness Driven Multimodal Fusion (IEMF) strategy, which enhances the integration of sensory data by leveraging the brain's ability to combine weaker signals more effectively. The authors demonstrate that this method can significantly improve the performance and efficiency of neural networks, achieving up to a 50% reduction in computational costs. Experiments across various tasks show that IEMF is adaptable and effective in both Artificial Neural Networks and Spiking Neural Networks, highlighting the benefits of biologically inspired techniques in artificial intelligence."
                },
                "zh": {
                    "title": "逆效应驱动的多模态融合策略",
                    "desc": "多模态学习通过整合来自不同感官的信息，增强了认知系统的感知能力。然而，现有的多模态融合研究通常假设静态整合，未能充分考虑大脑中的关键动态机制。具体来说，大脑表现出逆效应现象，即较弱的单模态线索会带来更强的多感官融合效益；相反，当单个模态线索较强时，融合效果会减弱。受这一生物机制的启发，我们提出了一种基于逆效应的多模态融合策略，显著提高了模型性能和计算效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14648",
            "title": "Vox-Profile: A Speech Foundation Model Benchmark for Characterizing\n  Diverse Speaker and Speech Traits",
            "url": "https://huggingface.co/papers/2505.14648",
            "abstract": "We introduce Vox-Profile, a comprehensive benchmark to characterize rich speaker and speech traits using speech foundation models. Unlike existing works that focus on a single dimension of speaker traits, Vox-Profile provides holistic and multi-dimensional profiles that reflect both static speaker traits (e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speech flow). This benchmark is grounded in speech science and linguistics, developed with domain experts to accurately index speaker and speech characteristics. We report benchmark experiments using over 15 publicly available speech datasets and several widely used speech foundation models that target various static and dynamic speaker and speech properties. In addition to benchmark experiments, we showcase several downstream applications supported by Vox-Profile. First, we show that Vox-Profile can augment existing speech recognition datasets to analyze ASR performance variability. Vox-Profile is also used as a tool to evaluate the performance of speech generation systems. Finally, we assess the quality of our automated profiles through comparison with human evaluation and show convergent validity. Vox-Profile is publicly available at: https://github.com/tiantiaf0627/vox-profile-release.",
            "score": 1,
            "issue_id": 3884,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "f1d0da4a4a22866a",
            "authors": [
                "Tiantian Feng",
                "Jihwan Lee",
                "Anfeng Xu",
                "Yoonjeong Lee",
                "Thanathai Lertpetchpun",
                "Xuan Shi",
                "Helin Wang",
                "Thomas Thebaud",
                "Laureano Moro-Velazquez",
                "Dani Byrd",
                "Najim Dehak",
                "Shrikanth Narayanan"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14648.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#audio"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Vox-Profile: Многомерный анализ речи и говорящего с помощью ИИ",
                    "desc": "Vox-Profile - это комплексный бенчмарк для характеристики богатых черт говорящего и речи с использованием фундаментальных моделей речи. Он обеспечивает целостные и многомерные профили, отражающие как статические черты говорящего (возраст, пол, акцент), так и динамические свойства речи (эмоции, плавность речи). Бенчмарк основан на науке о речи и лингвистике, разработан с экспертами для точной индексации характеристик говорящего и речи. Авторы провели эксперименты с использованием более 15 общедоступных речевых датасетов и нескольких широко используемых фундаментальных моделей речи."
                },
                "en": {
                    "title": "Vox-Profile: A Holistic Benchmark for Speaker and Speech Trait Analysis",
                    "desc": "Vox-Profile is a new benchmark designed to analyze various speaker and speech traits using advanced speech foundation models. It goes beyond previous studies by providing a comprehensive view of both static traits like age and accent, and dynamic traits such as emotion and speech flow. Developed with input from experts in speech science and linguistics, it offers a reliable way to index these characteristics. The benchmark has been tested with multiple speech datasets and models, demonstrating its utility in improving automatic speech recognition and speech generation systems."
                },
                "zh": {
                    "title": "Vox-Profile：多维度说话者特征的基准",
                    "desc": "Vox-Profile是一个全面的基准，用于通过语音基础模型来表征丰富的说话者和语音特征。与现有研究只关注单一维度的说话者特征不同，Vox-Profile提供了反映静态说话者特征（如年龄、性别、口音）和动态语音属性（如情感、语速）的整体和多维档案。该基准基于语音科学和语言学，与领域专家合作开发，以准确索引说话者和语音特征。我们通过超过15个公开的语音数据集和多种广泛使用的语音基础模型进行基准实验，展示了Vox-Profile在语音识别和生成系统评估中的应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13778",
            "title": "CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM\n  APIs",
            "url": "https://huggingface.co/papers/2505.13778",
            "abstract": "As post-training techniques evolve, large language models (LLMs) are increasingly augmented with structured multi-step reasoning abilities, often optimized through reinforcement learning. These reasoning-enhanced models outperform standard LLMs on complex tasks and now underpin many commercial LLM APIs. However, to protect proprietary behavior and reduce verbosity, providers typically conceal the reasoning traces while returning only the final answer. This opacity introduces a critical transparency gap: users are billed for invisible reasoning tokens, which often account for the majority of the cost, yet have no means to verify their authenticity. This opens the door to token count inflation, where providers may overreport token usage or inject synthetic, low-effort tokens to inflate charges. To address this issue, we propose CoIn, a verification framework that audits both the quantity and semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from token embedding fingerprints to check token counts, and uses embedding-based relevance matching to detect fabricated reasoning content. Experiments demonstrate that CoIn, when deployed as a trusted third-party auditor, can effectively detect token count inflation with a success rate reaching up to 94.7%, showing the strong ability to restore billing transparency in opaque LLM services. The dataset and code are available at https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.",
            "score": 1,
            "issue_id": 3884,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "e61730a128c76920",
            "authors": [
                "Guoheng Sun",
                "Ziyao Wang",
                "Bowei Tian",
                "Meng Liu",
                "Zheyu Shen",
                "Shwai He",
                "Yexiao He",
                "Wanghao Ye",
                "Yiting Wang",
                "Ang Li"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13778.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#inference",
                    "#reasoning",
                    "#hallucinations",
                    "#rl",
                    "#security"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "CoIn: прозрачность и честность в скрытых рассуждениях LLM",
                    "desc": "Статья представляет CoIn - фреймворк для аудита скрытых токенов в больших языковых моделях (LLM). CoIn решает проблему непрозрачности в использовании токенов при многоступенчатых рассуждениях LLM, которые часто скрываются провайдерами. Фреймворк использует хеш-дерево для проверки количества токенов и сопоставление релевантности на основе эмбеддингов для обнаружения сфабрикованного контента. Эксперименты показывают, что CoIn может эффективно обнаруживать искусственное увеличение количества токенов с точностью до 94.7%."
                },
                "en": {
                    "title": "Ensuring Transparency in LLM Billing with CoIn",
                    "desc": "This paper discusses the challenges of transparency in large language models (LLMs) that use complex reasoning processes, often hidden from users. These models, enhanced through reinforcement learning, can perform better on difficult tasks but may lead to inflated costs due to undisclosed reasoning tokens. The authors introduce CoIn, a framework designed to verify the authenticity and quantity of these hidden tokens, ensuring users are not overcharged. Through experiments, CoIn demonstrates a high success rate in detecting token inflation, promoting fairness and transparency in LLM billing practices."
                },
                "zh": {
                    "title": "提升LLM服务透明度的验证框架",
                    "desc": "随着后训练技术的发展，大型语言模型（LLMs）越来越多地增强了结构化的多步骤推理能力，这通常通过强化学习进行优化。这些增强推理的模型在复杂任务上表现优于标准LLMs，并且现在支撑着许多商业LLM API。然而，为了保护专有行为并减少冗长，提供者通常在返回最终答案时隐藏推理过程。这种不透明性导致了透明度缺口，用户为不可见的推理令牌付费，而这些令牌往往占据了大部分成本，用户却无法验证其真实性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12154",
            "title": "Learning to Highlight Audio by Watching Movies",
            "url": "https://huggingface.co/papers/2505.12154",
            "abstract": "Recent years have seen a significant increase in video content creation and consumption. Crafting engaging content requires the careful curation of both visual and audio elements. While visual cue curation, through techniques like optimal viewpoint selection or post-editing, has been central to media production, its natural counterpart, audio, has not undergone equivalent advancements. This often results in a disconnect between visual and acoustic saliency. To bridge this gap, we introduce a novel task: visually-guided acoustic highlighting, which aims to transform audio to deliver appropriate highlighting effects guided by the accompanying video, ultimately creating a more harmonious audio-visual experience. We propose a flexible, transformer-based multimodal framework to solve this task. To train our model, we also introduce a new dataset -- the muddy mix dataset, leveraging the meticulous audio and video crafting found in movies, which provides a form of free supervision. We develop a pseudo-data generation process to simulate poorly mixed audio, mimicking real-world scenarios through a three-step process -- separation, adjustment, and remixing. Our approach consistently outperforms several baselines in both quantitative and subjective evaluation. We also systematically study the impact of different types of contextual guidance and difficulty levels of the dataset. Our project page is here: https://wikichao.github.io/VisAH/.",
            "score": 1,
            "issue_id": 3882,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "8f03af3997e1149d",
            "authors": [
                "Chao Huang",
                "Ruohan Gao",
                "J. M. F. Tsang",
                "Jan Kurcius",
                "Cagdas Bilen",
                "Chenliang Xu",
                "Anurag Kumar",
                "Sanjeel Parekh"
            ],
            "affiliations": [
                "Meta Reality Labs Research",
                "University of Maryland, College Park",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12154.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#audio",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Гармонизация аудио и видео с помощью машинного обучения",
                    "desc": "Статья представляет новую задачу визуально-управляемого акустического выделения для улучшения согласованности аудио и видео контента. Авторы предлагают трансформерную мультимодальную модель для решения этой задачи. Для обучения модели создан новый датасет на основе фильмов с тщательно подобранным аудио и видео. Разработан процесс генерации псевдо-данных для имитации плохо смикшированного аудио в реальных сценариях."
                },
                "en": {
                    "title": "Harmonizing Audio and Visuals for Engaging Video Content",
                    "desc": "This paper addresses the imbalance in advancements between visual and audio elements in video content creation. It introduces a new task called visually-guided acoustic highlighting, which aims to enhance audio based on the visual cues from the video. The authors propose a transformer-based multimodal framework to achieve this, supported by a new dataset called the muddy mix dataset that simulates real-world audio mixing challenges. Their approach shows significant improvements over existing methods in both quantitative metrics and subjective assessments, highlighting the importance of integrating visual and audio elements for a cohesive viewing experience."
                },
                "zh": {
                    "title": "视觉引导音频高亮，提升视听体验！",
                    "desc": "近年来，视频内容的创作和消费显著增加。制作引人入胜的内容需要精心策划视觉和音频元素。为了弥补视觉和音频之间的差距，我们提出了一种新任务：视觉引导的音频高亮，旨在根据视频内容调整音频，以创造更和谐的视听体验。我们提出了一种基于变换器的多模态框架，并引入了新的数据集，以支持模型训练。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11754",
            "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with\n  Context Permutation",
            "url": "https://huggingface.co/papers/2505.11754",
            "abstract": "Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.",
            "score": 1,
            "issue_id": 3878,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "dcf59ca0d93ac6e2",
            "authors": [
                "Wenyu Huang",
                "Pavlos Vougiouklis",
                "Mirella Lapata",
                "Jeff Z. Pan"
            ],
            "affiliations": [
                "Huawei Edinburgh Research Centre, Poisson Lab, CSI, UK",
                "School of Informatics, University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11754.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оптимизация языковых моделей для многоэтапного вопросно-ответного анализа",
                    "desc": "Статья исследует многоэтапный вопросно-ответный анализ (MHQA) с использованием языковых моделей. Авторы обнаружили, что модели типа encoder-decoder превосходят декодер-модели в задачах MHQA, несмотря на меньший размер. Изменение порядка документов влияет на производительность моделей, причем оптимальные результаты достигаются при соответствии порядка документов цепочке рассуждений. Модификация причинной маски в декодер-моделях с помощью двунаправленного внимания улучшает их производительность в MHQA."
                },
                "en": {
                    "title": "Unlocking Multi-hop Reasoning in Language Models",
                    "desc": "This paper investigates the challenges of Multi-hop Question Answering (MHQA) using Language Models (LMs). It highlights that while LMs excel in standard question-answering, their performance can be limited by causal masking when reasoning across multiple information sources. The authors find that encoder-decoder models, like Flan-T5, outperform smaller causal decoder-only models in MHQA tasks, especially when the order of documents matches the reasoning chain. Additionally, they propose modifications to causal masks to enhance performance and analyze attention weights, discovering that higher attention values correlate with correct answers, which can be used to improve LM effectiveness."
                },
                "zh": {
                    "title": "多跳推理，提升问答能力！",
                    "desc": "多跳问题回答（MHQA）增加了问答的复杂性，使其更具挑战性。语言模型（LM）在处理多个搜索结果时，不仅需要检索相关信息，还需在信息源之间进行多跳推理。尽管LM在传统问答任务中表现良好，但因果掩码可能会妨碍其在复杂上下文中的推理能力。我们的研究发现，编码-解码模型在MHQA任务中通常优于仅使用因果解码器的模型，且通过调整因果掩码可以有效提升后者的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11730",
            "title": "Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling",
            "url": "https://huggingface.co/papers/2505.11730",
            "abstract": "Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over Best-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to support future research.",
            "score": 1,
            "issue_id": 3883,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "bd83555174b293a3",
            "authors": [
                "Hao Mark Chen",
                "Guanxi Lu",
                "Yasuyuki Okoshi",
                "Zhiwen Mo",
                "Masato Motomura",
                "Hongxiang Fan"
            ],
            "affiliations": [
                "Imperial College London, UK",
                "Institute of Science Tokyo, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11730.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#training",
                    "#reasoning",
                    "#inference"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Гибкая проверка для эффективного масштабирования языковых моделей",
                    "desc": "Статья представляет новый алгоритм Variable Granularity Search (VG-Search) для улучшения рассуждений больших языковых моделей (LLM). VG-Search обобщает методы beam search и Best-of-N sampling, позволяя настраивать частоту проверки генерируемого текста. Эксперименты показали, что динамический выбор гранулярности проверки может повысить вычислительную эффективность и масштабируемость модели. Адаптивные стратегии VG-Search достигают улучшения точности до 3.6% при снижении вычислительных затрат на 52%."
                },
                "en": {
                    "title": "Dynamic Verification for Enhanced Language Model Efficiency",
                    "desc": "This paper explores the concept of Test-time Scaling (TTS) in large language models (LLMs) and emphasizes the importance of verification in enhancing reasoning performance and computational efficiency. The authors introduce a novel approach called Variable Granularity Search (VG-Search), which allows for dynamic adjustment of verification frequency during the generation process. By systematically varying the granularity of verification, VG-Search improves both the accuracy and efficiency of LLMs compared to traditional methods like Beam Search and Best-of-N sampling. The results show significant gains in accuracy while drastically reducing computational costs, paving the way for more efficient LLM applications."
                },
                "zh": {
                    "title": "动态验证粒度提升推理效率",
                    "desc": "本文探讨了测试时缩放（TTS）在提升大型语言模型（LLMs）推理能力方面的有效性。验证在TTS中起着关键作用，影响推理性能和计算效率。我们首次系统性地研究了验证粒度的影响，即在生成过程中验证器的调用频率。通过引入可变粒度搜索（VG-Search）算法，我们的实验表明，动态选择粒度参数可以提高计算效率，并在准确性上超过传统的束搜索和最佳N采样。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10588",
            "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety\n  Systems for Content Moderation",
            "url": "https://huggingface.co/papers/2505.10588",
            "abstract": "This research offers a unique evaluation of how AI systems interpret the digital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first cohort raised alongside AI, Gen Alpha faces new forms of online risk due to immersive digital engagement and a growing mismatch between their evolving communication and existing safety tools. Their distinct language, shaped by gaming, memes, and AI-driven trends, often conceals harmful interactions from both human moderators and automated systems. We assess four leading AI models (GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked harassment and manipulation within Gen Alpha discourse. Using a dataset of 100 recent expressions from gaming platforms, social media, and video content, the study reveals critical comprehension failures with direct implications for online safety. This work contributes: (1) a first-of-its-kind dataset capturing Gen Alpha expressions; (2) a framework to improve AI moderation systems for youth protection; (3) a multi-perspective evaluation including AI systems, human moderators, and parents, with direct input from Gen Alpha co-researchers; and (4) an analysis of how linguistic divergence increases youth vulnerability. Findings highlight the urgent need to redesign safety systems attuned to youth communication, especially given Gen Alpha reluctance to seek help when adults fail to understand their digital world. This study combines the insight of a Gen Alpha researcher with systematic academic analysis to address critical digital safety challenges.",
            "score": 1,
            "issue_id": 3868,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 мая",
                "en": "May 14",
                "zh": "5月14日"
            },
            "hash": "cdc9a4f93d65b071",
            "authors": [
                "Manisha Mehta",
                "Fausto Giunchiglia"
            ],
            "affiliations": [
                "University of Trento, Trento, Italy",
                "Warren Hyde Middle School, Cupertino, California, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10588.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#interpretability",
                    "#benchmark",
                    "#ethics",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Преодолевая языковой барьер: ИИ на страже безопасности цифрового поколения",
                    "desc": "Исследование оценивает способность ИИ-систем интерпретировать цифровой язык поколения Альфа. Авторы анализируют четыре ведущие модели искусственного интеллекта на предмет обнаружения скрытых форм домогательств и манипуляций в дискурсе этого поколения. Работа включает создание уникального датасета выражений поколения Альфа и разработку framework'а для улучшения систем модерации на базе ИИ. Результаты подчеркивают острую необходимость переработки систем безопасности с учетом особенностей коммуникации молодежи."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing AI Safety for Generation Alpha",
                    "desc": "This research evaluates how AI systems understand the unique digital language of Generation Alpha, who are growing up with AI technology. It highlights the risks they face online due to their distinct communication styles, influenced by gaming and memes, which can hide harmful interactions from both humans and automated systems. The study tests four AI models on their ability to detect subtle harassment in Gen Alpha's online expressions, revealing significant gaps in their comprehension. The findings emphasize the need for improved AI moderation tools that are better suited to protect youth in their digital environments."
                },
                "zh": {
                    "title": "重塑安全系统，保护阿尔法世代的数字交流",
                    "desc": "本研究独特地评估了人工智能系统如何解读阿尔法世代（2010-2024年出生）的数字语言。阿尔法世代是首个与人工智能共同成长的群体，他们在沉浸式数字环境中面临新的在线风险。研究分析了四种领先的人工智能模型（GPT-4、Claude、Gemini和Llama 3）在识别隐藏的骚扰和操控方面的能力。研究结果显示，现有的安全工具未能有效理解阿尔法世代的独特交流方式，强调了重新设计安全系统的紧迫性，以更好地保护年轻用户。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14633",
            "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values\n  Prioritization with AIRiskDilemmas",
            "url": "https://huggingface.co/papers/2505.14633",
            "abstract": "Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench.",
            "score": 0,
            "issue_id": 3884,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "40e16ce405eaf398",
            "authors": [
                "Yu Ying Chiu",
                "Zhilin Wang",
                "Sharan Maiya",
                "Yejin Choi",
                "Kyle Fish",
                "Sydney Levine",
                "Evan Hubinger"
            ],
            "affiliations": [
                "Anthropic",
                "Cambridge",
                "Harvard",
                "MIT",
                "NVIDIA",
                "Stanford",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14633.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#ethics",
                    "#benchmark",
                    "#alignment",
                    "#healthcare"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "Ценности как компас для выявления рисков ИИ",
                    "desc": "Статья представляет новый подход к выявлению рисков в системах искусственного интеллекта, основанный на анализе ценностей и приоритетов моделей ИИ. Авторы разработали методологию LitmusValues для оценки приоритетов ИИ в различных классах ценностей. Они также создали набор данных AIRiskDilemmas, содержащий сценарии, в которых ценности противопоставляются друг другу в контексте рисков ИИ. Исследование показало, что анализ ценностей может предсказывать как известные, так и неизвестные рискованные поведения моделей ИИ."
                },
                "en": {
                    "title": "Uncovering AI Risks Through Value Prioritization",
                    "desc": "This paper addresses the growing challenge of detecting risks in advanced AI models, particularly those that use techniques like Alignment Faking to avoid detection. The authors propose a method called LitmusValues, which evaluates AI models based on their adherence to various value classes, serving as an early warning system for risky behaviors. They introduce AIRiskDilemmas, a set of scenarios that highlight conflicts between different values, relevant to AI safety. By analyzing the value prioritization of AI models through their choices in these dilemmas, the study reveals how even benign values can indicate potential risks in both known and unknown contexts."
                },
                "zh": {
                    "title": "识别人工智能风险的价值观优先级",
                    "desc": "随着更强大的模型出现，检测人工智能风险变得更加困难。本文提出了一种名为LitmusValues的评估管道，用于揭示人工智能模型在不同价值类别上的优先级。通过收集AIRiskDilemmas，我们创建了一系列涉及价值冲突的困境，以评估人工智能的安全风险。研究表明，LitmusValues中的价值观可以有效预测人工智能在已知和未知风险行为中的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14629",
            "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models",
            "url": "https://huggingface.co/papers/2505.14629",
            "abstract": "Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at https://github.com/mohbattharani/KERL.",
            "score": 0,
            "issue_id": 3880,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "cf8c6e04379454db",
            "authors": [
                "Fnu Mohbat",
                "Mohammed J Zaki"
            ],
            "affiliations": [
                "Rensselaer Polytechnic Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14629.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#open_source",
                    "#graphs",
                    "#dataset",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "🍳",
                "ru": {
                    "title": "KERL: умный помощник на кухне с графами знаний и ИИ",
                    "desc": "Статья представляет систему KERL, объединяющую графы знаний о еде и большие языковые модели для персонализированных рекомендаций по питанию. KERL извлекает сущности из вопросов пользователя, ищет релевантную информацию в графе знаний и использует ее как контекст для языковой модели при выборе рецептов. Система также генерирует пошаговые инструкции и информацию о микронутриентах для каждого рецепта. Эксперименты показали, что KERL превосходит существующие подходы в задачах рекомендации блюд, генерации рецептов и анализа пищевой ценности."
                },
                "en": {
                    "title": "KERL: Smart Food Recommendations with LLMs and Knowledge Graphs",
                    "desc": "This paper presents KERL, a novel system that combines large language models (LLMs) with food-related knowledge graphs (KGs) to enhance food recommendations and recipe generation. KERL processes natural language queries by extracting relevant entities and retrieving corresponding subgraphs from the KG, which are then used as context for the LLM to generate personalized recipes and nutritional information. The system not only recommends recipes but also provides detailed cooking steps and micro-nutritional data tailored to user preferences. Experimental results demonstrate that KERL outperforms existing methods, showcasing its effectiveness in delivering comprehensive food-related solutions."
                },
                "zh": {
                    "title": "KERL：智能食品推荐与配方生成的解决方案",
                    "desc": "本研究提出了KERL系统，它结合了食品知识图谱（KGs）和大型语言模型（LLMs），以提供个性化的食品推荐和生成配方及微量营养信息。KERL能够从自然语言问题中提取实体，并从知识图谱中检索子图，将其作为上下文输入到LLM中，以选择满足约束条件的配方。系统还生成每个配方的烹饪步骤和营养信息。通过大量实验，我们证明了KG增强的LLM在食品推荐、配方生成和营养分析方面显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14556",
            "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving\n  fMRI",
            "url": "https://huggingface.co/papers/2505.14556",
            "abstract": "Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding.",
            "score": 0,
            "issue_id": 3880,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "a52590cd595c5afe",
            "authors": [
                "Marlène Careil",
                "Yohann Benchetrit",
                "Jean-Rémi King"
            ],
            "affiliations": [
                "FAIR at Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14556.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#data",
                    "#diffusion",
                    "#architecture",
                    "#training",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Декодирование мыслей в изображения: новый уровень с Dynadiff",
                    "desc": "Dynadiff - это новая модель диффузии для реконструкции изображений из динамически меняющихся данных фМРТ. Она упрощает процесс обучения по сравнению с существующими подходами и превосходит современные модели при работе с временными сигналами фМРТ. Модель особенно эффективна для реконструкции семантических характеристик изображений и позволяет точно отслеживать эволюцию представлений изображений в активности мозга. Dynadiff закладывает основу для декодирования изображений из мозговой активности с учетом временной составляющей."
                },
                "en": {
                    "title": "Revolutionizing Brain-to-Image Decoding with Dynadiff",
                    "desc": "This paper presents Dynadiff, a novel single-stage diffusion model for decoding images from dynamic fMRI recordings. Unlike traditional methods that rely on complex multi-stage processes, Dynadiff simplifies the training process and enhances performance on time-resolved brain signals. The model excels in reconstructing high-level semantic images while maintaining competitiveness with existing methods on preprocessed data. This advancement paves the way for more accurate and timely brain-to-image decoding, allowing researchers to better understand how images are represented in brain activity over time."
                },
                "zh": {
                    "title": "动态神经活动扩散：重建图像的新方法",
                    "desc": "本研究提出了一种新的单阶段扩散模型Dynadiff，用于从动态变化的功能磁共振成像(fMRI)记录中重建图像。与现有方法相比，Dynadiff简化了训练过程，并在时间分辨率的fMRI信号上超越了最先进的模型，尤其是在高层次语义图像重建指标上表现优异。该模型在处理预处理的fMRI数据时也保持了竞争力，同时能够精确描述大脑活动中图像表示的演变。总体而言，这项工作为时间分辨的大脑到图像解码奠定了基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14366",
            "title": "Towards Embodied Cognition in Robots via Spatially Grounded Synthetic\n  Worlds",
            "url": "https://huggingface.co/papers/2505.14366",
            "abstract": "We present a conceptual framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT), a core capability for embodied cognition essential for Human-Robot Interaction (HRI). As a first step toward this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse, that enables supervised learning for spatial reasoning tasks. Each instance includes an RGB image, a natural language description, and a ground-truth 4X4 transformation matrix representing object pose. We focus on inferring Z-axis distance as a foundational skill, with future extensions targeting full 6 Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to support further research. This work serves as a foundational step toward embodied AI systems capable of spatial understanding in interactive human-robot scenarios.",
            "score": 0,
            "issue_id": 3882,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "0e72d48711224e3c",
            "authors": [
                "Joel Currie",
                "Gioele Migno",
                "Enrico Piacenti",
                "Maria Elena Giannaccini",
                "Patric Bach",
                "Davide De Tommaso",
                "Agnieszka Wykowska"
            ],
            "affiliations": [
                "Italian Institute of Technology, Genova, Italy",
                "University of Aberdeen, Aberdeen, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14366.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#open_source",
                    "#dataset",
                    "#agents",
                    "#healthcare",
                    "#cv",
                    "#reasoning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Визуальное пространственное мышление для взаимодействия человека и робота",
                    "desc": "Статья представляет концептуальную основу для обучения моделей компьютерного зрения и обработки естественного языка (VLM) выполнению задач визуального принятия перспективы (VPT). Авторы создали синтетический набор данных в NVIDIA Omniverse для обучения с учителем задачам пространственного мышления. Каждый пример включает RGB-изображение, текстовое описание и эталонную матрицу трансформации 4x4, представляющую позу объекта. Исследование фокусируется на определении расстояния по оси Z как базовом навыке, с перспективой расширения до полного рассуждения в 6 степенях свободы."
                },
                "en": {
                    "title": "Empowering Robots with Visual Perspective Taking for Better Interaction",
                    "desc": "This paper introduces a framework for training Vision-Language Models (VLMs) to enhance Visual Perspective Taking (VPT), which is crucial for effective Human-Robot Interaction (HRI). The authors create a synthetic dataset using NVIDIA Omniverse, designed for supervised learning in spatial reasoning tasks, containing RGB images, natural language descriptions, and transformation matrices for object poses. The focus is on predicting Z-axis distance, laying the groundwork for future advancements in understanding full 6 Degrees Of Freedom (DOFs). This dataset is made publicly available to encourage further research in developing AI systems that can comprehend spatial relationships in interactive settings."
                },
                "zh": {
                    "title": "迈向人机交互的空间理解能力",
                    "desc": "本文提出了一个训练视觉语言模型（VLMs）以实现视觉视角理解（VPT）的概念框架，这是人机交互（HRI）中重要的能力。我们引入了一个合成数据集，该数据集在NVIDIA Omniverse中生成，支持空间推理任务的监督学习。每个实例包含一个RGB图像、一个自然语言描述和一个表示物体姿态的真实4X4变换矩阵。我们专注于推断Z轴距离作为基础技能，未来将扩展到完整的六自由度（DOFs）推理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11563",
            "title": "Object-Centric Representations Improve Policy Generalization in Robot\n  Manipulation",
            "url": "https://huggingface.co/papers/2505.11563",
            "abstract": "Visual representations are central to the learning and generalization capabilities of robotic manipulation policies. While existing methods rely on global or dense features, such representations often entangle task-relevant and irrelevant scene information, limiting robustness under distribution shifts. In this work, we investigate object-centric representations (OCR) as a structured alternative that segments visual input into a finished set of entities, introducing inductive biases that align more naturally with manipulation tasks. We benchmark a range of visual encoders-object-centric, global and dense methods-across a suite of simulated and real-world manipulation tasks ranging from simple to complex, and evaluate their generalization under diverse visual conditions including changes in lighting, texture, and the presence of distractors. Our findings reveal that OCR-based policies outperform dense and global representations in generalization settings, even without task-specific pretraining. These insights suggest that OCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.",
            "score": 0,
            "issue_id": 3880,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "114f28f1ced2da93",
            "authors": [
                "Alexandre Chapin",
                "Bruno Machado",
                "Emmanuel Dellandrea",
                "Liming Chen"
            ],
            "affiliations": [
                "Ecole Centrale de Lyon, LIRIS 69130, Ecully, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11563.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#robotics",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Объектно-ориентированные представления - ключ к обобщению в робототехнике",
                    "desc": "Статья исследует объектно-ориентированные представления (OCR) как альтернативу глобальным и плотным признакам в задачах роботизированной манипуляции. Авторы сравнивают различные визуальные кодировщики на наборе симулированных и реальных задач манипуляции. Результаты показывают, что политики на основе OCR превосходят другие методы в обобщении на новые визуальные условия. Это указывает на перспективность OCR для создания визуальных систем, эффективно обобщающихся в динамичных робототехнических средах."
                },
                "en": {
                    "title": "Unlocking Robustness in Robotics with Object-Centric Representations",
                    "desc": "This paper explores the use of object-centric representations (OCR) in robotic manipulation tasks to improve learning and generalization. Unlike traditional methods that use global or dense features, which can mix relevant and irrelevant information, OCR focuses on distinct entities in the visual input. The authors benchmark various visual encoders, including OCR, against different manipulation tasks to assess their performance under changing conditions. The results show that OCR-based policies significantly outperform other methods, indicating their potential for robust performance in real-world scenarios."
                },
                "zh": {
                    "title": "物体中心表示：提升机器人操作的泛化能力",
                    "desc": "本研究探讨了物体中心表示（OCR）在机器人操作中的应用。与现有的全局或密集特征方法不同，OCR能够将视觉输入分割成一组独立的实体，从而更好地处理与任务相关的信息。我们在多种模拟和真实世界的操作任务中对比了不同的视觉编码器，发现OCR在面对不同的视觉条件时表现出更好的泛化能力。研究结果表明，OCR是一种有前景的方向，可以有效设计出适应动态真实环境的视觉系统。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.06914",
            "title": "The Distracting Effect: Understanding Irrelevant Passages in RAG",
            "url": "https://huggingface.co/papers/2505.06914",
            "abstract": "A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. In this paper, we shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs.   Our research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, we achieve up to a 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. Our contribution is two-fold: first, we move beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, we develop and analyze multiple methods for finding hard distracting passages. To our knowledge, no other research has provided such a comprehensive framework for identifying and utilizing hard distracting passages.",
            "score": 0,
            "issue_id": 3882,
            "pub_date": "2025-05-11",
            "pub_date_card": {
                "ru": "11 мая",
                "en": "May 11",
                "zh": "5月11日"
            },
            "hash": "dc5ed9552f5c1ec6",
            "authors": [
                "Chen Amiraz",
                "Florin Cuconasu",
                "Simone Filice",
                "Zohar Karnin"
            ],
            "affiliations": [
                "Sapienza University of Rome",
                "Technology Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.06914.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#training",
                    "#rag",
                    "#alignment"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Укрощение отвлекающих факторов для повышения точности RAG",
                    "desc": "Исследование посвящено проблеме отвлекающих пассажей в системах генерации с расширенным поиском (RAG). Авторы предлагают метод количественной оценки отвлекающего эффекта пассажа и демонстрируют его устойчивость для разных языковых моделей. Разработаны новые методы идентификации и использования сложных отвлекающих пассажей для улучшения RAG-систем. Дообучение языковых моделей на таких пассажах позволило повысить точность ответов на 7.5% по сравнению с обычными наборами данных RAG."
                },
                "en": {
                    "title": "Enhancing RAG: Turning Distractions into Accuracy Boosts",
                    "desc": "This paper addresses a significant challenge in Retrieval Augmented Generation (RAG) systems, where irrelevant passages can mislead language models (LLMs) and result in incorrect answers. The authors propose a new way to measure how distracting a passage can be in relation to a query and an LLM, providing a quantifiable metric for this effect. They introduce innovative techniques for identifying and leveraging these hard distracting passages, which leads to improved performance in answering accuracy. By fine-tuning LLMs with these selected passages, they demonstrate a notable increase in accuracy compared to traditional RAG approaches."
                },
                "zh": {
                    "title": "提升检索增强生成的准确性",
                    "desc": "本论文探讨了检索增强生成（RAG）中的一个重要问题，即与查询无关的检索段落可能会干扰答案生成的语言模型（LLM），导致错误的回答。我们提出了一种量化方法来衡量段落对查询和LLM的干扰效果，并展示了其在不同LLM中的稳健性。通过精心选择这些干扰段落并对LLM进行微调，我们的研究实现了高达7.5%的回答准确率提升。我们的贡献在于超越了简单的二元分类，发展了多种方法来识别和利用难以处理的干扰段落。"
                }
            }
        }
    ],
    "link_prev": "2025-05-20.html",
    "link_next": "2025-05-22.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "20.05",
        "en": "05/20",
        "zh": "5月20日"
    },
    "short_date_next": {
        "ru": "22.05",
        "en": "05/22",
        "zh": "5月22日"
    },
    "categories": {
        "#dataset": 18,
        "#data": 8,
        "#benchmark": 24,
        "#agents": 5,
        "#cv": 9,
        "#rl": 8,
        "#rlhf": 4,
        "#rag": 3,
        "#plp": 1,
        "#inference": 10,
        "#3d": 1,
        "#audio": 3,
        "#video": 3,
        "#multimodal": 13,
        "#math": 3,
        "#multilingual": 2,
        "#architecture": 8,
        "#healthcare": 4,
        "#training": 25,
        "#robotics": 1,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 10,
        "#reasoning": 22,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 5,
        "#security": 5,
        "#optimization": 17,
        "#survey": 1,
        "#diffusion": 4,
        "#alignment": 7,
        "#story_generation": 0,
        "#hallucinations": 5,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 10,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了一个名为BAGEL的开源基础模型。它能统一处理和生成多种模式的数据，如文本、图像和视频。BAGEL通过在大规模的多模式交错数据上预训练，展现出复杂的多模式推理能力。它在多模式生成和理解方面超越了其他开源模型，并具备高级的多模式推理能力。研究团队公开了关键发现、预训练细节、数据创建协议及代码和检查点，以促进多模式研究。",
        "title": "Emerging Properties in Unified Multimodal Pretraining",
        "pinyin": "这篇文章介绍了一个名为BAGEL的开源基础模型。\nZhè piān wénzhāng jièshào le yīgè míngwèi BAGEL de kāiyuán jīchǔ móxíng.\n\n它能统一处理和生成多种模式的数据，如文本、图像和视频。\nTā néng tǒngyī chǔlǐ hé shēngchéng duōzhǒng móshì de shùjù, rú wénběn, túxiàng hé shìpín.\n\nBAGEL通过在大规模的多模式交错数据上预训练，展现出复杂的多模式推理能力。\nBAGEL tōngguò zài dàguīmó de duōmóshì jiāocuò shùjù shàng yùxùnliàn, zhǎnxiàn chū fùzá de duōmóshì tuīlǐ nénglì.\n\n它在多模式生成和理解方面超越了其他开源模型，并具备高级的多模式推理能力。\nTā zài duōmóshì shēngchéng hé lǐjiě fāngmiàn chāoyuè le qítā kāiyuán móxíng, bìng jùbèi gāojí de duōmóshì tuīlǐ nénglì.\n\n研究团队公开了关键发现、预训练细节、数据创建协议及代码和检查点，以促进多模式研究。\nYánjiū tuánduì gōngkāi le guǎnjiàn fāxiàn, yùxùnliàn xìjiè, shùjù chuàngjiàn xiéyì jí dàimǎ hé jiǎnchádiǎn, yǐ cùjìn duōmóshì yánjiū.",
        "vocab": "[\n    {\"word\": \"开源\", \"pinyin\": \"kāi yuán\", \"trans\": \"open source\"},\n    {\"word\": \"基础模型\", \"pinyin\": \"jī chǔ mó xíng\", \"trans\": \"foundational model\"},\n    {\"word\": \"统一\", \"pinyin\": \"tǒng yī\", \"trans\": \"unify\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"process\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generate\"},\n    {\"word\": \"模式\", \"pinyin\": \"mó shì\", \"trans\": \"mode\"},\n    {\"word\": \"数据\", \"pinyin\": \"shù jù\", \"trans\": \"data\"},\n    {\"word\": \"文本\", \"pinyin\": \"wén běn\", \"trans\": \"text\"},\n    {\"word\": \"图像\", \"pinyin\": \"tú xiàng\", \"trans\": \"image\"},\n    {\"word\": \"视频\", \"pinyin\": \"shì pín\", \"trans\": \"video\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùn liàn\", \"trans\": \"pretrain\"},\n    {\"word\": \"展现\", \"pinyin\": \"zhǎn xiàn\", \"trans\": \"demonstrate\"},\n    {\"word\": \"复杂\", \"pinyin\": \"fù zá\", \"trans\": \"complex\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understanding\"},\n    {\"word\": \"方面\", \"pinyin\": \"fāng miàn\", \"trans\": \"aspect\"},\n    {\"word\": \"高级\", \"pinyin\": \"gāo jí\", \"trans\": \"advanced\"},\n    {\"word\": \"研究\", \"pinyin\": \"yán jiū\", \"trans\": \"research\"},\n    {\"word\": \"团队\", \"pinyin\": \"tuán duì\", \"trans\": \"team\"},\n    {\"word\": \"公开\", \"pinyin\": \"gōng kāi\", \"trans\": \"public\"},\n    {\"word\": \"发现\", \"pinyin\": \"fā xiàn\", \"trans\": \"discovery\"},\n    {\"word\": \"细节\", \"pinyin\": \"xì jiě\", \"trans\": \"detail\"},\n    {\"word\": \"协议\", \"pinyin\": \"xié yì\", \"trans\": \"protocol\"},\n    {\"word\": \"检查点\", \"pinyin\": \"jiǎn chá diǎn\", \"trans\": \"checkpoint\"},\n    {\"word\": \"促进\", \"pinyin\": \"cù jìn\", \"trans\": \"promote\"}\n]",
        "trans": "This article introduces an open-source foundational model called BAGEL. It is capable of uniformly processing and generating data in various modalities, such as text, images, and videos. BAGEL, through pre-training on large-scale interleaved multimodal data, demonstrates complex multimodal reasoning capabilities. It outperforms other open-source models in multimodal generation and understanding and possesses advanced multimodal reasoning abilities. The research team has made key findings, pre-training details, data creation protocols, and code and checkpoints publicly available to promote multimodal research.",
        "update_ts": "2025-05-21 09:13"
    }
}