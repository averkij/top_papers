{
    "date": {
        "ru": "21 мая",
        "en": "May 21",
        "zh": "5月21日"
    },
    "time_utc": "2025-05-21 00:55",
    "weekday": 2,
    "issue_id": 3867,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.11820",
            "title": "Chain-of-Model Learning for Language Model",
            "url": "https://huggingface.co/papers/2505.11820",
            "abstract": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the concept of Chain-of-Representation (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains) at the hidden dimension level. In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise Chain-of-Language-Model (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a KV sharing mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models. Our code will be released in the future at: https://github.com/microsoft/CoLM.",
            "score": 68,
            "issue_id": 3848,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "2e8115f0fe78856b",
            "authors": [
                "Kaitao Song",
                "Xiaohua Wang",
                "Xu Tan",
                "Huiqiang Jiang",
                "Chengruidong Zhang",
                "Yongliang Shen",
                "Cen LU",
                "Zihao Li",
                "Zifan Song",
                "Caihua Shan",
                "Yansen Wang",
                "Kan Ren",
                "Xiaoqing Zheng",
                "Tao Qin",
                "Yuqing Yang",
                "Dongsheng Li",
                "Lili Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "Microsoft Research",
                "ShanghaiTech University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11820.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#inference",
                    "#agi",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🔗",
                "ru": {
                    "title": "Цепная революция в языковых моделях: гибкость и эффективность",
                    "desc": "В этой статье представлена новая парадигма обучения под названием Chain-of-Model (CoM), которая внедряет причинно-следственные связи в скрытые состояния каждого слоя модели в виде цепочки. Авторы вводят концепцию Chain-of-Representation (CoR), формулирующую скрытые состояния на каждом уровне как комбинацию нескольких под-представлений на уровне скрытых измерений. На основе этого принципа разработана архитектура Chain-of-Language-Model (CoLM), которая внедряет идею CoM в каждый слой Transformer. Экспериментальные результаты показывают, что семейство моделей CoLM достигает сопоставимой производительности со стандартным Transformer, одновременно обеспечивая большую гибкость в масштабировании и развертывании."
                },
                "en": {
                    "title": "Scaling Language Models with Chain-of-Model Efficiency",
                    "desc": "This paper introduces a new learning approach called Chain-of-Model (CoM), which enhances model training efficiency by incorporating causal relationships into the hidden states of each layer. It presents the Chain-of-Representation (CoR) concept, where hidden states are formed from multiple sub-representations, allowing each layer to only access its preceding chains. The CoM framework enables models to scale up by adding more chains, providing flexibility in deploying various sub-models of different sizes. The Chain-of-Language-Model (CoLM) and its variant CoLM-Air further optimize Transformer architectures by sharing key-value pairs across chains, resulting in improved performance and adaptability for language models."
                },
                "zh": {
                    "title": "链式模型：灵活高效的语言模型新范式",
                    "desc": "本文提出了一种新颖的学习范式，称为链式模型（CoM），它将因果关系融入每一层的隐藏状态，以链式结构提高模型训练的效率和推理的灵活性。我们引入了链式表示（CoR）的概念，将每一层的隐藏状态表示为多个子表示的组合（即链）。在每一层中，输出表示的每个链只能查看输入表示中所有前面的链，从而使得基于CoM框架构建的模型能够通过增加链的数量逐步扩大模型规模，并提供不同大小的子模型以实现灵活推理。基于这一原理，我们设计了链式语言模型（CoLM），并进一步引入了CoLM-Air，通过引入键值共享机制，计算第一个链中的所有键和值，然后在所有链之间共享，从而展示了额外的可扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13417",
            "title": "AdaptThink: Reasoning Models Can Learn When to Think",
            "url": "https://huggingface.co/papers/2505.13417",
            "abstract": "Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https://github.com/THU-KEG/AdaptThink.",
            "score": 56,
            "issue_id": 3845,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "edd33223d8d833a7",
            "authors": [
                "Jiajie Zhang",
                "Nianyi Lin",
                "Lei Hou",
                "Ling Feng",
                "Juanzi Li"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13417.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Адаптивное мышление для оптимизации рассуждений ИИ",
                    "desc": "Исследователи представили новый алгоритм AdaptThink, который учит модели рассуждения адаптивно выбирать оптимальный режим мышления в зависимости от сложности задачи. Алгоритм использует метод обучения с подкреплением и включает в себя ограниченную целевую функцию оптимизации и стратегию выборки по важности. Эксперименты показали, что AdaptThink значительно сокращает вычислительные затраты при одновременном повышении производительности моделей. На трех наборах математических данных алгоритм сократил среднюю длину ответа модели DeepSeek-R1-Distill-Qwen-1.5B на 53% и повысил ее точность на 2.4%."
                },
                "en": {
                    "title": "Optimize Reasoning with Adaptive Thinking Modes!",
                    "desc": "This paper introduces AdaptThink, a reinforcement learning algorithm designed to optimize reasoning models by allowing them to choose between two thinking modes: NoThinking and traditional thinking. NoThinking enables models to skip lengthy reasoning processes for simpler tasks, improving efficiency without sacrificing performance. AdaptThink employs a constrained optimization objective to encourage the use of NoThinking while maintaining overall accuracy, and it uses importance sampling to balance training between both modes. The results show that AdaptThink significantly reduces inference costs and enhances performance on math tasks, demonstrating the effectiveness of adaptive thinking-mode selection."
                },
                "zh": {
                    "title": "自适应思考模式选择，提升推理效率与质量",
                    "desc": "最近，大型推理模型在各种任务上表现出色，但其冗长的思考过程显著增加了推理开销，导致效率成为瓶颈。本文提出了一种名为NoThinking的方法，鼓励推理模型跳过思考，直接生成最终解决方案，适用于相对简单的任务。基于此，我们提出了AdaptThink，这是一种新颖的强化学习算法，旨在根据问题难度自适应选择最佳思考模式。实验表明，AdaptThink显著降低了推理成本，同时提高了模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11896",
            "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.11896",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and decreased average response tokens by 69.06%, while maintaining high performance on complex tasks.",
            "score": 45,
            "issue_id": 3846,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "bdc79864df7cbd51",
            "authors": [
                "Chenwei Lou",
                "Zewei Sun",
                "Xinnian Liang",
                "Meng Qu",
                "Wei Shen",
                "Wenqi Wang",
                "Yuntao Li",
                "Qingping Yang",
                "Shuangzhi Wu"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11896.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rlhf",
                    "#rl",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "AdaCoT: Умное рассуждение для языковых моделей",
                    "desc": "AdaCoT - это новый фреймворк, позволяющий крупным языковым моделям (LLM) адаптивно решать, когда использовать метод цепочки рассуждений (Chain-of-Thought, CoT). Используя обучение с подкреплением, в частности Proximal Policy Optimization (PPO), AdaCoT оптимизирует баланс между производительностью модели и вычислительными затратами, связанными с применением CoT. Ключевым техническим вкладом является метод Selective Loss Masking (SLM), предотвращающий коллапс границы принятия решений во время многоэтапного обучения с подкреплением. Эксперименты показывают, что AdaCoT значительно снижает использование CoT для запросов, не требующих сложных рассуждений, сохраняя при этом высокую производительность на сложных задачах."
                },
                "en": {
                    "title": "Adaptive Reasoning for Efficient Language Models",
                    "desc": "This paper presents AdaCoT, a new framework that improves the efficiency of Large Language Models (LLMs) by adaptively deciding when to use Chain-of-Thought (CoT) prompting. Traditional CoT prompting can be computationally expensive, especially for simpler queries, but AdaCoT optimizes this by framing the decision to use CoT as a Pareto optimization problem. The authors employ reinforcement learning, specifically Proximal Policy Optimization (PPO), to dynamically adjust when CoT is triggered based on the complexity of the input. Their approach includes a technique called Selective Loss Masking (SLM) to ensure stable training, resulting in significant reductions in CoT usage while maintaining high performance on complex tasks."
                },
                "zh": {
                    "title": "自适应链式推理，提升效率与性能",
                    "desc": "大型语言模型（LLMs）在处理复杂推理任务时表现出色，但在某些情况下面临挑战。为了解决这一问题，本文提出了AdaCoT（自适应链式推理），它允许模型根据输入的复杂性自适应地决定是否使用链式推理。我们将自适应推理视为一个帕累托优化问题，旨在平衡模型性能与链式推理的计算成本。实验结果表明，AdaCoT在不需要复杂推理的查询中显著减少了链式推理的使用，提升了效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11254",
            "title": "Delta Attention: Fast and Accurate Sparse Attention Inference by Delta\n  Correction",
            "url": "https://huggingface.co/papers/2505.11254",
            "abstract": "The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills.",
            "score": 35,
            "issue_id": 3847,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "2aba31b686859e82",
            "authors": [
                "Jeffrey Willette",
                "Heejun Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11254.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#long_context",
                    "#architecture",
                    "#inference",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Коррекция распределения для эффективного разреженного внимания",
                    "desc": "Статья предлагает новый метод для повышения эффективности разреженного внимания в трансформерах. Авторы обнаружили, что разреженное вычисление вызывает сдвиг распределения в выходных данных механизма внимания, что приводит к снижению производительности. Предложенная процедура корректирует этот сдвиг, приближая распределение выходных данных разреженного внимания к квадратичному. Метод может применяться поверх любого алгоритма разреженного внимания, значительно повышая точность при сохранении высокой разреженности и скорости обработки."
                },
                "en": {
                    "title": "Boosting Sparse Attention: Aligning Outputs for Enhanced Performance",
                    "desc": "This paper addresses the inefficiencies of the attention mechanism in transformers, which typically has a quadratic complexity that increases inference costs for long sequences. It highlights that while sparse attention methods can reduce computation, they often lead to performance degradation due to a distributional shift in attention outputs. The authors propose a novel procedure to correct this shift, aligning sparse attention outputs more closely with those of traditional quadratic attention. Their method significantly improves performance, achieving an average increase of 36 percentage points while maintaining high sparsity and speed, making it much faster than existing methods."
                },
                "zh": {
                    "title": "稀疏注意力的分布修正，提升性能与效率",
                    "desc": "本文探讨了变换器的注意力机制在处理长序列时的计算复杂度问题，导致推理成本高和延迟大。尽管注意力矩阵通常是稀疏的，但稀疏注意力推理方法在减少计算负担的同时，可能会导致性能下降。我们发现，性能下降的一个原因是稀疏计算引起了注意力输出的分布偏移，这使得解码时的查询与预填阶段的键对齐不佳。为了解决这个问题，我们提出了一种简单而有效的修正方法，使稀疏注意力输出的分布更接近于二次注意力，从而显著提高了性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13227",
            "title": "Scaling Computer-Use Grounding via User Interface Decomposition and\n  Synthesis",
            "url": "https://huggingface.co/papers/2505.13227",
            "abstract": "Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.",
            "score": 34,
            "issue_id": 3849,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "fe01e0daca57b031",
            "authors": [
                "Tianbao Xie",
                "Jiaqi Deng",
                "Xiaochuan Li",
                "Junlin Yang",
                "Haoyuan Wu",
                "Jixuan Chen",
                "Wenjing Hu",
                "Xinyuan Wang",
                "Yuhui Xu",
                "Zekun Wang",
                "Yiheng Xu",
                "Junli Wang",
                "Doyen Sahoo",
                "Tao Yu",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13227.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#graphs",
                    "#agents",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Революция в обучении ИИ работе с компьютерными интерфейсами",
                    "desc": "Статья представляет новый бенчмарк OSWorld-G для оценки способности моделей машинного обучения к интерпретации естественного языка в контексте графических интерфейсов. Авторы также создали крупнейший датасет Jedi, содержащий 4 миллиона примеров для обучения моделей взаимодействию с компьютерными интерфейсами. Модели, обученные на Jedi, превзошли существующие подходы на нескольких бенчмарках, включая OSWorld-G. Исследование показало, что улучшенное понимание интерфейсов значительно повышает способности крупных языковых моделей выполнять сложные компьютерные задачи."
                },
                "en": {
                    "title": "Enhancing GUI Grounding with Comprehensive Datasets and Models",
                    "desc": "This paper addresses the challenge of GUI grounding, which is the process of translating natural language commands into actions on graphical user interfaces. Current benchmarks are limited as they only focus on simple tasks, neglecting the complexities of real-world interactions that require understanding of software context and layout. The authors introduce OSWorld-G, a new benchmark with 564 detailed samples and the Jedi dataset, which contains 4 million examples to improve grounding tasks. Their findings show that using the Jedi dataset significantly enhances the performance of multi-scale models in executing complex computer tasks, demonstrating the importance of specialized data for effective grounding."
                },
                "zh": {
                    "title": "提升计算机使用代理的基础能力",
                    "desc": "本论文探讨了图形用户界面（GUI）基础的自然语言指令映射问题，指出现有基准测试过于简化，无法反映真实世界的复杂交互。为了解决这一问题，作者提出了OSWorld-G基准，包含564个精细注释的样本，涵盖文本匹配、元素识别、布局理解和精确操作等多种任务类型。此外，作者合成并发布了最大的计算机使用基础数据集Jedi，包含400万个示例，展示了其在复杂计算机任务中的有效性。通过详细的消融研究，论文还识别了影响基础性能的关键因素，并验证了不同界面元素的专门数据结合能够实现对新界面的组合泛化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13389",
            "title": "Faster Video Diffusion with Trainable Sparse Attention",
            "url": "https://huggingface.co/papers/2505.13389",
            "abstract": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on a small subset of positions. We turn this observation into VSA, a trainable, hardware-efficient sparse attention that replaces full attention at both training and inference. In VSA, a lightweight coarse stage pools tokens into tiles and identifies high-weight critical tokens; a fine stage computes token-level attention only inside those tiles subjecting to block computing layout to ensure hard efficiency. This leads to a single differentiable kernel that trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of FlashAttention3 MFU. We perform a large sweep of ablation studies and scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA reaches a Pareto point that cuts training FLOPS by 2.53times with no drop in diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention time by 6times and lowers end-to-end generation time from 31s to 18s with comparable quality. These results establish trainable sparse attention as a practical alternative to full attention and a key enabler for further scaling of video diffusion models.",
            "score": 26,
            "issue_id": 3850,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "33a48c202961951b",
            "authors": [
                "Peiyuan Zhang",
                "Haofeng Huang",
                "Yongqi Chen",
                "Will Lin",
                "Zhengzhong Liu",
                "Ion Stoica",
                "Eric P. Xing",
                "Hao Zhang"
            ],
            "affiliations": [
                "MBZUAI",
                "UC Berkeley",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13389.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#diffusion",
                    "#training",
                    "#open_source",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Эффективное разреженное внимание для масштабирования видео-диффузионных моделей",
                    "desc": "Статья представляет новый метод разреженного внимания (VSA) для масштабирования видео-диффузионных трансформеров. VSA использует двухэтапный подход: грубый этап для выявления критических токенов и тонкий этап для вычисления внимания только внутри важных областей. Метод обучается от начала до конца, не требует пост-обработки и сохраняет 85% эффективности по сравнению с полным вниманием. Эксперименты показывают, что VSA сокращает вычислительные затраты в 2,53 раза без потери качества, а также ускоряет генерацию в 6 раз для существующих моделей."
                },
                "en": {
                    "title": "Efficient Attention for Scalable Video Diffusion",
                    "desc": "This paper introduces VSA, a novel trainable sparse attention mechanism designed to improve the efficiency of video diffusion transformers (DiTs). By focusing on a small subset of critical tokens, VSA reduces the computational burden associated with traditional quadratic attention. The method consists of a coarse stage that pools tokens and identifies important ones, followed by a fine stage that computes attention only within these selected tokens. The results demonstrate that VSA significantly decreases training FLOPS while maintaining performance, making it a viable alternative for scaling video diffusion models."
                },
                "zh": {
                    "title": "可训练稀疏注意力：视频扩散模型的新选择",
                    "desc": "这篇论文提出了一种名为VSA的可训练稀疏注意力机制，旨在解决视频扩散变换器（DiTs）在处理3D注意力时的计算限制。VSA通过将注意力计算分为粗略阶段和精细阶段，显著提高了计算效率，同时保持了高效的训练性能。实验结果表明，VSA在不降低扩散损失的情况下，将训练的FLOPS减少了2.53倍，并且在开源模型Wan-2.1上实现了6倍的注意力计算加速。该研究表明，可训练的稀疏注意力是全注意力的有效替代方案，并为视频扩散模型的进一步扩展提供了关键支持。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13379",
            "title": "Thinkless: LLM Learns When to Think",
            "url": "https://huggingface.co/papers/2505.13379",
            "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at https://github.com/VainF/Thinkless",
            "score": 25,
            "issue_id": 3846,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "d41117eabc11e5c3",
            "authors": [
                "Gongfan Fang",
                "Xinyin Ma",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13379.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Умное переключение между кратким и развернутым мышлением в языковых моделях",
                    "desc": "Статья представляет Thinkless - обучаемую систему, позволяющую языковым моделям адаптивно выбирать между кратким и развернутым рассуждением в зависимости от сложности задачи. Система использует обучение с подкреплением и два управляющих токена: <short> для кратких ответов и <think> для детального рассуждения. В основе метода лежит алгоритм DeGRPO, который разделяет цель обучения на выбор режима рассуждения и улучшение точности ответов. Эмпирические результаты показывают, что Thinkless способен сократить использование длинных цепочек рассуждений на 50-90%, значительно повышая эффективность моделей."
                },
                "en": {
                    "title": "Thinkless: Smart Reasoning for Efficient Language Models",
                    "desc": "This paper introduces Thinkless, a framework designed to enhance the efficiency of Reasoning Language Models (RLMs) by enabling them to choose between short-form and long-form reasoning based on task complexity. The framework utilizes reinforcement learning and two control tokens, <short> for brief answers and <think> for detailed reasoning, to guide the model's response strategy. A novel algorithm called Decoupled Group Relative Policy Optimization (DeGRPO) is employed to separate the learning objectives, allowing for better control over reasoning mode selection and response accuracy. The results show that Thinkless can significantly reduce the reliance on long-chain reasoning, improving computational efficiency while maintaining performance on various benchmarks."
                },
                "zh": {
                    "title": "让模型学会何时思考",
                    "desc": "本文提出了一种名为Thinkless的可学习框架，旨在提高大型语言模型（LLM）在推理任务中的效率。该框架通过强化学习训练，使模型能够根据任务复杂性和自身能力自适应选择短期或长期推理。Thinkless使用两个控制标记<short>和<think>来分别表示简洁回答和详细推理。实验结果表明，Thinkless能够将长期推理的使用减少50%至90%，显著提升推理语言模型的效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13308",
            "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient\n  in Latent Space",
            "url": "https://huggingface.co/papers/2505.13308",
            "abstract": "Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.",
            "score": 23,
            "issue_id": 3851,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "bd9e494dc68db18c",
            "authors": [
                "Hengli Li",
                "Chenxi Li",
                "Tong Wu",
                "Xuekai Zhu",
                "Yuxuan Wang",
                "Zhaoxin Yu",
                "Eric Hanchen Jiang",
                "Song-Chun Zhu",
                "Zixia Jia",
                "Ying Nian Wu",
                "Zilong Zheng"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University",
                "Institute for Artificial Intelligence, Peking University",
                "Institute of Automation, Chinese Academy of Sciences",
                "NLCo Lab, Beijing Institute for General Artificial Intelligence",
                "Shanghai Jiao Tong University",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13308.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#agi"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LatentSeek: Повышение способности рассуждать у ИИ через адаптацию в латентном пространстве",
                    "desc": "Статья представляет LatentSeek - новый фреймворк для улучшения способностей рассуждения больших языковых моделей (LLM) с помощью адаптации на уровне экземпляров во время тестирования в латентном пространстве модели. LatentSeek использует градиент политики для итеративного обновления латентных представлений, руководствуясь самогенерируемыми сигналами вознаграждения. Метод превосходит сильные базовые линии на различных тестах рассуждений и демонстрирует высокую эффективность, обычно сходясь за несколько итераций. LatentSeek позиционируется как легковесное, масштабируемое и эффективное решение для улучшения способностей рассуждения LLM."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with LatentSeek",
                    "desc": "This paper addresses the challenges of reasoning in Large Language Models (LLMs) as they strive for Artificial General Intelligence (AGI). It introduces LatentSeek, a framework that improves reasoning by adapting the model's latent space during test time, rather than updating parameters. By using policy gradient methods, LatentSeek iteratively refines latent representations based on self-generated rewards, leading to enhanced performance on reasoning tasks. The results show that LatentSeek outperforms existing methods and is efficient, converging quickly while still benefiting from additional iterations."
                },
                "zh": {
                    "title": "LatentSeek：提升LLM推理能力的新方法",
                    "desc": "这篇论文探讨了大型语言模型（LLMs）在推理能力方面的挑战，尤其是在追求通用人工智能（AGI）时。作者提出了一种新框架LatentSeek，通过在模型的潜在空间中进行测试时实例级适应（TTIA），来增强LLMs的推理能力。与以往方法不同，LatentSeek利用策略梯度迭代更新潜在表示，并通过自生成的奖励信号进行指导。实验结果表明，LatentSeek在多个推理基准测试中表现优异，显示出其在潜在空间中进行测试时扩展的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12082",
            "title": "Model Merging in Pre-training of Large Language Models",
            "url": "https://huggingface.co/papers/2505.12082",
            "abstract": "Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.",
            "score": 23,
            "issue_id": 3854,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "7f25885def33b040",
            "authors": [
                "Yunshui Li",
                "Yiyuan Ma",
                "Shen Yan",
                "Chaoyi Zhang",
                "Jing Liu",
                "Jianqiao Lu",
                "Ziwen Xu",
                "Mengzhao Chen",
                "Minrui Wang",
                "Shiyi Zhan",
                "Jin Ma",
                "Xunhao Lai",
                "Yao Luo",
                "Xingyan Bin",
                "Hongbin Ren",
                "Mingji Han",
                "Wenhao Hao",
                "Bairen Yi",
                "LingJun Liu",
                "Bole Ma",
                "Xiaoying Jia",
                "Zhou Xun",
                "Liang Xiang",
                "Yonghui Wu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.12082.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Слияние моделей: путь к эффективному предобучению больших языковых моделей",
                    "desc": "Статья исследует применение техники слияния моделей в процессе предварительного обучения больших языковых моделей. Авторы проводят эксперименты с плотными архитектурами и архитектурами Mixture-of-Experts, демонстрируя значительное улучшение производительности при слиянии контрольных точек, обученных с постоянной скоростью обучения. Результаты показывают возможность более эффективной разработки моделей и снижения затрат на обучение. Исследование предоставляет новые идеи о механизмах слияния моделей и практические рекомендации для сообщества открытого исходного кода."
                },
                "en": {
                    "title": "Enhancing Language Models through Effective Model Merging",
                    "desc": "This paper explores the technique of model merging to improve large language models during their pre-training phase. The authors conduct extensive experiments on various architectures, including dense models and Mixture-of-Experts (MoE), to assess the impact of merging checkpoints. They find that using constant learning rates during merging not only enhances model performance but also allows for better predictions of training behavior. The study provides valuable insights and practical guidelines for the open-source community to implement effective model merging strategies, ultimately reducing training costs and improving efficiency."
                },
                "zh": {
                    "title": "模型合并：提升预训练效率的新方法",
                    "desc": "模型合并是一种有前景的技术，可以增强大型语言模型，但在大规模预训练中的应用仍然相对未被探索。本文全面研究了在预训练过程中使用的模型合并技术。通过对数百万到超过1000亿参数的密集和混合专家（MoE）架构进行广泛实验，我们证明了使用恒定学习率训练的检查点合并不仅显著提高了性能，还能准确预测退火行为。这些改进使得模型开发更加高效，并显著降低了训练成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13427",
            "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable\n  Step-Level Supervision",
            "url": "https://huggingface.co/papers/2505.13427",
            "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive progress in vision-language understanding, they still struggle with complex multi-step reasoning, often producing logically inconsistent or partially correct solutions. A key limitation lies in the lack of fine-grained supervision over intermediate reasoning steps. To address this, we propose MM-PRM, a process reward model trained within a fully automated, scalable framework. We first build MM-Policy, a strong multimodal model trained on diverse mathematical reasoning data. Then, we construct MM-K12, a curated dataset of 10,000 multimodal math problems with verifiable answers, which serves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based pipeline, we generate over 700k step-level annotations without human labeling. The resulting PRM is used to score candidate reasoning paths in the Best-of-N inference setup and achieves significant improvements across both in-domain (MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.) benchmarks. Further analysis confirms the effectiveness of soft labels, smaller learning rates, and path diversity in optimizing PRM performance. MM-PRM demonstrates that process supervision is a powerful tool for enhancing the logical robustness of multimodal reasoning systems. We release all our codes and data at https://github.com/ModalMinds/MM-PRM.",
            "score": 20,
            "issue_id": 3847,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "22362149c9b7b5ae",
            "authors": [
                "Lingxiao Du",
                "Fanqing Meng",
                "Zongkai Liu",
                "Zhixiang Zhou",
                "Ping Luo",
                "Qiaosheng Zhang",
                "Wenqi Shao"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13427.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#multimodal",
                    "#math",
                    "#training",
                    "#open_source",
                    "#benchmark",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Автоматизированное обучение мультимодальных моделей пошаговым рассуждениям",
                    "desc": "В статье представлена модель MM-PRM, обучающаяся оценивать промежуточные шаги рассуждений в мультимодальных задачах. Авторы создали датасет MM-K12 с 10 000 мультимодальных математических задач и использовали метод Монте-Карло для генерации более 700 тысяч аннотаций шагов решения без участия человека. Применение MM-PRM в схеме вывода Best-of-N значительно улучшило результаты как на тестовом наборе MM-K12, так и на внешних бенчмарках. Исследование показывает эффективность использования мягких меток, меньших скоростей обучения и разнообразия путей для оптимизации производительности модели."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with Process Supervision",
                    "desc": "This paper introduces MM-PRM, a novel process reward model designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs) in solving complex math problems. The authors highlight that MLLMs often struggle with multi-step reasoning due to insufficient supervision of intermediate steps. To overcome this, they create a strong multimodal model called MM-Policy and a new dataset, MM-K12, containing 10,000 multimodal math problems. By employing a Monte Carlo Tree Search method, they generate over 700,000 annotations to train the PRM, which significantly enhances the logical consistency of reasoning in various benchmarks."
                },
                "zh": {
                    "title": "过程监督提升多模态推理的逻辑稳健性",
                    "desc": "这篇论文介绍了一种新的多模态过程奖励模型（MM-PRM），旨在提高多模态大语言模型在复杂多步骤推理中的表现。研究发现，现有模型在推理过程中缺乏细粒度的监督，导致逻辑不一致或部分正确的结果。为了解决这个问题，作者构建了一个强大的多模态模型MM-Policy，并创建了一个包含10,000个可验证答案的多模态数学问题数据集MM-K12。通过无人工标注的方式生成超过70万条步骤级注释，MM-PRM在推理路径评分中表现出显著的改进，证明了过程监督在增强多模态推理系统逻辑稳健性方面的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13215",
            "title": "Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation",
            "url": "https://huggingface.co/papers/2505.13215",
            "abstract": "Recent advancements in dynamic 3D scene reconstruction have shown promising results, enabling high-fidelity 3D novel view synthesis with improved temporal consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an appealing approach due to its ability to model high-fidelity spatial and temporal variations. However, existing methods suffer from substantial computational and memory overhead due to the redundant allocation of 4D Gaussians to static regions, which can also degrade image quality. In this work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework that adaptively represents static regions with 3D Gaussians while reserving 4D Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian representation and iteratively converts temporally invariant Gaussians into 3D, significantly reducing the number of parameters and improving computational efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation, capturing complex motions with high fidelity. Our approach achieves significantly faster training times compared to baseline 4D Gaussian Splatting methods while maintaining or improving the visual quality.",
            "score": 20,
            "issue_id": 3846,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "0ab00a261298ad44",
            "authors": [
                "Seungjun Oh",
                "Younggeun Lee",
                "Hyejin Jeon",
                "Eunbyung Park"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Sungkyunkwan University",
                "Department of Artificial Intelligence, Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13215.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Гибридный 3D-4D подход для эффективной реконструкции динамических сцен",
                    "desc": "Статья представляет новый метод 3D-4DGS для реконструкции динамических 3D-сцен. Он комбинирует 3D гауссианы для статичных областей и 4D гауссианы для динамических элементов. Это позволяет значительно сократить вычислительные затраты и память по сравнению с полностью 4D подходом. Метод демонстрирует более быстрое обучение при сохранении или улучшении визуального качества."
                },
                "en": {
                    "title": "Efficient 3D-4D Scene Reconstruction with Hybrid Gaussian Splatting",
                    "desc": "This paper presents a new method called hybrid 3D-4D Gaussian Splatting (3D-4DGS) for dynamic 3D scene reconstruction. It combines 3D and 4D Gaussian representations to efficiently model static and dynamic elements in a scene. By converting static regions to 3D Gaussians, the method reduces computational load and memory usage while preserving the quality of dynamic elements with 4D Gaussians. The results show that 3D-4DGS achieves faster training times and improved visual quality compared to traditional 4D Gaussian Splatting techniques."
                },
                "zh": {
                    "title": "高效的动态3D场景重建新方法",
                    "desc": "最近动态3D场景重建的进展显示出良好的效果，能够实现高保真度的3D新视图合成，并提高时间一致性。在这些方法中，4D高斯点云（4DGS）因其能够建模高保真的空间和时间变化而受到关注。然而，现有方法在静态区域冗余分配4D高斯时，导致了显著的计算和内存开销，并可能降低图像质量。我们提出了一种混合3D-4D高斯点云（3D-4DGS）框架，能够自适应地用3D高斯表示静态区域，同时为动态元素保留4D高斯，从而显著提高计算效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12805",
            "title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with\n  LoRA",
            "url": "https://huggingface.co/papers/2505.12805",
            "abstract": "Low-Rank Adaptation (LoRA), which introduces a product of two trainable low-rank matrices into frozen pre-trained weights, is widely used for efficient fine-tuning of language models in federated learning (FL). However, when combined with differentially private stochastic gradient descent (DP-SGD), LoRA faces substantial noise amplification: DP-SGD perturbs per-sample gradients, and the matrix multiplication of the LoRA update (BA) intensifies this effect. Freezing one matrix (e.g., A) reduces the noise but restricts model expressiveness, often resulting in suboptimal adaptation. To address this, we propose FedSVD, a simple yet effective method that introduces a global reparameterization based on singular value decomposition (SVD). In our approach, each client optimizes only the B matrix and transmits it to the server. The server aggregates the B matrices, computes the product BA using the previous A, and refactorizes the result via SVD. This yields a new adaptive A composed of the orthonormal right singular vectors of BA, and an updated B containing the remaining SVD components. This reparameterization avoids quadratic noise amplification, while allowing A to better capture the principal directions of the aggregate updates. Moreover, the orthonormal structure of A bounds the gradient norms of B and preserves more signal under DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD consistently improves stability and performance across a variety of privacy settings and benchmarks, outperforming relevant baselines under both private and non-private regimes.",
            "score": 20,
            "issue_id": 3848,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "41ef4fd84db0c7fb",
            "authors": [
                "Seanie Lee",
                "Sangwoo Park",
                "Dong Bok Lee",
                "Dominik Wagner",
                "Haebin Seong",
                "Tobias Bocklet",
                "Juho Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "KAIST",
                "Technische Hochschule Nürnberg Georg Simon Ohm"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12805.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#benchmark",
                    "#security",
                    "#optimization"
                ],
                "emoji": "🔒",
                "ru": {
                    "title": "FedSVD: Защищенное федеративное обучение языковых моделей с сохранением эффективности",
                    "desc": "Статья представляет новый метод FedSVD для эффективного федеративного обучения языковых моделей с дифференциальной приватностью. FedSVD решает проблему усиления шума в методе Low-Rank Adaptation (LoRA) при использовании DP-SGD. Метод использует сингулярное разложение (SVD) для глобальной репараметризации, что позволяет избежать квадратичного усиления шума. FedSVD показывает улучшенную стабильность и производительность по сравнению с базовыми методами в различных настройках приватности."
                },
                "en": {
                    "title": "Enhancing Federated Learning with FedSVD: A Noise-Resilient Approach",
                    "desc": "This paper presents FedSVD, a novel method that enhances the fine-tuning of language models in federated learning while addressing the challenges posed by noise amplification in differentially private stochastic gradient descent (DP-SGD). By utilizing singular value decomposition (SVD), FedSVD allows clients to optimize only one matrix (B) and send it to a central server, which then aggregates these updates to improve model adaptation. This approach mitigates the noise amplification that occurs when combining LoRA with DP-SGD, ensuring that the model remains expressive without compromising privacy. The results demonstrate that FedSVD improves both stability and performance across various privacy settings, outperforming existing methods."
                },
                "zh": {
                    "title": "FedSVD：优化联邦学习中的低秩适应",
                    "desc": "本文提出了一种名为FedSVD的方法，旨在解决低秩适应（LoRA）在联邦学习中与差分隐私随机梯度下降（DP-SGD）结合时的噪声放大问题。通过引入基于奇异值分解（SVD）的全局重参数化，FedSVD允许每个客户端仅优化B矩阵并将其传输到服务器。服务器聚合B矩阵，计算BA的乘积，并通过SVD重新因式分解，从而生成新的适应性A和更新后的B。该方法有效减少了噪声放大，同时提高了模型的稳定性和性能，尤其在隐私设置下表现优异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12504",
            "title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language\n  Models",
            "url": "https://huggingface.co/papers/2505.12504",
            "abstract": "Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates and improper clipping can lead to training collapse. To address this issue, we propose Clipped Policy Gradient Optimization with Policy Drift (CPGD), a novel algorithm designed to stabilize policy learning in LMs. CPGD introduces a policy drift constraint based on KL divergence to dynamically regularize policy updates, and leverages a clip mechanism on the logarithm of the ratio to prevent excessive policy updates. We provide theoretical justification for CPGD and demonstrate through empirical analysis that it mitigates the instability observed in prior approaches. Furthermore, we show that CPGD significantly improves performance while maintaining training stability. Our implementation balances theoretical rigor with practical usability, offering a robust alternative for RL in the post-training of LMs. We release our code at https://github.com/ModalMinds/MM-EUREKA.",
            "score": 20,
            "issue_id": 3847,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 мая",
                "en": "May 18",
                "zh": "5月18日"
            },
            "hash": "f8b07da7e5e43f1e",
            "authors": [
                "Zongkai Liu",
                "Fanqing Meng",
                "Lingxiao Du",
                "Zhixiang Zhou",
                "Chao Yu",
                "Wenqi Shao",
                "Qiaosheng Zhang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "Sun Yat-Sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12504.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Стабильное обучение с подкреплением для языковых моделей",
                    "desc": "Статья представляет новый алгоритм CPGD для стабилизации обучения с подкреплением языковых моделей. CPGD вводит ограничение на дрейф политики на основе KL-дивергенции и использует механизм отсечения для предотвращения чрезмерных обновлений политики. Авторы теоретически обосновывают CPGD и эмпирически демонстрируют, что он уменьшает нестабильность, наблюдаемую в предыдущих подходах. Результаты показывают значительное улучшение производительности при сохранении стабильности обучения."
                },
                "en": {
                    "title": "Stabilizing Reinforcement Learning with CPGD",
                    "desc": "This paper introduces Clipped Policy Gradient Optimization with Policy Drift (CPGD), a new algorithm aimed at improving the stability of reinforcement learning in language models. Traditional methods often face issues like training collapse due to large policy updates and improper clipping. CPGD addresses these challenges by using a KL divergence-based policy drift constraint to regulate updates and a clipping mechanism to limit excessive changes. The authors provide theoretical support for CPGD and demonstrate its effectiveness in enhancing performance while ensuring stable training."
                },
                "zh": {
                    "title": "稳定强化学习，提升语言模型性能",
                    "desc": "最近，基于规则的强化学习（RL）在语言模型（LM）的推理能力上取得了显著进展，但现有的RL方法如GRPO、REINFORCE++和RLOO常常面临训练不稳定的问题。为了解决这个问题，我们提出了一种新算法——带有策略漂移的剪切策略梯度优化（CPGD），旨在稳定语言模型中的策略学习。CPGD通过基于KL散度的策略漂移约束动态地规范策略更新，并利用对数比率的剪切机制防止过大的策略更新。我们的理论分析和实证结果表明，CPGD不仅缓解了之前方法的训练不稳定性，还显著提高了性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12992",
            "title": "Fractured Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2505.12992",
            "abstract": "Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning.",
            "score": 16,
            "issue_id": 3849,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "464c0b3fac842217",
            "authors": [
                "Baohao Liao",
                "Hanze Dong",
                "Yuhui Xu",
                "Doyen Sahoo",
                "Christof Monz",
                "Junnan Li",
                "Caiming Xiong"
            ],
            "affiliations": [
                "Salesforce AI Research",
                "University of Amsterdam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12992.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное масштабирование рассуждений языковых моделей без переобучения",
                    "desc": "Эта статья представляет новый метод под названием Fractured Sampling для улучшения рассуждений больших языковых моделей (LLM) во время вывода. Метод основан на идее усечения цепочки рассуждений (Chain-of-Thought, CoT) и позволяет балансировать между полным CoT и генерацией только ответа по трем осям: количество траекторий рассуждений, количество финальных решений на траекторию и глубина усечения. Авторы показывают, что Fractured Sampling достигает лучшего соотношения точности и вычислительных затрат на пяти эталонных наборах данных для задач рассуждения. Результаты демонстрируют значительное улучшение масштабирования производительности LLM при рассуждениях без необходимости переобучения модели."
                },
                "en": {
                    "title": "Efficient Reasoning with Fractured Sampling",
                    "desc": "This paper presents a new method called Fractured Sampling that enhances the reasoning abilities of large language models (LLMs) during inference without the need for retraining. It builds on the concept of Chain-of-Thought (CoT) prompting, which improves accuracy by generating intermediate reasoning steps, but often at a high token cost. The authors demonstrate that a truncated version of CoT can achieve similar results with significantly fewer tokens. By exploring different ways to balance reasoning depth and the number of solutions, Fractured Sampling offers a more efficient approach that improves accuracy while reducing computational costs."
                },
                "zh": {
                    "title": "高效推理：Fractured Sampling的创新之路",
                    "desc": "本文探讨了一种新的推理时间缩放技术，称为Fractured Sampling，旨在提高大型语言模型（LLMs）的推理能力。通过截断链式思维（CoT），该方法在生成最终答案时减少了所需的token数量，同时保持了与完整CoT相似的准确性。Fractured Sampling在推理轨迹数量、每条轨迹的最终解决方案数量和推理深度等三个维度上进行插值，从而优化了准确性与成本的平衡。实验结果表明，该方法在多个推理基准上表现出色，能够实现更高效和可扩展的LLM推理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13444",
            "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2505.13444",
            "abstract": "Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.",
            "score": 15,
            "issue_id": 3848,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "7bd36c8068fb640c",
            "authors": [
                "Liyan Tang",
                "Grace Kim",
                "Xinyu Zhao",
                "Thom Lake",
                "Wenxuan Ding",
                "Fangcong Yin",
                "Prasann Singhal",
                "Manya Wadhwa",
                "Zeyu Leo Liu",
                "Zayne Sprague",
                "Ramya Namuduri",
                "Bodun Hu",
                "Juan Diego Rodriguez",
                "Puyuan Peng",
                "Greg Durrett"
            ],
            "affiliations": [
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13444.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#interpretability",
                    "#cv",
                    "#benchmark",
                    "#synthetic",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Раскрывая пробелы в визуальном мышлении ИИ при анализе диаграмм",
                    "desc": "Статья представляет новый тестовый набор данных ChartMuseum для оценки понимания диаграмм моделями компьютерного зрения и обработки естественного языка. Исследование показывает, что современные мультимодальные модели значительно уступают людям в задачах, требующих сложного визуального анализа диаграмм. Авторы обнаружили существенное снижение производительности моделей при увеличении визуальной сложности задач. ChartMuseum эффективно выявляет разрыв между возможностями моделей и людей в понимании диаграмм, особенно в задачах, требующих преимущественно визуального рассуждения."
                },
                "en": {
                    "title": "Bridging the Gap in Chart Understanding: Visual vs. Textual Reasoning",
                    "desc": "This paper addresses the challenges faced by large vision-language models (LVLMs) in understanding charts, highlighting their struggle with visual reasoning compared to textual reasoning. The authors present a case study using a synthetic dataset that shows a significant drop in model performance as visual complexity increases, while human performance remains stable. They introduce ChartMuseum, a new benchmark for Chart Question Answering (QA) that includes 1,162 expert-annotated questions designed to test both visual and textual reasoning. The results reveal a substantial performance gap between humans and models, with the best model achieving only 63% accuracy compared to 93% for humans, particularly struggling with questions that require visual reasoning."
                },
                "zh": {
                    "title": "图表理解：人类与模型的差距",
                    "desc": "图表理解对大型视觉语言模型（LVLMs）提出了独特的挑战，因为它需要复杂的文本和视觉推理能力的结合。当前的LVLM在这些技能之间存在显著的不平衡，尤其是在视觉推理方面表现不佳。我们通过一个合成数据集进行案例研究，发现随着视觉复杂性的增加，模型性能显著下降，而人类的表现则保持稳定。我们引入了ChartMuseum，这是一个新的图表问答基准，包含1162个专家注释的问题，旨在评估复杂的视觉和文本推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11932",
            "title": "Neuro-Symbolic Query Compiler",
            "url": "https://huggingface.co/papers/2505.11932",
            "abstract": "Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar rules and compiler design, to bridge this gap. It theoretically designs a minimal yet sufficient Backus-Naur Form (BNF) grammar G[q] to formalize complex queries. Unlike previous methods, this grammar maintains completeness while minimizing redundancy. Based on this, QCompiler includes a Query Expression Translator, a Lexical Syntax Parser, and a Recursive Descent Processor to compile queries into Abstract Syntax Trees (ASTs) for execution. The atomicity of the sub-queries in the leaf nodes ensures more precise document retrieval and response generation, significantly improving the RAG system's ability to address complex queries.",
            "score": 14,
            "issue_id": 3846,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "9445be4eff7e4edc",
            "authors": [
                "Yuyao Zhang",
                "Zhicheng Dou",
                "Xiaoxi Li",
                "Jiajie Jin",
                "Yongkang Wu",
                "Zhonghua Li",
                "Qi Ye",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Huawei Poisson Lab",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11932.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#rag",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Компиляция запросов для точного поиска в RAG-системах",
                    "desc": "QCompiler - это нейро-символическая система для улучшения понимания сложных запросов в RAG-системах. Она использует минимальную грамматику BNF для формализации запросов и компилирует их в абстрактные синтаксические деревья. Это позволяет более точно извлекать документы и генерировать ответы на сложные запросы с вложенными структурами. QCompiler включает в себя переводчик выражений запросов, лексический синтаксический анализатор и рекурсивный процессор."
                },
                "en": {
                    "title": "Enhancing Query Understanding in RAG Systems with QCompiler",
                    "desc": "This paper introduces QCompiler, a neuro-symbolic framework designed to enhance the understanding of complex search queries in Retrieval-Augmented Generation (RAG) systems. It utilizes a specially designed Backus-Naur Form (BNF) grammar to formalize these queries, ensuring that they are both complete and free of unnecessary complexity. QCompiler operates through a series of components, including a Query Expression Translator and a Lexical Syntax Parser, which work together to convert queries into Abstract Syntax Trees (ASTs). By focusing on the atomicity of sub-queries, QCompiler improves the accuracy of document retrieval and response generation for intricate queries."
                },
                "zh": {
                    "title": "提升RAG系统的复杂查询识别能力",
                    "desc": "本文提出了一种名为QCompiler的神经符号框架，旨在提高检索增强生成（RAG）系统对复杂查询的识别能力。QCompiler基于语言语法规则和编译器设计，设计了一种最小但足够的巴科斯-诺尔形式（BNF）语法G[q]，以形式化复杂查询。与以往方法不同，这种语法在保持完整性的同时，减少了冗余。通过将查询编译成抽象语法树（AST），QCompiler能够更精确地检索文档并生成响应，从而显著提升RAG系统处理复杂查询的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12346",
            "title": "SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy\n  Optimization",
            "url": "https://huggingface.co/papers/2505.12346",
            "abstract": "Large language models (LLMs) exhibit varying levels of confidence across input prompts (questions): some lead to consistent, semantically similar answers, while others yield diverse or contradictory outputs. This variation reflects LLM's uncertainty about the input prompt, a signal of how confidently the model understands a given problem. However, vanilla Group Relative Policy Optimization (GRPO) treats all prompts equally during policy updates, ignoring this important information about the model's knowledge boundaries. To address this limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which explicitly measures LLMs' uncertainty of the input prompts semantic entropy. Semantic entropy measures the diversity of meaning in multiple generated answers given a prompt and uses this to modulate the magnitude of policy updates. This uncertainty-aware training mechanism enables dynamic adjustment of policy update magnitudes based on question uncertainty. It allows more conservative updates on high-uncertainty questions while maintaining the original learning signal on confident ones. Experimental results on five mathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva 34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new state-of-the-art performance in average accuracy, validating the effectiveness of uncertainty-aware policy optimization.",
            "score": 13,
            "issue_id": 3857,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 мая",
                "en": "May 18",
                "zh": "5月18日"
            },
            "hash": "fbe6291cd68e8efb",
            "authors": [
                "Minghan Chen",
                "Guikun Chen",
                "Wenguan Wang",
                "Yi Yang"
            ],
            "affiliations": [
                "ReLER Lab, CCAI, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12346.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#math",
                    "#benchmark",
                    "#rl",
                    "#hallucinations"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Умное обучение ИИ: учитываем неуверенность модели",
                    "desc": "Статья представляет новый метод обучения больших языковых моделей под названием SEED-GRPO. Этот подход учитывает уверенность модели в ответах на различные вопросы, измеряя семантическую энтропию генерируемых ответов. SEED-GRPO модулирует величину обновлений политики в зависимости от неопределенности вопроса, что позволяет более консервативно обновлять параметры на сложных вопросах. Эксперименты на пяти математических бенчмарках показали, что SEED-GRPO достигает нового уровня точности, превосходящего современные методы."
                },
                "en": {
                    "title": "Enhancing LLM Training with Uncertainty Awareness",
                    "desc": "This paper introduces SEED-GRPO, an advanced method for training large language models (LLMs) that takes into account the model's uncertainty regarding input prompts. Traditional Group Relative Policy Optimization (GRPO) treats all prompts the same, which can overlook important variations in the model's confidence. SEED-GRPO incorporates semantic entropy, a measure of the diversity of answers generated for a prompt, to adjust how much the model learns from different questions. By applying this uncertainty-aware approach, the model can make more cautious updates for prompts it finds challenging, leading to improved performance on mathematical reasoning tasks."
                },
                "zh": {
                    "title": "基于不确定性的策略优化新方法",
                    "desc": "大型语言模型（LLMs）在处理输入提示时表现出不同的信心水平，有些提示产生一致且语义相似的答案，而另一些则产生多样或矛盾的输出。这种变化反映了模型对输入提示的不确定性，表明模型对特定问题的理解程度。传统的群体相对策略优化（GRPO）在策略更新时对所有提示一视同仁，忽略了模型知识边界的重要信息。为了解决这个问题，我们提出了SEED-GRPO（语义熵增强的GRPO），它通过测量输入提示的语义熵来显式考虑LLMs的不确定性，从而动态调整策略更新的幅度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12081",
            "title": "VisionReasoner: Unified Visual Perception and Reasoning via\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.12081",
            "abstract": "Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, a unified framework capable of reasoning and solving multiple visual perception tasks within a shared model. Specifically, by designing novel multi-object cognitive learning strategies and systematic task reformulation, VisionReasoner enhances its reasoning capabilities to analyze visual inputs, and addresses diverse perception tasks in a unified framework. The model generates a structured reasoning process before delivering the desired outputs responding to user queries. To rigorously assess unified visual perception capabilities, we evaluate VisionReasoner on ten diverse tasks spanning three critical domains: detection, segmentation, and counting. Experimental results show that VisionReasoner achieves superior performance as a unified model, outperforming Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg (segmentation), and 15.3% on CountBench (counting).",
            "score": 13,
            "issue_id": 3845,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "9b7953f88ae7653d",
            "authors": [
                "Yuqi Liu",
                "Tianyuan Qu",
                "Zhisheng Zhong",
                "Bohao Peng",
                "Shu Liu",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "SmartMore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12081.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Единая модель для многозадачного визуального восприятия",
                    "desc": "В статье представлен VisionReasoner - унифицированная модель для решения различных задач визуального восприятия. Модель использует новые стратегии когнитивного обучения с несколькими объектами и систематическое переформулирование задач для улучшения способностей к рассуждению. VisionReasoner генерирует структурированный процесс рассуждений перед выдачей ответов на запросы пользователей. Экспериментальные результаты показывают превосходство VisionReasoner над Qwen2.5VL в задачах обнаружения, сегментации и подсчета объектов."
                },
                "en": {
                    "title": "VisionReasoner: Unifying Visual Perception with Advanced Reasoning",
                    "desc": "This paper presents VisionReasoner, a unified framework designed to enhance visual perception tasks through advanced reasoning capabilities. It employs innovative multi-object cognitive learning strategies and reformulates tasks systematically to improve its performance across various visual challenges. The model processes visual inputs in a structured manner, allowing it to effectively respond to user queries. Evaluation results demonstrate that VisionReasoner significantly outperforms existing models in detection, segmentation, and counting tasks, showcasing its effectiveness as a comprehensive solution for visual perception."
                },
                "zh": {
                    "title": "统一视觉感知的推理能力",
                    "desc": "本文介绍了一种名为VisionReasoner的统一框架，能够在共享模型中处理多种视觉感知任务。通过设计新颖的多对象认知学习策略和系统的任务重构，VisionReasoner增强了其推理能力，以分析视觉输入并解决多样的感知任务。该模型在生成所需输出之前，会先进行结构化的推理过程，以响应用户查询。实验结果表明，VisionReasoner在检测、分割和计数等三个关键领域的十个任务上表现优异，超越了Qwen2.5VL。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.07704",
            "title": "Through the Looking Glass: Common Sense Consistency Evaluation of Weird\n  Images",
            "url": "https://huggingface.co/papers/2505.07704",
            "abstract": "Measuring how real images look is a complex task in artificial intelligence research. For example, an image of a boy with a vacuum cleaner in a desert violates common sense. We introduce a novel method, which we call Through the Looking Glass (TLG), to assess image common sense consistency using Large Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging LVLMs to extract atomic facts from these images, we obtain a mix of accurate facts. We proceed by fine-tuning a compact attention-pooling classifier over encoded atomic facts. Our TLG has achieved a new state-of-the-art performance on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning component.",
            "score": 13,
            "issue_id": 3862,
            "pub_date": "2025-05-12",
            "pub_date_card": {
                "ru": "12 мая",
                "en": "May 12",
                "zh": "5月12日"
            },
            "hash": "9fc123da51142c1e",
            "authors": [
                "Elisei Rykov",
                "Kseniia Petrushina",
                "Kseniia Titova",
                "Anton Razzhigaev",
                "Alexander Panchenko",
                "Vasily Konovalov"
            ],
            "affiliations": [
                "AIRI",
                "MTS AI",
                "Moscow Institute of Physics and Technology",
                "Skoltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.07704.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#interpretability",
                    "#cv",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Новый взгляд на здравый смысл в компьютерном зрении",
                    "desc": "Исследователи представили новый метод оценки здравого смысла в изображениях под названием Through the Looking Glass (TLG). Метод использует крупные мультимодальные модели (LVLMs) и энкодер на основе трансформеров для извлечения атомарных фактов из изображений. Затем эти факты обрабатываются с помощью компактного классификатора с пулингом внимания. TLG достиг нового уровня производительности на датасетах WHOOPS! и WEIRD."
                },
                "en": {
                    "title": "Assessing Image Realism with Through the Looking Glass (TLG)",
                    "desc": "This paper presents a new method called Through the Looking Glass (TLG) for evaluating the common sense consistency of images using Large Vision-Language Models (LVLMs). The approach involves extracting atomic facts from images with a Transformer-based encoder, which helps in understanding the context of the images better. By fine-tuning a compact attention-pooling classifier on these encoded facts, TLG improves the accuracy of image assessments. The method has set a new benchmark in performance on the WHOOPS! and WEIRD datasets, demonstrating its effectiveness in measuring image realism."
                },
                "zh": {
                    "title": "透过镜子：图像常识一致性的新方法",
                    "desc": "在人工智能研究中，评估真实图像的外观是一项复杂的任务。我们提出了一种新方法，称为透过镜子（TLG），用于评估图像的一致性与常识。该方法利用大型视觉语言模型（LVLMs）提取图像中的基本事实，并通过对这些事实进行编码来获得准确的信息。我们的TLG在WHOOPS!和WEIRD数据集上实现了新的最先进性能，同时使用了紧凑的微调组件。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13180",
            "title": "ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2505.13180",
            "abstract": "Integrating Large Language Models with symbolic planners is a promising direction for obtaining verifiable and grounded plans compared to planning in natural language, with recent works extending this idea to visual domains using Vision-Language Models (VLMs). However, rigorous comparison between VLM-grounded symbolic approaches and methods that plan directly with a VLM has been hindered by a lack of common environments, evaluation protocols and model coverage. We introduce ViPlan, the first open-source benchmark for Visual Planning with symbolic predicates and VLMs. ViPlan features a series of increasingly challenging tasks in two domains: a visual variant of the classic Blocksworld planning problem and a simulated household robotics environment. We benchmark nine open-source VLM families across multiple sizes, along with selected closed models, evaluating both VLM-grounded symbolic planning and using the models directly to propose actions. We find symbolic planning to outperform direct VLM planning in Blocksworld, where accurate image grounding is crucial, whereas the opposite is true in the household robotics tasks, where commonsense knowledge and the ability to recover from errors are beneficial. Finally, we show that across most models and methods, there is no significant benefit to using Chain-of-Thought prompting, suggesting that current VLMs still struggle with visual reasoning.",
            "score": 11,
            "issue_id": 3851,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "333ba599d7e29ff8",
            "authors": [
                "Matteo Merler",
                "Nicola Dainese",
                "Minttu Alakuijala",
                "Giovanni Bonetta",
                "Pietro Ferrazzi",
                "Yu Tian",
                "Bernardo Magnini",
                "Pekka Marttinen"
            ],
            "affiliations": [
                "Department of Computer Science, Aalto University",
                "Department of Mathematics, Università degli Studi di Padova",
                "Fondazione Bruno Kessler"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13180.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#cv",
                    "#games",
                    "#reasoning",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ViPlan: новый бенчмарк для сравнения символьного и прямого визуального планирования",
                    "desc": "Статья представляет ViPlan - первый открытый бенчмарк для визуального планирования с использованием символьных предикатов и моделей визуально-языкового восприятия (VLM). Авторы сравнивают эффективность символьного планирования на основе VLM и прямого планирования с помощью VLM в двух доменах: визуальном варианте классической задачи Blocksworld и симулированной среде домашней робототехники. Исследование показывает, что символьное планирование превосходит прямое планирование VLM в Blocksworld, где критически важна точная привязка к изображению, в то время как в задачах домашней робототехники наблюдается обратная ситуация. Авторы также обнаружили, что использование метода Chain-of-Thought не дает значительных преимуществ для большинства моделей и методов."
                },
                "en": {
                    "title": "ViPlan: Bridging Symbolic Planning and Vision-Language Models for Better Visual Reasoning",
                    "desc": "This paper discusses the integration of Large Language Models (LLMs) with symbolic planners to create more reliable and understandable plans in visual tasks. It introduces ViPlan, an open-source benchmark designed to evaluate various planning methods using Vision-Language Models (VLMs) in two distinct environments: Blocksworld and household robotics. The study compares the performance of symbolic planning against direct VLM planning, revealing that symbolic methods excel in scenarios requiring precise image grounding, while VLMs perform better in tasks that demand commonsense reasoning. Additionally, the findings indicate that current VLMs do not significantly benefit from Chain-of-Thought prompting, highlighting ongoing challenges in visual reasoning capabilities."
                },
                "zh": {
                    "title": "视觉规划的新基准：ViPlan",
                    "desc": "本论文介绍了ViPlan，这是第一个用于视觉规划的开源基准，结合了符号规划和视觉语言模型（VLM）。我们设计了一系列逐渐增加难度的任务，包括经典的Blocksworld规划问题和模拟家庭机器人环境。通过对九个开源VLM模型进行基准测试，我们发现符号规划在Blocksworld中表现优于直接使用VLM进行规划，而在家庭机器人任务中则相反。最后，我们的研究表明，当前的VLM在视觉推理方面仍然存在困难，使用Chain-of-Thought提示并没有显著提高性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11855",
            "title": "When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification\n  of Scientific Research",
            "url": "https://huggingface.co/papers/2505.11855",
            "abstract": "Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore a complementary application: using LLMs as verifiers to automate the academic verification of scientific manuscripts. To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.",
            "score": 8,
            "issue_id": 3849,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "8432e529923dacc5",
            "authors": [
                "Guijin Son",
                "Jiwoo Hong",
                "Honglu Fan",
                "Heejeong Nam",
                "Hyunwoo Ko",
                "Seungwon Lim",
                "Jinyeop Song",
                "Jinha Choi",
                "Gonçalo Paulo",
                "Youngjae Yu",
                "Stella Biderman"
            ],
            "affiliations": [
                "Boeing Korea",
                "EleutherAI",
                "KAIST",
                "MIT",
                "OneLineAI",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11855.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#science",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Большие языковые модели пока не готовы быть научными рецензентами",
                    "desc": "Статья представляет SPOT - набор данных из 83 опубликованных научных работ с 91 значительной ошибкой, приведшей к опечаткам или отзыву. Авторы оценивают способность современных больших языковых моделей (LLM) обнаруживать эти ошибки в качестве автоматических верификаторов научных рукописей. Результаты показывают, что даже лучшие модели достигают лишь 21.1% полноты и 6.1% точности, демонстрируя ненадежность и непоследовательность. Качественный анализ выявляет, что ошибки моделей напоминают студенческие заблуждения, указывая на значительный разрыв между текущими возможностями LLM и требованиями к надежной ИИ-assisted верификации научных работ."
                },
                "en": {
                    "title": "Bridging the Gap: LLMs as Verifiers in Scientific Discovery",
                    "desc": "This paper investigates the use of large language models (LLMs) as tools for verifying scientific manuscripts, rather than just generating content. The authors introduce a dataset called SPOT, which includes published papers with significant errors that could lead to errata or retraction. They evaluate various state-of-the-art LLMs on this dataset and find that their performance is lacking, with low recall and precision rates. The study concludes that current LLMs are not yet reliable enough for academic verification, as they often make errors similar to those of novice students."
                },
                "zh": {
                    "title": "大型语言模型在学术验证中的挑战",
                    "desc": "这篇论文探讨了大型语言模型（LLMs）在学术验证中的应用，提出了一个名为SPOT的数据集，包含83篇已发表论文和91个显著错误。研究发现，当前最先进的LLMs在识别错误方面的表现不佳，最高召回率仅为21.1%，精确率为6.1%。此外，模型的置信度普遍较低，且在多次独立测试中，模型很少能重新发现相同的错误。通过与领域专家的定性分析，发现即使是最强的模型也会犯类似学生级别的误解错误，显示出当前LLMs在可靠的学术验证中存在显著差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12849",
            "title": "Accelerate TarFlow Sampling with GS-Jacobi Iteration",
            "url": "https://huggingface.co/papers/2505.12849",
            "abstract": "Image generation models have achieved widespread applications. As an instance, the TarFlow model combines the transformer architecture with Normalizing Flow models, achieving state-of-the-art results on multiple benchmarks. However, due to the causal form of attention requiring sequential computation, TarFlow's sampling process is extremely slow. In this paper, we demonstrate that through a series of optimization strategies, TarFlow sampling can be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as GS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow model have varying importance: a small number of blocks play a major role in image generation tasks, while other blocks contribute relatively little; some blocks are sensitive to initial values and prone to numerical overflow, while others are relatively robust. Based on these two characteristics, we propose the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM is used to identify whether a TarFlow block is \"simple\" (converges in few iterations) or \"tough\" (requires more iterations); IGM is used to evaluate whether the initial value of the iteration is good. Experiments on four TarFlow models demonstrate that GS-Jacobi sampling can significantly enhance sampling efficiency while maintaining the quality of generated images (measured by FID), achieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in Img64uncond, and 2.51x in Img64cond without degrading FID scores or sample quality. Code and checkpoints are accessible on https://github.com/encoreus/GS-Jacobi_for_TarFlow",
            "score": 7,
            "issue_id": 3846,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "191f5a409cc6b32e",
            "authors": [
                "Ben Liu",
                "Zhen Qin"
            ],
            "affiliations": [
                "TapTap, Shanghai, China",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12849.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#open_source",
                    "#cv",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение генерации изображений в TarFlow с помощью оптимизированных итераций",
                    "desc": "Данная статья представляет метод ускорения процесса сэмплирования в модели TarFlow для генерации изображений. Авторы применяют итерационный метод Гаусса-Зейделя-Якоби и вводят две метрики: Convergence Ranking Metric (CRM) и Initial Guessing Metric (IGM). CRM используется для определения сложности блоков TarFlow, а IGM оценивает качество начальных значений для итераций. Эксперименты показали значительное ускорение сэмплирования (до 5.32 раз) без ухудшения качества генерируемых изображений."
                },
                "en": {
                    "title": "Accelerating TarFlow: Faster Sampling without Quality Loss",
                    "desc": "This paper presents an optimization strategy for the TarFlow model, which combines transformer architecture with Normalizing Flow for image generation. The authors introduce the Gauss-Seidel-Jacobi (GS-Jacobi) iteration method to accelerate the slow sampling process of TarFlow. They identify that certain blocks within the model are more critical for image generation and propose metrics to evaluate their performance: the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM). Experimental results show that the GS-Jacobi method significantly improves sampling speed while preserving image quality, achieving notable speed-ups across various benchmarks."
                },
                "zh": {
                    "title": "加速TarFlow采样，提升图像生成效率",
                    "desc": "图像生成模型在多个应用中取得了显著进展。TarFlow模型结合了变换器架构和归一化流模型，在多个基准测试中达到了最先进的结果。然而，由于因果注意力的顺序计算，TarFlow的采样过程非常缓慢。本文通过优化策略，利用高斯-赛德尔-雅可比迭代方法显著加速了TarFlow的采样过程，同时保持生成图像的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13388",
            "title": "R3: Robust Rubric-Agnostic Reward Models",
            "url": "https://huggingface.co/papers/2505.13388",
            "abstract": "Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce R3, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. R3 enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3",
            "score": 6,
            "issue_id": 3860,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "2589c2eb838a62f9",
            "authors": [
                "David Anugraha",
                "Zilu Tang",
                "Lester James V. Miranda",
                "Hanyang Zhao",
                "Mohammad Rifqi Farhansyah",
                "Garry Kuwanto",
                "Derry Wijaya",
                "Genta Indra Winata"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "Boston University",
                "Capital One",
                "Columbia University",
                "Institut Teknologi Bandung",
                "Monash Indonesia",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13388.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#optimization",
                    "#benchmark",
                    "#alignment",
                    "#interpretability",
                    "#rlhf"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "R3: Прозрачное и гибкое моделирование наград для языковых моделей",
                    "desc": "R3 - это новая система моделирования наград для языковых моделей. Она обеспечивает лучшую интерпретируемость и контролируемость по сравнению с существующими подходами. R3 агностична к конкретным рубрикам и может обобщаться на различные аспекты оценки. Система предоставляет обоснованные оценки, что повышает прозрачность и гибкость при оценке языковых моделей."
                },
                "en": {
                    "title": "R3: A New Era in Reward Modeling for Language Alignment",
                    "desc": "This paper presents R3, a new framework for reward modeling that aims to improve the alignment of language model outputs with human preferences. Unlike traditional reward models that focus on narrow objectives, R3 is designed to be rubric-agnostic and generalizable, allowing it to adapt to various evaluation criteria. It also enhances interpretability by providing reasoned score assignments, making it easier to understand how scores are derived. Overall, R3 promotes a more transparent and flexible approach to evaluating language models, ensuring they align better with diverse human values."
                },
                "zh": {
                    "title": "R3：提升语言模型对人类价值的对齐能力",
                    "desc": "奖励模型在将语言模型的输出与人类偏好对齐中至关重要，但现有方法往往缺乏可控性和可解释性。这些模型通常针对狭窄的目标进行优化，限制了它们在更广泛下游任务中的通用性。此外，它们的标量输出在没有上下文推理的情况下难以解释。为了解决这些限制，我们提出了R3，这是一种新颖的奖励建模框架，具有与评分标准无关、跨评估维度的可推广性，并提供可解释的推理评分分配。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12058",
            "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset\n  Generation & Smoke-Tests for Continuous LLM Evaluation",
            "url": "https://huggingface.co/papers/2505.12058",
            "abstract": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost. Born out of the tight feedback-loop demands building the Comet Opik prompt-optimization SDK, where waiting on heavyweight benchmarks breaks developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with a tiny synthetic-data generator pypi package built on provider-agnostic LiteLLM. The generator lets practitioners mint their own tiny packs in any language, domain, or difficulty, while ten ready-made packs already cover Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian, Spanish, and Turkish. Every dataset ships with Croissant metadata and plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so teams can drop deterministic micro-benchmarks directly into pull-request gates, prompt-engineering loops, and production dashboards without touching GPU budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet reliably flags prompt-template errors, tokenizer drift, and fine-tuning side-effects long before full-scale suites like MMLU or BIG-Bench would finish configuring. The entire framework is released to accelerate continuous, resource-efficient quality assurance across the generative-AI ecosystem.",
            "score": 6,
            "issue_id": 3853,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "db0c871b64bd6d89",
            "authors": [
                "Vincent Koc"
            ],
            "affiliations": [
                "Comet ML, Inc. New York, NY, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12058.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#dataset",
                    "#synthetic",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Молниеносное тестирование языковых моделей для всех",
                    "desc": "TQB++ представляет собой легковесный многоязычный набор тестов для быстрой проверки языковых моделей. Он включает в себя золотой стандарт из 52 вопросов на английском языке и генератор синтетических данных для создания тестов на других языках. TQB++ позволяет быстро и эффективно тестировать модели, не требуя значительных вычислительных ресурсов. Этот инструмент особенно полезен для непрерывной интеграции и быстрой оценки качества моделей в процессе разработки."
                },
                "en": {
                    "title": "Quick and Efficient Testing for Language Models",
                    "desc": "Tiny QA Benchmark++ (TQB++) is a lightweight, multilingual testing suite designed for large-language-model (LLM) pipelines, providing a quick and cost-effective way to ensure model quality. It includes a small set of gold-standard tests and a synthetic data generator that allows users to create custom test packs in various languages and domains. TQB++ integrates easily with existing tools, enabling developers to incorporate micro-benchmarks into their workflows without significant resource expenditure. This framework aims to enhance continuous quality assurance in generative AI by quickly identifying issues like prompt errors and tokenizer drift before more extensive testing is conducted."
                },
                "zh": {
                    "title": "轻量级多语言测试，提升AI质量保障",
                    "desc": "Tiny QA Benchmark++（TQB++）是一个超轻量级的多语言测试套件，旨在为大型语言模型（LLM）提供快速的单元测试数据集，运行时间短且成本低。它结合了一个52项的英语金标准集和一个基于LiteLLM的合成数据生成器，允许用户生成适合任何语言、领域或难度的小数据包。TQB++提供了十个现成的数据包，覆盖多种语言，并且每个数据集都附带了元数据和可直接使用的文件，方便集成到现有的开发流程中。这个框架的设计旨在加速生成式人工智能生态系统中的持续、资源高效的质量保证。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13437",
            "title": "FinePhys: Fine-grained Human Action Generation by Explicitly\n  Incorporating Physical Laws for Effective Skeletal Guidance",
            "url": "https://huggingface.co/papers/2505.13437",
            "abstract": "Despite significant advances in video generation, synthesizing physically plausible human actions remains a persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as \"switch leap with 0.5 turn\" poses substantial difficulties for current methods, often yielding unsatisfactory results. To bridge this gap, we propose FinePhys, a Fine-grained human action generation framework that incorporates Physics to obtain effective skeletal guidance. Specifically, FinePhys first estimates 2D poses in an online manner and then performs 2D-to-3D dimension lifting via in-context learning. To mitigate the instability and limited interpretability of purely data-driven 3D poses, we further introduce a physics-based motion re-estimation module governed by Euler-Lagrange equations, calculating joint accelerations via bidirectional temporal updating. The physically predicted 3D poses are then fused with data-driven ones, offering multi-scale 2D heatmap guidance for the diffusion process. Evaluated on three fine-grained action subsets from FineGym (FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms competitive baselines. Comprehensive qualitative results further demonstrate FinePhys's ability to generate more natural and plausible fine-grained human actions.",
            "score": 4,
            "issue_id": 3847,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "28a08dbfb09c6639",
            "authors": [
                "Dian Shao",
                "Mingfei Shi",
                "Shengda Xu",
                "Haodong Chen",
                "Yongle Huang",
                "Binglu Wang"
            ],
            "affiliations": [
                "School of Astronautics, Northwestern Polytechnical University, Xian, China",
                "School of Automation, Northwestern Polytechnical University, Xian, China",
                "School of Software, Northwestern Polytechnical University, Xian, China",
                "Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13437.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "🤸",
                "ru": {
                    "title": "Точная генерация движений человека с помощью физического моделирования",
                    "desc": "Статья представляет FinePhys - фреймворк для генерации точных движений человека с использованием физических моделей. Система сначала оценивает 2D позы, затем преобразует их в 3D с помощью обучения в контексте. Далее применяется физическое моделирование на основе уравнений Эйлера-Лагранжа для улучшения 3D поз. Полученные физически корректные позы комбинируются с данными для создания многомасштабных 2D тепловых карт, используемых в процессе диффузии."
                },
                "en": {
                    "title": "Bridging Physics and Data for Realistic Human Action Generation",
                    "desc": "This paper presents FinePhys, a framework designed to improve the generation of fine-grained human actions in videos by integrating physics-based modeling. It addresses the challenges of synthesizing complex movements, such as gymnastics routines, by first estimating 2D poses and then converting them to 3D using in-context learning. To enhance the stability and interpretability of the generated poses, FinePhys employs a motion re-estimation module based on Euler-Lagrange equations, which calculates joint accelerations through bidirectional temporal updates. The combination of physics-based predictions with data-driven approaches results in more realistic and coherent human actions, as demonstrated by superior performance on benchmark datasets."
                },
                "zh": {
                    "title": "FinePhys：物理驱动的细粒度人类动作生成框架",
                    "desc": "尽管视频生成技术取得了显著进展，但合成物理上合理的人类动作仍然是一个持续的挑战，尤其是在建模细粒度语义和复杂时间动态方面。本文提出了FinePhys，一个细粒度人类动作生成框架，结合物理学以获得有效的骨骼指导。FinePhys首先以在线方式估计2D姿势，然后通过上下文学习进行2D到3D的维度提升。通过引入基于物理的运动重新估计模块，FinePhys能够生成更自然和合理的细粒度人类动作。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10238",
            "title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation",
            "url": "https://huggingface.co/papers/2505.10238",
            "abstract": "Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks a significant step forward in this field and opens a new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are on: https://github.com/DINGYANB/MTVCrafter.",
            "score": 4,
            "issue_id": 3851,
            "pub_date": "2025-05-15",
            "pub_date_card": {
                "ru": "15 мая",
                "en": "May 15",
                "zh": "5月15日"
            },
            "hash": "5d8979fc79f9bd4e",
            "authors": [
                "Yanbo Ding",
                "Xirui Hu",
                "Zhizhi Guo",
                "Yali Wang"
            ],
            "affiliations": [
                "chinatelecom.cn",
                "siat.ac.cn",
                "xjtu.edu.cn"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10238.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#video"
                ],
                "emoji": "🕺",
                "ru": {
                    "title": "Революция в анимации человека: от 2D к 4D движению",
                    "desc": "MTVCrafter - это новый подход к анимации изображений человека, использующий токенизацию 4D движения вместо 2D-изображений поз. Метод вводит 4DMoT для квантования 3D последовательностей движения в токены и MV-DiT для эффективного использования этих токенов в анимации. MTVCrafter превосходит существующие методы по метрике FID-VID на 65% и хорошо обобщается на разнообразных персонажей в открытом мире. Этот подход открывает новое направление в генерации видео с управлением позой человека."
                },
                "en": {
                    "title": "Revolutionizing Human Animation with 4D Motion Tokens",
                    "desc": "This paper presents MTVCrafter, a novel framework for human image animation that utilizes raw 3D motion sequences instead of traditional 2D-rendered pose images. By introducing 4DMoT, the framework quantizes 3D motion into 4D motion tokens, which provide enhanced spatio-temporal cues and greater flexibility in animation control. Additionally, MV-DiT employs motion-aware attention mechanisms to effectively utilize these motion tokens, allowing for more expressive human image generation in complex 3D environments. The results demonstrate that MTVCrafter achieves state-of-the-art performance, significantly improving generalization across various character types and styles."
                },
                "zh": {
                    "title": "开创4D运动标记的人像动画新方向",
                    "desc": "人像动画技术在数字人类应用中越来越受到关注，但现有方法主要依赖于2D渲染的姿态图像，限制了其泛化能力并丢失了重要的3D信息。为了解决这个问题，我们提出了MTVCrafter框架，它直接建模原始的3D运动序列（即4D运动），并引入了4DMoT（4D运动标记器）将3D运动序列量化为4D运动标记。与2D渲染的姿态图像相比，4D运动标记提供了更强的时空线索，避免了姿态图像与角色之间严格的像素级对齐，从而实现了更灵活和解耦的控制。我们的实验表明，MTVCrafter在多种风格和场景下对不同的开放世界角色具有良好的泛化能力，取得了最先进的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12996",
            "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.12996",
            "abstract": "In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance.",
            "score": 3,
            "issue_id": 3850,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "56ff8af5ab05144f",
            "authors": [
                "Jiaan Wang",
                "Fandong Meng",
                "Jie Zhou"
            ],
            "affiliations": [
                "Pattern Recognition Center, WeChat AI, Tencent Inc"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12996.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multilingual",
                    "#rl",
                    "#low_resource",
                    "#machine_translation"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Революция в машинном переводе: от одноязычного к многоязычному совершенству",
                    "desc": "Статья описывает новый метод моделирования вознаграждения для обучения с подкреплением в нейронном машинном переводе. Авторы используют сравнение результатов перевода с сильной моделью рассуждений (LRM) для формирования вознаграждений. Эксперименты показывают превосходство этого метода, достигая нового уровня качества в литературном переводе. Метод успешно расширен на многоязычный перевод для 11 языков, демонстрируя впечатляющие результаты в 90 направлениях перевода."
                },
                "en": {
                    "title": "Revolutionizing Multilingual Translation with Advanced Reward Modeling",
                    "desc": "This paper discusses advancements in large reasoning models (LRMs) for neural machine translation (MT), particularly focusing on improving translation capabilities through reinforcement learning (RL). The authors introduce a novel reward modeling method that evaluates translation quality by comparing outputs from a policy MT model against a powerful LRM, DeepSeek-R1-671B. Their approach not only enhances performance in literary translation but also extends to 11 languages, achieving state-of-the-art results and surpassing existing LRMs. The lightweight reward modeling allows for effective transfer of translation skills across multiple languages, demonstrating significant improvements in multilingual MT performance."
                },
                "zh": {
                    "title": "强化学习助力多语言翻译新突破",
                    "desc": "近年来，大型推理模型（LRMs）如OpenAI-o1和DeepSeek-R1在复杂问题上展现了出色的能力，尤其是在数学和编程方面。一些开创性研究尝试将LRMs的成功应用于神经机器翻译（MT），并通过强化学习（RL）构建具有深度推理能力的MT模型。尽管取得了一些进展，但这些研究主要集中在高资源语言上，如英语和中文，其他语言的表现仍不明确。此外，我们设计了一种新的奖励建模方法，通过与强大的LRM（如DeepSeek-R1-671B）比较翻译结果，为MT模型提供奖励，从而充分发挥强化学习的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12120",
            "title": "HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for\n  Computational Pathology",
            "url": "https://huggingface.co/papers/2505.12120",
            "abstract": "Recent advancements in Digital Pathology (DP), particularly through artificial intelligence and Foundation Models, have underscored the importance of large-scale, diverse, and richly annotated datasets. Despite their critical role, publicly available Whole Slide Image (WSI) datasets often lack sufficient scale, tissue diversity, and comprehensive clinical metadata, limiting the robustness and generalizability of AI models. In response, we introduce the HISTAI dataset, a large, multimodal, open-access WSI collection comprising over 60,000 slides from various tissue types. Each case in the HISTAI dataset is accompanied by extensive clinical metadata, including diagnosis, demographic information, detailed pathological annotations, and standardized diagnostic coding. The dataset aims to fill gaps identified in existing resources, promoting innovation, reproducibility, and the development of clinically relevant computational pathology solutions. The dataset can be accessed at https://github.com/HistAI/HISTAI.",
            "score": 3,
            "issue_id": 3855,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "4774ecdbd93d4a7b",
            "authors": [
                "Dmitry Nechaev",
                "Alexey Pchelnikov",
                "Ekaterina Ivanova"
            ],
            "affiliations": [
                "HistAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12120.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#healthcare",
                    "#dataset",
                    "#data"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "HISTAI: Крупномасштабный набор данных для прорыва в цифровой патологии",
                    "desc": "Статья представляет новый набор данных HISTAI для цифровой патологии. Он содержит более 60 000 изображений срезов тканей различных типов с подробными клиническими метаданными. Каждый случай сопровождается диагнозом, демографической информацией, патологическими аннотациями и стандартизированным диагностическим кодированием. Набор данных HISTAI призван устранить пробелы в существующих ресурсах и способствовать разработке клинически значимых решений в области вычислительной патологии."
                },
                "en": {
                    "title": "Empowering AI in Pathology with the HISTAI Dataset",
                    "desc": "This paper presents the HISTAI dataset, a significant advancement in Digital Pathology that addresses the limitations of existing Whole Slide Image (WSI) datasets. It comprises over 60,000 slides from diverse tissue types, ensuring a large-scale and multimodal resource for training AI models. Each slide is enriched with comprehensive clinical metadata, including diagnosis and detailed annotations, which enhances the dataset's utility for research and clinical applications. By providing this open-access resource, the authors aim to foster innovation and improve the robustness of AI solutions in computational pathology."
                },
                "zh": {
                    "title": "HISTAI数据集：推动数字病理学的创新与发展",
                    "desc": "本论文介绍了HISTAI数据集，这是一个大型的多模态开放获取的全幻灯片图像（WSI）集合，包含超过60,000张来自不同组织类型的幻灯片。该数据集配备了丰富的临床元数据，包括诊断、人口统计信息、详细的病理注释和标准化的诊断编码。HISTAI数据集旨在填补现有资源中的空白，促进创新、可重复性以及临床相关计算病理解决方案的发展。通过提供多样化和丰富注释的数据，HISTAI将增强人工智能模型的鲁棒性和普适性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11497",
            "title": "QVGen: Pushing the Limit of Quantized Video Generative Models",
            "url": "https://huggingface.co/papers/2505.11497",
            "abstract": "Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules (Phi) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of Phi, we propose a rank-decay strategy that progressively eliminates Phi. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization gamma to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across 4 state-of-the-art (SOTA) video DMs, with parameter sizes ranging from 1.3B sim14B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of +25.28 in Dynamic Degree and +8.43 in Scene Consistency on VBench.",
            "score": 3,
            "issue_id": 3851,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "48866ba1d6ad838b",
            "authors": [
                "Yushi Huang",
                "Ruihao Gong",
                "Jing Liu",
                "Yifu Ding",
                "Chengtao Lv",
                "Haotong Qin",
                "Jun Zhang"
            ],
            "affiliations": [
                "Beihang University",
                "ETH Zürich",
                "Hong Kong University of Science and Technology",
                "Monash University",
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11497.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#inference",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Революция в квантовании видео-диффузионных моделей",
                    "desc": "QVGen - это новая система обучения с учетом квантования для высокопроизводительных и эффективных при выводе видео-диффузионных моделей при экстремально низкобитном квантовании. Система вводит вспомогательные модули для уменьшения ошибок квантования и улучшения сходимости. Используется стратегия уменьшения ранга для устранения накладных расходов при выводе. Эксперименты показывают, что QVGen достигает качества полной точности при 4-битных настройках и значительно превосходит существующие методы."
                },
                "en": {
                    "title": "Efficient Video Synthesis with Low-Bit Quantization",
                    "desc": "This paper introduces QVGen, a new framework for quantization-aware training (QAT) specifically designed for video diffusion models (DMs). It addresses the challenges of high computational and memory requirements by enabling efficient video synthesis at extremely low-bit quantization levels, such as 4-bit. The authors demonstrate that reducing the gradient norm is crucial for effective convergence in QAT and propose auxiliary modules to minimize quantization errors. Additionally, they implement a rank-decay strategy to eliminate inference overhead while maintaining performance, achieving significant improvements over existing methods in video quality metrics."
                },
                "zh": {
                    "title": "量化感知训练，提升视频合成效率！",
                    "desc": "视频扩散模型（DMs）在高质量视频合成方面取得了显著进展，但其计算和内存需求高，限制了实际应用。本文提出了一种新颖的量化感知训练（QAT）框架QVGen，旨在在极低位量化（如4位或更低）下实现高性能和高效推理。我们通过理论分析表明，降低梯度范数对QAT的收敛至关重要，并引入辅助模块（Phi）来减小量化误差，从而显著提高收敛性。通过逐步消除Phi的推理开销，我们的实验表明QVGen在4位设置下首次实现了与全精度相当的质量，并显著超越了现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11484",
            "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2505.11484",
            "abstract": "Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.",
            "score": 3,
            "issue_id": 3851,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "4a502c14d07f6e6c",
            "authors": [
                "Yige Xu",
                "Xu Guo",
                "Zhiwei Zeng",
                "Chunyan Miao"
            ],
            "affiliations": [
                "Alibaba-NTU Global e-Sustainability CorpLab (ANGEL)",
                "College of Computing and Data Science, Nanyang Technological University, Singapore",
                "Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly",
                "KTH Royal Institute of Technology, Sweden"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11484.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#benchmark",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение рассуждений ИИ через разнообразные латентные мысли",
                    "desc": "Статья представляет метод SoftCoT++, улучшающий рассуждения языковых моделей в непрерывном латентном пространстве. В отличие от дискретных методов, SoftCoT++ позволяет исследовать разнообразные пути рассуждений путем возмущения латентных представлений. Метод применяет контрастивное обучение для повышения разнообразия мягких представлений мыслей. Эксперименты показывают, что SoftCoT++ превосходит базовый SoftCoT и хорошо сочетается с другими методами масштабирования."
                },
                "en": {
                    "title": "Enhancing Reasoning with Diverse Latent Thoughts",
                    "desc": "This paper introduces SoftCoT++, an advanced method for Test-Time Scaling (TTS) that enhances reasoning performance in machine learning models. Unlike traditional TTS methods that work with discrete tokens, SoftCoT++ leverages continuous latent space to improve the quality of reasoning by allowing for diverse exploration of thought paths. By perturbing latent thoughts with specialized initial tokens and using contrastive learning, the method promotes diversity in soft thought representations. Experimental results show that SoftCoT++ significantly outperforms previous methods, demonstrating its effectiveness across various reasoning benchmarks and compatibility with existing scaling techniques."
                },
                "zh": {
                    "title": "多样化思维路径的探索新方法",
                    "desc": "测试时扩展（TTS）是一种在推理过程中通过分配额外计算来提高推理性能的方法，而不改变模型参数。最近的研究表明，在连续潜在空间中进行思考可以进一步提升推理性能。与离散解码不同，连续空间中的潜在表示是固定的，这限制了多样化的探索。为了解决这个问题，我们提出了SoftCoT++，通过扰动潜在思维并应用对比学习来促进思维路径的多样性，从而扩展了SoftCoT在测试时扩展范式中的应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13840",
            "title": "EfficientLLM: Efficiency in Large Language Models",
            "url": "https://huggingface.co/papers/2505.13840",
            "abstract": "Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first comprehensive empirical study evaluating efficiency techniques for LLMs at scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our study systematically explores three key axes: (1) architecture pretraining (efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts (MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and (3) inference (quantization methods: int4, float16). We define six fine-grained metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy Consumption, Compression Rate) to capture hardware saturation, latency-throughput balance, and carbon cost. Evaluating over 100 model-technique pairs (0.5B-72B parameters), we derive three core insights: (i) Efficiency involves quantifiable trade-offs: no single method is universally optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by 40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5% accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal memory-latency trade-offs for constrained devices, MLA achieves lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency only beyond 14B parameters. (iii) Techniques generalize across modalities: we extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides essential guidance for researchers and engineers navigating the efficiency-performance landscape of next-generation foundation models.",
            "score": 2,
            "issue_id": 3867,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 мая",
                "en": "May 20",
                "zh": "5月20日"
            },
            "hash": "01e23584249e610e",
            "authors": [
                "Zhengqing Yuan",
                "Weixiang Sun",
                "Yixin Liu",
                "Huichi Zhou",
                "Rong Zhou",
                "Yiyang Li",
                "Zheyuan Zhang",
                "Wei Song",
                "Yue Huang",
                "Haolong Jia",
                "Keerthiram Murugesan",
                "Yu Wang",
                "Lifang He",
                "Jianfeng Gao",
                "Lichao Sun",
                "Yanfang Ye"
            ],
            "affiliations": [
                "Imperial College London",
                "International Business Machines Corporation (IBM)",
                "Lehigh University",
                "Microsoft Research",
                "Rutgers University",
                "University of Illinois Chicago",
                "University of Notre Dame"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13840.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#open_source",
                    "#multimodal",
                    "#inference",
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "EfficientLLM: Компромиссы эффективности в мире больших языковых моделей",
                    "desc": "Статья представляет EfficientLLM - новый бенчмарк и первое комплексное эмпирическое исследование методов повышения эффективности больших языковых моделей (LLM) в масштабе. Исследование систематически изучает три ключевых направления: архитектуру предобучения, тонкую настройку и инференс, используя шесть метрик для оценки аппаратной утилизации, баланса латентности и пропускной способности, а также углеродных затрат. Оценивая более 100 пар модель-метод, авторы выявляют количественные компромиссы эффективности, зависимость оптимумов от задачи и масштаба, а также обобщаемость техник на другие модальности. EfficientLLM предоставляет важные рекомендации для исследователей и инженеров в области эффективности и производительности моделей нового поколения."
                },
                "en": {
                    "title": "Optimizing Efficiency in Large Language Models",
                    "desc": "This paper presents EfficientLLM, a benchmark designed to evaluate the efficiency of Large Language Models (LLMs) at scale. It systematically investigates various techniques across architecture pretraining, fine-tuning, and inference, using a comprehensive set of metrics to assess performance. The study reveals that efficiency techniques involve trade-offs that vary by task and model size, highlighting that no single method is best for all scenarios. Additionally, the findings indicate that these efficiency techniques can be applied across different model types, including vision and vision-language models, providing valuable resources for future research."
                },
                "zh": {
                    "title": "高效模型，节能降耗！",
                    "desc": "大型语言模型（LLMs）在技术上取得了显著进展，但其不断增长的参数数量和上下文窗口导致了高昂的计算、能源和经济成本。我们提出了EfficientLLM，这是一个新的基准，首次全面评估了大规模LLMs的效率技术。研究系统地探讨了架构预训练、微调和推理等三个关键方面，并定义了六个细致的指标来捕捉硬件饱和度、延迟-吞吐量平衡和碳成本。通过对100多个模型-技术对的评估，我们得出了效率涉及可量化权衡、最优解依赖于任务和规模以及技术在不同模态间的通用性等三大核心见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12872",
            "title": "From Grunts to Grammar: Emergent Language from Cooperative Foraging",
            "url": "https://huggingface.co/papers/2505.12872",
            "abstract": "Early cavemen relied on gestures, vocalizations, and simple signals to coordinate, plan, avoid predators, and share resources. Today, humans collaborate using complex languages to achieve remarkable results. What drives this evolution in communication? How does language emerge, adapt, and become vital for teamwork? Understanding the origins of language remains a challenge. A leading hypothesis in linguistics and anthropology posits that language evolved to meet the ecological and social demands of early human cooperation. Language did not arise in isolation, but through shared survival goals. Inspired by this view, we investigate the emergence of language in multi-agent Foraging Games. These environments are designed to reflect the cognitive and ecological constraints believed to have influenced the evolution of communication. Agents operate in a shared grid world with only partial knowledge about other agents and the environment, and must coordinate to complete games like picking up high-value targets or executing temporally ordered actions. Using end-to-end deep reinforcement learning, agents learn both actions and communication strategies from scratch. We find that agents develop communication protocols with hallmark features of natural language: arbitrariness, interchangeability, displacement, cultural transmission, and compositionality. We quantify each property and analyze how different factors, such as population size and temporal dependencies, shape specific aspects of the emergent language. Our framework serves as a platform for studying how language can evolve from partial observability, temporal reasoning, and cooperative goals in embodied multi-agent settings. We will release all data, code, and models publicly.",
            "score": 2,
            "issue_id": 3853,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "cb62284e65b8c471",
            "authors": [
                "Maytus Piriyajitakonkij",
                "Rujikorn Charakorn",
                "Weicheng Tao",
                "Wei Pan",
                "Mingfei Sun",
                "Cheston Tan",
                "Mengmi Zhang"
            ],
            "affiliations": [
                "Centre for Frontier AI Research (CFAR), ASTAR, Singapore",
                "College of Computing and Data Science, Nanyang Technological University, Singapore",
                "Department of Computer Science, The University of Manchester, United Kingdom",
                "Institute for Infocomm Research (I2R), ASTAR, Singapore",
                "Sakana AI, Japan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12872.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#agents",
                    "#reasoning",
                    "#open_source",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Эволюция языка через призму искусственного интеллекта",
                    "desc": "Статья исследует эволюцию языка в контексте многоагентных игр по добыче ресурсов, используя глубокое обучение с подкреплением. Авторы моделируют среду, отражающую когнитивные и экологические ограничения, повлиявшие на развитие коммуникации у ранних людей. Результаты показывают, что агенты развивают протоколы общения с ключевыми свойствами естественного языка: произвольность, взаимозаменяемость, смещение, культурную передачу и композиционность. Исследование предлагает платформу для изучения эволюции языка в условиях частичной наблюдаемости, временных зависимостей и кооперативных целей в многоагентных средах."
                },
                "en": {
                    "title": "Evolving Language Through Cooperative Learning in Multi-Agent Systems",
                    "desc": "This paper explores how language can emerge in multi-agent systems through the lens of Foraging Games, which simulate the ecological and social conditions of early human cooperation. Using end-to-end deep reinforcement learning, agents learn to communicate and coordinate their actions in a shared environment with limited information. The study reveals that these agents develop communication protocols that exhibit characteristics similar to natural language, such as arbitrariness and compositionality. By analyzing how factors like population size and temporal dependencies influence language features, the research provides insights into the evolutionary processes of communication."
                },
                "zh": {
                    "title": "语言的演化：从合作到沟通的旅程",
                    "desc": "这篇论文探讨了语言如何在多智能体觅食游戏中出现。研究表明，智能体在共享的环境中，通过深度强化学习学习行动和沟通策略，逐渐发展出具有自然语言特征的沟通协议。论文量化了语言的不同特性，如任意性、可互换性和文化传递等，并分析了人口规模和时间依赖性等因素如何影响语言的演变。该框架为研究语言如何在合作目标和部分可观察性中演化提供了平台。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11733",
            "title": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from\n  clinical case reports",
            "url": "https://huggingface.co/papers/2505.11733",
            "abstract": "Doctors and patients alike increasingly use Large Language Models (LLMs) to diagnose clinical cases. However, unlike domains such as math or coding, where correctness can be objectively defined by the final answer, medical diagnosis requires both the outcome and the reasoning process to be accurate. Currently, widely used medical benchmarks like MedQA and MMLU assess only accuracy in the final answer, overlooking the quality and faithfulness of the clinical reasoning process. To address this limitation, we introduce MedCaseReasoning, the first open-access dataset for evaluating LLMs on their ability to align with clinician-authored diagnostic reasoning. The dataset includes 14,489 diagnostic question-and-answer cases, each paired with detailed reasoning statements derived from open-access medical case reports. We evaluate state-of-the-art reasoning LLMs on MedCaseReasoning and find significant shortcomings in their diagnoses and reasoning: for instance, the top-performing open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy and mentions only 64% of the clinician reasoning statements (recall). However, we demonstrate that fine-tuning LLMs on the reasoning traces derived from MedCaseReasoning significantly improves diagnostic accuracy and clinical reasoning recall by an average relative gain of 29% and 41%, respectively. The open-source dataset, code, and models are available at https://github.com/kevinwu23/Stanford-MedCaseReasoning.",
            "score": 2,
            "issue_id": 3864,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "0eaea70de66597a0",
            "authors": [
                "Kevin Wu",
                "Eric Wu",
                "Rahul Thapa",
                "Kevin Wei",
                "Angela Zhang",
                "Arvind Suresh",
                "Jacqueline J. Tao",
                "Min Woo Sun",
                "Alejandro Lozano",
                "James Zou"
            ],
            "affiliations": [
                "Stanford University",
                "University of California, San Francisco",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11733.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#training",
                    "#benchmark",
                    "#alignment",
                    "#data",
                    "#healthcare",
                    "#reasoning"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "Новый стандарт оценки ИИ в медицинской диагностике",
                    "desc": "Представлен новый датасет MedCaseReasoning для оценки способности языковых моделей (LLM) соответствовать клиническому мышлению врачей при постановке диагнозов. Датасет содержит более 14 тысяч диагностических случаев с подробными рассуждениями, основанными на медицинских отчетах. Тестирование современных LLM на этом датасете выявило значительные недостатки в их диагностике и рассуждениях. Однако дообучение моделей на данных MedCaseReasoning значительно улучшило точность диагностики и полноту клинических рассуждений."
                },
                "en": {
                    "title": "Enhancing Medical Diagnosis with Reasoning Evaluation",
                    "desc": "This paper addresses the limitations of current medical benchmarks that only evaluate the final accuracy of Large Language Models (LLMs) in clinical diagnosis. It introduces MedCaseReasoning, a new dataset designed to assess both the accuracy of diagnoses and the quality of reasoning behind them, featuring 14,489 cases with clinician-authored explanations. The authors evaluate existing LLMs and find that they struggle with both diagnostic accuracy and recalling reasoning statements. By fine-tuning these models on the new dataset, they achieve significant improvements in both diagnostic accuracy and reasoning recall, demonstrating the importance of evaluating the reasoning process in medical AI applications."
                },
                "zh": {
                    "title": "提升医学诊断的推理能力",
                    "desc": "本研究提出了MedCaseReasoning数据集，用于评估大型语言模型（LLMs）在医学诊断中的推理能力。与传统的医学基准不同，该数据集不仅关注最终答案的准确性，还重视临床推理过程的质量和可信度。我们发现当前最先进的LLMs在诊断和推理方面存在显著不足，尤其是在准确性和推理回忆率上。通过对LLMs进行微调，使用MedCaseReasoning的数据，诊断准确性和推理回忆率分别提高了29%和41%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11475",
            "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across\n  Diverse Tasks and Languages",
            "url": "https://huggingface.co/papers/2505.11475",
            "abstract": "Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): https://huggingface.co/datasets/nvidia/HelpSteer3#preference",
            "score": 2,
            "issue_id": 3854,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "82282826ff02b787",
            "authors": [
                "Zhilin Wang",
                "Jiaqi Zeng",
                "Olivier Delalleau",
                "Hoo-Chang Shin",
                "Felipe Soares",
                "Alexander Bukharin",
                "Ellie Evans",
                "Yi Dong",
                "Oleksii Kuchaiev"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11475.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#rlhf",
                    "#multilingual",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "HelpSteer3-Preference: новый стандарт данных для RLHF",
                    "desc": "Статья представляет HelpSteer3-Preference - высококачественный набор данных предпочтений для обучения языковых моделей с помощью обучения с подкреплением на основе обратной связи от человека (RLHF). Набор содержит более 40 000 образцов, охватывающих различные реальные применения больших языковых моделей (LLM), включая задачи в области STEM, программирования и многоязычные сценарии. Используя HelpSteer3-Preference, авторы обучили модели вознаграждения (Reward Models), достигшие наилучших результатов на бенчмарках RM-Bench и JudgeBench. Также показано применение набора данных для обучения генеративных моделей вознаграждения и настройки политик с помощью RLHF."
                },
                "en": {
                    "title": "Enhancing Language Models with High-Quality Preference Data",
                    "desc": "This paper presents HelpSteer3-Preference, a new dataset designed to improve the training of language models using Reinforcement Learning from Human Feedback (RLHF). It contains over 40,000 high-quality, human-annotated preference samples that cover a wide range of real-world applications, including STEM and coding tasks. The dataset has been shown to significantly enhance the performance of Reward Models (RMs), achieving top scores on benchmark tests. Additionally, the paper discusses how this dataset can be utilized to train Generative RMs and align policy models with RLHF techniques."
                },
                "zh": {
                    "title": "提升语言模型的偏好数据集",
                    "desc": "本论文介绍了HelpSteer3-Preference，这是一个高质量的人类标注偏好数据集，包含超过40,000个样本，旨在提升通用领域的语言模型训练。该数据集涵盖了多种真实世界应用，包括STEM、编程和多语言场景，具有多样性和高质量。通过使用HelpSteer3-Preference，我们训练的奖励模型在RM-Bench和JudgeBench上取得了优异的表现，显著提高了之前模型的性能。此外，我们还展示了如何利用该数据集训练生成性奖励模型，并将策略模型与人类反馈强化学习对齐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10420",
            "title": "Learned Lightweight Smartphone ISP with Unpaired Data",
            "url": "https://huggingface.co/papers/2505.10420",
            "abstract": "The Image Signal Processor (ISP) is a fundamental component in modern smartphone cameras responsible for conversion of RAW sensor image data to RGB images with a strong focus on perceptual quality. Recent work highlights the potential of deep learning approaches and their ability to capture details with a quality increasingly close to that of professional cameras. A difficult and costly step when developing a learned ISP is the acquisition of pixel-wise aligned paired data that maps the raw captured by a smartphone camera sensor to high-quality reference images. In this work, we address this challenge by proposing a novel training method for a learnable ISP that eliminates the need for direct correspondences between raw images and ground-truth data with matching content. Our unpaired approach employs a multi-term loss function guided by adversarial training with multiple discriminators processing feature maps from pre-trained networks to maintain content structure while learning color and texture characteristics from the target RGB dataset. Using lightweight neural network architectures suitable for mobile devices as backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm UltraISP datasets. Compared to paired training methods, our unpaired learning strategy shows strong potential and achieves high fidelity across multiple evaluation metrics. The code and pre-trained models are available at https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .",
            "score": 2,
            "issue_id": 3855,
            "pub_date": "2025-05-15",
            "pub_date_card": {
                "ru": "15 мая",
                "en": "May 15",
                "zh": "5月15日"
            },
            "hash": "0ba36884ee806c6f",
            "authors": [
                "Andrei Arhire",
                "Radu Timofte"
            ],
            "affiliations": [
                "Computer Vision Lab, CAIDAS & IFI University of Wurzburg",
                "Faculty of Computer Science Alexandru Ioan Cuza University of Iasi"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10420.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#training",
                    "#dataset"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "Умная обработка фото без парных данных",
                    "desc": "Статья описывает новый метод обучения Image Signal Processor (ISP) для смартфонов без использования парных данных. Авторы предлагают подход с несколькими дискриминаторами и предобученными сетями для сохранения структуры контента при обучении цвету и текстуре. Метод был протестирован на наборах данных Zurich RAW to RGB и Fujifilm UltraISP с использованием легких нейронных архитектур. Результаты показывают высокую эффективность предложенного подхода по сравнению с методами, требующими парных данных."
                },
                "en": {
                    "title": "Revolutionizing Smartphone Imaging with Unpaired Learning!",
                    "desc": "This paper presents a new method for training an Image Signal Processor (ISP) using unpaired data, which means it does not require exact matches between raw images and high-quality reference images. The authors utilize a multi-term loss function and adversarial training with multiple discriminators to effectively learn color and texture characteristics while preserving content structure. Their approach is designed for lightweight neural networks, making it suitable for mobile devices. The results demonstrate that this unpaired learning strategy outperforms traditional paired methods in terms of image quality and fidelity."
                },
                "zh": {
                    "title": "无配对数据的学习型图像信号处理器",
                    "desc": "本文提出了一种新颖的训练方法，用于学习图像信号处理器（ISP），旨在解决传统方法中需要成对的原始图像和高质量参考图像的问题。我们的方法采用无配对的学习策略，通过对抗训练和多重判别器来保持内容结构，同时学习目标RGB数据集的颜色和纹理特征。使用适合移动设备的轻量级神经网络架构，我们在多个数据集上评估了该方法，结果显示出与成对训练方法相比，具有更高的保真度和潜力。代码和预训练模型可在指定链接获取。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13181",
            "title": "Efficient Speech Language Modeling via Energy Distance in Continuous\n  Latent Space",
            "url": "https://huggingface.co/papers/2505.13181",
            "abstract": "We introduce SLED, an alternative approach to speech language modeling by encoding speech waveforms into sequences of continuous latent representations and modeling them autoregressively using an energy distance objective. The energy distance offers an analytical measure of the distributional gap by contrasting simulated and target samples, enabling efficient training to capture the underlying continuous autoregressive distribution. By bypassing reliance on residual vector quantization, SLED avoids discretization errors and eliminates the need for the complicated hierarchical architectures common in existing speech language models. It simplifies the overall modeling pipeline while preserving the richness of speech information and maintaining inference efficiency. Empirical results demonstrate that SLED achieves strong performance in both zero-shot and streaming speech synthesis, showing its potential for broader applications in general-purpose speech language models.",
            "score": 1,
            "issue_id": 3867,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "5121a5b0ecc79bb1",
            "authors": [
                "Zhengrui Ma",
                "Yang Feng",
                "Chenze Shao",
                "Fandong Meng",
                "Jie Zhou",
                "Min Zhang"
            ],
            "affiliations": [
                "Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences",
                "Pattern Recognition Center, WeChat AI, Tencent Inc",
                "School of Future Science and Engineering, Soochow University",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13181.jpg",
            "data": {
                "categories": [
                    "#audio"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "SLED: Непрерывное моделирование речи без дискретизации",
                    "desc": "SLED - это новый подход к моделированию речи, кодирующий звуковые волны в последовательности непрерывных латентных представлений и моделирующий их авторегрессивно с использованием целевой функции энергетического расстояния. Метод позволяет избежать ошибок дискретизации и упрощает архитектуру модели по сравнению с существующими языковыми моделями речи. SLED сохраняет богатство речевой информации и обеспечивает эффективный вывод. Эмпирические результаты показывают высокую производительность SLED в задачах синтеза речи без предварительного обучения и в потоковом режиме."
                },
                "en": {
                    "title": "SLED: Simplifying Speech Modeling with Continuous Representations",
                    "desc": "SLED is a new method for speech language modeling that transforms speech waveforms into continuous latent representations. It uses an autoregressive approach combined with an energy distance objective to effectively measure and minimize the differences between simulated and actual speech samples. This method avoids common issues found in traditional models, such as discretization errors and complex architectures, leading to a simpler and more efficient modeling process. The results show that SLED performs well in generating speech, even in zero-shot and streaming scenarios, indicating its versatility for various speech applications."
                },
                "zh": {
                    "title": "SLED：简化语音建模的新方法",
                    "desc": "我们介绍了一种名为SLED的语音语言建模新方法，它通过将语音波形编码为连续潜在表示序列，并使用能量距离目标进行自回归建模。能量距离提供了一种分析性度量，用于对比模拟样本和目标样本之间的分布差异，从而实现高效训练，捕捉潜在的连续自回归分布。SLED避免了对残差向量量化的依赖，消除了离散化误差，并简化了现有语音语言模型中常见的复杂层次结构。实证结果表明，SLED在零样本和流式语音合成中表现出色，显示出其在通用语音语言模型中的广泛应用潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12781",
            "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation\n  through Low-Rank Clone",
            "url": "https://huggingface.co/papers/2505.12781",
            "abstract": "Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.",
            "score": 1,
            "issue_id": 3860,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "1ba09e10a601b724",
            "authors": [
                "Jitai Hao",
                "Qiang Huang",
                "Hao Liu",
                "Xinyan Xiao",
                "Zhaochun Ren",
                "Jun Yu"
            ],
            "affiliations": [
                "Baidu Inc.",
                "Harbin Institute of Technology, Shenzhen",
                "Leiden University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12781.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#transfer_learning",
                    "#training",
                    "#small_models"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное обучение малых языковых моделей с помощью низкоранговых проекций",
                    "desc": "Статья представляет новый метод предобучения малых языковых моделей (SLM) под названием Low-Rank Clone (LRC). LRC использует низкоранговые проекционные матрицы для эффективного сжатия весов учителя и клонирования активаций, включая сигналы из полносвязных слоев. Этот подход позволяет достичь поведенческой эквивалентности с сильными моделями-учителями, максимизируя передачу знаний. Эксперименты показывают, что LRC достигает или превосходит современные модели, обученные на триллионах токенов, используя только 20 миллиардов токенов, что повышает эффективность обучения более чем в 1000 раз."
                },
                "en": {
                    "title": "Efficient Training of Small Language Models with Low-Rank Clone",
                    "desc": "This paper presents a new method called Low-Rank Clone (LRC) for training Small Language Models (SLMs) efficiently. LRC addresses three main challenges in model training: reducing information loss from hard pruning, improving the alignment of representations, and better utilizing activations from Feed-Forward Networks. By using low-rank projection matrices, LRC allows for soft pruning and aligns student activations with those of a teacher model, enhancing knowledge transfer. The results show that LRC achieves high performance comparable to larger models while significantly reducing the amount of training data needed."
                },
                "zh": {
                    "title": "低秩克隆：高效训练小型语言模型的创新方法",
                    "desc": "本文提出了一种名为低秩克隆（Low-Rank Clone, LRC）的高效预训练方法，旨在解决训练小型语言模型（SLMs）时面临的信息损失、表示对齐效率低和激活利用不足等挑战。LRC通过构建低秩投影矩阵，实现了教师模型权重的软修剪和学生模型激活的对齐，特别是前馈网络（FFN）的信号。该方法最大化了知识转移的效率，消除了对显式对齐模块的需求。实验结果表明，LRC在使用仅20B标记的情况下，能够匹配或超越在万亿标记上训练的最先进模型，达到了超过1000倍的训练效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12257",
            "title": "LLM Context Conditioning and PWP Prompting for Multimodal Validation of\n  Chemical Formulas",
            "url": "https://huggingface.co/papers/2505.12257",
            "abstract": "Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. This exploratory proof-of-concept (PoC) study investigates structured LLM context conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a methodological strategy to modulate this LLM behavior at inference time. The approach is designed to enhance the reliability of readily available, general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for precise validation tasks, crucially relying only on their standard chat interfaces without API access or model modifications. To explore this methodology, we focused on validating chemical formulas within a single, complex test paper with known textual and image-based errors. Several prompting strategies were evaluated: while basic prompts proved unreliable, an approach adapting PWP structures to rigorously condition the LLM's analytical mindset appeared to improve textual error identification with both models. Notably, this method also guided Gemini 2.5 Pro to repeatedly identify a subtle image-based formula error previously overlooked during manual review, a task where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight specific LLM operational modes that impede detail-oriented validation and suggest that PWP-informed context conditioning offers a promising and highly accessible technique for developing more robust LLM-driven analytical workflows, particularly for tasks requiring meticulous error detection in scientific and technical documents. Extensive validation beyond this limited PoC is necessary to ascertain broader applicability.",
            "score": 1,
            "issue_id": 3849,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 мая",
                "en": "May 18",
                "zh": "5月18日"
            },
            "hash": "74901d316cc1d6cb",
            "authors": [
                "Evgeny Markhasin"
            ],
            "affiliations": [
                "Lobachevsky State University of Nizhny Novgorod"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12257.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#multimodal",
                    "#interpretability",
                    "#inference"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Повышение точности LLM в обнаружении ошибок через структурированную настройку контекста",
                    "desc": "Это исследование изучает структурированный подход к настройке контекста больших языковых моделей (LLM) для улучшения их способности выявлять технические ошибки в сложных научных документах. Методология основана на принципах устойчивого рабочего процесса запросов (PWP) и направлена на повышение надежности общедоступных LLM для задач точной валидации. Эксперименты показали, что адаптированный PWP-подход улучшил идентификацию текстовых ошибок и даже позволил модели Gemini 2.5 Pro обнаружить скрытую ошибку в формуле на изображении. Результаты указывают на потенциал этого метода для разработки более надежных аналитических рабочих процессов на основе LLM, особенно для задач, требующих тщательного обнаружения ошибок в научно-технических документах."
                },
                "en": {
                    "title": "Enhancing LLM Accuracy in Scientific Error Detection with PWP",
                    "desc": "This paper explores how to improve Large Language Models (LLMs) in identifying subtle errors in complex scientific documents, especially those with images and formulas. It introduces a method called Persistent Workflow Prompting (PWP) to better condition LLMs during their analysis, enhancing their ability to detect inaccuracies. The study specifically tests this approach on Gemini 2.5 Pro and ChatGPT Plus o3, showing that PWP can significantly improve error detection compared to basic prompting strategies. The findings suggest that this method could lead to more reliable LLMs for validating technical content, although further research is needed to confirm its effectiveness across different contexts."
                },
                "zh": {
                    "title": "提升LLM在科学文档中的错误识别能力",
                    "desc": "本研究探讨了如何利用结构化的上下文条件来改善大型语言模型（LLMs）在复杂科学和技术文档中的错误识别能力。研究采用了持久工作提示（PWP）原则，旨在提高LLMs在推理时的表现，尤其是在验证化学公式时。通过对不同提示策略的评估，发现适应PWP结构的提示能够有效提高文本错误的识别率，并成功发现了之前未被手动审查识别的图像公式错误。该方法为开发更强大的LLM驱动分析工作流提供了有希望的技术，尤其是在需要细致错误检测的任务中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11988",
            "title": "TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique\n  Annotation in Cyber Threat Intelligence Text",
            "url": "https://huggingface.co/papers/2505.11988",
            "abstract": "Accurately identifying adversarial techniques in security texts is critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations, such as custom hard-negative mining and denoising, resources rarely available in specialized domains.   We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimal text-technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques.   Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights.",
            "score": 1,
            "issue_id": 3850,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 мая",
                "en": "May 17",
                "zh": "5月17日"
            },
            "hash": "95b404534e69c826",
            "authors": [
                "Ahmed Lekssays",
                "Utsav Shukla",
                "Husrev Taha Sencar",
                "Md Rizwan Parvez"
            ],
            "affiliations": [
                "Independent Researcher",
                "Qatar Computing Research Institute, Doha, Qatar"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11988.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#data",
                    "#hallucinations",
                    "#rag",
                    "#benchmark"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Точное распознавание техник злоумышленников с минимумом данных",
                    "desc": "TechniqueRAG - это новая система для идентификации техник противников в текстах по кибербезопасности. Она использует извлечение информации и генеративные языковые модели, обученные на небольшом количестве примеров. TechniqueRAG применяет переранжирование с помощью LLM для улучшения релевантности извлеченной информации. Эксперименты показывают, что система достигает наилучших результатов без необходимости в больших размеченных датасетах."
                },
                "en": {
                    "title": "Enhancing Cyber Defense with TechniqueRAG: Precision without Excessive Resources",
                    "desc": "This paper introduces TechniqueRAG, a novel framework designed to improve the identification of adversarial techniques in cybersecurity texts. It combines retrieval-augmented generation (RAG) with instruction-tuned large language models (LLMs) to enhance domain specificity while minimizing the need for extensive labeled datasets. By fine-tuning only the generation component and employing zero-shot LLM re-ranking, TechniqueRAG improves the quality of retrieved candidates, ensuring they are more relevant to the specific domain of adversarial techniques. The results show that TechniqueRAG outperforms existing methods, achieving high accuracy without the heavy resource demands typically associated with task-specific optimizations."
                },
                "zh": {
                    "title": "提升安全文本对抗技术识别的创新框架",
                    "desc": "本文提出了一种名为TechniqueRAG的框架，旨在提高对安全文本中对抗性技术的识别能力。该框架结合了现成的检索器、经过指令调优的大型语言模型（LLM）和最小的文本-技术对，解决了数据稀缺的问题。通过仅对生成组件进行微调，TechniqueRAG避免了对资源密集型检索训练的依赖，同时通过零-shot LLM重排序提高了检索质量和领域特异性。实验结果表明，TechniqueRAG在多个安全基准上实现了最先进的性能，无需大量特定任务的优化或标记数据。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10831",
            "title": "Creating General User Models from Computer Use",
            "url": "https://huggingface.co/papers/2505.10831",
            "abstract": "Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture that user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.",
            "score": 1,
            "issue_id": 3858,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 мая",
                "en": "May 16",
                "zh": "5月16日"
            },
            "hash": "b70f4a83c98a9326",
            "authors": [
                "Omar Shaikh",
                "Shardul Sapkota",
                "Shan Rizvi",
                "Eric Horvitz",
                "Joon Sung Park",
                "Diyi Yang",
                "Michael S. Bernstein"
            ],
            "affiliations": [
                "Independent",
                "Microsoft Research",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10831.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#agents",
                    "#multimodal",
                    "#agi",
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "GUM: универсальная модель пользователя для создания интуитивных интерфейсов будущего",
                    "desc": "Статья представляет архитектуру общей модели пользователя (GUM), которая учится понимать человека, наблюдая за его взаимодействием с компьютером. GUM анализирует неструктурированные данные и создает взвешенные предположения о знаниях и предпочтениях пользователя. Эта модель может делать выводы о контексте действий пользователя, например, о подготовке к свадьбе или трудностях при работе над текстом. GUM позволяет создавать более умные чат-ассистенты, управлять уведомлениями ОС и разрабатывать интерактивных агентов, адаптирующихся к предпочтениям пользователя в различных приложениях."
                },
                "en": {
                    "title": "Empowering Technology to Anticipate User Needs",
                    "desc": "This paper introduces a General User Model (GUM) that learns about users by observing their interactions with computers, using unstructured data like device screenshots. GUMs create confidence-weighted propositions to capture user preferences and behaviors, allowing for flexible reasoning about user needs. The architecture can infer context from multimodal observations, enabling proactive assistants that can suggest actions without explicit user requests. Overall, GUMs enhance human-computer interaction by providing a more comprehensive understanding of user behavior and preferences across different applications."
                },
                "zh": {
                    "title": "通用用户模型：智能人机交互的新未来",
                    "desc": "本文提出了一种通用用户模型（GUM），旨在通过观察用户与计算机的互动来学习用户的偏好和习惯。GUM能够处理非结构化的用户观察数据（如设备截图），并构建出反映用户知识和偏好的置信加权命题。通过多模态观察，GUM可以推断用户的需求，并根据上下文检索相关命题，持续修正已有命题。研究表明，基于GUM的助手能够主动识别用户需求并执行有用的建议，从而实现更智能的人机交互。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03332",
            "title": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning",
            "url": "https://huggingface.co/papers/2505.03332",
            "abstract": "Critical peer review of scientific manuscripts presents a significant challenge for Large Language Models (LLMs), partly due to data limitations and the complexity of expert reasoning. This report introduces Persistent Workflow Prompting (PWP), a potentially broadly applicable prompt engineering methodology designed to bridge this gap using standard LLM chat interfaces (zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical analysis of experimental chemistry manuscripts, featuring a hierarchical, modular architecture (structured via Markdown) that defines detailed analysis workflows. We develop this PWP prompt through iterative application of meta-prompting techniques and meta-reasoning aimed at systematically codifying expert review workflows, including tacit knowledge. Submitted once at the start of a session, this PWP prompt equips the LLM with persistent workflows triggered by subsequent queries, guiding modern reasoning LLMs through systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM identifying major methodological flaws in a test case while mitigating LLM input bias and performing complex tasks, including distinguishing claims from evidence, integrating text/photo/figure analysis to infer parameters, executing quantitative feasibility checks, comparing estimates against claims, and assessing a priori plausibility. To ensure transparency and facilitate replication, we provide full prompts, detailed demonstration analyses, and logs of interactive chats as supplementary resources. Beyond the specific application, this work offers insights into the meta-development process itself, highlighting the potential of PWP, informed by detailed workflow formalization, to enable sophisticated analysis using readily available LLMs for complex scientific tasks.",
            "score": 1,
            "issue_id": 3849,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 мая",
                "en": "May 6",
                "zh": "5月6日"
            },
            "hash": "9c52936c2b9a7443",
            "authors": [
                "Evgeny Markhasin"
            ],
            "affiliations": [
                "Lobachevsky State University of Nizhny Novgorod"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03332.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#data",
                    "#science",
                    "#multimodal",
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "PWP: Новый подход к критическому анализу научных работ с помощью LLM",
                    "desc": "Статья представляет новый метод инженерии промптов под названием Persistent Workflow Prompting (PWP) для улучшения способности больших языковых моделей (LLM) проводить критический анализ научных рукописей. PWP использует иерархическую модульную архитектуру для определения детальных рабочих процессов анализа, которые сохраняются в течение сессии. Метод был разработан с помощью итеративного применения мета-промптинга и мета-рассуждений для систематической кодификации экспертных процессов рецензирования. Демонстрации показывают, что PWP-управляемая LLM способна выявлять серьезные методологические недостатки и выполнять сложные задачи анализа."
                },
                "en": {
                    "title": "Empowering LLMs for Expert Scientific Review with Persistent Workflow Prompting",
                    "desc": "This paper addresses the challenges faced by Large Language Models (LLMs) in performing critical peer reviews of scientific manuscripts, particularly due to data limitations and the intricacies of expert reasoning. It introduces a new methodology called Persistent Workflow Prompting (PWP), which allows users to create structured prompts that guide LLMs through detailed analysis workflows without needing coding skills. The PWP framework is designed to help LLMs systematically evaluate scientific content by integrating various forms of data, such as text and images, to identify flaws and assess claims. The authors demonstrate the effectiveness of PWP in analyzing experimental chemistry manuscripts, showcasing its ability to enhance LLM performance in complex scientific evaluations."
                },
                "zh": {
                    "title": "持久工作流提示：提升科学评审的智能化",
                    "desc": "这篇论文介绍了一种新的提示工程方法，称为持久工作流提示（PWP），旨在帮助大型语言模型（LLMs）进行科学手稿的批判性同行评审。PWP通过标准的LLM聊天界面，使用分层模块化的架构，定义详细的分析工作流程，能够系统化地编码专家评审的工作流程。通过迭代的元提示技术和元推理，PWP能够引导LLM进行多模态评估，识别实验化学手稿中的主要方法论缺陷。该方法不仅提供了具体应用的示例，还展示了如何利用现有的LLM进行复杂科学任务的深入分析。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12973",
            "title": "Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models",
            "url": "https://huggingface.co/papers/2505.12973",
            "abstract": "Homograph disambiguation remains a significant challenge in grapheme-to-phoneme (G2P) conversion, especially for low-resource languages. This challenge is twofold: (1) creating balanced and comprehensive homograph datasets is labor-intensive and costly, and (2) specific disambiguation strategies introduce additional latency, making them unsuitable for real-time applications such as screen readers and other accessibility tools. In this paper, we address both issues. First, we propose a semi-automated pipeline for constructing homograph-focused datasets, introduce the HomoRich dataset generated through this pipeline, and demonstrate its effectiveness by applying it to enhance a state-of-the-art deep learning-based G2P system for Persian. Second, we advocate for a paradigm shift - utilizing rich offline datasets to inform the development of fast, rule-based methods suitable for latency-sensitive accessibility applications like screen readers. To this end, we improve one of the most well-known rule-based G2P systems, eSpeak, into a fast homograph-aware version, HomoFast eSpeak. Our results show an approximate 30% improvement in homograph disambiguation accuracy for the deep learning-based and eSpeak systems.",
            "score": 0,
            "issue_id": 3854,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 мая",
                "en": "May 19",
                "zh": "5月19日"
            },
            "hash": "e144d477e8eb1cab",
            "authors": [
                "Mahta Fetrat Qharabagh",
                "Zahra Dehghanian",
                "Hamid R. Rabiee"
            ],
            "affiliations": [
                "Dep. of Computer Engineering, Sharif University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12973.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#healthcare",
                    "#low_resource",
                    "#dataset"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Эффективное разрешение омографов для G2P конверсии в малоресурсных языках",
                    "desc": "Статья посвящена проблеме разрешения омографов в задаче преобразования графем в фонемы (G2P) для малоресурсных языков. Авторы предлагают полуавтоматический метод создания датасетов с омографами и представляют датасет HomoRich. Они демонстрируют эффективность подхода, применяя его для улучшения современной системы G2P на основе глубокого обучения для персидского языка. Кроме того, авторы разрабатывают быструю версию известной системы eSpeak с поддержкой омографов - HomoFast eSpeak, показывая улучшение точности разрешения омографов примерно на 30%."
                },
                "en": {
                    "title": "Enhancing G2P with Efficient Homograph Disambiguation",
                    "desc": "This paper tackles the problem of homograph disambiguation in grapheme-to-phoneme (G2P) conversion, particularly for low-resource languages. It introduces a semi-automated method to create homograph datasets, exemplified by the new HomoRich dataset, which enhances a deep learning G2P system for Persian. Additionally, the authors propose a shift towards using comprehensive offline datasets to develop efficient, rule-based G2P methods that are suitable for real-time applications. The improved eSpeak system, named HomoFast eSpeak, demonstrates a significant increase in disambiguation accuracy, making it more effective for accessibility tools."
                },
                "zh": {
                    "title": "提升同形异义词消歧的智能解决方案",
                    "desc": "同形异义词消歧在图形到音素（G2P）转换中仍然是一个重要挑战，尤其是在资源匮乏的语言中。本文提出了一种半自动化的流程来构建同形异义词数据集，并生成了HomoRich数据集，以增强波斯语的深度学习G2P系统。我们还倡导利用丰富的离线数据集来开发适合实时应用的快速规则基础方法。通过改进著名的规则基础G2P系统eSpeak，我们的HomoFast eSpeak版本在同形异义词消歧准确性上提高了约30%。"
                }
            }
        }
    ],
    "link_prev": "2025-05-20.html",
    "link_next": "2025-05-22.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "20.05",
        "en": "05/20",
        "zh": "5月20日"
    },
    "short_date_next": {
        "ru": "22.05",
        "en": "05/22",
        "zh": "5月22日"
    },
    "categories": {
        "#dataset": 11,
        "#data": 10,
        "#benchmark": 18,
        "#agents": 4,
        "#cv": 6,
        "#rl": 7,
        "#rlhf": 4,
        "#rag": 2,
        "#plp": 0,
        "#inference": 8,
        "#3d": 3,
        "#audio": 1,
        "#video": 5,
        "#multimodal": 9,
        "#math": 3,
        "#multilingual": 3,
        "#architecture": 11,
        "#healthcare": 3,
        "#training": 19,
        "#robotics": 0,
        "#agi": 3,
        "#games": 2,
        "#interpretability": 6,
        "#reasoning": 18,
        "#transfer_learning": 2,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 21,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 15,
        "#small_models": 1,
        "#science": 3,
        "#low_resource": 2
    },
    "zh": {
        "text": "这篇文章提出了一种新的学习范式，称为Chain-of-Model (CoM)。它将因果关系引入每层的隐藏状态，形成链式结构，提高了模型训练的扩展效率和部署的灵活性。作者引入了Chain-of-Representation (CoR)的概念，将每层的隐藏状态表示为多个子表示（即链）的组合。每层中，每个链只能查看输入表示中的所有前序链。因此，基于CoM框架的模型可以通过增加链来逐步扩展模型大小，并通过使用不同的链数量提供多个不同大小的子模型进行弹性推理。基于这一原则，作者设计了Chain-of-Language-Model (CoLM)，并进一步引入了KV共享机制的CoLM-Air，以实现更多的扩展功能。实验结果表明，CoLM系列模型在性能上与标准Transformer相当，同时提供了更大的灵活性。",
        "title": "Chain-of-Model Learning for Language Model",
        "pinyin": "Zhè piān wénzhāng tíchū le yīzhǒng xīn de xuéxí fànshì, chēngwéi Chain-of-Model (CoM). Tā jiāng yīnguǒ guānxì yǐnrù měi céng de yǐncáng zhuàngtài, xíngchéng liànshì jiégòu, tīgāo le móxíng xùnliàn de kuòzhǎn xiàolǜ hé bùshǔ de línghuóxìng. Zuòzhě yǐnrù le Chain-of-Representation (CoR) de gàiniàn, jiāng měi céng de yǐncáng zhuàngtài biǎoshì wéi duōgè zǐ biǎoshì (jiē liàn) de zǔhé. Měi céng zhōng, měi gè liàn zhǐnéng chá kàn shūrù biǎoshì zhōng de suǒyǒu qiánxù liàn. Yīncǐ, jīyú CoM kuàngjià de móxíng kěyǐ tōngguò zēngjiā liàn lái zhúbù kuòzhǎn móxíng dàxìng, bìng tōngguò shǐyòng bùtóng de liàn shùliàng tígōng duōgè bùtóng dàxìng de zǐ móxíng jìnxíng tánxìng tuīlǐ. Jīyú zhè yī yuánzé, zuòzhě shèjì le Chain-of-Language-Model (CoLM), bìng jìn yībù yǐnrù le KV gòngxiǎng jīzhì de CoLM-Air, yǐ shíxiàn gèng duō de kuòzhǎn gōngnéng. Shíyàn jiéguǒ biǎomíng, CoLM xìliè móxíng zài xìngnéng shàng yǔ biāozhǔn Transformer xiāngdāng, tóngshí tígōng le gèng dà de línghuóxìng.",
        "vocab": "[\n    {\"word\": \"范式\", \"pinyin\": \"fàn shì\", \"trans\": \"paradigm\"},\n    {\"word\": \"Chain-of-Model\", \"pinyin\": \"Chèin-òf-Módel\", \"trans\": \"Chain-of-Model\"},\n    {\"word\": \"因果关系\", \"pinyin\": \"yīn guǒ guān xì\", \"trans\": \"causal relationship\"},\n    {\"word\": \"隐藏状态\", \"pinyin\": \"yǐn cáng zhuàng tài\", \"trans\": \"hidden state\"},\n    {\"word\": \"链式结构\", \"pinyin\": \"liàn shì jiégòu\", \"trans\": \"chain structure\"},\n    {\"word\": \"扩展效率\", \"pinyin\": \"kuò zhǎn xiào lǜ\", \"trans\": \"scalability\"},\n    {\"word\": \"部署\", \"pinyin\": \"bù shǔ\", \"trans\": \"deployment\"},\n    {\"word\": \"灵活性\", \"pinyin\": \"líng huó xìng\", \"trans\": \"flexibility\"},\n    {\"word\": \"Chain-of-Representation\", \"pinyin\": \"Chèin-òf-Rěprizen téi shēn\", \"trans\": \"Chain-of-Representation\"},\n    {\"word\": \"子表示\", \"pinyin\": \"zǐ biǎo shì\", \"trans\": \"sub-representation\"},\n    {\"word\": \"组合\", \"pinyin\": \"zǔ hé\", \"trans\": \"combination\"},\n    {\"word\": \"前序链\", \"pinyin\": \"qián xù liàn\", \"trans\": \"preceding chain\"},\n    {\"word\": \"弹性推理\", \"pinyin\": \"tán xìng tuī lǐ\", \"trans\": \"elastic inference\"},\n    {\"word\": \"Chain-of-Language-Model\", \"pinyin\": \"Chèin-òf-Lánggù Módel\", \"trans\": \"Chain-of-Language-Model\"},\n    {\"word\": \"KV共享机制\", \"pinyin\": \"KV gòng xiǎng jī zhì\", \"trans\": \"KV sharing mechanism\"},\n    {\"word\": \"CoLM-Air\", \"pinyin\": \"CoLM-Éir\", \"trans\": \"CoLM-Air\"},\n    {\"word\": \"扩展功能\", \"pinyin\": \"kuò zhǎn gōng néng\", \"trans\": \"extended functionality\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"Tèinshèin fōměi\", \"trans\": \"Transformer\"}\n]",
        "trans": "This article proposes a new learning paradigm called Chain-of-Model (CoM). It introduces causality into the hidden states of each layer, forming a chain-like structure that enhances the scalability of model training and the flexibility of deployment. The authors introduce the concept of Chain-of-Representation (CoR), representing the hidden states of each layer as a combination of multiple sub-representations (i.e., chains). Within each layer, each chain can only view all preceding chains in the input representation. Therefore, models based on the CoM framework can incrementally scale the model size by adding chains and provide multiple sub-models of different sizes for elastic inference by using different numbers of chains. Based on this principle, the authors designed Chain-of-Language-Model (CoLM) and further introduced CoLM-Air with a KV sharing mechanism to achieve more scalable functionalities. Experimental results show that the CoLM series models perform comparably to standard Transformers while offering greater flexibility.",
        "update_ts": "2025-05-20 09:13"
    }
}