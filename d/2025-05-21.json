{
    "date": {
        "ru": "21 Ğ¼Ğ°Ñ",
        "en": "May 21",
        "zh": "5æœˆ21æ—¥"
    },
    "time_utc": "2025-05-21 02:30",
    "weekday": 2,
    "issue_id": 3868,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.14683",
            "title": "Emerging Properties in Unified Multimodal Pretraining",
            "url": "https://huggingface.co/papers/2505.14683",
            "abstract": "Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open0source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder0only model pretrained on trillions of tokens curated from large0scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/",
            "score": 13,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "57522649bb8f8010",
            "authors": [
                "Chaorui Deng",
                "Deyao Zhu",
                "Kunchang Li",
                "Chenhui Gou",
                "Feng Li",
                "Zeyu Wang",
                "Shu Zhong",
                "Weihao Yu",
                "Xiaonan Nie",
                "Ziang Song",
                "Guang Shi",
                "Haoqi Fan"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Hong Kong University of Science and Technology",
                "Monash University",
                "Shenzhen Institutes of Advanced Technology",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14683.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#reasoning",
                    "#open_source",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ¥¯",
                "ru": {
                    "title": "BAGEL: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "BAGEL - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ…, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. BAGEL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ 3D-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "BAGEL: Unifying Multimodal AI for Enhanced Understanding and Generation",
                    "desc": "This paper presents BAGEL, an open-source foundational model designed for multimodal understanding and generation. BAGEL is a decoder-only model that has been pretrained on a vast dataset comprising text, images, videos, and web content. By leveraging this diverse multimodal data, BAGEL demonstrates advanced capabilities in complex reasoning tasks, outperforming existing open-source models. The authors aim to promote further research in multimodal AI by sharing their findings, pretraining methods, and code with the community."
                },
                "zh": {
                    "title": "BAGELï¼šå¼€æºå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºBAGELçš„å¼€æºåŸºç¡€æ¨¡å‹ï¼Œå®ƒæ”¯æŒå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚BAGELæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§£ç å™¨æ¨¡å‹ï¼Œç»è¿‡åœ¨å¤§é‡æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç½‘ç»œæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚é€šè¿‡ä½¿ç”¨å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®ï¼ŒBAGELåœ¨å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†æ–¹é¢å±•ç°å‡ºæ–°çš„èƒ½åŠ›ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¼€æºç»Ÿä¸€æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡åˆ†äº«å…³é”®å‘ç°ã€é¢„è®­ç»ƒç»†èŠ‚å’Œæ•°æ®åˆ›å»ºåè®®ï¼Œä¿ƒè¿›å¤šæ¨¡æ€ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14513",
            "title": "Latent Flow Transformer",
            "url": "https://huggingface.co/papers/2505.14513",
            "abstract": "Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in preserving coupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms.",
            "score": 5,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "3683bab427c47086",
            "authors": [
                "Yen-Chen Wu",
                "Feng-Ting Liao",
                "Meng-Hsi Chen",
                "Pei-Chen Ho",
                "Farhang Nabiei",
                "Da-shan Shiu"
            ],
            "affiliations": [
                "MediaTek Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14513.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ²: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Latent Flow Transformer (LFT), Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. LFT Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Flow Walking Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Pythia-410M Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LFT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¶Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞµĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Efficient Layer Compression with Latent Flow Transformers",
                    "desc": "This paper introduces the Latent Flow Transformer (LFT), a new architecture for large language models that replaces multiple discrete layers with a single learned transport operator. By utilizing flow matching, LFT achieves significant model compression while still being compatible with traditional transformer designs. The authors also present the Flow Walking (FW) algorithm to enhance the coupling preservation in flow-based methods. Experimental results show that LFT can effectively reduce the number of layers while improving performance metrics, bridging the gap between autoregressive and flow-based generation techniques."
                },
                "zh": {
                    "title": "æ½œåœ¨æµå˜æ¢å™¨ï¼šé«˜æ•ˆå‹ç¼©å¤§è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹â€”â€”æ½œåœ¨æµå˜æ¢å™¨ï¼ˆLatent Flow Transformer, LFTï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚LFTé€šè¿‡ä½¿ç”¨å­¦ä¹ çš„ä¼ è¾“ç®—å­æ›¿ä»£å¤šä¸ªç¦»æ•£å±‚ï¼Œä»è€Œå®ç°æ˜¾è‘—çš„å‹ç¼©ï¼ŒåŒæ—¶ä¿æŒä¸åŸå§‹æ¶æ„çš„å…¼å®¹æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æµæ­¥è¡Œï¼ˆFlow Walking, FWï¼‰ç®—æ³•ï¼Œä»¥è§£å†³ç°æœ‰æµåŸºæ–¹æ³•åœ¨ä¿æŒè€¦åˆæ–¹é¢çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLFTåœ¨å‹ç¼©å±‚æ•°çš„åŒæ—¶ï¼Œèƒ½å¤Ÿåœ¨æ€§èƒ½ä¸Šè¶…è¶Šä¼ ç»Ÿçš„å±‚è·³è¿‡æ–¹æ³•ï¼Œç¼©å°è‡ªå›å½’å’Œæµç”ŸæˆèŒƒå¼ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13866",
            "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
            "url": "https://huggingface.co/papers/2505.13866",
            "abstract": "Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers. While this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and throughput of token generation, limiting the practical deployment of such models. We propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths. RPC periodically compresses the KV cache by retaining KV cache that receive high importance score, which are computed using a selector window composed of recently generated queries. Experiments show that RPC improves generation throughput of QwQ-32B by up to 1.60times compared to the inference with full KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression.",
            "score": 5,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "72f6460e348e135a",
            "authors": [
                "Jiwon Song",
                "Dongwon Jo",
                "Yulhwa Kim",
                "Jae-Joon Kim"
            ],
            "affiliations": [
                "Seoul National University",
                "Sungkyunkwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13866.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ñ ĞŸÑƒÑ‚Ğ¸ Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RPC) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. RPC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ KV-ĞºÑÑˆ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RPC ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ QwQ-32B Ğ´Ğ¾ 1.60 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ KV-ĞºÑÑˆĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ»ĞµĞ´Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Efficient Inference with Reasoning Path Compression",
                    "desc": "This paper introduces Reasoning Path Compression (RPC), a method designed to enhance the efficiency of reasoning-focused language models during inference. By utilizing the concept of semantic sparsity, RPC compresses the key-value (KV) cache, retaining only the most important elements based on recent queries. This approach significantly increases the throughput of token generation while only slightly affecting accuracy. The results indicate that RPC can improve the performance of large models like QwQ-32B, making them more practical for real-world applications."
                },
                "zh": {
                    "title": "æ¨ç†è·¯å¾„å‹ç¼©ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "æœ€è¿‘ä¸“æ³¨äºæ¨ç†çš„è¯­è¨€æ¨¡å‹é€šè¿‡ç”Ÿæˆè¾ƒé•¿çš„ä¸­é—´æ¨ç†è·¯å¾„æ¥å®ç°é«˜å‡†ç¡®ç‡ã€‚è¿™ç§æ–¹æ³•åœ¨è§£å†³éœ€è¦é€»è¾‘æ€ç»´çš„é—®é¢˜æ—¶éå¸¸æœ‰æ•ˆï¼Œä½†é•¿æ¨ç†è·¯å¾„æ˜¾è‘—å¢åŠ äº†å†…å­˜ä½¿ç”¨å’Œä»¤ç‰Œç”Ÿæˆçš„ååé‡ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å®é™…åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ¨ç†è·¯å¾„å‹ç¼©ï¼ˆRPCï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æ¨ç†è·¯å¾„çš„è¯­ä¹‰ç¨€ç–æ€§æ¥åŠ é€Ÿæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒRPCåœ¨AIME 2024åŸºå‡†æµ‹è¯•ä¸­ç›¸æ¯”äºå®Œæ•´KVç¼“å­˜ï¼Œæå‡äº†QwQ-32Bçš„ç”Ÿæˆååé‡ï¼Œå‡†ç¡®ç‡ä»…ä¸‹é™1.2%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14680",
            "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search",
            "url": "https://huggingface.co/papers/2505.14680",
            "abstract": "Generative AI search is reshaping information retrieval by offering end-to-end answers to complex queries, reducing users' reliance on manually browsing and summarizing multiple web pages. However, while this paradigm enhances convenience, it disrupts the feedback-driven improvement loop that has historically powered the evolution of traditional Web search. Web search can continuously improve their ranking models by collecting large-scale, fine-grained user feedback (e.g., clicks, dwell time) at the document level. In contrast, generative AI search operates through a much longer search pipeline, spanning query decomposition, document retrieval, and answer generation, yet typically receives only coarse-grained feedback on the final answer. This introduces a feedback loop disconnect, where user feedback for the final output cannot be effectively mapped back to specific system components, making it difficult to improve each intermediate stage and sustain the feedback loop. In this paper, we envision NExT-Search, a next-generation paradigm designed to reintroduce fine-grained, process-level feedback into generative AI search. NExT-Search integrates two complementary modes: User Debug Mode, which allows engaged users to intervene at key stages; and Shadow User Mode, where a personalized user agent simulates user preferences and provides AI-assisted feedback for less interactive users. Furthermore, we envision how these feedback signals can be leveraged through online adaptation, which refines current search outputs in real-time, and offline update, which aggregates interaction logs to periodically fine-tune query decomposition, retrieval, and generation models. By restoring human control over key stages of the generative AI search pipeline, we believe NExT-Search offers a promising direction for building feedback-rich AI search systems that can evolve continuously alongside human feedback.",
            "score": 2,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "ace242db16327202",
            "authors": [
                "Sunhao Dai",
                "Wenjie Wang",
                "Liang Pang",
                "Jun Xu",
                "See-Kiong Ng",
                "Ji-Rong Wen",
                "Tat-Seng Chua"
            ],
            "affiliations": [
                "CAS Key Laboratory of AI Safety Institute of Computing Technology Chinese Academy of Sciences",
                "Gaoling School of Artificial Intelligence Renmin University of China",
                "National University of Singapore",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14680.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#rag",
                    "#rlhf",
                    "#agents",
                    "#alignment"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¸ÑĞº",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ NExT-Search, Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ½Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ ÑƒÑ‚Ñ€Ğ°Ñ‡ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜. NExT-Search Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°: Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ñ‚ĞµĞ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñƒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "NExT-Search: Enhancing Generative AI Search with User Feedback",
                    "desc": "This paper discusses the challenges of integrating user feedback into generative AI search systems, which provide direct answers to complex queries but lack detailed feedback mechanisms. Traditional web search benefits from fine-grained user interactions, allowing for continuous improvement of ranking models. The proposed NExT-Search framework aims to bridge this gap by introducing two modes of user feedback: User Debug Mode for active user engagement and Shadow User Mode for passive feedback collection. By leveraging both real-time and aggregated feedback, NExT-Search seeks to enhance the generative AI search process and ensure it evolves in response to user needs."
                },
                "zh": {
                    "title": "NExT-Searchï¼šé‡å¡‘ç”Ÿæˆå¼æœç´¢çš„åé¦ˆå¾ªç¯",
                    "desc": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æœç´¢æ­£åœ¨æ”¹å˜ä¿¡æ¯æ£€ç´¢ï¼Œé€šè¿‡æä¾›ç«¯åˆ°ç«¯çš„ç­”æ¡ˆæ¥åº”å¯¹å¤æ‚æŸ¥è¯¢ï¼Œå‡å°‘ç”¨æˆ·æ‰‹åŠ¨æµè§ˆå’Œæ€»ç»“å¤šä¸ªç½‘é¡µçš„ä¾èµ–ã€‚ç„¶è€Œï¼Œè¿™ç§æ–°æ¨¡å¼è™½ç„¶æé«˜äº†ä¾¿åˆ©æ€§ï¼Œå´æ‰“ç ´äº†ä¼ ç»Ÿç½‘é¡µæœç´¢ä¸­åŸºäºåé¦ˆçš„æ”¹è¿›å¾ªç¯ã€‚ä¼ ç»Ÿæœç´¢å¯ä»¥é€šè¿‡æ”¶é›†ç”¨æˆ·åé¦ˆï¼ˆå¦‚ç‚¹å‡»ç‡å’Œåœç•™æ—¶é—´ï¼‰æ¥ä¸æ–­æ”¹è¿›æ’åæ¨¡å‹ï¼Œè€Œç”Ÿæˆå¼æœç´¢åˆ™é¢ä¸´åé¦ˆå¾ªç¯æ–­è£‚çš„é—®é¢˜ï¼Œç”¨æˆ·åé¦ˆéš¾ä»¥æœ‰æ•ˆæ˜ å°„åˆ°ç³»ç»Ÿçš„å…·ä½“ç»„ä»¶ã€‚æœ¬æ–‡æå‡ºäº†NExT-Searchï¼Œæ—¨åœ¨å°†ç»†ç²’åº¦çš„è¿‡ç¨‹çº§åé¦ˆé‡æ–°å¼•å…¥ç”Ÿæˆå¼æœç´¢ï¼Œç»“åˆç”¨æˆ·è°ƒè¯•æ¨¡å¼å’Œå½±å­ç”¨æˆ·æ¨¡å¼ï¼Œä»¥å®ç°å®æ—¶å’Œç¦»çº¿çš„åé¦ˆä¿¡å·åˆ©ç”¨ï¼Œä»è€ŒæŒç»­æ”¹è¿›æœç´¢ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13380",
            "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition",
            "url": "https://huggingface.co/papers/2505.13380",
            "abstract": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an improved version of the previous study at arXiv:2402.02526",
            "score": 1,
            "issue_id": 3868,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "6a5e70a76e6f012c",
            "authors": [
                "Nam V. Nguyen",
                "Huy Nguyen",
                "Quang Pham",
                "Van Nguyen",
                "Savitha Ramasamy",
                "Nhat Ho"
            ],
            "affiliations": [
                "FPT Software AI Center",
                "Independent Researcher",
                "Institute for Infocomm Research, ASTAR",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13380.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼ĞµÑÑÑ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (SMoE) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'competition'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ softmax. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ CompeteSMoE Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ CompeteSMoE Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ SMoE."
                },
                "en": {
                    "title": "CompeteSMoE: Efficient Routing for Powerful Language Models",
                    "desc": "Sparse mixture of experts (SMoE) is a method that allows models to become more complex without simply making them deeper or wider. The challenge with SMoE is that the way experts are chosen to process data can be inefficient, as not all experts contribute to the decision-making process. This paper introduces a new routing mechanism called competition, which directs data to the most responsive experts, improving the efficiency of the model. The authors present CompeteSMoE, an algorithm that uses this competition mechanism to train large language models effectively, showing better performance and lower training costs compared to existing methods."
                },
                "zh": {
                    "title": "ç«äº‰æœºåˆ¶æå‡ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹çš„æ•ˆç‡",
                    "desc": "ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆSMoEï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆæå‡æ¨¡å‹å¤æ‚åº¦çš„æ–¹æ³•ï¼Œè¶…è¶Šäº†ç®€å•å¢åŠ ç½‘ç»œæ·±åº¦æˆ–å®½åº¦çš„æ–¹å¼ã€‚ç„¶è€Œï¼ŒSMoEçš„è®­ç»ƒä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯å› ä¸ºè®¡ç®—çš„ä¸“å®¶ä¸è·¯ç”±è¿‡ç¨‹ä¹‹é—´çš„è”ç³»ä¸å¤Ÿç›´æ¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æœºåˆ¶â€”â€”ç«äº‰ï¼Œèƒ½å¤Ÿå°†è¾“å…¥æ•°æ®æ›´æœ‰æ•ˆåœ°è·¯ç”±åˆ°å“åº”æœ€å¼ºçš„ä¸“å®¶ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†ç«äº‰æœºåˆ¶åœ¨æ ·æœ¬æ•ˆç‡ä¸Šä¼˜äºä¼ ç»Ÿçš„softmaxè·¯ç”±ï¼Œå¹¶å¼€å‘äº†CompeteSMoEç®—æ³•ï¼Œèƒ½å¤Ÿä»¥è¾ƒä½çš„è®­ç»ƒå¼€é”€å®ç°å¼ºå¤§çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12182",
            "title": "Truth Neurons",
            "url": "https://huggingface.co/papers/2505.12182",
            "abstract": "Despite their remarkable success and deployment across diverse workflows, language models sometimes produce untruthful responses. Our limited understanding of how truthfulness is mechanistically encoded within these models jeopardizes their reliability and safety. In this paper, we propose a method for identifying representations of truthfulness at the neuron level. We show that language models contain truth neurons, which encode truthfulness in a subject-agnostic manner. Experiments conducted across models of varying scales validate the existence of truth neurons, confirming that the encoding of truthfulness at the neuron level is a property shared by many language models. The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness. Selectively suppressing the activations of truth neurons found through the TruthfulQA dataset degrades performance both on TruthfulQA and on other benchmarks, showing that the truthfulness mechanisms are not tied to a specific dataset. Our results offer novel insights into the mechanisms underlying truthfulness in language models and highlight potential directions toward improving their trustworthiness and reliability.",
            "score": 1,
            "issue_id": 3868,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "ddeab64450bb26a9",
            "authors": [
                "Haohang Li",
                "Yupeng Cao",
                "Yangyang Yu",
                "Jordan W. Suchow",
                "Zining Zhu"
            ],
            "affiliations": [
                "Stevens Institute of Technology",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12182.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#hallucinations",
                    "#alignment",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ñ‹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ‚Ğ°Ğº Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ 'Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ñ‹', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ‚ĞµĞ¼Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞŸĞ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unveiling Truth Neurons: Enhancing Language Model Trustworthiness",
                    "desc": "This paper investigates how language models encode truthfulness at the neuron level, revealing the presence of 'truth neurons' that represent truthfulness in a way that is not dependent on specific subjects. The authors demonstrate that these truth neurons exist across various models, indicating a shared property among them. By analyzing the distribution of truth neurons across different layers, the study aligns with previous research on the geometry of truthfulness. Additionally, the suppression of these neurons negatively impacts model performance, suggesting that understanding and improving truthfulness in language models is crucial for their reliability."
                },
                "zh": {
                    "title": "æ­ç¤ºè¯­è¨€æ¨¡å‹ä¸­çš„çœŸç›¸ç¥ç»å…ƒ",
                    "desc": "å°½ç®¡è¯­è¨€æ¨¡å‹åœ¨å„ç§å·¥ä½œæµç¨‹ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†æœ‰æ—¶ä¼šäº§ç”Ÿä¸çœŸå®çš„å›ç­”ã€‚æˆ‘ä»¬å¯¹è¿™äº›æ¨¡å‹ä¸­çœŸç›¸ç¼–ç æœºåˆ¶çš„ç†è§£æœ‰é™ï¼Œè¿™å½±å“äº†å®ƒä»¬çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡ç¥ç»å…ƒå±‚é¢è¯†åˆ«çœŸç›¸çš„è¡¨ç¤ºï¼Œå‘ç°è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨ç¼–ç çœŸç›¸çš„çœŸç›¸ç¥ç»å…ƒã€‚å®éªŒè¡¨æ˜ï¼ŒçœŸç›¸ç¥ç»å…ƒçš„å­˜åœ¨æ˜¯è®¸å¤šè¯­è¨€æ¨¡å‹çš„å…±åŒç‰¹æ€§ï¼Œå¹¶ä¸”å…¶åˆ†å¸ƒæ¨¡å¼ä¸çœŸç›¸çš„å‡ ä½•ç‰¹å¾ä¸€è‡´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14178",
            "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits",
            "url": "https://huggingface.co/papers/2505.14178",
            "abstract": "Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.",
            "score": 0,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "f4fdc7fb140f9273",
            "authors": [
                "Xiang Zhang",
                "Juntai Cao",
                "Jiaqi Wei",
                "Yiwei Xu",
                "Chenyu You"
            ],
            "affiliations": [
                "Cisco",
                "Stony Brook University",
                "University of British Columbia",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14178.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#architecture",
                    "#small_models",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ĞºĞ»ÑÑ‡ Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ…' (Token Awareness) Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ°Ñ€ÑƒÑˆĞ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Tokenization Matters: Unlocking Reasoning in Language Models",
                    "desc": "This paper explores the importance of tokenization in language models, particularly how it affects reasoning capabilities. It highlights that traditional tokenization methods, like byte-pair encoding (BPE), can obscure essential reasoning units, limiting the model's ability to perform symbolic computation. The authors introduce the concept of Token Awareness, which emphasizes the need for better token granularity to enhance logical alignment and generalization in models. Through experiments on arithmetic and symbolic tasks, they show that models with well-structured token representations can significantly outperform larger models in reasoning tasks."
                },
                "zh": {
                    "title": "åˆ†è¯ç»“æ„å†³å®šæ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œåˆ†è¯ï¼ˆTokenizationï¼‰å¯¹æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œåˆ†è¯æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åŸºäºå­è¯çš„æ–¹æ³•ï¼ˆå¦‚å­—èŠ‚å¯¹ç¼–ç BPEï¼‰ï¼Œä¼šåˆå¹¶æˆ–æ¨¡ç³ŠåŸºæœ¬çš„æ¨ç†å•å…ƒï¼Œä»è€Œå¦¨ç¢ç¬¦å·è®¡ç®—ã€‚æˆ‘ä»¬å¼•å…¥äº†â€œToken Awarenessâ€çš„æ¦‚å¿µï¼Œå¼ºè°ƒäº†åˆ†è¯ç²’åº¦ä¸ä½³å¦‚ä½•å¹²æ‰°é€»è¾‘å¯¹é½ï¼Œé˜»ç¢æ¨¡å‹çš„ç¬¦å·ç¨‹åºæ³›åŒ–ã€‚é€šè¿‡å¯¹ç®—æœ¯å’Œç¬¦å·ä»»åŠ¡çš„ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†åˆ†è¯ç»“æ„æ˜¾è‘—å½±å“æ¨ç†æ€§èƒ½ï¼Œè¾ƒå°çš„æ¨¡å‹åœ¨å¯¹é½æ ¼å¼ä¸‹èƒ½å¤Ÿè¶…è¶Šæ›´å¤§çš„ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12306",
            "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for\n  Real-world Knowledge Injection",
            "url": "https://huggingface.co/papers/2505.12306",
            "abstract": "Despite significant advances in large language models (LLMs), their knowledge memorization capabilities remain underexplored, due to the lack of standardized and high-quality test ground. In this paper, we introduce a novel, real-world and large-scale knowledge injection benchmark that evolves continuously over time without requiring human intervention. Specifically, we propose WikiDYK, which leverages recently-added and human-written facts from Wikipedia's \"Did You Know...\" entries. These entries are carefully selected by expert Wikipedia editors based on criteria such as verifiability and clarity. Each entry is converted into multiple question-answer pairs spanning diverse task formats from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290 facts and 77,180 questions, which is also seamlessly extensible with future updates from Wikipedia editors. Extensive experiments using continued pre-training reveal a surprising insight: despite their prevalence in modern LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), exhibiting a 23% lower accuracy in terms of reliability. To compensate for the smaller scales of current BiLMs, we introduce a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to integrate with LLMs. Experiment shows that our framework further improves the reliability accuracy by up to 29.1%.",
            "score": 0,
            "issue_id": 3868,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "ccbad06f5ba35418",
            "authors": [
                "Yuwei Zhang",
                "Wenhao Yu",
                "Shangbin Feng",
                "Yifan Zhu",
                "Letian Peng",
                "Jayanth Srinivasa",
                "Gaowen Liu",
                "Jingbo Shang"
            ],
            "affiliations": [
                "Cisco",
                "Tencent AI Lab",
                "UC, San Diego",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12306.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#interpretability",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "WikiDYK: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WikiDYK Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. WikiDYK Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ° Wikipedia 'Did You Know...', Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ¸Ñ… Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (BiLM) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ñ‡ĞµĞ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (CLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ğ¸ BiLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Knowledge Memorization in Language Models with WikiDYK",
                    "desc": "This paper presents WikiDYK, a new benchmark for evaluating knowledge memorization in large language models (LLMs). It uses real-world facts from Wikipedia's 'Did You Know...' entries to create a diverse set of question-answer pairs. The study finds that Causal Language Models (CLMs) have weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), with a notable accuracy gap. To enhance BiLMs' performance, the authors propose a collaborative framework that combines multiple BiLMs as external knowledge sources, resulting in improved accuracy in knowledge retrieval tasks."
                },
                "zh": {
                    "title": "çŸ¥è¯†è®°å¿†èƒ½åŠ›çš„æ–°åŸºå‡†ï¼šWikiDYK",
                    "desc": "å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬çš„çŸ¥è¯†è®°å¿†èƒ½åŠ›ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ã€çœŸå®ä¸–ç•Œçš„å¤§è§„æ¨¡çŸ¥è¯†æ³¨å…¥åŸºå‡†ï¼Œåä¸ºWikiDYKï¼Œèƒ½å¤Ÿéšç€æ—¶é—´çš„æ¨ç§»ä¸æ–­æ¼”å˜ï¼Œè€Œæ— éœ€äººå·¥å¹²é¢„ã€‚WikiDYKåˆ©ç”¨ç»´åŸºç™¾ç§‘â€œä½ çŸ¥é“å—...â€æ¡ç›®ä¸­æœ€è¿‘æ·»åŠ çš„ã€ç”±äººç±»æ’°å†™çš„äº‹å®ï¼Œç»è¿‡ä¸“å®¶ç¼–è¾‘çš„ä¸¥æ ¼ç­›é€‰ï¼Œç¡®ä¿å…¶å¯éªŒè¯æ€§å’Œæ¸…æ™°æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å› æœè¯­è¨€æ¨¡å‹ï¼ˆCLMsï¼‰åœ¨ç°ä»£LLMsä¸­æ™®éå­˜åœ¨ï¼Œä½†å…¶çŸ¥è¯†è®°å¿†èƒ½åŠ›æ˜¾è‘—ä½äºåŒå‘è¯­è¨€æ¨¡å‹ï¼ˆBiLMsï¼‰ï¼Œå‡†ç¡®æ€§ä½23%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10588",
            "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety\n  Systems for Content Moderation",
            "url": "https://huggingface.co/papers/2505.10588",
            "abstract": "This research offers a unique evaluation of how AI systems interpret the digital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first cohort raised alongside AI, Gen Alpha faces new forms of online risk due to immersive digital engagement and a growing mismatch between their evolving communication and existing safety tools. Their distinct language, shaped by gaming, memes, and AI-driven trends, often conceals harmful interactions from both human moderators and automated systems. We assess four leading AI models (GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked harassment and manipulation within Gen Alpha discourse. Using a dataset of 100 recent expressions from gaming platforms, social media, and video content, the study reveals critical comprehension failures with direct implications for online safety. This work contributes: (1) a first-of-its-kind dataset capturing Gen Alpha expressions; (2) a framework to improve AI moderation systems for youth protection; (3) a multi-perspective evaluation including AI systems, human moderators, and parents, with direct input from Gen Alpha co-researchers; and (4) an analysis of how linguistic divergence increases youth vulnerability. Findings highlight the urgent need to redesign safety systems attuned to youth communication, especially given Gen Alpha reluctance to seek help when adults fail to understand their digital world. This study combines the insight of a Gen Alpha researcher with systematic academic analysis to address critical digital safety challenges.",
            "score": 0,
            "issue_id": 3868,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "cdc9a4f93d65b071",
            "authors": [
                "Manisha Mehta",
                "Fausto Giunchiglia"
            ],
            "affiliations": [
                "University of Trento, Trento, Italy",
                "Warren Hyde Middle School, Cupertino, California, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10588.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#interpretability",
                    "#benchmark",
                    "#ethics",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ñ€ÑŒĞµÑ€: Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹Ğº Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ ĞĞ»ÑŒÑ„Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ Ğ´Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ² Ğ´Ğ¸ÑĞºÑƒÑ€ÑĞµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ ĞĞ»ÑŒÑ„Ğ° Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ framework'Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ˜Ğ˜. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¾ÑÑ‚Ñ€ÑƒÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»Ğ¾Ğ´ĞµĞ¶Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing AI Safety for Generation Alpha",
                    "desc": "This research evaluates how AI systems understand the unique digital language of Generation Alpha, who are growing up with AI technology. It highlights the risks they face online due to their distinct communication styles, influenced by gaming and memes, which can hide harmful interactions from both humans and automated systems. The study tests four AI models on their ability to detect subtle harassment in Gen Alpha's online expressions, revealing significant gaps in their comprehension. The findings emphasize the need for improved AI moderation tools that are better suited to protect youth in their digital environments."
                },
                "zh": {
                    "title": "é‡å¡‘å®‰å…¨ç³»ç»Ÿï¼Œä¿æŠ¤é˜¿å°”æ³•ä¸–ä»£çš„æ•°å­—äº¤æµ",
                    "desc": "æœ¬ç ”ç©¶ç‹¬ç‰¹åœ°è¯„ä¼°äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿå¦‚ä½•è§£è¯»é˜¿å°”æ³•ä¸–ä»£ï¼ˆ2010-2024å¹´å‡ºç”Ÿï¼‰çš„æ•°å­—è¯­è¨€ã€‚é˜¿å°”æ³•ä¸–ä»£æ˜¯é¦–ä¸ªä¸äººå·¥æ™ºèƒ½å…±åŒæˆé•¿çš„ç¾¤ä½“ï¼Œä»–ä»¬åœ¨æ²‰æµ¸å¼æ•°å­—ç¯å¢ƒä¸­é¢ä¸´æ–°çš„åœ¨çº¿é£é™©ã€‚ç ”ç©¶åˆ†æäº†å››ç§é¢†å…ˆçš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆGPT-4ã€Claudeã€Geminiå’ŒLlama 3ï¼‰åœ¨è¯†åˆ«éšè—çš„éªšæ‰°å’Œæ“æ§æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„å®‰å…¨å·¥å…·æœªèƒ½æœ‰æ•ˆç†è§£é˜¿å°”æ³•ä¸–ä»£çš„ç‹¬ç‰¹äº¤æµæ–¹å¼ï¼Œå¼ºè°ƒäº†é‡æ–°è®¾è®¡å®‰å…¨ç³»ç»Ÿçš„ç´§è¿«æ€§ï¼Œä»¥æ›´å¥½åœ°ä¿æŠ¤å¹´è½»ç”¨æˆ·ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-20.html",
    "link_next": "2025-05-22.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "20.05",
        "en": "05/20",
        "zh": "5æœˆ20æ—¥"
    },
    "short_date_next": {
        "ru": "22.05",
        "en": "05/22",
        "zh": "5æœˆ22æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 5,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºChain-of-Model (CoM)ã€‚å®ƒå°†å› æœå…³ç³»å¼•å…¥æ¯å±‚çš„éšè—çŠ¶æ€ï¼Œå½¢æˆé“¾å¼ç»“æ„ï¼Œæé«˜äº†æ¨¡å‹è®­ç»ƒçš„æ‰©å±•æ•ˆç‡å’Œéƒ¨ç½²çš„çµæ´»æ€§ã€‚ä½œè€…å¼•å…¥äº†Chain-of-Representation (CoR)çš„æ¦‚å¿µï¼Œå°†æ¯å±‚çš„éšè—çŠ¶æ€è¡¨ç¤ºä¸ºå¤šä¸ªå­è¡¨ç¤ºï¼ˆå³é“¾ï¼‰çš„ç»„åˆã€‚æ¯å±‚ä¸­ï¼Œæ¯ä¸ªé“¾åªèƒ½æŸ¥çœ‹è¾“å…¥è¡¨ç¤ºä¸­çš„æ‰€æœ‰å‰åºé“¾ã€‚å› æ­¤ï¼ŒåŸºäºCoMæ¡†æ¶çš„æ¨¡å‹å¯ä»¥é€šè¿‡å¢åŠ é“¾æ¥é€æ­¥æ‰©å±•æ¨¡å‹å¤§å°ï¼Œå¹¶é€šè¿‡ä½¿ç”¨ä¸åŒçš„é“¾æ•°é‡æä¾›å¤šä¸ªä¸åŒå¤§å°çš„å­æ¨¡å‹è¿›è¡Œå¼¹æ€§æ¨ç†ã€‚åŸºäºè¿™ä¸€åŸåˆ™ï¼Œä½œè€…è®¾è®¡äº†Chain-of-Language-Model (CoLM)ï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†KVå…±äº«æœºåˆ¶çš„CoLM-Airï¼Œä»¥å®ç°æ›´å¤šçš„æ‰©å±•åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLMç³»åˆ—æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸æ ‡å‡†Transformerç›¸å½“ï¼ŒåŒæ—¶æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚",
        "title": "Chain-of-Model Learning for Language Model",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÃ­chÅ« le yÄ«zhÇ’ng xÄ«n de xuÃ©xÃ­ fÃ nshÃ¬, chÄ“ngwÃ©i Chain-of-Model (CoM). TÄ jiÄng yÄ«nguÇ’ guÄnxÃ¬ yÇnrÃ¹ mÄ›i cÃ©ng de yÇncÃ¡ng zhuÃ ngtÃ i, xÃ­ngchÃ©ng liÃ nshÃ¬ jiÃ©gÃ²u, tÄ«gÄo le mÃ³xÃ­ng xÃ¹nliÃ n de kuÃ²zhÇn xiÃ olÇœ hÃ© bÃ¹shÇ” de lÃ­nghuÃ³xÃ¬ng. ZuÃ²zhÄ› yÇnrÃ¹ le Chain-of-Representation (CoR) de gÃ iniÃ n, jiÄng mÄ›i cÃ©ng de yÇncÃ¡ng zhuÃ ngtÃ i biÇoshÃ¬ wÃ©i duÅgÃ¨ zÇ biÇoshÃ¬ (jiÄ“ liÃ n) de zÇ”hÃ©. MÄ›i cÃ©ng zhÅng, mÄ›i gÃ¨ liÃ n zhÇnÃ©ng chÃ¡ kÃ n shÅ«rÃ¹ biÇoshÃ¬ zhÅng de suÇ’yÇ’u qiÃ¡nxÃ¹ liÃ n. YÄ«ncÇ, jÄ«yÃº CoM kuÃ ngjiÃ  de mÃ³xÃ­ng kÄ›yÇ tÅngguÃ² zÄ“ngjiÄ liÃ n lÃ¡i zhÃºbÃ¹ kuÃ²zhÇn mÃ³xÃ­ng dÃ xÃ¬ng, bÃ¬ng tÅngguÃ² shÇyÃ²ng bÃ¹tÃ³ng de liÃ n shÃ¹liÃ ng tÃ­gÅng duÅgÃ¨ bÃ¹tÃ³ng dÃ xÃ¬ng de zÇ mÃ³xÃ­ng jÃ¬nxÃ­ng tÃ¡nxÃ¬ng tuÄ«lÇ. JÄ«yÃº zhÃ¨ yÄ« yuÃ¡nzÃ©, zuÃ²zhÄ› shÃ¨jÃ¬ le Chain-of-Language-Model (CoLM), bÃ¬ng jÃ¬n yÄ«bÃ¹ yÇnrÃ¹ le KV gÃ²ngxiÇng jÄ«zhÃ¬ de CoLM-Air, yÇ shÃ­xiÃ n gÃ¨ng duÅ de kuÃ²zhÇn gÅngnÃ©ng. ShÃ­yÃ n jiÃ©guÇ’ biÇomÃ­ng, CoLM xÃ¬liÃ¨ mÃ³xÃ­ng zÃ i xÃ¬ngnÃ©ng shÃ ng yÇ” biÄozhÇ”n Transformer xiÄngdÄng, tÃ³ngshÃ­ tÃ­gÅng le gÃ¨ng dÃ  de lÃ­nghuÃ³xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"èŒƒå¼\", \"pinyin\": \"fÃ n shÃ¬\", \"trans\": \"paradigm\"},\n    {\"word\": \"Chain-of-Model\", \"pinyin\": \"ChÃ¨in-Ã²f-MÃ³del\", \"trans\": \"Chain-of-Model\"},\n    {\"word\": \"å› æœå…³ç³»\", \"pinyin\": \"yÄ«n guÇ’ guÄn xÃ¬\", \"trans\": \"causal relationship\"},\n    {\"word\": \"éšè—çŠ¶æ€\", \"pinyin\": \"yÇn cÃ¡ng zhuÃ ng tÃ i\", \"trans\": \"hidden state\"},\n    {\"word\": \"é“¾å¼ç»“æ„\", \"pinyin\": \"liÃ n shÃ¬ jiÃ©gÃ²u\", \"trans\": \"chain structure\"},\n    {\"word\": \"æ‰©å±•æ•ˆç‡\", \"pinyin\": \"kuÃ² zhÇn xiÃ o lÇœ\", \"trans\": \"scalability\"},\n    {\"word\": \"éƒ¨ç½²\", \"pinyin\": \"bÃ¹ shÇ”\", \"trans\": \"deployment\"},\n    {\"word\": \"çµæ´»æ€§\", \"pinyin\": \"lÃ­ng huÃ³ xÃ¬ng\", \"trans\": \"flexibility\"},\n    {\"word\": \"Chain-of-Representation\", \"pinyin\": \"ChÃ¨in-Ã²f-RÄ›prizen tÃ©i shÄ“n\", \"trans\": \"Chain-of-Representation\"},\n    {\"word\": \"å­è¡¨ç¤º\", \"pinyin\": \"zÇ biÇo shÃ¬\", \"trans\": \"sub-representation\"},\n    {\"word\": \"ç»„åˆ\", \"pinyin\": \"zÇ” hÃ©\", \"trans\": \"combination\"},\n    {\"word\": \"å‰åºé“¾\", \"pinyin\": \"qiÃ¡n xÃ¹ liÃ n\", \"trans\": \"preceding chain\"},\n    {\"word\": \"å¼¹æ€§æ¨ç†\", \"pinyin\": \"tÃ¡n xÃ¬ng tuÄ« lÇ\", \"trans\": \"elastic inference\"},\n    {\"word\": \"Chain-of-Language-Model\", \"pinyin\": \"ChÃ¨in-Ã²f-LÃ¡nggÃ¹ MÃ³del\", \"trans\": \"Chain-of-Language-Model\"},\n    {\"word\": \"KVå…±äº«æœºåˆ¶\", \"pinyin\": \"KV gÃ²ng xiÇng jÄ« zhÃ¬\", \"trans\": \"KV sharing mechanism\"},\n    {\"word\": \"CoLM-Air\", \"pinyin\": \"CoLM-Ã‰ir\", \"trans\": \"CoLM-Air\"},\n    {\"word\": \"æ‰©å±•åŠŸèƒ½\", \"pinyin\": \"kuÃ² zhÇn gÅng nÃ©ng\", \"trans\": \"extended functionality\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"TÃ¨inshÃ¨in fÅmÄ›i\", \"trans\": \"Transformer\"}\n]",
        "trans": "This article proposes a new learning paradigm called Chain-of-Model (CoM). It introduces causality into the hidden states of each layer, forming a chain-like structure that enhances the scalability of model training and the flexibility of deployment. The authors introduce the concept of Chain-of-Representation (CoR), representing the hidden states of each layer as a combination of multiple sub-representations (i.e., chains). Within each layer, each chain can only view all preceding chains in the input representation. Therefore, models based on the CoM framework can incrementally scale the model size by adding chains and provide multiple sub-models of different sizes for elastic inference by using different numbers of chains. Based on this principle, the authors designed Chain-of-Language-Model (CoLM) and further introduced CoLM-Air with a KV sharing mechanism to achieve more scalable functionalities. Experimental results show that the CoLM series models perform comparably to standard Transformers while offering greater flexibility.",
        "update_ts": "2025-05-20 09:13"
    }
}