{
    "date": {
        "ru": "21 Ğ¼Ğ°Ñ",
        "en": "May 21",
        "zh": "5æœˆ21æ—¥"
    },
    "time_utc": "2025-05-21 03:37",
    "weekday": 2,
    "issue_id": 3869,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.14683",
            "title": "Emerging Properties in Unified Multimodal Pretraining",
            "url": "https://huggingface.co/papers/2505.14683",
            "abstract": "Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open0source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder0only model pretrained on trillions of tokens curated from large0scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/",
            "score": 29,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "57522649bb8f8010",
            "authors": [
                "Chaorui Deng",
                "Deyao Zhu",
                "Kunchang Li",
                "Chenhui Gou",
                "Feng Li",
                "Zeyu Wang",
                "Shu Zhong",
                "Weihao Yu",
                "Xiaonan Nie",
                "Ziang Song",
                "Guang Shi",
                "Haoqi Fan"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Hong Kong University of Science and Technology",
                "Monash University",
                "Shenzhen Institutes of Advanced Technology",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14683.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#reasoning",
                    "#open_source",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ¥¯",
                "ru": {
                    "title": "BAGEL: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "BAGEL - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ…, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. BAGEL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ 3D-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "BAGEL: Unifying Multimodal AI for Enhanced Understanding and Generation",
                    "desc": "This paper presents BAGEL, an open-source foundational model designed for multimodal understanding and generation. BAGEL is a decoder-only model that has been pretrained on a vast dataset comprising text, images, videos, and web content. By leveraging this diverse multimodal data, BAGEL demonstrates advanced capabilities in complex reasoning tasks, outperforming existing open-source models. The authors aim to promote further research in multimodal AI by sharing their findings, pretraining methods, and code with the community."
                },
                "zh": {
                    "title": "BAGELï¼šå¼€æºå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºBAGELçš„å¼€æºåŸºç¡€æ¨¡å‹ï¼Œå®ƒæ”¯æŒå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚BAGELæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§£ç å™¨æ¨¡å‹ï¼Œç»è¿‡åœ¨å¤§é‡æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç½‘ç»œæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚é€šè¿‡ä½¿ç”¨å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®ï¼ŒBAGELåœ¨å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†æ–¹é¢å±•ç°å‡ºæ–°çš„èƒ½åŠ›ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¼€æºç»Ÿä¸€æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡åˆ†äº«å…³é”®å‘ç°ã€é¢„è®­ç»ƒç»†èŠ‚å’Œæ•°æ®åˆ›å»ºåè®®ï¼Œä¿ƒè¿›å¤šæ¨¡æ€ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11594",
            "title": "SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training",
            "url": "https://huggingface.co/papers/2505.11594",
            "abstract": "The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code will be available at https://github.com/thu-ml/SageAttention.",
            "score": 16,
            "issue_id": 3869,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "33309444d442b40c",
            "authors": [
                "Jintao Zhang",
                "Jia Wei",
                "Pengle Zhang",
                "Xiaoming Xu",
                "Haofeng Huang",
                "Haoxu Wang",
                "Kai Jiang",
                "Jun Zhu",
                "Jianfei Chen"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11594.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ¾Ñ‚ FP4 Ğ´Ğ¾ 8-Ğ±Ğ¸Ñ‚",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. Ğ’Ğ¾-Ğ¿ĞµÑ€Ğ²Ñ‹Ñ…, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ½Ñ‹Ğµ ÑĞ´Ñ€Ğ° FP4 Ğ² GPU Blackwell Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 5-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ FlashAttention. Ğ’Ğ¾-Ğ²Ñ‚Ğ¾Ñ€Ñ‹Ñ…, Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ 8-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Attention: Fast and Efficient for Training and Inference",
                    "desc": "This paper addresses the inefficiency of attention mechanisms in machine learning, which typically have a quadratic time complexity. The authors introduce enhancements using FP4 Tensor Cores in Blackwell GPUs, achieving a significant speedup in attention computation, reaching 1038 TOPS on the RTX5090. Additionally, they explore low-bit attention for training tasks, proposing an 8-bit attention method that maintains performance during fine-tuning while showing slower convergence during pretraining. This work not only improves inference speed but also expands the application of low-bit attention to training, making it a versatile solution for large model training."
                },
                "zh": {
                    "title": "æå‡æ³¨æ„åŠ›æœºåˆ¶æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ³¨æ„åŠ›æœºåˆ¶çš„æ•ˆç‡é—®é¢˜ï¼Œä¸»è¦ç”±äºå…¶äºŒæ¬¡æ—¶é—´å¤æ‚åº¦ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨Blackwell GPUä¸­çš„æ–°FP4 Tensor Coresæ¥åŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—ï¼Œå®ç°äº†åœ¨RTX5090ä¸Šè¾¾åˆ°1038 TOPSçš„æ€§èƒ½ï¼Œç›¸æ¯”äºæœ€å¿«çš„FlashAttentionæå‡äº†5å€ã€‚æˆ‘ä»¬çš„FP4æ³¨æ„åŠ›å¯ä»¥ä»¥å³æ’å³ç”¨çš„æ–¹å¼åŠ é€Ÿå„ç§æ¨¡å‹çš„æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é¦–æ¬¡å°†ä½ä½æ³¨æ„åŠ›åº”ç”¨äºè®­ç»ƒä»»åŠ¡ï¼Œè®¾è®¡äº†é«˜æ•ˆçš„8ä½æ³¨æ„åŠ›ï¼Œå®éªŒè¡¨æ˜åœ¨å¾®è°ƒä»»åŠ¡ä¸­è¡¨ç°æ— æŸï¼Œä½†åœ¨é¢„è®­ç»ƒä»»åŠ¡ä¸­æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14513",
            "title": "Latent Flow Transformer",
            "url": "https://huggingface.co/papers/2505.14513",
            "abstract": "Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in preserving coupling by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms.",
            "score": 8,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "3683bab427c47086",
            "authors": [
                "Yen-Chen Wu",
                "Feng-Ting Liao",
                "Meng-Hsi Chen",
                "Pei-Chen Ho",
                "Farhang Nabiei",
                "Da-shan Shiu"
            ],
            "affiliations": [
                "MediaTek Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14513.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ²: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Latent Flow Transformer (LFT), Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. LFT Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Flow Walking Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Pythia-410M Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LFT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¶Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞµĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Efficient Layer Compression with Latent Flow Transformers",
                    "desc": "This paper introduces the Latent Flow Transformer (LFT), a new architecture for large language models that replaces multiple discrete layers with a single learned transport operator. By utilizing flow matching, LFT achieves significant model compression while still being compatible with traditional transformer designs. The authors also present the Flow Walking (FW) algorithm to enhance the coupling preservation in flow-based methods. Experimental results show that LFT can effectively reduce the number of layers while improving performance metrics, bridging the gap between autoregressive and flow-based generation techniques."
                },
                "zh": {
                    "title": "æ½œåœ¨æµå˜æ¢å™¨ï¼šé«˜æ•ˆå‹ç¼©å¤§è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹â€”â€”æ½œåœ¨æµå˜æ¢å™¨ï¼ˆLatent Flow Transformer, LFTï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚LFTé€šè¿‡ä½¿ç”¨å­¦ä¹ çš„ä¼ è¾“ç®—å­æ›¿ä»£å¤šä¸ªç¦»æ•£å±‚ï¼Œä»è€Œå®ç°æ˜¾è‘—çš„å‹ç¼©ï¼ŒåŒæ—¶ä¿æŒä¸åŸå§‹æ¶æ„çš„å…¼å®¹æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æµæ­¥è¡Œï¼ˆFlow Walking, FWï¼‰ç®—æ³•ï¼Œä»¥è§£å†³ç°æœ‰æµåŸºæ–¹æ³•åœ¨ä¿æŒè€¦åˆæ–¹é¢çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLFTåœ¨å‹ç¼©å±‚æ•°çš„åŒæ—¶ï¼Œèƒ½å¤Ÿåœ¨æ€§èƒ½ä¸Šè¶…è¶Šä¼ ç»Ÿçš„å±‚è·³è¿‡æ–¹æ³•ï¼Œç¼©å°è‡ªå›å½’å’Œæµç”ŸæˆèŒƒå¼ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13866",
            "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
            "url": "https://huggingface.co/papers/2505.13866",
            "abstract": "Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers. While this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and throughput of token generation, limiting the practical deployment of such models. We propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths. RPC periodically compresses the KV cache by retaining KV cache that receive high importance score, which are computed using a selector window composed of recently generated queries. Experiments show that RPC improves generation throughput of QwQ-32B by up to 1.60times compared to the inference with full KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression.",
            "score": 6,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "72f6460e348e135a",
            "authors": [
                "Jiwon Song",
                "Dongwon Jo",
                "Yulhwa Kim",
                "Jae-Joon Kim"
            ],
            "affiliations": [
                "Seoul National University",
                "Sungkyunkwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13866.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ñ ĞŸÑƒÑ‚Ğ¸ Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RPC) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. RPC Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ KV-ĞºÑÑˆ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RPC ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ QwQ-32B Ğ´Ğ¾ 1.60 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ KV-ĞºÑÑˆĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ»ĞµĞ´Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Efficient Inference with Reasoning Path Compression",
                    "desc": "This paper introduces Reasoning Path Compression (RPC), a method designed to enhance the efficiency of reasoning-focused language models during inference. By utilizing the concept of semantic sparsity, RPC compresses the key-value (KV) cache, retaining only the most important elements based on recent queries. This approach significantly increases the throughput of token generation while only slightly affecting accuracy. The results indicate that RPC can improve the performance of large models like QwQ-32B, making them more practical for real-world applications."
                },
                "zh": {
                    "title": "æ¨ç†è·¯å¾„å‹ç¼©ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "æœ€è¿‘ä¸“æ³¨äºæ¨ç†çš„è¯­è¨€æ¨¡å‹é€šè¿‡ç”Ÿæˆè¾ƒé•¿çš„ä¸­é—´æ¨ç†è·¯å¾„æ¥å®ç°é«˜å‡†ç¡®ç‡ã€‚è¿™ç§æ–¹æ³•åœ¨è§£å†³éœ€è¦é€»è¾‘æ€ç»´çš„é—®é¢˜æ—¶éå¸¸æœ‰æ•ˆï¼Œä½†é•¿æ¨ç†è·¯å¾„æ˜¾è‘—å¢åŠ äº†å†…å­˜ä½¿ç”¨å’Œä»¤ç‰Œç”Ÿæˆçš„ååé‡ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å®é™…åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ¨ç†è·¯å¾„å‹ç¼©ï¼ˆRPCï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æ¨ç†è·¯å¾„çš„è¯­ä¹‰ç¨€ç–æ€§æ¥åŠ é€Ÿæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒRPCåœ¨AIME 2024åŸºå‡†æµ‹è¯•ä¸­ç›¸æ¯”äºå®Œæ•´KVç¼“å­˜ï¼Œæå‡äº†QwQ-32Bçš„ç”Ÿæˆååé‡ï¼Œå‡†ç¡®ç‡ä»…ä¸‹é™1.2%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14652",
            "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
            "url": "https://huggingface.co/papers/2505.14652",
            "abstract": "Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks.",
            "score": 4,
            "issue_id": 3869,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "494fe90709dc6c63",
            "authors": [
                "Xueguang Ma",
                "Qian Liu",
                "Dongfu Jiang",
                "Ge Zhang",
                "Zejun Ma",
                "Wenhu Chen"
            ],
            "affiliations": [
                "M-A-P",
                "Singapore",
                "TikTok",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14652.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#benchmark",
                    "#math",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ĞµĞ»ÑŒ: Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ General-Reasoner, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ General-Reasoner Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering LLMs with General-Reasoner for Diverse Reasoning",
                    "desc": "This paper introduces General-Reasoner, a new approach to improve the reasoning abilities of large language models (LLMs) across various fields. It leverages a large-scale dataset of questions with verifiable answers, which is created through web crawling, to train LLMs without the need for prior supervised fine-tuning. Additionally, it employs a generative model-based answer verifier that enhances the model's ability to understand context and think through problems. The results show that General-Reasoner significantly outperforms existing methods in reasoning tasks, especially in mathematics, while also being effective in other domains like physics and finance."
                },
                "zh": {
                    "title": "æå‡LLMæ¨ç†èƒ½åŠ›çš„æ–°èŒƒå¼",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒèŒƒå¼â€”â€”General-Reasonerï¼Œæ—¨åœ¨å¢å¼ºLLMåœ¨å¤šé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é—®é¢˜æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åŸºäºç”Ÿæˆæ¨¡å‹çš„ç­”æ¡ˆéªŒè¯å™¨ï¼Œå–ä»£äº†ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„éªŒè¯æ–¹æ³•ã€‚é€šè¿‡åœ¨å¤šä¸ªé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒGeneral-Reasoneråœ¨æ¨ç†æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14680",
            "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search",
            "url": "https://huggingface.co/papers/2505.14680",
            "abstract": "Generative AI search is reshaping information retrieval by offering end-to-end answers to complex queries, reducing users' reliance on manually browsing and summarizing multiple web pages. However, while this paradigm enhances convenience, it disrupts the feedback-driven improvement loop that has historically powered the evolution of traditional Web search. Web search can continuously improve their ranking models by collecting large-scale, fine-grained user feedback (e.g., clicks, dwell time) at the document level. In contrast, generative AI search operates through a much longer search pipeline, spanning query decomposition, document retrieval, and answer generation, yet typically receives only coarse-grained feedback on the final answer. This introduces a feedback loop disconnect, where user feedback for the final output cannot be effectively mapped back to specific system components, making it difficult to improve each intermediate stage and sustain the feedback loop. In this paper, we envision NExT-Search, a next-generation paradigm designed to reintroduce fine-grained, process-level feedback into generative AI search. NExT-Search integrates two complementary modes: User Debug Mode, which allows engaged users to intervene at key stages; and Shadow User Mode, where a personalized user agent simulates user preferences and provides AI-assisted feedback for less interactive users. Furthermore, we envision how these feedback signals can be leveraged through online adaptation, which refines current search outputs in real-time, and offline update, which aggregates interaction logs to periodically fine-tune query decomposition, retrieval, and generation models. By restoring human control over key stages of the generative AI search pipeline, we believe NExT-Search offers a promising direction for building feedback-rich AI search systems that can evolve continuously alongside human feedback.",
            "score": 3,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "ace242db16327202",
            "authors": [
                "Sunhao Dai",
                "Wenjie Wang",
                "Liang Pang",
                "Jun Xu",
                "See-Kiong Ng",
                "Ji-Rong Wen",
                "Tat-Seng Chua"
            ],
            "affiliations": [
                "CAS Key Laboratory of AI Safety Institute of Computing Technology Chinese Academy of Sciences",
                "Gaoling School of Artificial Intelligence Renmin University of China",
                "National University of Singapore",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14680.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#rag",
                    "#rlhf",
                    "#agents",
                    "#alignment"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ² Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¸ÑĞº",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ NExT-Search, Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ½Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ ÑƒÑ‚Ñ€Ğ°Ñ‡ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜. NExT-Search Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°: Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ñ‚ĞµĞ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñƒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "NExT-Search: Enhancing Generative AI Search with User Feedback",
                    "desc": "This paper discusses the challenges of integrating user feedback into generative AI search systems, which provide direct answers to complex queries but lack detailed feedback mechanisms. Traditional web search benefits from fine-grained user interactions, allowing for continuous improvement of ranking models. The proposed NExT-Search framework aims to bridge this gap by introducing two modes of user feedback: User Debug Mode for active user engagement and Shadow User Mode for passive feedback collection. By leveraging both real-time and aggregated feedback, NExT-Search seeks to enhance the generative AI search process and ensure it evolves in response to user needs."
                },
                "zh": {
                    "title": "NExT-Searchï¼šé‡å¡‘ç”Ÿæˆå¼æœç´¢çš„åé¦ˆå¾ªç¯",
                    "desc": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æœç´¢æ­£åœ¨æ”¹å˜ä¿¡æ¯æ£€ç´¢ï¼Œé€šè¿‡æä¾›ç«¯åˆ°ç«¯çš„ç­”æ¡ˆæ¥åº”å¯¹å¤æ‚æŸ¥è¯¢ï¼Œå‡å°‘ç”¨æˆ·æ‰‹åŠ¨æµè§ˆå’Œæ€»ç»“å¤šä¸ªç½‘é¡µçš„ä¾èµ–ã€‚ç„¶è€Œï¼Œè¿™ç§æ–°æ¨¡å¼è™½ç„¶æé«˜äº†ä¾¿åˆ©æ€§ï¼Œå´æ‰“ç ´äº†ä¼ ç»Ÿç½‘é¡µæœç´¢ä¸­åŸºäºåé¦ˆçš„æ”¹è¿›å¾ªç¯ã€‚ä¼ ç»Ÿæœç´¢å¯ä»¥é€šè¿‡æ”¶é›†ç”¨æˆ·åé¦ˆï¼ˆå¦‚ç‚¹å‡»ç‡å’Œåœç•™æ—¶é—´ï¼‰æ¥ä¸æ–­æ”¹è¿›æ’åæ¨¡å‹ï¼Œè€Œç”Ÿæˆå¼æœç´¢åˆ™é¢ä¸´åé¦ˆå¾ªç¯æ–­è£‚çš„é—®é¢˜ï¼Œç”¨æˆ·åé¦ˆéš¾ä»¥æœ‰æ•ˆæ˜ å°„åˆ°ç³»ç»Ÿçš„å…·ä½“ç»„ä»¶ã€‚æœ¬æ–‡æå‡ºäº†NExT-Searchï¼Œæ—¨åœ¨å°†ç»†ç²’åº¦çš„è¿‡ç¨‹çº§åé¦ˆé‡æ–°å¼•å…¥ç”Ÿæˆå¼æœç´¢ï¼Œç»“åˆç”¨æˆ·è°ƒè¯•æ¨¡å¼å’Œå½±å­ç”¨æˆ·æ¨¡å¼ï¼Œä»¥å®ç°å®æ—¶å’Œç¦»çº¿çš„åé¦ˆä¿¡å·åˆ©ç”¨ï¼Œä»è€ŒæŒç»­æ”¹è¿›æœç´¢ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14640",
            "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
            "url": "https://huggingface.co/papers/2505.14640",
            "abstract": "Large multimodal models (LMMs) have recently emerged as a powerful tool for long video understanding (LVU), prompting the development of standardized LVU benchmarks to evaluate their performance. However, our investigation reveals a rather sober lesson for existing LVU benchmarks. First, most existing benchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation results are inflated due to the possibility of guessing the correct answer; Second, a significant portion of questions in these benchmarks have strong priors to allow models to answer directly without even reading the input video. For example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame from a long video on Video-MME. We also observe that increasing the number of frames does not necessarily lead to improvement on existing benchmarks, which is counterintuitive. As a result, the validity and robustness of current LVU benchmarks are undermined, impeding a faithful assessment of LMMs' long-video understanding capability. To tackle this problem, we propose VideoEval-Pro, a realistic LVU benchmark containing questions with open-ended short-answer, which truly require understanding the entire video. VideoEval-Pro assesses both segment-level and full-video understanding through perception and reasoning tasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the following findings: (1) video LMMs show drastic performance (>25\\%) drops on open-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do not lead to higher open-ended scores on VideoEval-Pro; (3) compared to other MCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input frames. Our results show that VideoEval-Pro offers a more realistic and reliable measure of long video understanding, providing a clearer view of progress in this domain.",
            "score": 2,
            "issue_id": 3869,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "45d64d535935c6a4",
            "authors": [
                "Wentao Ma",
                "Weiming Ren",
                "Yiming Jia",
                "Zhuofeng Li",
                "Ping Nie",
                "Ge Zhang",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Independent",
                "M-A-P",
                "Shanghai University",
                "University of Toronto",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14640.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LMM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ·Ğ°Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ¸Ğ·-Ğ·Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VideoEval-Pro Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ¶Ğµ, Ñ‡ĞµĞ¼ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LMM Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Long Video Understanding with Realistic Benchmarks",
                    "desc": "This paper discusses the limitations of current benchmarks for evaluating long video understanding (LVU) in large multimodal models (LMMs). It highlights that many existing benchmarks rely on multiple-choice questions (MCQs), which can inflate performance scores due to guessing and prior knowledge. The authors introduce VideoEval-Pro, a new benchmark that uses open-ended questions requiring comprehensive video understanding, thus providing a more accurate assessment of LMM capabilities. Their findings reveal significant performance drops for LMMs on open-ended questions compared to MCQs, indicating that current benchmarks may not effectively measure true understanding of long videos."
                },
                "zh": {
                    "title": "VideoEval-Proï¼šæå‡é•¿è§†é¢‘ç†è§£çš„çœŸå®è¯„ä¼°",
                    "desc": "å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨é•¿è§†é¢‘ç†è§£ï¼ˆLVUï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰çš„LVUåŸºå‡†æµ‹è¯•å­˜åœ¨é—®é¢˜ã€‚è®¸å¤šåŸºå‡†ä¾èµ–å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ï¼Œè¿™å¯¼è‡´è¯„ä¼°ç»“æœè¢«å¤¸å¤§ï¼Œå› ä¸ºæ¨¡å‹å¯èƒ½é€šè¿‡çŒœæµ‹è·å¾—æ­£ç¡®ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œéƒ¨åˆ†é—®é¢˜çš„å…ˆéªŒä¿¡æ¯ä½¿å¾—æ¨¡å‹å¯ä»¥åœ¨ä¸è§‚çœ‹è§†é¢‘çš„æƒ…å†µä¸‹ç›´æ¥å›ç­”ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VideoEval-ProåŸºå‡†ï¼Œé‡‡ç”¨å¼€æ”¾å¼çŸ­ç­”æ¡ˆé—®é¢˜ï¼ŒçœŸæ­£è€ƒå¯Ÿæ¨¡å‹å¯¹æ•´ä¸ªè§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14464",
            "title": "Not All Correct Answers Are Equal: Why Your Distillation Source Matters",
            "url": "https://huggingface.co/papers/2505.14464",
            "abstract": "Distillation has emerged as a practical and effective approach to enhance the reasoning capabilities of open-source language models. In this work, we conduct a large-scale empirical study on reasoning data distillation by collecting verified outputs from three state-of-the-art teacher models-AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We construct three parallel datasets and analyze their distributions, revealing that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench. The AM-based model consistently achieves the best performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing longer responses for harder tasks and shorter ones for simpler tasks. These findings highlight the value of high-quality, verified reasoning traces. We release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support future research on open and high-performing reasoning-oriented language models. The datasets are publicly available on Hugging FaceDatasets are available on Hugging Face: \\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled{AM-Thinking-v1-Distilled}, https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled{AM-Qwen3-Distilled}.}.",
            "score": 2,
            "issue_id": 3869,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "709996374c466144",
            "authors": [
                "Xiaoyu Tian",
                "Yunjie Ji",
                "Haotian Wang",
                "Shuaiting Chen",
                "Sitong Zhao",
                "Yiping Peng",
                "Han Zhao",
                "Xiangang Li"
            ],
            "affiliations": [
                "Beike (Ke.com)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14464.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#benchmark",
                    "#data",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ Ñ‚Ñ€ĞµÑ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 1,89 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ AM-Thinking-v1, Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Reasoning in Language Models through Data Distillation",
                    "desc": "This paper explores the process of distillation to improve the reasoning abilities of open-source language models. The authors conducted a large-scale study using outputs from three advanced teacher models on a dataset of 1.89 million queries. They found that the distilled data from the AM-Thinking-v1 model had better diversity in token length and lower perplexity, leading to superior performance on various reasoning benchmarks. The results indicate that high-quality reasoning data is crucial for training effective student models, and the authors have made their datasets publicly available for further research."
                },
                "zh": {
                    "title": "è’¸é¦æŠ€æœ¯æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†é€šè¿‡è’¸é¦æŠ€æœ¯æå‡å¼€æºè¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚æˆ‘ä»¬æ”¶é›†äº†æ¥è‡ªä¸‰ç§å…ˆè¿›æ•™å¸ˆæ¨¡å‹çš„éªŒè¯è¾“å‡ºï¼Œå¹¶æ„å»ºäº†ä¸‰ä¸ªå¹³è¡Œæ•°æ®é›†è¿›è¡Œåˆ†æã€‚ç»“æœæ˜¾ç¤ºï¼ŒAM-Thinking-v1è’¸é¦æ•°æ®åœ¨æ ‡è®°é•¿åº¦å¤šæ ·æ€§å’Œå›°æƒ‘åº¦æ–¹é¢è¡¨ç°æ›´ä½³ã€‚ç»è¿‡è®­ç»ƒçš„å­¦ç”Ÿæ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯AM-Thinking-v1æ¨¡å‹åœ¨å„é¡¹æµ‹è¯•ä¸­å‡å–å¾—äº†æœ€ä½³æˆç»©ï¼Œå±•ç¤ºäº†é«˜è´¨é‡æ¨ç†è½¨è¿¹çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13380",
            "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via\n  Competition",
            "url": "https://huggingface.co/papers/2505.13380",
            "abstract": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an improved version of the previous study at arXiv:2402.02526",
            "score": 1,
            "issue_id": 3868,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "6a5e70a76e6f012c",
            "authors": [
                "Nam V. Nguyen",
                "Huy Nguyen",
                "Quang Pham",
                "Van Nguyen",
                "Savitha Ramasamy",
                "Nhat Ho"
            ],
            "affiliations": [
                "FPT Software AI Center",
                "Independent Researcher",
                "Institute for Infocomm Research, ASTAR",
                "The University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13380.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼ĞµÑÑÑ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (SMoE) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'competition'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ softmax. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ CompeteSMoE Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ CompeteSMoE Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ SMoE."
                },
                "en": {
                    "title": "CompeteSMoE: Efficient Routing for Powerful Language Models",
                    "desc": "Sparse mixture of experts (SMoE) is a method that allows models to become more complex without simply making them deeper or wider. The challenge with SMoE is that the way experts are chosen to process data can be inefficient, as not all experts contribute to the decision-making process. This paper introduces a new routing mechanism called competition, which directs data to the most responsive experts, improving the efficiency of the model. The authors present CompeteSMoE, an algorithm that uses this competition mechanism to train large language models effectively, showing better performance and lower training costs compared to existing methods."
                },
                "zh": {
                    "title": "ç«äº‰æœºåˆ¶æå‡ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹çš„æ•ˆç‡",
                    "desc": "ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆSMoEï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆæå‡æ¨¡å‹å¤æ‚åº¦çš„æ–¹æ³•ï¼Œè¶…è¶Šäº†ç®€å•å¢åŠ ç½‘ç»œæ·±åº¦æˆ–å®½åº¦çš„æ–¹å¼ã€‚ç„¶è€Œï¼ŒSMoEçš„è®­ç»ƒä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯å› ä¸ºè®¡ç®—çš„ä¸“å®¶ä¸è·¯ç”±è¿‡ç¨‹ä¹‹é—´çš„è”ç³»ä¸å¤Ÿç›´æ¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æœºåˆ¶â€”â€”ç«äº‰ï¼Œèƒ½å¤Ÿå°†è¾“å…¥æ•°æ®æ›´æœ‰æ•ˆåœ°è·¯ç”±åˆ°å“åº”æœ€å¼ºçš„ä¸“å®¶ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†ç«äº‰æœºåˆ¶åœ¨æ ·æœ¬æ•ˆç‡ä¸Šä¼˜äºä¼ ç»Ÿçš„softmaxè·¯ç”±ï¼Œå¹¶å¼€å‘äº†CompeteSMoEç®—æ³•ï¼Œèƒ½å¤Ÿä»¥è¾ƒä½çš„è®­ç»ƒå¼€é”€å®ç°å¼ºå¤§çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12448",
            "title": "SSR: Enhancing Depth Perception in Vision-Language Models via\n  Rationale-Guided Spatial Reasoning",
            "url": "https://huggingface.co/papers/2505.12448",
            "abstract": "Despite impressive advancements in Visual-Language Models (VLMs) for multi-modal tasks, their reliance on RGB inputs limits precise spatial understanding. Existing methods for integrating spatial cues, such as point clouds or depth, either require specialized sensors or fail to effectively exploit depth information for higher-order reasoning. To this end, we propose a novel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that transforms raw depth data into structured, interpretable textual rationales. These textual rationales serve as meaningful intermediate representations to significantly enhance spatial reasoning capabilities. Additionally, we leverage knowledge distillation to compress the generated rationales into compact latent embeddings, which facilitate resource-efficient and plug-and-play integration into existing VLMs without retraining. To enable comprehensive evaluation, we introduce a new dataset named SSR-CoT, a million-scale visual-language reasoning dataset enriched with intermediate spatial reasoning annotations, and present SSRBench, a comprehensive multi-task benchmark. Extensive experiments on multiple benchmarks demonstrate SSR substantially improves depth utilization and enhances spatial reasoning, thereby advancing VLMs toward more human-like multi-modal understanding. Our project page is at https://yliu-cs.github.io/SSR.",
            "score": 1,
            "issue_id": 3869,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "18ffd5153e838d86",
            "authors": [
                "Yang Liu",
                "Ming Ma",
                "Xiaomin Yu",
                "Pengxiang Ding",
                "Han Zhao",
                "Mingyang Sun",
                "Siteng Huang",
                "Donglin Wang"
            ],
            "affiliations": [
                "Alibaba DAMO Academy",
                "Harbin Institute of Technology",
                "Shanghai Innovation Institute",
                "The Hong Kong University of Science and Technology (Guangzhou)",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12448.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#cv",
                    "#interpretability",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SSR (Spatial Sense and Reasoning) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (VLM). SSR Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¶Ğ¸Ğ¼Ğ°ÑÑ‚ÑÑ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SSR-CoT Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SSRBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SSR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Enhancing Spatial Reasoning in VLMs with SSR",
                    "desc": "This paper introduces a new method called Spatial Sense and Reasoning (SSR) to improve how Visual-Language Models (VLMs) understand spatial information. SSR transforms raw depth data into structured textual rationales, which help the model reason about space more effectively. The authors also use knowledge distillation to create compact representations of these rationales, allowing for easy integration into existing VLMs without needing to retrain them. To support their research, they present a new dataset, SSR-CoT, and a benchmark, SSRBench, which show that SSR significantly enhances spatial reasoning in VLMs."
                },
                "zh": {
                    "title": "æå‡ç©ºé—´æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å¯¹RGBè¾“å…¥çš„ä¾èµ–é™åˆ¶äº†ç²¾ç¡®çš„ç©ºé—´ç†è§£ã€‚ç°æœ‰çš„æ–¹æ³•åœ¨æ•´åˆç©ºé—´çº¿ç´¢æ—¶ï¼Œå¾€å¾€éœ€è¦ä¸“ç”¨ä¼ æ„Ÿå™¨æˆ–æ— æ³•æœ‰æ•ˆåˆ©ç”¨æ·±åº¦ä¿¡æ¯è¿›è¡Œæ›´é«˜é˜¶çš„æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç©ºé—´æ„ŸçŸ¥ä¸æ¨ç†æ–¹æ³•ï¼ˆSSRï¼‰ï¼Œè¯¥æ¡†æ¶å°†åŸå§‹æ·±åº¦æ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–çš„å¯è§£é‡Šæ–‡æœ¬æ¨ç†ã€‚è¿™äº›æ–‡æœ¬æ¨ç†ä½œä¸ºæœ‰æ„ä¹‰çš„ä¸­é—´è¡¨ç¤ºï¼Œæ˜¾è‘—å¢å¼ºäº†ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡çŸ¥è¯†è’¸é¦å°†ç”Ÿæˆçš„æ¨ç†å‹ç¼©ä¸ºç´§å‡‘çš„æ½œåœ¨åµŒå…¥ï¼Œä¾¿äºä¸ç°æœ‰VLMsçš„é«˜æ•ˆé›†æˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12182",
            "title": "Truth Neurons",
            "url": "https://huggingface.co/papers/2505.12182",
            "abstract": "Despite their remarkable success and deployment across diverse workflows, language models sometimes produce untruthful responses. Our limited understanding of how truthfulness is mechanistically encoded within these models jeopardizes their reliability and safety. In this paper, we propose a method for identifying representations of truthfulness at the neuron level. We show that language models contain truth neurons, which encode truthfulness in a subject-agnostic manner. Experiments conducted across models of varying scales validate the existence of truth neurons, confirming that the encoding of truthfulness at the neuron level is a property shared by many language models. The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness. Selectively suppressing the activations of truth neurons found through the TruthfulQA dataset degrades performance both on TruthfulQA and on other benchmarks, showing that the truthfulness mechanisms are not tied to a specific dataset. Our results offer novel insights into the mechanisms underlying truthfulness in language models and highlight potential directions toward improving their trustworthiness and reliability.",
            "score": 1,
            "issue_id": 3868,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "ddeab64450bb26a9",
            "authors": [
                "Haohang Li",
                "Yupeng Cao",
                "Yangyang Yu",
                "Jordan W. Suchow",
                "Zining Zhu"
            ],
            "affiliations": [
                "Stevens Institute of Technology",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12182.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#hallucinations",
                    "#alignment",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ñ‹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ‚Ğ°Ğº Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ 'Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ñ‹', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ‚ĞµĞ¼Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞŸĞ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unveiling Truth Neurons: Enhancing Language Model Trustworthiness",
                    "desc": "This paper investigates how language models encode truthfulness at the neuron level, revealing the presence of 'truth neurons' that represent truthfulness in a way that is not dependent on specific subjects. The authors demonstrate that these truth neurons exist across various models, indicating a shared property among them. By analyzing the distribution of truth neurons across different layers, the study aligns with previous research on the geometry of truthfulness. Additionally, the suppression of these neurons negatively impacts model performance, suggesting that understanding and improving truthfulness in language models is crucial for their reliability."
                },
                "zh": {
                    "title": "æ­ç¤ºè¯­è¨€æ¨¡å‹ä¸­çš„çœŸç›¸ç¥ç»å…ƒ",
                    "desc": "å°½ç®¡è¯­è¨€æ¨¡å‹åœ¨å„ç§å·¥ä½œæµç¨‹ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†æœ‰æ—¶ä¼šäº§ç”Ÿä¸çœŸå®çš„å›ç­”ã€‚æˆ‘ä»¬å¯¹è¿™äº›æ¨¡å‹ä¸­çœŸç›¸ç¼–ç æœºåˆ¶çš„ç†è§£æœ‰é™ï¼Œè¿™å½±å“äº†å®ƒä»¬çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡ç¥ç»å…ƒå±‚é¢è¯†åˆ«çœŸç›¸çš„è¡¨ç¤ºï¼Œå‘ç°è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨ç¼–ç çœŸç›¸çš„çœŸç›¸ç¥ç»å…ƒã€‚å®éªŒè¡¨æ˜ï¼ŒçœŸç›¸ç¥ç»å…ƒçš„å­˜åœ¨æ˜¯è®¸å¤šè¯­è¨€æ¨¡å‹çš„å…±åŒç‰¹æ€§ï¼Œå¹¶ä¸”å…¶åˆ†å¸ƒæ¨¡å¼ä¸çœŸç›¸çš„å‡ ä½•ç‰¹å¾ä¸€è‡´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14178",
            "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits",
            "url": "https://huggingface.co/papers/2505.14178",
            "abstract": "Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.",
            "score": 0,
            "issue_id": 3868,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "f4fdc7fb140f9273",
            "authors": [
                "Xiang Zhang",
                "Juntai Cao",
                "Jiaqi Wei",
                "Yiwei Xu",
                "Chenyu You"
            ],
            "affiliations": [
                "Cisco",
                "Stony Brook University",
                "University of British Columbia",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14178.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#architecture",
                    "#small_models",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ĞºĞ»ÑÑ‡ Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ…' (Token Awareness) Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ½ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ°Ñ€ÑƒÑˆĞ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Tokenization Matters: Unlocking Reasoning in Language Models",
                    "desc": "This paper explores the importance of tokenization in language models, particularly how it affects reasoning capabilities. It highlights that traditional tokenization methods, like byte-pair encoding (BPE), can obscure essential reasoning units, limiting the model's ability to perform symbolic computation. The authors introduce the concept of Token Awareness, which emphasizes the need for better token granularity to enhance logical alignment and generalization in models. Through experiments on arithmetic and symbolic tasks, they show that models with well-structured token representations can significantly outperform larger models in reasoning tasks."
                },
                "zh": {
                    "title": "åˆ†è¯ç»“æ„å†³å®šæ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œåˆ†è¯ï¼ˆTokenizationï¼‰å¯¹æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œåˆ†è¯æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åŸºäºå­è¯çš„æ–¹æ³•ï¼ˆå¦‚å­—èŠ‚å¯¹ç¼–ç BPEï¼‰ï¼Œä¼šåˆå¹¶æˆ–æ¨¡ç³ŠåŸºæœ¬çš„æ¨ç†å•å…ƒï¼Œä»è€Œå¦¨ç¢ç¬¦å·è®¡ç®—ã€‚æˆ‘ä»¬å¼•å…¥äº†â€œToken Awarenessâ€çš„æ¦‚å¿µï¼Œå¼ºè°ƒäº†åˆ†è¯ç²’åº¦ä¸ä½³å¦‚ä½•å¹²æ‰°é€»è¾‘å¯¹é½ï¼Œé˜»ç¢æ¨¡å‹çš„ç¬¦å·ç¨‹åºæ³›åŒ–ã€‚é€šè¿‡å¯¹ç®—æœ¯å’Œç¬¦å·ä»»åŠ¡çš„ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†åˆ†è¯ç»“æ„æ˜¾è‘—å½±å“æ¨ç†æ€§èƒ½ï¼Œè¾ƒå°çš„æ¨¡å‹åœ¨å¯¹é½æ ¼å¼ä¸‹èƒ½å¤Ÿè¶…è¶Šæ›´å¤§çš„ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14135",
            "title": "Hunyuan-Game: Industrial-grade Intelligent Game Creation Model",
            "url": "https://huggingface.co/papers/2505.14135",
            "abstract": "Intelligent game creation represents a transformative advancement in game development, utilizing generative artificial intelligence to dynamically generate and enhance game content. Despite notable progress in generative models, the comprehensive synthesis of high-quality game assets, including both images and videos, remains a challenging frontier. To create high-fidelity game content that simultaneously aligns with player preferences and significantly boosts designer efficiency, we present Hunyuan-Game, an innovative project designed to revolutionize intelligent game production. Hunyuan-Game encompasses two primary branches: image generation and video generation. The image generation component is built upon a vast dataset comprising billions of game images, leading to the development of a group of customized image generation models tailored for game scenarios: (1) General Text-to-Image Generation. (2) Game Visual Effects Generation, involving text-to-effect and reference image-based game visual effect generation. (3) Transparent Image Generation for characters, scenes, and game visual effects. (4) Game Character Generation based on sketches, black-and-white images, and white models. The video generation component is built upon a comprehensive dataset of millions of game and anime videos, leading to the development of five core algorithmic models, each targeting critical pain points in game development and having robust adaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2) 360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4) Generative Video Super-Resolution. (5) Interactive Game Video Generation. These image and video generation models not only exhibit high-level aesthetic expression but also deeply integrate domain-specific knowledge, establishing a systematic understanding of diverse game and anime art styles.",
            "score": 0,
            "issue_id": 3869,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "344469b85ea1e75e",
            "authors": [
                "Ruihuang Li",
                "Caijin Zhou",
                "Shoujian Zheng",
                "Jianxiang Lu",
                "Jiabin Huang",
                "Comi Chen",
                "Junshu Tang",
                "Guangzheng Xu",
                "Jiale Tao",
                "Hongmei Wang",
                "Donghao Li",
                "Wenqing Yu",
                "Senbo Wang",
                "Zhimin Li",
                "Yetshuan Shi",
                "Haoyu Yang",
                "Yukun Wang",
                "Wenxun Dai",
                "Jiaqi Li",
                "Linqing Wang",
                "Qixun Wang",
                "Zhiyong Xu",
                "Yingfang Zhang",
                "Jiangfeng Xiong",
                "Weijie Kong",
                "Chao Zhang",
                "Hongxin Zhang",
                "Qiaoling Zheng",
                "Weiting Guo",
                "Xinchi Deng",
                "Yixuan Li",
                "Renjia Wei",
                "Yulin Jian",
                "Duojun Huang",
                "Xuhua Ren",
                "Sihuan Lin",
                "Yifu Sun",
                "Yuan Zhou",
                "Joey Wang",
                "Qin Lin",
                "Jingmiao Yu",
                "Jihong Zhang",
                "Caesar Zhong",
                "Di Wang",
                "Yuhong Liu",
                "Linus",
                "Jie Jiang",
                "Longhuang Wu",
                "Shuai Shao",
                "Qinglin Lu"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14135.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#cv",
                    "#games",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ³Ñ€: Ğ˜Ğ˜ Ğ½Ğ° ÑĞ»ÑƒĞ¶Ğ±Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ²",
                    "desc": "ĞŸÑ€Ğ¾ĞµĞºÑ‚ Hunyuan-Game Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ³Ñ€ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ´Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµÑ‚Ğ²Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸Ğ³Ñ€."
                },
                "en": {
                    "title": "Revolutionizing Game Development with AI-Driven Content Creation",
                    "desc": "The paper introduces Hunyuan-Game, a project that leverages generative artificial intelligence to enhance game development by creating high-quality game assets. It focuses on two main areas: image generation and video generation, utilizing extensive datasets of game images and videos. The image generation models include various techniques for creating game visuals, such as text-to-image and character generation from sketches. The video generation models address specific challenges in game video production, offering solutions like image-to-video synthesis and interactive video generation, all while maintaining aesthetic quality and understanding of game art styles."
                },
                "zh": {
                    "title": "æ™ºèƒ½æ¸¸æˆåˆ›ä½œçš„æœªæ¥",
                    "desc": "æ™ºèƒ½æ¸¸æˆåˆ›ä½œæ˜¯æ¸¸æˆå¼€å‘ä¸­çš„ä¸€é¡¹å˜é©æ€§è¿›å±•ï¼Œåˆ©ç”¨ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åŠ¨æ€ç”Ÿæˆå’Œå¢å¼ºæ¸¸æˆå†…å®¹ã€‚å°½ç®¡ç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†é«˜è´¨é‡æ¸¸æˆèµ„äº§çš„ç»¼åˆåˆæˆä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚Hunyuan-Gameé¡¹ç›®æ—¨åœ¨é€šè¿‡å›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œæå‡æ¸¸æˆå†…å®¹çš„è´¨é‡å’Œè®¾è®¡å¸ˆçš„æ•ˆç‡ã€‚è¯¥é¡¹ç›®åŒ…æ‹¬å›¾åƒç”Ÿæˆå’Œè§†é¢‘ç”Ÿæˆä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼Œæ¶µç›–äº†å¤šç§å®šåˆ¶åŒ–çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿæ»¡è¶³ä¸åŒæ¸¸æˆåœºæ™¯çš„éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.12306",
            "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for\n  Real-world Knowledge Injection",
            "url": "https://huggingface.co/papers/2505.12306",
            "abstract": "Despite significant advances in large language models (LLMs), their knowledge memorization capabilities remain underexplored, due to the lack of standardized and high-quality test ground. In this paper, we introduce a novel, real-world and large-scale knowledge injection benchmark that evolves continuously over time without requiring human intervention. Specifically, we propose WikiDYK, which leverages recently-added and human-written facts from Wikipedia's \"Did You Know...\" entries. These entries are carefully selected by expert Wikipedia editors based on criteria such as verifiability and clarity. Each entry is converted into multiple question-answer pairs spanning diverse task formats from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290 facts and 77,180 questions, which is also seamlessly extensible with future updates from Wikipedia editors. Extensive experiments using continued pre-training reveal a surprising insight: despite their prevalence in modern LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), exhibiting a 23% lower accuracy in terms of reliability. To compensate for the smaller scales of current BiLMs, we introduce a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to integrate with LLMs. Experiment shows that our framework further improves the reliability accuracy by up to 29.1%.",
            "score": 0,
            "issue_id": 3868,
            "pub_date": "2025-05-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ",
                "en": "May 18",
                "zh": "5æœˆ18æ—¥"
            },
            "hash": "ccbad06f5ba35418",
            "authors": [
                "Yuwei Zhang",
                "Wenhao Yu",
                "Shangbin Feng",
                "Yifan Zhu",
                "Letian Peng",
                "Jayanth Srinivasa",
                "Gaowen Liu",
                "Jingbo Shang"
            ],
            "affiliations": [
                "Cisco",
                "Tencent AI Lab",
                "UC, San Diego",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.12306.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#interpretability",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "WikiDYK: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WikiDYK Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. WikiDYK Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ° Wikipedia 'Did You Know...', Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ¸Ñ… Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (BiLM) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ñ‡ĞµĞ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (CLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ğ¸ BiLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Knowledge Memorization in Language Models with WikiDYK",
                    "desc": "This paper presents WikiDYK, a new benchmark for evaluating knowledge memorization in large language models (LLMs). It uses real-world facts from Wikipedia's 'Did You Know...' entries to create a diverse set of question-answer pairs. The study finds that Causal Language Models (CLMs) have weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), with a notable accuracy gap. To enhance BiLMs' performance, the authors propose a collaborative framework that combines multiple BiLMs as external knowledge sources, resulting in improved accuracy in knowledge retrieval tasks."
                },
                "zh": {
                    "title": "çŸ¥è¯†è®°å¿†èƒ½åŠ›çš„æ–°åŸºå‡†ï¼šWikiDYK",
                    "desc": "å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬çš„çŸ¥è¯†è®°å¿†èƒ½åŠ›ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ã€çœŸå®ä¸–ç•Œçš„å¤§è§„æ¨¡çŸ¥è¯†æ³¨å…¥åŸºå‡†ï¼Œåä¸ºWikiDYKï¼Œèƒ½å¤Ÿéšç€æ—¶é—´çš„æ¨ç§»ä¸æ–­æ¼”å˜ï¼Œè€Œæ— éœ€äººå·¥å¹²é¢„ã€‚WikiDYKåˆ©ç”¨ç»´åŸºç™¾ç§‘â€œä½ çŸ¥é“å—...â€æ¡ç›®ä¸­æœ€è¿‘æ·»åŠ çš„ã€ç”±äººç±»æ’°å†™çš„äº‹å®ï¼Œç»è¿‡ä¸“å®¶ç¼–è¾‘çš„ä¸¥æ ¼ç­›é€‰ï¼Œç¡®ä¿å…¶å¯éªŒè¯æ€§å’Œæ¸…æ™°æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å› æœè¯­è¨€æ¨¡å‹ï¼ˆCLMsï¼‰åœ¨ç°ä»£LLMsä¸­æ™®éå­˜åœ¨ï¼Œä½†å…¶çŸ¥è¯†è®°å¿†èƒ½åŠ›æ˜¾è‘—ä½äºåŒå‘è¯­è¨€æ¨¡å‹ï¼ˆBiLMsï¼‰ï¼Œå‡†ç¡®æ€§ä½23%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11966",
            "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative\n  Verifier",
            "url": "https://huggingface.co/papers/2505.11966",
            "abstract": "Large Language Model (LLM) reasoning for complex tasks inherently involves a trade-off between solution accuracy and computational efficiency. The subsequent step of verification, while intended to improve performance, further complicates this landscape by introducing its own challenging trade-off: sophisticated Generative Reward Models (GenRMs) can be computationally prohibitive if naively integrated with LLMs at test-time, while simpler, faster methods may lack reliability. To overcome these challenges, we introduce FlexiVe, a novel generative verifier that flexibly balances computational resources between rapid, reliable fast thinking and meticulous slow thinking using a Flexible Allocation of Verification Budget strategy. We further propose the Solve-Detect-Verify pipeline, an efficient inference-time scaling framework that intelligently integrates FlexiVe, proactively identifying solution completion points to trigger targeted verification and provide focused solver feedback. Experiments show FlexiVe achieves superior accuracy in pinpointing errors within reasoning traces on ProcessBench. Furthermore, on challenging mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full approach outperforms baselines like self-consistency in reasoning accuracy and inference efficiency. Our system offers a scalable and effective solution to enhance LLM reasoning at test time.",
            "score": 0,
            "issue_id": 3869,
            "pub_date": "2025-05-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ",
                "en": "May 17",
                "zh": "5æœˆ17æ—¥"
            },
            "hash": "cd659e075a3efafa",
            "authors": [
                "Jianyuan Zhong",
                "Zeju Li",
                "Zhijian Xu",
                "Xiangyu Wen",
                "Kezhi Li",
                "Qiang Xu"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11966.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#math",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FlexiVe - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). FlexiVe Ğ³Ğ¸Ğ±ĞºĞ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Solve-Detect-Verify Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FlexiVe Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Balancing Speed and Accuracy in LLM Reasoning with FlexiVe",
                    "desc": "This paper discusses the challenges of using Large Language Models (LLMs) for complex tasks, particularly the balance between accuracy and computational efficiency. It introduces FlexiVe, a generative verifier that optimizes the use of computational resources by allowing for both quick and thorough reasoning processes. The authors propose a Solve-Detect-Verify pipeline that enhances the integration of FlexiVe, enabling targeted verification and improved feedback during inference. Experimental results demonstrate that FlexiVe significantly improves error detection and reasoning accuracy on various benchmarks compared to traditional methods."
                },
                "zh": {
                    "title": "çµæ´»éªŒè¯ï¼Œæå‡æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡æ¨ç†ä¸­çš„å‡†ç¡®æ€§ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”ŸæˆéªŒè¯å™¨FlexiVeï¼Œå®ƒé€šè¿‡çµæ´»åˆ†é…éªŒè¯é¢„ç®—ï¼Œåœ¨å¿«é€Ÿå¯é çš„æ€ç»´ä¸ç»†è‡´æ…¢æ€ç»´ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†Solve-Detect-Verifyç®¡é“ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¨ç†æ—¶é—´æ‰©å±•æ¡†æ¶ï¼Œèƒ½å¤Ÿæ™ºèƒ½æ•´åˆFlexiVeï¼Œä¸»åŠ¨è¯†åˆ«è§£å†³æ–¹æ¡ˆå®Œæˆç‚¹ä»¥è§¦å‘é’ˆå¯¹æ€§çš„éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlexiVeåœ¨ProcessBenchä¸Šèƒ½å¤Ÿæ›´å‡†ç¡®åœ°å®šä½æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œå¹¶åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10588",
            "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety\n  Systems for Content Moderation",
            "url": "https://huggingface.co/papers/2505.10588",
            "abstract": "This research offers a unique evaluation of how AI systems interpret the digital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first cohort raised alongside AI, Gen Alpha faces new forms of online risk due to immersive digital engagement and a growing mismatch between their evolving communication and existing safety tools. Their distinct language, shaped by gaming, memes, and AI-driven trends, often conceals harmful interactions from both human moderators and automated systems. We assess four leading AI models (GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked harassment and manipulation within Gen Alpha discourse. Using a dataset of 100 recent expressions from gaming platforms, social media, and video content, the study reveals critical comprehension failures with direct implications for online safety. This work contributes: (1) a first-of-its-kind dataset capturing Gen Alpha expressions; (2) a framework to improve AI moderation systems for youth protection; (3) a multi-perspective evaluation including AI systems, human moderators, and parents, with direct input from Gen Alpha co-researchers; and (4) an analysis of how linguistic divergence increases youth vulnerability. Findings highlight the urgent need to redesign safety systems attuned to youth communication, especially given Gen Alpha reluctance to seek help when adults fail to understand their digital world. This study combines the insight of a Gen Alpha researcher with systematic academic analysis to address critical digital safety challenges.",
            "score": 0,
            "issue_id": 3868,
            "pub_date": "2025-05-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ",
                "en": "May 14",
                "zh": "5æœˆ14æ—¥"
            },
            "hash": "cdc9a4f93d65b071",
            "authors": [
                "Manisha Mehta",
                "Fausto Giunchiglia"
            ],
            "affiliations": [
                "University of Trento, Trento, Italy",
                "Warren Hyde Middle School, Cupertino, California, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10588.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#interpretability",
                    "#benchmark",
                    "#ethics",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ñ€ÑŒĞµÑ€: Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹Ğº Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ ĞĞ»ÑŒÑ„Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ Ğ´Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ² Ğ´Ğ¸ÑĞºÑƒÑ€ÑĞµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ ĞĞ»ÑŒÑ„Ğ° Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ framework'Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Ğ˜Ğ˜. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¾ÑÑ‚Ñ€ÑƒÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»Ğ¾Ğ´ĞµĞ¶Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing AI Safety for Generation Alpha",
                    "desc": "This research evaluates how AI systems understand the unique digital language of Generation Alpha, who are growing up with AI technology. It highlights the risks they face online due to their distinct communication styles, influenced by gaming and memes, which can hide harmful interactions from both humans and automated systems. The study tests four AI models on their ability to detect subtle harassment in Gen Alpha's online expressions, revealing significant gaps in their comprehension. The findings emphasize the need for improved AI moderation tools that are better suited to protect youth in their digital environments."
                },
                "zh": {
                    "title": "é‡å¡‘å®‰å…¨ç³»ç»Ÿï¼Œä¿æŠ¤é˜¿å°”æ³•ä¸–ä»£çš„æ•°å­—äº¤æµ",
                    "desc": "æœ¬ç ”ç©¶ç‹¬ç‰¹åœ°è¯„ä¼°äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿå¦‚ä½•è§£è¯»é˜¿å°”æ³•ä¸–ä»£ï¼ˆ2010-2024å¹´å‡ºç”Ÿï¼‰çš„æ•°å­—è¯­è¨€ã€‚é˜¿å°”æ³•ä¸–ä»£æ˜¯é¦–ä¸ªä¸äººå·¥æ™ºèƒ½å…±åŒæˆé•¿çš„ç¾¤ä½“ï¼Œä»–ä»¬åœ¨æ²‰æµ¸å¼æ•°å­—ç¯å¢ƒä¸­é¢ä¸´æ–°çš„åœ¨çº¿é£é™©ã€‚ç ”ç©¶åˆ†æäº†å››ç§é¢†å…ˆçš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆGPT-4ã€Claudeã€Geminiå’ŒLlama 3ï¼‰åœ¨è¯†åˆ«éšè—çš„éªšæ‰°å’Œæ“æ§æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„å®‰å…¨å·¥å…·æœªèƒ½æœ‰æ•ˆç†è§£é˜¿å°”æ³•ä¸–ä»£çš„ç‹¬ç‰¹äº¤æµæ–¹å¼ï¼Œå¼ºè°ƒäº†é‡æ–°è®¾è®¡å®‰å…¨ç³»ç»Ÿçš„ç´§è¿«æ€§ï¼Œä»¥æ›´å¥½åœ°ä¿æŠ¤å¹´è½»ç”¨æˆ·ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-20.html",
    "link_next": "2025-05-22.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "20.05",
        "en": "05/20",
        "zh": "5æœˆ20æ—¥"
    },
    "short_date_next": {
        "ru": "22.05",
        "en": "05/22",
        "zh": "5æœˆ22æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 3,
        "#benchmark": 8,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 6,
        "#reasoning": 8,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºChain-of-Model (CoM)ã€‚å®ƒå°†å› æœå…³ç³»å¼•å…¥æ¯å±‚çš„éšè—çŠ¶æ€ï¼Œå½¢æˆé“¾å¼ç»“æ„ï¼Œæé«˜äº†æ¨¡å‹è®­ç»ƒçš„æ‰©å±•æ•ˆç‡å’Œéƒ¨ç½²çš„çµæ´»æ€§ã€‚ä½œè€…å¼•å…¥äº†Chain-of-Representation (CoR)çš„æ¦‚å¿µï¼Œå°†æ¯å±‚çš„éšè—çŠ¶æ€è¡¨ç¤ºä¸ºå¤šä¸ªå­è¡¨ç¤ºï¼ˆå³é“¾ï¼‰çš„ç»„åˆã€‚æ¯å±‚ä¸­ï¼Œæ¯ä¸ªé“¾åªèƒ½æŸ¥çœ‹è¾“å…¥è¡¨ç¤ºä¸­çš„æ‰€æœ‰å‰åºé“¾ã€‚å› æ­¤ï¼ŒåŸºäºCoMæ¡†æ¶çš„æ¨¡å‹å¯ä»¥é€šè¿‡å¢åŠ é“¾æ¥é€æ­¥æ‰©å±•æ¨¡å‹å¤§å°ï¼Œå¹¶é€šè¿‡ä½¿ç”¨ä¸åŒçš„é“¾æ•°é‡æä¾›å¤šä¸ªä¸åŒå¤§å°çš„å­æ¨¡å‹è¿›è¡Œå¼¹æ€§æ¨ç†ã€‚åŸºäºè¿™ä¸€åŸåˆ™ï¼Œä½œè€…è®¾è®¡äº†Chain-of-Language-Model (CoLM)ï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†KVå…±äº«æœºåˆ¶çš„CoLM-Airï¼Œä»¥å®ç°æ›´å¤šçš„æ‰©å±•åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLMç³»åˆ—æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸æ ‡å‡†Transformerç›¸å½“ï¼ŒåŒæ—¶æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚",
        "title": "Chain-of-Model Learning for Language Model",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÃ­chÅ« le yÄ«zhÇ’ng xÄ«n de xuÃ©xÃ­ fÃ nshÃ¬, chÄ“ngwÃ©i Chain-of-Model (CoM). TÄ jiÄng yÄ«nguÇ’ guÄnxÃ¬ yÇnrÃ¹ mÄ›i cÃ©ng de yÇncÃ¡ng zhuÃ ngtÃ i, xÃ­ngchÃ©ng liÃ nshÃ¬ jiÃ©gÃ²u, tÄ«gÄo le mÃ³xÃ­ng xÃ¹nliÃ n de kuÃ²zhÇn xiÃ olÇœ hÃ© bÃ¹shÇ” de lÃ­nghuÃ³xÃ¬ng. ZuÃ²zhÄ› yÇnrÃ¹ le Chain-of-Representation (CoR) de gÃ iniÃ n, jiÄng mÄ›i cÃ©ng de yÇncÃ¡ng zhuÃ ngtÃ i biÇoshÃ¬ wÃ©i duÅgÃ¨ zÇ biÇoshÃ¬ (jiÄ“ liÃ n) de zÇ”hÃ©. MÄ›i cÃ©ng zhÅng, mÄ›i gÃ¨ liÃ n zhÇnÃ©ng chÃ¡ kÃ n shÅ«rÃ¹ biÇoshÃ¬ zhÅng de suÇ’yÇ’u qiÃ¡nxÃ¹ liÃ n. YÄ«ncÇ, jÄ«yÃº CoM kuÃ ngjiÃ  de mÃ³xÃ­ng kÄ›yÇ tÅngguÃ² zÄ“ngjiÄ liÃ n lÃ¡i zhÃºbÃ¹ kuÃ²zhÇn mÃ³xÃ­ng dÃ xÃ¬ng, bÃ¬ng tÅngguÃ² shÇyÃ²ng bÃ¹tÃ³ng de liÃ n shÃ¹liÃ ng tÃ­gÅng duÅgÃ¨ bÃ¹tÃ³ng dÃ xÃ¬ng de zÇ mÃ³xÃ­ng jÃ¬nxÃ­ng tÃ¡nxÃ¬ng tuÄ«lÇ. JÄ«yÃº zhÃ¨ yÄ« yuÃ¡nzÃ©, zuÃ²zhÄ› shÃ¨jÃ¬ le Chain-of-Language-Model (CoLM), bÃ¬ng jÃ¬n yÄ«bÃ¹ yÇnrÃ¹ le KV gÃ²ngxiÇng jÄ«zhÃ¬ de CoLM-Air, yÇ shÃ­xiÃ n gÃ¨ng duÅ de kuÃ²zhÇn gÅngnÃ©ng. ShÃ­yÃ n jiÃ©guÇ’ biÇomÃ­ng, CoLM xÃ¬liÃ¨ mÃ³xÃ­ng zÃ i xÃ¬ngnÃ©ng shÃ ng yÇ” biÄozhÇ”n Transformer xiÄngdÄng, tÃ³ngshÃ­ tÃ­gÅng le gÃ¨ng dÃ  de lÃ­nghuÃ³xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"èŒƒå¼\", \"pinyin\": \"fÃ n shÃ¬\", \"trans\": \"paradigm\"},\n    {\"word\": \"Chain-of-Model\", \"pinyin\": \"ChÃ¨in-Ã²f-MÃ³del\", \"trans\": \"Chain-of-Model\"},\n    {\"word\": \"å› æœå…³ç³»\", \"pinyin\": \"yÄ«n guÇ’ guÄn xÃ¬\", \"trans\": \"causal relationship\"},\n    {\"word\": \"éšè—çŠ¶æ€\", \"pinyin\": \"yÇn cÃ¡ng zhuÃ ng tÃ i\", \"trans\": \"hidden state\"},\n    {\"word\": \"é“¾å¼ç»“æ„\", \"pinyin\": \"liÃ n shÃ¬ jiÃ©gÃ²u\", \"trans\": \"chain structure\"},\n    {\"word\": \"æ‰©å±•æ•ˆç‡\", \"pinyin\": \"kuÃ² zhÇn xiÃ o lÇœ\", \"trans\": \"scalability\"},\n    {\"word\": \"éƒ¨ç½²\", \"pinyin\": \"bÃ¹ shÇ”\", \"trans\": \"deployment\"},\n    {\"word\": \"çµæ´»æ€§\", \"pinyin\": \"lÃ­ng huÃ³ xÃ¬ng\", \"trans\": \"flexibility\"},\n    {\"word\": \"Chain-of-Representation\", \"pinyin\": \"ChÃ¨in-Ã²f-RÄ›prizen tÃ©i shÄ“n\", \"trans\": \"Chain-of-Representation\"},\n    {\"word\": \"å­è¡¨ç¤º\", \"pinyin\": \"zÇ biÇo shÃ¬\", \"trans\": \"sub-representation\"},\n    {\"word\": \"ç»„åˆ\", \"pinyin\": \"zÇ” hÃ©\", \"trans\": \"combination\"},\n    {\"word\": \"å‰åºé“¾\", \"pinyin\": \"qiÃ¡n xÃ¹ liÃ n\", \"trans\": \"preceding chain\"},\n    {\"word\": \"å¼¹æ€§æ¨ç†\", \"pinyin\": \"tÃ¡n xÃ¬ng tuÄ« lÇ\", \"trans\": \"elastic inference\"},\n    {\"word\": \"Chain-of-Language-Model\", \"pinyin\": \"ChÃ¨in-Ã²f-LÃ¡nggÃ¹ MÃ³del\", \"trans\": \"Chain-of-Language-Model\"},\n    {\"word\": \"KVå…±äº«æœºåˆ¶\", \"pinyin\": \"KV gÃ²ng xiÇng jÄ« zhÃ¬\", \"trans\": \"KV sharing mechanism\"},\n    {\"word\": \"CoLM-Air\", \"pinyin\": \"CoLM-Ã‰ir\", \"trans\": \"CoLM-Air\"},\n    {\"word\": \"æ‰©å±•åŠŸèƒ½\", \"pinyin\": \"kuÃ² zhÇn gÅng nÃ©ng\", \"trans\": \"extended functionality\"},\n    {\"word\": \"Transformer\", \"pinyin\": \"TÃ¨inshÃ¨in fÅmÄ›i\", \"trans\": \"Transformer\"}\n]",
        "trans": "This article proposes a new learning paradigm called Chain-of-Model (CoM). It introduces causality into the hidden states of each layer, forming a chain-like structure that enhances the scalability of model training and the flexibility of deployment. The authors introduce the concept of Chain-of-Representation (CoR), representing the hidden states of each layer as a combination of multiple sub-representations (i.e., chains). Within each layer, each chain can only view all preceding chains in the input representation. Therefore, models based on the CoM framework can incrementally scale the model size by adding chains and provide multiple sub-models of different sizes for elastic inference by using different numbers of chains. Based on this principle, the authors designed Chain-of-Language-Model (CoLM) and further introduced CoLM-Air with a KV sharing mechanism to achieve more scalable functionalities. Experimental results show that the CoLM series models perform comparably to standard Transformers while offering greater flexibility.",
        "update_ts": "2025-05-20 09:13"
    }
}