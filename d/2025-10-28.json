{
    "date": {
        "ru": "28 октября",
        "en": "October 28",
        "zh": "10月28日"
    },
    "time_utc": "2025-10-28 03:37",
    "weekday": 1,
    "issue_id": 6645,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.21817",
            "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing,\n  Speaking, and Acting",
            "url": "https://huggingface.co/papers/2510.21817",
            "abstract": "VITA-E, a dual-model embodied interaction framework, enables concurrent and interruptible vision-language-action capabilities, enhancing real-time user interaction and multitasking.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Vision-Language-Action (VLA) models are often constrained by a rigid, static interaction paradigm, which lacks the ability to see, hear, speak, and act concurrently as well as handle real-time user interruptions dynamically. This hinders seamless embodied collaboration, resulting in an inflexible and unresponsive user experience. To address these limitations, we introduce VITA-E, a novel embodied interaction framework designed for both behavioral concurrency and nearly real-time interruption. The core of our approach is a dual-model architecture where two parallel VLA instances operate as an ``Active Model'' and a ``Standby Model'', allowing the embodied agent to observe its environment, listen to user speech, provide verbal responses, and execute actions, all concurrently and interruptibly, mimicking human-like multitasking capabilities. We further propose a ``model-as-controller'' paradigm, where we fine-tune the VLM to generate special tokens that serve as direct system-level commands, coupling the model's reasoning with the system's behavior. Experiments conducted on a physical humanoid platform demonstrate that VITA-E can reliably handle complex interactive scenarios. Our framework is compatible with various dual-system VLA models, achieving an extremely high success rate on emergency stops and speech interruptions while also successfully performing concurrent speech and action. This represents a significant step towards more natural and capable embodied assistants.",
            "score": 27,
            "issue_id": 6645,
            "pub_date": "2025-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "e4786dd390bcb847",
            "authors": [
                "Xiaoyu Liu",
                "Chaoyou Fu",
                "Chi Yan",
                "Chu Wu",
                "Haihan Gao",
                "Yi-Fan Zhang",
                "Shaoqi Dong",
                "Cheng Qian",
                "Bin Luo",
                "Xiuyong Yang",
                "Guanwu Li",
                "Yusheng Cai",
                "Yunhang Shen",
                "Deqiang Jiang",
                "Haoyu Cao",
                "Xing Sun",
                "Caifeng Shan",
                "Ran He"
            ],
            "affiliations": [
                "CASIA",
                "Fourier Intelligence Inc.",
                "Nanjing University",
                "Tencent Youtu Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21817.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#agi",
                    "#interpretability",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Двухмодельная система для прерываемого взаимодействия роботов",
                    "desc": "Современные Vision-Language-Action модели работают по жёсткому сценарию и не могут одновременно видеть, слышать, говорить и действовать, как это делают люди. Авторы представляют VITA-E — архитектуру с двумя параллельными VLA моделями (активной и резервной), которая позволяет роботу выполнять несколько задач одновременно и реагировать на прерывания пользователя в реальном времени. Система использует подход «модель-как-контроллер», где VLM генерирует специальные токены для управления системой на низком уровне. Эксперименты на физической гуманоидной платформе показали высокую надёжность при экстренных остановках, прерываниях речью и параллельном выполнении действий."
                },
                "en": {
                    "title": "Empowering Multitasking in AI with VITA-E",
                    "desc": "VITA-E is a new framework that improves how machines interact with users by allowing them to see, hear, speak, and act at the same time. It uses a dual-model system with an 'Active Model' for immediate tasks and a 'Standby Model' ready to respond to interruptions, making interactions feel more natural and fluid. This framework addresses the limitations of traditional Vision-Language-Action (VLA) models, which often struggle with multitasking and real-time responses. Experiments show that VITA-E can effectively manage complex interactions, enhancing the capabilities of embodied assistants."
                },
                "zh": {
                    "title": "VITA-E：实现实时多任务的智能交互框架",
                    "desc": "VITA-E是一个双模型的具身交互框架，能够同时进行视觉-语言-行动的处理，提升实时用户交互和多任务处理能力。当前的视觉-语言-行动模型常常受到静态交互模式的限制，无法动态应对用户的实时干扰。VITA-E通过双模型架构，使得具身代理能够同时观察环境、听取用户语音、提供口头回应和执行动作，模拟人类的多任务能力。实验表明，VITA-E在复杂交互场景中表现出色，能够高效处理紧急停止和语音干扰，标志着具身助手向更自然和更强大的方向迈出了重要一步。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.22201",
            "title": "ACG: Action Coherence Guidance for Flow-based VLA models",
            "url": "https://huggingface.co/papers/2510.22201",
            "abstract": "Action Coherence Guidance (ACG) improves action coherence in Vision-Language-Action (VLA) models during test time, enhancing performance in diverse manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion and flow matching models have emerged as powerful robot policies, enabling Vision-Language-Action (VLA) models to generalize across diverse scenes and instructions. Yet, when trained via imitation learning, their high generative capacity makes them sensitive to noise in human demonstrations: jerks, pauses, and jitter which reduce action coherence. Reduced action coherence causes instability and trajectory drift during deployment, failures that are catastrophic in fine-grained manipulation where precision is crucial. In this paper, we present Action Coherence Guidance (ACG) for VLA models, a training-free test-time guidance algorithm that improves action coherence and thereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and real-world SO-101 tasks, ACG consistently improves action coherence and boosts success rates across diverse manipulation tasks. Code and project page are available at https://github.com/DAVIAN-Robotics/ACG and https://DAVIAN-Robotics.github.io/ACG , respectively.",
            "score": 15,
            "issue_id": 6645,
            "pub_date": "2025-10-25",
            "pub_date_card": {
                "ru": "25 октября",
                "en": "October 25",
                "zh": "10月25日"
            },
            "hash": "bf324c122f095102",
            "authors": [
                "Minho Park",
                "Kinam Kim",
                "Junha Hyung",
                "Hyojin Jang",
                "Hoiyeong Jin",
                "Jooyeol Yun",
                "Hojoon Lee",
                "Jaegul Choo"
            ],
            "affiliations": [
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.22201.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#games",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Плавные движения роботов без дополнительного обучения",
                    "desc": "Статья представляет метод Action Coherence Guidance (ACG) для улучшения согласованности действий в Vision-Language-Action (VLA) моделях во время тестирования. Диффузионные модели и flow matching модели, обученные методом имитационного обучения, чувствительны к шуму в человеческих демонстрациях — рывкам, паузам и дрожанию, что снижает плавность действий робота. ACG — это алгоритм guidance, который работает без дополнительного обучения и повышает согласованность траекторий, что критично для точных манипуляционных задач. Эксперименты на бенчмарках RoboCasa, DexMimicGen и реальных задачах SO-101 показали стабильное улучшение плавности действий и увеличение показателей успешности."
                },
                "en": {
                    "title": "Enhancing Robotic Precision with Action Coherence Guidance",
                    "desc": "This paper introduces Action Coherence Guidance (ACG), a method designed to enhance the action coherence of Vision-Language-Action (VLA) models during their testing phase. ACG addresses the issue of noise in human demonstrations, which can lead to erratic movements and reduced performance in robotic manipulation tasks. By applying ACG, the models can maintain stability and precision, crucial for tasks requiring fine manipulation. The effectiveness of ACG is demonstrated through evaluations on various benchmarks, showing significant improvements in action coherence and overall success rates."
                },
                "zh": {
                    "title": "提升视觉-语言-动作模型的动作一致性",
                    "desc": "本文提出了一种名为动作一致性引导（ACG）的方法，旨在提高视觉-语言-动作（VLA）模型在测试阶段的动作一致性。通过解决模仿学习中人类演示的噪声问题，ACG能够减少因抖动、停顿等导致的动作不稳定性。该方法在RoboCasa、DexMimicGen和真实世界SO-101任务中进行了评估，结果显示ACG显著提高了动作一致性和成功率。ACG是一种无训练的测试时引导算法，能够在多样化的操作任务中提升模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.23607",
            "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations",
            "url": "https://huggingface.co/papers/2510.23607",
            "abstract": "Concerto, a minimalist model combining 3D self-distillation and 2D-3D joint embedding, achieves superior spatial feature learning and outperforms existing models in scene understanding and open-world perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency.",
            "score": 9,
            "issue_id": 6645,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "c99437fd83f1fbf7",
            "authors": [
                "Yujia Zhang",
                "Xiaoyang Wu",
                "Yixing Lao",
                "Chengyao Wang",
                "Zhuotao Tian",
                "Naiyan Wang",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "Harbin Institute of Technology (Shenzhen)",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.23607.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#benchmark",
                    "#3d",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🎼",
                "ru": {
                    "title": "Обучение пространственному восприятию через синергию 2D и 3D",
                    "desc": "Concerto — это модель для изучения пространственных концептов, вдохновлённая тем, как люди учатся через мультисенсорный опыт. Метод сочетает 3D self-distillation внутри одной модальности и joint embedding между 2D и 3D данными. Модель превосходит существующие 2D и 3D self-supervised модели на 14.2% и 4.8% соответственно в задачах понимания 3D сцен. Concerto достигает state-of-the-art результатов на бенчмарках вроде ScanNet и может быть адаптирована для open-world восприятия через проекцию в пространство CLIP."
                },
                "en": {
                    "title": "Concerto: Revolutionizing Spatial Understanding with Minimalist Design",
                    "desc": "Concerto is a novel machine learning model designed for spatial cognition, inspired by how humans learn concepts through multiple senses. It utilizes 3D self-distillation and a joint embedding of 2D and 3D data to enhance feature learning. The model demonstrates significant improvements in scene understanding, outperforming existing state-of-the-art models in both 2D and 3D tasks. Additionally, Concerto's ability to project representations into language space facilitates open-world perception, showcasing its versatility and effectiveness in spatial representation learning."
                },
                "zh": {
                    "title": "Concerto：简约模型，卓越空间理解",
                    "desc": "Concerto是一种简约模型，结合了3D自我蒸馏和2D-3D联合嵌入，能够更好地学习空间特征。它受到人类多感官协同学习的启发，模拟了人类的概念学习过程。尽管结构简单，Concerto在空间认知上表现出更连贯和信息丰富的特征，超越了现有的2D和3D自监督模型。通过全量调优，Concerto在多个场景理解基准上设定了新的最先进结果，展示了其在开放世界感知中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.23451",
            "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with\n  Free-Form Preferences",
            "url": "https://huggingface.co/papers/2510.23451",
            "abstract": "Omni-Reward addresses modality imbalance and preference rigidity in reward models by introducing a benchmark, dataset, and model that support multiple modalities and free-form preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks.",
            "score": 8,
            "issue_id": 6645,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "957ca9be32cf1b0d",
            "authors": [
                "Zhuoran Jin",
                "Hongbang Yuan",
                "Kejian Zhu",
                "Jiachun Li",
                "Pengfei Cao",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao"
            ],
            "affiliations": [
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.23451.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#alignment",
                    "#architecture"
                ],
                "emoji": "🎁",
                "ru": {
                    "title": "Универсальная модель вознаграждения для всех модальностей и персонализированных предпочтений",
                    "desc": "Статья представляет Omni-Reward — систему для обучения reward models, которые работают с множеством модальностей (текст, изображения, видео, аудио, 3D). Авторы решают две проблемы: дисбаланс модальностей (большинство моделей фокусируются только на тексте и изображениях) и жёсткость предпочтений (обучение на фиксированных бинарных парах не учитывает разнообразие персональных предпочтений). Omni-RewardBench — первый бенчмарк для оценки мультимодальных reward models со свободной формой предпочтений, включающий 9 задач. Датасет Omni-RewardData содержит 248K пар общих предпочтений и 69K пар для instruction-tuning, а модель Omni-RewardModel показывает сильные результаты на различных бенчмарках."
                },
                "en": {
                    "title": "Empowering AI with Omni-Modal Reward Modeling",
                    "desc": "The paper introduces Omni-Reward, a new approach to improve reward models (RMs) in AI by addressing two main issues: modality imbalance and preference rigidity. Modality imbalance refers to the limited focus of existing RMs on text and images, neglecting other important modalities like video and audio. Preference rigidity highlights the problem of using fixed binary preferences, which do not reflect the diverse and complex nature of human preferences. Omni-Reward provides a comprehensive benchmark, a large multimodal dataset, and a versatile model that supports various modalities and free-form preferences, enhancing the alignment of AI behaviors with human values."
                },
                "zh": {
                    "title": "全模态奖励建模的未来",
                    "desc": "Omni-Reward 旨在解决奖励模型中的模态不平衡和偏好刚性问题。它引入了一个基准、数据集和模型，支持多种模态和自由形式的偏好。该研究包括三个主要部分：Omni-RewardBench基准，涵盖文本、图像、视频、音频和3D等五种模态的九个任务；Omni-RewardData数据集，包含248K个通用偏好对和69K个指令调优对；以及Omni-RewardModel模型，结合了判别式和生成式奖励模型，在多个基准测试中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.23603",
            "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary Granularity",
            "url": "https://huggingface.co/papers/2510.23603",
            "abstract": "PixelRefer is a unified region-level MLLM framework that enhances fine-grained object-centric understanding using a Scale-Adaptive Object Tokenizer and Object-Centric Infusion module, achieving high performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency.",
            "score": 3,
            "issue_id": 6645,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "17dedd3c9dff5a56",
            "authors": [
                "Yuqian Yuan",
                "Wenqiao Zhang",
                "Xin Li",
                "Shihao Wang",
                "Kehan Li",
                "Wentong Li",
                "Jun Xiao",
                "Lei Zhang",
                "Beng Chin Ooi"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "The Hong Kong Polytechnic University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.23603.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#games",
                    "#dataset",
                    "#optimization",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Точное понимание объектов в мультимодальных LLM",
                    "desc": "PixelRefer — это фреймворк для мультимодальных LLM, который фокусируется на детальном понимании отдельных объектов в изображениях и видео, а не только на общей сцене. Авторы предлагают Scale-Adaptive Object Tokenizer для создания компактных представлений объектов из выделенных пользователем областей. Облегчённая версия PixelRefer-Lite использует Object-Centric Infusion модуль, который предварительно интегрирует глобальный контекст в токены объектов, значительно снижая вычислительные затраты. Для обучения модели был создан датасет PixelRefer-2.2M с высококачественными инструкциями для работы с объектами."
                },
                "en": {
                    "title": "Enhancing Object Understanding with PixelRefer",
                    "desc": "PixelRefer is a new framework designed to improve how machines understand specific objects in images and videos, rather than just the overall scene. It uses a Scale-Adaptive Object Tokenizer to create detailed representations of objects, allowing for better reasoning about them. The framework also includes an Object-Centric Infusion module that helps combine global context with object information, making it more efficient. With a specially curated dataset for training, PixelRefer shows strong performance even with fewer examples, while its lighter version, PixelRefer-Lite, maintains accuracy with reduced computational demands."
                },
                "zh": {
                    "title": "PixelRefer：细粒度物体理解的新突破",
                    "desc": "PixelRefer是一个统一的区域级多模态大语言模型框架，旨在增强对细粒度物体的理解。它采用了可缩放的物体标记器和物体中心注入模块，能够在图像和视频中实现用户指定区域的高级理解。通过生成紧凑且语义丰富的物体表示，PixelRefer显著提高了计算效率，同时保持了高语义保真度。实验结果表明，PixelRefer在较少的训练样本下实现了领先的性能，而其高效变体PixelRefer-Lite在准确性和效率上也表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.23544",
            "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking",
            "url": "https://huggingface.co/papers/2510.23544",
            "abstract": "LIMRANK-SYNTHESIZER generates synthetic data to fine-tune LIMRANK, achieving competitive performance with minimal supervision on information reranking tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving.",
            "score": 3,
            "issue_id": 6645,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "ea49406b584c9f63",
            "authors": [
                "Tingyu Song",
                "Yilun Zhao",
                "Siyue Zhang",
                "Chen Zhao",
                "Arman Cohan"
            ],
            "affiliations": [
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.23544.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#rag",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#synthetic",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Эффективное ранжирование информации с минимальным количеством данных",
                    "desc": "Статья представляет LIMRANK-SYNTHESIZER — pipeline для генерации синтетических данных, который позволяет обучать модель ранжирования LIMRANK с минимальным объёмом размеченных данных. Вместо затратного fine-tuning на больших датасетах, авторы используют менее 5% данных по сравнению с предыдущими подходами. LIMRANK демонстрирует конкурентные результаты на бенчмарках BRIGHT и FollowIR, требующих сложного рассуждения и следования инструкциям. Модель показывает сильную способность к генерализации на задачах поиска научной литературы и retrieval-augmented generation."
                },
                "en": {
                    "title": "Efficient Reranking with Minimal Supervision",
                    "desc": "LIMRANK-SYNTHESIZER is a novel approach that generates synthetic data to enhance the performance of the LIMRANK model for information reranking tasks. Unlike traditional methods that require extensive fine-tuning with large datasets, this method shows that high-quality supervision can be minimal yet effective. The pipeline produces diverse and realistic reranking examples, allowing LIMRANK to be fine-tuned efficiently. Evaluations on benchmarks like BRIGHT and FollowIR reveal that LIMRANK can achieve competitive results while using significantly less training data than previous approaches."
                },
                "zh": {
                    "title": "用合成数据提升信息重排序性能",
                    "desc": "LIMRANK-SYNTHESIZER 是一种生成合成数据的方法，用于微调 LIMRANK 模型，以在信息重排序任务中实现竞争性能。与传统方法需要大量计算资源进行大规模微调不同，我们的方法只需少量高质量的监督数据。我们设计了 LIMRANK-SYNTHESIZER，这是一种可重用的开源管道，用于生成多样化、具有挑战性和真实感的重排序示例。实验结果表明，LIMRANK 在使用不到 5% 的数据进行训练的情况下，仍能在多个基准测试中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.22733",
            "title": "E^2Rank: Your Text Embedding can Also be an Effective\n  and Efficient Listwise Reranker",
            "url": "https://huggingface.co/papers/2510.22733",
            "abstract": "A unified framework extends a single text embedding model to perform both retrieval and listwise reranking, achieving state-of-the-art results with low latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Text embedding models serve as a fundamental component in real-world search applications. By mapping queries and documents into a shared embedding space, they deliver competitive retrieval performance with high efficiency. However, their ranking fidelity remains limited compared to dedicated rerankers, especially recent LLM-based listwise rerankers, which capture fine-grained query-document and document-document interactions. In this paper, we propose a simple yet effective unified framework E^2Rank, means Efficient Embedding-based Ranking (also means Embedding-to-Rank), which extends a single text embedding model to perform both high-quality retrieval and listwise reranking through continued training under a listwise ranking objective, thereby achieving strong effectiveness with remarkable efficiency. By applying cosine similarity between the query and document embeddings as a unified ranking function, the listwise ranking prompt, which is constructed from the original query and its candidate documents, serves as an enhanced query enriched with signals from the top-K documents, akin to pseudo-relevance feedback (PRF) in traditional retrieval models. This design preserves the efficiency and representational quality of the base embedding model while significantly improving its reranking performance. Empirically, E^2Rank achieves state-of-the-art results on the BEIR reranking benchmark and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also show that the ranking training process improves embedding performance on the MTEB benchmark. Our findings indicate that a single embedding model can effectively unify retrieval and reranking, offering both computational efficiency and competitive ranking accuracy.",
            "score": 3,
            "issue_id": 6645,
            "pub_date": "2025-10-26",
            "pub_date_card": {
                "ru": "26 октября",
                "en": "October 26",
                "zh": "10月26日"
            },
            "hash": "487d3fd10fdcad61",
            "authors": [
                "Qi Liu",
                "Yanzhao Zhang",
                "Mingxin Li",
                "Dingkun Long",
                "Pengjun Xie",
                "Jiaxin Mao"
            ],
            "affiliations": [
                "Alibaba Group",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.22733.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#rag"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Одна embedding-модель для поиска и ранжирования",
                    "desc": "Статья представляет E^2Rank — единую框架, которая расширяет возможности одной текстовой embedding-модели для выполнения как поиска, так и listwise reranking. Модель дообучается с использованием listwise ranking objective, где запрос обогащается информацией из топ-K документов по аналогии с pseudo-relevance feedback. Для ранжирования используется простое косинусное сходство между эмбеддингами запроса и документов, что обеспечивает высокую эффективность и низкую latency. E^2Rank достигает state-of-the-art результатов на бенчмарках BEIR и BRIGHT, демонстрируя, что одна embedding-модель может эффективно объединить задачи retrieval и reranking."
                },
                "en": {
                    "title": "Unifying Retrieval and Reranking with E^2Rank",
                    "desc": "This paper introduces E^2Rank, a unified framework that enhances a single text embedding model to perform both document retrieval and listwise reranking. By training the model under a listwise ranking objective, it achieves high-quality retrieval while maintaining low latency. The framework utilizes cosine similarity for ranking, allowing it to incorporate signals from top candidate documents, similar to traditional pseudo-relevance feedback. The results show that E^2Rank outperforms existing methods on various benchmarks, demonstrating that a single embedding model can efficiently unify retrieval and reranking tasks."
                },
                "zh": {
                    "title": "统一框架，实现高效检索与重排序",
                    "desc": "本文提出了一种统一框架E^2Rank，旨在通过扩展单一文本嵌入模型，实现高效的检索和列表重排序。该框架通过在列表排序目标下继续训练，提升了检索质量和重排序性能。E^2Rank利用余弦相似度作为统一的排序函数，增强了查询的信号，从而提高了重排序的效果。实验结果表明，E^2Rank在BEIR重排序基准上达到了最先进的结果，并在BRIGHT基准上表现出竞争力，同时保持了低延迟。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.22907",
            "title": "Language Server CLI Empowers Language Agents with Process Rewards",
            "url": "https://huggingface.co/papers/2510.22907",
            "abstract": "Lanser-CLI orchestrates Language Server Protocol servers for coding agents and CI, providing deterministic workflows and actionable process rewards based on verified code facts.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models routinely hallucinate APIs and mislocalize edits, while language servers compute verified, IDE-grade facts about real code. We present Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language Server Protocol (LSP) server for coding agents and CI, exposing deterministic, replayable workflows. Our position is that language servers provide not only structural information (definitions, references, types, diagnostics) but also an actionable process reward: machine-checked, step-wise signals that align an agent's planning loop with program reality. In this work, Lanser-CLI contributes: (i) a robust addressing scheme beyond brittle \"file:line:col\" via a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a principled relocation algorithm; (ii) deterministic Analysis Bundles that normalize Language Server responses and capture environment/capability metadata with stable content hashes; (iii) a safety envelope for mutating operations (rename, code actions) with preview, workspace jails, and Git-aware, transactional apply; and (iv) a process-reward functional derived from Language Server facts (diagnostic deltas, disambiguation confidence, and safe-apply checks) that is computable online and replayable offline. We formalize determinism under frozen snapshots and establish a monotonicity property for the process reward, making it suitable for process supervision and counterfactual analysis. Project Page: https://github.com/yifanzhang-pro/lanser-cli",
            "score": 2,
            "issue_id": 6645,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "9c063981ba8b33a5",
            "authors": [
                "Yifan Zhang",
                "Lanser Contributors"
            ],
            "affiliations": [
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.22907.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#agents",
                    "#optimization",
                    "#plp",
                    "#training"
                ],
                "emoji": "🔧",
                "ru": {
                    "title": "Проверенные факты вместо галлюцинаций: language servers как награда для coding-агентов",
                    "desc": "Статья представляет Lanser-CLI — инструмент командной строки для управления Language Server Protocol серверами, предназначенный для coding-агентов и CI/CD систем. В отличие от LLM, которые часто галлюцинируют API и неправильно локализуют изменения кода, language servers предоставляют проверенную информацию о реальном коде. Система вводит детерминированные Analysis Bundles, улучшенную схему адресации через Selector DSL и безопасные механизмы для операций изменения кода. Ключевая идея — использовать проверенные факты от language server как process reward для обучения агентов, что делает процесс воспроизводимым и выровненным с реальностью программного кода."
                },
                "en": {
                    "title": "Reliable Coding with Deterministic Workflows",
                    "desc": "Lanser-CLI is a command-line interface that manages Language Server Protocol (LSP) servers to enhance coding agents and continuous integration (CI) processes. It aims to provide reliable workflows by utilizing verified code facts, addressing the common issues of large language models that often generate incorrect API calls and misplace code edits. The system introduces a Selector DSL for precise code addressing, deterministic Analysis Bundles for consistent responses, and a safety envelope for code modifications. Additionally, it offers a process-reward mechanism based on language server diagnostics, ensuring that the planning of coding agents aligns with actual program behavior."
                },
                "zh": {
                    "title": "Lanser-CLI：编程代理的智能助手",
                    "desc": "Lanser-CLI 是一个用于编程代理和持续集成的命令行工具，它协调语言服务器协议（LSP）服务器，提供确定性的工作流程和基于验证代码事实的可操作过程奖励。该工具解决了大型语言模型在 API 生成和编辑定位方面的不足，通过提供结构信息和可操作的过程奖励，帮助代理更好地与程序现实对齐。Lanser-CLI 引入了一种强大的地址方案，确保语言服务器的响应标准化，并提供安全的操作环境。通过在线计算和离线重放的过程奖励，Lanser-CLI 使得过程监督和反事实分析变得更加可行。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21003",
            "title": "Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models\n  with Conditional Score Distillation",
            "url": "https://huggingface.co/papers/2510.21003",
            "abstract": "A new method, Distilled Decoding 2 (DD2), enables one-step sampling for image auto-regressive models with minimal performance degradation and significant speed-up compared to previous methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Image Auto-regressive (AR) models have emerged as a powerful paradigm of visual generative models. Despite their promising performance, they suffer from slow generation speed due to the large number of sampling steps required. Although Distilled Decoding 1 (DD1) was recently proposed to enable few-step sampling for image AR models, it still incurs significant performance degradation in the one-step setting, and relies on a pre-defined mapping that limits its flexibility. In this work, we propose a new method, Distilled Decoding 2 (DD2), to further advances the feasibility of one-step sampling for image AR models. Unlike DD1, DD2 does not without rely on a pre-defined mapping. We view the original AR model as a teacher model which provides the ground truth conditional score in the latent embedding space at each token position. Based on this, we propose a novel conditional score distillation loss to train a one-step generator. Specifically, we train a separate network to predict the conditional score of the generated distribution and apply score distillation at every token position conditioned on previous tokens. Experimental results show that DD2 enables one-step sampling for image AR models with an minimal FID increase from 3.40 to 5.43 on ImageNet-256. Compared to the strongest baseline DD1, DD2 reduces the gap between the one-step sampling and original AR model by 67%, with up to 12.3times training speed-up simultaneously. DD2 takes a significant step toward the goal of one-step AR generation, opening up new possibilities for fast and high-quality AR modeling. Code is available at https://github.com/imagination-research/Distilled-Decoding-2.",
            "score": 2,
            "issue_id": 6645,
            "pub_date": "2025-10-23",
            "pub_date_card": {
                "ru": "23 октября",
                "en": "October 23",
                "zh": "10月23日"
            },
            "hash": "c525f5bb0a9bd473",
            "authors": [
                "Enshu Liu",
                "Qian Chen",
                "Xuefei Ning",
                "Shengen Yan",
                "Guohao Dai",
                "Zinan Lin",
                "Yu Wang"
            ],
            "affiliations": [
                "Infinigence-AI Beijing, China",
                "Microsoft Research Redmond, WA, USA",
                "Shanghai Jiaotong University Shanghai, China",
                "Tsinghua University Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21003.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#training"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Одношаговая генерация изображений через дистилляцию score-функций",
                    "desc": "Статья представляет новый метод Distilled Decoding 2 (DD2) для ускорения генерации изображений в авторегрессионных моделях. В отличие от предыдущего подхода DD1, новый метод не использует предопределённое отображение и обучает отдельную сеть предсказывать conditional score в пространстве латентных эмбеддингов. DD2 позволяет генерировать изображения за один шаг вместо множества итераций, при этом метрика FID увеличивается минимально с 3.40 до 5.43 на ImageNet-256. Метод сокращает разрыв в качестве между одношаговой генерацией и оригинальной AR моделью на 67% и ускоряет обучение в 12.3 раза по сравнению с DD1."
                },
                "en": {
                    "title": "Fast and Flexible One-Step Sampling with DD2",
                    "desc": "The paper introduces Distilled Decoding 2 (DD2), a new method for improving one-step sampling in image auto-regressive (AR) models. DD2 addresses the slow generation speed of previous methods by eliminating the need for a pre-defined mapping, which enhances flexibility. It utilizes a conditional score distillation loss to train a generator that predicts the conditional score based on previous tokens, significantly reducing performance degradation. Experimental results demonstrate that DD2 achieves faster sampling with minimal quality loss, making it a promising advancement in the field of visual generative models."
                },
                "zh": {
                    "title": "一步采样，快速高效的图像生成",
                    "desc": "本文提出了一种新方法，称为蒸馏解码2（DD2），旨在提高图像自回归模型的一步采样效率。与之前的蒸馏解码1（DD1）相比，DD2在保持性能的同时显著加快了生成速度。DD2不依赖于预定义的映射，而是将原始自回归模型视为教师模型，通过条件分数蒸馏损失来训练生成器。实验结果表明，DD2在ImageNet-256上实现了一步采样，FID值仅从3.40增加到5.43，且训练速度提高了12.3倍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.23594",
            "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error\n  Detection",
            "url": "https://huggingface.co/papers/2510.23594",
            "abstract": "PRISM-Bench evaluates models' reasoning processes by identifying errors in step-by-step solutions to visual puzzles, highlighting gaps between fluent generation and logical consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PRISM-Bench, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs.",
            "score": 1,
            "issue_id": 6645,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "0225ecde49430d54",
            "authors": [
                "Yusu Qian",
                "Cheng Wan",
                "Chao Jia",
                "Yinfei Yang",
                "Qingyu Zhao",
                "Zhe Gan"
            ],
            "affiliations": [
                "Apple",
                "Cornell",
                "Weill Cornell Medicine"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.23594.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#benchmark",
                    "#interpretability",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Проверка логики, а не только ответов: новый подход к оценке рассуждений AI",
                    "desc": "В статье представлен PRISM-Bench — бенчмарк для оценки процесса рассуждений моделей на основе визуальных головоломок. В отличие от традиционных методов оценки, которые проверяют только правильность финального ответа, PRISM-Bench требует от моделей находить первую ошибку в пошаговом решении (chain-of-thought). Исследование показывает существенный разрыв между способностью современных мультимодальных LLM генерировать правдоподобные рассуждения и их умением обнаруживать логические ошибки. Бенчмарк включает задачи, требующие многошаговых символических, геометрических и аналогических рассуждений, что позволяет глубже оценить логическую последовательность и надёжность AI-моделей."
                },
                "en": {
                    "title": "Evaluating Reasoning, Not Just Answers",
                    "desc": "PRISM-Bench is a new benchmark designed to evaluate how well machine learning models reason through visual puzzles. It focuses on identifying errors in the step-by-step reasoning process rather than just checking if the final answer is correct. By requiring models to pinpoint the first mistake in a chain-of-thought, it assesses their logical consistency and error detection abilities. This approach reveals that many advanced models struggle with reasoning, even when they generate plausible solutions, highlighting the importance of thorough diagnostic evaluations in machine learning."
                },
                "zh": {
                    "title": "推理能力的细致评估：PRISM-Bench",
                    "desc": "PRISM-Bench 是一个用于评估模型推理过程的基准，专注于识别视觉难题逐步解决中的错误。与以往仅测量最终答案准确性的评估不同，PRISM-Bench 引入了一项诊断任务：给定一个视觉难题和包含一个错误的逐步推理链，模型必须识别出第一个错误步骤。这个设置使得对逻辑一致性、错误检测和视觉推理的细致评估成为可能。研究表明，尽管一些模型能够生成流畅的推理链，但在识别简单逻辑错误方面仍存在显著差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.23571",
            "title": "RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim\n  Translation",
            "url": "https://huggingface.co/papers/2510.23571",
            "abstract": "A new benchmarking framework uses large-scale simulated environments with human feedback to evaluate robot policies, addressing limitations in real-world testing and existing simulation benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining \"success\" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.",
            "score": 1,
            "issue_id": 6645,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "6fc2464d3d86a26e",
            "authors": [
                "Yash Jangir",
                "Yidi Zhang",
                "Kashu Yamazaki",
                "Chenyu Zhang",
                "Kuan-Hsun Tu",
                "Tsung-Wei Ke",
                "Lei Ke",
                "Yonatan Bisk",
                "Katerina Fragkiadaki"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "National Taiwan University",
                "Peking University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.23571.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#benchmark",
                    "#agents",
                    "#games",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Симуляция с человеческой обратной связью для оценки роботических политик",
                    "desc": "Статья представляет новый фреймворк для тестирования роботических политик, который переносит оценку VLA (Vision-Language-Action) моделей в крупномасштабные симуляции с участием людей. Авторы автоматически конвертируют видео демонстрации из датасетов в симулированные среды используя VLM, 2D-to-3D генерацию и дифференцируемый рендеринг. Оценка политик происходит через автоматический VLM-скоринг и предпочтения краудворкеров, что делает процесс масштабируемым и воспроизводимым. Система позволяет тестировать робастность политик через контролируемые возмущения среды, такие как изменение текстур и расположения объектов."
                },
                "en": {
                    "title": "Revolutionizing Robot Evaluation with Human-Enhanced Simulations",
                    "desc": "This paper presents a new benchmarking framework designed to evaluate robot policies in large-scale simulated environments with human feedback. It addresses the limitations of real-world testing, which is often slow and unsafe, and existing simulation benchmarks that fail to assess models trained on real-world data. The framework utilizes advances in vision-language models and generative modeling to create simulated environments from video demonstrations, allowing for more effective evaluation of robot policies. By incorporating human preference judgments and systematically varying simulation conditions, the framework enhances the robustness and scalability of robot policy assessments."
                },
                "zh": {
                    "title": "新基准框架：提升机器人策略评估的有效性",
                    "desc": "本文提出了一种新的基准测试框架，旨在通过大规模模拟环境和人类反馈来评估机器人策略。这种方法解决了现实世界测试和现有模拟基准的局限性，尤其是在评估多样化任务的机器人通用性方面。通过利用视觉-语言模型和生成建模技术，本文能够将视频演示自动转换为模拟环境中的对应物。最终，这种框架提供了一种可持续、可重复和可扩展的基准，帮助评估在真实世界中训练的机器人操作策略。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.22946",
            "title": "LightBagel: A Light-weighted, Double Fusion Framework for Unified\n  Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2510.22946",
            "abstract": "A unified multimodal model achieves competitive performance with efficient fusion of generation and understanding models, using interleaved multimodal self-attention blocks and minimal training resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling.",
            "score": 1,
            "issue_id": 6645,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "0276884d3592cb5a",
            "authors": [
                "Zeyu Wang",
                "Zilong Chen",
                "Chenhui Gou",
                "Feng Li",
                "Chaorui Deng",
                "Deyao Zhu",
                "Kunchang Li",
                "Weihao Yu",
                "Haoqin Tu",
                "Haoqi Fan",
                "Cihang Xie"
            ],
            "affiliations": [
                "ByteDance Seed Project",
                "Monash University",
                "Tsinghua University",
                "UC Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.22946.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "🔗",
                "ru": {
                    "title": "Объединяй и властвуй: эффективная мультимодальная модель из готовых компонентов",
                    "desc": "Исследователи предложили эффективный способ создания мультимодальной модели путём объединения уже существующих специализированных моделей для генерации и понимания изображений. Ключевая идея заключается в добавлении блоков мультимодального self-attention между оригинальными блоками базовых моделей, что позволяет сохранить их сильные стороны. Такой подход требует обучения всего на 35 миллиардах токенов и показывает конкурентные результаты в генерации изображений по текстовым описаниям и редактировании изображений. Авторы открыто публикуют код, веса модели и датасеты для дальнейших исследований в области мультимодального моделирования."
                },
                "en": {
                    "title": "Efficient Fusion for Unified Multimodal Mastery",
                    "desc": "This paper presents a unified multimodal model that efficiently combines generation and understanding capabilities using interleaved multimodal self-attention blocks. By fusing existing models instead of training from scratch, the approach significantly reduces computational resource requirements while maintaining competitive performance. The design allows for effective integration of high-level semantic information from understanding tasks with low-level spatial data from generation tasks. The model achieves strong results on various benchmarks with minimal training data, promoting further research in unified multimodal modeling."
                },
                "zh": {
                    "title": "高效融合，统一多模态模型的未来",
                    "desc": "这篇论文提出了一种统一的多模态模型，通过高效融合生成和理解模型，取得了竞争力的性能。研究者们采用了交错的多模态自注意力块，并且只需最少的训练资源。该模型在多个基准测试中表现出色，例如在文本到图像生成任务中达到了0.91的GenEval分数。通过公开代码、模型权重和数据集，作者希望支持未来的统一多模态建模研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.22200",
            "title": "LongCat-Video Technical Report",
            "url": "https://huggingface.co/papers/2510.22200",
            "abstract": "LongCat-Video, a 13.6B parameter video generation model based on the Diffusion Transformer framework, excels in efficient and high-quality long video generation across multiple tasks using unified architecture, coarse-to-fine generation, and block sparse attention.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation is a critical pathway toward world models, with efficient long video inference as a key capability. Toward this end, we introduce LongCat-Video, a foundational video generation model with 13.6B parameters, delivering strong performance across multiple video generation tasks. It particularly excels in efficient and high-quality long video generation, representing our first step toward world models. Key features include: Unified architecture for multiple tasks: Built on the Diffusion Transformer (DiT) framework, LongCat-Video supports Text-to-Video, Image-to-Video, and Video-Continuation tasks with a single model; Long video generation: Pretraining on Video-Continuation tasks enables LongCat-Video to maintain high quality and temporal coherence in the generation of minutes-long videos; Efficient inference: LongCat-Video generates 720p, 30fps videos within minutes by employing a coarse-to-fine generation strategy along both the temporal and spatial axes. Block Sparse Attention further enhances efficiency, particularly at high resolutions; Strong performance with multi-reward RLHF: Multi-reward RLHF training enables LongCat-Video to achieve performance on par with the latest closed-source and leading open-source models. Code and model weights are publicly available to accelerate progress in the field.",
            "score": 1,
            "issue_id": 6645,
            "pub_date": "2025-10-25",
            "pub_date_card": {
                "ru": "25 октября",
                "en": "October 25",
                "zh": "10月25日"
            },
            "hash": "70ad765a1f2c1821",
            "authors": [
                "Meituan LongCat Team",
                "Xunliang Cai",
                "Qilong Huang",
                "Zhuoliang Kang",
                "Hongyu Li",
                "Shijun Liang",
                "Liya Ma",
                "Siyu Ren",
                "Xiaoming Wei",
                "Rixu Xie",
                "Tong Zhang"
            ],
            "affiliations": [
                "Meituan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.22200.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#video",
                    "#rlhf",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Генерация длинных видео высокого качества с единой архитектурой",
                    "desc": "LongCat-Video — это модель генерации видео на основе Diffusion Transformer с 13.6 миллиардами параметров, специализирующаяся на создании длинных видео высокого качества. Модель использует единую архитектуру для решения нескольких задач: генерация видео из текста, из изображений и продолжение видео. Для эффективной генерации применяется стратегия от грубого к детальному (coarse-to-fine) по временной и пространственной осям, а также блочное разреженное внимание (Block Sparse Attention). Обучение с использованием multi-reward RLHF позволяет модели достигать производительности на уровне лучших закрытых и открытых решений, генерируя видео 720p при 30 fps за считанные минуты."
                },
                "en": {
                    "title": "LongCat-Video: Efficient Long Video Generation Unleashed!",
                    "desc": "LongCat-Video is a powerful video generation model with 13.6 billion parameters, designed to create high-quality long videos efficiently. It utilizes the Diffusion Transformer framework, allowing it to perform various tasks like Text-to-Video and Video-Continuation with a single architecture. The model excels in generating long videos by maintaining quality and coherence through pretraining on specific tasks and employing a coarse-to-fine generation strategy. Additionally, its use of Block Sparse Attention improves efficiency, enabling the generation of 720p videos at 30 frames per second in just minutes, while multi-reward RLHF training enhances its performance to match leading models."
                },
                "zh": {
                    "title": "高效生成高质量长视频的革命性模型",
                    "desc": "LongCat-Video是一种基于扩散变换器框架的13.6亿参数视频生成模型，能够高效生成高质量的长视频。该模型支持多种任务，包括文本到视频、图像到视频和视频续写，采用统一架构进行处理。通过在视频续写任务上的预训练，LongCat-Video能够在生成数分钟长的视频时保持高质量和时间一致性。其高效推理能力使得生成720p、30fps的视频仅需几分钟，且通过块稀疏注意力机制进一步提升了高分辨率下的效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.21800",
            "title": "MARS-M: When Variance Reduction Meets Matrices",
            "url": "https://huggingface.co/papers/2510.21800",
            "abstract": "MARS-M, a new optimizer combining Muon and MARS techniques, achieves faster convergence and better performance in large-scale neural network training.  \t\t\t\t\tAI-generated summary \t\t\t\t Matrix-based preconditioned optimizers, such as Muon, have recently been shown to be more efficient than scalar-based optimizers for training large-scale neural networks, including large language models (LLMs). On the other hand, recent benchmarks on optimizers for LLM pre-training have demonstrated that variance-reduction techniques such as MARS can achieve substantial speedups over standard optimizers that do not employ variance reduction. In this paper, to achieve the best of both worlds, we introduce MARS-M, a new optimizer that integrates the variance reduction technique in MARS with Muon. Under standard regularity conditions, we prove that Muon-M converges to a first-order stationary point at a rate of mathcal{O}(T^{-1/3}), which improves upon mathcal{O}(T^{-1/4}) rate attained by Muon. Our empirical results on language modeling and computer vision tasks demonstrate that MARS-M consistently yields lower losses and improved performance across various downstream benchmarks. The implementation of MARS-M is available at https://github.com/AGI-Arena/MARS/MARS_M.",
            "score": 1,
            "issue_id": 6645,
            "pub_date": "2025-10-20",
            "pub_date_card": {
                "ru": "20 октября",
                "en": "October 20",
                "zh": "10月20日"
            },
            "hash": "fbe37bd50ecb7e40",
            "authors": [
                "Yifeng Liu",
                "Angela Yuan",
                "Quanquan Gu"
            ],
            "affiliations": [
                "Department of Computer Science, University of California, Los Angeles, CA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.21800.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "MARS-M: ускоренная оптимизация через матричное предобусловливание и уменьшение дисперсии",
                    "desc": "В статье представлен MARS-M — новый оптимизатор для обучения больших нейронных сетей, который объединяет матричное предобусловливание из метода Muon и технику уменьшения дисперсии из MARS. Авторы доказали, что MARS-M достигает скорости сходимости O(T^{-1/3}), что лучше, чем O(T^{-1/4}) у обычного Muon. Эксперименты на задачах языкового моделирования и компьютерного зрения показали, что новый оптимизатор обеспечивает более низкие значения функции потерь и лучшую производительность. Метод особенно эффективен для обучения больших языковых моделей (LLM)."
                },
                "en": {
                    "title": "MARS-M: Faster Training for Large Neural Networks!",
                    "desc": "MARS-M is a new optimizer that combines the strengths of Muon and MARS techniques to enhance the training of large-scale neural networks. By integrating variance-reduction methods from MARS with the efficiency of matrix-based optimizers like Muon, MARS-M achieves faster convergence rates. Theoretical analysis shows that MARS-M converges to a stationary point more quickly than Muon alone, improving the convergence rate from O(T^{-1/4}) to O(T^{-1/3}). Empirical results indicate that MARS-M consistently outperforms existing optimizers in terms of loss reduction and overall performance on various tasks, including language modeling and computer vision."
                },
                "zh": {
                    "title": "MARS-M：加速大规模神经网络训练的新优化器",
                    "desc": "MARS-M是一种新型优化器，结合了Muon和MARS技术，能够在大规模神经网络训练中实现更快的收敛和更好的性能。最近的研究表明，基于矩阵的预条件优化器如Muon在训练大型语言模型时比标量优化器更高效。同时，MARS技术通过减少方差，显著加快了预训练的速度。本文证明了MARS-M在标准条件下以更快的收敛速度达到一阶驻点，并在语言建模和计算机视觉任务中表现出更低的损失和更好的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-10-27.html",
    "link_next": "2025-10-29.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "27.10",
        "en": "10/27",
        "zh": "10月27日"
    },
    "short_date_next": {
        "ru": "29.10",
        "en": "10/29",
        "zh": "10月29日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 8,
        "#agents": 4,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 2,
        "#plp": 1,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 2,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 2,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}