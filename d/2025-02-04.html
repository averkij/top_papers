
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 20 papers. February 4.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">4 февраля</span> | <span id="title-articles-count">20 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-03.html">⬅️ <span id="prev-date">03.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-05.html">➡️ <span id="next-date">05.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'};
        let feedDateNext = {'ru': '05.02', 'en': '02/05', 'zh': '2月5日'};
        let feedDatePrev = {'ru': '03.02', 'en': '02/03', 'zh': '2月3日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.01237', 'title': 'The Differences Between Direct Alignment Algorithms are a Blur', 'url': 'https://huggingface.co/papers/2502.01237', 'abstract': 'Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the rewards used in those losses (e.g., likelihood ratios of policy and reference policy, or odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required (two-stage vs. one-stage). We first show that one-stage methods underperform two-stage methods. To address this, we incorporate an explicit SFT phase and introduce the beta parameter, controlling the strength of preference optimization, into single-stage ORPO and ASFT. These modifications improve their performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT), matching two-stage methods like DPO. Further analysis reveals that the key factor is whether the approach uses pairwise or pointwise objectives, rather than the specific implicit reward or loss function. These results highlight the importance of careful evaluation to avoid premature claims of performance gains or overall superiority in alignment algorithms.', 'score': 42, 'issue_id': 2022, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '18ba45e237fff5e1', 'authors': ['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], 'affiliations': ['T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.01237.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Прямое выравнивание: простой путь к улучшению языковых моделей', 'desc': 'В статье рассматриваются алгоритмы прямого выравнивания (DAA) как альтернатива обучению с подкреплением в контексте выравнивания языковых моделей. Авторы классифицируют DAA по типу функции потерь, используемым наградам и необходимости предварительного обучения. Исследование показывает, что двухэтапные методы превосходят одноэтапные, а ключевым фактором эффективности является использование попарных, а не поточечных целевых функций. Результаты подчеркивают важность тщательной оценки при сравнении алгоритмов выравнивания языковых моделей.'}, 'en': {'title': 'Simplifying Language Model Alignment with Direct Optimization', 'desc': 'This paper introduces Direct Alignment Algorithms (DAAs) as a simpler alternative to traditional Reinforcement Learning from Human Feedback (RLHF) methods. DAAs optimize language model alignment directly, avoiding the complexities of reinforcement learning and reward modeling. The study compares one-stage and two-stage methods, revealing that incorporating a Supervised Fine-Tuning (SFT) phase significantly enhances performance. The findings emphasize the importance of the choice between pairwise and pointwise ranking losses in achieving effective alignment, cautioning against hasty conclusions about algorithm superiority.'}, 'zh': {'title': '优化对齐算法，提升模型性能！', 'desc': '直接对齐算法（DAAs）通过直接优化策略来简化语言模型的对齐，取代了人类反馈强化学习中的强化学习和奖励建模。DAAs可以根据其排名损失（成对与点对）和使用的奖励类型进行分类。研究表明，一阶段方法的表现不如两阶段方法，因此我们引入了显式的监督微调阶段，并在单阶段的ORPO和ASFT中加入了控制偏好优化强度的beta参数。这些改进使得它们在Alpaca Eval 2中的表现得到了显著提升，表明选择成对或点对目标是关键因素，而不是具体的隐式奖励或损失函数。'}}}, {'id': 'https://huggingface.co/papers/2502.01061', 'title': 'OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models', 'url': 'https://huggingface.co/papers/2502.01061', 'abstract': 'End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)', 'score': 34, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '56b819a66e336562', 'authors': ['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.01061.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#video', '#training', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'OmniHuman: универсальная модель для генерации реалистичных видео с людьми', 'desc': 'OmniHuman - это новая модель на основе Diffusion Transformer для генерации реалистичных видео с людьми. Она использует смешанные условия движения при обучении, что позволяет масштабировать данные и улучшить качество генерации. Модель поддерживает различные типы портретов, взаимодействие с объектами и сложные позы тела. OmniHuman может генерировать видео на основе аудио, видео или комбинированных сигналов управления.'}, 'en': {'title': 'OmniHuman: Revolutionizing Realistic Human Animation Generation', 'desc': 'The paper presents OmniHuman, a new framework for generating realistic human animations from audio inputs. It utilizes a Diffusion Transformer architecture that enhances the training process by incorporating motion-related conditions, allowing for better scalability in video generation. OmniHuman is designed to handle various types of human portraits and interactions, producing high-quality videos that can depict talking, singing, and complex body movements. This approach not only improves the realism of the generated videos but also increases flexibility by supporting multiple input modalities such as audio and video.'}, 'zh': {'title': 'OmniHuman：灵活真实的人类动画生成', 'desc': '本文提出了一种名为OmniHuman的框架，旨在提升人类动画生成的质量和灵活性。该框架基于扩散变换器，通过在训练阶段混合与运动相关的条件来扩展数据规模。OmniHuman支持多种人像内容和不同的驱动模式，如音频驱动和视频驱动，能够生成高度真实的人类视频。与现有方法相比，OmniHuman不仅生成更真实的视频，还提供了更大的输入灵活性。'}}}, {'id': 'https://huggingface.co/papers/2502.01456', 'title': 'Process Reinforcement through Implicit Rewards', 'url': 'https://huggingface.co/papers/2502.01456', 'abstract': "Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phrase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME's effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.", 'score': 33, 'issue_id': 2019, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '9d62c40e4bafac91', 'authors': ['Ganqu Cui', 'Lifan Yuan', 'Zefan Wang', 'Hanbin Wang', 'Wendi Li', 'Bingxiang He', 'Yuchen Fan', 'Tianyu Yu', 'Qixin Xu', 'Weize Chen', 'Jiarui Yuan', 'Huayu Chen', 'Kaiyan Zhang', 'Xingtai Lv', 'Shuo Wang', 'Yuan Yao', 'Xu Han', 'Hao Peng', 'Yu Cheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou', 'Ning Ding'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Shanghai Jiaotong University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.01456.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'PRIME: Эффективное обучение ИИ-моделей с неявными процессными наградами', 'desc': 'Эта статья представляет новый метод обучения с подкреплением для больших языковых моделей, называемый PRIME. Он использует неявные процессные награды, что позволяет обновлять модели вознаграждения процесса в режиме онлайн, используя только развертывания политики и метки результатов. PRIME решает проблемы обучения с плотными наградами, такие как уязвимость к взлому наград и высокая стоимость сбора качественных меток процесса. Метод показал значительное улучшение производительности на задачах рассуждения по сравнению с базовыми моделями, используя при этом меньше данных для обучения.'}, 'en': {'title': 'Unlocking LLM Potential with PRIME: Efficient Training through Implicit Rewards', 'desc': 'This paper introduces PRIME, a method that enhances the training of large language models (LLMs) using dense process rewards instead of traditional sparse outcome rewards. Dense rewards help improve training efficiency and address credit assignment issues, but collecting high-quality process labels has been a challenge. PRIME allows for online updates of process reward models using only policy rollouts and outcome labels, which reduces the need for extensive reward model training. The results show that PRIME significantly improves reasoning performance in tasks like math and coding, achieving better results with less training data compared to existing models.'}, 'zh': {'title': 'PRIME：提升大语言模型推理效率的新方法', 'desc': '本文提出了一种新的方法PRIME（通过隐式奖励进行过程强化学习），旨在解决大语言模型（LLMs）在复杂多步骤推理任务中的训练效率问题。传统的稀疏结果奖励在训练过程中存在效率低下和信用分配等问题，而PRIME通过仅使用策略回滚和结果标签来实现在线过程奖励模型（PRM）的更新。该方法避免了现有方法中需要的专门奖励模型训练阶段，从而显著降低了开发成本。实验结果表明，PRIME在数学和编码竞赛任务中表现出色，相较于传统模型有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2502.01534', 'title': 'Preference Leakage: A Contamination Problem in LLM-as-a-judge', 'url': 'https://huggingface.co/papers/2502.01534', 'abstract': 'Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.', 'score': 17, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd2508b2b8b82b41b', 'authors': ['Dawei Li', 'Renliang Sun', 'Yue Huang', 'Ming Zhong', 'Bohan Jiang', 'Jiawei Han', 'Xiangliang Zhang', 'Wei Wang', 'Huan Liu'], 'affiliations': ['Arizona State University', 'University of California, Los Angeles', 'University of Illinois Urbana Champaign', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2502.01534.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#training', '#dataset', '#leakage'], 'emoji': '🕵️', 'ru': {'title': 'Осторожно: LLM-судьи могут быть предвзяты!', 'desc': "Исследование выявляет проблему 'утечки предпочтений' при использовании больших языковых моделей (LLM) в качестве судей для оценки других моделей. Эта проблема возникает из-за связанности между генераторами синтетических данных и LLM-оценщиками. Эксперименты подтверждают предвзятость судей к связанным с ними моделям-ученикам на различных базовых моделях и эталонных тестах. Результаты указывают на то, что утечка предпочтений является распространенной и трудно обнаруживаемой проблемой в области использования LLM в качестве судей."}, 'en': {'title': 'Uncovering Preference Leakage: A Hidden Bias in LLM Evaluation', 'desc': 'This paper discusses a problem called preference leakage in the context of using Large Language Models (LLMs) as judges for data annotation. Preference leakage occurs when the relationship between the data generators and the evaluators leads to biased evaluations, particularly when they are similar or related models. The authors identify three types of relatedness that can cause this issue and demonstrate through experiments that judges show bias towards their related models. The findings highlight that preference leakage is a significant and often unnoticed challenge in LLM-based model development.'}, 'zh': {'title': '偏好泄漏：LLM评判中的隐患', 'desc': '本文探讨了大型语言模型（LLM）作为评判者和基于LLM的数据合成在模型开发中的应用。我们揭示了偏好泄漏这一问题，它是由合成数据生成器与LLM评估者之间的相关性引起的。通过定义三种常见的相关性，我们进行了广泛的实验，证实了评判者对其相关学生模型的偏见。研究表明，偏好泄漏是一个普遍存在且难以检测的问题，影响了LLM作为评判者的有效性。'}}}, {'id': 'https://huggingface.co/papers/2501.18636', 'title': 'SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model', 'url': 'https://huggingface.co/papers/2501.18636', 'abstract': 'The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.', 'score': 11, 'issue_id': 2023, 'pub_date': '2025-01-28', 'pub_date_card': {'ru': '28 января', 'en': 'January 28', 'zh': '1月28日'}, 'hash': '3a201d426049658a', 'authors': ['Xun Liang', 'Simin Niu', 'Zhiyu Li', 'Sensen Zhang', 'Hanyu Wang', 'Feiyu Xiong', 'Jason Zhaoxin Fan', 'Bo Tang', 'Shichao Song', 'Mengwei Wang', 'Jiawei Yang'], 'affiliations': ['Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, School of Artificial Intelligence, Beihang University, Beijing, China', 'Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.18636.jpg', 'data': {'categories': ['#rag', '#security', '#dataset', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'SafeRAG: оценка уязвимостей генерации с дополнением на основе извлечения', 'desc': 'В статье представлен бенчмарк SafeRAG для оценки безопасности генерации с дополнением на основе извлечения (RAG). Авторы классифицируют атаки на RAG и создают набор данных для их моделирования. Эксперименты показывают, что RAG уязвим ко всем типам атак, даже самым очевидным. Исследование демонстрирует, что существующие компоненты RAG не способны эффективно противостоять атакам, что приводит к снижению качества работы системы.'}, 'en': {'title': 'Strengthening RAG: Evaluating Vulnerabilities in Knowledge Integration', 'desc': 'This paper addresses the security vulnerabilities of retrieval-augmented generation (RAG) systems, which combine external knowledge with large language models (LLMs) for knowledge-intensive tasks. The authors introduce a benchmark called SafeRAG to evaluate the security of RAG by classifying various attack types that can manipulate knowledge. They create a dataset specifically for testing these vulnerabilities and simulate different attack scenarios to assess the impact on RAG performance. The results show that RAG systems are significantly susceptible to these attacks, leading to a decline in service quality, highlighting the need for improved security measures.'}, 'zh': {'title': '提升RAG安全性，抵御知识攻击！', 'desc': '本文介绍了一种名为SafeRAG的基准，用于评估检索增强生成（RAG）模型的安全性。我们将攻击任务分为银噪声、上下文冲突、软广告和拒绝服务等类型，并为每种任务手动构建了安全评估数据集。通过使用SafeRAG数据集，我们模拟了RAG可能遇到的各种攻击场景。实验结果表明，RAG对所有攻击任务表现出显著的脆弱性，甚至最明显的攻击任务也能轻易绕过现有的检索器和过滤器，导致RAG服务质量下降。'}}}, {'id': 'https://huggingface.co/papers/2502.01068', 'title': 'FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation', 'url': 'https://huggingface.co/papers/2502.01068', 'abstract': 'While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00times and 1.40times improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.', 'score': 9, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '58ab72f123d7a4b6', 'authors': ['Dongwon Jo', 'Jiwon Song', 'Yulhwa Kim', 'Jae-Joon Kim'], 'affiliations': ['Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea', 'Department of Semiconductor Systems Engineering, Sungkyunkwan University, Suwon, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.01068.jpg', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'FastKV: Ускорение обработки длинных последовательностей в LLM', 'desc': 'Статья представляет FastKV - новый метод сжатия кэша ключ-значение (KV) для больших языковых моделей (LLM), направленный на улучшение латентности при обработке длинных последовательностей. FastKV использует подход Token-Selective Propagation (TSP), который сохраняет полную контекстную информацию в начальных слоях LLM и выборочно распространяет только часть этой информации в более глубоких слоях. Метод также включает сжатие кэша KV с учетом grouped-query attention (GQA) для повышения эффективности памяти и вычислений. Эксперименты показывают, что FastKV достигает значительного улучшения времени до первого токена и пропускной способности по сравнению с современными методами, сохраняя при этом точность на уровне базовых показателей.'}, 'en': {'title': 'FastKV: Speeding Up Long-Context Processing in LLMs', 'desc': 'This paper presents FastKV, a new method for compressing key-value (KV) caches in large language models (LLMs) to improve computational efficiency and reduce latency. FastKV uses a Token-Selective Propagation (TSP) strategy that keeps full context information in the early layers of the model while selectively passing on only part of this information in the deeper layers. Additionally, it employs grouped-query attention (GQA) to enhance both memory usage and processing speed. Experimental results demonstrate that FastKV significantly improves time-to-first-token and throughput while maintaining accuracy on long-context tasks.'}, 'zh': {'title': 'FastKV：提升长上下文处理速度的创新方法', 'desc': '本文介绍了一种名为FastKV的KV缓存压缩方法，旨在提高长上下文序列的处理速度。FastKV采用了一种新颖的选择性传播方法（TSP），在LLM的初始层保留完整的上下文信息，而在更深层次中仅选择性传播部分信息。该方法还结合了分组查询注意力（GQA）来优化内存和计算效率。实验结果表明，FastKV在首次令牌时间和吞吐量方面分别比现有的HeadKV方法提高了2.00倍和1.40倍，同时在长上下文基准测试中保持了与基线相当的准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.00094', 'title': 'AIN: The Arabic INclusive Large Multimodal Model', 'url': 'https://huggingface.co/papers/2502.00094', 'abstract': "Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications.", 'score': 9, 'issue_id': 2018, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': 'e2d63540ee133732', 'authors': ['Ahmed Heakl', 'Sara Ghaboura', 'Omkar Thawkar', 'Fahad Shahbaz Khan', 'Hisham Cholakkal', 'Rao Muhammad Anwer', 'Salman Khan'], 'affiliations': ['Aalto University', 'Australian National University', 'Linköping University', 'Mohamed bin Zayed University of AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.00094.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#low_resource', '#multimodal', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'AIN: Прорыв в арабоязычном мультимодальном ИИ', 'desc': 'Представлена модель AIN - двуязычная мультимодальная языковая модель для арабского и английского языков. Модель обучена на 3,6 миллионах высококачественных мультимодальных арабско-английских образцов данных. AIN демонстрирует передовые результаты в арабском языке, сохраняя при этом сильные визуальные возможности для английского. На бенчмарке CAMEL-Bench, охватывающем 38 поддоменов, 7B-версия AIN превосходит GPT-4o на 3,4% в среднем по восьми доменам.'}, 'en': {'title': 'Empowering Arabic with Advanced Multimodal AI', 'desc': 'This paper presents AIN, the Arabic Inclusive Multimodal Model, which is designed to enhance the performance of large multimodal models (LMMs) specifically for Arabic and English. AIN utilizes a substantial dataset of 3.6 million high-quality Arabic-English multimodal samples to achieve state-of-the-art results in Arabic language tasks. The model excels across various domains, as evidenced by its performance on the CAMEL-Bench benchmark, where it surpasses GPT-4o in multiple sub-domains. AIN aims to provide advanced generative AI tools for Arabic speakers, addressing the current limitations in Arabic multimodal understanding.'}, 'zh': {'title': '推动阿拉伯语多模态AI的进步', 'desc': '随着大型语言模型（LLMs）和多模态模型（LMMs）的快速发展，阿拉伯语的研究仍然相对滞后。我们提出了AIN模型，这是一个旨在提升阿拉伯语和英语的多模态模型，利用了360万高质量的阿拉伯语-英语多模态数据样本。AIN在多个领域表现出色，尤其是在复杂的视觉理解和多图像理解方面，超越了现有的GPT-4o模型。该模型的优越性能为阿拉伯语使用者提供了先进的多模态生成AI工具，推动了相关应用的发展。'}}}, {'id': 'https://huggingface.co/papers/2502.01142', 'title': 'DeepRAG: Thinking to Retrieval Step by Step for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01142', 'abstract': 'Large Language Models (LLMs) have shown remarkable potential in reasoning while they still suffer from severe factual hallucinations due to timeliness, accuracy, and coverage of parametric knowledge. Meanwhile, integrating reasoning with retrieval-augmented generation (RAG) remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling strategic and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency while improving answer accuracy by 21.99%, demonstrating its effectiveness in optimizing retrieval-augmented reasoning.', 'score': 7, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'e26994166d750227', 'authors': ['Xinyan Guan', 'Jiali Zeng', 'Fandong Meng', 'Chunlei Xin', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Le Sun', 'Jie Zhou'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.01142.jpg', 'data': {'categories': ['#optimization', '#hallucinations', '#reasoning', '#rag', '#rl'], 'emoji': '🧠', 'ru': {'title': 'DeepRAG: умное сочетание поиска и рассуждений для ИИ', 'desc': 'DeepRAG - это новый фреймворк, моделирующий рассуждения с поиском информации как марковский процесс принятия решений. Он итеративно декомпозирует запросы, динамически определяя необходимость внешнего поиска или параметрического рассуждения на каждом шаге. Эксперименты показывают, что DeepRAG повышает эффективность поиска и точность ответов на 21.99%. Фреймворк решает проблемы больших языковых моделей, связанные с фактическими галлюцинациями и неэффективной декомпозицией задач при интеграции рассуждений с поиском.'}, 'en': {'title': 'Enhancing Reasoning with Smart Retrieval', 'desc': 'This paper introduces DeepRAG, a new framework that enhances retrieval-augmented reasoning in large language models. It treats the retrieval process as a Markov Decision Process (MDP), allowing for more strategic and adaptive retrieval of information. By breaking down queries into smaller parts, DeepRAG can decide whether to fetch external knowledge or use its built-in reasoning capabilities at each step. The results show that DeepRAG not only makes retrieval more efficient but also increases the accuracy of answers by nearly 22%.'}, 'zh': {'title': 'DeepRAG：优化检索增强推理的新框架', 'desc': '大型语言模型（LLMs）在推理方面展现了显著的潜力，但仍然面临严重的事实幻觉问题，主要由于参数知识的时效性、准确性和覆盖率不足。将推理与检索增强生成（RAG）结合起来仍然具有挑战性，主要是因为任务分解不有效和冗余检索可能引入噪声，降低响应质量。本文提出了DeepRAG框架，将检索增强推理建模为马尔可夫决策过程（MDP），实现了战略性和自适应的检索。实验表明，DeepRAG在提高检索效率的同时，答案准确性提高了21.99%，证明了其在优化检索增强推理方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.00698', 'title': 'MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models', 'url': 'https://huggingface.co/papers/2502.00698', 'abstract': 'IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.   Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide.', 'score': 6, 'issue_id': 2027, 'pub_date': '2025-02-02', 'pub_date_card': {'ru': '2 февраля', 'en': 'February 2', 'zh': '2月2日'}, 'hash': '960e3460f0ab8e56', 'authors': ['Huanqia Cai', 'Yijun Yang', 'Winston Hu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.00698.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальный ИИ проваливает тест на интеллект', 'desc': 'Статья представляет новый фреймворк MM-IQ для оценки когнитивных способностей мультимодальных систем искусственного интеллекта. Фреймворк включает 2,710 тестовых заданий по 8 типам рассуждений, аналогично тестам IQ для людей. Результаты показали, что даже передовые мультимодальные модели демонстрируют точность лишь немного выше случайного угадывания (27.49% против 25%). Это подчеркивает существенный разрыв между текущими возможностями ИИ и базовыми когнитивными способностями человека.'}, 'en': {'title': 'Bridging the Cognitive Divide in AI with MM-IQ', 'desc': 'This paper introduces MM-IQ, a new evaluation framework designed to measure cognitive abilities in artificial intelligence systems, similar to how IQ tests assess human intelligence. It includes 2,710 test items across 8 reasoning paradigms, providing a comprehensive benchmark for multimodal models. The evaluation of various state-of-the-art models shows that their performance is only slightly better than random guessing, indicating significant limitations in their reasoning capabilities. This finding emphasizes the urgent need for advancements in AI to better replicate human-like reasoning skills.'}, 'zh': {'title': 'MM-IQ：评估多模态系统的认知能力新标准', 'desc': '本论文提出了一种名为MM-IQ的综合评估框架，用于量化多模态系统中的认知能力。该框架包含2710个精心策划的测试项目，涵盖8种不同的推理范式。通过对领先的多模态模型进行系统评估，结果显示即使是最先进的架构，其表现也仅略高于随机猜测。这个显著的性能差距表明当前多模态系统在接近人类基本推理能力方面的不足，强调了需要进行范式转变的进步。'}}}, {'id': 'https://huggingface.co/papers/2502.01208', 'title': 'Almost Surely Safe Alignment of Large Language Models at Inference-Time', 'url': 'https://huggingface.co/papers/2502.01208', 'abstract': "Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses.", 'score': 6, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'fdbe8816f1c6476a', 'authors': ['Xiaotong Ji', 'Shyam Sundhar Ramesh', 'Matthieu Zimmer', 'Ilija Bogunovic', 'Jun Wang', 'Haitham Bou Ammar'], 'affiliations': ['Huawei Noahs Ark Lab', 'Imperial College London', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2502.01208.jpg', 'data': {'categories': ['#ethics', '#rlhf', '#inference', '#alignment'], 'emoji': '🛡️', 'ru': {'title': 'Безопасная генерация ответов LLM без переобучения', 'desc': 'Статья представляет новый подход к выравниванию больших языковых моделей (LLM) во время вывода, обеспечивающий генерацию безопасных ответов с вероятностью, близкой к единице. Авторы формулируют задачу как ограниченный марковский процесс принятия решений в латентном пространстве модели. Они вводят состояние безопасности, которое отслеживает эволюцию ограничений безопасности и позволяет доказать формальные гарантии безопасности. На основе этого подхода разработан метод InferenceGuard, который эффективно балансирует безопасность и производительность задачи, превосходя существующие методы выравнивания во время вывода.'}, 'en': {'title': 'InferenceGuard: Ensuring Safe Responses from LLMs at Inference Time', 'desc': "This paper addresses the issue of biased or unsafe responses from large language models (LLMs) by introducing a new method for aligning their outputs during inference. Instead of retraining the model, the authors propose a constrained Markov decision process that operates in the LLM's latent space to ensure safe response generation. They enhance this process with a safety state that monitors safety constraints, providing formal guarantees of safety. The proposed method, called InferenceGuard, shows improved performance in generating safe and aligned responses compared to existing techniques without altering the model's weights."}, 'zh': {'title': '推理时安全对齐的新方法', 'desc': '本文提出了一种新的推理时对齐方法，旨在确保大型语言模型（LLM）生成安全的响应。我们将安全生成推理时响应的问题框架化为一个约束的马尔可夫决策过程（MDP），并在LLM的潜在空间中进行处理。通过引入一个安全状态来跟踪安全约束的演变，我们能够在解决MDP时提供正式的安全保证。我们提出的InferenceGuard实现了在不修改模型权重的情况下安全对齐LLM，并在生成安全和对齐响应方面优于现有的方法。'}}}, {'id': 'https://huggingface.co/papers/2502.01100', 'title': 'ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning', 'url': 'https://huggingface.co/papers/2502.01100', 'abstract': 'We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty.   Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.', 'score': 6, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '901fd196d7bfe394', 'authors': ['Bill Yuchen Lin', 'Ronan Le Bras', 'Kyle Richardson', 'Ashish Sabharwal', 'Radha Poovendran', 'Peter Clark', 'Yejin Choi'], 'affiliations': ['Allen Institute for AI', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.01100.jpg', 'data': {'categories': ['#reasoning', '#training', '#inference', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Проклятие сложности в логическом мышлении LLM', 'desc': 'В статье исследуются возможности логического рассуждения больших языковых моделей (LLM) и их масштабируемость в сложных задачах немонотонного вывода. Для этого представлена ZebraLogic, комплексная система оценки производительности LLM на логических головоломках, основанных на задачах удовлетворения ограничений (CSP). ZebraLogic позволяет генерировать головоломки с контролируемой сложностью, что помогает систематически изучать пределы масштабируемости моделей, таких как Llama и DeepSeek-R1. Результаты показывают значительное снижение точности по мере увеличения сложности задач, что авторы называют "проклятием сложности".'}, 'en': {'title': 'Unraveling the Limits of Logical Reasoning in Large Language Models', 'desc': "This paper examines how well large language models (LLMs) can perform logical reasoning, especially in complex scenarios where reasoning does not follow a straightforward path. The authors introduce ZebraLogic, a new framework designed to evaluate LLMs on logic grid puzzles that are based on constraint satisfaction problems (CSPs). Through this framework, they discover that as the complexity of the puzzles increases, the accuracy of the models significantly decreases, a challenge they refer to as the 'curse of complexity.' The study also suggests methods to improve reasoning capabilities, such as using advanced sampling techniques and self-verification prompts, while highlighting the limitations of current LLMs in handling complex reasoning tasks."}, 'zh': {'title': '揭示大型语言模型推理能力的复杂性挑战', 'desc': '本文研究了大型语言模型（LLMs）的逻辑推理能力及其在复杂非单调推理中的可扩展性。我们引入了ZebraLogic，一个全面的评估框架，用于评估LLM在基于约束满足问题（CSPs）的逻辑网格谜题上的推理表现。研究结果显示，随着问题复杂性的增加，模型的准确性显著下降，这一现象被称为复杂性诅咒。我们还探讨了增强逻辑推理的策略，包括最佳采样、回溯机制和自我验证提示。'}}}, {'id': 'https://huggingface.co/papers/2502.01081', 'title': 'The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles', 'url': 'https://huggingface.co/papers/2502.01081', 'abstract': "The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available https://github.com/declare-lab/LLM-PuzzleTest.", 'score': 6, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '7585b424ff041825', 'authors': ['Vernon Y. H. Toh', 'Yew Ken Chia', 'Deepanway Ghosal', 'Soujanya Poria'], 'affiliations': ['Singapore University of Technology and Design (SUTD)'], 'pdf_title_img': 'assets/pdf/title_img/2502.01081.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#agi', '#open_source', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эволюция рассуждений: от символов к мультимодальности', 'desc': 'Статья рассматривает эволюцию возможностей рассуждения в мультимодальных задачах у языковых моделей серий GPT и OpenAI. Авторы отмечают значительный прогресс модели o3 в решении символических паттернов, но подчеркивают необходимость исследования мультимодальных сценариев. Проводится анализ производительности моделей на сложных визуально-лингвистических головоломках, требующих абстрактного и алгоритмического мышления. Результаты показывают общую тенденцию улучшения способностей к рассуждению, но выявляют сохраняющиеся трудности даже у передовых моделей в некоторых типах задач.'}, 'en': {'title': 'Advancing Reasoning in Multimodal AI: A New Era for LLMs', 'desc': "This paper discusses the advancements in Large Language Models (LLMs) with the release of OpenAI's o1 and o3, which show improved reasoning abilities. The o3 model has demonstrated superior problem-solving skills compared to humans on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, the study highlights that these models primarily focus on symbolic reasoning, while human reasoning often involves multimodal inputs like vision and language. The authors emphasize the need for further research into multimodal reasoning capabilities, as the o1 model, despite its high performance, still faces challenges with simple multimodal and algorithmic puzzles."}, 'zh': {'title': '多模态推理能力的探索与挑战', 'desc': '本文探讨了OpenAI的o1和o3模型在大型语言模型中的先进推理能力。o3在抽象和推理语料库（ARC-AGI）中超越了人类，表现出色，但该基准仅限于符号模式。人类通常在多模态场景中进行推理，因此需要研究多模态任务中的高级推理能力。尽管o1在推理能力上有所提升，但在简单的多模态难题和算法难题上仍然存在不足。'}}}, {'id': 'https://huggingface.co/papers/2502.01591', 'title': 'Improving Transformer World Models for Data-Efficient RL', 'url': 'https://huggingface.co/papers/2502.01591', 'abstract': 'We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna with warmup", which trains the policy on real and imaginary data, (b) "nearest neighbor tokenizer" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep.', 'score': 4, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': 'd9195e9417fce419', 'authors': ['Antoine Dedieu', 'Joseph Ortiz', 'Xinghua Lou', 'Carter Wendelken', 'Wolfgang Lehrach', 'J Swaroop Guntupalli', 'Miguel Lazaro-Gredilla', 'Kevin Patrick Murphy'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.01591.jpg', 'data': {'categories': ['#rl', '#architecture', '#benchmark', '#games', '#training', '#reasoning', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Новый рубеж в model-based RL: превосходя человека в Craftax-classic', 'desc': 'Статья представляет новый подход к обучению с подкреплением на основе модели, достигающий наилучших результатов на бенчмарке Craftax-classic. Авторы разработали алгоритм, превосходящий как предыдущий SOTA-метод DreamerV3, так и человеческий уровень производительности. Ключевые улучшения включают комбинированную архитектуру политики с CNN и RNN, обучение на реальных и воображаемых данных, токенизацию изображений методом ближайших соседей и блочное teacher forcing для трансформерной модели мира. Эти инновации позволили значительно повысить эффективность использования данных в сложной среде, требующей широкого спектра навыков.'}, 'en': {'title': 'Revolutionizing Model-Based RL for Superior Game Performance', 'desc': "This paper introduces a new model-based reinforcement learning (MBRL) approach that excels in the Craftax-classic benchmark, a complex 2D survival game. The proposed algorithm achieves a remarkable reward of 67.4% after just 1 million environment steps, surpassing previous methods like DreamerV3 and even human performance. Key innovations include a novel policy architecture that integrates convolutional neural networks (CNNs) and recurrent neural networks (RNNs), along with enhancements like 'Dyna with warmup' for training on both real and simulated data. Additional techniques such as a nearest neighbor tokenizer for image patches and block teacher forcing for future reasoning further boost the model's efficiency and effectiveness."}, 'zh': {'title': '基于模型的强化学习新突破！', 'desc': '本文提出了一种基于模型的强化学习方法，在Craftax-classic基准测试中取得了新的最佳表现。这是一款开放世界的2D生存游戏，要求智能体展现出强大的泛化能力、深度探索能力和长期推理能力。我们的MBRL算法在仅1M环境步骤后获得了67.4%的奖励，显著超越了DreamerV3的53.2%，并首次超过了人类表现的65.0%。该方法通过构建一个最先进的无模型基线，并结合CNN和RNN的新型策略架构，进一步提升了样本效率。'}}}, {'id': 'https://huggingface.co/papers/2502.01441', 'title': 'Improved Training Technique for Latent Consistency Models', 'url': 'https://huggingface.co/papers/2502.01441', 'abstract': 'Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-c scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: https://github.com/quandao10/sLCT/', 'score': 4, 'issue_id': 2018, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '2ea077ca6fd7397f', 'authors': ['Quan Dao', 'Khanh Doan', 'Di Liu', 'Trung Le', 'Dimitris Metaxas'], 'affiliations': ['Monash University', 'Rutgers University', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.01441.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#architecture', '#open_source', '#diffusion', '#video'], 'emoji': '🧠', 'ru': {'title': 'Преодоление выбросов в латентном пространстве для улучшения консистентных моделей', 'desc': 'Эта статья представляет новый подход к обучению консистентных моделей в латентном пространстве для генеративных задач. Авторы обнаружили, что латентные данные часто содержат импульсивные выбросы, которые ухудшают производительность iCT. Для решения этой проблемы они предложили использовать функцию потерь Коши вместо Псевдо-Хубера, а также ввели диффузионные потери на ранних временных шагах и применили оптимальный транспорт. Эти стратегии позволили успешно обучить латентные консистентные модели, способные к высококачественному сэмплированию за один-два шага.'}, 'en': {'title': 'Enhancing Latent Consistency Models for High-Quality Generation', 'desc': "This paper introduces advancements in consistency models, a type of generative model that can create high-quality outputs efficiently. The authors focus on improving performance in latent spaces, where data often contains outliers that hinder model effectiveness. By replacing traditional loss functions with Cauchy losses and incorporating diffusion loss, they enhance the model's robustness against these outliers. Additionally, they propose an adaptive scaling-c scheduler and Non-scaling LayerNorm to optimize training, resulting in latent consistency models that perform comparably to diffusion models in generating images and videos."}, 'zh': {'title': '提升一致性模型性能的创新方法', 'desc': '一致性模型是一种新型生成模型，能够在单步或多步中生成高质量样本。最近，这些模型在像素空间中表现出色，达到了与扩散模型相当的效果。然而，在大规模数据集上进行一致性训练的成功，尤其是在文本到图像和视频生成任务中，取决于潜在空间的表现。为了解决潜在数据中的异常值对性能的影响，本文提出了使用Cauchy损失替代伪Huber损失，并引入扩散损失和最优传输方法，以提高模型的鲁棒性和性能。'}}}, {'id': 'https://huggingface.co/papers/2502.01637', 'title': 'Scaling Embedding Layers in Language Models', 'url': 'https://huggingface.co/papers/2502.01637', 'abstract': 'We propose SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent n-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. During inference, they are precomputed and stored in off-accelerator memory with minimal impact on inference speed. SCONE enables two new scaling strategies: increasing the number of cached n-gram embeddings and scaling the model used to learn them, all while maintaining fixed inference-time FLOPS. We show that scaling both aspects allows SCONE to outperform a 1.9B parameter baseline across diverse corpora, while using only half the inference-time FLOPS.', 'score': 3, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '478a2a0ee08530a8', 'authors': ['Da Yu', 'Edith Cohen', 'Badih Ghazi', 'Yangsibo Huang', 'Pritish Kamath', 'Ravi Kumar', 'Daogao Liu', 'Chiyuan Zhang'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.01637.jpg', 'data': {'categories': ['#architecture', '#inference', '#long_context', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей без увеличения вычислительных затрат', 'desc': 'SCONE - это новый метод расширения слоёв входных эмбеддингов для улучшения производительности языковых моделей при увеличении размера слоя. Он вводит эмбеддинги для частых n-грамм, обеспечивая контекстуализированное представление для каждого входного токена. Эти эмбеддинги обучаются отдельной моделью и предварительно вычисляются для использования во время инференса. SCONE позволяет масштабировать количество кэшированных n-граммных эмбеддингов и модель для их обучения, сохраняя фиксированное количество FLOPS при инференсе.'}, 'en': {'title': 'Enhancing Language Models with SCONE: Scalable N-gram Embeddings for Better Performance', 'desc': 'SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding) is a novel approach designed to improve the performance of language models as they grow in size. It introduces embeddings for common n-grams while keeping the original vocabulary intact, which helps in providing better contextual representations for input tokens. These n-gram embeddings are learned through a separate model during training and stored in off-accelerator memory to ensure fast inference. By scaling both the number of cached n-gram embeddings and the model that learns them, SCONE achieves superior performance compared to a large baseline model while maintaining efficient inference-time computations.'}, 'zh': {'title': 'SCONE：提升语言模型性能的新方法', 'desc': '我们提出了一种方法SCONE（可扩展的上下文化的离线N-gram嵌入），旨在通过扩展输入嵌入层来提升语言模型的性能。SCONE在保持原有词汇的同时，引入了一组常见n-gram的嵌入，以提供每个输入标记的上下文化表示。这些嵌入在训练过程中由一个单独的模型学习，并在推理时预先计算并存储在离线加速器内存中，几乎不影响推理速度。通过增加缓存的n-gram嵌入数量和扩展学习它们的模型，SCONE在多种语料库上超越了1.9B参数的基线，同时仅使用一半的推理时间FLOPS。'}}}, {'id': 'https://huggingface.co/papers/2502.01639', 'title': 'SliderSpace: Decomposing the Visual Capabilities of Diffusion Models', 'url': 'https://huggingface.co/papers/2502.01639', 'abstract': "We present SliderSpace, a framework for automatically decomposing the visual capabilities of diffusion models into controllable and human-understandable directions. Unlike existing control methods that require a user to specify attributes for each edit direction individually, SliderSpace discovers multiple interpretable and diverse directions simultaneously from a single text prompt. Each direction is trained as a low-rank adaptor, enabling compositional control and the discovery of surprising possibilities in the model's latent space. Through extensive experiments on state-of-the-art diffusion models, we demonstrate SliderSpace's effectiveness across three applications: concept decomposition, artistic style exploration, and diversity enhancement. Our quantitative evaluation shows that SliderSpace-discovered directions decompose the visual structure of model's knowledge effectively, offering insights into the latent capabilities encoded within diffusion models. User studies further validate that our method produces more diverse and useful variations compared to baselines. Our code, data and trained weights are available at https://sliderspace.baulab.info", 'score': 2, 'issue_id': 2027, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '559003a020b42709', 'authors': ['Rohit Gandikota', 'Zongze Wu', 'Richard Zhang', 'David Bau', 'Eli Shechtman', 'Nick Kolkin'], 'affiliations': ['Adobe Research', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01639.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#dataset', '#open_source', '#multimodal', '#interpretability', '#cv'], 'emoji': '🎚️', 'ru': {'title': 'SliderSpace: Раскрытие скрытых возможностей диффузионных моделей', 'desc': 'SliderSpace - это фреймворк для автоматической декомпозиции визуальных возможностей диффузионных моделей на управляемые и понятные человеку направления. Он обнаруживает множество интерпретируемых и разнообразных направлений одновременно из одного текстового запроса. Каждое направление обучается как адаптер низкого ранга, что позволяет осуществлять композиционный контроль. Эксперименты показывают эффективность SliderSpace в различных приложениях, включая декомпозицию концепций и исследование художественных стилей.'}, 'en': {'title': 'Unlocking Creativity in Diffusion Models with SliderSpace', 'desc': "SliderSpace is a novel framework designed to break down the visual capabilities of diffusion models into controllable and understandable components. It automatically identifies multiple interpretable directions from a single text prompt, eliminating the need for users to define each edit direction manually. By training each direction as a low-rank adaptor, SliderSpace allows for compositional control and reveals unexpected possibilities within the model's latent space. Extensive experiments show that it effectively enhances concept decomposition, artistic style exploration, and diversity, outperforming existing methods in generating diverse and useful variations."}, 'zh': {'title': 'SliderSpace：可控的视觉能力分解', 'desc': 'SliderSpace是一个框架，用于自动分解扩散模型的视觉能力，使其可控且易于理解。与现有方法不同，SliderSpace可以从单个文本提示中同时发现多个可解释和多样化的方向，而无需用户逐个指定属性。每个方向被训练为低秩适配器，从而实现组合控制，并在模型的潜在空间中发现意想不到的可能性。通过对最先进的扩散模型进行广泛实验，我们证明了SliderSpace在概念分解、艺术风格探索和多样性增强等三个应用中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.01584', 'title': 'PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01584', 'abstract': "Existing benchmarks for frontier models often test specialized, ``PhD-level'' knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot.   Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with ``I give up'' before providing an answer that it knows is wrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases, it does not ``finish thinking,'' which suggests the need for an inference-time technique to ``wrap up'' before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.", 'score': 2, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '6cf23c9aeb70961a', 'authors': ['Carolyn Jane Anderson', 'Joydeep Biswas', 'Aleksander Boruch-Gruszecki', 'Federico Cassano', 'Molly Q Feldman', 'Arjun Guha', 'Francesca Lucchetti', 'Zixuan Wu'], 'affiliations': ['Charles University', 'Cursor', 'Northeastern University', 'Oberlin College', 'University of Texas at Austin', 'Wellesley College'], 'pdf_title_img': 'assets/pdf/title_img/2502.01584.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#inference', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Новый бенчмарк раскрывает скрытые возможности и недостатки языковых моделей', 'desc': 'Авторы представляют новый бенчмарк для оценки языковых моделей, основанный на головоломках NPR Sunday Puzzle Challenge. В отличие от существующих тестов, требующих специализированных знаний, этот бенчмарк опирается на общие знания и легко проверяем. Исследование выявило значительное превосходство модели OpenAI o1 над другими моделями рассуждений. Анализ также обнаружил новые типы ошибок у моделей, такие как преждевременная капитуляция и неуверенность в ответах.'}, 'en': {'title': 'Bridging Knowledge Gaps in AI Reasoning with General Knowledge Benchmarks', 'desc': "This paper introduces a new benchmark for evaluating machine learning models, focusing on general knowledge rather than specialized expertise. The benchmark is based on the NPR Sunday Puzzle Challenge, making it accessible yet challenging for both humans and AI models. The authors highlight that existing benchmarks may not reveal certain capability gaps, as demonstrated by OpenAI's model outperforming others in reasoning tasks. Additionally, the study identifies specific failure modes in models, such as premature concessions and uncertainty in outputs, suggesting improvements in inference techniques are necessary."}, 'zh': {'title': '挑战性与可验证性的全新基准测试', 'desc': '现有的前沿模型基准测试通常考察专业的、难以理解的知识。我们提出了一种基于NPR周日谜题挑战的基准，要求仅具备一般知识。该基准对人类和模型都具有挑战性，但正确答案易于验证，模型的错误也容易发现。我们的研究揭示了现有基准中未显现的能力差距，OpenAI o1在推理模型中表现优异，且分析推理输出揭示了新的失败类型。'}}}, {'id': 'https://huggingface.co/papers/2502.01636', 'title': 'Lifelong Sequential Knowledge Editing without Model Degradation', 'url': 'https://huggingface.co/papers/2502.01636', 'abstract': 'Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of the original model. We first show that locate-then-edit knowledge editing methods lead to overfitting on the edited facts. We also show that continuous knowledge editing using these methods leads to disproportionate growth in the norm of the edited matrix. We then provide a crucial insight into the inner workings of locate-then-edit methods. We show that norm-growth is a hidden trick employed by these methods that gives larger importance to the output activations produced from the edited layers. With this "importance hacking", the edited layers provide a much larger contributions to the model\'s output. To mitigate these issues, we present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. ENCORE controls for overfitting and the disproportionate norm-growth to enable long-term sequential editing, where we are able to perform up to 10,000 sequential edits without loss of downstream performance. ENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B.', 'score': 1, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '919dd5274b620b3a', 'authors': ['Akshat Gupta', 'Phudish Prateepamornkul', 'Maochuan Lu', 'Ahmed Alaa', 'Thomas Hartvigsen', 'Gopala Anumanchipalli'], 'affiliations': ['SCB DataX', 'University of California, Berkeley', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2502.01636.jpg', 'data': {'categories': ['#training', '#interpretability', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное масштабирование редактирования знаний в нейросетях', 'desc': 'Статья посвящена улучшению методов последовательного редактирования знаний в больших языковых моделях. Авторы выявили проблемы переобучения и непропорционального роста нормы при использовании существующих методов редактирования. Они предложили новый метод ENCORE, который контролирует переобучение и рост нормы, позволяя выполнять до 10 000 последовательных правок без потери производительности модели. ENCORE также показывает значительное ускорение по сравнению с другими методами редактирования знаний.'}, 'en': {'title': 'ENCORE: Efficient Knowledge Editing Without Degradation', 'desc': "This paper investigates the challenges of sequential knowledge editing in machine learning models, particularly focusing on the degradation of model performance after numerous edits. It identifies that traditional locate-then-edit methods can lead to overfitting and excessive growth in the norm of the edited parameters. The authors introduce a new method called ENCORE, which employs early stopping and norm constraints to prevent these issues, allowing for effective long-term editing. ENCORE not only maintains the model's performance after 10,000 edits but also operates significantly faster than existing methods."}, 'zh': {'title': 'ENCORE：高效的知识编辑解决方案', 'desc': '本论文研究了在知识编辑中进行大规模顺序编辑时模型性能下降的原因。我们发现，定位后编辑的方法容易导致对编辑事实的过拟合，并且连续的知识编辑会导致编辑矩阵的范数不成比例地增长。为了解决这些问题，我们提出了ENCORE方法，通过早停和范数约束来控制过拟合和范数增长，从而实现长时间的顺序编辑。ENCORE能够在不损失下游性能的情况下，进行多达10,000次的顺序编辑，并且比现有方法更快。'}}}, {'id': 'https://huggingface.co/papers/2501.18055', 'title': 'Current Pathology Foundation Models are unrobust to Medical Center Differences', 'url': 'https://huggingface.co/papers/2501.18055', 'abstract': 'Pathology Foundation Models (FMs) hold great promise for healthcare. Before they can be used in clinical practice, it is essential to ensure they are robust to variations between medical centers. We measure whether pathology FMs focus on biological features like tissue and cancer type, or on the well known confounding medical center signatures introduced by staining procedure and other differences. We introduce the Robustness Index. This novel robustness metric reflects to what degree biological features dominate confounding features. Ten current publicly available pathology FMs are evaluated. We find that all current pathology foundation models evaluated represent the medical center to a strong degree. Significant differences in the robustness index are observed. Only one model so far has a robustness index greater than one, meaning biological features dominate confounding features, but only slightly. A quantitative approach to measure the influence of medical center differences on FM-based prediction performance is described. We analyze the impact of unrobustness on classification performance of downstream models, and find that cancer-type classification errors are not random, but specifically attributable to same-center confounders: images of other classes from the same medical center. We visualize FM embedding spaces, and find these are more strongly organized by medical centers than by biological factors. As a consequence, the medical center of origin is predicted more accurately than the tissue source and cancer type. The robustness index introduced here is provided with the aim of advancing progress towards clinical adoption of robust and reliable pathology FMs.', 'score': 0, 'issue_id': 2024, 'pub_date': '2025-01-29', 'pub_date_card': {'ru': '29 января', 'en': 'January 29', 'zh': '1月29日'}, 'hash': '444c1f08656649e7', 'authors': ['Edwin D. de Jong', 'Eric Marcus', 'Jonas Teuwen'], 'affiliations': ['Aignostics', 'Antoni van Leeuwenhoek Hospital (AvL)', 'Kaiko', 'The Netherlands Cancer Institute Amsterdam (NKI)'], 'pdf_title_img': 'assets/pdf/title_img/2501.18055.jpg', 'data': {'categories': ['#healthcare', '#ethics', '#security', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Путь к надежным фундаментальным моделям в патологии: преодоление влияния медицинских центров', 'desc': 'Статья рассматривает проблему надежности фундаментальных моделей (ФМ) в патологии для использования в клинической практике. Авторы вводят новую метрику - Индекс надежности, который отражает степень доминирования биологических признаков над мешающими факторами, связанными с особенностями медицинских центров. Оценка десяти публично доступных ФМ показала, что все они сильно зависят от специфики медицинских центров, и только одна модель имеет индекс надежности больше единицы. Исследование демонстрирует, что ошибки классификации типов рака связаны с конфаундерами из того же медицинского центра, а пространства вложений ФМ организованы больше по медицинским центрам, чем по биологическим факторам.'}, 'en': {'title': 'Ensuring Robustness in Pathology Models for Clinical Use', 'desc': "This paper discusses the evaluation of Pathology Foundation Models (FMs) to ensure they are reliable for clinical use. It introduces a new metric called the Robustness Index, which measures how much these models focus on biological features versus confounding factors from different medical centers. The study finds that most current pathology FMs are heavily influenced by the medical center's characteristics rather than the actual biological data. The authors emphasize the need for models that prioritize biological features to improve classification accuracy and support clinical adoption."}, 'zh': {'title': '确保病理模型的鲁棒性，助力临床应用', 'desc': '病理基础模型（FMs）在医疗保健中具有很大潜力，但在临床应用之前，必须确保它们对不同医疗中心的变化具有鲁棒性。我们提出了鲁棒性指数，这是一种新颖的鲁棒性度量，反映生物特征在多大程度上主导了混淆特征。研究发现，当前的病理基础模型在很大程度上代表了医疗中心，只有一个模型的鲁棒性指数大于一，表明生物特征略微主导混淆特征。通过定量方法分析医疗中心差异对模型预测性能的影响，发现癌症类型分类错误与同中心混淆因素密切相关。'}}}, {'id': 'https://huggingface.co/papers/2502.00314', 'title': 'A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation', 'url': 'https://huggingface.co/papers/2502.00314', 'abstract': "The retroperitoneum hosts a variety of tumors, including rare benign and malignant types, which pose diagnostic and treatment challenges due to their infrequency and proximity to vital structures. Estimating tumor volume is difficult due to their irregular shapes, and manual segmentation is time-consuming. Automatic segmentation using U-Net and its variants, incorporating Vision Transformer (ViT) elements, has shown promising results but struggles with high computational demands. To address this, architectures like the Mamba State Space Model (SSM) and Extended Long-Short Term Memory (xLSTM) offer efficient solutions by handling long-range dependencies with lower resource consumption. This study evaluates U-Net enhancements, including CNN, ViT, Mamba, and xLSTM, on a new in-house CT dataset and a public organ segmentation dataset. The proposed ViLU-Net model integrates Vi-blocks for improved segmentation. Results highlight xLSTM's efficiency in the U-Net framework. The code is publicly accessible on GitHub.", 'score': 0, 'issue_id': 2023, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 февраля', 'en': 'February 1', 'zh': '2月1日'}, 'hash': '51ad114da14800b7', 'authors': ['Moein Heidari', 'Ehsan Khodapanah Aghdam', 'Alexander Manzella', 'Daniel Hsu', 'Rebecca Scalabrino', 'Wenjin Chen', 'David J. Foran', 'Ilker Hacihaliloglu'], 'affiliations': ['Beth Israel Deaconess Medical Center, Boston, MA, United States', 'Center for Biomedical Imaging and Informatics, Rutgers Cancer Institute, New Brunswick, NJ, United States', 'Department of Medicine, University of British Columbia, British Columbia, Canada', 'Department of Radiology, University of British Columbia, British Columbia, Canada', 'Harvard Medical School, Boston, MA, United States', 'Independent Researcher, Tabriz, Iran', 'Memorial Sloan Kettering Cancer Center, New York, NY, United States', 'Rutgers Robert Wood Johnson Medical School, New Brunswick, NJ, United States', 'School of Biomedical Engineering, University of British Columbia, British Columbia, Canada', 'Weill Cornell Medical School, New York, NY, United States'], 'pdf_title_img': 'assets/pdf/title_img/2502.00314.jpg', 'data': {'categories': ['#architecture', '#cv', '#training', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Эффективная сегментация опухолей с помощью усовершенствованных нейросетей', 'desc': 'Статья посвящена автоматической сегментации опухолей забрюшинного пространства с использованием различных архитектур глубокого обучения. Авторы сравнивают эффективность U-Net и его модификаций, включая CNN, ViT, Mamba SSM и xLSTM, на новом наборе данных КТ. Предложенная модель ViLU-Net интегрирует Vi-блоки для улучшения сегментации. Результаты показывают, что xLSTM обеспечивает наиболее эффективную работу в рамках архитектуры U-Net.'}, 'en': {'title': 'Efficient Tumor Segmentation with ViLU-Net: Merging U-Net and Vision Transformers', 'desc': "This paper addresses the challenges of segmenting tumors in the retroperitoneum, which can be irregularly shaped and difficult to analyze. It explores the use of advanced machine learning models, particularly U-Net and its enhancements, to automate the segmentation process. The study introduces the ViLU-Net model, which incorporates Vision Transformer blocks to improve segmentation accuracy while maintaining computational efficiency. Results indicate that the xLSTM architecture significantly enhances the U-Net framework's performance, making it a promising approach for medical image analysis."}, 'zh': {'title': '高效肿瘤分割：ViLU-Net的创新应用', 'desc': '本研究探讨了在后腹膜肿瘤的自动分割中使用U-Net及其变体的有效性。这些肿瘤形状不规则，手动分割耗时且困难，因此需要更高效的自动化方法。研究中引入了Mamba状态空间模型和扩展长短期记忆（xLSTM）等架构，以降低计算资源消耗并处理长距离依赖。最终提出的ViLU-Net模型通过集成Vi-blocks，显著提高了分割效果，xLSTM在U-Net框架中的效率表现尤为突出。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi (1)', '#alignment (2)', '#architecture (6)', '#audio', '#benchmark (8)', '#cv (2)', '#data', '#dataset (7)', '#diffusion (3)', '#ethics (2)', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (5)', '#interpretability (2)', '#leakage (1)', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (5)', '#open_source (5)', '#optimization (8)', '#plp', '#rag (2)', '#reasoning (7)', '#rl (3)', '#rlhf (2)', '#robotics', '#science', '#security (2)', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (12)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-04 14:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-04 14:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-04 14:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    