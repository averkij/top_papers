
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 20 papers. February 4.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ</span> | <span id="title-articles-count">20 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-03.html">â¬…ï¸ <span id="prev-date">03.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-05.html">â¡ï¸ <span id="next-date">05.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '4 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 4', 'zh': '2æœˆ4æ—¥'};
        let feedDateNext = {'ru': '05.02', 'en': '02/05', 'zh': '2æœˆ5æ—¥'};
        let feedDatePrev = {'ru': '03.02', 'en': '02/03', 'zh': '2æœˆ3æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.01237', 'title': 'The Differences Between Direct Alignment Algorithms are a Blur', 'url': 'https://huggingface.co/papers/2502.01237', 'abstract': 'Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the rewards used in those losses (e.g., likelihood ratios of policy and reference policy, or odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required (two-stage vs. one-stage). We first show that one-stage methods underperform two-stage methods. To address this, we incorporate an explicit SFT phase and introduce the beta parameter, controlling the strength of preference optimization, into single-stage ORPO and ASFT. These modifications improve their performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT), matching two-stage methods like DPO. Further analysis reveals that the key factor is whether the approach uses pairwise or pointwise objectives, rather than the specific implicit reward or loss function. These results highlight the importance of careful evaluation to avoid premature claims of performance gains or overall superiority in alignment algorithms.', 'score': 42, 'issue_id': 2022, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '18ba45e237fff5e1', 'authors': ['Alexey Gorbatovski', 'Boris Shaposhnikov', 'Viacheslav Sinii', 'Alexey Malakhov', 'Daniil Gavrilov'], 'affiliations': ['T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.01237.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ (DAA) ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ DAA Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ñƒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ, Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ…, Ğ° Ğ½Ğµ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Simplifying Language Model Alignment with Direct Optimization', 'desc': 'This paper introduces Direct Alignment Algorithms (DAAs) as a simpler alternative to traditional Reinforcement Learning from Human Feedback (RLHF) methods. DAAs optimize language model alignment directly, avoiding the complexities of reinforcement learning and reward modeling. The study compares one-stage and two-stage methods, revealing that incorporating a Supervised Fine-Tuning (SFT) phase significantly enhances performance. The findings emphasize the importance of the choice between pairwise and pointwise ranking losses in achieving effective alignment, cautioning against hasty conclusions about algorithm superiority.'}, 'zh': {'title': 'ä¼˜åŒ–å¯¹é½ç®—æ³•ï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼', 'desc': 'ç›´æ¥å¯¹é½ç®—æ³•ï¼ˆDAAsï¼‰é€šè¿‡ç›´æ¥ä¼˜åŒ–ç­–ç•¥æ¥ç®€åŒ–è¯­è¨€æ¨¡å‹çš„å¯¹é½ï¼Œå–ä»£äº†äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ä¸­çš„å¼ºåŒ–å­¦ä¹ å’Œå¥–åŠ±å»ºæ¨¡ã€‚DAAså¯ä»¥æ ¹æ®å…¶æ’åæŸå¤±ï¼ˆæˆå¯¹ä¸ç‚¹å¯¹ï¼‰å’Œä½¿ç”¨çš„å¥–åŠ±ç±»å‹è¿›è¡Œåˆ†ç±»ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸€é˜¶æ®µæ–¹æ³•çš„è¡¨ç°ä¸å¦‚ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†æ˜¾å¼çš„ç›‘ç£å¾®è°ƒé˜¶æ®µï¼Œå¹¶åœ¨å•é˜¶æ®µçš„ORPOå’ŒASFTä¸­åŠ å…¥äº†æ§åˆ¶åå¥½ä¼˜åŒ–å¼ºåº¦çš„betaå‚æ•°ã€‚è¿™äº›æ”¹è¿›ä½¿å¾—å®ƒä»¬åœ¨Alpaca Eval 2ä¸­çš„è¡¨ç°å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œè¡¨æ˜é€‰æ‹©æˆå¯¹æˆ–ç‚¹å¯¹ç›®æ ‡æ˜¯å…³é”®å› ç´ ï¼Œè€Œä¸æ˜¯å…·ä½“çš„éšå¼å¥–åŠ±æˆ–æŸå¤±å‡½æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01061', 'title': 'OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models', 'url': 'https://huggingface.co/papers/2502.01061', 'abstract': 'End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)', 'score': 34, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '56b819a66e336562', 'authors': ['Gaojie Lin', 'Jianwen Jiang', 'Jiaqi Yang', 'Zerong Zheng', 'Chao Liang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.01061.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#video', '#training', '#diffusion'], 'emoji': 'ğŸ­', 'ru': {'title': 'OmniHuman: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸', 'desc': 'OmniHuman - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Diffusion Transformer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ², Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ñ‹ Ñ‚ĞµĞ»Ğ°. OmniHuman Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'OmniHuman: Revolutionizing Realistic Human Animation Generation', 'desc': 'The paper presents OmniHuman, a new framework for generating realistic human animations from audio inputs. It utilizes a Diffusion Transformer architecture that enhances the training process by incorporating motion-related conditions, allowing for better scalability in video generation. OmniHuman is designed to handle various types of human portraits and interactions, producing high-quality videos that can depict talking, singing, and complex body movements. This approach not only improves the realism of the generated videos but also increases flexibility by supporting multiple input modalities such as audio and video.'}, 'zh': {'title': 'OmniHumanï¼šçµæ´»çœŸå®çš„äººç±»åŠ¨ç”»ç”Ÿæˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºOmniHumançš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡äººç±»åŠ¨ç”»ç”Ÿæˆçš„è´¨é‡å’Œçµæ´»æ€§ã€‚è¯¥æ¡†æ¶åŸºäºæ‰©æ•£å˜æ¢å™¨ï¼Œé€šè¿‡åœ¨è®­ç»ƒé˜¶æ®µæ··åˆä¸è¿åŠ¨ç›¸å…³çš„æ¡ä»¶æ¥æ‰©å±•æ•°æ®è§„æ¨¡ã€‚OmniHumanæ”¯æŒå¤šç§äººåƒå†…å®¹å’Œä¸åŒçš„é©±åŠ¨æ¨¡å¼ï¼Œå¦‚éŸ³é¢‘é©±åŠ¨å’Œè§†é¢‘é©±åŠ¨ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜åº¦çœŸå®çš„äººç±»è§†é¢‘ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOmniHumanä¸ä»…ç”Ÿæˆæ›´çœŸå®çš„è§†é¢‘ï¼Œè¿˜æä¾›äº†æ›´å¤§çš„è¾“å…¥çµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01456', 'title': 'Process Reinforcement through Implicit Rewards', 'url': 'https://huggingface.co/papers/2502.01456', 'abstract': "Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phrase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME's effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.", 'score': 33, 'issue_id': 2019, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '9d62c40e4bafac91', 'authors': ['Ganqu Cui', 'Lifan Yuan', 'Zefan Wang', 'Hanbin Wang', 'Wendi Li', 'Bingxiang He', 'Yuchen Fan', 'Tianyu Yu', 'Qixin Xu', 'Weize Chen', 'Jiarui Yuan', 'Huayu Chen', 'Kaiyan Zhang', 'Xingtai Lv', 'Shuo Wang', 'Yuan Yao', 'Xu Han', 'Hao Peng', 'Yu Cheng', 'Zhiyuan Liu', 'Maosong Sun', 'Bowen Zhou', 'Ning Ding'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Shanghai Jiaotong University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2502.01456.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'PRIME: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ PRIME. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¼ĞµÑ‚ĞºĞ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². PRIME Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ·Ğ»Ğ¾Ğ¼Ñƒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ±Ğ¾Ñ€Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking LLM Potential with PRIME: Efficient Training through Implicit Rewards', 'desc': 'This paper introduces PRIME, a method that enhances the training of large language models (LLMs) using dense process rewards instead of traditional sparse outcome rewards. Dense rewards help improve training efficiency and address credit assignment issues, but collecting high-quality process labels has been a challenge. PRIME allows for online updates of process reward models using only policy rollouts and outcome labels, which reduces the need for extensive reward model training. The results show that PRIME significantly improves reasoning performance in tasks like math and coding, achieving better results with less training data compared to existing models.'}, 'zh': {'title': 'PRIMEï¼šæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•PRIMEï¼ˆé€šè¿‡éšå¼å¥–åŠ±è¿›è¡Œè¿‡ç¨‹å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸­çš„è®­ç»ƒæ•ˆç‡é—®é¢˜ã€‚ä¼ ç»Ÿçš„ç¨€ç–ç»“æœå¥–åŠ±åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œä¿¡ç”¨åˆ†é…ç­‰é—®é¢˜ï¼Œè€ŒPRIMEé€šè¿‡ä»…ä½¿ç”¨ç­–ç•¥å›æ»šå’Œç»“æœæ ‡ç­¾æ¥å®ç°åœ¨çº¿è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„æ›´æ–°ã€‚è¯¥æ–¹æ³•é¿å…äº†ç°æœ‰æ–¹æ³•ä¸­éœ€è¦çš„ä¸“é—¨å¥–åŠ±æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œä»è€Œæ˜¾è‘—é™ä½äº†å¼€å‘æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRIMEåœ¨æ•°å­¦å’Œç¼–ç ç«èµ›ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01534', 'title': 'Preference Leakage: A Contamination Problem in LLM-as-a-judge', 'url': 'https://huggingface.co/papers/2502.01534', 'abstract': 'Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.', 'score': 17, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': 'd2508b2b8b82b41b', 'authors': ['Dawei Li', 'Renliang Sun', 'Yue Huang', 'Ming Zhong', 'Bohan Jiang', 'Jiawei Han', 'Xiangliang Zhang', 'Wei Wang', 'Huan Liu'], 'affiliations': ['Arizona State University', 'University of California, Los Angeles', 'University of Illinois Urbana Champaign', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2502.01534.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#training', '#dataset', '#leakage'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'ĞÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾: LLM-ÑÑƒĞ´ÑŒĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ñ‹!', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ 'ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹' Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ LLM-Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ ÑÑƒĞ´ĞµĞ¹ Ğº ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ Ğ½Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°Ğ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒÑ‚ĞµÑ‡ĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¾Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ĞµĞ¹."}, 'en': {'title': 'Uncovering Preference Leakage: A Hidden Bias in LLM Evaluation', 'desc': 'This paper discusses a problem called preference leakage in the context of using Large Language Models (LLMs) as judges for data annotation. Preference leakage occurs when the relationship between the data generators and the evaluators leads to biased evaluations, particularly when they are similar or related models. The authors identify three types of relatedness that can cause this issue and demonstrate through experiments that judges show bias towards their related models. The findings highlight that preference leakage is a significant and often unnoticed challenge in LLM-based model development.'}, 'zh': {'title': 'åå¥½æ³„æ¼ï¼šLLMè¯„åˆ¤ä¸­çš„éšæ‚£', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„åˆ¤è€…å’ŒåŸºäºLLMçš„æ•°æ®åˆæˆåœ¨æ¨¡å‹å¼€å‘ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æ­ç¤ºäº†åå¥½æ³„æ¼è¿™ä¸€é—®é¢˜ï¼Œå®ƒæ˜¯ç”±åˆæˆæ•°æ®ç”Ÿæˆå™¨ä¸LLMè¯„ä¼°è€…ä¹‹é—´çš„ç›¸å…³æ€§å¼•èµ·çš„ã€‚é€šè¿‡å®šä¹‰ä¸‰ç§å¸¸è§çš„ç›¸å…³æ€§ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯å®äº†è¯„åˆ¤è€…å¯¹å…¶ç›¸å…³å­¦ç”Ÿæ¨¡å‹çš„åè§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåå¥½æ³„æ¼æ˜¯ä¸€ä¸ªæ™®éå­˜åœ¨ä¸”éš¾ä»¥æ£€æµ‹çš„é—®é¢˜ï¼Œå½±å“äº†LLMä½œä¸ºè¯„åˆ¤è€…çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2501.18636', 'title': 'SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model', 'url': 'https://huggingface.co/papers/2501.18636', 'abstract': 'The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.', 'score': 11, 'issue_id': 2023, 'pub_date': '2025-01-28', 'pub_date_card': {'ru': '28 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 28', 'zh': '1æœˆ28æ—¥'}, 'hash': '3a201d426049658a', 'authors': ['Xun Liang', 'Simin Niu', 'Zhiyu Li', 'Sensen Zhang', 'Hanyu Wang', 'Feiyu Xiong', 'Jason Zhaoxin Fan', 'Bo Tang', 'Shichao Song', 'Mengwei Wang', 'Jiawei Yang'], 'affiliations': ['Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, School of Artificial Intelligence, Beihang University, Beijing, China', 'Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.18636.jpg', 'data': {'categories': ['#rag', '#security', '#dataset', '#benchmark'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'SafeRAG: Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SafeRAG Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° RAG Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RAG ÑƒÑĞ·Ğ²Ğ¸Ğ¼ ĞºĞ¾ Ğ²ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ°Ñ‚Ğ°Ğº, Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğ¼ Ğ¾Ñ‡ĞµĞ²Ğ¸Ğ´Ğ½Ñ‹Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ RAG Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'Strengthening RAG: Evaluating Vulnerabilities in Knowledge Integration', 'desc': 'This paper addresses the security vulnerabilities of retrieval-augmented generation (RAG) systems, which combine external knowledge with large language models (LLMs) for knowledge-intensive tasks. The authors introduce a benchmark called SafeRAG to evaluate the security of RAG by classifying various attack types that can manipulate knowledge. They create a dataset specifically for testing these vulnerabilities and simulate different attack scenarios to assess the impact on RAG performance. The results show that RAG systems are significantly susceptible to these attacks, leading to a decline in service quality, highlighting the need for improved security measures.'}, 'zh': {'title': 'æå‡RAGå®‰å…¨æ€§ï¼ŒæŠµå¾¡çŸ¥è¯†æ”»å‡»ï¼', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSafeRAGçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å‹çš„å®‰å…¨æ€§ã€‚æˆ‘ä»¬å°†æ”»å‡»ä»»åŠ¡åˆ†ä¸ºé“¶å™ªå£°ã€ä¸Šä¸‹æ–‡å†²çªã€è½¯å¹¿å‘Šå’Œæ‹’ç»æœåŠ¡ç­‰ç±»å‹ï¼Œå¹¶ä¸ºæ¯ç§ä»»åŠ¡æ‰‹åŠ¨æ„å»ºäº†å®‰å…¨è¯„ä¼°æ•°æ®é›†ã€‚é€šè¿‡ä½¿ç”¨SafeRAGæ•°æ®é›†ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿäº†RAGå¯èƒ½é‡åˆ°çš„å„ç§æ”»å‡»åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAGå¯¹æ‰€æœ‰æ”»å‡»ä»»åŠ¡è¡¨ç°å‡ºæ˜¾è‘—çš„è„†å¼±æ€§ï¼Œç”šè‡³æœ€æ˜æ˜¾çš„æ”»å‡»ä»»åŠ¡ä¹Ÿèƒ½è½»æ˜“ç»•è¿‡ç°æœ‰çš„æ£€ç´¢å™¨å’Œè¿‡æ»¤å™¨ï¼Œå¯¼è‡´RAGæœåŠ¡è´¨é‡ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01068', 'title': 'FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation', 'url': 'https://huggingface.co/papers/2502.01068', 'abstract': 'While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00times and 1.40times improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV.', 'score': 9, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '58ab72f123d7a4b6', 'authors': ['Dongwon Jo', 'Jiwon Song', 'Yulhwa Kim', 'Jae-Joon Kim'], 'affiliations': ['Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea', 'Department of Semiconductor Systems Engineering, Sungkyunkwan University, Suwon, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.01068.jpg', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'FastKV: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FastKV - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ (KV) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. FastKV Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Token-Selective Propagation (TSP), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… LLM Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ğ°ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞ»Ğ¾ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºÑÑˆĞ° KV Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ grouped-query attention (GQA) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FastKV Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'FastKV: Speeding Up Long-Context Processing in LLMs', 'desc': 'This paper presents FastKV, a new method for compressing key-value (KV) caches in large language models (LLMs) to improve computational efficiency and reduce latency. FastKV uses a Token-Selective Propagation (TSP) strategy that keeps full context information in the early layers of the model while selectively passing on only part of this information in the deeper layers. Additionally, it employs grouped-query attention (GQA) to enhance both memory usage and processing speed. Experimental results demonstrate that FastKV significantly improves time-to-first-token and throughput while maintaining accuracy on long-context tasks.'}, 'zh': {'title': 'FastKVï¼šæå‡é•¿ä¸Šä¸‹æ–‡å¤„ç†é€Ÿåº¦çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFastKVçš„KVç¼“å­˜å‹ç¼©æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é•¿ä¸Šä¸‹æ–‡åºåˆ—çš„å¤„ç†é€Ÿåº¦ã€‚FastKVé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„é€‰æ‹©æ€§ä¼ æ’­æ–¹æ³•ï¼ˆTSPï¼‰ï¼Œåœ¨LLMçš„åˆå§‹å±‚ä¿ç•™å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œåœ¨æ›´æ·±å±‚æ¬¡ä¸­ä»…é€‰æ‹©æ€§ä¼ æ’­éƒ¨åˆ†ä¿¡æ¯ã€‚è¯¥æ–¹æ³•è¿˜ç»“åˆäº†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰æ¥ä¼˜åŒ–å†…å­˜å’Œè®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFastKVåœ¨é¦–æ¬¡ä»¤ç‰Œæ—¶é—´å’Œååé‡æ–¹é¢åˆ†åˆ«æ¯”ç°æœ‰çš„HeadKVæ–¹æ³•æé«˜äº†2.00å€å’Œ1.40å€ï¼ŒåŒæ—¶åœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ä¸åŸºçº¿ç›¸å½“çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.00094', 'title': 'AIN: The Arabic INclusive Large Multimodal Model', 'url': 'https://huggingface.co/papers/2502.00094', 'abstract': "Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications.", 'score': 9, 'issue_id': 2018, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 31', 'zh': '1æœˆ31æ—¥'}, 'hash': 'e2d63540ee133732', 'authors': ['Ahmed Heakl', 'Sara Ghaboura', 'Omkar Thawkar', 'Fahad Shahbaz Khan', 'Hisham Cholakkal', 'Rao Muhammad Anwer', 'Salman Khan'], 'affiliations': ['Aalto University', 'Australian National University', 'LinkÃ¶ping University', 'Mohamed bin Zayed University of AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.00094.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#low_resource', '#multimodal', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'AIN: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ°Ñ€Ğ°Ğ±Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ AIN - Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 3,6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾-Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. AIN Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ CAMEL-Bench, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¼ 38 Ğ¿Ğ¾Ğ´Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ², 7B-Ğ²ĞµÑ€ÑĞ¸Ñ AIN Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4o Ğ½Ğ° 3,4% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ¿Ğ¾ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼.'}, 'en': {'title': 'Empowering Arabic with Advanced Multimodal AI', 'desc': 'This paper presents AIN, the Arabic Inclusive Multimodal Model, which is designed to enhance the performance of large multimodal models (LMMs) specifically for Arabic and English. AIN utilizes a substantial dataset of 3.6 million high-quality Arabic-English multimodal samples to achieve state-of-the-art results in Arabic language tasks. The model excels across various domains, as evidenced by its performance on the CAMEL-Bench benchmark, where it surpasses GPT-4o in multiple sub-domains. AIN aims to provide advanced generative AI tools for Arabic speakers, addressing the current limitations in Arabic multimodal understanding.'}, 'zh': {'title': 'æ¨åŠ¨é˜¿æ‹‰ä¼¯è¯­å¤šæ¨¡æ€AIçš„è¿›æ­¥', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œé˜¿æ‹‰ä¼¯è¯­çš„ç ”ç©¶ä»ç„¶ç›¸å¯¹æ»åã€‚æˆ‘ä»¬æå‡ºäº†AINæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æå‡é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œåˆ©ç”¨äº†360ä¸‡é«˜è´¨é‡çš„é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­å¤šæ¨¡æ€æ•°æ®æ ·æœ¬ã€‚AINåœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„è§†è§‰ç†è§£å’Œå¤šå›¾åƒç†è§£æ–¹é¢ï¼Œè¶…è¶Šäº†ç°æœ‰çš„GPT-4oæ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„ä¼˜è¶Šæ€§èƒ½ä¸ºé˜¿æ‹‰ä¼¯è¯­ä½¿ç”¨è€…æä¾›äº†å…ˆè¿›çš„å¤šæ¨¡æ€ç”ŸæˆAIå·¥å…·ï¼Œæ¨åŠ¨äº†ç›¸å…³åº”ç”¨çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01142', 'title': 'DeepRAG: Thinking to Retrieval Step by Step for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01142', 'abstract': 'Large Language Models (LLMs) have shown remarkable potential in reasoning while they still suffer from severe factual hallucinations due to timeliness, accuracy, and coverage of parametric knowledge. Meanwhile, integrating reasoning with retrieval-augmented generation (RAG) remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling strategic and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency while improving answer accuracy by 21.99%, demonstrating its effectiveness in optimizing retrieval-augmented reasoning.', 'score': 7, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': 'e26994166d750227', 'authors': ['Xinyan Guan', 'Jiali Zeng', 'Fandong Meng', 'Chunlei Xin', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Le Sun', 'Jie Zhou'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.01142.jpg', 'data': {'categories': ['#optimization', '#hallucinations', '#reasoning', '#rag', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'DeepRAG: ÑƒĞ¼Ğ½Ğ¾Ğµ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ˜Ğ˜', 'desc': 'DeepRAG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DeepRAG Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° 21.99%. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼.'}, 'en': {'title': 'Enhancing Reasoning with Smart Retrieval', 'desc': 'This paper introduces DeepRAG, a new framework that enhances retrieval-augmented reasoning in large language models. It treats the retrieval process as a Markov Decision Process (MDP), allowing for more strategic and adaptive retrieval of information. By breaking down queries into smaller parts, DeepRAG can decide whether to fetch external knowledge or use its built-in reasoning capabilities at each step. The results show that DeepRAG not only makes retrieval more efficient but also increases the accuracy of answers by nearly 22%.'}, 'zh': {'title': 'DeepRAGï¼šä¼˜åŒ–æ£€ç´¢å¢å¼ºæ¨ç†çš„æ–°æ¡†æ¶', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ–¹é¢å±•ç°äº†æ˜¾è‘—çš„æ½œåŠ›ï¼Œä½†ä»ç„¶é¢ä¸´ä¸¥é‡çš„äº‹å®å¹»è§‰é—®é¢˜ï¼Œä¸»è¦ç”±äºå‚æ•°çŸ¥è¯†çš„æ—¶æ•ˆæ€§ã€å‡†ç¡®æ€§å’Œè¦†ç›–ç‡ä¸è¶³ã€‚å°†æ¨ç†ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç»“åˆèµ·æ¥ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦æ˜¯å› ä¸ºä»»åŠ¡åˆ†è§£ä¸æœ‰æ•ˆå’Œå†—ä½™æ£€ç´¢å¯èƒ½å¼•å…¥å™ªå£°ï¼Œé™ä½å“åº”è´¨é‡ã€‚æœ¬æ–‡æå‡ºäº†DeepRAGæ¡†æ¶ï¼Œå°†æ£€ç´¢å¢å¼ºæ¨ç†å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå®ç°äº†æˆ˜ç•¥æ€§å’Œè‡ªé€‚åº”çš„æ£€ç´¢ã€‚å®éªŒè¡¨æ˜ï¼ŒDeepRAGåœ¨æé«˜æ£€ç´¢æ•ˆç‡çš„åŒæ—¶ï¼Œç­”æ¡ˆå‡†ç¡®æ€§æé«˜äº†21.99%ï¼Œè¯æ˜äº†å…¶åœ¨ä¼˜åŒ–æ£€ç´¢å¢å¼ºæ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.00698', 'title': 'MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models', 'url': 'https://huggingface.co/papers/2502.00698', 'abstract': 'IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.   Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide.', 'score': 6, 'issue_id': 2027, 'pub_date': '2025-02-02', 'pub_date_card': {'ru': '2 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 2', 'zh': '2æœˆ2æ—¥'}, 'hash': '960e3460f0ab8e56', 'authors': ['Huanqia Cai', 'Yijun Yang', 'Winston Hu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.00698.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#reasoning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµÑÑ‚ Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº MM-IQ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2,710 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ 8 Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ Ñ‚ĞµÑÑ‚Ğ°Ğ¼ IQ Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹ÑˆĞµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ (27.49% Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 25%). Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ˜Ğ˜ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Bridging the Cognitive Divide in AI with MM-IQ', 'desc': 'This paper introduces MM-IQ, a new evaluation framework designed to measure cognitive abilities in artificial intelligence systems, similar to how IQ tests assess human intelligence. It includes 2,710 test items across 8 reasoning paradigms, providing a comprehensive benchmark for multimodal models. The evaluation of various state-of-the-art models shows that their performance is only slightly better than random guessing, indicating significant limitations in their reasoning capabilities. This finding emphasizes the urgent need for advancements in AI to better replicate human-like reasoning skills.'}, 'zh': {'title': 'MM-IQï¼šè¯„ä¼°å¤šæ¨¡æ€ç³»ç»Ÿçš„è®¤çŸ¥èƒ½åŠ›æ–°æ ‡å‡†', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMM-IQçš„ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºé‡åŒ–å¤šæ¨¡æ€ç³»ç»Ÿä¸­çš„è®¤çŸ¥èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…å«2710ä¸ªç²¾å¿ƒç­–åˆ’çš„æµ‹è¯•é¡¹ç›®ï¼Œæ¶µç›–8ç§ä¸åŒçš„æ¨ç†èŒƒå¼ã€‚é€šè¿‡å¯¹é¢†å…ˆçš„å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¶æ„ï¼Œå…¶è¡¨ç°ä¹Ÿä»…ç•¥é«˜äºéšæœºçŒœæµ‹ã€‚è¿™ä¸ªæ˜¾è‘—çš„æ€§èƒ½å·®è·è¡¨æ˜å½“å‰å¤šæ¨¡æ€ç³»ç»Ÿåœ¨æ¥è¿‘äººç±»åŸºæœ¬æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ï¼Œå¼ºè°ƒäº†éœ€è¦è¿›è¡ŒèŒƒå¼è½¬å˜çš„è¿›æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01208', 'title': 'Almost Surely Safe Alignment of Large Language Models at Inference-Time', 'url': 'https://huggingface.co/papers/2502.01208', 'abstract': "Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses.", 'score': 6, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': 'fdbe8816f1c6476a', 'authors': ['Xiaotong Ji', 'Shyam Sundhar Ramesh', 'Matthieu Zimmer', 'Ilija Bogunovic', 'Jun Wang', 'Haitham Bou Ammar'], 'affiliations': ['Huawei Noahs Ark Lab', 'Imperial College London', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2502.01208.jpg', 'data': {'categories': ['#ethics', '#rlhf', '#inference', '#alignment'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² LLM Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğº ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ InferenceGuard, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'InferenceGuard: Ensuring Safe Responses from LLMs at Inference Time', 'desc': "This paper addresses the issue of biased or unsafe responses from large language models (LLMs) by introducing a new method for aligning their outputs during inference. Instead of retraining the model, the authors propose a constrained Markov decision process that operates in the LLM's latent space to ensure safe response generation. They enhance this process with a safety state that monitors safety constraints, providing formal guarantees of safety. The proposed method, called InferenceGuard, shows improved performance in generating safe and aligned responses compared to existing techniques without altering the model's weights."}, 'zh': {'title': 'æ¨ç†æ—¶å®‰å…¨å¯¹é½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ—¶å¯¹é½æ–¹æ³•ï¼Œæ—¨åœ¨ç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå®‰å…¨çš„å“åº”ã€‚æˆ‘ä»¬å°†å®‰å…¨ç”Ÿæˆæ¨ç†æ—¶å“åº”çš„é—®é¢˜æ¡†æ¶åŒ–ä¸ºä¸€ä¸ªçº¦æŸçš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶åœ¨LLMçš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå¤„ç†ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªå®‰å…¨çŠ¶æ€æ¥è·Ÿè¸ªå®‰å…¨çº¦æŸçš„æ¼”å˜ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨è§£å†³MDPæ—¶æä¾›æ­£å¼çš„å®‰å…¨ä¿è¯ã€‚æˆ‘ä»¬æå‡ºçš„InferenceGuardå®ç°äº†åœ¨ä¸ä¿®æ”¹æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹å®‰å…¨å¯¹é½LLMï¼Œå¹¶åœ¨ç”Ÿæˆå®‰å…¨å’Œå¯¹é½å“åº”æ–¹é¢ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01100', 'title': 'ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning', 'url': 'https://huggingface.co/papers/2502.01100', 'abstract': 'We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty.   Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.', 'score': 6, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '901fd196d7bfe394', 'authors': ['Bill Yuchen Lin', 'Ronan Le Bras', 'Kyle Richardson', 'Ashish Sabharwal', 'Radha Poovendran', 'Peter Clark', 'Yejin Choi'], 'affiliations': ['Allen Institute for AI', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.01100.jpg', 'data': {'categories': ['#reasoning', '#training', '#inference', '#benchmark'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞŸÑ€Ğ¾ĞºĞ»ÑÑ‚Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½ĞµĞ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ZebraLogic, ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ½Ğ° Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ°Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ (CSP). ZebraLogic Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Llama Ğ¸ DeepSeek-R1. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ "Ğ¿Ñ€Ğ¾ĞºĞ»ÑÑ‚Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸".'}, 'en': {'title': 'Unraveling the Limits of Logical Reasoning in Large Language Models', 'desc': "This paper examines how well large language models (LLMs) can perform logical reasoning, especially in complex scenarios where reasoning does not follow a straightforward path. The authors introduce ZebraLogic, a new framework designed to evaluate LLMs on logic grid puzzles that are based on constraint satisfaction problems (CSPs). Through this framework, they discover that as the complexity of the puzzles increases, the accuracy of the models significantly decreases, a challenge they refer to as the 'curse of complexity.' The study also suggests methods to improve reasoning capabilities, such as using advanced sampling techniques and self-verification prompts, while highlighting the limitations of current LLMs in handling complex reasoning tasks."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å¤æ‚æ€§æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é€»è¾‘æ¨ç†èƒ½åŠ›åŠå…¶åœ¨å¤æ‚éå•è°ƒæ¨ç†ä¸­çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ZebraLogicï¼Œä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMåœ¨åŸºäºçº¦æŸæ»¡è¶³é—®é¢˜ï¼ˆCSPsï¼‰çš„é€»è¾‘ç½‘æ ¼è°œé¢˜ä¸Šçš„æ¨ç†è¡¨ç°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œéšç€é—®é¢˜å¤æ‚æ€§çš„å¢åŠ ï¼Œæ¨¡å‹çš„å‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ï¼Œè¿™ä¸€ç°è±¡è¢«ç§°ä¸ºå¤æ‚æ€§è¯…å’’ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†å¢å¼ºé€»è¾‘æ¨ç†çš„ç­–ç•¥ï¼ŒåŒ…æ‹¬æœ€ä½³é‡‡æ ·ã€å›æº¯æœºåˆ¶å’Œè‡ªæˆ‘éªŒè¯æç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01081', 'title': 'The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles', 'url': 'https://huggingface.co/papers/2502.01081', 'abstract': "The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available https://github.com/declare-lab/LLM-PuzzleTest.", 'score': 6, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '7585b424ff041825', 'authors': ['Vernon Y. H. Toh', 'Yew Ken Chia', 'Deepanway Ghosal', 'Soujanya Poria'], 'affiliations': ['Singapore University of Technology and Design (SUTD)'], 'pdf_title_img': 'assets/pdf/title_img/2502.01081.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#agi', '#open_source', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµÑ€Ğ¸Ğ¹ GPT Ğ¸ OpenAI. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ o3 Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ², Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸ĞµÑÑ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ñƒ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Advancing Reasoning in Multimodal AI: A New Era for LLMs', 'desc': "This paper discusses the advancements in Large Language Models (LLMs) with the release of OpenAI's o1 and o3, which show improved reasoning abilities. The o3 model has demonstrated superior problem-solving skills compared to humans on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, the study highlights that these models primarily focus on symbolic reasoning, while human reasoning often involves multimodal inputs like vision and language. The authors emphasize the need for further research into multimodal reasoning capabilities, as the o1 model, despite its high performance, still faces challenges with simple multimodal and algorithmic puzzles."}, 'zh': {'title': 'å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„æ¢ç´¢ä¸æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡æ¢è®¨äº†OpenAIçš„o1å’Œo3æ¨¡å‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å…ˆè¿›æ¨ç†èƒ½åŠ›ã€‚o3åœ¨æŠ½è±¡å’Œæ¨ç†è¯­æ–™åº“ï¼ˆARC-AGIï¼‰ä¸­è¶…è¶Šäº†äººç±»ï¼Œè¡¨ç°å‡ºè‰²ï¼Œä½†è¯¥åŸºå‡†ä»…é™äºç¬¦å·æ¨¡å¼ã€‚äººç±»é€šå¸¸åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­è¿›è¡Œæ¨ç†ï¼Œå› æ­¤éœ€è¦ç ”ç©¶å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„é«˜çº§æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡o1åœ¨æ¨ç†èƒ½åŠ›ä¸Šæœ‰æ‰€æå‡ï¼Œä½†åœ¨ç®€å•çš„å¤šæ¨¡æ€éš¾é¢˜å’Œç®—æ³•éš¾é¢˜ä¸Šä»ç„¶å­˜åœ¨ä¸è¶³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01591', 'title': 'Improving Transformer World Models for Data-Efficient RL', 'url': 'https://huggingface.co/papers/2502.01591', 'abstract': 'We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 67.4% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna with warmup", which trains the policy on real and imaginary data, (b) "nearest neighbor tokenizer" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep.', 'score': 4, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': 'd9195e9417fce419', 'authors': ['Antoine Dedieu', 'Joseph Ortiz', 'Xinghua Lou', 'Carter Wendelken', 'Wolfgang Lehrach', 'J Swaroop Guntupalli', 'Miguel Lazaro-Gredilla', 'Kevin Patrick Murphy'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.01591.jpg', 'data': {'categories': ['#rl', '#architecture', '#benchmark', '#games', '#training', '#reasoning', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² model-based RL: Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Craftax-classic', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Craftax-classic. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¹ SOTA-Ğ¼ĞµÑ‚Ğ¾Ğ´ DreamerV3, Ñ‚Ğ°Ğº Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ CNN Ğ¸ RNN, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹ Ğ¸ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğµ teacher forcing Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°. Ğ­Ñ‚Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ĞµĞ¹ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'Revolutionizing Model-Based RL for Superior Game Performance', 'desc': "This paper introduces a new model-based reinforcement learning (MBRL) approach that excels in the Craftax-classic benchmark, a complex 2D survival game. The proposed algorithm achieves a remarkable reward of 67.4% after just 1 million environment steps, surpassing previous methods like DreamerV3 and even human performance. Key innovations include a novel policy architecture that integrates convolutional neural networks (CNNs) and recurrent neural networks (RNNs), along with enhancements like 'Dyna with warmup' for training on both real and simulated data. Additional techniques such as a nearest neighbor tokenizer for image patches and block teacher forcing for future reasoning further boost the model's efficiency and effectiveness."}, 'zh': {'title': 'åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–°çªç ´ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåœ¨Craftax-classicåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„æœ€ä½³è¡¨ç°ã€‚è¿™æ˜¯ä¸€æ¬¾å¼€æ”¾ä¸–ç•Œçš„2Dç”Ÿå­˜æ¸¸æˆï¼Œè¦æ±‚æ™ºèƒ½ä½“å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€æ·±åº¦æ¢ç´¢èƒ½åŠ›å’Œé•¿æœŸæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„MBRLç®—æ³•åœ¨ä»…1Mç¯å¢ƒæ­¥éª¤åè·å¾—äº†67.4%çš„å¥–åŠ±ï¼Œæ˜¾è‘—è¶…è¶Šäº†DreamerV3çš„53.2%ï¼Œå¹¶é¦–æ¬¡è¶…è¿‡äº†äººç±»è¡¨ç°çš„65.0%ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºä¸€ä¸ªæœ€å…ˆè¿›çš„æ— æ¨¡å‹åŸºçº¿ï¼Œå¹¶ç»“åˆCNNå’ŒRNNçš„æ–°å‹ç­–ç•¥æ¶æ„ï¼Œè¿›ä¸€æ­¥æå‡äº†æ ·æœ¬æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01441', 'title': 'Improved Training Technique for Latent Consistency Models', 'url': 'https://huggingface.co/papers/2502.01441', 'abstract': 'Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the success of scaling consistency training to large-scale datasets, particularly for text-to-image and video generation tasks, is determined by performance in the latent space. In this work, we analyze the statistical differences between pixel and latent spaces, discovering that latent data often contains highly impulsive outliers, which significantly degrade the performance of iCT in the latent space. To address this, we replace Pseudo-Huber losses with Cauchy losses, effectively mitigating the impact of outliers. Additionally, we introduce a diffusion loss at early timesteps and employ optimal transport (OT) coupling to further enhance performance. Lastly, we introduce the adaptive scaling-c scheduler to manage the robust training process and adopt Non-scaling LayerNorm in the architecture to better capture the statistics of the features and reduce outlier impact. With these strategies, we successfully train latent consistency models capable of high-quality sampling with one or two steps, significantly narrowing the performance gap between latent consistency and diffusion models. The implementation is released here: https://github.com/quandao10/sLCT/', 'score': 4, 'issue_id': 2018, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '2ea077ca6fd7397f', 'authors': ['Quan Dao', 'Khanh Doan', 'Di Liu', 'Trung Le', 'Dimitris Metaxas'], 'affiliations': ['Monash University', 'Rutgers University', 'VinAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.01441.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#architecture', '#open_source', '#diffusion', '#video'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑÑ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ iCT. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ĞšĞ¾ÑˆĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞŸÑĞµĞ²Ğ´Ğ¾-Ğ¥ÑƒĞ±ĞµÑ€Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚. Ğ­Ñ‚Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½-Ğ´Ğ²Ğ° ÑˆĞ°Ğ³Ğ°.'}, 'en': {'title': 'Enhancing Latent Consistency Models for High-Quality Generation', 'desc': "This paper introduces advancements in consistency models, a type of generative model that can create high-quality outputs efficiently. The authors focus on improving performance in latent spaces, where data often contains outliers that hinder model effectiveness. By replacing traditional loss functions with Cauchy losses and incorporating diffusion loss, they enhance the model's robustness against these outliers. Additionally, they propose an adaptive scaling-c scheduler and Non-scaling LayerNorm to optimize training, resulting in latent consistency models that perform comparably to diffusion models in generating images and videos."}, 'zh': {'title': 'æå‡ä¸€è‡´æ€§æ¨¡å‹æ€§èƒ½çš„åˆ›æ–°æ–¹æ³•', 'desc': 'ä¸€è‡´æ€§æ¨¡å‹æ˜¯ä¸€ç§æ–°å‹ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å•æ­¥æˆ–å¤šæ­¥ä¸­ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚æœ€è¿‘ï¼Œè¿™äº›æ¨¡å‹åœ¨åƒç´ ç©ºé—´ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ•ˆæœã€‚ç„¶è€Œï¼Œåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œä¸€è‡´æ€§è®­ç»ƒçš„æˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå–å†³äºæ½œåœ¨ç©ºé—´çš„è¡¨ç°ã€‚ä¸ºäº†è§£å†³æ½œåœ¨æ•°æ®ä¸­çš„å¼‚å¸¸å€¼å¯¹æ€§èƒ½çš„å½±å“ï¼Œæœ¬æ–‡æå‡ºäº†ä½¿ç”¨CauchyæŸå¤±æ›¿ä»£ä¼ªHuberæŸå¤±ï¼Œå¹¶å¼•å…¥æ‰©æ•£æŸå¤±å’Œæœ€ä¼˜ä¼ è¾“æ–¹æ³•ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01637', 'title': 'Scaling Embedding Layers in Language Models', 'url': 'https://huggingface.co/papers/2502.01637', 'abstract': 'We propose SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent n-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. During inference, they are precomputed and stored in off-accelerator memory with minimal impact on inference speed. SCONE enables two new scaling strategies: increasing the number of cached n-gram embeddings and scaling the model used to learn them, all while maintaining fixed inference-time FLOPS. We show that scaling both aspects allows SCONE to outperform a 1.9B parameter baseline across diverse corpora, while using only half the inference-time FLOPS.', 'score': 3, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '478a2a0ee08530a8', 'authors': ['Da Yu', 'Edith Cohen', 'Badih Ghazi', 'Yangsibo Huang', 'Pritish Kamath', 'Ravi Kumar', 'Daogao Liu', 'Chiyuan Zhang'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.01637.jpg', 'data': {'categories': ['#architecture', '#inference', '#long_context', '#training', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚', 'desc': 'SCONE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ñ‘Ğ² Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ÑĞ»Ğ¾Ñ. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ñ‡Ğ°ÑÑ‚Ñ‹Ñ… n-Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­Ñ‚Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. SCONE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… n-Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ FLOPS Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ.'}, 'en': {'title': 'Enhancing Language Models with SCONE: Scalable N-gram Embeddings for Better Performance', 'desc': 'SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding) is a novel approach designed to improve the performance of language models as they grow in size. It introduces embeddings for common n-grams while keeping the original vocabulary intact, which helps in providing better contextual representations for input tokens. These n-gram embeddings are learned through a separate model during training and stored in off-accelerator memory to ensure fast inference. By scaling both the number of cached n-gram embeddings and the model that learns them, SCONE achieves superior performance compared to a large baseline model while maintaining efficient inference-time computations.'}, 'zh': {'title': 'SCONEï¼šæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•SCONEï¼ˆå¯æ‰©å±•çš„ä¸Šä¸‹æ–‡åŒ–çš„ç¦»çº¿N-gramåµŒå…¥ï¼‰ï¼Œæ—¨åœ¨é€šè¿‡æ‰©å±•è¾“å…¥åµŒå…¥å±‚æ¥æå‡è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚SCONEåœ¨ä¿æŒåŸæœ‰è¯æ±‡çš„åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ç»„å¸¸è§n-gramçš„åµŒå…¥ï¼Œä»¥æä¾›æ¯ä¸ªè¾“å…¥æ ‡è®°çš„ä¸Šä¸‹æ–‡åŒ–è¡¨ç¤ºã€‚è¿™äº›åµŒå…¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”±ä¸€ä¸ªå•ç‹¬çš„æ¨¡å‹å­¦ä¹ ï¼Œå¹¶åœ¨æ¨ç†æ—¶é¢„å…ˆè®¡ç®—å¹¶å­˜å‚¨åœ¨ç¦»çº¿åŠ é€Ÿå™¨å†…å­˜ä¸­ï¼Œå‡ ä¹ä¸å½±å“æ¨ç†é€Ÿåº¦ã€‚é€šè¿‡å¢åŠ ç¼“å­˜çš„n-gramåµŒå…¥æ•°é‡å’Œæ‰©å±•å­¦ä¹ å®ƒä»¬çš„æ¨¡å‹ï¼ŒSCONEåœ¨å¤šç§è¯­æ–™åº“ä¸Šè¶…è¶Šäº†1.9Bå‚æ•°çš„åŸºçº¿ï¼ŒåŒæ—¶ä»…ä½¿ç”¨ä¸€åŠçš„æ¨ç†æ—¶é—´FLOPSã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01639', 'title': 'SliderSpace: Decomposing the Visual Capabilities of Diffusion Models', 'url': 'https://huggingface.co/papers/2502.01639', 'abstract': "We present SliderSpace, a framework for automatically decomposing the visual capabilities of diffusion models into controllable and human-understandable directions. Unlike existing control methods that require a user to specify attributes for each edit direction individually, SliderSpace discovers multiple interpretable and diverse directions simultaneously from a single text prompt. Each direction is trained as a low-rank adaptor, enabling compositional control and the discovery of surprising possibilities in the model's latent space. Through extensive experiments on state-of-the-art diffusion models, we demonstrate SliderSpace's effectiveness across three applications: concept decomposition, artistic style exploration, and diversity enhancement. Our quantitative evaluation shows that SliderSpace-discovered directions decompose the visual structure of model's knowledge effectively, offering insights into the latent capabilities encoded within diffusion models. User studies further validate that our method produces more diverse and useful variations compared to baselines. Our code, data and trained weights are available at https://sliderspace.baulab.info", 'score': 2, 'issue_id': 2027, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '559003a020b42709', 'authors': ['Rohit Gandikota', 'Zongze Wu', 'Richard Zhang', 'David Bau', 'Eli Shechtman', 'Nick Kolkin'], 'affiliations': ['Adobe Research', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2502.01639.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#dataset', '#open_source', '#multimodal', '#interpretability', '#cv'], 'emoji': 'ğŸšï¸', 'ru': {'title': 'SliderSpace: Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'SliderSpace - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. ĞšĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ SliderSpace Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹.'}, 'en': {'title': 'Unlocking Creativity in Diffusion Models with SliderSpace', 'desc': "SliderSpace is a novel framework designed to break down the visual capabilities of diffusion models into controllable and understandable components. It automatically identifies multiple interpretable directions from a single text prompt, eliminating the need for users to define each edit direction manually. By training each direction as a low-rank adaptor, SliderSpace allows for compositional control and reveals unexpected possibilities within the model's latent space. Extensive experiments show that it effectively enhances concept decomposition, artistic style exploration, and diversity, outperforming existing methods in generating diverse and useful variations."}, 'zh': {'title': 'SliderSpaceï¼šå¯æ§çš„è§†è§‰èƒ½åŠ›åˆ†è§£', 'desc': 'SliderSpaceæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åˆ†è§£æ‰©æ•£æ¨¡å‹çš„è§†è§‰èƒ½åŠ›ï¼Œä½¿å…¶å¯æ§ä¸”æ˜“äºç†è§£ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒSliderSpaceå¯ä»¥ä»å•ä¸ªæ–‡æœ¬æç¤ºä¸­åŒæ—¶å‘ç°å¤šä¸ªå¯è§£é‡Šå’Œå¤šæ ·åŒ–çš„æ–¹å‘ï¼Œè€Œæ— éœ€ç”¨æˆ·é€ä¸ªæŒ‡å®šå±æ€§ã€‚æ¯ä¸ªæ–¹å‘è¢«è®­ç»ƒä¸ºä½ç§©é€‚é…å™¨ï¼Œä»è€Œå®ç°ç»„åˆæ§åˆ¶ï¼Œå¹¶åœ¨æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­å‘ç°æ„æƒ³ä¸åˆ°çš„å¯èƒ½æ€§ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†SliderSpaceåœ¨æ¦‚å¿µåˆ†è§£ã€è‰ºæœ¯é£æ ¼æ¢ç´¢å’Œå¤šæ ·æ€§å¢å¼ºç­‰ä¸‰ä¸ªåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01584', 'title': 'PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models', 'url': 'https://huggingface.co/papers/2502.01584', 'abstract': "Existing benchmarks for frontier models often test specialized, ``PhD-level'' knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot.   Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with ``I give up'' before providing an answer that it knows is wrong. R1 can also be remarkably ``uncertain'' in its output and in rare cases, it does not ``finish thinking,'' which suggests the need for an inference-time technique to ``wrap up'' before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.", 'score': 2, 'issue_id': 2024, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '6cf23c9aeb70961a', 'authors': ['Carolyn Jane Anderson', 'Joydeep Biswas', 'Aleksander Boruch-Gruszecki', 'Federico Cassano', 'Molly Q Feldman', 'Arjun Guha', 'Francesca Lucchetti', 'Zixuan Wu'], 'affiliations': ['Charles University', 'Cursor', 'Northeastern University', 'Oberlin College', 'University of Texas at Austin', 'Wellesley College'], 'pdf_title_img': 'assets/pdf/title_img/2502.01584.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#inference', '#benchmark'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ°Ñ… NPR Sunday Puzzle Challenge. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ÑÑ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ»ĞµĞ³ĞºĞ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ OpenAI o1 Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ» Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ ĞºĞ°Ğ¿Ğ¸Ñ‚ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¸ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ….'}, 'en': {'title': 'Bridging Knowledge Gaps in AI Reasoning with General Knowledge Benchmarks', 'desc': "This paper introduces a new benchmark for evaluating machine learning models, focusing on general knowledge rather than specialized expertise. The benchmark is based on the NPR Sunday Puzzle Challenge, making it accessible yet challenging for both humans and AI models. The authors highlight that existing benchmarks may not reveal certain capability gaps, as demonstrated by OpenAI's model outperforming others in reasoning tasks. Additionally, the study identifies specific failure modes in models, such as premature concessions and uncertainty in outputs, suggesting improvements in inference techniques are necessary."}, 'zh': {'title': 'æŒ‘æˆ˜æ€§ä¸å¯éªŒè¯æ€§çš„å…¨æ–°åŸºå‡†æµ‹è¯•', 'desc': 'ç°æœ‰çš„å‰æ²¿æ¨¡å‹åŸºå‡†æµ‹è¯•é€šå¸¸è€ƒå¯Ÿä¸“ä¸šçš„ã€éš¾ä»¥ç†è§£çš„çŸ¥è¯†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºNPRå‘¨æ—¥è°œé¢˜æŒ‘æˆ˜çš„åŸºå‡†ï¼Œè¦æ±‚ä»…å…·å¤‡ä¸€èˆ¬çŸ¥è¯†ã€‚è¯¥åŸºå‡†å¯¹äººç±»å’Œæ¨¡å‹éƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†æ­£ç¡®ç­”æ¡ˆæ˜“äºéªŒè¯ï¼Œæ¨¡å‹çš„é”™è¯¯ä¹Ÿå®¹æ˜“å‘ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†ç°æœ‰åŸºå‡†ä¸­æœªæ˜¾ç°çš„èƒ½åŠ›å·®è·ï¼ŒOpenAI o1åœ¨æ¨ç†æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åˆ†ææ¨ç†è¾“å‡ºæ­ç¤ºäº†æ–°çš„å¤±è´¥ç±»å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.01636', 'title': 'Lifelong Sequential Knowledge Editing without Model Degradation', 'url': 'https://huggingface.co/papers/2502.01636', 'abstract': 'Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of the original model. We first show that locate-then-edit knowledge editing methods lead to overfitting on the edited facts. We also show that continuous knowledge editing using these methods leads to disproportionate growth in the norm of the edited matrix. We then provide a crucial insight into the inner workings of locate-then-edit methods. We show that norm-growth is a hidden trick employed by these methods that gives larger importance to the output activations produced from the edited layers. With this "importance hacking", the edited layers provide a much larger contributions to the model\'s output. To mitigate these issues, we present ENCORE - Early stopping and Norm-Constrained Robust knowledge Editing. ENCORE controls for overfitting and the disproportionate norm-growth to enable long-term sequential editing, where we are able to perform up to 10,000 sequential edits without loss of downstream performance. ENCORE is also 61% faster than MEMIT and 64% faster than AlphaEdit on Llama3-8B.', 'score': 1, 'issue_id': 2020, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 3', 'zh': '2æœˆ3æ—¥'}, 'hash': '919dd5274b620b3a', 'authors': ['Akshat Gupta', 'Phudish Prateepamornkul', 'Maochuan Lu', 'Ahmed Alaa', 'Thomas Hartvigsen', 'Gopala Anumanchipalli'], 'affiliations': ['SCB DataX', 'University of California, Berkeley', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2502.01636.jpg', 'data': {'categories': ['#training', '#interpretability', '#architecture', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ½Ğ¾Ñ€Ğ¼Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ENCORE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ¾ÑÑ‚ Ğ½Ğ¾Ñ€Ğ¼Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´Ğ¾ 10 000 Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğº Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ENCORE Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'ENCORE: Efficient Knowledge Editing Without Degradation', 'desc': "This paper investigates the challenges of sequential knowledge editing in machine learning models, particularly focusing on the degradation of model performance after numerous edits. It identifies that traditional locate-then-edit methods can lead to overfitting and excessive growth in the norm of the edited parameters. The authors introduce a new method called ENCORE, which employs early stopping and norm constraints to prevent these issues, allowing for effective long-term editing. ENCORE not only maintains the model's performance after 10,000 edits but also operates significantly faster than existing methods."}, 'zh': {'title': 'ENCOREï¼šé«˜æ•ˆçš„çŸ¥è¯†ç¼–è¾‘è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†åœ¨çŸ¥è¯†ç¼–è¾‘ä¸­è¿›è¡Œå¤§è§„æ¨¡é¡ºåºç¼–è¾‘æ—¶æ¨¡å‹æ€§èƒ½ä¸‹é™çš„åŸå› ã€‚æˆ‘ä»¬å‘ç°ï¼Œå®šä½åç¼–è¾‘çš„æ–¹æ³•å®¹æ˜“å¯¼è‡´å¯¹ç¼–è¾‘äº‹å®çš„è¿‡æ‹Ÿåˆï¼Œå¹¶ä¸”è¿ç»­çš„çŸ¥è¯†ç¼–è¾‘ä¼šå¯¼è‡´ç¼–è¾‘çŸ©é˜µçš„èŒƒæ•°ä¸æˆæ¯”ä¾‹åœ°å¢é•¿ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ENCOREæ–¹æ³•ï¼Œé€šè¿‡æ—©åœå’ŒèŒƒæ•°çº¦æŸæ¥æ§åˆ¶è¿‡æ‹Ÿåˆå’ŒèŒƒæ•°å¢é•¿ï¼Œä»è€Œå®ç°é•¿æ—¶é—´çš„é¡ºåºç¼–è¾‘ã€‚ENCOREèƒ½å¤Ÿåœ¨ä¸æŸå¤±ä¸‹æ¸¸æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œå¤šè¾¾10,000æ¬¡çš„é¡ºåºç¼–è¾‘ï¼Œå¹¶ä¸”æ¯”ç°æœ‰æ–¹æ³•æ›´å¿«ã€‚'}}}, {'id': 'https://huggingface.co/papers/2501.18055', 'title': 'Current Pathology Foundation Models are unrobust to Medical Center Differences', 'url': 'https://huggingface.co/papers/2501.18055', 'abstract': 'Pathology Foundation Models (FMs) hold great promise for healthcare. Before they can be used in clinical practice, it is essential to ensure they are robust to variations between medical centers. We measure whether pathology FMs focus on biological features like tissue and cancer type, or on the well known confounding medical center signatures introduced by staining procedure and other differences. We introduce the Robustness Index. This novel robustness metric reflects to what degree biological features dominate confounding features. Ten current publicly available pathology FMs are evaluated. We find that all current pathology foundation models evaluated represent the medical center to a strong degree. Significant differences in the robustness index are observed. Only one model so far has a robustness index greater than one, meaning biological features dominate confounding features, but only slightly. A quantitative approach to measure the influence of medical center differences on FM-based prediction performance is described. We analyze the impact of unrobustness on classification performance of downstream models, and find that cancer-type classification errors are not random, but specifically attributable to same-center confounders: images of other classes from the same medical center. We visualize FM embedding spaces, and find these are more strongly organized by medical centers than by biological factors. As a consequence, the medical center of origin is predicted more accurately than the tissue source and cancer type. The robustness index introduced here is provided with the aim of advancing progress towards clinical adoption of robust and reliable pathology FMs.', 'score': 0, 'issue_id': 2024, 'pub_date': '2025-01-29', 'pub_date_card': {'ru': '29 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 29', 'zh': '1æœˆ29æ—¥'}, 'hash': '444c1f08656649e7', 'authors': ['Edwin D. de Jong', 'Eric Marcus', 'Jonas Teuwen'], 'affiliations': ['Aignostics', 'Antoni van Leeuwenhoek Hospital (AvL)', 'Kaiko', 'The Netherlands Cancer Institute Amsterdam (NKI)'], 'pdf_title_img': 'assets/pdf/title_img/2501.18055.jpg', 'data': {'categories': ['#healthcare', '#ethics', '#security', '#dataset'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞŸÑƒÑ‚ÑŒ Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ² Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ†ĞµĞ½Ñ‚Ñ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ¤Ğœ) Ğ² Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ - Ğ˜Ğ½Ğ´ĞµĞºÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ°Ğ´ Ğ¼ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ†ĞµĞ½Ñ‚Ñ€Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° Ğ´ĞµÑÑÑ‚Ğ¸ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¤Ğœ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµ Ğ¾Ğ½Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ†ĞµĞ½Ñ‚Ñ€Ğ¾Ğ², Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ĞºĞ° ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ ĞºĞ¾Ğ½Ñ„Ğ°ÑƒĞ½Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°, Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¤Ğœ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ¼, Ñ‡ĞµĞ¼ Ğ¿Ğ¾ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼.'}, 'en': {'title': 'Ensuring Robustness in Pathology Models for Clinical Use', 'desc': "This paper discusses the evaluation of Pathology Foundation Models (FMs) to ensure they are reliable for clinical use. It introduces a new metric called the Robustness Index, which measures how much these models focus on biological features versus confounding factors from different medical centers. The study finds that most current pathology FMs are heavily influenced by the medical center's characteristics rather than the actual biological data. The authors emphasize the need for models that prioritize biological features to improve classification accuracy and support clinical adoption."}, 'zh': {'title': 'ç¡®ä¿ç—…ç†æ¨¡å‹çš„é²æ£’æ€§ï¼ŒåŠ©åŠ›ä¸´åºŠåº”ç”¨', 'desc': 'ç—…ç†åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨åŒ»ç–—ä¿å¥ä¸­å…·æœ‰å¾ˆå¤§æ½œåŠ›ï¼Œä½†åœ¨ä¸´åºŠåº”ç”¨ä¹‹å‰ï¼Œå¿…é¡»ç¡®ä¿å®ƒä»¬å¯¹ä¸åŒåŒ»ç–—ä¸­å¿ƒçš„å˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚æˆ‘ä»¬æå‡ºäº†é²æ£’æ€§æŒ‡æ•°ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„é²æ£’æ€§åº¦é‡ï¼Œåæ˜ ç”Ÿç‰©ç‰¹å¾åœ¨å¤šå¤§ç¨‹åº¦ä¸Šä¸»å¯¼äº†æ··æ·†ç‰¹å¾ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„ç—…ç†åŸºç¡€æ¨¡å‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»£è¡¨äº†åŒ»ç–—ä¸­å¿ƒï¼Œåªæœ‰ä¸€ä¸ªæ¨¡å‹çš„é²æ£’æ€§æŒ‡æ•°å¤§äºä¸€ï¼Œè¡¨æ˜ç”Ÿç‰©ç‰¹å¾ç•¥å¾®ä¸»å¯¼æ··æ·†ç‰¹å¾ã€‚é€šè¿‡å®šé‡æ–¹æ³•åˆ†æåŒ»ç–—ä¸­å¿ƒå·®å¼‚å¯¹æ¨¡å‹é¢„æµ‹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°ç™Œç—‡ç±»å‹åˆ†ç±»é”™è¯¯ä¸åŒä¸­å¿ƒæ··æ·†å› ç´ å¯†åˆ‡ç›¸å…³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.00314', 'title': 'A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation', 'url': 'https://huggingface.co/papers/2502.00314', 'abstract': "The retroperitoneum hosts a variety of tumors, including rare benign and malignant types, which pose diagnostic and treatment challenges due to their infrequency and proximity to vital structures. Estimating tumor volume is difficult due to their irregular shapes, and manual segmentation is time-consuming. Automatic segmentation using U-Net and its variants, incorporating Vision Transformer (ViT) elements, has shown promising results but struggles with high computational demands. To address this, architectures like the Mamba State Space Model (SSM) and Extended Long-Short Term Memory (xLSTM) offer efficient solutions by handling long-range dependencies with lower resource consumption. This study evaluates U-Net enhancements, including CNN, ViT, Mamba, and xLSTM, on a new in-house CT dataset and a public organ segmentation dataset. The proposed ViLU-Net model integrates Vi-blocks for improved segmentation. Results highlight xLSTM's efficiency in the U-Net framework. The code is publicly accessible on GitHub.", 'score': 0, 'issue_id': 2023, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 1', 'zh': '2æœˆ1æ—¥'}, 'hash': '51ad114da14800b7', 'authors': ['Moein Heidari', 'Ehsan Khodapanah Aghdam', 'Alexander Manzella', 'Daniel Hsu', 'Rebecca Scalabrino', 'Wenjin Chen', 'David J. Foran', 'Ilker Hacihaliloglu'], 'affiliations': ['Beth Israel Deaconess Medical Center, Boston, MA, United States', 'Center for Biomedical Imaging and Informatics, Rutgers Cancer Institute, New Brunswick, NJ, United States', 'Department of Medicine, University of British Columbia, British Columbia, Canada', 'Department of Radiology, University of British Columbia, British Columbia, Canada', 'Harvard Medical School, Boston, MA, United States', 'Independent Researcher, Tabriz, Iran', 'Memorial Sloan Kettering Cancer Center, New York, NY, United States', 'Rutgers Robert Wood Johnson Medical School, New Brunswick, NJ, United States', 'School of Biomedical Engineering, University of British Columbia, British Columbia, Canada', 'Weill Cornell Medical School, New York, NY, United States'], 'pdf_title_img': 'assets/pdf/title_img/2502.00314.jpg', 'data': {'categories': ['#architecture', '#cv', '#training', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿ÑƒÑ…Ğ¾Ğ»ĞµĞ¹ Ğ·Ğ°Ğ±Ñ€ÑÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ U-Net Ğ¸ ĞµĞ³Ğ¾ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ CNN, ViT, Mamba SSM Ğ¸ xLSTM, Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞšĞ¢. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ViLU-Net Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Vi-Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ xLSTM Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ U-Net.'}, 'en': {'title': 'Efficient Tumor Segmentation with ViLU-Net: Merging U-Net and Vision Transformers', 'desc': "This paper addresses the challenges of segmenting tumors in the retroperitoneum, which can be irregularly shaped and difficult to analyze. It explores the use of advanced machine learning models, particularly U-Net and its enhancements, to automate the segmentation process. The study introduces the ViLU-Net model, which incorporates Vision Transformer blocks to improve segmentation accuracy while maintaining computational efficiency. Results indicate that the xLSTM architecture significantly enhances the U-Net framework's performance, making it a promising approach for medical image analysis."}, 'zh': {'title': 'é«˜æ•ˆè‚¿ç˜¤åˆ†å‰²ï¼šViLU-Netçš„åˆ›æ–°åº”ç”¨', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨åè…¹è†œè‚¿ç˜¤çš„è‡ªåŠ¨åˆ†å‰²ä¸­ä½¿ç”¨U-NetåŠå…¶å˜ä½“çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›è‚¿ç˜¤å½¢çŠ¶ä¸è§„åˆ™ï¼Œæ‰‹åŠ¨åˆ†å‰²è€—æ—¶ä¸”å›°éš¾ï¼Œå› æ­¤éœ€è¦æ›´é«˜æ•ˆçš„è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚ç ”ç©¶ä¸­å¼•å…¥äº†MambaçŠ¶æ€ç©ºé—´æ¨¡å‹å’Œæ‰©å±•é•¿çŸ­æœŸè®°å¿†ï¼ˆxLSTMï¼‰ç­‰æ¶æ„ï¼Œä»¥é™ä½è®¡ç®—èµ„æºæ¶ˆè€—å¹¶å¤„ç†é•¿è·ç¦»ä¾èµ–ã€‚æœ€ç»ˆæå‡ºçš„ViLU-Netæ¨¡å‹é€šè¿‡é›†æˆVi-blocksï¼Œæ˜¾è‘—æé«˜äº†åˆ†å‰²æ•ˆæœï¼ŒxLSTMåœ¨U-Netæ¡†æ¶ä¸­çš„æ•ˆç‡è¡¨ç°å°¤ä¸ºçªå‡ºã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi (1)', '#alignment (2)', '#architecture (6)', '#audio', '#benchmark (8)', '#cv (2)', '#data', '#dataset (7)', '#diffusion (3)', '#ethics (2)', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (5)', '#interpretability (2)', '#leakage (1)', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (5)', '#open_source (5)', '#optimization (8)', '#plp', '#rag (2)', '#reasoning (7)', '#rl (3)', '#rlhf (2)', '#robotics', '#science', '#security (2)', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (12)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-02-04 14:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-04 14:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-04 14:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    