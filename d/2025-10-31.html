
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. October 31.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-30.html">â¬…ï¸ <span id="prev-date">30.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-11-03.html">â¡ï¸ <span id="next-date">03.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'};
        let feedDateNext = {'ru': '03.11', 'en': '11/03', 'zh': '11æœˆ3æ—¥'};
        let feedDatePrev = {'ru': '30.10', 'en': '10/30', 'zh': '10æœˆ30æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.26583', 'title': 'Emu3.5: Native Multimodal Models are World Learners', 'url': 'https://huggingface.co/papers/2510.26583', 'abstract': 'We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research.', 'score': 45, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': 'e453f75d16182f5e', 'authors': ['Yufeng Cui', 'Honghao Chen', 'Haoge Deng', 'Xu Huang', 'Xinghang Li', 'Jirong Liu', 'Yang Liu', 'Zhuoyan Luo', 'Jinsheng Wang', 'Wenxuan Wang', 'Yueze Wang', 'Chengyuan Wang', 'Fan Zhang', 'Yingli Zhao', 'Ting Pan', 'Xianduo Li', 'Zecheng Hao', 'Wenxuan Ma', 'Zhuo Chen', 'Yulong Ao', 'Tiejun Huang', 'Zhongyuan Wang', 'Xinlong Wang'], 'affiliations': ['BAAI'], 'pdf_title_img': 'assets/pdf/title_img/2510.26583.jpg', 'data': {'categories': ['#open_source', '#optimization', '#inference', '#multimodal', '#rl', '#agi', '#cv', '#games'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ñ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Emu3.5 â€” ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ 10 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ñ… Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ”Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ² 20 Ñ€Ğ°Ğ· Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Discrete Diffusion Adaptation (DiDA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Gemini 2.5 Flash.'}, 'en': {'title': 'Emu3.5: Revolutionizing Multimodal Predictions with Speed and Precision', 'desc': 'Emu3.5 is a large-scale multimodal world model that predicts future states using both vision and language inputs. It is trained on a massive dataset of over 10 trillion tokens, allowing it to generate outputs that seamlessly combine visual and textual information. The model employs a novel technique called Discrete Diffusion Adaptation (DiDA) to enhance inference speed, achieving a 20x improvement in efficiency. Emu3.5 demonstrates advanced capabilities in multimodal reasoning, world modeling, and generation tasks, outperforming existing models in various benchmarks.'}, 'zh': {'title': 'Emu3.5ï¼šå¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹çš„æœªæ¥', 'desc': 'Emu3.5æ˜¯ä¸€ç§å¤§è§„æ¨¡çš„å¤šæ¨¡æ€ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯ã€‚å®ƒé€šè¿‡ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ç›®æ ‡ï¼Œåœ¨åŒ…å«è¶…è¿‡10ä¸‡äº¿ä¸ªæ ‡è®°çš„è§†è§‰-è¯­è¨€æ•°æ®é›†ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„é¢„è®­ç»ƒã€‚è¯¥æ¨¡å‹æ”¯æŒäº¤é”™çš„è§†è§‰-è¯­è¨€è¾“å…¥å’Œè¾“å‡ºï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œåè®­ç»ƒï¼Œä»¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒEmu3.5å¼•å…¥äº†ç¦»æ•£æ‰©æ•£é€‚åº”ï¼ˆDiDAï¼‰æŠ€æœ¯ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ï¼Œä½¿æ¯å¼ å›¾åƒçš„æ¨ç†é€Ÿåº¦æé«˜çº¦20å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.15510', 'title': 'Exploring Conditions for Diffusion models in Robotic Control', 'url': 'https://huggingface.co/papers/2510.15510', 'abstract': "ORCA uses learnable task and visual prompts to adapt pre-trained text-to-image diffusion models for robotic control, achieving state-of-the-art performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While pre-trained visual representations have significantly advanced imitation learning, they are often task-agnostic as they remain frozen during policy learning. In this work, we explore leveraging pre-trained text-to-image diffusion models to obtain task-adaptive visual representations for robotic control, without fine-tuning the model itself. However, we find that naively applying textual conditions - a successful strategy in other vision domains - yields minimal or even negative gains in control tasks. We attribute this to the domain gap between the diffusion model's training data and robotic control environments, leading us to argue for conditions that consider the specific, dynamic visual information required for control. To this end, we propose ORCA, which introduces learnable task prompts that adapt to the control environment and visual prompts that capture fine-grained, frame-specific details. Through facilitating task-adaptive representations with our newly devised conditions, our approach achieves state-of-the-art performance on various robotic control benchmarks, significantly surpassing prior methods.", 'score': 34, 'issue_id': 6717, 'pub_date': '2025-10-17', 'pub_date_card': {'ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 17', 'zh': '10æœˆ17æ—¥'}, 'hash': 'd72f6bb74245abe8', 'authors': ['Heeseong Shin', 'Byeongho Heo', 'Dongyoon Han', 'Seungryong Kim', 'Taekyung Kim'], 'affiliations': ['KAIST AI', 'NAVER AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2510.15510.jpg', 'data': {'categories': ['#agents', '#optimization', '#benchmark', '#diffusion', '#cv', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ', 'desc': 'ORCA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… text-to-image Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑÑ€ĞµĞ´Ğ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº ÑÑ€ĞµĞ´Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Adapting Pre-trained Models for Superior Robotic Control', 'desc': 'ORCA is a method that enhances robotic control by using learnable prompts with pre-trained text-to-image diffusion models. Instead of fine-tuning the models, ORCA adapts them to specific tasks by introducing task and visual prompts that focus on the dynamic visual information needed for control. This approach addresses the limitations of traditional methods that apply textual conditions, which often do not translate well to robotic environments. As a result, ORCA achieves state-of-the-art performance on robotic control benchmarks, outperforming previous techniques.'}, 'zh': {'title': 'ORCAï¼šæ™ºèƒ½æœºå™¨äººæ§åˆ¶çš„æ–°æ–¹æ³•', 'desc': 'ORCAæ˜¯ä¸€ç§åˆ©ç”¨å¯å­¦ä¹ çš„ä»»åŠ¡æç¤ºå’Œè§†è§‰æç¤ºæ¥é€‚åº”é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººæ§åˆ¶çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè€Œæ˜¯é€šè¿‡å¼•å…¥ç‰¹å®šçš„åŠ¨æ€è§†è§‰ä¿¡æ¯æ¥è§£å†³ä¼ ç»Ÿæ–¹æ³•åœ¨æ§åˆ¶ä»»åŠ¡ä¸­æ•ˆæœä¸ä½³çš„é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç®€å•åœ°åº”ç”¨æ–‡æœ¬æ¡ä»¶åœ¨æœºå™¨äººæ§åˆ¶ä»»åŠ¡ä¸­æ•ˆæœæœ‰é™ï¼Œå› æ­¤éœ€è¦æ›´ç²¾ç»†çš„æ¡ä»¶è®¾è®¡ã€‚æœ€ç»ˆï¼ŒORCAåœ¨å¤šä¸ªæœºå™¨äººæ§åˆ¶åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26802', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark', 'url': 'https://huggingface.co/papers/2510.26802', 'abstract': 'Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io', 'score': 24, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': 'b84e32b67921db6d', 'authors': ['Ziyu Guo', 'Xinyan Chen', 'Renrui Zhang', 'Ruichuan An', 'Yu Qi', 'Dongzhi Jiang', 'Xiangtai Li', 'Manyuan Zhang', 'Hongsheng Li', 'Pheng-Ann Heng'], 'affiliations': ['CUHK', 'IMIXR', 'MMLab', 'Northeastern University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2510.26802.jpg', 'data': {'categories': ['#video', '#reasoning', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¸: Ğ¾Ğ±ĞµÑ‰Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ğ»Ğ¸, Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Veo-3 Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ¾Ğ»Ğ¸ zero-shot reasoner Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ ĞµÑ‘ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ 12 Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸ embodied Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MME-CoF. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²ÑĞ·ÑĞ¼Ğ¸, ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¼Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ reasoning ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Exploring the Reasoning Limits of Video Generation Models', 'desc': "This paper investigates the reasoning capabilities of advanced video generation models, particularly focusing on Veo-3. The authors assess the model's performance across various reasoning dimensions, such as spatial and temporal logic, using a newly created benchmark called MME-CoF. While the findings show that these models can handle short-term spatial coherence and local dynamics well, they struggle with long-term causal reasoning and abstract logic. Ultimately, the study concludes that while these video models are not yet reliable as independent zero-shot reasoners, they show potential as supportive tools in visual reasoning tasks."}, 'zh': {'title': 'è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ç ”ç©¶', 'desc': 'æœ€è¿‘çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸã€æ—¶é—´ä¸€è‡´çš„è§†é¢‘ï¼Œè¡¨æ˜å®ƒä»¬å¯èƒ½ç¼–ç äº†å¤§é‡çš„ä¸–ç•ŒçŸ¥è¯†ã€‚é™¤äº†ç°å®åˆæˆå¤–ï¼Œè¿™äº›æ¨¡å‹è¿˜è¡¨ç°å‡ºè§†è§‰æ„ŸçŸ¥ã€å»ºæ¨¡å’Œæ“æ§çš„è¡Œä¸ºã€‚æœ¬æ–‡é€šè¿‡å¯¹é¢†å…ˆçš„Veo-3æ¨¡å‹è¿›è¡Œå®è¯ç ”ç©¶ï¼Œè¯„ä¼°å…¶åœ¨12ä¸ªç»´åº¦ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç©ºé—´ã€å‡ ä½•ã€ç‰©ç†ã€æ—¶é—´å’Œå…·èº«é€»è¾‘ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡å½“å‰è§†é¢‘æ¨¡å‹åœ¨çŸ­æœŸç©ºé—´ä¸€è‡´æ€§å’Œå±€éƒ¨åŠ¨æ€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é•¿æœŸå› æœæ¨ç†å’ŒæŠ½è±¡é€»è¾‘æ–¹é¢ä»ç„¶æœ‰é™ï¼Œå°šä¸å…·å¤‡ä½œä¸ºç‹¬ç«‹é›¶-shotæ¨ç†å™¨çš„å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26692', 'title': 'Kimi Linear: An Expressive, Efficient Attention Architecture', 'url': 'https://huggingface.co/papers/2510.26692', 'abstract': 'We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.   We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.   To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.', 'score': 23, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '0b8a017f9816e001', 'authors': ['Kimi Team', 'Yu Zhang', 'Zongyu Lin', 'Xingcheng Yao', 'Jiaxi Hu', 'Fanqing Meng', 'Chengyin Liu', 'Xin Men', 'Songlin Yang', 'Zhiyuan Li', 'Wentao Li', 'Enzhe Lu', 'Weizhou Liu', 'Yanru Chen', 'Weixin Xu', 'Longhui Yu', 'Yejie Wang', 'Yu Fan', 'Longguang Zhong', 'Enming Yuan', 'Dehao Zhang', 'Yizhi Zhang', 'T. Y. Liu', 'Haiming Wang', 'Shengjun Fang', 'Weiran He', 'Shaowei Liu', 'Yiwei Li', 'Jianlin Su', 'Jiezhong Qiu', 'Bo Pang', 'Junjie Yan', 'Zhejun Jiang', 'Weixiao Huang', 'Bohong Yin', 'Jiacheng You', 'Chu Wei', 'Zhengtao Wang', 'Chao Hong', 'Yutian Chen', 'Guanduo Chen', 'Yucheng Wang', 'Huabin Zheng', 'Feng Wang', 'Yibo Liu', 'Mengnan Dong', 'Zheng Zhang', 'Siyuan Pan', 'Wenhao Wu', 'Yuhao Wu', 'Longyu Guan', 'Jiawen Tao', 'Guohong Fu', 'Xinran Xu', 'Yuzhi Wang', 'Guokun Lai', 'Yuxin Wu', 'Xinyu Zhou', 'Zhilin Yang', 'Yulun Du'], 'affiliations': ['MoonshotAI'], 'pdf_title_img': 'assets/pdf/title_img/2510.26692.jpg', 'data': {'categories': ['#open_source', '#architecture', '#optimization', '#long_context', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Kimi Linear Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… â€” Ğ¾Ñ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ´Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ Kimi Delta Attention (KDA) â€” Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ RNN. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ 3B Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ KV-ĞºÑÑˆĞ° Ğ½Ğ° 75% Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² 6 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 1M Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ²ĞµÑĞ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Kimi Linear: Revolutionizing Attention with Efficiency and Performance', 'desc': 'Kimi Linear is a new hybrid linear attention architecture that surpasses traditional full attention methods in various contexts, including short and long sequences, as well as reinforcement learning scenarios. It features Kimi Delta Attention (KDA), which enhances the Gated DeltaNet model with a more precise gating mechanism, optimizing the use of limited RNN memory. The architecture employs a unique chunkwise algorithm that leverages Diagonal-Plus-Low-Rank (DPLR) transition matrices to significantly lower computational costs while adhering to classical delta rules. Experimental results indicate that Kimi Linear not only outperforms full Multi-Head Latent Attention (MLA) but also reduces key-value cache usage and increases decoding speed, making it a highly efficient alternative for attention-based tasks.'}, 'zh': {'title': 'Kimi Linearï¼šè¶…è¶Šå…¨æ³¨æ„åŠ›çš„é«˜æ•ˆæ¶æ„', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºKimi Linearçš„æ··åˆçº¿æ€§æ³¨æ„åŠ›æ¶æ„ï¼Œå®ƒé¦–æ¬¡åœ¨å…¬å¹³æ¯”è¾ƒä¸­è¶…è¶Šäº†å…¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€‚ç”¨äºçŸ­ä¸Šä¸‹æ–‡ã€é•¿ä¸Šä¸‹æ–‡å’Œå¼ºåŒ–å­¦ä¹ ç­‰å¤šç§åœºæ™¯ã€‚å…¶æ ¸å¿ƒæ˜¯Kimi Delta Attentionï¼ˆKDAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¡¨è¾¾èƒ½åŠ›å¼ºçš„çº¿æ€§æ³¨æ„åŠ›æ¨¡å—ï¼Œé€šè¿‡æ›´ç»†ç²’åº¦çš„é—¨æ§æœºåˆ¶ï¼Œæå‡äº†æœ‰é™çŠ¶æ€RNNå†…å­˜çš„ä½¿ç”¨æ•ˆç‡ã€‚æˆ‘ä»¬è®¾è®¡çš„åˆ†å—ç®—æ³•é€šè¿‡ç‰¹æ®Šçš„å¯¹è§’åŠ ä½ç§©ï¼ˆDPLRï¼‰è½¬ç§»çŸ©é˜µå˜ä½“ï¼Œå®ç°äº†é«˜æ•ˆçš„ç¡¬ä»¶åˆ©ç”¨ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒäº†ä¸ç»å…¸å¢é‡è§„åˆ™çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKimi Linearåœ¨ç›¸åŒçš„è®­ç»ƒæ¡ä»¶ä¸‹ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºå…¨å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰ï¼Œå¹¶ä¸”åœ¨å¤„ç†é•¿è¾“å…¥å’Œè¾“å‡ºæ—¶è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26794', 'title': 'The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation', 'url': 'https://huggingface.co/papers/2510.26794', 'abstract': 'Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.', 'score': 21, 'issue_id': 6714, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '930f4b60c73c7768', 'authors': ['Jing Lin', 'Ruisi Wang', 'Junzhe Lu', 'Ziqi Huang', 'Guorui Song', 'Ailing Zeng', 'Xian Liu', 'Chen Wei', 'Wanqi Yin', 'Qingping Sun', 'Zhongang Cai', 'Lei Yang', 'Ziwei Liu'], 'affiliations': ['NVIDIA Research', 'Nanyang Technological University', 'SenseTime Research', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.26794.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#open_source', '#cv', '#multimodal', '#diffusion', '#transfer_learning'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ: Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ·Ğ°Ğ¸Ğ¼ÑÑ‚Ğ²ÑƒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ViMoGen-228K Ğ¸Ğ· 228 Ñ‚Ñ‹ÑÑÑ‡ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ motion capture, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¾Ñ‚ video generation Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ViMoGen Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ diffusion transformer Ñ flow matching Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ conditioning Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MBench Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Bridging Video and Motion: A New Era for 3D Human Motion Generation', 'desc': 'This paper addresses the limitations of current 3D human motion generation (MoGen) models in generalization by leveraging insights from video generation (ViGen). The authors introduce a new dataset, ViMoGen-228K, which contains 228,000 high-quality motion samples that combine motion capture data with semantically rich annotations from web videos. They propose a novel model, ViMoGen, which utilizes a flow-matching-based diffusion transformer to integrate knowledge from both MoGen and ViGen, and also introduce a lighter version, ViMoGen-light, for improved efficiency. Finally, they create MBench, a benchmark for evaluating motion generation quality, demonstrating that their framework significantly enhances performance in various evaluation metrics.'}, 'zh': {'title': 'çŸ¥è¯†è½¬ç§»ï¼Œæå‡ä¸‰ç»´åŠ¨ä½œç”Ÿæˆèƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå°†è§†é¢‘ç”Ÿæˆï¼ˆViGenï¼‰é¢†åŸŸçš„çŸ¥è¯†è½¬ç§»åˆ°ä¸‰ç»´äººç±»åŠ¨ä½œç”Ÿæˆï¼ˆMoGenï¼‰ä¸­ï¼Œä»¥æé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†ViMoGen-228Kï¼ŒåŒ…å«228,000ä¸ªé«˜è´¨é‡åŠ¨ä½œæ ·æœ¬ï¼Œç»“åˆäº†é«˜ä¿çœŸå…‰å­¦åŠ¨ä½œæ•æ‰æ•°æ®å’Œè¯­ä¹‰æ³¨é‡Šçš„åŠ¨ä½œã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ViMoGenï¼Œä¸€ä¸ªåŸºäºæµåŒ¹é…çš„æ‰©æ•£å˜æ¢å™¨ï¼Œé€šè¿‡é—¨æ§å¤šæ¨¡æ€æ¡ä»¶åŒ–æ¥ç»Ÿä¸€MoCapæ•°æ®å’ŒViGenæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†MBenchï¼Œä¸€ä¸ªåˆ†å±‚åŸºå‡†ï¼Œç”¨äºå¯¹åŠ¨ä½œè´¨é‡ã€æç¤ºä¿çœŸåº¦å’Œæ³›åŒ–èƒ½åŠ›è¿›è¡Œç»†è‡´è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¡†æ¶åœ¨è‡ªåŠ¨å’Œäººå·¥è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26800', 'title': 'OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes', 'url': 'https://huggingface.co/papers/2510.26800', 'abstract': 'There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.', 'score': 15, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '5ddc588a37cb2d17', 'authors': ['Yukun Huang', 'Jiwen Yu', 'Yanning Zhou', 'Jianan Wang', 'Xintao Wang', 'Pengfei Wan', 'Xihui Liu'], 'affiliations': ['Astribot', 'Kuaishou Technology', 'Tencent', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.26800.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#3d', '#synthetic', '#games'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞÑ‚ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼ Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ 3D-ÑÑ†ĞµĞ½Ğ°Ğ¼ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OmniX â€” ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 2D generative Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… 2D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… prior'Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ PBR-Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ² Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ cross-modal Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ completion Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ÑÑ 3D-ÑÑ†ĞµĞ½Ñ‹, Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ physically based rendering, Ñ€ĞµĞ»Ğ°Ğ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼Ğ¼ĞµÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²."}, 'en': {'title': 'OmniX: Bridging 2D Generative Models for Realistic 3D Scene Creation', 'desc': "This paper introduces OmniX, a novel framework that enhances panorama-based 2D lifting for generating realistic 3D scenes. It utilizes advanced 2D generative models to perceive and create geometry, textures, and materials suitable for physically based rendering (PBR). Unlike traditional methods that focus solely on visual appearance, OmniX integrates intrinsic properties into the generation process. The authors also present a large-scale synthetic panorama dataset to support various panoramic vision tasks, demonstrating the model's effectiveness in creating immersive virtual environments."}, 'zh': {'title': 'OmniXï¼šå…¨æ™¯è§†è§‰çš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ„å»º3Dåœºæ™¯çš„ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼šç¨‹åºç”Ÿæˆå’Œ2Dæå‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºOmniXçš„ç»Ÿä¸€æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºå¤§çš„2Dç”Ÿæˆæ¨¡å‹æ¥ç”Ÿæˆé€‚åˆç‰©ç†åŸºç¡€æ¸²æŸ“çš„3Dåœºæ™¯ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒOmniXä¸ä»…å…³æ³¨å¤–è§‚ç”Ÿæˆï¼Œè¿˜é‡è§†å‡ ä½•ã€çº¹ç†å’Œææ–™çš„å†…åœ¨å±æ€§æ„ŸçŸ¥ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„åˆæˆå…¨æ™¯æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒOmniXåœ¨å…¨æ™¯è§†è§‰æ„ŸçŸ¥å’Œ3Dåœºæ™¯ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†æ²‰æµ¸å¼è™šæ‹Ÿä¸–ç•Œçš„ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26768', 'title': 'AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions', 'url': 'https://huggingface.co/papers/2510.26768', 'abstract': 'We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. https://amo-bench.github.io/', 'score': 15, 'issue_id': 6715, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '8bfe84b563305313', 'authors': ['Shengnan An', 'Xunliang Cai', 'Xuezhi Cao', 'Xiaoyu Li', 'Yehao Lin', 'Junlin Liu', 'Xinxuan Lv', 'Dan Ma', 'Xuanlin Wang', 'Ziwen Wang', 'Shuang Zhou'], 'affiliations': ['Harbin Institute of Technology', 'Meituan', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2510.26768.jpg', 'data': {'categories': ['#math', '#benchmark', '#reasoning'], 'emoji': 'ğŸ†', 'ru': {'title': 'ĞĞ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ LLM Ğ² Ñ‚ÑƒĞ¿Ğ¸Ğº', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ AMO-Bench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ñ‹ Ğ¸ Ğ²Ñ‹ÑˆĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 50 Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ±ĞµĞ· Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 26 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 52.4% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 40%. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ inference.'}, 'en': {'title': 'Raising the Bar for Mathematical Reasoning in LLMs', 'desc': 'AMO-Bench is a new benchmark designed to evaluate the mathematical reasoning abilities of large language models (LLMs) using challenging problems that meet International Mathematical Olympiad standards. Unlike previous benchmarks that relied on high school math competitions, AMO-Bench features 50 original problems crafted by experts to avoid data memorization issues. Each problem requires only a final answer, allowing for efficient automatic grading. Experimental results show that even the best LLMs struggle with these problems, achieving only 52.4% accuracy, indicating a significant opportunity for improvement in mathematical reasoning capabilities.'}, 'zh': {'title': 'AMO-Benchï¼šæå‡è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†', 'desc': 'æˆ‘ä»¬æå‡ºäº†AMO-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜çº§æ•°å­¦æ¨ç†åŸºå‡†ï¼ŒåŒ…å«50ä¸ªéš¾åº¦è¾¾åˆ°å¥¥æ—åŒ¹å…‹æ°´å¹³æˆ–æ›´é«˜çš„äººä¸ºè®¾è®¡é—®é¢˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–äºé«˜ä¸­æ•°å­¦ç«èµ›æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†ç”±äºæ€§èƒ½é¥±å’Œï¼Œè®¸å¤šç«èµ›å·²ä¸å†æœ‰æ•ˆã€‚AMO-Benché€šè¿‡ç¡®ä¿æ‰€æœ‰é—®é¢˜éƒ½ç»è¿‡ä¸“å®¶äº¤å‰éªŒè¯ï¼Œç¬¦åˆå›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIMOï¼‰çš„éš¾åº¦æ ‡å‡†ï¼Œå¹¶ä¸”æ˜¯å…¨æ–°çš„åŸåˆ›é—®é¢˜ï¼Œæ¥å¼•å…¥æ›´ä¸¥æ ¼çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒAMO-Benchä¸­çš„æ¯ä¸ªé—®é¢˜åªéœ€æä¾›æœ€ç»ˆç­”æ¡ˆï¼Œä¾¿äºè‡ªåŠ¨åŒ–å’Œç¨³å¥çš„è¯„åˆ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26658', 'title': 'The Era of Agentic Organization: Learning to Organize with Language\n  Models', 'url': 'https://huggingface.co/papers/2510.26658', 'abstract': 'We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training.', 'score': 14, 'issue_id': 6716, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': 'ffa85621bc7e2c42', 'authors': ['Zewen Chi', 'Li Dong', 'Qingxiu Dong', 'Yaru Hao', 'Xun Wu', 'Shaohan Huang', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.26658.jpg', 'data': {'categories': ['#rl', '#inference', '#math', '#reasoning', '#agents', '#optimization'], 'emoji': 'ğŸ”€', 'ru': {'title': 'ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ (AsyncThink) Ğ´Ğ»Ñ LLM, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ñ€ĞºĞµÑ€Ğ°Ğ¼Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ. Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ğ´ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ latency Ğ½Ğ° 28% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Collaborative Intelligence with AsyncThink', 'desc': 'This paper introduces a novel approach called asynchronous thinking (AsyncThink) for enhancing the collaborative problem-solving capabilities of AI agents. By structuring the reasoning process into concurrently executable tasks, AsyncThink allows agents to work together more efficiently than traditional methods. The proposed protocol involves an organizer that assigns tasks to worker agents, merges their findings, and generates coherent solutions, all while optimizing the process through reinforcement learning. Experimental results show that AsyncThink not only reduces inference latency by 28% but also improves accuracy in mathematical reasoning, demonstrating its ability to generalize to new tasks without extra training.'}, 'zh': {'title': 'å¼‚æ­¥æ€ç»´ï¼šåä½œè§£å†³å¤æ‚é—®é¢˜çš„æ–°èŒƒå¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„äººå·¥æ™ºèƒ½æ—¶ä»£ï¼Œç§°ä¸ºä»£ç†ç»„ç»‡ï¼Œå¼ºè°ƒæ™ºèƒ½ä½“é€šè¿‡åä½œè§£å†³å¤æ‚é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†å¼‚æ­¥æ€ç»´ï¼ˆAsyncThinkï¼‰ä½œä¸ºä¸€ç§æ–°çš„æ¨ç†èŒƒå¼ï¼Œèƒ½å¤Ÿå°†å†…éƒ¨æ€ç»´è¿‡ç¨‹ç»„ç»‡æˆå¯å¹¶å‘æ‰§è¡Œçš„ç»“æ„ã€‚è¯¥æ€ç»´åè®®å…è®¸ç»„ç»‡è€…åŠ¨æ€åˆ†é…å­æŸ¥è¯¢ç»™å·¥ä½œè€…ï¼Œåˆå¹¶ä¸­é—´çŸ¥è¯†ï¼Œç”Ÿæˆè¿è´¯çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼ŒAsyncThinkåœ¨æ¨ç†å»¶è¿Ÿä¸Šæ¯”å¹¶è¡Œæ€ç»´é™ä½äº†28%ï¼ŒåŒæ—¶åœ¨æ•°å­¦æ¨ç†çš„å‡†ç¡®æ€§ä¸Šæœ‰æ‰€æå‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹æœªè§ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.25992', 'title': 'Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.25992', 'abstract': 'Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical "actions". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model\'s actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.', 'score': 12, 'issue_id': 6713, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': 'ab88bca35e9e2ff1', 'authors': ['Yihe Deng', 'I-Hung Hsu', 'Jun Yan', 'Zifeng Wang', 'Rujun Han', 'Gufeng Zhang', 'Yanfei Chen', 'Wei Wang', 'Tomas Pfister', 'Chen-Yu Lee'], 'affiliations': ['Google Cloud', 'Google Cloud AI Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2510.25992.jpg', 'data': {'categories': ['#optimization', '#agents', '#training', '#small_models', '#reasoning', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ¶Ğ°Ğ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Supervised Reinforcement Learning (SRL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ€ĞµĞ´ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², SRL ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ, ÑĞ»ĞµĞ´ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ reinforcement learning Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering LLMs with Supervised Reinforcement Learning for Better Reasoning', 'desc': 'This paper introduces Supervised Reinforcement Learning (SRL) as a new approach to improve the reasoning capabilities of Large Language Models (LLMs). SRL reformulates problem-solving into generating a sequence of logical actions, allowing the model to engage in internal reasoning before making decisions. By providing smoother rewards based on the similarity to expert actions, SRL enhances learning even when initial attempts are incorrect. The framework not only improves performance on reasoning tasks but also generalizes well to software engineering applications, making it a versatile tool for training LLMs.'}, 'zh': {'title': 'ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼šæå‡å¤šæ­¥æ¨ç†èƒ½åŠ›çš„å…³é”®', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éœ€è¦å¤šæ­¥æ¨ç†çš„é—®é¢˜ä¸Šå¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚é’ˆå¯¹å°è§„æ¨¡å¼€æºæ¨¡å‹ï¼Œå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ­£ç¡®è§£ç­”ç¨€å°‘çš„æƒ…å†µä¸‹æ•ˆæœä¸ä½³ï¼Œè€Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åˆ™å®¹æ˜“é€šè¿‡é€å­—æ¨¡ä»¿å¯¼è‡´è¿‡æ‹Ÿåˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆSRLï¼‰ï¼Œè¯¥æ¡†æ¶å°†é—®é¢˜è§£å†³é‡æ–°å®šä¹‰ä¸ºç”Ÿæˆä¸€ç³»åˆ—é€»è¾‘â€œåŠ¨ä½œâ€ã€‚SRLé€šè¿‡åœ¨æ¯ä¸ªåŠ¨ä½œä¹‹å‰ç”Ÿæˆå†…éƒ¨æ¨ç†ç‹¬ç™½ï¼Œæä¾›åŸºäºæ¨¡å‹åŠ¨ä½œä¸ä¸“å®¶åŠ¨ä½œç›¸ä¼¼åº¦çš„å¹³æ»‘å¥–åŠ±ï¼Œä»è€Œæœ‰æ•ˆæå‡å°æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.25628', 'title': 'EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\n  Health Record Analysis', 'url': 'https://huggingface.co/papers/2510.25628', 'abstract': 'Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis.', 'score': 8, 'issue_id': 6714, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': '68de771aa478bf84', 'authors': ['Yusheng Liao', 'Chaoyi Wu', 'Junwei Liu', 'Shuyang Jiang', 'Pengcheng Qiu', 'Haowen Wang', 'Yun Yue', 'Shuai Zhen', 'Jian Wang', 'Qianrui Fan', 'Jinjie Gu', 'Ya Zhang', 'Yanfeng Wang', 'Yu Wang', 'Weidi Xie'], 'affiliations': ['Fudan University, Shanghai, China', 'Intelligence Computing and Sensing Laboratory, Peking University, Beijing, China', 'Intelligence Healthcare Department, AntGroup, Hangzhou, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.25628.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#healthcare', '#reasoning', '#training', '#science'], 'emoji': 'ğŸ¥', 'ru': {'title': 'EHR-R1: AI-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ĞºĞ°Ñ€Ñ‚', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EHR-R1 â€” ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ LLM Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ĞºĞ°Ñ€Ñ‚ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ EHR-Ins â€” Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 300 Ñ‚Ñ‹ÑÑÑ‡ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¿Ğ¾ 42 Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¼Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ reinforcement learning. EHR-R1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4o Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 30 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MIMIC-Bench Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° 10% Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ AUROC Ğ² zero-shot Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ½Ğ° EHRSHOT.'}, 'en': {'title': 'Revolutionizing EHR Analysis with Enhanced Reasoning Models', 'desc': 'This paper addresses the challenges of analyzing Electronic Health Records (EHRs) using large language models (LLMs). It introduces EHR-Ins, a comprehensive dataset designed for EHR reasoning, which includes 300,000 reasoning cases and 4 million non-reasoning cases across 42 tasks. The authors develop EHR-R1, a series of reasoning-enhanced LLMs that utilize a multi-stage training approach to improve their reasoning capabilities and domain knowledge for EHR analysis. The results demonstrate that EHR-R1 outperforms existing models, providing a significant advancement in the reliability and relevance of EHR analysis in clinical settings.'}, 'zh': {'title': 'æå‡ç”µå­å¥åº·è®°å½•åˆ†æçš„æ™ºèƒ½æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ¨ç†æŒ‡ä»¤æ•°æ®é›†EHR-Insï¼ŒåŒ…å«30ä¸‡ä¸ªé«˜è´¨é‡æ¨ç†æ¡ˆä¾‹å’Œ400ä¸‡ä¸ªéæ¨ç†æ¡ˆä¾‹ï¼Œè¦†ç›–42ä¸ªä¸åŒçš„EHRä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ€ç»´å›¾çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå¤§è§„æ¨¡ç”Ÿæˆé«˜è´¨é‡çš„æ¨ç†æ•°æ®ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†EHR-R1ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—é’ˆå¯¹EHRåˆ†æçš„å¢å¼ºæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‚æ•°é‡é«˜è¾¾720äº¿ã€‚é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒï¼ŒåŒ…æ‹¬é¢†åŸŸé€‚åº”ã€æ¨ç†å¢å¼ºå’Œå¼ºåŒ–å­¦ä¹ ï¼ŒEHR-R1ç³»ç»Ÿæ€§åœ°è·å–äº†é¢†åŸŸçŸ¥è¯†å’Œå¤šæ ·çš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†EHRåˆ†æçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26213', 'title': 'OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal\n  Document Layout Generation', 'url': 'https://huggingface.co/papers/2510.26213', 'abstract': 'Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage Coarse-to-Fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse category definitions, and 2) transferring the knowledge to a specific domain with fine-grained annotations. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M^{6}Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, models, and dataset will be publicly released.', 'score': 5, 'issue_id': 6717, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '63784033fbe5180b', 'authors': ['Hengrui Kang', 'Zhuangcheng Gu', 'Zhiyuan Zhao', 'Zichen Wen', 'Bin Wang', 'Weijia Li', 'Conghui He'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2510.26213.jpg', 'data': {'categories': ['#open_source', '#dataset', '#training', '#transfer_learning', '#data', '#architecture'], 'emoji': 'ğŸ“°', 'ru': {'title': 'OmniLayout: ĞœĞ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ OmniLayout-1M â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑˆĞµÑÑ‚ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ³Ğ°Ğ·ĞµÑ‚ Ğ´Ğ¾ Ğ¶ÑƒÑ€Ğ½Ğ°Ğ»Ğ¾Ğ², Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸. Ğ”Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ OmniLayout-LLM â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 0.5B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ. Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ M^6Doc.'}, 'en': {'title': 'Revolutionizing Document Layout Generation with OmniLayout-1M and OmniLayout-LLM', 'desc': 'This paper addresses the gap in document layout generation by introducing OmniLayout-1M, a large dataset containing a million diverse document layouts from various genres. The authors highlight that most existing research has focused on document layout analysis, neglecting the generative aspect, particularly for complex document types like newspapers and magazines. To tackle the challenges of generating coherent layouts, they propose OmniLayout-LLM, a 0.5 billion parameter model that employs a two-stage Coarse-to-Fine learning approach. Their experiments show that this model outperforms existing layout generation methods and general-purpose language models, demonstrating its effectiveness across multiple document types.'}, 'zh': {'title': 'å¤šæ ·åŒ–æ–‡æ¡£å¸ƒå±€ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†æ–‡æ¡£äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ï¼Œå°¤å…¶æ˜¯æ–‡æ¡£å¸ƒå±€ç”Ÿæˆçš„ç ”ç©¶ã€‚ä¸ºäº†å¡«è¡¥ç°æœ‰ç ”ç©¶ä¸­å¤šæ ·åŒ–å¸ƒå±€çš„ç¼ºå¤±ï¼Œä½œè€…åˆ›å»ºäº†OmniLayout-1Mæ•°æ®é›†ï¼ŒåŒ…å«å…­ç§å¸¸è§æ–‡æ¡£ç±»å‹çš„å¸ƒå±€ã€‚æ–‡ç« è¿˜æå‡ºäº†OmniLayout-LLMæ¨¡å‹ï¼Œé‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„ç²—åˆ°ç»†å­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜åœ¨å¤æ‚é¢†åŸŸä¸­çš„å¸ƒå±€ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé¢†åŸŸçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¸ƒå±€ç”Ÿæˆä¸“å®¶å’Œæœ€æ–°çš„é€šç”¨å¤§è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26298', 'title': 'Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in\n  Web Games', 'url': 'https://huggingface.co/papers/2510.26298', 'abstract': "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlas's web interaction capabilities using browser-based games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird, and Stein.world. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at https://atlas-game-eval.github.io.", 'score': 4, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '17bebfc38acb1c32', 'authors': ['Jingran Zhang', 'Ning Li', 'Justin Cui'], 'affiliations': ['UC San Diego', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2510.26298.jpg', 'data': {'categories': ['#agents', '#multimodal', '#reasoning', '#benchmark', '#games'], 'emoji': 'ğŸ®', 'ru': {'title': 'ĞÑ‚Ğ»Ğ°Ñ Ğ¾Ñ‚ OpenAI: ÑĞ¸Ğ»Ñ‘Ğ½ Ğ² Ğ»Ğ¾Ğ³Ğ¸ĞºĞµ, ÑĞ»Ğ°Ğ± Ğ² Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ChatGPT Atlas, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ¼ĞµĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºÑƒÑ€ÑĞ¾Ñ€ Ğ¸ ĞºĞ»Ğ°Ğ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñƒ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ…: T-Rex Runner, Sudoku, Flappy Bird Ğ¸ Stein.world. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€Ğ¾Ğ´Ğµ ÑÑƒĞ´Ğ¾ĞºÑƒ, Ñ€ĞµÑˆĞ°Ñ Ğ¸Ñ… Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ»ÑĞ´ĞµĞ¹, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ»Ğ°ÑÑŒ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ AI Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¿Ğ¾ĞºĞ° Ğ¸Ğ¼ĞµĞµÑ‚ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'Evaluating Atlas: Strong in Logic, Struggling in Real-Time Interaction', 'desc': "OpenAI's ChatGPT Atlas enhances web interaction by allowing the model to analyze webpages and perform user actions like clicking and typing. This paper evaluates Atlas's performance in browser-based games to understand its capabilities in dynamic environments. The results indicate that Atlas excels in logical reasoning tasks, completing puzzles like Sudoku faster than humans, but struggles with real-time games that require quick reflexes and precise control. These findings highlight the model's strengths in analytical tasks while revealing significant limitations in interactive scenarios that demand immediate responses."}, 'zh': {'title': 'Atlasï¼šç½‘é¡µäº¤äº’çš„æ–°æ¢ç´¢', 'desc': 'OpenAIçš„ChatGPT Atlaså¼•å…¥äº†æ–°çš„ç½‘é¡µäº¤äº’èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ†æç½‘é¡µã€å¤„ç†ç”¨æˆ·æ„å›¾ï¼Œå¹¶ç›´æ¥åœ¨æµè§ˆå™¨ä¸­æ‰§è¡Œå…‰æ ‡å’Œé”®ç›˜è¾“å…¥ã€‚å°½ç®¡å…¶åœ¨ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­çš„èƒ½åŠ›å·²å¾—åˆ°éªŒè¯ï¼Œä½†åœ¨åŠ¨æ€äº¤äº’ç¯å¢ƒä¸­çš„è¡¨ç°ä»ç„¶è¾ƒå°‘è¢«æ¢ç´¢ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨åŸºäºæµè§ˆå™¨çš„æ¸¸æˆï¼ˆå¦‚è°·æ­Œçš„T-Rex Runnerã€æ•°ç‹¬ã€Flappy Birdå’ŒStein.worldï¼‰è¿›è¡Œæ—©æœŸè¯„ä¼°ï¼Œé‡‡ç”¨æ¸¸æˆå†…è¡¨ç°åˆ†æ•°ä½œä¸ºé‡åŒ–æŒ‡æ ‡æ¥è¯„ä¼°ä¸åŒä»»åŠ¡ç±»å‹çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒAtlasåœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°ç‹¬ï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œå®Œæˆéš¾é¢˜çš„é€Ÿåº¦æ˜¾è‘—å¿«äºäººç±»åŸºçº¿ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®æ—¶æœºå’Œè¿åŠ¨æ§åˆ¶çš„å®æ—¶æ¸¸æˆä¸­è¡¨ç°ä¸ä½³ï¼Œå¸¸å¸¸æ— æ³•çªç ´åˆå§‹éšœç¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.25779', 'title': 'Magentic Marketplace: An Open-Source Environment for Studying Agentic\n  Markets', 'url': 'https://huggingface.co/papers/2510.25779', 'abstract': 'As LLM agents advance, they are increasingly mediating economic decisions, ranging from product discovery to transactions, on behalf of users. Such applications promise benefits but also raise many questions about agent accountability and value for users. Addressing these questions requires understanding how agents behave in realistic market conditions. However, previous research has largely evaluated agents in constrained settings, such as single-task marketplaces (e.g., negotiation) or structured two-agent interactions. Real-world markets are fundamentally different: they require agents to handle diverse economic activities and coordinate within large, dynamic ecosystems where multiple agents with opaque behaviors may engage in open-ended dialogues. To bridge this gap, we investigate two-sided agentic marketplaces where Assistant agents represent consumers and Service agents represent competing businesses. To study these interactions safely, we develop Magentic-Marketplace-- a simulated environment where Assistants and Services can operate. This environment enables us to study key market dynamics: the utility agents achieve, behavioral biases, vulnerability to manipulation, and how search mechanisms shape market outcomes. Our experiments show that frontier models can approach optimal welfare-- but only under ideal search conditions. Performance degrades sharply with scale, and all models exhibit severe first-proposal bias, creating 10-30x advantages for response speed over quality. These findings reveal how behaviors emerge across market conditions, informing the design of fair and efficient agentic marketplaces.', 'score': 4, 'issue_id': 6714, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': 'fc91b6d1ec75911d', 'authors': ['Gagan Bansal', 'Wenyue Hua', 'Zezhou Huang', 'Adam Fourney', 'Amanda Swearngin', 'Will Epperson', 'Tyler Payne', 'Jake M. Hofman', 'Brendan Lucier', 'Chinmay Singh', 'Markus Mobius', 'Akshay Nambi', 'Archana Yadav', 'Kevin Gao', 'David M. Rothschild', 'Aleksandrs Slivkins', 'Daniel G. Goldstein', 'Hussein Mozannar', 'Nicole Immorlica', 'Maya Murad', 'Matthew Vogel', 'Subbarao Kambhampati', 'Eric Horvitz', 'Saleema Amershi'], 'affiliations': ['Arizona State University', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2510.25779.jpg', 'data': {'categories': ['#multimodal', '#ethics', '#reasoning', '#agents'], 'emoji': 'ğŸ¤', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ñ‹Ğ½ĞºĞµ: ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ñ€Ñ‹Ğ½ĞºĞ°Ñ…, Ğ³Ğ´Ğµ Ğ¾Ğ´Ğ½Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ĞµĞ¹, Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ â€” ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¸Ğ·Ğ½ĞµÑÑ‹. Ğ”Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Magentic-Marketplace, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ»Ğ°Ğ³Ğ¾ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ½Ğ¾ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ĞºĞ¾ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğº Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ (first-proposal bias), ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ² 10-30 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ°Ğ´ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼.'}, 'en': {'title': 'Navigating the Complexities of Agentic Marketplaces', 'desc': 'This paper explores how large language model (LLM) agents can influence economic decisions in real-world markets, where they act on behalf of users. It highlights the need to understand agent behavior in complex, dynamic environments rather than simplified settings. The authors introduce a simulated environment called Magentic-Marketplace to analyze interactions between consumer-representing Assistants and competing Service agents. Their findings indicate that while advanced models can achieve good outcomes under ideal conditions, they struggle with scale and exhibit biases that can significantly impact market efficiency.'}, 'zh': {'title': 'æ¢ç´¢ä»£ç†å¸‚åœºçš„å…¬å¹³ä¸æ•ˆç‡', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å‘å±•ï¼Œå®ƒä»¬åœ¨ç»æµå†³ç­–ä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ï¼ŒåŒ…æ‹¬äº§å“å‘ç°å’Œäº¤æ˜“ã€‚è¿™äº›åº”ç”¨è™½ç„¶å¸¦æ¥äº†å¥½å¤„ï¼Œä½†ä¹Ÿå¼•å‘äº†å…³äºä»£ç†è´£ä»»å’Œç”¨æˆ·ä»·å€¼çš„è®¸å¤šé—®é¢˜ã€‚ä¸ºäº†ç†è§£ä»£ç†åœ¨ç°å®å¸‚åœºæ¡ä»¶ä¸‹çš„è¡Œä¸ºï¼Œæˆ‘ä»¬å¼€å‘äº†Magentic-Marketplaceï¼Œä¸€ä¸ªæ¨¡æ‹Ÿç¯å¢ƒï¼Œç ”ç©¶æ¶ˆè´¹è€…ä»£ç†å’Œç«äº‰ä¼ä¸šä»£ç†ä¹‹é—´çš„äº’åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å‰æ²¿æ¨¡å‹åœ¨ç†æƒ³æœç´¢æ¡ä»¶ä¸‹å¯ä»¥æ¥è¿‘æœ€ä½³ç¦åˆ©ï¼Œä½†åœ¨è§„æ¨¡æ‰©å¤§æ—¶æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½è¡¨ç°å‡ºä¸¥é‡çš„é¦–æ¬¡ææ¡ˆåè§ï¼Œå¯¼è‡´å“åº”é€Ÿåº¦ç›¸å¯¹äºè´¨é‡æœ‰10-30å€çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26787', 'title': 'Remote Labor Index: Measuring AI Automation of Remote Work', 'url': 'https://huggingface.co/papers/2510.26787', 'abstract': 'AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.', 'score': 2, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '4b733966c35e82f0', 'authors': ['Mantas Mazeika', 'Alice Gatti', 'Cristina Menghini', 'Udari Madhushani Sehwag', 'Shivam Singhal', 'Yury Orlovskiy', 'Steven Basart', 'Manasi Sharma', 'Denis Peskoff', 'Elaine Lau', 'Jaehyuk Lim', 'Lachlan Carroll', 'Alice Blair', 'Vinaya Sivakumar', 'Sumana Basu', 'Brad Kenstler', 'Yuntao Ma', 'Julian Michael', 'Xiaoke Li', 'Oliver Ingebretsen', 'Aditya Mehta', 'Jean Mottola', 'John Teichmann', 'Kevin Yu', 'Zaina Shaik', 'Adam Khoja', 'Richard Ren', 'Jason Hausenloy', 'Long Phan', 'Ye Htet', 'Ankit Aich', 'Tahseen Rabbani', 'Vivswan Shah', 'Andriy Novykov', 'Felix Binder', 'Kirill Chugunov', 'Luis Ramirez', 'Matias Geralnik', 'HernÃ¡n Mesura', 'Dean Lee', 'Ed-Yeremai Hernandez Cardona', 'Annette Diamond', 'Summer Yue', 'Alexandr Wang', 'Bing Liu', 'Ernesto Hernandez', 'Dan Hendrycks'], 'affiliations': ['Center for AI Safety', 'Scale AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.26787.jpg', 'data': {'categories': ['#agents', '#science', '#reasoning', '#benchmark'], 'emoji': 'ğŸ¢', 'ru': {'title': 'Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€ÑƒĞ´Ğ°: AI Ğ¿Ğ¾ĞºĞ° ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ 2.5% Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Remote Labor Index (RLI) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 2.5% Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ AI Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€ÑƒĞ´Ğ°, Ğ¾Ñ‚Ğ´ĞµĞ»ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° AI Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ°Ğ¼ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ñ‚ÑŒÑÑ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ñ€Ñ‹Ğ½ĞºĞµ Ñ‚Ñ€ÑƒĞ´Ğ°.'}, 'en': {'title': "Measuring AI's Real-World Impact on Labor Automation", 'desc': "This paper introduces the Remote Labor Index (RLI), a new benchmark designed to assess the performance of AI agents in real-world economic tasks. The RLI evaluates how well AI can automate valuable projects across various sectors, providing a practical measure of AI's capabilities. The findings reveal that current AI agents perform poorly on the RLI, with the best achieving only a 2.5% automation rate. This research aims to provide empirical evidence for discussions about AI's impact on labor and help stakeholders understand and manage the implications of AI-driven automation."}, 'zh': {'title': 'é‡åŒ–AIè‡ªåŠ¨åŒ–çš„ç»æµä»·å€¼', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æŒ‡æ ‡ï¼Œç§°ä¸ºè¿œç¨‹åŠ³åŠ¨æŒ‡æ•°ï¼ˆRLIï¼‰ï¼Œç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½åœ¨å®é™…ç»æµé¡¹ç›®ä¸­çš„è¡¨ç°ã€‚RLIæ˜¯ä¸€ä¸ªå¤šè¡Œä¸šçš„åŸºå‡†ï¼Œæ—¨åœ¨è¡¡é‡AIä»£ç†åœ¨çœŸå®ä¸–ç•Œä¸­çš„è‡ªåŠ¨åŒ–èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒAIä»£ç†åœ¨RLIä¸Šçš„è¡¨ç°æ¥è¿‘æœ€ä½æ°´å¹³ï¼Œæœ€é«˜çš„è‡ªåŠ¨åŒ–ç‡ä»…ä¸º2.5%ã€‚è¿™äº›ç»“æœä¸ºAIè‡ªåŠ¨åŒ–çš„è®¨è®ºæä¾›äº†å®è¯ä¾æ®ï¼Œå¸®åŠ©åˆ©ç›Šç›¸å…³è€…æ›´å¥½åœ°ç†è§£å’Œåº”å¯¹AIé©±åŠ¨çš„åŠ³åŠ¨è‡ªåŠ¨åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26140', 'title': 'FullPart: Generating each 3D Part at Full Resolution', 'url': 'https://huggingface.co/papers/2510.26140', 'abstract': 'Part-based 3D generation holds great potential for various applications. Previous part generators that represent parts using implicit vector-set tokens often suffer from insufficient geometric details. Another line of work adopts an explicit voxel representation but shares a global voxel grid among all parts; this often causes small parts to occupy too few voxels, leading to degraded quality. In this paper, we propose FullPart, a novel framework that combines both implicit and explicit paradigms. It first derives the bounding box layout through an implicit box vector-set diffusion process, a task that implicit diffusion handles effectively since box tokens contain little geometric detail. Then, it generates detailed parts, each within its own fixed full-resolution voxel grid. Instead of sharing a global low-resolution space, each part in our method - even small ones - is generated at full resolution, enabling the synthesis of intricate details. We further introduce a center-point encoding strategy to address the misalignment issue when exchanging information between parts of different actual sizes, thereby maintaining global coherence. Moreover, to tackle the scarcity of reliable part data, we present PartVerse-XL, the largest human-annotated 3D part dataset to date with 40K objects and 320K parts. Extensive experiments demonstrate that FullPart achieves state-of-the-art results in 3D part generation. We will release all code, data, and model to benefit future research in 3D part generation.', 'score': 2, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '37da8a03be0a8489', 'authors': ['Lihe Ding', 'Shaocong Dong', 'Yaokun Li', 'Chenjian Gao', 'Xiao Chen', 'Rui Han', 'Yihao Kuang', 'Hong Zhang', 'Bo Huang', 'Zhanpeng Huang', 'Zibin Wang', 'Dan Xu', 'Tianfan Xue'], 'affiliations': ['CUHK', 'Chongqing University', 'HKUST', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.26140.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#3d', '#dataset'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞŸĞ¾Ğ»Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹: ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ÑĞ²Ğ¾Ñ‘ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FullPart â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‡Ğ°ÑÑ‚ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ (implicit) Ğ¸ ÑĞ²Ğ½Ñ‹Ğµ (explicit) Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ±Ğ¾ĞºÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸, ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ² ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¾ĞºÑĞµĞ»Ğ½Ğ¾Ğ¹ ÑĞµÑ‚ĞºĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ³Ğ´Ğµ Ğ²ÑĞµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ´ĞµĞ»Ğ¸Ğ»Ğ¸ Ğ¾Ğ´Ğ½Ñƒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞµÑ‚ĞºÑƒ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ PartVerse-XL â€” ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ 3D-Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ñ 40 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ 320 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing 3D Part Generation with FullPart', 'desc': 'This paper introduces FullPart, a new framework for generating 3D parts that combines implicit and explicit methods. It uses an implicit diffusion process to create a bounding box layout, which is effective for low-detail shapes. Each part is then generated in its own high-resolution voxel grid, allowing for detailed and intricate designs without the limitations of shared low-resolution spaces. Additionally, the authors present PartVerse-XL, a large dataset to improve the training of models in 3D part generation, and demonstrate that FullPart achieves top performance in this area.'}, 'zh': {'title': 'FullPartï¼šæå‡3Déƒ¨åˆ†ç”Ÿæˆçš„å…¨æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFullPartçš„æ–°æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›åŸºäºéƒ¨åˆ†çš„3Dç”Ÿæˆã€‚è¯¥æ¡†æ¶ç»“åˆäº†éšå¼å’Œæ˜¾å¼çš„è¡¨ç¤ºæ–¹æ³•ï¼Œé¦–å…ˆé€šè¿‡éšå¼ç›’å­å‘é‡é›†æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆè¾¹ç•Œæ¡†å¸ƒå±€ã€‚ç„¶åï¼Œä¸ºæ¯ä¸ªéƒ¨åˆ†ç”Ÿæˆè¯¦ç»†çš„3Då½¢çŠ¶ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½åœ¨å…¶è‡ªå·±çš„å…¨åˆ†è¾¨ç‡ä½“ç´ ç½‘æ ¼ä¸­ç”Ÿæˆï¼Œé¿å…äº†å°éƒ¨åˆ†å ç”¨è¿‡å°‘ä½“ç´ çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸­å¿ƒç‚¹ç¼–ç ç­–ç•¥ï¼Œä»¥è§£å†³ä¸åŒå¤§å°éƒ¨åˆ†ä¹‹é—´ä¿¡æ¯äº¤æ¢æ—¶çš„å¯¹é½é—®é¢˜ï¼Œå¹¶å‘å¸ƒäº†PartVerse-XLæ•°æ®é›†ï¼Œä»¥æ”¯æŒ3Déƒ¨åˆ†ç”Ÿæˆçš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26474', 'title': 'Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing', 'url': 'https://huggingface.co/papers/2510.26474', 'abstract': 'Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew effect"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.', 'score': 1, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': 'd7c19ec426ccd7e8', 'authors': ['Xin Guo', 'Zhiheng Xi', 'Yiwen Ding', 'Yitao Zhai', 'Xiaowei Shi', 'Xunliang Cai', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang'], 'affiliations': ['Fudan University', 'Meituan', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.26474.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#cv', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ¼ ĞœĞ°Ñ‚Ñ„ĞµÑ Ğ² ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLMs): Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ "ÑÑ„Ñ„ĞµĞºÑ‚ ĞœĞ°Ñ‚Ñ„ĞµÑ", ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑÑ‘ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² ÑƒÑ‰ĞµÑ€Ğ± ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen2-VL Ğ¸ InternVL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… reasoning ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° 3.86 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Balancing Reasoning Skills in Vision-Language Models', 'desc': 'This paper addresses a significant challenge in self-improvement for large vision-language models (LVLMs), where the models tend to excel at simple queries but struggle with complex ones, leading to an imbalance in their reasoning capabilities. This phenomenon, referred to as the "Matthew effect," results in models focusing on easier tasks and neglecting more difficult reasoning challenges. To mitigate this issue, the authors propose four strategies aimed at re-balancing the model\'s learning process by reshaping data distribution and resampling trajectories. Their experiments show that these strategies enhance the models\' visual reasoning abilities, achieving an average improvement of 3.86 points over traditional self-improvement methods.'}, 'zh': {'title': 'å¹³è¡¡æ¨ç†èƒ½åŠ›ï¼Œæå‡æ¨¡å‹è¡¨ç°', 'desc': 'è‡ªæˆ‘æå‡å·²æˆä¸ºæé«˜å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸»æµæ–¹æ³•ï¼Œæ¨¡å‹é€šè¿‡è¿­ä»£æ¢ç´¢å’Œå­¦ä¹ æˆåŠŸçš„è½¨è¿¹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæ¨¡å‹åœ¨å¤„ç†ç®€å•æŸ¥è¯¢æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚æŸ¥è¯¢ä¸Šå´åŠ›ä¸ä»å¿ƒã€‚è¿™å¯¼è‡´äº†ä¼˜åŒ–çš„ä¸å¹³è¡¡ï¼Œä½¿æ¨¡å‹æ›´å€¾å‘äºç®€å•æ¨ç†æŠ€èƒ½ï¼Œè€ŒæŠ‘åˆ¶äº†å…¶å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å››ç§é«˜æ•ˆç­–ç•¥ï¼Œä»¥å®ç°æ¢ç´¢å’Œå­¦ä¹ è¿‡ç¨‹ä¸­çš„å¤´å°¾é‡å¹³è¡¡ï¼Œä»è€Œæå‡è§†è§‰æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26160', 'title': 'CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark', 'url': 'https://huggingface.co/papers/2510.26160', 'abstract': 'Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no comprehensive benchmark for this task, especially regarding wearables scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn conversations across 13 domains, including 6.2K egocentric images designed to mimic captures from wearable devices. We carefully constructed the questions to reflect real-world scenarios and challenges, including five types of image-quality issues, six question types, varying entity popularity, differing information dynamism, and different conversation turns. We design three tasks: single-source augmentation, multi-source augmentation, and multi-turn conversations -- each paired with an associated retrieval corpus and APIs for both image-KG retrieval and webpage retrieval. Our evaluation shows that straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM single- and multi-turn QA, respectively, whereas state-of-the-art industry solutions have similar quality (32%/45%), underscoring ample room for improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K participants and 5K submissions, with winning solutions improving baseline performance by 28%, highlighting its early impact on advancing the field.', 'score': 1, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '6be5612dcf7842dd', 'authors': ['Jiaqi Wang', 'Xiao Yang', 'Kai Sun', 'Parth Suresh', 'Sanat Sharma', 'Adam Czyzewski', 'Derek Andersen', 'Surya Appini', 'Arkav Banerjee', 'Sajal Choudhary', 'Shervin Ghasemlou', 'Ziqiang Guan', 'Akil Iyer', 'Haidar Khan', 'Lingkun Kong', 'Roy Luo', 'Tiffany Ma', 'Zhen Qiao', 'David Tran', 'Wenfang Xu', 'Skyler Yeatman', 'Chen Zhou', 'Gunveer Gujral', 'Yinglong Xia', 'Shane Moon', 'Nicolas Scheffer', 'Nirav Shah', 'Eun Chang', 'Yue Liu', 'Florian Metze', 'Tammy Stark', 'Zhaleh Feizollahi', 'Andrea Jessee', 'Mangesh Pujari', 'Ahmed Aly', 'Babak Damavandi', 'Rakesh Wanga', 'Anuj Kumar', 'Rohit Patel', 'Wen-tau Yih', 'Xin Luna Dong'], 'affiliations': ['FAIR, Meta', 'Meta', 'Meta Reality Labs', 'Meta Superintelligence Labs'], 'pdf_title_img': 'assets/pdf/title_img/2510.26160.jpg', 'data': {'categories': ['#multimodal', '#rag', '#benchmark', '#dataset'], 'emoji': 'ğŸ‘“', 'ru': {'title': 'CRAG-MM: Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¾Ñ‡ĞºĞ¾Ğ² Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CRAG-MM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾Ğ± Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¼ Ğ¼Ğ¸Ñ€Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾ÑĞ¸Ğ¼Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ²Ñ€Ğ¾Ğ´Ğµ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¾Ñ‡ĞºĞ¾Ğ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 6,5 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 6,2 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ÑÑŠÑ‘Ğ¼ĞºÑƒ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°, Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğµ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 32-45% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑƒĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ² KDD Cup 2025, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 28%.'}, 'en': {'title': 'Advancing Multi-Modal Conversations with CRAG-MM Benchmark', 'desc': 'This paper introduces CRAG-MM, a new benchmark for Multi-Modal Retrieval-Augmented Generation (MM-RAG) specifically designed for wearable devices like smart glasses. It includes 6.5K triplets of images, questions, and answers, along with 2K visual-based multi-turn conversations across various domains. The benchmark addresses real-world challenges by incorporating diverse question types and image-quality issues, facilitating the evaluation of retrieval methods. Results indicate that current RAG approaches have significant room for improvement, as evidenced by the performance metrics achieved in the KDD Cup 2025 competition.'}, 'zh': {'title': 'å¯ç©¿æˆ´è®¾å¤‡çš„å¤šæ¨¡æ€å¯¹è¯æ–°åŸºå‡†', 'desc': 'å¯ç©¿æˆ´è®¾å¤‡å¦‚æ™ºèƒ½çœ¼é•œæ­£åœ¨æ”¹å˜äººä»¬ä¸å‘¨å›´ç¯å¢ƒçš„äº’åŠ¨æ–¹å¼ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè·å–è§†é‡ä¸­å®ä½“çš„ä¿¡æ¯ã€‚å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMM-RAGï¼‰åœ¨æ”¯æŒæ­¤ç±»é—®é¢˜ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œä½†ç›®å‰å°šç¼ºä¹é’ˆå¯¹å¯ç©¿æˆ´åœºæ™¯çš„å…¨é¢åŸºå‡†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CRAG-MMâ€”â€”ä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€å¤šè½®å¯¹è¯çš„ç»¼åˆRAGåŸºå‡†ï¼ŒåŒ…å«6500ä¸ªï¼ˆå›¾åƒã€é—®é¢˜ã€ç­”æ¡ˆï¼‰ä¸‰å…ƒç»„å’Œ2000ä¸ªåŸºäºè§†è§‰çš„å¤šè½®å¯¹è¯ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰çš„RAGæ–¹æ³•åœ¨CRAG-MMçš„å•è½®å’Œå¤šè½®é—®ç­”ä¸­ä»…å®ç°äº†32%å’Œ43%çš„çœŸå®åº¦ï¼Œè¡¨æ˜è¯¥é¢†åŸŸä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.25132', 'title': 'EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme\n  Backbone Generation', 'url': 'https://huggingface.co/papers/2510.25132', 'abstract': "Designing enzyme backbones with substrate-specific functionality is a critical challenge in computational protein engineering. Current generative models excel in protein design but face limitations in binding data, substrate-specific control, and flexibility for de novo enzyme backbone generation. To address this, we introduce EnzyBind, a dataset with 11,100 experimentally validated enzyme-substrate pairs specifically curated from PDBbind. Building on this, we propose EnzyControl, a method that enables functional and substrate-specific control in enzyme backbone generation. Our approach generates enzyme backbones conditioned on MSA-annotated catalytic sites and their corresponding substrates, which are automatically extracted from curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter, a lightweight, modular component integrated into a pretrained motif-scaffolding model, allowing it to become substrate-aware. A two-stage training paradigm further refines the model's ability to generate accurate and functional enzyme structures. Experiments show that our EnzyControl achieves the best performance across structural and functional metrics on EnzyBind and EnzyBench benchmarks, with particularly notable improvements of 13\\% in designability and 13\\% in catalytic efficiency compared to the baseline models. The code is released at https://github.com/Vecteur-libre/EnzyControl.", 'score': 1, 'issue_id': 6714, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': '289ff7b39a55aab4', 'authors': ['Chao Song', 'Zhiyuan Liu', 'Han Huang', 'Liang Wang', 'Qiong Wang', 'Jianyu Shi', 'Hui Yu', 'Yihang Zhou', 'Yang Zhang'], 'affiliations': ['Institute of Automation at CAS', 'National University of Singapore', 'Northwestern Polytechnical University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.25132.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#data', '#dataset', '#training'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ„ĞµÑ€Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ÑÑƒĞ±ÑÑ‚Ñ€Ğ°Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ EnzyControl â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ„ĞµÑ€Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼ ÑÑƒĞ±ÑÑ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ EnzyBind Ñ 11,100 ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ñ„ĞµÑ€Ğ¼ĞµĞ½Ñ‚-ÑÑƒĞ±ÑÑ‚Ñ€Ğ°Ñ‚ Ğ¸ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ EnzyAdapter, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ backbone Ñ„ĞµÑ€Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑƒĞ±ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 13% Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ñƒ Ğ¸ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Enzyme Design with EnzyControl', 'desc': 'This paper presents EnzyBind, a new dataset containing 11,100 validated enzyme-substrate pairs to enhance enzyme design in computational protein engineering. The authors introduce EnzyControl, a method that allows for the generation of enzyme backbones that are specifically tailored to bind certain substrates. EnzyControl utilizes a lightweight component called EnzyAdapter, which integrates with a pretrained model to improve substrate awareness during backbone generation. The results demonstrate significant improvements in designability and catalytic efficiency, outperforming existing models on benchmark tests.'}, 'zh': {'title': 'æ™ºèƒ½è®¾è®¡ç‰¹å®šåŠŸèƒ½é…¶éª¨æ¶çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•EnzyControlï¼Œç”¨äºè®¾è®¡å…·æœ‰ç‰¹å®šåº•ç‰©åŠŸèƒ½çš„é…¶éª¨æ¶ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºEnzyBindçš„æ•°æ®é›†ï¼ŒåŒ…å«11,100å¯¹ç»è¿‡å®éªŒéªŒè¯çš„é…¶-åº•ç‰©é…å¯¹ï¼Œä»¥æ”¯æŒé…¶è®¾è®¡ã€‚EnzyControlé€šè¿‡åˆ©ç”¨å¤šåºåˆ—æ¯”å¯¹ï¼ˆMSAï¼‰æ³¨é‡Šçš„å‚¬åŒ–ä½ç‚¹å’Œç›¸åº”çš„åº•ç‰©ï¼Œç”Ÿæˆæ¡ä»¶åŒ–çš„é…¶éª¨æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEnzyControlåœ¨ç»“æ„å’ŒåŠŸèƒ½æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè®¾è®¡èƒ½åŠ›å’Œå‚¬åŒ–æ•ˆç‡åˆ†åˆ«æé«˜äº†13%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26781', 'title': 'ChartAB: A Benchmark for Chart Grounding & Dense Alignment', 'url': 'https://huggingface.co/papers/2510.26781', 'abstract': 'Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel "ChartAlign Benchmark (ChartAB)" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs\' capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models.', 'score': 0, 'issue_id': 6718, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '354cf23aec454383', 'authors': ['Aniruddh Bansal', 'Davit Soselia', 'Dang Nguyen', 'Tianyi Zhou'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2510.26781.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#cv', '#reasoning'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ChartAB: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ² VLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ChartAB Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ inference Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚Ğ¸, Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLM Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Chart Understanding in Vision-Language Models', 'desc': "This paper addresses the limitations of existing vision-language models (VLMs) in accurately understanding and analyzing charts. It introduces the ChartAlign Benchmark (ChartAB), which evaluates VLMs on tasks like extracting data, localizing elements, and recognizing attributes from various charts. The benchmark includes a JSON template for calculating specific evaluation metrics and employs a two-stage inference workflow to assess the models' ability to compare elements across charts. The findings reveal significant insights into the strengths and weaknesses of current VLMs in chart grounding, indicating areas for improvement."}, 'zh': {'title': 'æå‡å›¾è¡¨ç†è§£çš„è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•â€”â€”"ChartAlign Benchmark (ChartAB)"ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾è¡¨åŸºç¡€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬æå–è¡¨æ ¼æ•°æ®ã€å®šä½å¯è§†åŒ–å…ƒç´ ä»¥åŠè¯†åˆ«ä¸åŒç±»å‹å’Œå¤æ‚æ€§çš„å›¾è¡¨å±æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªJSONæ¨¡æ¿ï¼Œä»¥ä¾¿ä¸ºæ¯ä¸ªåŸºç¡€ä»»åŠ¡è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ä¸¤é˜¶æ®µæ¨ç†å·¥ä½œæµç¨‹ï¼Œä»¥è¯„ä¼°VLMsåœ¨å¯¹æ¯”ä¸¤ä¸ªå›¾è¡¨æ—¶çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹å¤šä¸ªæœ€æ–°VLMsçš„è¯„ä¼°åˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†å®ƒä»¬åœ¨å›¾è¡¨ç†è§£ä¸­çš„æ„ŸçŸ¥åå·®ã€å¼±ç‚¹ã€é²æ£’æ€§å’Œå¹»è§‰ç­‰æ–°è§è§£ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (6)', '#agi (1)', '#alignment', '#architecture (3)', '#audio', '#benchmark (10)', '#cv (5)', '#data (3)', '#dataset (7)', '#diffusion (3)', '#ethics (1)', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (2)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (2)', '#multilingual', '#multimodal (6)', '#open_source (5)', '#optimization (6)', '#plp', '#rag (1)', '#reasoning (10)', '#rl (3)', '#rlhf', '#robotics (1)', '#science (2)', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic (1)', '#training (6)', '#transfer_learning (2)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-10-31 09:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-31 09:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-31 09:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    