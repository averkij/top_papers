
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. October 31.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">31 октября</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-30.html">⬅️ <span id="prev-date">30.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-11-03.html">➡️ <span id="next-date">03.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'};
        let feedDateNext = {'ru': '03.11', 'en': '11/03', 'zh': '11月3日'};
        let feedDatePrev = {'ru': '30.10', 'en': '10/30', 'zh': '10月30日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.26583', 'title': 'Emu3.5: Native Multimodal Models are World Learners', 'url': 'https://huggingface.co/papers/2510.26583', 'abstract': 'We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research.', 'score': 45, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'e453f75d16182f5e', 'authors': ['Yufeng Cui', 'Honghao Chen', 'Haoge Deng', 'Xu Huang', 'Xinghang Li', 'Jirong Liu', 'Yang Liu', 'Zhuoyan Luo', 'Jinsheng Wang', 'Wenxuan Wang', 'Yueze Wang', 'Chengyuan Wang', 'Fan Zhang', 'Yingli Zhao', 'Ting Pan', 'Xianduo Li', 'Zecheng Hao', 'Wenxuan Ma', 'Zhuo Chen', 'Yulong Ao', 'Tiejun Huang', 'Zhongyuan Wang', 'Xinlong Wang'], 'affiliations': ['BAAI'], 'pdf_title_img': 'assets/pdf/title_img/2510.26583.jpg', 'data': {'categories': ['#open_source', '#optimization', '#inference', '#multimodal', '#rl', '#agi', '#cv', '#games'], 'emoji': '🌍', 'ru': {'title': 'Мультимодальная модель мира с единым предсказанием следующего токена', 'desc': 'Представлена модель Emu3.5 — крупномасштабная мультимодальная модель, которая предсказывает следующее состояние для изображений и текста одновременно. Модель обучена на корпусе из более 10 триллионов токенов, преимущественно из видео и их транскриптов, используя единую задачу предсказания следующего токена. Для ускорения инференса в 20 раз предложен метод Discrete Diffusion Adaptation (DiDA), который заменяет последовательную генерацию токенов на параллельное двунаправленное предсказание. Модель демонстрирует сильные способности в генерации изображений, мультимодальном рассуждении и моделировании мира, показывая результаты сопоставимые с Gemini 2.5 Flash.'}, 'en': {'title': 'Emu3.5: Revolutionizing Multimodal Predictions with Speed and Precision', 'desc': 'Emu3.5 is a large-scale multimodal world model that predicts future states using both vision and language inputs. It is trained on a massive dataset of over 10 trillion tokens, allowing it to generate outputs that seamlessly combine visual and textual information. The model employs a novel technique called Discrete Diffusion Adaptation (DiDA) to enhance inference speed, achieving a 20x improvement in efficiency. Emu3.5 demonstrates advanced capabilities in multimodal reasoning, world modeling, and generation tasks, outperforming existing models in various benchmarks.'}, 'zh': {'title': 'Emu3.5：多模态世界模型的未来', 'desc': 'Emu3.5是一种大规模的多模态世界模型，能够同时处理视觉和语言信息。它通过统一的下一个标记预测目标，在包含超过10万亿个标记的视觉-语言数据集上进行端到端的预训练。该模型支持交错的视觉-语言输入和输出，并通过大规模强化学习进行后训练，以增强多模态推理和生成能力。此外，Emu3.5引入了离散扩散适应（DiDA）技术，提高了推理效率，使每张图像的推理速度提高约20倍。'}}}, {'id': 'https://huggingface.co/papers/2510.15510', 'title': 'Exploring Conditions for Diffusion models in Robotic Control', 'url': 'https://huggingface.co/papers/2510.15510', 'abstract': "ORCA uses learnable task and visual prompts to adapt pre-trained text-to-image diffusion models for robotic control, achieving state-of-the-art performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While pre-trained visual representations have significantly advanced imitation learning, they are often task-agnostic as they remain frozen during policy learning. In this work, we explore leveraging pre-trained text-to-image diffusion models to obtain task-adaptive visual representations for robotic control, without fine-tuning the model itself. However, we find that naively applying textual conditions - a successful strategy in other vision domains - yields minimal or even negative gains in control tasks. We attribute this to the domain gap between the diffusion model's training data and robotic control environments, leading us to argue for conditions that consider the specific, dynamic visual information required for control. To this end, we propose ORCA, which introduces learnable task prompts that adapt to the control environment and visual prompts that capture fine-grained, frame-specific details. Through facilitating task-adaptive representations with our newly devised conditions, our approach achieves state-of-the-art performance on various robotic control benchmarks, significantly surpassing prior methods.", 'score': 34, 'issue_id': 6717, 'pub_date': '2025-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'd72f6bb74245abe8', 'authors': ['Heeseong Shin', 'Byeongho Heo', 'Dongyoon Han', 'Seungryong Kim', 'Taekyung Kim'], 'affiliations': ['KAIST AI', 'NAVER AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2510.15510.jpg', 'data': {'categories': ['#agents', '#optimization', '#benchmark', '#diffusion', '#cv', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Обучаемые промпты для адаптации диффузионных моделей к робототехнике', 'desc': 'ORCA использует обучаемые промпты для адаптации предобученных text-to-image диффузионных моделей к задачам управления роботами. Авторы обнаружили, что простое применение текстовых условий не работает из-за разрыва между данными обучения диффузионной модели и средой робототехники. Они предложили использовать обучаемые задачные промпты для адаптации к среде и визуальные промпты для захвата детальной информации из каждого кадра. Метод достигает state-of-the-art результатов на бенчмарках робототехнического управления, значительно превосходя предыдущие подходы.'}, 'en': {'title': 'Adapting Pre-trained Models for Superior Robotic Control', 'desc': 'ORCA is a method that enhances robotic control by using learnable prompts with pre-trained text-to-image diffusion models. Instead of fine-tuning the models, ORCA adapts them to specific tasks by introducing task and visual prompts that focus on the dynamic visual information needed for control. This approach addresses the limitations of traditional methods that apply textual conditions, which often do not translate well to robotic environments. As a result, ORCA achieves state-of-the-art performance on robotic control benchmarks, outperforming previous techniques.'}, 'zh': {'title': 'ORCA：智能机器人控制的新方法', 'desc': 'ORCA是一种利用可学习的任务提示和视觉提示来适应预训练的文本到图像扩散模型的方法，旨在提高机器人控制的性能。该方法不需要对模型进行微调，而是通过引入特定的动态视觉信息来解决传统方法在控制任务中效果不佳的问题。研究表明，简单地应用文本条件在机器人控制任务中效果有限，因此需要更精细的条件设计。最终，ORCA在多个机器人控制基准测试中实现了最先进的性能，显著超越了之前的方法。'}}}, {'id': 'https://huggingface.co/papers/2510.26802', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark', 'url': 'https://huggingface.co/papers/2510.26802', 'abstract': 'Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io', 'score': 24, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'b84e32b67921db6d', 'authors': ['Ziyu Guo', 'Xinyan Chen', 'Renrui Zhang', 'Ruichuan An', 'Yu Qi', 'Dongzhi Jiang', 'Xiangtai Li', 'Manyuan Zhang', 'Hongsheng Li', 'Pheng-Ann Heng'], 'affiliations': ['CUHK', 'IMIXR', 'MMLab', 'Northeastern University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2510.26802.jpg', 'data': {'categories': ['#video', '#reasoning', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'Видео-модели как визуальные движки: обещания и ограничения рассуждений', 'desc': 'Исследователи проверили, может ли современная модель генерации видео Veo-3 выступать в роли zero-shot reasoner для сложных задач визуального рассуждения. Они оценили её способности по 12 направлениям, включая пространственную, геометрическую, физическую, временную и embodied логику, создав для этого специальный бенчмарк MME-CoF. Оказалось, что модель хорошо справляется с краткосрочной пространственной согласованностью и локальной динамикой, но испытывает трудности с долгосрочными причинно-следственными связями, строгими геометрическими ограничениями и абстрактной логикой. Авторы заключают, что видео-модели пока не готовы быть самостоятельными reasoning системами, но могут служить полезным дополнением к специализированным моделям рассуждения.'}, 'en': {'title': 'Exploring the Reasoning Limits of Video Generation Models', 'desc': "This paper investigates the reasoning capabilities of advanced video generation models, particularly focusing on Veo-3. The authors assess the model's performance across various reasoning dimensions, such as spatial and temporal logic, using a newly created benchmark called MME-CoF. While the findings show that these models can handle short-term spatial coherence and local dynamics well, they struggle with long-term causal reasoning and abstract logic. Ultimately, the study concludes that while these video models are not yet reliable as independent zero-shot reasoners, they show potential as supportive tools in visual reasoning tasks."}, 'zh': {'title': '视频生成模型的推理能力研究', 'desc': '最近的视频生成模型能够生成高保真、时间一致的视频，表明它们可能编码了大量的世界知识。除了现实合成外，这些模型还表现出视觉感知、建模和操控的行为。本文通过对领先的Veo-3模型进行实证研究，评估其在12个维度上的推理能力，包括空间、几何、物理、时间和具身逻辑。研究发现，尽管当前视频模型在短期空间一致性和局部动态方面表现出色，但在长期因果推理和抽象逻辑方面仍然有限，尚不具备作为独立零-shot推理器的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2510.26692', 'title': 'Kimi Linear: An Expressive, Efficient Attention Architecture', 'url': 'https://huggingface.co/papers/2510.26692', 'abstract': 'We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.   We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.   To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.', 'score': 23, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '0b8a017f9816e001', 'authors': ['Kimi Team', 'Yu Zhang', 'Zongyu Lin', 'Xingcheng Yao', 'Jiaxi Hu', 'Fanqing Meng', 'Chengyin Liu', 'Xin Men', 'Songlin Yang', 'Zhiyuan Li', 'Wentao Li', 'Enzhe Lu', 'Weizhou Liu', 'Yanru Chen', 'Weixin Xu', 'Longhui Yu', 'Yejie Wang', 'Yu Fan', 'Longguang Zhong', 'Enming Yuan', 'Dehao Zhang', 'Yizhi Zhang', 'T. Y. Liu', 'Haiming Wang', 'Shengjun Fang', 'Weiran He', 'Shaowei Liu', 'Yiwei Li', 'Jianlin Su', 'Jiezhong Qiu', 'Bo Pang', 'Junjie Yan', 'Zhejun Jiang', 'Weixiao Huang', 'Bohong Yin', 'Jiacheng You', 'Chu Wei', 'Zhengtao Wang', 'Chao Hong', 'Yutian Chen', 'Guanduo Chen', 'Yucheng Wang', 'Huabin Zheng', 'Feng Wang', 'Yibo Liu', 'Mengnan Dong', 'Zheng Zhang', 'Siyuan Pan', 'Wenhao Wu', 'Yuhao Wu', 'Longyu Guan', 'Jiawen Tao', 'Guohong Fu', 'Xinran Xu', 'Yuzhi Wang', 'Guokun Lai', 'Yuxin Wu', 'Xinyu Zhou', 'Zhilin Yang', 'Yulun Du'], 'affiliations': ['MoonshotAI'], 'pdf_title_img': 'assets/pdf/title_img/2510.26692.jpg', 'data': {'categories': ['#open_source', '#architecture', '#optimization', '#long_context', '#training'], 'emoji': '⚡', 'ru': {'title': 'Линейное внимание обходит полное: эффективность встречается с производительностью', 'desc': 'Представлена архитектура Kimi Linear с гибридным линейным вниманием, которая впервые превосходит полное внимание в различных сценариях — от коротких контекстов до обучения с подкреплением. В основе лежит Kimi Delta Attention (KDA) — выразительный модуль линейного внимания с улучшенным механизмом гейтинга для эффективного использования памяти RNN. Модель с 3B активными параметрами показывает превосходство над полным вниманием, одновременно сокращая использование KV-кэша на 75% и ускоряя декодирование в 6 раз для контекста в 1M токенов. Авторы открыли исходный код и веса предобученных моделей для дальнейших исследований.'}, 'en': {'title': 'Kimi Linear: Revolutionizing Attention with Efficiency and Performance', 'desc': 'Kimi Linear is a new hybrid linear attention architecture that surpasses traditional full attention methods in various contexts, including short and long sequences, as well as reinforcement learning scenarios. It features Kimi Delta Attention (KDA), which enhances the Gated DeltaNet model with a more precise gating mechanism, optimizing the use of limited RNN memory. The architecture employs a unique chunkwise algorithm that leverages Diagonal-Plus-Low-Rank (DPLR) transition matrices to significantly lower computational costs while adhering to classical delta rules. Experimental results indicate that Kimi Linear not only outperforms full Multi-Head Latent Attention (MLA) but also reduces key-value cache usage and increases decoding speed, making it a highly efficient alternative for attention-based tasks.'}, 'zh': {'title': 'Kimi Linear：超越全注意力的高效架构', 'desc': '本文介绍了一种名为Kimi Linear的混合线性注意力架构，它首次在公平比较中超越了全注意力机制，适用于短上下文、长上下文和强化学习等多种场景。其核心是Kimi Delta Attention（KDA），这是一种表达能力强的线性注意力模块，通过更细粒度的门控机制，提升了有限状态RNN内存的使用效率。我们设计的分块算法通过特殊的对角加低秩（DPLR）转移矩阵变体，实现了高效的硬件利用，显著减少了计算量，同时保持了与经典增量规则的一致性。实验结果表明，Kimi Linear在相同的训练条件下，性能显著优于全多头潜在注意力（MLA），并且在处理长输入和输出时表现出更高的效率。'}}}, {'id': 'https://huggingface.co/papers/2510.26794', 'title': 'The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation', 'url': 'https://huggingface.co/papers/2510.26794', 'abstract': 'Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.', 'score': 21, 'issue_id': 6714, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '930f4b60c73c7768', 'authors': ['Jing Lin', 'Ruisi Wang', 'Junzhe Lu', 'Ziqi Huang', 'Guorui Song', 'Ailing Zeng', 'Xian Liu', 'Chen Wei', 'Wanqi Yin', 'Qingping Sun', 'Zhongang Cai', 'Lei Yang', 'Ziwei Liu'], 'affiliations': ['NVIDIA Research', 'Nanyang Technological University', 'SenseTime Research', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.26794.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#open_source', '#cv', '#multimodal', '#diffusion', '#transfer_learning'], 'emoji': '🎬', 'ru': {'title': 'От видео к движению: перенос знаний для генерации 3D-анимации человека', 'desc': 'Исследователи предложили новый подход к генерации 3D-движений человека, заимствуя знания из области генерации видео. Они создали крупный датасет ViMoGen-228K из 228 тысяч образцов движений, объединяющий данные motion capture, аннотированные видео и синтетические примеры от video generation моделей. Разработанная архитектура ViMoGen использует diffusion transformer с flow matching и мультимодальное conditioning для объединения разных источников данных. Также представлен новый бенчмарк MBench для детальной оценки качества движений, соответствия текстовым описаниям и способности к генерализации.'}, 'en': {'title': 'Bridging Video and Motion: A New Era for 3D Human Motion Generation', 'desc': 'This paper addresses the limitations of current 3D human motion generation (MoGen) models in generalization by leveraging insights from video generation (ViGen). The authors introduce a new dataset, ViMoGen-228K, which contains 228,000 high-quality motion samples that combine motion capture data with semantically rich annotations from web videos. They propose a novel model, ViMoGen, which utilizes a flow-matching-based diffusion transformer to integrate knowledge from both MoGen and ViGen, and also introduce a lighter version, ViMoGen-light, for improved efficiency. Finally, they create MBench, a benchmark for evaluating motion generation quality, demonstrating that their framework significantly enhances performance in various evaluation metrics.'}, 'zh': {'title': '知识转移，提升三维动作生成能力', 'desc': '本论文提出了一种新的框架，将视频生成（ViGen）领域的知识转移到三维人类动作生成（MoGen）中，以提高其泛化能力。我们引入了一个大型数据集ViMoGen-228K，包含228,000个高质量动作样本，结合了高保真光学动作捕捉数据和语义注释的动作。我们还提出了ViMoGen，一个基于流匹配的扩散变换器，通过门控多模态条件化来统一MoCap数据和ViGen模型的先验知识。最后，我们设计了MBench，一个分层基准，用于对动作质量、提示保真度和泛化能力进行细致评估，实验结果表明我们的框架在自动和人工评估中均显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2510.26800', 'title': 'OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes', 'url': 'https://huggingface.co/papers/2510.26800', 'abstract': 'There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.', 'score': 15, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '5ddc588a37cb2d17', 'authors': ['Yukun Huang', 'Jiwen Yu', 'Yanning Zhou', 'Jianan Wang', 'Xintao Wang', 'Pengfei Wan', 'Xihui Liu'], 'affiliations': ['Astribot', 'Kuaishou Technology', 'Tencent', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.26800.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#3d', '#synthetic', '#games'], 'emoji': '🌐', 'ru': {'title': 'От панорам к реалистичным 3D-сценам с физически корректным рендерингом', 'desc': "Статья представляет OmniX — универсальную систему для создания графически готовых 3D-сцен из панорамных изображений с использованием 2D generative моделей. Ключевая идея заключается в адаптации существующих 2D генеративных prior'ов для восприятия геометрии, текстур и PBR-материалов в панорамном формате. Система использует легковесный cross-modal адаптер для решения широкого спектра задач: восприятия, генерации и completion панорам. В результате получаются 3D-сцены, пригодные для physically based rendering, релайтинга и симуляций, что открывает новые возможности для создания иммерсивных виртуальных миров."}, 'en': {'title': 'OmniX: Bridging 2D Generative Models for Realistic 3D Scene Creation', 'desc': "This paper introduces OmniX, a novel framework that enhances panorama-based 2D lifting for generating realistic 3D scenes. It utilizes advanced 2D generative models to perceive and create geometry, textures, and materials suitable for physically based rendering (PBR). Unlike traditional methods that focus solely on visual appearance, OmniX integrates intrinsic properties into the generation process. The authors also present a large-scale synthetic panorama dataset to support various panoramic vision tasks, demonstrating the model's effectiveness in creating immersive virtual environments."}, 'zh': {'title': 'OmniX：全景视觉的统一框架', 'desc': '本文探讨了构建3D场景的两种主要方法：程序生成和2D提升。我们提出了一种名为OmniX的统一框架，利用强大的2D生成模型来生成适合物理基础渲染的3D场景。与现有方法不同，OmniX不仅关注外观生成，还重视几何、纹理和材料的内在属性感知。通过构建一个大规模的合成全景数据集，我们的实验表明，OmniX在全景视觉感知和3D场景生成方面表现出色，推动了沉浸式虚拟世界的生成。'}}}, {'id': 'https://huggingface.co/papers/2510.26768', 'title': 'AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions', 'url': 'https://huggingface.co/papers/2510.26768', 'abstract': 'We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. https://amo-bench.github.io/', 'score': 15, 'issue_id': 6715, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '8bfe84b563305313', 'authors': ['Shengnan An', 'Xunliang Cai', 'Xuezhi Cao', 'Xiaoyu Li', 'Yehao Lin', 'Junlin Liu', 'Xinxuan Lv', 'Dan Ma', 'Xuanlin Wang', 'Ziwen Wang', 'Shuang Zhou'], 'affiliations': ['Harbin Institute of Technology', 'Meituan', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2510.26768.jpg', 'data': {'categories': ['#math', '#benchmark', '#reasoning'], 'emoji': '🏆', 'ru': {'title': 'Олимпиадная математика ставит LLM в тупик', 'desc': 'Исследователи представили AMO-Bench — новый бенчмарк для оценки математических способностей LLM с задачами уровня Международной математической олимпиады и выше. Бенчмарк содержит 50 оригинальных задач, созданных экспертами, которые требуют только финального ответа без доказательства, что позволяет автоматически оценивать результаты. Тестирование 26 различных LLM показало, что даже лучшая модель достигла только 52.4% точности, а большинство моделей набрали менее 40%. Результаты демонстрируют значительный потенциал для улучшения математического мышления в современных языковых моделях, особенно при увеличении вычислительных ресурсов на этапе inference.'}, 'en': {'title': 'Raising the Bar for Mathematical Reasoning in LLMs', 'desc': 'AMO-Bench is a new benchmark designed to evaluate the mathematical reasoning abilities of large language models (LLMs) using challenging problems that meet International Mathematical Olympiad standards. Unlike previous benchmarks that relied on high school math competitions, AMO-Bench features 50 original problems crafted by experts to avoid data memorization issues. Each problem requires only a final answer, allowing for efficient automatic grading. Experimental results show that even the best LLMs struggle with these problems, achieving only 52.4% accuracy, indicating a significant opportunity for improvement in mathematical reasoning capabilities.'}, 'zh': {'title': 'AMO-Bench：提升语言模型数学推理能力的新基准', 'desc': '我们提出了AMO-Bench，这是一个高级数学推理基准，包含50个难度达到奥林匹克水平或更高的人为设计问题。现有的基准测试主要依赖于高中数学竞赛来评估大型语言模型（LLMs）的数学推理能力，但由于性能饱和，许多竞赛已不再有效。AMO-Bench通过确保所有问题都经过专家交叉验证，符合国际数学奥林匹克（IMO）的难度标准，并且是全新的原创问题，来引入更严格的挑战。此外，AMO-Bench中的每个问题只需提供最终答案，便于自动化和稳健的评分。'}}}, {'id': 'https://huggingface.co/papers/2510.26658', 'title': 'The Era of Agentic Organization: Learning to Organize with Language\n  Models', 'url': 'https://huggingface.co/papers/2510.26658', 'abstract': 'We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training.', 'score': 14, 'issue_id': 6716, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'ffa85621bc7e2c42', 'authors': ['Zewen Chi', 'Li Dong', 'Qingxiu Dong', 'Yaru Hao', 'Xun Wu', 'Shaohan Huang', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.26658.jpg', 'data': {'categories': ['#rl', '#inference', '#math', '#reasoning', '#agents', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'Асинхронное мышление: AI-агенты решают задачи параллельно и коллаборативно', 'desc': 'В статье представлена концепция асинхронного мышления (AsyncThink) для LLM, где модель организует процесс рассуждений как параллельные вычислительные структуры. Специальный протокол включает организатора, который распределяет подзадачи между воркерами, объединяет промежуточные результаты и формирует итоговое решение. Структура мышления оптимизируется через reinforcement learning, что позволяет агентам работать коллаборативно над сложными задачами. Эксперименты показывают снижение latency на 28% по сравнению с параллельным мышлением при улучшении точности на математических задачах, причём система обобщает навыки на новые задачи без дополнительного обучения.'}, 'en': {'title': 'Unlocking Collaborative Intelligence with AsyncThink', 'desc': 'This paper introduces a novel approach called asynchronous thinking (AsyncThink) for enhancing the collaborative problem-solving capabilities of AI agents. By structuring the reasoning process into concurrently executable tasks, AsyncThink allows agents to work together more efficiently than traditional methods. The proposed protocol involves an organizer that assigns tasks to worker agents, merges their findings, and generates coherent solutions, all while optimizing the process through reinforcement learning. Experimental results show that AsyncThink not only reduces inference latency by 28% but also improves accuracy in mathematical reasoning, demonstrating its ability to generalize to new tasks without extra training.'}, 'zh': {'title': '异步思维：协作解决复杂问题的新范式', 'desc': '本文提出了一种新的人工智能时代，称为代理组织，强调智能体通过协作解决复杂问题。我们引入了异步思维（AsyncThink）作为一种新的推理范式，能够将内部思维过程组织成可并发执行的结构。该思维协议允许组织者动态分配子查询给工作者，合并中间知识，生成连贯的解决方案。实验表明，AsyncThink在推理延迟上比并行思维降低了28%，同时在数学推理的准确性上有所提升，并且能够有效应对未见任务。'}}}, {'id': 'https://huggingface.co/papers/2510.25992', 'title': 'Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.25992', 'abstract': 'Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical "actions". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model\'s actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.', 'score': 12, 'issue_id': 6713, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'ab88bca35e9e2ff1', 'authors': ['Yihe Deng', 'I-Hung Hsu', 'Jun Yan', 'Zifeng Wang', 'Rujun Han', 'Gufeng Zhang', 'Yanfei Chen', 'Wei Wang', 'Tomas Pfister', 'Chen-Yu Lee'], 'affiliations': ['Google Cloud', 'Google Cloud AI Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2510.25992.jpg', 'data': {'categories': ['#optimization', '#agents', '#training', '#small_models', '#reasoning', '#rl'], 'emoji': '🎯', 'ru': {'title': 'Обучение через пошаговое подражание экспертам', 'desc': 'Статья представляет метод Supervised Reinforcement Learning (SRL), который помогает небольшим языковым моделям решать сложные задачи, требующие многошагового рассуждения. Вместо точного копирования решений или редкого получения правильных ответов, SRL учит модель генерировать внутренний монолог и получает награды за сходство каждого шага с действиями эксперта. Этот подход обеспечивает более богатый обучающий сигнал даже при неправильных решениях и позволяет гибко рассуждать, следуя экспертным демонстрациям. Метод показывает лучшие результаты при комбинации с классическим reinforcement learning и успешно применяется не только к задачам рассуждения, но и к разработке программного обеспечения.'}, 'en': {'title': 'Empowering LLMs with Supervised Reinforcement Learning for Better Reasoning', 'desc': 'This paper introduces Supervised Reinforcement Learning (SRL) as a new approach to improve the reasoning capabilities of Large Language Models (LLMs). SRL reformulates problem-solving into generating a sequence of logical actions, allowing the model to engage in internal reasoning before making decisions. By providing smoother rewards based on the similarity to expert actions, SRL enhances learning even when initial attempts are incorrect. The framework not only improves performance on reasoning tasks but also generalizes well to software engineering applications, making it a versatile tool for training LLMs.'}, 'zh': {'title': '监督强化学习：提升多步推理能力的关键', 'desc': '大型语言模型（LLMs）在需要多步推理的问题上常常表现不佳。针对小规模开源模型，强化学习与可验证奖励（RLVR）在正确解答稀少的情况下效果不佳，而监督微调（SFT）则容易通过逐字模仿导致过拟合。为了解决这个问题，我们提出了监督强化学习（SRL），该框架将问题解决重新定义为生成一系列逻辑“动作”。SRL通过在每个动作之前生成内部推理独白，提供基于模型动作与专家动作相似度的平滑奖励，从而有效提升小模型的学习能力。'}}}, {'id': 'https://huggingface.co/papers/2510.25628', 'title': 'EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\n  Health Record Analysis', 'url': 'https://huggingface.co/papers/2510.25628', 'abstract': 'Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis.', 'score': 8, 'issue_id': 6714, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '68de771aa478bf84', 'authors': ['Yusheng Liao', 'Chaoyi Wu', 'Junwei Liu', 'Shuyang Jiang', 'Pengcheng Qiu', 'Haowen Wang', 'Yun Yue', 'Shuai Zhen', 'Jian Wang', 'Qianrui Fan', 'Jinjie Gu', 'Ya Zhang', 'Yanfeng Wang', 'Yu Wang', 'Weidi Xie'], 'affiliations': ['Fudan University, Shanghai, China', 'Intelligence Computing and Sensing Laboratory, Peking University, Beijing, China', 'Intelligence Healthcare Department, AntGroup, Hangzhou, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.25628.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#healthcare', '#reasoning', '#training', '#science'], 'emoji': '🏥', 'ru': {'title': 'EHR-R1: AI-модель с продвинутыми рассуждениями для анализа медицинских карт', 'desc': 'Статья представляет EHR-R1 — специализированную LLM для анализа электронных медицинских карт с улучшенными способностями к рассуждениям. Авторы создали EHR-Ins — датасет из 300 тысяч случаев с рассуждениями и 4 миллионов обычных случаев по 42 медицинским задачам, используя инновационный метод генерации данных на основе графов мышления. Модель обучалась в несколько этапов: адаптация к медицинскому домену, улучшение способностей к рассуждениям и reinforcement learning. EHR-R1 превосходит GPT-4o более чем на 30 пунктов на бенчмарке MIMIC-Bench и показывает на 10% лучший результат по метрике AUROC в zero-shot режиме на EHRSHOT.'}, 'en': {'title': 'Revolutionizing EHR Analysis with Enhanced Reasoning Models', 'desc': 'This paper addresses the challenges of analyzing Electronic Health Records (EHRs) using large language models (LLMs). It introduces EHR-Ins, a comprehensive dataset designed for EHR reasoning, which includes 300,000 reasoning cases and 4 million non-reasoning cases across 42 tasks. The authors develop EHR-R1, a series of reasoning-enhanced LLMs that utilize a multi-stage training approach to improve their reasoning capabilities and domain knowledge for EHR analysis. The results demonstrate that EHR-R1 outperforms existing models, providing a significant advancement in the reliability and relevance of EHR analysis in clinical settings.'}, 'zh': {'title': '提升电子健康记录分析的智能推理能力', 'desc': '本论文介绍了一种新的电子健康记录（EHR）推理指令数据集EHR-Ins，包含30万个高质量推理案例和400万个非推理案例，覆盖42个不同的EHR任务。我们提出了一种基于思维图的框架，能够大规模生成高质量的推理数据。基于此，我们开发了EHR-R1，这是一系列针对EHR分析的增强推理大型语言模型，参数量高达720亿。通过多阶段训练，包括领域适应、推理增强和强化学习，EHR-R1系统性地获取了领域知识和多样的推理能力，显著提高了EHR分析的准确性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2510.26213', 'title': 'OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal\n  Document Layout Generation', 'url': 'https://huggingface.co/papers/2510.26213', 'abstract': 'Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage Coarse-to-Fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse category definitions, and 2) transferring the knowledge to a specific domain with fine-grained annotations. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M^{6}Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, models, and dataset will be publicly released.', 'score': 5, 'issue_id': 6717, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '63784033fbe5180b', 'authors': ['Hengrui Kang', 'Zhuangcheng Gu', 'Zhiyuan Zhao', 'Zichen Wen', 'Bin Wang', 'Weijia Li', 'Conghui He'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2510.26213.jpg', 'data': {'categories': ['#open_source', '#dataset', '#training', '#transfer_learning', '#data', '#architecture'], 'emoji': '📰', 'ru': {'title': 'OmniLayout: Миллион разнообразных макетов для генерации документов', 'desc': 'Исследователи представили OmniLayout-1M — первый датасет с миллионом разнообразных макетов документов, охватывающий шесть типов документов от газет до журналов, а не только академические статьи. Для работы с такими сложными данными они разработали OmniLayout-LLM — компактную модель на 0.5B параметров с двухэтапным обучением от грубого к точному. Сначала модель изучает универсальные принципы компоновки на большом датасете с общими категориями, затем переносит знания на конкретную область с детальными аннотациями. Эксперименты показали, что подход значительно превосходит как специализированные методы генерации макетов, так и современные LLM общего назначения на датасете M^6Doc.'}, 'en': {'title': 'Revolutionizing Document Layout Generation with OmniLayout-1M and OmniLayout-LLM', 'desc': 'This paper addresses the gap in document layout generation by introducing OmniLayout-1M, a large dataset containing a million diverse document layouts from various genres. The authors highlight that most existing research has focused on document layout analysis, neglecting the generative aspect, particularly for complex document types like newspapers and magazines. To tackle the challenges of generating coherent layouts, they propose OmniLayout-LLM, a 0.5 billion parameter model that employs a two-stage Coarse-to-Fine learning approach. Their experiments show that this model outperforms existing layout generation methods and general-purpose language models, demonstrating its effectiveness across multiple document types.'}, 'zh': {'title': '多样化文档布局生成的新突破', 'desc': '本文介绍了文档人工智能的快速发展，尤其是文档布局生成的研究。为了填补现有研究中多样化布局的缺失，作者创建了OmniLayout-1M数据集，包含六种常见文档类型的布局。文章还提出了OmniLayout-LLM模型，采用了两阶段的粗到细学习方法，以提高在复杂领域中的布局生成能力。实验结果表明，该方法在多个领域的表现优于现有的布局生成专家和最新的通用大语言模型。'}}}, {'id': 'https://huggingface.co/papers/2510.26298', 'title': 'Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in\n  Web Games', 'url': 'https://huggingface.co/papers/2510.26298', 'abstract': "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlas's web interaction capabilities using browser-based games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird, and Stein.world. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at https://atlas-game-eval.github.io.", 'score': 4, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '17bebfc38acb1c32', 'authors': ['Jingran Zhang', 'Ning Li', 'Justin Cui'], 'affiliations': ['UC San Diego', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2510.26298.jpg', 'data': {'categories': ['#agents', '#multimodal', '#reasoning', '#benchmark', '#games'], 'emoji': '🎮', 'ru': {'title': 'Атлас от OpenAI: силён в логике, слаб в реакции', 'desc': 'Исследователи протестировали новую модель ChatGPT Atlas, которая умеет взаимодействовать с веб-страницами через курсор и клавиатуру. Тестирование проводилось на браузерных играх: T-Rex Runner, Sudoku, Flappy Bird и Stein.world. Модель показала отличные результаты в логических задачах вроде судоку, решая их быстрее людей, но провалилась в динамичных играх, требующих точной реакции и контроля времени. Это демонстрирует, что AI хорошо справляется с аналитическими задачами, но пока имеет серьёзные ограничения в интерактивных средах реального времени.'}, 'en': {'title': 'Evaluating Atlas: Strong in Logic, Struggling in Real-Time Interaction', 'desc': "OpenAI's ChatGPT Atlas enhances web interaction by allowing the model to analyze webpages and perform user actions like clicking and typing. This paper evaluates Atlas's performance in browser-based games to understand its capabilities in dynamic environments. The results indicate that Atlas excels in logical reasoning tasks, completing puzzles like Sudoku faster than humans, but struggles with real-time games that require quick reflexes and precise control. These findings highlight the model's strengths in analytical tasks while revealing significant limitations in interactive scenarios that demand immediate responses."}, 'zh': {'title': 'Atlas：网页交互的新探索', 'desc': 'OpenAI的ChatGPT Atlas引入了新的网页交互能力，使模型能够分析网页、处理用户意图，并直接在浏览器中执行光标和键盘输入。尽管其在信息检索任务中的能力已得到验证，但在动态交互环境中的表现仍然较少被探索。我们通过使用基于浏览器的游戏（如谷歌的T-Rex Runner、数独、Flappy Bird和Stein.world）进行早期评估，采用游戏内表现分数作为量化指标来评估不同任务类型的性能。结果表明，Atlas在逻辑推理任务（如数独）中表现出色，完成难题的速度显著快于人类基线，但在需要精确时机和运动控制的实时游戏中表现不佳，常常无法突破初始障碍。'}}}, {'id': 'https://huggingface.co/papers/2510.25779', 'title': 'Magentic Marketplace: An Open-Source Environment for Studying Agentic\n  Markets', 'url': 'https://huggingface.co/papers/2510.25779', 'abstract': 'As LLM agents advance, they are increasingly mediating economic decisions, ranging from product discovery to transactions, on behalf of users. Such applications promise benefits but also raise many questions about agent accountability and value for users. Addressing these questions requires understanding how agents behave in realistic market conditions. However, previous research has largely evaluated agents in constrained settings, such as single-task marketplaces (e.g., negotiation) or structured two-agent interactions. Real-world markets are fundamentally different: they require agents to handle diverse economic activities and coordinate within large, dynamic ecosystems where multiple agents with opaque behaviors may engage in open-ended dialogues. To bridge this gap, we investigate two-sided agentic marketplaces where Assistant agents represent consumers and Service agents represent competing businesses. To study these interactions safely, we develop Magentic-Marketplace-- a simulated environment where Assistants and Services can operate. This environment enables us to study key market dynamics: the utility agents achieve, behavioral biases, vulnerability to manipulation, and how search mechanisms shape market outcomes. Our experiments show that frontier models can approach optimal welfare-- but only under ideal search conditions. Performance degrades sharply with scale, and all models exhibit severe first-proposal bias, creating 10-30x advantages for response speed over quality. These findings reveal how behaviors emerge across market conditions, informing the design of fair and efficient agentic marketplaces.', 'score': 4, 'issue_id': 6714, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'fc91b6d1ec75911d', 'authors': ['Gagan Bansal', 'Wenyue Hua', 'Zezhou Huang', 'Adam Fourney', 'Amanda Swearngin', 'Will Epperson', 'Tyler Payne', 'Jake M. Hofman', 'Brendan Lucier', 'Chinmay Singh', 'Markus Mobius', 'Akshay Nambi', 'Archana Yadav', 'Kevin Gao', 'David M. Rothschild', 'Aleksandrs Slivkins', 'Daniel G. Goldstein', 'Hussein Mozannar', 'Nicole Immorlica', 'Maya Murad', 'Matthew Vogel', 'Subbarao Kambhampati', 'Eric Horvitz', 'Saleema Amershi'], 'affiliations': ['Arizona State University', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2510.25779.jpg', 'data': {'categories': ['#multimodal', '#ethics', '#reasoning', '#agents'], 'emoji': '🤝', 'ru': {'title': 'Агенты на рынке: скорость побеждает качество', 'desc': 'Исследователи изучают поведение LLM-агентов в двусторонних рынках, где одни агенты представляют потребителей, а другие — конкурирующие бизнесы. Для безопасного тестирования создана симулированная среда Magentic-Marketplace, позволяющая анализировать благосостояние пользователей, поведенческие искажения и уязвимости к манипуляциям. Эксперименты показали, что современные модели могут достигать оптимальных результатов только в идеальных условиях поиска, но их производительность резко падает при масштабировании. Все модели демонстрируют сильное смещение к первому предложению (first-proposal bias), создавая преимущество в 10-30 раз для скорости ответа над качеством.'}, 'en': {'title': 'Navigating the Complexities of Agentic Marketplaces', 'desc': 'This paper explores how large language model (LLM) agents can influence economic decisions in real-world markets, where they act on behalf of users. It highlights the need to understand agent behavior in complex, dynamic environments rather than simplified settings. The authors introduce a simulated environment called Magentic-Marketplace to analyze interactions between consumer-representing Assistants and competing Service agents. Their findings indicate that while advanced models can achieve good outcomes under ideal conditions, they struggle with scale and exhibit biases that can significantly impact market efficiency.'}, 'zh': {'title': '探索代理市场的公平与效率', 'desc': '随着大型语言模型（LLM）代理的发展，它们在经济决策中扮演着越来越重要的角色，包括产品发现和交易。这些应用虽然带来了好处，但也引发了关于代理责任和用户价值的许多问题。为了理解代理在现实市场条件下的行为，我们开发了Magentic-Marketplace，一个模拟环境，研究消费者代理和竞争企业代理之间的互动。实验结果表明，尽管前沿模型在理想搜索条件下可以接近最佳福利，但在规模扩大时性能急剧下降，所有模型都表现出严重的首次提案偏见，导致响应速度相对于质量有10-30倍的优势。'}}}, {'id': 'https://huggingface.co/papers/2510.26787', 'title': 'Remote Labor Index: Measuring AI Automation of Remote Work', 'url': 'https://huggingface.co/papers/2510.26787', 'abstract': 'AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.', 'score': 2, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '4b733966c35e82f0', 'authors': ['Mantas Mazeika', 'Alice Gatti', 'Cristina Menghini', 'Udari Madhushani Sehwag', 'Shivam Singhal', 'Yury Orlovskiy', 'Steven Basart', 'Manasi Sharma', 'Denis Peskoff', 'Elaine Lau', 'Jaehyuk Lim', 'Lachlan Carroll', 'Alice Blair', 'Vinaya Sivakumar', 'Sumana Basu', 'Brad Kenstler', 'Yuntao Ma', 'Julian Michael', 'Xiaoke Li', 'Oliver Ingebretsen', 'Aditya Mehta', 'Jean Mottola', 'John Teichmann', 'Kevin Yu', 'Zaina Shaik', 'Adam Khoja', 'Richard Ren', 'Jason Hausenloy', 'Long Phan', 'Ye Htet', 'Ankit Aich', 'Tahseen Rabbani', 'Vivswan Shah', 'Andriy Novykov', 'Felix Binder', 'Kirill Chugunov', 'Luis Ramirez', 'Matias Geralnik', 'Hernán Mesura', 'Dean Lee', 'Ed-Yeremai Hernandez Cardona', 'Annette Diamond', 'Summer Yue', 'Alexandr Wang', 'Bing Liu', 'Ernesto Hernandez', 'Dan Hendrycks'], 'affiliations': ['Center for AI Safety', 'Scale AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.26787.jpg', 'data': {'categories': ['#agents', '#science', '#reasoning', '#benchmark'], 'emoji': '🏢', 'ru': {'title': 'Реальная автоматизация труда: AI пока справляется только с 2.5% задач', 'desc': 'Исследователи представили Remote Labor Index (RLI) — новый бенчмарк для оценки способности AI-агентов выполнять реальные экономически значимые задачи из разных секторов. Несмотря на впечатляющие результаты на исследовательских бенчмарках, AI-агенты показали крайне низкую производительность на практических задачах: лучший агент автоматизировал только 2.5% работы. Это исследование помогает объективно оценить реальное влияние AI на автоматизацию труда, отделяя теоретические достижения от практической применимости. Бенчмарк создаёт основу для отслеживания прогресса AI в автоматизации и помогает заинтересованным сторонам готовиться к изменениям на рынке труда.'}, 'en': {'title': "Measuring AI's Real-World Impact on Labor Automation", 'desc': "This paper introduces the Remote Labor Index (RLI), a new benchmark designed to assess the performance of AI agents in real-world economic tasks. The RLI evaluates how well AI can automate valuable projects across various sectors, providing a practical measure of AI's capabilities. The findings reveal that current AI agents perform poorly on the RLI, with the best achieving only a 2.5% automation rate. This research aims to provide empirical evidence for discussions about AI's impact on labor and help stakeholders understand and manage the implications of AI-driven automation."}, 'zh': {'title': '量化AI自动化的经济价值', 'desc': '本文介绍了一种新的指标，称为远程劳动指数（RLI），用于评估人工智能在实际经济项目中的表现。RLI是一个多行业的基准，旨在衡量AI代理在真实世界中的自动化能力。研究发现，AI代理在RLI上的表现接近最低水平，最高的自动化率仅为2.5%。这些结果为AI自动化的讨论提供了实证依据，帮助利益相关者更好地理解和应对AI驱动的劳动自动化。'}}}, {'id': 'https://huggingface.co/papers/2510.26140', 'title': 'FullPart: Generating each 3D Part at Full Resolution', 'url': 'https://huggingface.co/papers/2510.26140', 'abstract': 'Part-based 3D generation holds great potential for various applications. Previous part generators that represent parts using implicit vector-set tokens often suffer from insufficient geometric details. Another line of work adopts an explicit voxel representation but shares a global voxel grid among all parts; this often causes small parts to occupy too few voxels, leading to degraded quality. In this paper, we propose FullPart, a novel framework that combines both implicit and explicit paradigms. It first derives the bounding box layout through an implicit box vector-set diffusion process, a task that implicit diffusion handles effectively since box tokens contain little geometric detail. Then, it generates detailed parts, each within its own fixed full-resolution voxel grid. Instead of sharing a global low-resolution space, each part in our method - even small ones - is generated at full resolution, enabling the synthesis of intricate details. We further introduce a center-point encoding strategy to address the misalignment issue when exchanging information between parts of different actual sizes, thereby maintaining global coherence. Moreover, to tackle the scarcity of reliable part data, we present PartVerse-XL, the largest human-annotated 3D part dataset to date with 40K objects and 320K parts. Extensive experiments demonstrate that FullPart achieves state-of-the-art results in 3D part generation. We will release all code, data, and model to benefit future research in 3D part generation.', 'score': 2, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '37da8a03be0a8489', 'authors': ['Lihe Ding', 'Shaocong Dong', 'Yaokun Li', 'Chenjian Gao', 'Xiao Chen', 'Rui Han', 'Yihao Kuang', 'Hong Zhang', 'Bo Huang', 'Zhanpeng Huang', 'Zibin Wang', 'Dan Xu', 'Tianfan Xue'], 'affiliations': ['CUHK', 'Chongqing University', 'HKUST', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.26140.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#3d', '#dataset'], 'emoji': '🧩', 'ru': {'title': 'Полноразмерная генерация частей: каждой детали своё пространство', 'desc': 'Статья представляет FullPart — новый подход к генерации 3D-объектов по частям, который комбинирует неявные (implicit) и явные (explicit) методы представления. Сначала система определяет расположение ограничивающих боксов через диффузионный процесс с векторными токенами, а затем генерирует детализированные части, каждую в собственной вокселной сетке полного разрешения. Это позволяет создавать мелкие части с высокой детализацией, в отличие от предыдущих методов, где все части делили одну глобальную сетку низкого разрешения. Авторы также представляют PartVerse-XL — крупнейший размеченный вручную датасет 3D-частей с 40 тысячами объектов и 320 тысячами частей.'}, 'en': {'title': 'Revolutionizing 3D Part Generation with FullPart', 'desc': 'This paper introduces FullPart, a new framework for generating 3D parts that combines implicit and explicit methods. It uses an implicit diffusion process to create a bounding box layout, which is effective for low-detail shapes. Each part is then generated in its own high-resolution voxel grid, allowing for detailed and intricate designs without the limitations of shared low-resolution spaces. Additionally, the authors present PartVerse-XL, a large dataset to improve the training of models in 3D part generation, and demonstrate that FullPart achieves top performance in this area.'}, 'zh': {'title': 'FullPart：提升3D部分生成的全新框架', 'desc': '本文提出了一种名为FullPart的新框架，用于改进基于部分的3D生成。该框架结合了隐式和显式的表示方法，首先通过隐式盒子向量集扩散过程生成边界框布局。然后，为每个部分生成详细的3D形状，每个部分都在其自己的全分辨率体素网格中生成，避免了小部分占用过少体素的问题。我们还引入了中心点编码策略，以解决不同大小部分之间信息交换时的对齐问题，并发布了PartVerse-XL数据集，以支持3D部分生成的研究。'}}}, {'id': 'https://huggingface.co/papers/2510.26474', 'title': 'Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing', 'url': 'https://huggingface.co/papers/2510.26474', 'abstract': 'Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew effect"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.', 'score': 1, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'd7c19ec426ccd7e8', 'authors': ['Xin Guo', 'Zhiheng Xi', 'Yiwen Ding', 'Yitao Zhai', 'Xiaowei Shi', 'Xunliang Cai', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang'], 'affiliations': ['Fudan University', 'Meituan', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.26474.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#cv', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Борьба с эффектом Матфея в самообучении визуальных моделей', 'desc': 'Исследователи обнаружили проблему в процессе самообучения больших визуально-языковых моделей (LVLMs): модели хорошо справляются с простыми запросами, но испытывают трудности со сложными задачами. Этот дисбаланс усиливается с каждой итерацией обучения, создавая "эффект Матфея", когда модель всё больше фокусируется на простых задачах в ущерб сложным. Авторы предложили четыре стратегии для балансировки обучения: изменение распределения данных и пересэмплирование траекторий решения. Эксперименты на моделях Qwen2-VL и InternVL показали улучшение визуальных reasoning способностей на 3.86 пункта по сравнению с обычным самообучением.'}, 'en': {'title': 'Balancing Reasoning Skills in Vision-Language Models', 'desc': 'This paper addresses a significant challenge in self-improvement for large vision-language models (LVLMs), where the models tend to excel at simple queries but struggle with complex ones, leading to an imbalance in their reasoning capabilities. This phenomenon, referred to as the "Matthew effect," results in models focusing on easier tasks and neglecting more difficult reasoning challenges. To mitigate this issue, the authors propose four strategies aimed at re-balancing the model\'s learning process by reshaping data distribution and resampling trajectories. Their experiments show that these strategies enhance the models\' visual reasoning abilities, achieving an average improvement of 3.86 points over traditional self-improvement methods.'}, 'zh': {'title': '平衡推理能力，提升模型表现', 'desc': '自我提升已成为提高大型视觉语言模型（LVLM）推理能力的主流方法，模型通过迭代探索和学习成功的轨迹。然而，我们发现一个关键问题：模型在处理简单查询时表现出色，但在复杂查询上却力不从心。这导致了优化的不平衡，使模型更倾向于简单推理技能，而抑制了其处理复杂推理任务的能力。为了解决这个问题，我们提出了四种高效策略，以实现探索和学习过程中的头尾重平衡，从而提升视觉推理能力。'}}}, {'id': 'https://huggingface.co/papers/2510.26160', 'title': 'CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark', 'url': 'https://huggingface.co/papers/2510.26160', 'abstract': 'Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no comprehensive benchmark for this task, especially regarding wearables scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn conversations across 13 domains, including 6.2K egocentric images designed to mimic captures from wearable devices. We carefully constructed the questions to reflect real-world scenarios and challenges, including five types of image-quality issues, six question types, varying entity popularity, differing information dynamism, and different conversation turns. We design three tasks: single-source augmentation, multi-source augmentation, and multi-turn conversations -- each paired with an associated retrieval corpus and APIs for both image-KG retrieval and webpage retrieval. Our evaluation shows that straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM single- and multi-turn QA, respectively, whereas state-of-the-art industry solutions have similar quality (32%/45%), underscoring ample room for improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K participants and 5K submissions, with winning solutions improving baseline performance by 28%, highlighting its early impact on advancing the field.', 'score': 1, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '6be5612dcf7842dd', 'authors': ['Jiaqi Wang', 'Xiao Yang', 'Kai Sun', 'Parth Suresh', 'Sanat Sharma', 'Adam Czyzewski', 'Derek Andersen', 'Surya Appini', 'Arkav Banerjee', 'Sajal Choudhary', 'Shervin Ghasemlou', 'Ziqiang Guan', 'Akil Iyer', 'Haidar Khan', 'Lingkun Kong', 'Roy Luo', 'Tiffany Ma', 'Zhen Qiao', 'David Tran', 'Wenfang Xu', 'Skyler Yeatman', 'Chen Zhou', 'Gunveer Gujral', 'Yinglong Xia', 'Shane Moon', 'Nicolas Scheffer', 'Nirav Shah', 'Eun Chang', 'Yue Liu', 'Florian Metze', 'Tammy Stark', 'Zhaleh Feizollahi', 'Andrea Jessee', 'Mangesh Pujari', 'Ahmed Aly', 'Babak Damavandi', 'Rakesh Wanga', 'Anuj Kumar', 'Rohit Patel', 'Wen-tau Yih', 'Xin Luna Dong'], 'affiliations': ['FAIR, Meta', 'Meta', 'Meta Reality Labs', 'Meta Superintelligence Labs'], 'pdf_title_img': 'assets/pdf/title_img/2510.26160.jpg', 'data': {'categories': ['#multimodal', '#rag', '#benchmark', '#dataset'], 'emoji': '👓', 'ru': {'title': 'CRAG-MM: Бенчмарк для умных очков с мультимодальными диалогами', 'desc': 'Исследователи создали новый бенчмарк CRAG-MM для оценки систем, отвечающих на вопросы об окружающем мире через носимые устройства вроде умных очков. Датасет содержит 6,5 тысяч пар изображений и вопросов, включая 6,2 тысячи эгоцентрических фотографий, имитирующих съёмку от первого лица, с различными проблемами качества и типами вопросов. Текущие RAG-системы и промышленные решения достигают лишь 32-45% точности на этой задаче, что показывает большой простор для улучшений. Бенчмарк уже используется в KDD Cup 2025, где победители улучшили базовые результаты на 28%.'}, 'en': {'title': 'Advancing Multi-Modal Conversations with CRAG-MM Benchmark', 'desc': 'This paper introduces CRAG-MM, a new benchmark for Multi-Modal Retrieval-Augmented Generation (MM-RAG) specifically designed for wearable devices like smart glasses. It includes 6.5K triplets of images, questions, and answers, along with 2K visual-based multi-turn conversations across various domains. The benchmark addresses real-world challenges by incorporating diverse question types and image-quality issues, facilitating the evaluation of retrieval methods. Results indicate that current RAG approaches have significant room for improvement, as evidenced by the performance metrics achieved in the KDD Cup 2025 competition.'}, 'zh': {'title': '可穿戴设备的多模态对话新基准', 'desc': '可穿戴设备如智能眼镜正在改变人们与周围环境的互动方式，使用户能够获取视野中实体的信息。多模态检索增强生成（MM-RAG）在支持此类问题中发挥了关键作用，但目前尚缺乏针对可穿戴场景的全面基准。为了解决这一问题，我们提出了CRAG-MM——一个针对多模态多轮对话的综合RAG基准，包含6500个（图像、问题、答案）三元组和2000个基于视觉的多轮对话。我们的评估显示，现有的RAG方法在CRAG-MM的单轮和多轮问答中仅实现了32%和43%的真实度，表明该领域仍有很大的改进空间。'}}}, {'id': 'https://huggingface.co/papers/2510.25132', 'title': 'EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme\n  Backbone Generation', 'url': 'https://huggingface.co/papers/2510.25132', 'abstract': "Designing enzyme backbones with substrate-specific functionality is a critical challenge in computational protein engineering. Current generative models excel in protein design but face limitations in binding data, substrate-specific control, and flexibility for de novo enzyme backbone generation. To address this, we introduce EnzyBind, a dataset with 11,100 experimentally validated enzyme-substrate pairs specifically curated from PDBbind. Building on this, we propose EnzyControl, a method that enables functional and substrate-specific control in enzyme backbone generation. Our approach generates enzyme backbones conditioned on MSA-annotated catalytic sites and their corresponding substrates, which are automatically extracted from curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter, a lightweight, modular component integrated into a pretrained motif-scaffolding model, allowing it to become substrate-aware. A two-stage training paradigm further refines the model's ability to generate accurate and functional enzyme structures. Experiments show that our EnzyControl achieves the best performance across structural and functional metrics on EnzyBind and EnzyBench benchmarks, with particularly notable improvements of 13\\% in designability and 13\\% in catalytic efficiency compared to the baseline models. The code is released at https://github.com/Vecteur-libre/EnzyControl.", 'score': 1, 'issue_id': 6714, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '289ff7b39a55aab4', 'authors': ['Chao Song', 'Zhiyuan Liu', 'Han Huang', 'Liang Wang', 'Qiong Wang', 'Jianyu Shi', 'Hui Yu', 'Yihang Zhou', 'Yang Zhang'], 'affiliations': ['Institute of Automation at CAS', 'National University of Singapore', 'Northwestern Polytechnical University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.25132.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#data', '#dataset', '#training'], 'emoji': '🧬', 'ru': {'title': 'Генерация ферментов под конкретные субстраты с контролем каталитических свойств', 'desc': 'Исследователи представили EnzyControl — метод для генерации структур ферментов с учётом специфичности к определённым субстратам. В основе подхода лежит датасет EnzyBind с 11,100 экспериментально подтверждёнными парами фермент-субстрат и лёгкий модульный компонент EnzyAdapter, встроенный в предобученную модель. Модель генерирует backbone ферментов с учётом каталитических сайтов и соответствующих субстратов, используя двухэтапную схему обучения. Эксперименты показали улучшение на 13% в способности к дизайну и каталитической эффективности по сравнению с базовыми моделями.'}, 'en': {'title': 'Revolutionizing Enzyme Design with EnzyControl', 'desc': 'This paper presents EnzyBind, a new dataset containing 11,100 validated enzyme-substrate pairs to enhance enzyme design in computational protein engineering. The authors introduce EnzyControl, a method that allows for the generation of enzyme backbones that are specifically tailored to bind certain substrates. EnzyControl utilizes a lightweight component called EnzyAdapter, which integrates with a pretrained model to improve substrate awareness during backbone generation. The results demonstrate significant improvements in designability and catalytic efficiency, outperforming existing models on benchmark tests.'}, 'zh': {'title': '智能设计特定功能酶骨架的创新方法', 'desc': '本研究提出了一种新的方法EnzyControl，用于设计具有特定底物功能的酶骨架。我们创建了一个名为EnzyBind的数据集，包含11,100对经过实验验证的酶-底物配对，以支持酶设计。EnzyControl通过利用多序列比对（MSA）注释的催化位点和相应的底物，生成条件化的酶骨架。实验结果表明，EnzyControl在结构和功能指标上表现优异，设计能力和催化效率分别提高了13%。'}}}, {'id': 'https://huggingface.co/papers/2510.26781', 'title': 'ChartAB: A Benchmark for Chart Grounding & Dense Alignment', 'url': 'https://huggingface.co/papers/2510.26781', 'abstract': 'Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel "ChartAlign Benchmark (ChartAB)" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs\' capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models.', 'score': 0, 'issue_id': 6718, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '354cf23aec454383', 'authors': ['Aniruddh Bansal', 'Davit Soselia', 'Dang Nguyen', 'Tianyi Zhou'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2510.26781.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#cv', '#reasoning'], 'emoji': '📊', 'ru': {'title': 'ChartAB: новый стандарт для оценки понимания графиков в VLM', 'desc': 'Статья представляет новый бенчмарк ChartAB для оценки способностей vision-language моделей (VLM) в задачах детального понимания графиков и диаграмм. Исследователи обнаружили, что современные VLM плохо справляются с извлечением табличных данных, локализацией элементов визуализации и распознаванием атрибутов на графиках различных типов. Бенчмарк включает двухэтапный процесс inference для оценки способности моделей сравнивать и сопоставлять элементы между несколькими графиками. Анализ выявил специфические слабости, галлюцинации и систематические ошибки восприятия в современных VLM при работе с графиками.'}, 'en': {'title': 'Enhancing Chart Understanding in Vision-Language Models', 'desc': "This paper addresses the limitations of existing vision-language models (VLMs) in accurately understanding and analyzing charts. It introduces the ChartAlign Benchmark (ChartAB), which evaluates VLMs on tasks like extracting data, localizing elements, and recognizing attributes from various charts. The benchmark includes a JSON template for calculating specific evaluation metrics and employs a two-stage inference workflow to assess the models' ability to compare elements across charts. The findings reveal significant insights into the strengths and weaknesses of current VLMs in chart grounding, indicating areas for improvement."}, 'zh': {'title': '提升图表理解的视觉语言模型评估', 'desc': '本文介绍了一种新的基准测试——"ChartAlign Benchmark (ChartAB)"，旨在评估视觉语言模型（VLMs）在图表基础任务中的表现。这些任务包括提取表格数据、定位可视化元素以及识别不同类型和复杂性的图表属性。我们设计了一个JSON模板，以便为每个基础任务计算评估指标，并引入了一个新的两阶段推理工作流程，以评估VLMs在对比两个图表时的能力。通过对多个最新VLMs的评估分析，我们揭示了它们在图表理解中的感知偏差、弱点、鲁棒性和幻觉等新见解。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (6)', '#agi (1)', '#alignment', '#architecture (3)', '#audio', '#benchmark (10)', '#cv (5)', '#data (3)', '#dataset (7)', '#diffusion (3)', '#ethics (1)', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (2)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (2)', '#multilingual', '#multimodal (6)', '#open_source (5)', '#optimization (6)', '#plp', '#rag (1)', '#reasoning (10)', '#rl (3)', '#rlhf', '#robotics (1)', '#science (2)', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic (1)', '#training (6)', '#transfer_learning (2)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-31 09:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-31 09:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-31 09:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    