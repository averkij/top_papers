
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 29 papers. October 31.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">31 октября</span> | <span id="title-articles-count">29 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-30.html">⬅️ <span id="prev-date">30.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-11-03.html">➡️ <span id="next-date">03.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'};
        let feedDateNext = {'ru': '03.11', 'en': '11/03', 'zh': '11月3日'};
        let feedDatePrev = {'ru': '30.10', 'en': '10/30', 'zh': '10月30日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.26697', 'title': 'The End of Manual Decoding: Towards Truly End-to-End Language Models', 'url': 'https://huggingface.co/papers/2510.26697', 'abstract': 'The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly "end-to-end" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.   Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from "hacking the test set"-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., "generate with low randomness") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding.', 'score': 86, 'issue_id': 6724, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '3eb223cd84c1505d', 'authors': ['Zhichao Wang', 'Dongyang Ma', 'Xinting Huang', 'Deng Cai', 'Tian Lan', 'Jiahao Xu', 'Haitao Mi', 'Xiaoying Tang', 'Yan Wang'], 'affiliations': ['Tencent AI Lab', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2510.26697.jpg', 'data': {'categories': ['#architecture', '#training', '#alignment', '#optimization', '#benchmark'], 'emoji': '🎛️', 'ru': {'title': 'Модель сама учится управлять своей случайностью', 'desc': 'Исследователи представили AutoDeco — архитектуру, которая позволяет LLM самостоятельно контролировать параметры генерации текста. Вместо ручной настройки гиперпараметров вроде temperature и top-p, модель предсказывает оптимальные значения для каждого токена в процессе генерации. Эксперименты показали, что такой подход превосходит стандартные стратегии декодирования и приближается к результатам oracle-настройки на тестовых данных. Особенно интересно, что модель научилась понимать инструкции на естественном языке типа «генерируй с низкой случайностью» и динамически адаптировать свои параметры.'}, 'en': {'title': 'Revolutionizing Decoding: AutoDeco for True End-to-End LLMs', 'desc': 'This paper critiques the term "end-to-end" as applied to large language models (LLMs), highlighting that they rely on a complex decoding process that requires manual tuning of hyperparameters. It introduces AutoDeco, a new architecture that allows LLMs to learn and control their own decoding strategies, making the process truly end-to-end. By adding lightweight heads to the transformer model, AutoDeco can dynamically predict context-specific parameters like temperature and top-p values during generation. The results show that AutoDeco outperforms traditional decoding methods and can adapt to natural language instructions, enabling more flexible and interactive text generation.'}, 'zh': {'title': '真正的端到端生成：AutoDeco的创新解码策略', 'desc': '这篇论文指出，当前大型语言模型（LLM）所称的“端到端”生成实际上依赖于不可微分的解码过程，需要手动调整超参数。论文提出了一种新架构AutoDeco，使得生成过程真正实现“端到端”，通过学习控制解码策略。该方法在标准变换器上增加了轻量级头部，能够在每一步动态预测上下文特定的温度和top-p值。实验结果表明，AutoDeco在多个基准测试中显著优于默认解码策略，并且在指令驱动的解码控制方面展现出新的能力。'}}}, {'id': 'https://huggingface.co/papers/2510.26583', 'title': 'Emu3.5: Native Multimodal Models are World Learners', 'url': 'https://huggingface.co/papers/2510.26583', 'abstract': 'We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research.', 'score': 64, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'e453f75d16182f5e', 'authors': ['Yufeng Cui', 'Honghao Chen', 'Haoge Deng', 'Xu Huang', 'Xinghang Li', 'Jirong Liu', 'Yang Liu', 'Zhuoyan Luo', 'Jinsheng Wang', 'Wenxuan Wang', 'Yueze Wang', 'Chengyuan Wang', 'Fan Zhang', 'Yingli Zhao', 'Ting Pan', 'Xianduo Li', 'Zecheng Hao', 'Wenxuan Ma', 'Zhuo Chen', 'Yulong Ao', 'Tiejun Huang', 'Zhongyuan Wang', 'Xinlong Wang'], 'affiliations': ['BAAI'], 'pdf_title_img': 'assets/pdf/title_img/2510.26583.jpg', 'data': {'categories': ['#open_source', '#optimization', '#inference', '#multimodal', '#rl', '#agi', '#cv', '#games'], 'emoji': '🌍', 'ru': {'title': 'Мультимодальная модель мира с единым предсказанием следующего токена', 'desc': 'Представлена модель Emu3.5 — крупномасштабная мультимодальная модель, которая предсказывает следующее состояние для изображений и текста одновременно. Модель обучена на корпусе из более 10 триллионов токенов, преимущественно из видео и их транскриптов, используя единую задачу предсказания следующего токена. Для ускорения инференса в 20 раз предложен метод Discrete Diffusion Adaptation (DiDA), который заменяет последовательную генерацию токенов на параллельное двунаправленное предсказание. Модель демонстрирует сильные способности в генерации изображений, мультимодальном рассуждении и моделировании мира, показывая результаты сопоставимые с Gemini 2.5 Flash.'}, 'en': {'title': 'Emu3.5: Revolutionizing Multimodal Predictions with Speed and Precision', 'desc': 'Emu3.5 is a large-scale multimodal world model that predicts future states using both vision and language inputs. It is trained on a massive dataset of over 10 trillion tokens, allowing it to generate outputs that seamlessly combine visual and textual information. The model employs a novel technique called Discrete Diffusion Adaptation (DiDA) to enhance inference speed, achieving a 20x improvement in efficiency. Emu3.5 demonstrates advanced capabilities in multimodal reasoning, world modeling, and generation tasks, outperforming existing models in various benchmarks.'}, 'zh': {'title': 'Emu3.5：多模态世界模型的未来', 'desc': 'Emu3.5是一种大规模的多模态世界模型，能够同时处理视觉和语言信息。它通过统一的下一个标记预测目标，在包含超过10万亿个标记的视觉-语言数据集上进行端到端的预训练。该模型支持交错的视觉-语言输入和输出，并通过大规模强化学习进行后训练，以增强多模态推理和生成能力。此外，Emu3.5引入了离散扩散适应（DiDA）技术，提高了推理效率，使每张图像的推理速度提高约20倍。'}}}, {'id': 'https://huggingface.co/papers/2510.26692', 'title': 'Kimi Linear: An Expressive, Efficient Attention Architecture', 'url': 'https://huggingface.co/papers/2510.26692', 'abstract': 'We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.   We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.   To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.', 'score': 51, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '0b8a017f9816e001', 'authors': ['Kimi Team', 'Yu Zhang', 'Zongyu Lin', 'Xingcheng Yao', 'Jiaxi Hu', 'Fanqing Meng', 'Chengyin Liu', 'Xin Men', 'Songlin Yang', 'Zhiyuan Li', 'Wentao Li', 'Enzhe Lu', 'Weizhou Liu', 'Yanru Chen', 'Weixin Xu', 'Longhui Yu', 'Yejie Wang', 'Yu Fan', 'Longguang Zhong', 'Enming Yuan', 'Dehao Zhang', 'Yizhi Zhang', 'T. Y. Liu', 'Haiming Wang', 'Shengjun Fang', 'Weiran He', 'Shaowei Liu', 'Yiwei Li', 'Jianlin Su', 'Jiezhong Qiu', 'Bo Pang', 'Junjie Yan', 'Zhejun Jiang', 'Weixiao Huang', 'Bohong Yin', 'Jiacheng You', 'Chu Wei', 'Zhengtao Wang', 'Chao Hong', 'Yutian Chen', 'Guanduo Chen', 'Yucheng Wang', 'Huabin Zheng', 'Feng Wang', 'Yibo Liu', 'Mengnan Dong', 'Zheng Zhang', 'Siyuan Pan', 'Wenhao Wu', 'Yuhao Wu', 'Longyu Guan', 'Jiawen Tao', 'Guohong Fu', 'Xinran Xu', 'Yuzhi Wang', 'Guokun Lai', 'Yuxin Wu', 'Xinyu Zhou', 'Zhilin Yang', 'Yulun Du'], 'affiliations': ['MoonshotAI'], 'pdf_title_img': 'assets/pdf/title_img/2510.26692.jpg', 'data': {'categories': ['#open_source', '#architecture', '#optimization', '#long_context', '#training'], 'emoji': '⚡', 'ru': {'title': 'Линейное внимание обходит полное: эффективность встречается с производительностью', 'desc': 'Представлена архитектура Kimi Linear с гибридным линейным вниманием, которая впервые превосходит полное внимание в различных сценариях — от коротких контекстов до обучения с подкреплением. В основе лежит Kimi Delta Attention (KDA) — выразительный модуль линейного внимания с улучшенным механизмом гейтинга для эффективного использования памяти RNN. Модель с 3B активными параметрами показывает превосходство над полным вниманием, одновременно сокращая использование KV-кэша на 75% и ускоряя декодирование в 6 раз для контекста в 1M токенов. Авторы открыли исходный код и веса предобученных моделей для дальнейших исследований.'}, 'en': {'title': 'Kimi Linear: Revolutionizing Attention with Efficiency and Performance', 'desc': 'Kimi Linear is a new hybrid linear attention architecture that surpasses traditional full attention methods in various contexts, including short and long sequences, as well as reinforcement learning scenarios. It features Kimi Delta Attention (KDA), which enhances the Gated DeltaNet model with a more precise gating mechanism, optimizing the use of limited RNN memory. The architecture employs a unique chunkwise algorithm that leverages Diagonal-Plus-Low-Rank (DPLR) transition matrices to significantly lower computational costs while adhering to classical delta rules. Experimental results indicate that Kimi Linear not only outperforms full Multi-Head Latent Attention (MLA) but also reduces key-value cache usage and increases decoding speed, making it a highly efficient alternative for attention-based tasks.'}, 'zh': {'title': 'Kimi Linear：超越全注意力的高效架构', 'desc': '本文介绍了一种名为Kimi Linear的混合线性注意力架构，它首次在公平比较中超越了全注意力机制，适用于短上下文、长上下文和强化学习等多种场景。其核心是Kimi Delta Attention（KDA），这是一种表达能力强的线性注意力模块，通过更细粒度的门控机制，提升了有限状态RNN内存的使用效率。我们设计的分块算法通过特殊的对角加低秩（DPLR）转移矩阵变体，实现了高效的硬件利用，显著减少了计算量，同时保持了与经典增量规则的一致性。实验结果表明，Kimi Linear在相同的训练条件下，性能显著优于全多头潜在注意力（MLA），并且在处理长输入和输出时表现出更高的效率。'}}}, {'id': 'https://huggingface.co/papers/2510.26298', 'title': 'Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in\n  Web Games', 'url': 'https://huggingface.co/papers/2510.26298', 'abstract': "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlas's web interaction capabilities using browser-based games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird, and Stein.world. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at https://atlas-game-eval.github.io.", 'score': 40, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '17bebfc38acb1c32', 'authors': ['Jingran Zhang', 'Ning Li', 'Justin Cui'], 'affiliations': ['UC San Diego', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2510.26298.jpg', 'data': {'categories': ['#agents', '#multimodal', '#reasoning', '#benchmark', '#games'], 'emoji': '🎮', 'ru': {'title': 'Атлас от OpenAI: силён в логике, слаб в реакции', 'desc': 'Исследователи протестировали новую модель ChatGPT Atlas, которая умеет взаимодействовать с веб-страницами через курсор и клавиатуру. Тестирование проводилось на браузерных играх: T-Rex Runner, Sudoku, Flappy Bird и Stein.world. Модель показала отличные результаты в логических задачах вроде судоку, решая их быстрее людей, но провалилась в динамичных играх, требующих точной реакции и контроля времени. Это демонстрирует, что AI хорошо справляется с аналитическими задачами, но пока имеет серьёзные ограничения в интерактивных средах реального времени.'}, 'en': {'title': 'Evaluating Atlas: Strong in Logic, Struggling in Real-Time Interaction', 'desc': "OpenAI's ChatGPT Atlas enhances web interaction by allowing the model to analyze webpages and perform user actions like clicking and typing. This paper evaluates Atlas's performance in browser-based games to understand its capabilities in dynamic environments. The results indicate that Atlas excels in logical reasoning tasks, completing puzzles like Sudoku faster than humans, but struggles with real-time games that require quick reflexes and precise control. These findings highlight the model's strengths in analytical tasks while revealing significant limitations in interactive scenarios that demand immediate responses."}, 'zh': {'title': 'Atlas：网页交互的新探索', 'desc': 'OpenAI的ChatGPT Atlas引入了新的网页交互能力，使模型能够分析网页、处理用户意图，并直接在浏览器中执行光标和键盘输入。尽管其在信息检索任务中的能力已得到验证，但在动态交互环境中的表现仍然较少被探索。我们通过使用基于浏览器的游戏（如谷歌的T-Rex Runner、数独、Flappy Bird和Stein.world）进行早期评估，采用游戏内表现分数作为量化指标来评估不同任务类型的性能。结果表明，Atlas在逻辑推理任务（如数独）中表现出色，完成难题的速度显著快于人类基线，但在需要精确时机和运动控制的实时游戏中表现不佳，常常无法突破初始障碍。'}}}, {'id': 'https://huggingface.co/papers/2510.15510', 'title': 'Exploring Conditions for Diffusion models in Robotic Control', 'url': 'https://huggingface.co/papers/2510.15510', 'abstract': "ORCA uses learnable task and visual prompts to adapt pre-trained text-to-image diffusion models for robotic control, achieving state-of-the-art performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While pre-trained visual representations have significantly advanced imitation learning, they are often task-agnostic as they remain frozen during policy learning. In this work, we explore leveraging pre-trained text-to-image diffusion models to obtain task-adaptive visual representations for robotic control, without fine-tuning the model itself. However, we find that naively applying textual conditions - a successful strategy in other vision domains - yields minimal or even negative gains in control tasks. We attribute this to the domain gap between the diffusion model's training data and robotic control environments, leading us to argue for conditions that consider the specific, dynamic visual information required for control. To this end, we propose ORCA, which introduces learnable task prompts that adapt to the control environment and visual prompts that capture fine-grained, frame-specific details. Through facilitating task-adaptive representations with our newly devised conditions, our approach achieves state-of-the-art performance on various robotic control benchmarks, significantly surpassing prior methods.", 'score': 36, 'issue_id': 6717, 'pub_date': '2025-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'd72f6bb74245abe8', 'authors': ['Heeseong Shin', 'Byeongho Heo', 'Dongyoon Han', 'Seungryong Kim', 'Taekyung Kim'], 'affiliations': ['KAIST AI', 'NAVER AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2510.15510.jpg', 'data': {'categories': ['#agents', '#optimization', '#benchmark', '#diffusion', '#cv', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Обучаемые промпты для адаптации диффузионных моделей к робототехнике', 'desc': 'ORCA использует обучаемые промпты для адаптации предобученных text-to-image диффузионных моделей к задачам управления роботами. Авторы обнаружили, что простое применение текстовых условий не работает из-за разрыва между данными обучения диффузионной модели и средой робототехники. Они предложили использовать обучаемые задачные промпты для адаптации к среде и визуальные промпты для захвата детальной информации из каждого кадра. Метод достигает state-of-the-art результатов на бенчмарках робототехнического управления, значительно превосходя предыдущие подходы.'}, 'en': {'title': 'Adapting Pre-trained Models for Superior Robotic Control', 'desc': 'ORCA is a method that enhances robotic control by using learnable prompts with pre-trained text-to-image diffusion models. Instead of fine-tuning the models, ORCA adapts them to specific tasks by introducing task and visual prompts that focus on the dynamic visual information needed for control. This approach addresses the limitations of traditional methods that apply textual conditions, which often do not translate well to robotic environments. As a result, ORCA achieves state-of-the-art performance on robotic control benchmarks, outperforming previous techniques.'}, 'zh': {'title': 'ORCA：智能机器人控制的新方法', 'desc': 'ORCA是一种利用可学习的任务提示和视觉提示来适应预训练的文本到图像扩散模型的方法，旨在提高机器人控制的性能。该方法不需要对模型进行微调，而是通过引入特定的动态视觉信息来解决传统方法在控制任务中效果不佳的问题。研究表明，简单地应用文本条件在机器人控制任务中效果有限，因此需要更精细的条件设计。最终，ORCA在多个机器人控制基准测试中实现了最先进的性能，显著超越了之前的方法。'}}}, {'id': 'https://huggingface.co/papers/2510.26768', 'title': 'AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions', 'url': 'https://huggingface.co/papers/2510.26768', 'abstract': 'We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. https://amo-bench.github.io/', 'score': 30, 'issue_id': 6715, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '8bfe84b563305313', 'authors': ['Shengnan An', 'Xunliang Cai', 'Xuezhi Cao', 'Xiaoyu Li', 'Yehao Lin', 'Junlin Liu', 'Xinxuan Lv', 'Dan Ma', 'Xuanlin Wang', 'Ziwen Wang', 'Shuang Zhou'], 'affiliations': ['Harbin Institute of Technology', 'Meituan', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2510.26768.jpg', 'data': {'categories': ['#math', '#benchmark', '#reasoning'], 'emoji': '🏆', 'ru': {'title': 'Олимпиадная математика ставит LLM в тупик', 'desc': 'Исследователи представили AMO-Bench — новый бенчмарк для оценки математических способностей LLM с задачами уровня Международной математической олимпиады и выше. Бенчмарк содержит 50 оригинальных задач, созданных экспертами, которые требуют только финального ответа без доказательства, что позволяет автоматически оценивать результаты. Тестирование 26 различных LLM показало, что даже лучшая модель достигла только 52.4% точности, а большинство моделей набрали менее 40%. Результаты демонстрируют значительный потенциал для улучшения математического мышления в современных языковых моделях, особенно при увеличении вычислительных ресурсов на этапе inference.'}, 'en': {'title': 'Raising the Bar for Mathematical Reasoning in LLMs', 'desc': 'AMO-Bench is a new benchmark designed to evaluate the mathematical reasoning abilities of large language models (LLMs) using challenging problems that meet International Mathematical Olympiad standards. Unlike previous benchmarks that relied on high school math competitions, AMO-Bench features 50 original problems crafted by experts to avoid data memorization issues. Each problem requires only a final answer, allowing for efficient automatic grading. Experimental results show that even the best LLMs struggle with these problems, achieving only 52.4% accuracy, indicating a significant opportunity for improvement in mathematical reasoning capabilities.'}, 'zh': {'title': 'AMO-Bench：提升语言模型数学推理能力的新基准', 'desc': '我们提出了AMO-Bench，这是一个高级数学推理基准，包含50个难度达到奥林匹克水平或更高的人为设计问题。现有的基准测试主要依赖于高中数学竞赛来评估大型语言模型（LLMs）的数学推理能力，但由于性能饱和，许多竞赛已不再有效。AMO-Bench通过确保所有问题都经过专家交叉验证，符合国际数学奥林匹克（IMO）的难度标准，并且是全新的原创问题，来引入更严格的挑战。此外，AMO-Bench中的每个问题只需提供最终答案，便于自动化和稳健的评分。'}}}, {'id': 'https://huggingface.co/papers/2510.26802', 'title': 'Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark', 'url': 'https://huggingface.co/papers/2510.26802', 'abstract': 'Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io', 'score': 28, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'b84e32b67921db6d', 'authors': ['Ziyu Guo', 'Xinyan Chen', 'Renrui Zhang', 'Ruichuan An', 'Yu Qi', 'Dongzhi Jiang', 'Xiangtai Li', 'Manyuan Zhang', 'Hongsheng Li', 'Pheng-Ann Heng'], 'affiliations': ['CUHK', 'IMIXR', 'MMLab', 'Northeastern University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2510.26802.jpg', 'data': {'categories': ['#video', '#reasoning', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'Видео-модели как визуальные движки: обещания и ограничения рассуждений', 'desc': 'Исследователи проверили, может ли современная модель генерации видео Veo-3 выступать в роли zero-shot reasoner для сложных задач визуального рассуждения. Они оценили её способности по 12 направлениям, включая пространственную, геометрическую, физическую, временную и embodied логику, создав для этого специальный бенчмарк MME-CoF. Оказалось, что модель хорошо справляется с краткосрочной пространственной согласованностью и локальной динамикой, но испытывает трудности с долгосрочными причинно-следственными связями, строгими геометрическими ограничениями и абстрактной логикой. Авторы заключают, что видео-модели пока не готовы быть самостоятельными reasoning системами, но могут служить полезным дополнением к специализированным моделям рассуждения.'}, 'en': {'title': 'Exploring the Reasoning Limits of Video Generation Models', 'desc': "This paper investigates the reasoning capabilities of advanced video generation models, particularly focusing on Veo-3. The authors assess the model's performance across various reasoning dimensions, such as spatial and temporal logic, using a newly created benchmark called MME-CoF. While the findings show that these models can handle short-term spatial coherence and local dynamics well, they struggle with long-term causal reasoning and abstract logic. Ultimately, the study concludes that while these video models are not yet reliable as independent zero-shot reasoners, they show potential as supportive tools in visual reasoning tasks."}, 'zh': {'title': '视频生成模型的推理能力研究', 'desc': '最近的视频生成模型能够生成高保真、时间一致的视频，表明它们可能编码了大量的世界知识。除了现实合成外，这些模型还表现出视觉感知、建模和操控的行为。本文通过对领先的Veo-3模型进行实证研究，评估其在12个维度上的推理能力，包括空间、几何、物理、时间和具身逻辑。研究发现，尽管当前视频模型在短期空间一致性和局部动态方面表现出色，但在长期因果推理和抽象逻辑方面仍然有限，尚不具备作为独立零-shot推理器的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2510.19949', 'title': 'Surfer 2: The Next Generation of Cross-Platform Computer Use Agents', 'url': 'https://huggingface.co/papers/2510.19949', 'abstract': 'Building agents that generalize across web, desktop, and mobile environments remains an open challenge, as prior systems rely on environment-specific interfaces that limit cross-platform deployment. We introduce Surfer 2, a unified architecture operating purely from visual observations that achieves state-of-the-art performance across all three environments. Surfer 2 integrates hierarchical context management, decoupled planning and execution, and self-verification with adaptive recovery, enabling reliable operation over long task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior systems without task-specific fine-tuning. With multiple attempts, Surfer 2 exceeds human performance on all benchmarks. These results demonstrate that systematic orchestration amplifies foundation model capabilities and enables general-purpose computer control through visual interaction alone, while calling for a next-generation vision language model to achieve Pareto-optimal cost-efficiency.', 'score': 28, 'issue_id': 6722, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'a79108149a7b774f', 'authors': ['Mathieu Andreux', 'Märt Bakler', 'Yanael Barbier', 'Hamza Benchekroun', 'Emilien Biré', 'Antoine Bonnet', 'Riaz Bordie', 'Nathan Bout', 'Matthias Brunel', 'Aleix Cambray', 'Pierre-Louis Cedoz', 'Antoine Chassang', 'Gautier Cloix', 'Ethan Connelly', 'Alexandra Constantinou', 'Ramzi De Coster', 'Hubert de la Jonquiere', 'Aurélien Delfosse', 'Maxime Delpit', 'Alexis Deprez', 'Augustin Derupti', 'Mathieu Diaz', "Shannon D'Souza", 'Julie Dujardin', 'Abai Edmund', 'Michael Eickenberg', 'Armand Fatalot', 'Wissem Felissi', 'Isaac Herring', 'Xavier Koegler', 'Erwan Le Jumeau de Kergaradec', 'Aurélien Lac', 'Maxime Langevin', 'Corentin Lauverjat', 'Antonio Loison', 'Avshalom Manevich', 'Axel Moyal', 'Axel Nguyen Kerbel', 'Marinela Parovic', 'Julien Revelle', 'Guillaume Richard', 'Mats Richter', 'Ronan Riochet', 'María Santos', 'Romain Savidan', 'Laurent Sifre', 'Maxime Theillard', 'Marc Thibault', 'Ivan Valentini', 'Tony Wu', 'Laura Yie', 'Kai Yuan', 'Jevgenij Zubovskij'], 'affiliations': ['Company'], 'pdf_title_img': 'assets/pdf/title_img/2510.19949.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#agents', '#agi', '#optimization'], 'emoji': '🏄', 'ru': {'title': 'Универсальный визуальный агент для управления любыми интерфейсами', 'desc': 'Представлена система Surfer 2 — универсальная архитектура AI-агента, которая работает исключительно на основе визуальных наблюдений и способна управлять веб-браузерами, десктопными приложениями и мобильными устройствами. Система использует иерархическое управление контекстом, разделение планирования и исполнения, а также самопроверку с адаптивным восстановлением для надёжной работы на длинных горизонтах задач. Surfer 2 достигает state-of-the-art результатов на всех бенчмарках (97.1% на WebVoyager, 69.6% на WebArena, 60.1% на OSWorld и 87.1% на AndroidWorld) без специальной дообучки под конкретные задачи. При множественных попытках система превосходит человеческую производительность, демонстрируя, что систематическая оркестрация усиливает возможности foundation models для универсального управления компьютером через визуальное взаимодействие.'}, 'en': {'title': 'Surfer 2: Unifying Cross-Platform Agent Performance through Visual Interaction', 'desc': 'Surfer 2 is a new system designed to help agents work well across different platforms like web, desktop, and mobile without needing special adjustments for each one. It uses visual observations to understand and interact with these environments, achieving high accuracy in various tasks. The system combines advanced techniques like hierarchical context management and self-verification to ensure it can handle long and complex tasks reliably. Surfer 2 not only surpasses previous systems in performance but also exceeds human capabilities in several benchmarks, highlighting the potential of unified architectures in machine learning.'}, 'zh': {'title': '跨平台智能体的视觉交互新突破', 'desc': '本文介绍了Surfer 2，这是一种统一架构的智能体，能够在网页、桌面和移动环境中进行跨平台操作。该系统仅依赖视觉观察，集成了层次化上下文管理、解耦的规划与执行以及自我验证与自适应恢复功能。Surfer 2在多个基准测试中表现优异，准确率分别为WebVoyager 97.1%、WebArena 69.6%、OSWorld 60.1%和AndroidWorld 87.1%。这些结果表明，通过系统化的协调，Surfer 2能够显著提升基础模型的能力，实现仅通过视觉交互进行通用计算机控制。'}}}, {'id': 'https://huggingface.co/papers/2510.26794', 'title': 'The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation', 'url': 'https://huggingface.co/papers/2510.26794', 'abstract': 'Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.', 'score': 26, 'issue_id': 6714, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '930f4b60c73c7768', 'authors': ['Jing Lin', 'Ruisi Wang', 'Junzhe Lu', 'Ziqi Huang', 'Guorui Song', 'Ailing Zeng', 'Xian Liu', 'Chen Wei', 'Wanqi Yin', 'Qingping Sun', 'Zhongang Cai', 'Lei Yang', 'Ziwei Liu'], 'affiliations': ['NVIDIA Research', 'Nanyang Technological University', 'SenseTime Research', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.26794.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#open_source', '#cv', '#multimodal', '#diffusion', '#transfer_learning'], 'emoji': '🎬', 'ru': {'title': 'От видео к движению: перенос знаний для генерации 3D-анимации человека', 'desc': 'Исследователи предложили новый подход к генерации 3D-движений человека, заимствуя знания из области генерации видео. Они создали крупный датасет ViMoGen-228K из 228 тысяч образцов движений, объединяющий данные motion capture, аннотированные видео и синтетические примеры от video generation моделей. Разработанная архитектура ViMoGen использует diffusion transformer с flow matching и мультимодальное conditioning для объединения разных источников данных. Также представлен новый бенчмарк MBench для детальной оценки качества движений, соответствия текстовым описаниям и способности к генерализации.'}, 'en': {'title': 'Bridging Video and Motion: A New Era for 3D Human Motion Generation', 'desc': 'This paper addresses the limitations of current 3D human motion generation (MoGen) models in generalization by leveraging insights from video generation (ViGen). The authors introduce a new dataset, ViMoGen-228K, which contains 228,000 high-quality motion samples that combine motion capture data with semantically rich annotations from web videos. They propose a novel model, ViMoGen, which utilizes a flow-matching-based diffusion transformer to integrate knowledge from both MoGen and ViGen, and also introduce a lighter version, ViMoGen-light, for improved efficiency. Finally, they create MBench, a benchmark for evaluating motion generation quality, demonstrating that their framework significantly enhances performance in various evaluation metrics.'}, 'zh': {'title': '知识转移，提升三维动作生成能力', 'desc': '本论文提出了一种新的框架，将视频生成（ViGen）领域的知识转移到三维人类动作生成（MoGen）中，以提高其泛化能力。我们引入了一个大型数据集ViMoGen-228K，包含228,000个高质量动作样本，结合了高保真光学动作捕捉数据和语义注释的动作。我们还提出了ViMoGen，一个基于流匹配的扩散变换器，通过门控多模态条件化来统一MoCap数据和ViGen模型的先验知识。最后，我们设计了MBench，一个分层基准，用于对动作质量、提示保真度和泛化能力进行细致评估，实验结果表明我们的框架在自动和人工评估中均显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2510.25992', 'title': 'Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.25992', 'abstract': 'Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical "actions". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model\'s actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.', 'score': 22, 'issue_id': 6713, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'ab88bca35e9e2ff1', 'authors': ['Yihe Deng', 'I-Hung Hsu', 'Jun Yan', 'Zifeng Wang', 'Rujun Han', 'Gufeng Zhang', 'Yanfei Chen', 'Wei Wang', 'Tomas Pfister', 'Chen-Yu Lee'], 'affiliations': ['Google Cloud', 'Google Cloud AI Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2510.25992.jpg', 'data': {'categories': ['#optimization', '#agents', '#training', '#small_models', '#reasoning', '#rl'], 'emoji': '🎯', 'ru': {'title': 'Обучение через пошаговое подражание экспертам', 'desc': 'Статья представляет метод Supervised Reinforcement Learning (SRL), который помогает небольшим языковым моделям решать сложные задачи, требующие многошагового рассуждения. Вместо точного копирования решений или редкого получения правильных ответов, SRL учит модель генерировать внутренний монолог и получает награды за сходство каждого шага с действиями эксперта. Этот подход обеспечивает более богатый обучающий сигнал даже при неправильных решениях и позволяет гибко рассуждать, следуя экспертным демонстрациям. Метод показывает лучшие результаты при комбинации с классическим reinforcement learning и успешно применяется не только к задачам рассуждения, но и к разработке программного обеспечения.'}, 'en': {'title': 'Empowering LLMs with Supervised Reinforcement Learning for Better Reasoning', 'desc': 'This paper introduces Supervised Reinforcement Learning (SRL) as a new approach to improve the reasoning capabilities of Large Language Models (LLMs). SRL reformulates problem-solving into generating a sequence of logical actions, allowing the model to engage in internal reasoning before making decisions. By providing smoother rewards based on the similarity to expert actions, SRL enhances learning even when initial attempts are incorrect. The framework not only improves performance on reasoning tasks but also generalizes well to software engineering applications, making it a versatile tool for training LLMs.'}, 'zh': {'title': '监督强化学习：提升多步推理能力的关键', 'desc': '大型语言模型（LLMs）在需要多步推理的问题上常常表现不佳。针对小规模开源模型，强化学习与可验证奖励（RLVR）在正确解答稀少的情况下效果不佳，而监督微调（SFT）则容易通过逐字模仿导致过拟合。为了解决这个问题，我们提出了监督强化学习（SRL），该框架将问题解决重新定义为生成一系列逻辑“动作”。SRL通过在每个动作之前生成内部推理独白，提供基于模型动作与专家动作相似度的平滑奖励，从而有效提升小模型的学习能力。'}}}, {'id': 'https://huggingface.co/papers/2510.26658', 'title': 'The Era of Agentic Organization: Learning to Organize with Language\n  Models', 'url': 'https://huggingface.co/papers/2510.26658', 'abstract': 'We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training.', 'score': 19, 'issue_id': 6716, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'ffa85621bc7e2c42', 'authors': ['Zewen Chi', 'Li Dong', 'Qingxiu Dong', 'Yaru Hao', 'Xun Wu', 'Shaohan Huang', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.26658.jpg', 'data': {'categories': ['#rl', '#inference', '#math', '#reasoning', '#agents', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'Асинхронное мышление: AI-агенты решают задачи параллельно и коллаборативно', 'desc': 'В статье представлена концепция асинхронного мышления (AsyncThink) для LLM, где модель организует процесс рассуждений как параллельные вычислительные структуры. Специальный протокол включает организатора, который распределяет подзадачи между воркерами, объединяет промежуточные результаты и формирует итоговое решение. Структура мышления оптимизируется через reinforcement learning, что позволяет агентам работать коллаборативно над сложными задачами. Эксперименты показывают снижение latency на 28% по сравнению с параллельным мышлением при улучшении точности на математических задачах, причём система обобщает навыки на новые задачи без дополнительного обучения.'}, 'en': {'title': 'Unlocking Collaborative Intelligence with AsyncThink', 'desc': 'This paper introduces a novel approach called asynchronous thinking (AsyncThink) for enhancing the collaborative problem-solving capabilities of AI agents. By structuring the reasoning process into concurrently executable tasks, AsyncThink allows agents to work together more efficiently than traditional methods. The proposed protocol involves an organizer that assigns tasks to worker agents, merges their findings, and generates coherent solutions, all while optimizing the process through reinforcement learning. Experimental results show that AsyncThink not only reduces inference latency by 28% but also improves accuracy in mathematical reasoning, demonstrating its ability to generalize to new tasks without extra training.'}, 'zh': {'title': '异步思维：协作解决复杂问题的新范式', 'desc': '本文提出了一种新的人工智能时代，称为代理组织，强调智能体通过协作解决复杂问题。我们引入了异步思维（AsyncThink）作为一种新的推理范式，能够将内部思维过程组织成可并发执行的结构。该思维协议允许组织者动态分配子查询给工作者，合并中间知识，生成连贯的解决方案。实验表明，AsyncThink在推理延迟上比并行思维降低了28%，同时在数学推理的准确性上有所提升，并且能够有效应对未见任务。'}}}, {'id': 'https://huggingface.co/papers/2510.26800', 'title': 'OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes', 'url': 'https://huggingface.co/papers/2510.26800', 'abstract': 'There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.', 'score': 17, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '5ddc588a37cb2d17', 'authors': ['Yukun Huang', 'Jiwen Yu', 'Yanning Zhou', 'Jianan Wang', 'Xintao Wang', 'Pengfei Wan', 'Xihui Liu'], 'affiliations': ['Astribot', 'Kuaishou Technology', 'Tencent', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.26800.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#3d', '#synthetic', '#games'], 'emoji': '🌐', 'ru': {'title': 'От панорам к реалистичным 3D-сценам с физически корректным рендерингом', 'desc': "Статья представляет OmniX — универсальную систему для создания графически готовых 3D-сцен из панорамных изображений с использованием 2D generative моделей. Ключевая идея заключается в адаптации существующих 2D генеративных prior'ов для восприятия геометрии, текстур и PBR-материалов в панорамном формате. Система использует легковесный cross-modal адаптер для решения широкого спектра задач: восприятия, генерации и completion панорам. В результате получаются 3D-сцены, пригодные для physically based rendering, релайтинга и симуляций, что открывает новые возможности для создания иммерсивных виртуальных миров."}, 'en': {'title': 'OmniX: Bridging 2D Generative Models for Realistic 3D Scene Creation', 'desc': "This paper introduces OmniX, a novel framework that enhances panorama-based 2D lifting for generating realistic 3D scenes. It utilizes advanced 2D generative models to perceive and create geometry, textures, and materials suitable for physically based rendering (PBR). Unlike traditional methods that focus solely on visual appearance, OmniX integrates intrinsic properties into the generation process. The authors also present a large-scale synthetic panorama dataset to support various panoramic vision tasks, demonstrating the model's effectiveness in creating immersive virtual environments."}, 'zh': {'title': 'OmniX：全景视觉的统一框架', 'desc': '本文探讨了构建3D场景的两种主要方法：程序生成和2D提升。我们提出了一种名为OmniX的统一框架，利用强大的2D生成模型来生成适合物理基础渲染的3D场景。与现有方法不同，OmniX不仅关注外观生成，还重视几何、纹理和材料的内在属性感知。通过构建一个大规模的合成全景数据集，我们的实验表明，OmniX在全景视觉感知和3D场景生成方面表现出色，推动了沉浸式虚拟世界的生成。'}}}, {'id': 'https://huggingface.co/papers/2510.25897', 'title': 'MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and\n  efficiency', 'url': 'https://huggingface.co/papers/2510.25897', 'abstract': 'Current text-to-image generative models are trained on large uncurated datasets to enable diverse generation capabilities. However, this does not align well with user preferences. Recently, reward models have been specifically designed to perform post-hoc selection of generated images and align them to a reward, typically user preference. This discarding of informative data together with the optimizing for a single reward tend to harm diversity, semantic fidelity and efficiency. Instead of this post-processing, we propose to condition the model on multiple reward models during training to let the model learn user preferences directly. We show that this not only dramatically improves the visual quality of the generated images but it also significantly speeds up the training. Our proposed method, called MIRO, achieves state-of-the-art performances on the GenEval compositional benchmark and user-preference scores (PickAScore, ImageReward, HPSv2).', 'score': 11, 'issue_id': 6721, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '51678d69db0f4cac', 'authors': ['Nicolas Dufour', 'Lucas Degeorge', 'Arijit Ghosh', 'Vicky Kalogeiton', 'David Picard'], 'affiliations': ['AMIAD', 'LIGM, ENPC, IP Paris, CNRS, UGE', 'LIX, École Polytechnique, IP Paris'], 'pdf_title_img': 'assets/pdf/title_img/2510.25897.jpg', 'data': {'categories': ['#alignment', '#training', '#optimization', '#dataset', '#benchmark', '#cv'], 'emoji': '🎯', 'ru': {'title': 'Обучение с учётом предпочтений пользователей напрямую', 'desc': 'Современные модели генерации изображений обучаются на огромных неотфильтрованных датасетах, что не всегда соответствует предпочтениям пользователей. Обычно для выравнивания результатов используют reward модели для постобработки и отбора сгенерированных изображений, но это снижает разнообразие и эффективность. Авторы предлагают метод MIRO, который учитывает множественные reward модели непосредственно во время обучения, а не после генерации. Это значительно улучшает визуальное качество изображений, ускоряет обучение и достигает лучших результатов на бенчмарках GenEval и пользовательских оценках.'}, 'en': {'title': 'Directly Learning User Preferences for Better Image Generation', 'desc': 'This paper introduces MIRO, a novel approach to training text-to-image generative models that directly incorporates user preferences during the training phase. Instead of relying on post-hoc reward models that can limit diversity and quality, MIRO conditions the model on multiple reward signals, allowing it to learn user preferences more effectively. This method not only enhances the visual quality of generated images but also accelerates the training process. The results demonstrate that MIRO achieves state-of-the-art performance on various benchmarks, indicating its effectiveness in aligning generated images with user expectations.'}, 'zh': {'title': '直接学习用户偏好的生成模型', 'desc': '当前的文本到图像生成模型通常在大型未整理的数据集上训练，以实现多样化的生成能力。然而，这种方法与用户的偏好并不完全一致。最近，研究者们设计了奖励模型来对生成的图像进行后处理选择，以使其符合用户偏好。我们提出了一种新的方法MIRO，在训练过程中直接根据多个奖励模型来调整模型，从而提高生成图像的视觉质量和训练效率。'}}}, {'id': 'https://huggingface.co/papers/2510.25628', 'title': 'EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\n  Health Record Analysis', 'url': 'https://huggingface.co/papers/2510.25628', 'abstract': 'Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis.', 'score': 9, 'issue_id': 6714, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '68de771aa478bf84', 'authors': ['Yusheng Liao', 'Chaoyi Wu', 'Junwei Liu', 'Shuyang Jiang', 'Pengcheng Qiu', 'Haowen Wang', 'Yun Yue', 'Shuai Zhen', 'Jian Wang', 'Qianrui Fan', 'Jinjie Gu', 'Ya Zhang', 'Yanfeng Wang', 'Yu Wang', 'Weidi Xie'], 'affiliations': ['Fudan University, Shanghai, China', 'Intelligence Computing and Sensing Laboratory, Peking University, Beijing, China', 'Intelligence Healthcare Department, AntGroup, Hangzhou, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.25628.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#healthcare', '#reasoning', '#training', '#science'], 'emoji': '🏥', 'ru': {'title': 'EHR-R1: AI-модель с продвинутыми рассуждениями для анализа медицинских карт', 'desc': 'Статья представляет EHR-R1 — специализированную LLM для анализа электронных медицинских карт с улучшенными способностями к рассуждениям. Авторы создали EHR-Ins — датасет из 300 тысяч случаев с рассуждениями и 4 миллионов обычных случаев по 42 медицинским задачам, используя инновационный метод генерации данных на основе графов мышления. Модель обучалась в несколько этапов: адаптация к медицинскому домену, улучшение способностей к рассуждениям и reinforcement learning. EHR-R1 превосходит GPT-4o более чем на 30 пунктов на бенчмарке MIMIC-Bench и показывает на 10% лучший результат по метрике AUROC в zero-shot режиме на EHRSHOT.'}, 'en': {'title': 'Revolutionizing EHR Analysis with Enhanced Reasoning Models', 'desc': 'This paper addresses the challenges of analyzing Electronic Health Records (EHRs) using large language models (LLMs). It introduces EHR-Ins, a comprehensive dataset designed for EHR reasoning, which includes 300,000 reasoning cases and 4 million non-reasoning cases across 42 tasks. The authors develop EHR-R1, a series of reasoning-enhanced LLMs that utilize a multi-stage training approach to improve their reasoning capabilities and domain knowledge for EHR analysis. The results demonstrate that EHR-R1 outperforms existing models, providing a significant advancement in the reliability and relevance of EHR analysis in clinical settings.'}, 'zh': {'title': '提升电子健康记录分析的智能推理能力', 'desc': '本论文介绍了一种新的电子健康记录（EHR）推理指令数据集EHR-Ins，包含30万个高质量推理案例和400万个非推理案例，覆盖42个不同的EHR任务。我们提出了一种基于思维图的框架，能够大规模生成高质量的推理数据。基于此，我们开发了EHR-R1，这是一系列针对EHR分析的增强推理大型语言模型，参数量高达720亿。通过多阶段训练，包括领域适应、推理增强和强化学习，EHR-R1系统性地获取了领域知识和多样的推理能力，显著提高了EHR分析的准确性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2510.26213', 'title': 'OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal\n  Document Layout Generation', 'url': 'https://huggingface.co/papers/2510.26213', 'abstract': 'Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage Coarse-to-Fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse category definitions, and 2) transferring the knowledge to a specific domain with fine-grained annotations. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M^{6}Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, models, and dataset will be publicly released.', 'score': 8, 'issue_id': 6717, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '63784033fbe5180b', 'authors': ['Hengrui Kang', 'Zhuangcheng Gu', 'Zhiyuan Zhao', 'Zichen Wen', 'Bin Wang', 'Weijia Li', 'Conghui He'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2510.26213.jpg', 'data': {'categories': ['#open_source', '#dataset', '#training', '#transfer_learning', '#data', '#architecture'], 'emoji': '📰', 'ru': {'title': 'OmniLayout: Миллион разнообразных макетов для генерации документов', 'desc': 'Исследователи представили OmniLayout-1M — первый датасет с миллионом разнообразных макетов документов, охватывающий шесть типов документов от газет до журналов, а не только академические статьи. Для работы с такими сложными данными они разработали OmniLayout-LLM — компактную модель на 0.5B параметров с двухэтапным обучением от грубого к точному. Сначала модель изучает универсальные принципы компоновки на большом датасете с общими категориями, затем переносит знания на конкретную область с детальными аннотациями. Эксперименты показали, что подход значительно превосходит как специализированные методы генерации макетов, так и современные LLM общего назначения на датасете M^6Doc.'}, 'en': {'title': 'Revolutionizing Document Layout Generation with OmniLayout-1M and OmniLayout-LLM', 'desc': 'This paper addresses the gap in document layout generation by introducing OmniLayout-1M, a large dataset containing a million diverse document layouts from various genres. The authors highlight that most existing research has focused on document layout analysis, neglecting the generative aspect, particularly for complex document types like newspapers and magazines. To tackle the challenges of generating coherent layouts, they propose OmniLayout-LLM, a 0.5 billion parameter model that employs a two-stage Coarse-to-Fine learning approach. Their experiments show that this model outperforms existing layout generation methods and general-purpose language models, demonstrating its effectiveness across multiple document types.'}, 'zh': {'title': '多样化文档布局生成的新突破', 'desc': '本文介绍了文档人工智能的快速发展，尤其是文档布局生成的研究。为了填补现有研究中多样化布局的缺失，作者创建了OmniLayout-1M数据集，包含六种常见文档类型的布局。文章还提出了OmniLayout-LLM模型，采用了两阶段的粗到细学习方法，以提高在复杂领域中的布局生成能力。实验结果表明，该方法在多个领域的表现优于现有的布局生成专家和最新的通用大语言模型。'}}}, {'id': 'https://huggingface.co/papers/2510.25779', 'title': 'Magentic Marketplace: An Open-Source Environment for Studying Agentic\n  Markets', 'url': 'https://huggingface.co/papers/2510.25779', 'abstract': 'As LLM agents advance, they are increasingly mediating economic decisions, ranging from product discovery to transactions, on behalf of users. Such applications promise benefits but also raise many questions about agent accountability and value for users. Addressing these questions requires understanding how agents behave in realistic market conditions. However, previous research has largely evaluated agents in constrained settings, such as single-task marketplaces (e.g., negotiation) or structured two-agent interactions. Real-world markets are fundamentally different: they require agents to handle diverse economic activities and coordinate within large, dynamic ecosystems where multiple agents with opaque behaviors may engage in open-ended dialogues. To bridge this gap, we investigate two-sided agentic marketplaces where Assistant agents represent consumers and Service agents represent competing businesses. To study these interactions safely, we develop Magentic-Marketplace-- a simulated environment where Assistants and Services can operate. This environment enables us to study key market dynamics: the utility agents achieve, behavioral biases, vulnerability to manipulation, and how search mechanisms shape market outcomes. Our experiments show that frontier models can approach optimal welfare-- but only under ideal search conditions. Performance degrades sharply with scale, and all models exhibit severe first-proposal bias, creating 10-30x advantages for response speed over quality. These findings reveal how behaviors emerge across market conditions, informing the design of fair and efficient agentic marketplaces.', 'score': 8, 'issue_id': 6714, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'fc91b6d1ec75911d', 'authors': ['Gagan Bansal', 'Wenyue Hua', 'Zezhou Huang', 'Adam Fourney', 'Amanda Swearngin', 'Will Epperson', 'Tyler Payne', 'Jake M. Hofman', 'Brendan Lucier', 'Chinmay Singh', 'Markus Mobius', 'Akshay Nambi', 'Archana Yadav', 'Kevin Gao', 'David M. Rothschild', 'Aleksandrs Slivkins', 'Daniel G. Goldstein', 'Hussein Mozannar', 'Nicole Immorlica', 'Maya Murad', 'Matthew Vogel', 'Subbarao Kambhampati', 'Eric Horvitz', 'Saleema Amershi'], 'affiliations': ['Arizona State University', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2510.25779.jpg', 'data': {'categories': ['#multimodal', '#ethics', '#reasoning', '#agents'], 'emoji': '🤝', 'ru': {'title': 'Агенты на рынке: скорость побеждает качество', 'desc': 'Исследователи изучают поведение LLM-агентов в двусторонних рынках, где одни агенты представляют потребителей, а другие — конкурирующие бизнесы. Для безопасного тестирования создана симулированная среда Magentic-Marketplace, позволяющая анализировать благосостояние пользователей, поведенческие искажения и уязвимости к манипуляциям. Эксперименты показали, что современные модели могут достигать оптимальных результатов только в идеальных условиях поиска, но их производительность резко падает при масштабировании. Все модели демонстрируют сильное смещение к первому предложению (first-proposal bias), создавая преимущество в 10-30 раз для скорости ответа над качеством.'}, 'en': {'title': 'Navigating the Complexities of Agentic Marketplaces', 'desc': 'This paper explores how large language model (LLM) agents can influence economic decisions in real-world markets, where they act on behalf of users. It highlights the need to understand agent behavior in complex, dynamic environments rather than simplified settings. The authors introduce a simulated environment called Magentic-Marketplace to analyze interactions between consumer-representing Assistants and competing Service agents. Their findings indicate that while advanced models can achieve good outcomes under ideal conditions, they struggle with scale and exhibit biases that can significantly impact market efficiency.'}, 'zh': {'title': '探索代理市场的公平与效率', 'desc': '随着大型语言模型（LLM）代理的发展，它们在经济决策中扮演着越来越重要的角色，包括产品发现和交易。这些应用虽然带来了好处，但也引发了关于代理责任和用户价值的许多问题。为了理解代理在现实市场条件下的行为，我们开发了Magentic-Marketplace，一个模拟环境，研究消费者代理和竞争企业代理之间的互动。实验结果表明，尽管前沿模型在理想搜索条件下可以接近最佳福利，但在规模扩大时性能急剧下降，所有模型都表现出严重的首次提案偏见，导致响应速度相对于质量有10-30倍的优势。'}}}, {'id': 'https://huggingface.co/papers/2510.25867', 'title': 'MedVLSynther: Synthesizing High-Quality Visual Question Answering from\n  Medical Documents with Generator-Verifier LMMs', 'url': 'https://huggingface.co/papers/2510.25867', 'abstract': 'Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present MedVLSynther, a rubric-guided generator-verifier framework that synthesizes high-quality multiple-choice VQA items directly from open biomedical literature by conditioning on figures, captions, and in-text references. The generator produces self-contained stems and parallel, mutually exclusive options under a machine-checkable JSON schema; a multi-stage verifier enforces essential gates (self-containment, single correct answer, clinical validity, image-text consistency), awards fine-grained positive points, and penalizes common failure modes before acceptance. Applying this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs with reinforcement learning using verifiable rewards improves accuracy across six medical VQA benchmarks, achieving averages of 55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA, outperforming strong medical LMMs. A Ablations verify that both generation and verification are necessary and that more verified data consistently helps, and a targeted contamination analysis detects no leakage from evaluation suites. By operating entirely on open literature and open-weight models, MedVLSynther offers an auditable, reproducible, and privacy-preserving path to scalable medical VQA training data.', 'score': 5, 'issue_id': 6723, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '7dd11928a2489ed4', 'authors': ['Xiaoke Huang', 'Ningsen Wang', 'Hui Liu', 'Xianfeng Tang', 'Yuyin Zhou'], 'affiliations': ['Amazon Research', 'Fudan University', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2510.25867.jpg', 'data': {'categories': ['#data', '#rl', '#multimodal', '#training', '#healthcare', '#reasoning', '#synthetic', '#open_source', '#dataset'], 'emoji': '🏥', 'ru': {'title': 'Синтетические медицинские вопросы из научной литературы для обучения AI', 'desc': 'Исследователи представили MedVLSynther — систему для автоматической генерации качественных медицинских вопросов с вариантами ответов на основе изображений и текста из открытой научной литературы. Система использует генератор вопросов и многоступенчатый верификатор, который проверяет корректность, клиническую валидность и согласованность изображения с текстом. На основе этого метода был создан датасет MedSynVQA из 13,087 вопросов по 14,803 медицинским изображениям различных модальностей. Обучение LLM с reinforcement learning на этих данных значительно улучшило точность ответов на медицинские визуальные вопросы по сравнению с существующими медицинскими моделями.'}, 'en': {'title': 'Generating Quality Medical Questions from Open Literature', 'desc': 'The paper introduces MedVLSynther, a framework designed to generate high-quality medical visual question answering (VQA) items from open biomedical literature. It utilizes a generator-verifier approach, where the generator creates multiple-choice questions based on figures and text, while the verifier ensures the questions meet specific criteria for quality and validity. By applying this method to PubMed Central, the authors produced a dataset called MedSynVQA, which includes over 13,000 questions linked to various medical images. The framework enhances the training of large multimodal models through reinforcement learning, leading to improved performance on medical VQA benchmarks.'}, 'zh': {'title': '开放文献驱动的医学视觉问答系统', 'desc': '大型多模态模型（LMMs）在回答需要图像和文本联合推理的医学问题方面越来越有能力，但由于缺乏大型、开放可用的高质量语料库，训练通用医学视觉问答系统受到阻碍。我们提出了MedVLSynther，这是一种基于规则的生成-验证框架，能够直接从开放的生物医学文献中合成高质量的多项选择视觉问答题。该框架通过条件生成图像、标题和文本引用，生成自包含的题干和相互排斥的选项，并通过多阶段验证器确保题目的有效性和一致性。通过在PubMed Central应用该流程，我们获得了MedSynVQA，包含13,087个经过审核的问题，覆盖14,803张图像，训练开放权重的LMMs显著提高了在多个医学视觉问答基准上的准确性。'}}}, {'id': 'https://huggingface.co/papers/2510.26787', 'title': 'Remote Labor Index: Measuring AI Automation of Remote Work', 'url': 'https://huggingface.co/papers/2510.26787', 'abstract': 'AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.', 'score': 4, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '4b733966c35e82f0', 'authors': ['Mantas Mazeika', 'Alice Gatti', 'Cristina Menghini', 'Udari Madhushani Sehwag', 'Shivam Singhal', 'Yury Orlovskiy', 'Steven Basart', 'Manasi Sharma', 'Denis Peskoff', 'Elaine Lau', 'Jaehyuk Lim', 'Lachlan Carroll', 'Alice Blair', 'Vinaya Sivakumar', 'Sumana Basu', 'Brad Kenstler', 'Yuntao Ma', 'Julian Michael', 'Xiaoke Li', 'Oliver Ingebretsen', 'Aditya Mehta', 'Jean Mottola', 'John Teichmann', 'Kevin Yu', 'Zaina Shaik', 'Adam Khoja', 'Richard Ren', 'Jason Hausenloy', 'Long Phan', 'Ye Htet', 'Ankit Aich', 'Tahseen Rabbani', 'Vivswan Shah', 'Andriy Novykov', 'Felix Binder', 'Kirill Chugunov', 'Luis Ramirez', 'Matias Geralnik', 'Hernán Mesura', 'Dean Lee', 'Ed-Yeremai Hernandez Cardona', 'Annette Diamond', 'Summer Yue', 'Alexandr Wang', 'Bing Liu', 'Ernesto Hernandez', 'Dan Hendrycks'], 'affiliations': ['Center for AI Safety', 'Scale AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.26787.jpg', 'data': {'categories': ['#agents', '#science', '#reasoning', '#benchmark'], 'emoji': '🏢', 'ru': {'title': 'Реальная автоматизация труда: AI пока справляется только с 2.5% задач', 'desc': 'Исследователи представили Remote Labor Index (RLI) — новый бенчмарк для оценки способности AI-агентов выполнять реальные экономически значимые задачи из разных секторов. Несмотря на впечатляющие результаты на исследовательских бенчмарках, AI-агенты показали крайне низкую производительность на практических задачах: лучший агент автоматизировал только 2.5% работы. Это исследование помогает объективно оценить реальное влияние AI на автоматизацию труда, отделяя теоретические достижения от практической применимости. Бенчмарк создаёт основу для отслеживания прогресса AI в автоматизации и помогает заинтересованным сторонам готовиться к изменениям на рынке труда.'}, 'en': {'title': "Measuring AI's Real-World Impact on Labor Automation", 'desc': "This paper introduces the Remote Labor Index (RLI), a new benchmark designed to assess the performance of AI agents in real-world economic tasks. The RLI evaluates how well AI can automate valuable projects across various sectors, providing a practical measure of AI's capabilities. The findings reveal that current AI agents perform poorly on the RLI, with the best achieving only a 2.5% automation rate. This research aims to provide empirical evidence for discussions about AI's impact on labor and help stakeholders understand and manage the implications of AI-driven automation."}, 'zh': {'title': '量化AI自动化的经济价值', 'desc': '本文介绍了一种新的指标，称为远程劳动指数（RLI），用于评估人工智能在实际经济项目中的表现。RLI是一个多行业的基准，旨在衡量AI代理在真实世界中的自动化能力。研究发现，AI代理在RLI上的表现接近最低水平，最高的自动化率仅为2.5%。这些结果为AI自动化的讨论提供了实证依据，帮助利益相关者更好地理解和应对AI驱动的劳动自动化。'}}}, {'id': 'https://huggingface.co/papers/2510.25364', 'title': 'CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction\n  Tuning for BabyLMs', 'url': 'https://huggingface.co/papers/2510.25364', 'abstract': 'This work investigates whether small-scale LMs can benefit from instruction tuning. We compare conversational and question-answering instruction tuning datasets, applied either in a merged or sequential curriculum, using decoder-only models with 100M and 140M parameters. Evaluation spans both fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and psycholinguistic correlation) settings. Results show that instruction tuning yields small but consistent gains in fine-tuning scenarios, with sequential curricula outperforming merged data; however, improvements do not consistently transfer to zero-shot tasks, suggesting a trade-off between interaction-focused adaptation and broad linguistic generalization. These results highlight both the potential and the constraints of adapting human-inspired learning strategies to low-resource LMs, and point toward hybrid, curriculum-based approaches for enhancing generalization under ecological training limits.', 'score': 4, 'issue_id': 6723, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '3a21325c8bac5a85', 'authors': ['Luca Capone', 'Alessandro Bondielli', 'Alessandro Lenci'], 'affiliations': ['CoLing Lab, Department of Philology, Literature and Linguistics, University of Pisa', 'Department of Computer Science, University of Pisa'], 'pdf_title_img': 'assets/pdf/title_img/2510.25364.jpg', 'data': {'categories': ['#low_resource', '#small_models', '#training', '#transfer_learning'], 'emoji': '🎓', 'ru': {'title': 'Обучение маленьких моделей через инструкции: потенциал и ограничения', 'desc': 'Исследование проверяет, могут ли небольшие языковые модели (100M и 140M параметров) получить пользу от instruction tuning. Сравниваются разные датасеты для обучения диалогам и ответам на вопросы, применяемые либо совместно, либо последовательно. Результаты показывают небольшие, но стабильные улучшения при файн-тюнинге, особенно при последовательном обучении, но эти улучшения не всегда переносятся на zero-shot задачи. Работа демонстрирует компромисс между адаптацией к интерактивным задачам и общей лингвистической обобщающей способностью маленьких LLM.'}, 'en': {'title': 'Unlocking Potential: Instruction Tuning for Small-Scale LMs', 'desc': 'This paper explores the impact of instruction tuning on small-scale language models (LMs) with 100M and 140M parameters. It compares the effectiveness of using conversational and question-answering datasets in either a merged or sequential curriculum format. The study finds that instruction tuning leads to slight but consistent improvements in fine-tuning tasks, particularly with sequential curricula being more effective than merged datasets. However, these enhancements do not always carry over to zero-shot tasks, indicating a balance between focused learning and broader language understanding.'}, 'zh': {'title': '小规模语言模型的指令调优潜力与限制', 'desc': '本研究探讨了小规模语言模型（LMs）是否能从指令调优中受益。我们比较了对话和问答指令调优数据集，采用合并或顺序课程的方式，使用参数为1亿和1.4亿的解码器模型。评估涵盖了微调（SuperGLUE）和零样本（BLiMP、EWoK、WUGs、实体跟踪和心理语言学相关性）设置。结果表明，指令调优在微调场景中带来了小但一致的提升，顺序课程优于合并数据，但这种提升并未在零样本任务中一致转移，表明了以互动为中心的适应与广泛语言泛化之间的权衡。'}}}, {'id': 'https://huggingface.co/papers/2510.26160', 'title': 'CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark', 'url': 'https://huggingface.co/papers/2510.26160', 'abstract': 'Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no comprehensive benchmark for this task, especially regarding wearables scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn conversations across 13 domains, including 6.2K egocentric images designed to mimic captures from wearable devices. We carefully constructed the questions to reflect real-world scenarios and challenges, including five types of image-quality issues, six question types, varying entity popularity, differing information dynamism, and different conversation turns. We design three tasks: single-source augmentation, multi-source augmentation, and multi-turn conversations -- each paired with an associated retrieval corpus and APIs for both image-KG retrieval and webpage retrieval. Our evaluation shows that straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM single- and multi-turn QA, respectively, whereas state-of-the-art industry solutions have similar quality (32%/45%), underscoring ample room for improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K participants and 5K submissions, with winning solutions improving baseline performance by 28%, highlighting its early impact on advancing the field.', 'score': 3, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '6be5612dcf7842dd', 'authors': ['Jiaqi Wang', 'Xiao Yang', 'Kai Sun', 'Parth Suresh', 'Sanat Sharma', 'Adam Czyzewski', 'Derek Andersen', 'Surya Appini', 'Arkav Banerjee', 'Sajal Choudhary', 'Shervin Ghasemlou', 'Ziqiang Guan', 'Akil Iyer', 'Haidar Khan', 'Lingkun Kong', 'Roy Luo', 'Tiffany Ma', 'Zhen Qiao', 'David Tran', 'Wenfang Xu', 'Skyler Yeatman', 'Chen Zhou', 'Gunveer Gujral', 'Yinglong Xia', 'Shane Moon', 'Nicolas Scheffer', 'Nirav Shah', 'Eun Chang', 'Yue Liu', 'Florian Metze', 'Tammy Stark', 'Zhaleh Feizollahi', 'Andrea Jessee', 'Mangesh Pujari', 'Ahmed Aly', 'Babak Damavandi', 'Rakesh Wanga', 'Anuj Kumar', 'Rohit Patel', 'Wen-tau Yih', 'Xin Luna Dong'], 'affiliations': ['FAIR, Meta', 'Meta', 'Meta Reality Labs', 'Meta Superintelligence Labs'], 'pdf_title_img': 'assets/pdf/title_img/2510.26160.jpg', 'data': {'categories': ['#multimodal', '#rag', '#benchmark', '#dataset'], 'emoji': '👓', 'ru': {'title': 'CRAG-MM: Бенчмарк для умных очков с мультимодальными диалогами', 'desc': 'Исследователи создали новый бенчмарк CRAG-MM для оценки систем, отвечающих на вопросы об окружающем мире через носимые устройства вроде умных очков. Датасет содержит 6,5 тысяч пар изображений и вопросов, включая 6,2 тысячи эгоцентрических фотографий, имитирующих съёмку от первого лица, с различными проблемами качества и типами вопросов. Текущие RAG-системы и промышленные решения достигают лишь 32-45% точности на этой задаче, что показывает большой простор для улучшений. Бенчмарк уже используется в KDD Cup 2025, где победители улучшили базовые результаты на 28%.'}, 'en': {'title': 'Advancing Multi-Modal Conversations with CRAG-MM Benchmark', 'desc': 'This paper introduces CRAG-MM, a new benchmark for Multi-Modal Retrieval-Augmented Generation (MM-RAG) specifically designed for wearable devices like smart glasses. It includes 6.5K triplets of images, questions, and answers, along with 2K visual-based multi-turn conversations across various domains. The benchmark addresses real-world challenges by incorporating diverse question types and image-quality issues, facilitating the evaluation of retrieval methods. Results indicate that current RAG approaches have significant room for improvement, as evidenced by the performance metrics achieved in the KDD Cup 2025 competition.'}, 'zh': {'title': '可穿戴设备的多模态对话新基准', 'desc': '可穿戴设备如智能眼镜正在改变人们与周围环境的互动方式，使用户能够获取视野中实体的信息。多模态检索增强生成（MM-RAG）在支持此类问题中发挥了关键作用，但目前尚缺乏针对可穿戴场景的全面基准。为了解决这一问题，我们提出了CRAG-MM——一个针对多模态多轮对话的综合RAG基准，包含6500个（图像、问题、答案）三元组和2000个基于视觉的多轮对话。我们的评估显示，现有的RAG方法在CRAG-MM的单轮和多轮问答中仅实现了32%和43%的真实度，表明该领域仍有很大的改进空间。'}}}, {'id': 'https://huggingface.co/papers/2510.26474', 'title': 'Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing', 'url': 'https://huggingface.co/papers/2510.26474', 'abstract': 'Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew effect"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.', 'score': 2, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'd7c19ec426ccd7e8', 'authors': ['Xin Guo', 'Zhiheng Xi', 'Yiwen Ding', 'Yitao Zhai', 'Xiaowei Shi', 'Xunliang Cai', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang'], 'affiliations': ['Fudan University', 'Meituan', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.26474.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#cv', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Борьба с эффектом Матфея в самообучении визуальных моделей', 'desc': 'Исследователи обнаружили проблему в процессе самообучения больших визуально-языковых моделей (LVLMs): модели хорошо справляются с простыми запросами, но испытывают трудности со сложными задачами. Этот дисбаланс усиливается с каждой итерацией обучения, создавая "эффект Матфея", когда модель всё больше фокусируется на простых задачах в ущерб сложным. Авторы предложили четыре стратегии для балансировки обучения: изменение распределения данных и пересэмплирование траекторий решения. Эксперименты на моделях Qwen2-VL и InternVL показали улучшение визуальных reasoning способностей на 3.86 пункта по сравнению с обычным самообучением.'}, 'en': {'title': 'Balancing Reasoning Skills in Vision-Language Models', 'desc': 'This paper addresses a significant challenge in self-improvement for large vision-language models (LVLMs), where the models tend to excel at simple queries but struggle with complex ones, leading to an imbalance in their reasoning capabilities. This phenomenon, referred to as the "Matthew effect," results in models focusing on easier tasks and neglecting more difficult reasoning challenges. To mitigate this issue, the authors propose four strategies aimed at re-balancing the model\'s learning process by reshaping data distribution and resampling trajectories. Their experiments show that these strategies enhance the models\' visual reasoning abilities, achieving an average improvement of 3.86 points over traditional self-improvement methods.'}, 'zh': {'title': '平衡推理能力，提升模型表现', 'desc': '自我提升已成为提高大型视觉语言模型（LVLM）推理能力的主流方法，模型通过迭代探索和学习成功的轨迹。然而，我们发现一个关键问题：模型在处理简单查询时表现出色，但在复杂查询上却力不从心。这导致了优化的不平衡，使模型更倾向于简单推理技能，而抑制了其处理复杂推理任务的能力。为了解决这个问题，我们提出了四种高效策略，以实现探索和学习过程中的头尾重平衡，从而提升视觉推理能力。'}}}, {'id': 'https://huggingface.co/papers/2510.26140', 'title': 'FullPart: Generating each 3D Part at Full Resolution', 'url': 'https://huggingface.co/papers/2510.26140', 'abstract': 'Part-based 3D generation holds great potential for various applications. Previous part generators that represent parts using implicit vector-set tokens often suffer from insufficient geometric details. Another line of work adopts an explicit voxel representation but shares a global voxel grid among all parts; this often causes small parts to occupy too few voxels, leading to degraded quality. In this paper, we propose FullPart, a novel framework that combines both implicit and explicit paradigms. It first derives the bounding box layout through an implicit box vector-set diffusion process, a task that implicit diffusion handles effectively since box tokens contain little geometric detail. Then, it generates detailed parts, each within its own fixed full-resolution voxel grid. Instead of sharing a global low-resolution space, each part in our method - even small ones - is generated at full resolution, enabling the synthesis of intricate details. We further introduce a center-point encoding strategy to address the misalignment issue when exchanging information between parts of different actual sizes, thereby maintaining global coherence. Moreover, to tackle the scarcity of reliable part data, we present PartVerse-XL, the largest human-annotated 3D part dataset to date with 40K objects and 320K parts. Extensive experiments demonstrate that FullPart achieves state-of-the-art results in 3D part generation. We will release all code, data, and model to benefit future research in 3D part generation.', 'score': 2, 'issue_id': 6713, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '37da8a03be0a8489', 'authors': ['Lihe Ding', 'Shaocong Dong', 'Yaokun Li', 'Chenjian Gao', 'Xiao Chen', 'Rui Han', 'Yihao Kuang', 'Hong Zhang', 'Bo Huang', 'Zhanpeng Huang', 'Zibin Wang', 'Dan Xu', 'Tianfan Xue'], 'affiliations': ['CUHK', 'Chongqing University', 'HKUST', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.26140.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#3d', '#dataset'], 'emoji': '🧩', 'ru': {'title': 'Полноразмерная генерация частей: каждой детали своё пространство', 'desc': 'Статья представляет FullPart — новый подход к генерации 3D-объектов по частям, который комбинирует неявные (implicit) и явные (explicit) методы представления. Сначала система определяет расположение ограничивающих боксов через диффузионный процесс с векторными токенами, а затем генерирует детализированные части, каждую в собственной вокселной сетке полного разрешения. Это позволяет создавать мелкие части с высокой детализацией, в отличие от предыдущих методов, где все части делили одну глобальную сетку низкого разрешения. Авторы также представляют PartVerse-XL — крупнейший размеченный вручную датасет 3D-частей с 40 тысячами объектов и 320 тысячами частей.'}, 'en': {'title': 'Revolutionizing 3D Part Generation with FullPart', 'desc': 'This paper introduces FullPart, a new framework for generating 3D parts that combines implicit and explicit methods. It uses an implicit diffusion process to create a bounding box layout, which is effective for low-detail shapes. Each part is then generated in its own high-resolution voxel grid, allowing for detailed and intricate designs without the limitations of shared low-resolution spaces. Additionally, the authors present PartVerse-XL, a large dataset to improve the training of models in 3D part generation, and demonstrate that FullPart achieves top performance in this area.'}, 'zh': {'title': 'FullPart：提升3D部分生成的全新框架', 'desc': '本文提出了一种名为FullPart的新框架，用于改进基于部分的3D生成。该框架结合了隐式和显式的表示方法，首先通过隐式盒子向量集扩散过程生成边界框布局。然后，为每个部分生成详细的3D形状，每个部分都在其自己的全分辨率体素网格中生成，避免了小部分占用过少体素的问题。我们还引入了中心点编码策略，以解决不同大小部分之间信息交换时的对齐问题，并发布了PartVerse-XL数据集，以支持3D部分生成的研究。'}}}, {'id': 'https://huggingface.co/papers/2510.26020', 'title': 'PORTool: Tool-Use LLM Training with Rewarded Tree', 'url': 'https://huggingface.co/papers/2510.26020', 'abstract': 'Current tool-use large language models (LLMs) are trained on static datasets, enabling them to interact with external tools and perform multi-step, tool-integrated reasoning, which produces tool-call trajectories. However, these models imitate how a query is resolved in a generic tool-call routine, thereby failing to explore possible solutions and demonstrating limited performance in an evolved, dynamic tool-call environment. In this work, we propose PORTool, a reinforcement learning (RL) method that encourages a tool-use LLM to explore various trajectories yielding the correct answer. Specifically, this method starts with generating multiple rollouts for a given query, and some of them share the first few tool-call steps, thereby forming a tree-like structure. Next, we assign rewards to each step, based on its ability to produce a correct answer and make successful tool calls. A shared step across different trajectories receives the same reward, while different steps under the same fork receive different rewards. Finally, these step-wise rewards are used to calculate fork-relative advantages, blended with trajectory-relative advantages, to train the LLM for tool use. The experiments utilize 17 tools to address user queries, covering both time-sensitive and time-invariant topics. We conduct ablation studies to systematically justify the necessity and the design robustness of step-wise rewards. Furthermore, we compare the proposed PORTool with other training approaches and demonstrate significant improvements in final accuracy and the number of tool-call steps.', 'score': 2, 'issue_id': 6729, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '82366779b41daefb', 'authors': ['Feijie Wu', 'Weiwu Zhu', 'Yuxiang Zhang', 'Soumya Chatterjee', 'Jiarong Zhu', 'Fan Mo', 'Rodin Luo', 'Jing Gao'], 'affiliations': ['Apple', 'Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2510.26020.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#rl'], 'emoji': '🌳', 'ru': {'title': 'Обучение LLM использовать инструменты через исследование альтернативных путей', 'desc': 'Современные языковые модели обучаются на статических данных для работы с внешними инструментами, но они просто имитируют готовые решения, не исследуя альтернативные варианты. Исследователи предложили метод PORTool на основе обучения с подкреплением, который побуждает LLM изучать различные траектории вызова инструментов, формирующие древовидную структуру решений. Метод назначает награды каждому шагу в зависимости от его способности привести к правильному ответу, причём общие шаги получают одинаковые награды, а разветвляющиеся — разные. Эксперименты с 17 инструментами показали значительное улучшение точности и эффективности по сравнению с другими подходами к обучению.'}, 'en': {'title': 'Empowering LLMs with Dynamic Tool Exploration through PORTool', 'desc': 'This paper introduces PORTool, a reinforcement learning method designed to enhance the performance of large language models (LLMs) when using external tools. Unlike traditional models that follow a static approach, PORTool encourages exploration of multiple tool-call trajectories to find the best solution. The method employs a tree-like structure for generating rollouts, assigning rewards based on the effectiveness of each tool call in achieving the correct answer. Experimental results show that PORTool significantly improves accuracy and efficiency in tool use compared to existing training methods.'}, 'zh': {'title': '探索多路径，提升工具使用效率', 'desc': '当前的大型语言模型（LLMs）通常在静态数据集上训练，能够与外部工具互动并进行多步骤的推理。然而，这些模型在处理动态工具调用环境时表现有限，因为它们仅模仿通用的工具调用流程，未能探索多种解决方案。本文提出了一种名为PORTool的强化学习方法，鼓励工具使用的LLM探索多条可能的路径以获得正确答案。通过为每一步分配奖励，基于其产生正确答案和成功调用工具的能力，最终显著提高了模型的准确性和工具调用步骤的数量。'}}}, {'id': 'https://huggingface.co/papers/2510.25132', 'title': 'EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme\n  Backbone Generation', 'url': 'https://huggingface.co/papers/2510.25132', 'abstract': "Designing enzyme backbones with substrate-specific functionality is a critical challenge in computational protein engineering. Current generative models excel in protein design but face limitations in binding data, substrate-specific control, and flexibility for de novo enzyme backbone generation. To address this, we introduce EnzyBind, a dataset with 11,100 experimentally validated enzyme-substrate pairs specifically curated from PDBbind. Building on this, we propose EnzyControl, a method that enables functional and substrate-specific control in enzyme backbone generation. Our approach generates enzyme backbones conditioned on MSA-annotated catalytic sites and their corresponding substrates, which are automatically extracted from curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter, a lightweight, modular component integrated into a pretrained motif-scaffolding model, allowing it to become substrate-aware. A two-stage training paradigm further refines the model's ability to generate accurate and functional enzyme structures. Experiments show that our EnzyControl achieves the best performance across structural and functional metrics on EnzyBind and EnzyBench benchmarks, with particularly notable improvements of 13\\% in designability and 13\\% in catalytic efficiency compared to the baseline models. The code is released at https://github.com/Vecteur-libre/EnzyControl.", 'score': 2, 'issue_id': 6714, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '289ff7b39a55aab4', 'authors': ['Chao Song', 'Zhiyuan Liu', 'Han Huang', 'Liang Wang', 'Qiong Wang', 'Jianyu Shi', 'Hui Yu', 'Yihang Zhou', 'Yang Zhang'], 'affiliations': ['Institute of Automation at CAS', 'National University of Singapore', 'Northwestern Polytechnical University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.25132.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#data', '#dataset', '#training'], 'emoji': '🧬', 'ru': {'title': 'Генерация ферментов под конкретные субстраты с контролем каталитических свойств', 'desc': 'Исследователи представили EnzyControl — метод для генерации структур ферментов с учётом специфичности к определённым субстратам. В основе подхода лежит датасет EnzyBind с 11,100 экспериментально подтверждёнными парами фермент-субстрат и лёгкий модульный компонент EnzyAdapter, встроенный в предобученную модель. Модель генерирует backbone ферментов с учётом каталитических сайтов и соответствующих субстратов, используя двухэтапную схему обучения. Эксперименты показали улучшение на 13% в способности к дизайну и каталитической эффективности по сравнению с базовыми моделями.'}, 'en': {'title': 'Revolutionizing Enzyme Design with EnzyControl', 'desc': 'This paper presents EnzyBind, a new dataset containing 11,100 validated enzyme-substrate pairs to enhance enzyme design in computational protein engineering. The authors introduce EnzyControl, a method that allows for the generation of enzyme backbones that are specifically tailored to bind certain substrates. EnzyControl utilizes a lightweight component called EnzyAdapter, which integrates with a pretrained model to improve substrate awareness during backbone generation. The results demonstrate significant improvements in designability and catalytic efficiency, outperforming existing models on benchmark tests.'}, 'zh': {'title': '智能设计特定功能酶骨架的创新方法', 'desc': '本研究提出了一种新的方法EnzyControl，用于设计具有特定底物功能的酶骨架。我们创建了一个名为EnzyBind的数据集，包含11,100对经过实验验证的酶-底物配对，以支持酶设计。EnzyControl通过利用多序列比对（MSA）注释的催化位点和相应的底物，生成条件化的酶骨架。实验结果表明，EnzyControl在结构和功能指标上表现优异，设计能力和催化效率分别提高了13%。'}}}, {'id': 'https://huggingface.co/papers/2510.22282', 'title': 'CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language\n  Models via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2510.22282', 'abstract': 'Harnessing publicly available, large-scale web data, such as street view and satellite imagery, urban socio-economic sensing is of paramount importance for achieving global sustainable development goals. With the emergence of Large Vision-Language Models (LVLMs), new opportunities have arisen to solve this task by treating it as a multi-modal perception and understanding problem. However, recent studies reveal that LVLMs still struggle with accurate and interpretable socio-economic predictions from visual data. To address these limitations and maximize the potential of LVLMs, we introduce CityRiSE, a novel framework for Reasoning urban Socio-Economic status in LVLMs through pure reinforcement learning (RL). With carefully curated multi-modal data and verifiable reward design, our approach guides the LVLM to focus on semantically meaningful visual cues, enabling structured and goal-oriented reasoning for generalist socio-economic status prediction. Experiments demonstrate that CityRiSE with emergent reasoning process significantly outperforms existing baselines, improving both prediction accuracy and generalization across diverse urban contexts, particularly for prediction on unseen cities and unseen indicators. This work highlights the promise of combining RL and LVLMs for interpretable and generalist urban socio-economic sensing.', 'score': 1, 'issue_id': 6726, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': 'b412a4bf690e113f', 'authors': ['Tianhui Liu', 'Hetian Pang', 'Xin Zhang', 'Jie Feng', 'Yong Li', 'Pan Hui'], 'affiliations': ['Department of Electronic Engineering, BNRist, Tsinghua University', 'Information Hub, The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2510.22282.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#rl', '#dataset', '#multimodal', '#training'], 'emoji': '🌆', 'ru': {'title': 'Обучение AI рассуждать о благосостоянии городов по фотографиям', 'desc': 'Исследователи представили CityRiSE — новый фреймворк для предсказания социально-экономического статуса городов с помощью больших мультимодальных моделей (LVLMs). Метод использует reinforcement learning для обучения модели фокусироваться на значимых визуальных признаках в спутниковых снимках и панорамах улиц. Система показывает структурированное рассуждение и значительно превосходит существующие подходы в точности предсказаний, особенно на новых городах и показателях. Работа демонстрирует перспективность комбинации RL и мультимодальных моделей для интерпретируемого анализа городской среды.'}, 'en': {'title': 'Empowering Urban Insights with Reinforcement Learning and Vision-Language Models', 'desc': 'This paper presents CityRiSE, a new framework that enhances Large Vision-Language Models (LVLMs) for predicting urban socio-economic status using reinforcement learning (RL). The authors highlight that while LVLMs can process multi-modal data, they often fail to make accurate and interpretable predictions from visual inputs. CityRiSE addresses these challenges by employing a carefully designed reward system that encourages the model to focus on important visual features. Experimental results show that CityRiSE significantly improves prediction accuracy and generalization, especially in unfamiliar urban environments and with new socio-economic indicators.'}, 'zh': {'title': '利用强化学习提升城市社会经济状态预测', 'desc': '本论文提出了一种新的框架CityRiSE，用于通过强化学习（RL）推理城市社会经济状态。我们利用公开的大规模网络数据，如街景和卫星图像，来解决多模态感知和理解的问题。研究表明，现有的大型视觉语言模型（LVLMs）在从视觉数据中进行准确和可解释的社会经济预测方面存在困难。通过精心设计的多模态数据和可验证的奖励机制，CityRiSE能够引导LVLM关注语义上有意义的视觉线索，从而提高预测的准确性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2510.21970', 'title': 'Performance Trade-offs of Optimizing Small Language Models for\n  E-Commerce', 'url': 'https://huggingface.co/papers/2510.21970', 'abstract': 'Large Language Models (LLMs) offer state-of-the-art performance in natural language understanding and generation tasks. However, the deployment of leading commercial models for specialized tasks, such as e-commerce, is often hindered by high computational costs, latency, and operational expenses. This paper investigates the viability of smaller, open-weight models as a resource-efficient alternative. We present a methodology for optimizing a one-billion-parameter Llama 3.2 model for multilingual e-commerce intent recognition. The model was fine-tuned using Quantized Low-Rank Adaptation (QLoRA) on a synthetically generated dataset designed to mimic real-world user queries. Subsequently, we applied post-training quantization techniques, creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results demonstrate that the specialized 1B model achieves 99% accuracy, matching the performance of the significantly larger GPT-4.1 model. A detailed performance analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF formats on a CPU achieved a speedup of up to 18x in inference throughput and a reduction of over 90% in RAM consumption compared to the FP16 baseline. We conclude that small, properly optimized open-weight models are not just a viable but a more suitable alternative for domain-specific applications, offering state-of-the-art accuracy at a fraction of the computational cost.', 'score': 1, 'issue_id': 6730, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '3a7f11fd6bfdbb24', 'authors': ['Josip Tomo Licardo', 'Nikola Tankovic'], 'affiliations': ['Faculty of Informatics Juraj Dobrila University of Pula Zagrebačka 30 52100 Pula, Croatia'], 'pdf_title_img': 'assets/pdf/title_img/2510.21970.jpg', 'data': {'categories': ['#training', '#low_resource', '#small_models', '#open_source', '#optimization', '#multilingual', '#inference'], 'emoji': '💡', 'ru': {'title': 'Маленькие модели — большие возможности в электронной коммерции', 'desc': 'В статье рассматривается возможность использования меньших моделей с открытыми весами для задач в области электронной коммерции. Исследователи оптимизировали модель Llama 3.2 с одним миллиардом параметров для распознавания намерений пользователей на нескольких языках. Модель была дообучена с использованием метода Quantized Low-Rank Adaptation (QLoRA) и подвергнута пост-тренировочной квантизации для оптимизации под GPU и CPU. Результаты показали, что такая модель может достигать точности, сопоставимой с более крупными моделями, при значительно меньших вычислительных затратах.'}, 'en': {'title': 'Optimizing Small Models for Big Results in E-commerce', 'desc': 'This paper explores the use of smaller, open-weight models as efficient alternatives to large language models (LLMs) for specialized tasks like e-commerce. It focuses on optimizing a one-billion-parameter Llama 3.2 model for multilingual intent recognition through techniques like Quantized Low-Rank Adaptation (QLoRA) and post-training quantization. The results show that this optimized model can achieve 99% accuracy, comparable to larger models like GPT-4.1, while significantly reducing computational costs and resource usage. The findings highlight the importance of hardware considerations in model deployment, demonstrating that smaller models can outperform larger ones in specific applications.'}, 'zh': {'title': '小型优化模型，电子商务的理想选择', 'desc': '大型语言模型（LLMs）在自然语言理解和生成任务中表现出色，但在特定任务（如电子商务）中应用时，常因计算成本高、延迟和运营费用而受限。本文探讨了较小的开放权重模型作为资源高效替代方案的可行性。我们提出了一种优化一亿参数的Llama 3.2模型的方法，专注于多语言电子商务意图识别，并使用量化低秩适应（QLoRA）对合成数据集进行了微调。结果表明，经过优化的1B模型在准确率上达到了99%，与更大模型GPT-4.1的性能相当，同时在计算成本上更具优势。'}}}, {'id': 'https://huggingface.co/papers/2510.20976', 'title': 'L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks', 'url': 'https://huggingface.co/papers/2510.20976', 'abstract': 'Large language models have demonstrated remarkable reasoning capabilities across diverse natural language tasks. However, comparable breakthroughs in scientific discovery are more limited, because understanding complex physical phenomena demands multifaceted representations far beyond language alone. A compelling example is the design of functional materials such as MOFs-critical for a range of impactful applications like carbon capture and hydrogen storage. Navigating their vast and intricate design space in language-based representations interpretable by LLMs is challenging due to the numerous possible three-dimensional atomic arrangements and strict reticular rules of coordination geometry and topology. Despite promising early results in LLM-assisted discovery for simpler materials systems, MOF design remains heavily reliant on tacit human expertise rarely codified in textual information alone. To overcome this barrier, we introduce L2M3OF, the first multimodal LLM for MOFs. L2M3OF integrates crystal representation learning with language understanding to process structural, textual, and knowledge modalities jointly. L2M3OF employs a pre-trained crystal encoder with a lightweight projection layer to compress structural information into a token space, enabling efficient alignment with language instructions. To facilitate training and evaluation, we curate a structure-property-knowledge database of crystalline materials and benchmark L2M3OF against state-of-the-art closed-source LLMs such as GPT-5, Gemini-2.5-Pro and DeepSeek-R1. Experiments show that L2M3OF outperforms leading text-based closed-source LLMs in property prediction and knowledge generation tasks, despite using far fewer parameters. These results highlight the importance of multimodal approaches for porous material understanding and establish L2M3OF as a foundation for next-generation AI systems in materials discovery.', 'score': 1, 'issue_id': 6730, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'c9c897e2b434d4a5', 'authors': ['Jiyu Cui', 'Fang Wu', 'Haokai Zhao', 'Minggao Feng', 'Xenophon Evangelopoulos', 'Andrew I. Cooper', 'Yejin Choi'], 'affiliations': ['Department of Chemistry, University of Liverpool', 'Department of Computer Science, University of Stanford', 'Leverhulme Research Centre for Functional Materials Design, University of Liverpool', 'School of Computer Science and Engineering, University of New South Wales'], 'pdf_title_img': 'assets/pdf/title_img/2510.20976.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#science', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Мультимодальный LLM для дизайна пористых материалов', 'desc': 'Исследователи представили L2M3OF — первую мультимодальную языковую модель для работы с металл-органическими каркасами (MOF), которые критически важны для улавливания углерода и хранения водорода. Модель объединяет представление кристаллических структур с пониманием естественного языка, обрабатывая структурную, текстовую информацию и базу знаний совместно. Для этого используется предобученный энкодер кристаллов с легковесным проекционным слоем, который сжимает структурную информацию в токены для эффективного выравнивания с языковыми инструкциями. Эксперименты показали, что L2M3OF превосходит ведущие текстовые closed-source LLM вроде GPT-5 и Gemini-2.5-Pro в задачах предсказания свойств и генерации знаний, используя при этом значительно меньше параметров.'}, 'en': {'title': 'Revolutionizing MOF Discovery with Multimodal Learning', 'desc': "This paper presents L2M3OF, a novel multimodal large language model designed specifically for the discovery of metal-organic frameworks (MOFs). Unlike traditional language models that rely solely on text, L2M3OF integrates crystal representation learning to handle complex structural data alongside language inputs. By compressing structural information into a token space, it allows for effective communication between different data modalities, enhancing the model's ability to predict material properties and generate relevant knowledge. The results demonstrate that L2M3OF surpasses existing state-of-the-art models in performance while utilizing significantly fewer parameters, showcasing the potential of multimodal approaches in advancing materials science."}, 'zh': {'title': '多模态模型助力材料发现的未来', 'desc': '大型语言模型在自然语言任务中展现了出色的推理能力，但在科学发现方面的突破相对有限。设计功能材料（如金属有机框架材料）需要超越语言的复杂物理现象理解。我们提出了L2M3OF，这是第一个用于金属有机框架的多模态大型语言模型，结合了晶体表示学习和语言理解。实验表明，L2M3OF在属性预测和知识生成任务中优于现有的文本基础闭源大型语言模型，展示了多模态方法在材料理解中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2510.26781', 'title': 'ChartAB: A Benchmark for Chart Grounding & Dense Alignment', 'url': 'https://huggingface.co/papers/2510.26781', 'abstract': 'Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel "ChartAlign Benchmark (ChartAB)" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs\' capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models.', 'score': 0, 'issue_id': 6718, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '354cf23aec454383', 'authors': ['Aniruddh Bansal', 'Davit Soselia', 'Dang Nguyen', 'Tianyi Zhou'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2510.26781.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#cv', '#reasoning'], 'emoji': '📊', 'ru': {'title': 'ChartAB: новый стандарт для оценки понимания графиков в VLM', 'desc': 'Статья представляет новый бенчмарк ChartAB для оценки способностей vision-language моделей (VLM) в задачах детального понимания графиков и диаграмм. Исследователи обнаружили, что современные VLM плохо справляются с извлечением табличных данных, локализацией элементов визуализации и распознаванием атрибутов на графиках различных типов. Бенчмарк включает двухэтапный процесс inference для оценки способности моделей сравнивать и сопоставлять элементы между несколькими графиками. Анализ выявил специфические слабости, галлюцинации и систематические ошибки восприятия в современных VLM при работе с графиками.'}, 'en': {'title': 'Enhancing Chart Understanding in Vision-Language Models', 'desc': "This paper addresses the limitations of existing vision-language models (VLMs) in accurately understanding and analyzing charts. It introduces the ChartAlign Benchmark (ChartAB), which evaluates VLMs on tasks like extracting data, localizing elements, and recognizing attributes from various charts. The benchmark includes a JSON template for calculating specific evaluation metrics and employs a two-stage inference workflow to assess the models' ability to compare elements across charts. The findings reveal significant insights into the strengths and weaknesses of current VLMs in chart grounding, indicating areas for improvement."}, 'zh': {'title': '提升图表理解的视觉语言模型评估', 'desc': '本文介绍了一种新的基准测试——"ChartAlign Benchmark (ChartAB)"，旨在评估视觉语言模型（VLMs）在图表基础任务中的表现。这些任务包括提取表格数据、定位可视化元素以及识别不同类型和复杂性的图表属性。我们设计了一个JSON模板，以便为每个基础任务计算评估指标，并引入了一个新的两阶段推理工作流程，以评估VLMs在对比两个图表时的能力。通过对多个最新VLMs的评估分析，我们揭示了它们在图表理解中的感知偏差、弱点、鲁棒性和幻觉等新见解。'}}}, {'id': 'https://huggingface.co/papers/2510.24992', 'title': 'POWSM: A Phonetic Open Whisper-Style Speech Foundation Model', 'url': 'https://huggingface.co/papers/2510.24992', 'abstract': 'Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks. POWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing. Our model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR. Our training data, code and models are released to foster open science.', 'score': 0, 'issue_id': 6731, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'eaa83dc5d5e43372', 'authors': ['Chin-Jou Li', 'Kalvin Chang', 'Shikhar Bharadwaj', 'Eunjung Yeo', 'Kwanghee Choi', 'Jian Zhu', 'David Mortensen', 'Shinji Watanabe'], 'affiliations': ['Carnegie Mellon University', 'University of British Columbia', 'University of California, Berkeley', 'University of Texas, Austin'], 'pdf_title_img': 'assets/pdf/title_img/2510.24992.jpg', 'data': {'categories': ['#open_source', '#dataset', '#training', '#multimodal', '#audio', '#low_resource'], 'emoji': '🗣️', 'ru': {'title': 'Универсальная модель для всех фонетических задач в одном флаконе', 'desc': 'В статье представлена POWSM — первая унифицированная модель для работы с фонетическими задачами в обработке речи. Модель способна одновременно выполнять распознавание речи (ASR), распознавание фонем, конвертацию графем в фонемы (G2P) и обратно (P2G). POWSM работает лучше или на уровне специализированных моделей аналогичного размера, таких как Wav2Vec2Phoneme и ZIPA, при этом решая несколько задач одновременно. Модель основана на архитектуре Whisper и обеспечивает бесшовное преобразование между аудио, текстом и фонемами, что особенно полезно для низкоресурсных языков.'}, 'en': {'title': 'POWSM: Unifying Phonetic Tasks for Enhanced Speech Processing', 'desc': 'This paper presents POWSM, a unified framework for phonetic tasks in spoken language processing, including automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme (G2P), and phoneme-to-grapheme (P2G) conversion. Unlike previous approaches that treated these tasks separately, POWSM allows for simultaneous processing, enhancing efficiency and performance. The model demonstrates competitive results against specialized PR models while also supporting G2P and P2G tasks. By releasing the training data, code, and models, the authors aim to promote open science and collaboration in the field.'}, 'zh': {'title': '统一音素处理的创新框架', 'desc': '最近在语音处理领域取得了显著进展，尤其是在自动语音识别（ASR）、音素识别（PR）、字形到音素转换（G2P）和音素到字形转换（P2G）等任务上。尽管这些任务在概念上相似，但它们通常是孤立研究的，依赖于特定的架构和数据集。本文介绍了POWSM（音素开放式低语音模型），这是第一个能够联合执行多种音素相关任务的统一框架。POWSM实现了音频、文本（字形）和音素之间的无缝转换，为通用和低资源的语音处理开辟了新可能。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (7)', '#agi (2)', '#alignment (2)', '#architecture (5)', '#audio (1)', '#benchmark (14)', '#cv (6)', '#data (4)', '#dataset (12)', '#diffusion (3)', '#ethics (1)', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare (2)', '#inference (3)', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource (3)', '#machine_translation', '#math (2)', '#multilingual (1)', '#multimodal (10)', '#open_source (8)', '#optimization (11)', '#plp', '#rag (1)', '#reasoning (14)', '#rl (6)', '#rlhf', '#robotics (1)', '#science (3)', '#security', '#small_models (3)', '#story_generation', '#survey', '#synthetic (2)', '#training (14)', '#transfer_learning (3)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-11-01 12:45',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-11-01 12:45')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-11-01 12:45')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    