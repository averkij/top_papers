{
    "date": {
        "ru": "31 марта",
        "en": "March 31",
        "zh": "3月31日"
    },
    "time_utc": "2025-03-31 07:11",
    "weekday": 0,
    "issue_id": 2976,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.22675",
            "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation",
            "url": "https://huggingface.co/papers/2503.22675",
            "abstract": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose ReaRec, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\\%-50\\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.",
            "score": 18,
            "issue_id": 2971,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 марта",
                "en": "March 28",
                "zh": "3月28日"
            },
            "hash": "092ab2fa75277891",
            "authors": [
                "Jiakai Tang",
                "Sunhao Dai",
                "Teng Shi",
                "Jun Xu",
                "Xu Chen",
                "Wen Chen",
                "Wu Jian",
                "Yuning Jiang"
            ],
            "affiliations": [
                "Alibaba Group, Beijing, China",
                "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22675.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#dataset",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Многошаговое рассуждение для улучшения рекомендаций",
                    "desc": "ReaRec - это новый фреймворк для систем рекомендаций, использующий многошаговое рассуждение во время вывода для улучшения представления пользователей. Он применяет авторегрессивную подачу последнего скрытого состояния последовательности в рекомендательную систему, используя специальные эмбеддинги позиций рассуждения. Авторы также предлагают два метода обучения на основе рассуждений: Ensemble Reasoning Learning (ERL) и Progressive Reasoning Learning (PRL). Эксперименты показывают, что ReaRec значительно повышает эффективность различных архитектур последовательных рекомендательных систем."
                },
                "en": {
                    "title": "ReaRec: Elevating Sequential Recommendations with Multi-Step Reasoning",
                    "desc": "This paper introduces ReaRec, a novel framework for Sequential Recommendation (SeqRec) that enhances user representation through multi-step reasoning. Traditional methods often rely on a single forward computation, which limits their ability to capture the evolving nature of user preferences and understand less popular items. ReaRec addresses these limitations by using autoregressive techniques and special embeddings to improve the inference process. The proposed framework, along with two learning methods, shows significant performance improvements across various datasets, suggesting a new direction for research in recommendation systems."
                },
                "zh": {
                    "title": "ReaRec：提升顺序推荐的推理能力",
                    "desc": "顺序推荐（SeqRec）旨在通过捕捉用户历史交互中的顺序模式来预测下一个项目，这在许多现实世界的推荐系统中起着关键作用。现有方法主要采用直接的前向计算范式，最终的隐藏状态作为用户表示，但这种方法在建模用户偏好的复杂演变方面存在局限。为了解决这个问题，我们提出了ReaRec，这是第一个用于推荐系统的推理时计算框架，通过隐式多步推理增强用户表示。我们的实验表明，ReaRec显著提高了多个顺序推荐模型的性能，开辟了推理时计算的新研究方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22230",
            "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from\n  Human Feedback",
            "url": "https://huggingface.co/papers/2503.22230",
            "abstract": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.",
            "score": 13,
            "issue_id": 2972,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 марта",
                "en": "March 28",
                "zh": "3月28日"
            },
            "hash": "a994668c51ac51c4",
            "authors": [
                "Wei Shen",
                "Guanlin Liu",
                "Zheng Wu",
                "Ruofei Zhu",
                "Qingping Yang",
                "Chao Xin",
                "Yu Yue",
                "Lin Yan"
            ],
            "affiliations": [
                "ByteDance Seed"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22230.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#data",
                    "#reasoning",
                    "#alignment",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Усовершенствование RLHF: данные и разнообразие ответов как ключ к успеху",
                    "desc": "Статья исследует важность конструирования данных в обучении с подкреплением на основе обратной связи от человека (RLHF) для больших языковых моделей. Авторы предлагают гибридную систему вознаграждений, сочетающую верификаторы задач рассуждения (RTV) и генеративную модель вознаграждений (GenRM), для смягчения проблемы обмана вознаграждений. Они также вводят метод Pre-PPO для поддержания разнообразия ответов и повышения эффективности обучения. Результаты показывают, что предложенные методы значительно улучшают производительность RLHF, особенно в математических задачах и задачах кодирования."
                },
                "en": {
                    "title": "Enhancing RLHF: Bridging Data Gaps for Better AI Alignment",
                    "desc": "This paper focuses on improving Reinforcement Learning from Human Feedback (RLHF) for large language models by addressing issues in prompt-data construction. It identifies problems like reward hacking and reduced response diversity that hinder RLHF performance. The authors propose a hybrid reward system that combines reasoning task verifiers (RTV) with a generative reward model (GenRM) to counteract these issues. Additionally, they introduce a new prompt-selection method called Pre-PPO and emphasize the importance of prioritizing mathematical and coding tasks during training to enhance overall model performance."
                },
                "zh": {
                    "title": "优化人类反馈的强化学习方法",
                    "desc": "强化学习中的人类反馈（RLHF）对于使大型语言模型与人类偏好对齐至关重要。尽管近期研究集中在算法改进上，但提示数据构建的重要性却被忽视。本文探讨了RLHF性能扩展中的数据驱动瓶颈，特别是奖励黑客和响应多样性下降的问题。我们提出了一种混合奖励系统，结合推理任务验证器（RTV）和生成奖励模型（GenRM），以减轻奖励黑客现象，并提出了一种新颖的提示选择方法Pre-PPO，以保持响应多样性并增强学习效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22194",
            "title": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2503.22194",
            "abstract": "We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.",
            "score": 12,
            "issue_id": 2971,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 марта",
                "en": "March 28",
                "zh": "3月28日"
            },
            "hash": "6d93cc886c16f9b0",
            "authors": [
                "Yunhong Min",
                "Daehyeon Choi",
                "Kyeongmin Yeo",
                "Jihyun Lee",
                "Minhyuk Sung"
            ],
            "affiliations": [
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22194.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "Точное управление 3D ориентацией объектов при генерации изображений",
                    "desc": "ORIGEN - это первый метод для определения 3D ориентации объектов при генерации изображений из текста без предварительного обучения. Он использует предобученную дискриминативную модель для оценки 3D ориентации и одношаговую генеративную модель текст-изображение. Вместо градиентного спуска применяется семплирование на основе динамики Ланжевена с добавлением случайного шума. Эксперименты показывают превосходство ORIGEN над другими методами по количественным метрикам и оценкам пользователей."
                },
                "en": {
                    "title": "Revolutionizing 3D Orientation in Text-to-Image Generation with ORIGEN",
                    "desc": "ORIGEN is a novel method that enables zero-shot 3D orientation grounding in text-to-image generation, allowing for better control over how objects are oriented in three-dimensional space. Unlike previous methods that focused on 2D positioning, ORIGEN utilizes a reward-guided sampling approach that leverages a pretrained model for estimating 3D orientations. This method incorporates Langevin dynamics to enhance image realism while maintaining effective sampling, requiring minimal code changes. Experimental results demonstrate that ORIGEN surpasses existing training-based and test-time guidance techniques in both quantitative metrics and user evaluations."
                },
                "zh": {
                    "title": "ORIGEN：文本到图像生成中的3D方向定位新方法",
                    "desc": "我们介绍了ORIGEN，这是首个用于文本到图像生成中实现3D方向定位的零样本方法。以往的研究主要集中在2D定位上，缺乏对3D方向的控制。为了解决这个问题，我们提出了一种基于奖励引导的采样方法，利用预训练的判别模型进行3D方向估计，并结合一步文本到图像生成流模型。实验结果表明，ORIGEN在定量指标和用户研究中均优于基于训练和测试时引导的方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21614",
            "title": "A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond",
            "url": "https://huggingface.co/papers/2503.21614",
            "abstract": "Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.",
            "score": 10,
            "issue_id": 2976,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 марта",
                "en": "March 27",
                "zh": "3月27日"
            },
            "hash": "805b7cd4ec307c34",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#survey",
                    "#agents"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оптимизация рассуждений в крупных языковых моделях",
                    "desc": "Этот обзор посвящен повышению эффективности рассуждений в крупных моделях рассуждений (LRM). Авторы рассматривают проблему избыточно длинных цепочек рассуждений, которые часто содержат повторяющийся контент и излишний анализ. Обсуждаются методы улучшения эффективности на всех этапах жизненного цикла LRM - от предварительного обучения до вывода. Статья также предлагает перспективные направления для будущих исследований в этой быстро развивающейся области."
                },
                "en": {
                    "title": "Enhancing Efficiency in Large Reasoning Models",
                    "desc": "This paper discusses the challenges faced by Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI o1, particularly their tendency to generate long and redundant reasoning processes. These inefficiencies can complicate training and real-world applications, especially where efficient use of tokens is crucial. The authors review various strategies aimed at enhancing reasoning efficiency throughout the LRM lifecycle, from pretraining to inference. They also provide a GitHub repository to track advancements in this field, aiming to inspire further research and innovation."
                },
                "zh": {
                    "title": "提升推理效率，助力大型模型发展",
                    "desc": "最近的大型推理模型（LRMs），如DeepSeek-R1和OpenAI o1，通过扩展推理链（CoT）的长度在推理过程中取得了显著的性能提升。然而，它们往往生成过长的推理过程，内容冗余（例如，重复定义）、对简单问题的过度分析，以及对复杂任务的多条推理路径的表面探索。这种低效性在训练、推理和实际应用中引发了重大挑战，尤其是在代理系统中，令牌经济至关重要。本文综述了改善LRMs推理效率的最新努力，识别了常见的低效模式，并探讨了未来的研究方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21821",
            "title": "PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem Solving",
            "url": "https://huggingface.co/papers/2503.21821",
            "abstract": "We introduce PHYSICS, a comprehensive benchmark for university-level physics problem solving. It contains 1297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements.",
            "score": 9,
            "issue_id": 2972,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 марта",
                "en": "March 26",
                "zh": "3月26日"
            },
            "hash": "5254bbde255c8669",
            "authors": [
                "Kaiyue Feng",
                "Yilun Zhao",
                "Yixin Liu",
                "Tianyu Yang",
                "Chen Zhao",
                "John Sous",
                "Arman Cohan"
            ],
            "affiliations": [
                "New York University",
                "Notre Dame University",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21821.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rag",
                    "#benchmark",
                    "#science"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Новый вызов для ИИ: решение сложных задач по физике",
                    "desc": "PHYSICS - это новый набор данных для оценки способностей моделей машинного обучения решать задачи по физике университетского уровня. Он включает 1297 экспертно размеченных задач по шести основным разделам физики. Даже самые продвинутые языковые модели показывают ограниченную точность на этом наборе данных. Авторы провели анализ ошибок и исследовали различные стратегии промптинга и дополнения знаний с помощью RAG для определения путей улучшения моделей."
                },
                "en": {
                    "title": "Benchmarking Physics Problem Solving with PHYSICS",
                    "desc": "The paper presents PHYSICS, a benchmark designed to assess university-level physics problem solving capabilities. It includes 1297 expert-annotated problems across six fundamental physics domains, requiring both advanced knowledge and mathematical skills. The authors introduce an automated evaluation system to ensure accurate validation of model performance. Their findings reveal that even the top-performing model, o3-mini, only achieves 59.9% accuracy, indicating significant room for improvement in tackling complex scientific challenges."
                },
                "zh": {
                    "title": "物理问题解决的新基准",
                    "desc": "我们介绍了PHYSICS，这是一个全面的大学物理问题解决基准。它包含1297个专家注释的问题，涵盖经典力学、量子力学、热力学与统计力学、电磁学、原子物理和光学六个核心领域。每个问题都需要高级物理知识和数学推理能力。我们的评估显示，当前最先进的模型o3-mini的准确率仅为59.9%，这突显了解决高水平科学问题的重大挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19693",
            "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation",
            "url": "https://huggingface.co/papers/2503.19693",
            "abstract": "Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance",
            "score": 9,
            "issue_id": 2976,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "2c907e98a0aafb46",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#architecture",
                    "#low_resource",
                    "#training",
                    "#optimization",
                    "#data",
                    "#transfer_learning"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "Адаптация словаря для повышения эффективности LLM в специализированных областях",
                    "desc": "AdaptiVocab - это новый подход к адаптации словаря больших языковых моделей (LLM) для повышения их эффективности в узкоспециализированных областях. Метод заменяет токены в словаре на специфичные для домена n-граммы, что сокращает количество токенов, необходимых для обработки входных данных и генерации выходных. AdaptiVocab инициализирует новые вложения n-токенов с помощью экспоненциально взвешенной комбинации существующих вложений и использует легковесную фазу дообучения. Эксперименты показали, что AdaptiVocab уменьшает использование токенов более чем на 25% без ухудшения производительности."
                },
                "en": {
                    "title": "Enhancing LLM Efficiency with Domain-Specific Vocabulary",
                    "desc": "This paper presents AdaptiVocab, a method for improving the efficiency of Large Language Models (LLMs) in specific domains by adapting their vocabulary. Instead of using a general-purpose vocabulary, AdaptiVocab replaces tokens with domain-specific n-gram-based tokens, which reduces the number of tokens needed for processing and generation. The approach involves initializing new embeddings through a combination of existing ones and includes a lightweight fine-tuning process that can be done on a single GPU. The results demonstrate that AdaptiVocab can decrease token usage by over 25% while maintaining the quality of generated outputs and overall task performance."
                },
                "zh": {
                    "title": "提高大型语言模型效率的新方法",
                    "desc": "大型语言模型（LLMs）在通用模型中展现了令人印象深刻的多功能性，但其广泛应用伴随着高昂的计算开销，尤其是在自回归解码中，每一步都需要进行前向传播。针对特定领域的应用，通用能力并非必要，可以通过提高效率来进行交换。我们提出了一种新颖的领域适应方法——AdaptiVocab，通过调整词汇表来降低延迟和计算成本，旨在提高LLM在低资源领域的效率。AdaptiVocab可以应用于任何分词器和架构，通过用特定领域的n-gram代替原有的tokens，减少输入处理和输出生成所需的tokens数量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22236",
            "title": "Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal\n  Bridging",
            "url": "https://huggingface.co/papers/2503.22236",
            "abstract": "With the growing demand for high-fidelity 3D models from 2D images, existing methods still face significant challenges in accurately reproducing fine-grained geometric details due to limitations in domain gaps and inherent ambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel framework for generating high-fidelity 3D geometry from images via normal bridging. Hi3DGen consists of three key components: (1) an image-to-normal estimator that decouples the low-high frequency image pattern with noise injection and dual-stream training to achieve generalizable, stable, and sharp estimation; (2) a normal-to-geometry learning approach that uses normal-regularized latent diffusion learning to enhance 3D geometry generation fidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality dataset to support training. Extensive experiments demonstrate the effectiveness and superiority of our framework in generating rich geometric details, outperforming state-of-the-art methods in terms of fidelity. Our work provides a new direction for high-fidelity 3D geometry generation from images by leveraging normal maps as an intermediate representation.",
            "score": 7,
            "issue_id": 2972,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 марта",
                "en": "March 28",
                "zh": "3月28日"
            },
            "hash": "5024619189472d8c",
            "authors": [
                "Chongjie Ye",
                "Yushuang Wu",
                "Ziteng Lu",
                "Jiahao Chang",
                "Xiaoyang Guo",
                "Jiaqing Zhou",
                "Hao Zhao",
                "Xiaoguang Han"
            ],
            "affiliations": [
                "ByteDance",
                "The Chinese University of Hong Kong, Shenzhen",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22236.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "От плоского к объемному: революция в 3D-моделировании",
                    "desc": "Hi3DGen - это новый подход к созданию высококачественных 3D-моделей из 2D-изображений с использованием нормалей в качестве промежуточного представления. Метод состоит из трех ключевых компонентов: оценщика нормалей из изображений, генератора геометрии из нормалей на основе латентной диффузии, и пайплайна для синтеза качественных 3D-данных. Hi3DGen превосходит современные методы в точности воспроизведения мелких геометрических деталей. Этот подход открывает новое направление в генерации высококачественной 3D-геометрии из изображений."
                },
                "en": {
                    "title": "Bridging Normals for High-Fidelity 3D Generation",
                    "desc": "The paper introduces Hi3DGen, a new framework designed to create high-fidelity 3D models from 2D images. It tackles challenges like domain gaps and ambiguities in RGB images by using a method called normal bridging. Hi3DGen includes an image-to-normal estimator that improves the accuracy of geometric details through noise injection and dual-stream training. Additionally, it employs a normal-to-geometry learning approach and a 3D data synthesis pipeline to enhance the quality of the generated 3D models, showing superior performance compared to existing methods."
                },
                "zh": {
                    "title": "高保真3D几何体生成的新方向",
                    "desc": "随着对从2D图像生成高保真3D模型的需求增加，现有方法在准确再现细致几何细节方面仍面临重大挑战。为了解决这些问题，我们提出了Hi3DGen，一个通过法线桥接生成高保真3D几何体的新框架。Hi3DGen包含三个关键组件：图像到法线估计器、法线到几何体学习方法和3D数据合成管道。我们的实验表明，该框架在生成丰富几何细节方面的有效性和优越性，超越了现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.20785",
            "title": "Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency",
            "url": "https://huggingface.co/papers/2503.20785",
            "abstract": "We present Free4D, a novel tuning-free framework for 4D scene generation from a single image. Existing methods either focus on object-level generation, making scene-level generation infeasible, or rely on large-scale multi-view video datasets for expensive training, with limited generalization ability due to the scarcity of 4D scene data. In contrast, our key insight is to distill pre-trained foundation models for consistent 4D scene representation, which offers promising advantages such as efficiency and generalizability. 1) To achieve this, we first animate the input image using image-to-video diffusion models followed by 4D geometric structure initialization. 2) To turn this coarse structure into spatial-temporal consistent multiview videos, we design an adaptive guidance mechanism with a point-guided denoising strategy for spatial consistency and a novel latent replacement strategy for temporal coherence. 3) To lift these generated observations into consistent 4D representation, we propose a modulation-based refinement to mitigate inconsistencies while fully leveraging the generated information. The resulting 4D representation enables real-time, controllable rendering, marking a significant advancement in single-image-based 4D scene generation.",
            "score": 6,
            "issue_id": 2974,
            "pub_date": "2025-03-26",
            "pub_date_card": {
                "ru": "26 марта",
                "en": "March 26",
                "zh": "3月26日"
            },
            "hash": "1a3973da9e51afd7",
            "authors": [
                "Tianqi Liu",
                "Zihao Huang",
                "Zhaoxi Chen",
                "Guangcong Wang",
                "Shoukang Hu",
                "Liao Shen",
                "Huiqiang Sun",
                "Zhiguo Cao",
                "Wei Li",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Great Bay University",
                "Huazhong University of Science and Technology",
                "S-Lab, Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.20785.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "От одного кадра к полноценной 4D-сцене",
                    "desc": "Free4D - это новый подход к генерации 4D-сцен из одного изображения без дополнительного обучения. Метод использует предобученные фундаментальные модели для создания согласованного 4D-представления сцены. Free4D сначала анимирует входное изображение с помощью диффузионных моделей, затем применяет адаптивное руководство для пространственно-временной согласованности и модуляционное уточнение для устранения несоответствий. Результатом является 4D-представление, позволяющее рендерить сцену в реальном времени с возможностью управления."
                },
                "en": {
                    "title": "Revolutionizing 4D Scene Generation from a Single Image",
                    "desc": "Free4D is a new framework that generates 4D scenes from just one image without needing extensive tuning. Unlike previous methods that either focus on individual objects or require large datasets, Free4D uses pre-trained models to create consistent 4D representations efficiently. It employs image-to-video diffusion models to animate the input image and then refines the generated structure for spatial and temporal consistency. This approach allows for real-time rendering of 4D scenes, making it a significant step forward in scene generation technology."
                },
                "zh": {
                    "title": "从单图像生成4D场景的新突破",
                    "desc": "我们提出了Free4D，这是一个无需调优的框架，可以从单张图像生成4D场景。现有方法通常专注于物体级生成，导致场景级生成不可行，或者依赖于大规模多视角视频数据集进行昂贵的训练，且由于4D场景数据稀缺，泛化能力有限。我们的关键见解是提炼预训练的基础模型，以实现一致的4D场景表示，这提供了效率和泛化能力的优势。通过图像到视频的扩散模型，我们首先对输入图像进行动画处理，然后初始化4D几何结构，最终实现实时、可控的4D场景生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22268",
            "title": "Segment Any Motion in Videos",
            "url": "https://huggingface.co/papers/2503.22268",
            "abstract": "Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at https://motion-seg.github.io/.",
            "score": 5,
            "issue_id": 2972,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 марта",
                "en": "March 28",
                "zh": "3月28日"
            },
            "hash": "084606e82bff72ff",
            "authors": [
                "Nan Huang",
                "Wenzhao Zheng",
                "Chenfeng Xu",
                "Kurt Keutzer",
                "Shanghang Zhang",
                "Angjoo Kanazawa",
                "Qianqian Wang"
            ],
            "affiliations": [
                "Peking University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22268.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Улучшенная сегментация движущихся объектов с помощью траекторного внимания и семантических признаков",
                    "desc": "Эта статья представляет новый подход к сегментации движущихся объектов в видео. Метод объединяет траекторные признаки движения с семантическими признаками DINO и использует SAM2 для уточнения масок на уровне пикселей. Модель применяет пространственно-временное внимание к траекториям и раздельное кодирование движения и семантики. Результаты тестирования на различных наборах данных показывают превосходную производительность, особенно в сложных сценариях."
                },
                "en": {
                    "title": "Revolutionizing Moving Object Segmentation with Motion-Semantic Integration",
                    "desc": "This paper presents a new method for moving object segmentation in videos, which is essential for understanding visual scenes. The authors address the limitations of traditional optical flow techniques that struggle with issues like motion blur and background distractions. Their approach combines long-range motion cues with semantic features from a DINO model and uses an iterative strategy with SAM2 for detailed pixel-level segmentation. The proposed model shows superior performance on various datasets, particularly in complex scenarios involving multiple moving objects."
                },
                "zh": {
                    "title": "创新移动物体分割方法，提升视觉理解能力",
                    "desc": "移动物体分割是理解视觉场景的重要任务，具有广泛的应用。传统方法主要依赖光流来提供运动线索，但在处理部分运动、复杂变形、运动模糊和背景干扰时常常效果不佳。我们提出了一种新方法，结合长距离轨迹运动线索与基于DINO的语义特征，并通过迭代提示策略利用SAM2进行像素级掩膜细化。我们的模型采用时空轨迹注意力和运动-语义解耦嵌入，优先考虑运动，同时整合语义支持，经过广泛测试在多样化数据集上表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22329",
            "title": "A Refined Analysis of Massive Activations in LLMs",
            "url": "https://huggingface.co/papers/2503.22329",
            "abstract": "Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_massive_activations.",
            "score": 4,
            "issue_id": 2972,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 марта",
                "en": "March 28",
                "zh": "3月28日"
            },
            "hash": "025b5484847cd3d9",
            "authors": [
                "Louis Owen",
                "Nilabhra Roy Chowdhury",
                "Abhay Kumar",
                "Fabian Güra"
            ],
            "affiliations": [
                "BluOrion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22329.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#long_context",
                    "#inference",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый взгляд на массивные активации в LLM: не все так однозначно",
                    "desc": "Эта статья посвящена исследованию массивных активаций в больших языковых моделях (LLM). Авторы провели анализ различных архитектур LLM, включая модели на основе GLU и без него. Результаты опровергают некоторые предыдущие предположения, показывая, что не все массивные активации вредны, а существующие стратегии смягчения могут быть неэффективны в определенных случаях. Исследователи предлагают новые гибридные стратегии, сочетающие Target Variance Rescaling с Attention KV bias или Dynamic Tanh, для эффективного смягчения массивных активаций при сохранении производительности модели."
                },
                "en": {
                    "title": "Balancing Act: Mitigating Massive Activations in LLMs",
                    "desc": "This paper explores the phenomenon of massive activations in large language models (LLMs) and their implications for low-precision training and quantization. The authors analyze a variety of LLM architectures to understand the effects of massive activations, revealing that not all of them negatively impact model performance. They challenge previous assumptions by demonstrating that suppressing massive activations does not necessarily lead to worse outcomes in downstream tasks. Additionally, the paper introduces new hybrid strategies that effectively mitigate massive activations while maintaining model performance, particularly through the combination of Target Variance Rescaling and other techniques."
                },
                "zh": {
                    "title": "大激活的挑战与新策略",
                    "desc": "本文研究了大语言模型（LLMs）中的大激活现象，特别关注其在低精度训练和量化中的重要性。我们分析了多种LLM架构，包括基于GLU和非GLU的模型，发现并非所有大激活都是有害的，抑制它们并不会导致困惑度的爆炸或下游任务性能的崩溃。我们还发现，现有的缓解策略如注意力KV偏置在某些情况下是模型特定的且无效。因此，我们提出了新的混合缓解策略，结合目标方差重标定（TVR）与注意力KV偏置或动态Tanh（DyT），成功平衡了大激活的缓解与下游模型性能的保持。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21779",
            "title": "X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction",
            "url": "https://huggingface.co/papers/2503.21779",
            "abstract": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.",
            "score": 2,
            "issue_id": 2976,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 марта",
                "en": "March 27",
                "zh": "3月27日"
            },
            "hash": "628d77c9c1bdcd9e",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#architecture",
                    "#healthcare",
                    "#cv"
                ],
                "emoji": "🫁",
                "ru": {
                    "title": "Непрерывная 4D-КТ реконструкция без внешней синхронизации",
                    "desc": "Эта статья представляет X^2-Gaussian - новый метод для непрерывной реконструкции 4D-КТ изображений без использования внешних устройств синхронизации дыхания. Авторы объединяют динамическое рассеивание гауссовых функций с самоконтролируемым обучением респираторных движений. Метод использует пространственно-временную архитектуру кодировщика-декодировщика для моделирования анатомической динамики через предсказание деформаций гауссовых функций во времени. X^2-Gaussian превосходит традиционные методы на 9.93 дБ по метрике PSNR, открывая новые возможности для динамической клинической визуализации."
                },
                "en": {
                    "title": "Revolutionizing 4D CT with Continuous Motion Modeling",
                    "desc": "This paper presents X^2-Gaussian, a new method for 4D CT reconstruction that overcomes the limitations of traditional phase-binning techniques. By using a spatiotemporal encoder-decoder architecture, it models dynamic anatomical changes without relying on fixed phases, thus allowing for continuous-time reconstruction. The method incorporates self-supervised learning to understand patient-specific breathing patterns, eliminating the need for external respiratory gating devices. Experimental results show that X^2-Gaussian significantly improves image quality, achieving higher PSNR compared to existing methods."
                },
                "zh": {
                    "title": "X^2-Gaussian：无硬件的高保真4D CT重建新方法",
                    "desc": "四维计算机断层扫描（4D CT）重建对于捕捉动态解剖变化至关重要，但传统的相位分箱工作流程存在固有的局限性。现有方法将时间分辨率离散化为固定相位，使用呼吸门控设备，导致运动错位并限制临床实用性。本文提出了一种新框架X^2-Gaussian，通过结合动态辐射高斯点云和自监督呼吸运动学习，实现连续时间的4D CT重建。我们的模型通过时空编码器-解码器架构预测时间变化的高斯变形，消除了相位离散化的需求，并引入生理驱动的周期一致性损失，直接从投影中学习患者特定的呼吸周期。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.21732",
            "title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling",
            "url": "https://huggingface.co/papers/2503.21732",
            "abstract": "Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 1024^3 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.",
            "score": 2,
            "issue_id": 2972,
            "pub_date": "2025-03-27",
            "pub_date_card": {
                "ru": "27 марта",
                "en": "March 27",
                "zh": "3月27日"
            },
            "hash": "f17ea311cc796683",
            "authors": [
                "Xianglong He",
                "Zi-Xin Zou",
                "Chia-Hao Chen",
                "Yuan-Chen Guo",
                "Ding Liang",
                "Chun Yuan",
                "Wanli Ouyang",
                "Yan-Pei Cao",
                "Yangguang Li"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.21732.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Революция в 3D-моделировании: высокое разрешение и произвольная топология",
                    "desc": "SparseFlex - это новый метод представления изоповерхностей, позволяющий выполнять дифференцируемую реконструкцию 3D-моделей с разрешением до 1024^3 непосредственно на основе потерь рендеринга. Он сочетает точность Flexicubes с разреженной воксельной структурой, фокусируясь на областях, прилегающих к поверхности. Введена стратегия обучения на основе секционных вокселей с учетом фрустума, что значительно снижает потребление памяти. SparseFlex также позволяет реконструировать внутренние части сетки, используя только рендеринг для обучения."
                },
                "en": {
                    "title": "SparseFlex: Revolutionizing 3D Mesh Reconstruction with Efficiency and Detail",
                    "desc": "This paper presents SparseFlex, a new method for creating detailed 3D meshes with complex shapes and open surfaces. It uses a sparse voxel structure to focus on areas near the surface, allowing for efficient high-resolution mesh reconstruction directly from rendering losses. The method introduces a frustum-aware training strategy that reduces memory usage by activating only necessary voxels during rendering. SparseFlex achieves impressive results, outperforming previous techniques in accuracy and enabling the generation of intricate 3D shapes with varying topologies."
                },
                "zh": {
                    "title": "SparseFlex：高分辨率3D网格重建的新突破",
                    "desc": "本文介绍了一种名为SparseFlex的新型稀疏结构等值面表示方法，旨在解决高保真3D网格重建中的挑战。SparseFlex能够直接从渲染损失中进行可微分的网格重建，支持高达1024^3的分辨率。通过结合Flexicubes的准确性和稀疏体素结构，该方法有效处理开放表面，并引入了基于视锥的分段体素训练策略，显著降低了内存消耗。实验结果表明，SparseFlex在重建精度上达到了最先进的水平，成功生成了具有任意拓扑的高分辨率、细节丰富的3D形状。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18968",
            "title": "MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via\n  Reasoning Agentic Workflow",
            "url": "https://huggingface.co/papers/2503.18968",
            "abstract": "Developing reliable AI systems to assist human clinicians in multi-modal medical diagnosis has long been a key objective for researchers. Recently, Multi-modal Large Language Models (MLLMs) have gained significant attention and achieved success across various domains. With strong reasoning capabilities and the ability to perform diverse tasks based on user instructions, they hold great potential for enhancing medical diagnosis. However, directly applying MLLMs to the medical domain still presents challenges. They lack detailed perception of visual inputs, limiting their ability to perform quantitative image analysis, which is crucial for medical diagnostics. Additionally, MLLMs often exhibit hallucinations and inconsistencies in reasoning, whereas clinical diagnoses must adhere strictly to established criteria. To address these challenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system designed to achieve reliable, explainable, and precise medical diagnoses. This is accomplished through a hierarchical workflow: at the task level, knowledge-based reasoning generate reliable diagnostic plans for specific diseases following retrieved clinical criteria. While at the case level, multiple tool agents process multi-modal inputs, analyze different indicators according to the plan, and provide a final diagnosis based on both quantitative and qualitative evidence. Comprehensive experiments on both 2D and 3D medical diagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro, while case studies further highlight its reliability and interpretability. The code is available at https://github.com/jinlab-imvr/MedAgent-Pro.",
            "score": 2,
            "issue_id": 2973,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "662e73dea1e23d07",
            "authors": [
                "Ziyue Wang",
                "Junde Wu",
                "Chang Han Low",
                "Yueming Jin"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18968.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#3d",
                    "#multimodal",
                    "#reasoning",
                    "#interpretability",
                    "#healthcare",
                    "#agents"
                ],
                "emoji": "🩺",
                "ru": {
                    "title": "MedAgent-Pro: надежная мультимодальная диагностика с ИИ",
                    "desc": "Статья представляет MedAgent-Pro - агентную систему для надежной и объяснимой медицинской диагностики на основе мультимодальных больших языковых моделей (MLLM). Система решает проблемы прямого применения MLLM в медицине, такие как ограниченное восприятие визуальных данных и склонность к галлюцинациям. MedAgent-Pro использует иерархический рабочий процесс с рассуждениями на основе знаний и множеством инструментальных агентов для обработки мультимодальных входных данных. Эксперименты показывают превосходство и эффективность MedAgent-Pro в задачах 2D и 3D медицинской диагностики."
                },
                "en": {
                    "title": "MedAgent-Pro: Reliable AI for Accurate Medical Diagnosis",
                    "desc": "This paper introduces MedAgent-Pro, a new AI system designed to improve medical diagnosis by combining multi-modal inputs and evidence-based reasoning. It addresses the limitations of existing Multi-modal Large Language Models (MLLMs), which struggle with visual data and often produce unreliable outputs. MedAgent-Pro uses a hierarchical approach, where knowledge-based reasoning creates diagnostic plans and multiple tool agents analyze various indicators to provide accurate diagnoses. Experiments show that MedAgent-Pro outperforms traditional methods in both 2D and 3D medical tasks, demonstrating its reliability and interpretability in clinical settings."
                },
                "zh": {
                    "title": "提升医疗诊断的可靠性与可解释性",
                    "desc": "本论文提出了一种名为MedAgent-Pro的系统，旨在提高多模态医疗诊断的可靠性和准确性。该系统结合了基于知识的推理和多工具代理，能够处理多种输入并生成可靠的诊断计划。通过对2D和3D医疗诊断任务的全面实验，MedAgent-Pro展示了其优越性和有效性。该系统不仅提供了精确的诊断，还具备良好的可解释性，适合临床应用。"
                }
            }
        }
    ],
    "link_prev": "2025-03-28.html",
    "link_next": "2025-04-01.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "28.03",
        "en": "03/28",
        "zh": "3月28日"
    },
    "short_date_next": {
        "ru": "01.04",
        "en": "04/01",
        "zh": "4月1日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 2,
        "#benchmark": 1,
        "#agents": 2,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 5,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 2,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了一种名为Video-R1的新方法，用于在多模态大语言模型中激发视频推理能力。受DeepSeek-R1成功的启发，作者提出了T-GRPO算法，以解决直接应用RL训练中的时间建模和高质量数据缺乏的问题。他们还创建了两个包含图像和视频数据的数据集，并展示了Video-R1在多个视频推理基准上的显著改进。特别是，Video-R1-7B在VSI-bench上的准确率达到了35.8%，超越了商业模型GPT-4o。所有代码、模型和数据都已公开。",
        "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
        "pinyin": "这篇文章介绍了一种名为Video-R1的新方法，用于在多模态大语言模型中激发视频推理能力。受DeepSeek-R1成功的启发，作者提出了T-GRPO算法，以解决直接应用RL训练中的时间建模和高质量数据缺乏的问题。他们还创建了两个包含图像和视频数据的数据集，并展示了Video-R1在多个视频推理基准上的显著改进。特别是，Video-R1-7B在VSI-bench上的准确率达到了35.8%，超越了商业模型GPT-4o。所有代码、模型和数据都已公开。\n\nzhè piān wén zhāng jiè shào le yī zhǒng míng wéi Video-R1 de xīn fāng fǎ, yòng yú zài duō mó tài dà yǔ yán mó xíng zhōng jī fā shì pǐn tuī lǐ néng lì. shòu DeepSeek-R1 chéng gōng de qǐ fǎ, zuò zhě tí chū le T-GRPO suàn fǎ, yǐ jiě jué zhí jiē yòng RL xùn liàn zhōng de shí jiān jiàn mó hé gāo zhì liàng shù jué de quē fá de wèn tí. tā men hái chuàng jiàn le liǎng gè bāo hán tú xiàng hé shì pǐn shù jué de shù ju ji, bìng zhàn shì le Video-R1 zài duō gè shì pǐn tuī lǐ jī zhǔn shàng de xiǎn zhù gǎi jìn. tè bié shì, Video-R1-7B zài VSI-bench shàng de zhǔn què lǜ dá dào le 35.8%, chāo yuè le shāng yè mó xíng GPT-4o. suǒ yǒu dài mǎ, mó xíng hé shù ju dōu yǐ gōng kāi.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"激发\", \"pinyin\": \"jī fā\", \"trans\": \"stimulate\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"启发\", \"pinyin\": \"qǐ fā\", \"trans\": \"inspire\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"算法\", \"pinyin\": \"suàn fǎ\", \"trans\": \"algorithm\"},\n    {\"word\": \"解决\", \"pinyin\": \"jiě jué\", \"trans\": \"solve\"},\n    {\"word\": \"直接\", \"pinyin\": \"zhí jiē\", \"trans\": \"direct\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"training\"},\n    {\"word\": \"时间\", \"pinyin\": \"shí jiān\", \"trans\": \"time\"},\n    {\"word\": \"建模\", \"pinyin\": \"jiàn mó\", \"trans\": \"modeling\"},\n    {\"word\": \"高质量\", \"pinyin\": \"gāo zhì liàng\", \"trans\": \"high quality\"},\n    {\"word\": \"缺乏\", \"pinyin\": \"quē fá\", \"trans\": \"lack\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"展示\", \"pinyin\": \"zhǎn shì\", \"trans\": \"demonstrate\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"改进\", \"pinyin\": \"gǎi jìn\", \"trans\": \"improvement\"},\n    {\"word\": \"准确率\", \"pinyin\": \"zhǔn què lǜ\", \"trans\": \"accuracy\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"},\n    {\"word\": \"商业\", \"pinyin\": \"shāng yè\", \"trans\": \"commercial\"},\n    {\"word\": \"公开\", \"pinyin\": \"gōng kāi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces a new method called Video-R1 for eliciting video reasoning capabilities in multimodal large language models. Inspired by the success of DeepSeek-R1, the authors propose the T-GRPO algorithm to address the issues of temporal modeling in direct RL training and the lack of high-quality data. They also created two datasets containing image and video data and demonstrated significant improvements of Video-R1 on multiple video reasoning benchmarks. Notably, Video-R1-7B achieved an accuracy of 35.8% on the VSI-bench, surpassing the commercial model GPT-4o. All code, models, and data have been made publicly available.",
        "update_ts": "2025-03-30 12:41"
    }
}