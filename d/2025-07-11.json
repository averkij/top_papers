{
    "date": {
        "ru": "11 Ğ¸ÑĞ»Ñ",
        "en": "July 11",
        "zh": "7æœˆ11æ—¥"
    },
    "time_utc": "2025-07-11 00:58",
    "weekday": 4,
    "issue_id": 4760,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.07105",
            "title": "4KAgent: Agentic Any Image to 4K Super-Resolution",
            "url": "https://huggingface.co/papers/2507.07105",
            "abstract": "4KAgent, a unified agentic super-resolution system, enhances low-resolution images to 4K using profiling, perception, and restoration agents, achieving state-of-the-art performance across various imaging domains.  \t\t\t\t\tAI-generated summary \t\t\t\t We present 4KAgent, a unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at 256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, a module that customizes the 4KAgent pipeline based on bespoke use cases; (2) A Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make a tailored restoration plan; and (3) A Restoration Agent, which executes the plan, following a recursive execution-reflection paradigm, guided by a quality-driven mixture-of-expert policy to select the optimal output for each step. Additionally, 4KAgent embeds a specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing a total of 26 diverse benchmarks, setting new state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We will release all the code, models, and results at: https://4kagent.github.io.",
            "score": 52,
            "issue_id": 4743,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "3ef4be8673ff0eed",
            "authors": [
                "Yushen Zuo",
                "Qi Zheng",
                "Mingyang Wu",
                "Xinrui Jiang",
                "Renjie Li",
                "Jian Wang",
                "Yide Zhang",
                "Gengchen Mai",
                "Lihong V. Wang",
                "James Zou",
                "Xiaoyu Wang",
                "Ming-Hsuan Yang",
                "Zhengzhong Tu"
            ],
            "affiliations": [
                "CU Boulder",
                "California Institute of Technology",
                "Snap Inc.",
                "Stanford University",
                "Texas A&M University",
                "Topaz Labs",
                "UC Merced",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07105.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#cv",
                    "#open_source",
                    "#benchmark",
                    "#agents",
                    "#healthcare"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "4KAgent: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "4KAgent - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 4K. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ. 4KAgent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 26 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transforming Low-Res to Stunning 4K with 4KAgent!",
                    "desc": "4KAgent is an advanced super-resolution system that enhances low-resolution images to 4K quality using a combination of specialized agents. It features three main components: Profiling for customizing the process, a Perception Agent that analyzes images and creates restoration plans, and a Restoration Agent that implements these plans using a quality-driven approach. The system excels in transforming severely degraded images into high-quality outputs, including a dedicated module for improving facial details in portraits. Rigorous evaluations across various imaging tasks demonstrate its state-of-the-art performance, setting new benchmarks in multiple domains such as natural images, medical imaging, and AI-generated content."
                },
                "zh": {
                    "title": "4KAgentï¼šè¶…åˆ†è¾¨ç‡å›¾åƒå¤„ç†çš„æ–°çºªå…ƒ",
                    "desc": "4KAgentæ˜¯ä¸€ç§ç»Ÿä¸€çš„æ™ºèƒ½è¶…åˆ†è¾¨ç‡ç³»ç»Ÿï¼Œèƒ½å¤Ÿå°†ä½åˆ†è¾¨ç‡å›¾åƒæå‡è‡³4Kåˆ†è¾¨ç‡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶å®ç°ï¼šé…ç½®æ¨¡å—ã€æ„ŸçŸ¥ä»£ç†å’Œæ¢å¤ä»£ç†ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸åŒçš„ä½¿ç”¨åœºæ™¯å®šåˆ¶å¤„ç†æµç¨‹ã€‚4KAgentç‰¹åˆ«é€‚ç”¨äºå¤„ç†ä¸¥é‡å¤±çœŸçš„å›¾åƒï¼Œå¹¶èƒ½æ˜¾è‘—æå‡äººè„¸ç»†èŠ‚ï¼Œé€‚åˆè‚–åƒå’Œè‡ªæ‹ç…§ç‰‡ã€‚é€šè¿‡åœ¨å¤šä¸ªæˆåƒé¢†åŸŸçš„ä¸¥æ ¼è¯„ä¼°ï¼Œ4KAgentåœ¨å›¾åƒè´¨é‡å’Œä¿çœŸåº¦æ–¹é¢å‡è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†ä½çº§è§†è§‰ä»»åŠ¡çš„åˆ›æ–°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07095",
            "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data",
            "url": "https://huggingface.co/papers/2507.07095",
            "abstract": "A new dataset and evaluation framework improve zero-shot text-to-motion generation through a large-scale, high-quality dataset and a scalable model architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamental and challenging research area within the domains of computer vision, graphics, and robotics. Despite significant advancements in this field, current methodologies often face challenges regarding zero-shot generalization capabilities, largely attributable to the limited size of training datasets. Moreover, the lack of a comprehensive evaluation framework impedes the advancement of this task by failing to identify directions for improvement. In this work, we aim to push text-to-motion into a new era, that is, to achieve the generalization ability of zero-shot. To this end, firstly, we develop an efficient annotation pipeline and introduce MotionMillion-the largest human motion dataset to date, featuring over 2,000 hours and 2 million high-quality motion sequences. Additionally, we propose MotionMillion-Eval, the most comprehensive benchmark for evaluating zero-shot motion generation. Leveraging a scalable architecture, we scale our model to 7B parameters and validate its performance on MotionMillion-Eval. Our results demonstrate strong generalization to out-of-domain and complex compositional motions, marking a significant step toward zero-shot human motion generation. The code is available at https://github.com/VankouF/MotionMillion-Codes.",
            "score": 40,
            "issue_id": 4738,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "d8bd0a82b6576b80",
            "authors": [
                "Ke Fan",
                "Shunlin Lu",
                "Minyue Dai",
                "Runyi Yu",
                "Lixing Xiao",
                "Zhiyang Dou",
                "Junting Dong",
                "Lizhuang Ma",
                "Jingbo Wang"
            ],
            "affiliations": [
                "CUHK, Shenzhen",
                "East China Normal University",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07095.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#cv",
                    "#benchmark",
                    "#dataset",
                    "#robotics",
                    "#games"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MotionMillion Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ MotionMillion-Eval Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼. MotionMillion ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° ÑĞµĞ³Ğ¾Ğ´Ğ½ÑÑˆĞ½Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ¼ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Revolutionizing Zero-Shot Motion Generation with MotionMillion",
                    "desc": "This paper presents a new dataset and evaluation framework aimed at enhancing zero-shot text-to-motion generation. The authors introduce MotionMillion, the largest dataset of human motion sequences, which includes over 2 million high-quality motions. They also propose MotionMillion-Eval, a comprehensive benchmark for assessing the performance of zero-shot motion generation models. By utilizing a scalable model architecture with 7 billion parameters, the study demonstrates improved generalization capabilities for generating complex human motions from textual descriptions."
                },
                "zh": {
                    "title": "æ¨åŠ¨é›¶-shotäººç±»åŠ¨ä½œç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œä»¥æ”¹å–„é›¶-shotæ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆçš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼€å‘äº†MotionMillionï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„é«˜è´¨é‡äººç±»åŠ¨ä½œæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡2000å°æ—¶å’Œ200ä¸‡æ¡åŠ¨ä½œåºåˆ—ã€‚é€šè¿‡å¼•å…¥MotionMillion-Evalï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æ¥è¯„ä¼°é›¶-shotåŠ¨ä½œç”Ÿæˆçš„æ•ˆæœã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨7Bå‚æ•°çš„å¯æ‰©å±•æ¶æ„ä¸‹è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ç”Ÿæˆå¤æ‚çš„åŠ¨ä½œåºåˆ—ï¼Œæ ‡å¿—ç€é›¶-shotäººç±»åŠ¨ä½œç”Ÿæˆçš„é‡è¦è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.06448",
            "title": "Perception-Aware Policy Optimization for Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2507.06448",
            "abstract": "Perception-Aware Policy Optimization (PAPO) enhances reinforcement learning with verifiable rewards for multimodal reasoning by integrating implicit perception loss, improving visual perception and reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose Perception-Aware Policy Optimization (PAPO), a simple yet effective extension of GRPO that encourages the model to learn to perceive while learning to reason, entirely from internal supervision signals. Notably, PAPO does not rely on additional data curation, external reward models, or proprietary models. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term to the GRPO objective, which, despite its simplicity, yields significant overall improvements (4.4%) on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%, on tasks with high vision dependency. We also observe a substantial reduction (30.5%) in perception errors, indicating improved perceptual capabilities with PAPO. We conduct comprehensive analysis of PAPO and identify a unique loss hacking issue, which we rigorously analyze and mitigate through a Double Entropy Loss. Overall, our work introduces a deeper integration of perception-aware supervision into RLVR learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.",
            "score": 33,
            "issue_id": 4739,
            "pub_date": "2025-07-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ»Ñ",
                "en": "July 8",
                "zh": "7æœˆ8æ—¥"
            },
            "hash": "371bd96113b91da3",
            "authors": [
                "Zhenhailong Wang",
                "Xuehang Guo",
                "Sofia Stoica",
                "Haiyang Xu",
                "Hongru Wang",
                "Hyeonjeong Ha",
                "Xiusi Chen",
                "Yangyi Chen",
                "Ming Yan",
                "Fei Huang",
                "Heng Ji"
            ],
            "affiliations": [
                "Alibaba Group",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.06448.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Perception-Aware Policy Optimization (PAPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. PAPO Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞ²Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ GRPO, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¸ Ñ€ĞµÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ."
                },
                "en": {
                    "title": "Enhancing Multimodal Reasoning with Perception-Aware Learning",
                    "desc": "Perception-Aware Policy Optimization (PAPO) is a novel approach that enhances reinforcement learning by integrating implicit perception loss to improve visual reasoning in multimodal tasks. It addresses the limitations of existing methods that primarily focus on textual data, leading to errors in visual input perception. By introducing a KL divergence term to the GRPO objective, PAPO significantly boosts performance on multimodal benchmarks, especially in vision-dependent tasks. This method not only reduces perception errors but also establishes a new framework for visually grounded reasoning in reinforcement learning."
                },
                "zh": {
                    "title": "æå‡è§†è§‰æ¨ç†çš„æ„ŸçŸ¥ä¼˜åŒ–ç­–ç•¥",
                    "desc": "æ„ŸçŸ¥æ„è¯†ç­–ç•¥ä¼˜åŒ–ï¼ˆPAPOï¼‰é€šè¿‡æ•´åˆéšå¼æ„ŸçŸ¥æŸå¤±ï¼Œå¢å¼ºäº†å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„å¯éªŒè¯å¥–åŠ±ã€‚è¯¥æ–¹æ³•æ”¹å–„äº†è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è§†è§‰è¾“å…¥æ—¶æ˜¾è‘—å‡å°‘äº†é”™è¯¯ã€‚PAPOæ˜¯å¯¹GRPOçš„æœ‰æ•ˆæ‰©å±•ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰é¢å¤–æ•°æ®æˆ–å¤–éƒ¨å¥–åŠ±æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨å†…éƒ¨ç›‘ç£ä¿¡å·è¿›è¡Œå­¦ä¹ ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPAPOåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­æé«˜äº†4.4%çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨è§†è§‰ä¾èµ–æ€§é«˜çš„ä»»åŠ¡ä¸­ï¼Œæå‡æ¥è¿‘8.0%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.06920",
            "title": "Rethinking Verification for LLM Code Generation: From Generation to\n  Testing",
            "url": "https://huggingface.co/papers/2507.06920",
            "abstract": "A human-LLM collaborative method enhances code generation test case generation, improving reliability and detection rates in code evaluation benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have recently achieved notable success in code-generation benchmarks such as HumanEval and LiveCodeBench. However, a detailed examination reveals that these evaluation suites often comprise only a limited number of homogeneous test cases, resulting in subtle faults going undetected. This not only artificially inflates measured performance but also compromises accurate reward estimation in reinforcement learning frameworks utilizing verifiable rewards (RLVR). To address these critical shortcomings, we systematically investigate the test-case generation (TCG) task by proposing multi-dimensional metrics designed to rigorously quantify test-suite thoroughness. Furthermore, we introduce a human-LLM collaborative method (SAGA), leveraging human programming expertise with LLM reasoning capability, aimed at significantly enhancing both the coverage and the quality of generated test cases. In addition, we develop a TCGBench to facilitate the study of the TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc) of the code generation evaluation benchmark synthesized by SAGA is 10.78% higher than that of LiveCodeBench-v6. These results demonstrate the effectiveness of our proposed method. We hope this work contributes to building a scalable foundation for reliable LLM code evaluation, further advancing RLVR in code generation, and paving the way for automated adversarial test synthesis and adaptive benchmark integration.",
            "score": 23,
            "issue_id": 4738,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "93b419a1fb85f819",
            "authors": [
                "Zihan Ma",
                "Taolin Zhang",
                "Maosong Cao",
                "Wenwei Zhang",
                "Minnan Luo",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "MOE KLINNS Lab, Xian Jiaotong University, China",
                "School of Computer Science and Technology, Xian Jiaotong University, China",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.06920.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#dataset",
                    "#training",
                    "#games"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¸ Ğ˜Ğ˜ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SAGA Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TCGBench. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SAGA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 90.62% Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ 32.58% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ½Ğ° TCGBench. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Code Reliability with Human-LLM Collaboration",
                    "desc": "This paper presents a method called SAGA that combines human expertise with large language models (LLMs) to improve the generation of test cases for code evaluation. The authors highlight that existing benchmarks often miss subtle errors due to their limited and similar test cases, which can lead to misleading performance metrics. By introducing multi-dimensional metrics and a new test-case generation benchmark (TCGBench), they rigorously assess the thoroughness of test suites. The results show that SAGA significantly enhances detection rates and verifier accuracy, indicating its potential to improve the reliability of LLMs in code generation tasks."
                },
                "zh": {
                    "title": "äººæœºåä½œæå‡ä»£ç æµ‹è¯•ç”Ÿæˆçš„å¯é æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§äººç±»ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åä½œçš„æ–¹æ³•ï¼Œä»¥å¢å¼ºä»£ç ç”Ÿæˆä¸­çš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆï¼Œæå‡ä»£ç è¯„ä¼°åŸºå‡†çš„å¯é æ€§å’Œæ£€æµ‹ç‡ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„è¯„ä¼°å¥—ä»¶é€šå¸¸åªåŒ…å«æœ‰é™çš„åŒè´¨æµ‹è¯•ç”¨ä¾‹ï¼Œå¯¼è‡´ä¸€äº›ç»†å¾®é”™è¯¯æœªè¢«å‘ç°ï¼Œä»è€Œå½±å“äº†æ€§èƒ½è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å¤šç»´åº¦æŒ‡æ ‡æ¥é‡åŒ–æµ‹è¯•å¥—ä»¶çš„å…¨é¢æ€§ï¼Œå¹¶å¼•å…¥äº†SAGAæ–¹æ³•ï¼Œç»“åˆäººç±»ç¼–ç¨‹ä¸“å®¶ä¸LLMçš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹çš„è¦†ç›–ç‡å’Œè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAGAåœ¨TCGBenchä¸Šçš„æ£€æµ‹ç‡è¾¾åˆ°90.62%ï¼ŒéªŒè¯å™¨å‡†ç¡®ç‡ä¸º32.58%ï¼Œæ˜¾ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.06457",
            "title": "A Systematic Analysis of Hybrid Linear Attention",
            "url": "https://huggingface.co/papers/2507.06457",
            "abstract": "Research evaluates various linear attention models and their integration with full attention in Transformers, identifying key mechanisms like selective gating and hierarchical recurrence for enhanced recall performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformers face quadratic complexity and memory issues with long sequences, prompting the adoption of linear attention mechanisms using fixed-size hidden states. However, linear models often suffer from limited recall performance, leading to hybrid architectures that combine linear and full attention layers. Despite extensive hybrid architecture research, the choice of linear attention component has not been deeply explored. We systematically evaluate various linear attention models across generations - vector recurrences to advanced gating mechanisms - both standalone and hybridized. To enable this comprehensive analysis, we trained and open-sourced 72 models: 36 at 340M parameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six linear attention variants across five hybridization ratios. Benchmarking on standard language modeling and recall tasks reveals that superior standalone linear models do not necessarily excel in hybrids. While language modeling remains stable across linear-to-full attention ratios, recall significantly improves with increased full attention layers, particularly below a 3:1 ratio. Our study highlights selective gating, hierarchical recurrence, and controlled forgetting as critical for effective hybrid models. We recommend architectures such as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1 to achieve Transformer-level recall efficiently. Our models are open-sourced at https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.",
            "score": 17,
            "issue_id": 4739,
            "pub_date": "2025-07-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ»Ñ",
                "en": "July 8",
                "zh": "7æœˆ8æ—¥"
            },
            "hash": "31144b92e85957ae",
            "authors": [
                "Dustin Wang",
                "Rui-Jie Zhu",
                "Steven Abreu",
                "Yong Shan",
                "Taylor Kergan",
                "Yuqi Pan",
                "Yuhong Chou",
                "Zheng Li",
                "Ge Zhang",
                "Wenhao Huang",
                "Jason Eshraghian"
            ],
            "affiliations": [
                "ByteDance Seed",
                "CASIA",
                "M-A-P",
                "PolyU",
                "UC Santa Cruz",
                "University of Groningen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.06457.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#benchmark",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ñ‹ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ³ĞµĞ¹Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ 72 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ² Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Transformer Recall with Hybrid Linear Attention Models",
                    "desc": "This research investigates different linear attention models and their combination with full attention in Transformers to improve recall performance. It identifies important mechanisms like selective gating and hierarchical recurrence that enhance the effectiveness of these hybrid models. The study systematically evaluates 72 models, revealing that the best standalone linear models do not always perform well in hybrid settings. The findings suggest optimal architectures and ratios for combining linear and full attention to achieve better recall in language tasks."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å˜æ¢å™¨çš„æ··åˆæ³¨æ„åŠ›æ¨¡å‹",
                    "desc": "æœ¬ç ”ç©¶è¯„ä¼°äº†å„ç§çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹åŠå…¶ä¸å…¨æ³¨æ„åŠ›åœ¨å˜æ¢å™¨ä¸­çš„ç»“åˆï¼Œè¯†åˆ«å‡ºé€‰æ‹©æ€§é—¨æ§å’Œå±‚æ¬¡é€’å½’ç­‰å…³é”®æœºåˆ¶ï¼Œä»¥æé«˜å›å¿†æ€§èƒ½ã€‚å˜æ¢å™¨åœ¨å¤„ç†é•¿åºåˆ—æ—¶é¢ä¸´äºŒæ¬¡å¤æ‚æ€§å’Œå†…å­˜é—®é¢˜ï¼Œå› æ­¤é‡‡ç”¨äº†å›ºå®šå¤§å°çš„éšè—çŠ¶æ€çš„çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ã€‚ç„¶è€Œï¼Œçº¿æ€§æ¨¡å‹é€šå¸¸åœ¨å›å¿†æ€§èƒ½ä¸Šå­˜åœ¨å±€é™ï¼Œå¯¼è‡´å‡ºç°ç»“åˆçº¿æ€§å’Œå…¨æ³¨æ„åŠ›å±‚çš„æ··åˆæ¶æ„ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé€‰æ‹©æ€§é—¨æ§ã€å±‚æ¬¡é€’å½’å’Œæ§åˆ¶é—å¿˜æ˜¯æœ‰æ•ˆæ··åˆæ¨¡å‹çš„å…³é”®å› ç´ ï¼Œå¹¶æ¨èåœ¨3:1åˆ°6:1çš„çº¿æ€§ä¸å…¨æ³¨æ„åŠ›æ¯”ä¾‹ä¸‹çš„æ¶æ„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07017",
            "title": "First Return, Entropy-Eliciting Explore",
            "url": "https://huggingface.co/papers/2507.07017",
            "abstract": "FR3E enhances LLM reasoning by providing structured exploration through targeted rollouts at high-uncertainty points, leading to more stable training and accurate responses.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning abilities of Large Language Models (LLMs) but it struggles with unstable exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a structured exploration framework that identifies high-uncertainty decision points in reasoning trajectories and performs targeted rollouts to construct semantically grounded intermediate feedback. Our method provides targeted guidance without relying on dense supervision. Empirical results on mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable training, produces longer and more coherent responses, and increases the proportion of fully correct trajectories. These results highlight the framework's effectiveness in improving LLM reasoning through more robust and structured exploration.",
            "score": 15,
            "issue_id": 4740,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "93dc431bd34cdb14",
            "authors": [
                "Tianyu Zheng",
                "Tianshun Xing",
                "Qingshui Gu",
                "Taoran Liang",
                "Xingwei Qu",
                "Xin Zhou",
                "Yizhi Li",
                "Zhoufutu Wen",
                "Chenghua Lin",
                "Wenhao Huang",
                "Qian Liu",
                "Ge Zhang",
                "Zejun Ma"
            ],
            "affiliations": [
                "ByteDance",
                "M-A-P",
                "The University of Manchester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07017.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "FR3E: Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "FR3E - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ñ‹ Ğ² Ñ‚Ğ¾Ñ‡ĞºĞ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ LLM. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FR3E ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Structured Exploration for Enhanced LLM Reasoning",
                    "desc": "FR3E is a framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) by focusing on high-uncertainty decision points during their training. It utilizes targeted rollouts to provide structured exploration, which helps in generating semantically grounded feedback without needing extensive supervision. This approach leads to more stable training processes and improves the accuracy of the model's responses. Empirical tests demonstrate that FR3E results in longer, more coherent outputs and a higher rate of correct reasoning paths."
                },
                "zh": {
                    "title": "FR3Eï¼šæå‡LLMæ¨ç†çš„ç»“æ„åŒ–æ¢ç´¢",
                    "desc": "FR3Eæ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡åœ¨é«˜ä¸ç¡®å®šæ€§å†³ç­–ç‚¹è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æ¢ç´¢ï¼Œæä¾›ç»“æ„åŒ–çš„åé¦ˆï¼Œä»è€Œå®ç°æ›´ç¨³å®šçš„è®­ç»ƒå’Œæ›´å‡†ç¡®çš„å“åº”ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºå¯†é›†çš„ç›‘ç£ï¼Œè€Œæ˜¯é€šè¿‡æ„å»ºè¯­ä¹‰ä¸Šæ‰å®çš„ä¸­é—´åé¦ˆæ¥æŒ‡å¯¼æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFR3Eåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é•¿ã€æ›´è¿è´¯çš„å“åº”ï¼Œå¹¶æé«˜äº†å®Œå…¨æ­£ç¡®è½¨è¿¹çš„æ¯”ä¾‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05687",
            "title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in\n  LLMs",
            "url": "https://huggingface.co/papers/2507.05687",
            "abstract": "Kernel development in deep learning requires optimizing computational units across hardware while balancing memory management, parallelism, and hardware-specific optimizations through extensive empirical tuning. Although domain-specific languages like Triton simplify GPU programming by abstracting low-level details, developers must still manually tune critical parameters such as tile sizes and memory access patterns through iterative experimentation, creating substantial barriers to optimal performance and wider adoption. In this work, we introduce AutoTriton, the first model dedicated to Triton programming powered by reinforcement learning (RL). AutoTriton performs supervised fine-tuning (SFT) to be equipped with essential Triton programming expertise using a high-quality data gathering pipeline, and conducts RL with Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based reward and an execution-based reward to further improve Triton programming ability, sequentially. Experiments across five evaluation channels of TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves performance comparable to mainstream large models, including Claude-4-Sonnet and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial role of each module within AutoTriton, including the SFT stage, the RL stage, and the reward design strategy. These findings underscore the promise of RL for automatically generating high-performance kernels, and since high-performance kernels are core components of AI systems, this breakthrough establishes an important foundation for building more efficient AI systems. The model and code will be available at https://github.com/AI9Stars/AutoTriton.",
            "score": 13,
            "issue_id": 4738,
            "pub_date": "2025-07-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ»Ñ",
                "en": "July 8",
                "zh": "7æœˆ8æ—¥"
            },
            "hash": "1e138b69433f5481",
            "authors": [
                "Shangzhan Li",
                "Zefan Wang",
                "Ye He",
                "Yuxuan Li",
                "Qi Shi",
                "Jianling Li",
                "Yonggang Hu",
                "Wanxiang Che",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "OpenBMB",
                "Tianjin University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05687.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "AutoTriton: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ´ĞµÑ€ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL",
                    "desc": "AutoTriton - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL), Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Triton. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ (SFT) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Triton Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Group Relative Policy Optimization (GRPO) Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 8B-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ AutoTriton Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» RL Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ´ĞµÑ€, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "AutoTriton: Revolutionizing GPU Programming with Reinforcement Learning",
                    "desc": "This paper presents AutoTriton, a novel model designed to enhance Triton programming for GPU optimization using reinforcement learning (RL). It addresses the challenges developers face in manually tuning parameters for performance by automating the process through supervised fine-tuning and RL techniques. The model employs a unique reward system that combines rule-based and execution-based rewards to improve kernel generation. Experimental results show that AutoTriton achieves performance on par with leading models, highlighting its potential to streamline the development of high-performance kernels essential for AI systems."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–Tritonç¼–ç¨‹ï¼Œæå‡æ·±åº¦å­¦ä¹ æ€§èƒ½ï¼",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†AutoTritonï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨¡å‹ï¼Œæ—¨åœ¨ä¼˜åŒ–Tritonç¼–ç¨‹ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒå’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ŒAutoTritonèƒ½å¤Ÿè‡ªåŠ¨è°ƒæ•´å…³é”®å‚æ•°ï¼Œä»è€Œæé«˜GPUç¼–ç¨‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoTritonçš„è¡¨ç°ä¸ä¸»æµå¤§å‹æ¨¡å‹ç›¸å½“ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨ç”Ÿæˆé«˜æ€§èƒ½å†…æ ¸æ–¹é¢çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´é«˜æ•ˆçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†é‡è¦åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.06804",
            "title": "Towards Solving More Challenging IMO Problems via Decoupled Reasoning\n  and Proving",
            "url": "https://huggingface.co/papers/2507.06804",
            "abstract": "A novel framework decouples reasoning and proving in ATP to improve formal proving performance, achieving success on challenging IMO problems.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated Theorem Proving (ATP) in formal languages is a foundational challenge for AI. While Large Language Models (LLMs) have driven remarkable progress, a significant gap remains between their powerful informal reasoning capabilities and their weak formal proving performance. Recent studies show that the informal accuracy exceeds 80% while formal success remains below 8% on benchmarks like PutnamBench. We argue this gap persists because current state-of-the-art provers, by tightly coupling reasoning and proving, are trained with paradigms that inadvertently punish deep reasoning in favor of shallow, tactic-based strategies. To bridge this fundamental gap, we propose a novel framework that decouples high-level reasoning from low-level proof generation. Our approach utilizes two distinct, specialized models: a powerful, general-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an efficient Prover to rigorously verify them. This modular design liberates the model's full reasoning potential and bypasses the pitfalls of end-to-end training. We evaluate our method on a challenging set of post-2000 IMO problems, a problem set on which no prior open-source prover has reported success. Our decoupled framework successfully solves 5 of these problems, demonstrating a significant step towards automated reasoning on exceptionally difficult mathematical challenges. To foster future research, we release our full dataset of generated and verified lemmas for a wide range of IMO problems, available at https://tencent-imo.github.io/ .",
            "score": 10,
            "issue_id": 4738,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 Ğ¸ÑĞ»Ñ",
                "en": "July 7",
                "zh": "7æœˆ7æ—¥"
            },
            "hash": "4c82f29e08bec1a6",
            "authors": [
                "Zhenwen Liang",
                "Linfeng Song",
                "Yang Li",
                "Tao Yang",
                "Feng Zhang",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.06804.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#dataset",
                    "#training",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ñƒ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ñƒ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ Reasoner Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ñ†ĞµĞ»ĞµĞ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Prover Ğ´Ğ»Ñ Ğ¸Ñ… ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼ Ñ‚Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼ Ğ² ÑƒÑ‰ĞµÑ€Ğ± Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€ĞµÑˆĞ¸Ğ» 5 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ IMO Ğ¿Ğ¾ÑĞ»Ğµ 2000 Ğ³Ğ¾Ğ´Ğ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Decoupling Reasoning and Proving for Enhanced Theorem Proving",
                    "desc": "This paper presents a new framework for Automated Theorem Proving (ATP) that separates the processes of reasoning and proving to enhance performance on complex mathematical problems. Current models struggle with formal proving due to their reliance on shallow tactics, which limits deep reasoning capabilities. The proposed solution involves using two specialized models: a Reasoner that generates strategic subgoals and a Prover that verifies these goals rigorously. By decoupling these functions, the framework achieves notable success on challenging IMO problems, marking a significant advancement in automated reasoning."
                },
                "zh": {
                    "title": "è§£è€¦æ¨ç†ä¸è¯æ˜ï¼Œæå‡è‡ªåŠ¨å®šç†è¯æ˜æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå°†æ¨ç†ä¸è¯æ˜è§£è€¦ï¼Œä»¥æé«˜è‡ªåŠ¨å®šç†è¯æ˜ï¼ˆATPï¼‰çš„æ€§èƒ½ã€‚å½“å‰çš„æœ€å…ˆè¿›è¯æ˜å™¨å°†æ¨ç†ä¸è¯æ˜ç´§å¯†ç»“åˆï¼Œå¯¼è‡´æ·±åº¦æ¨ç†å—åˆ°æŠ‘åˆ¶ï¼Œè€Œæ›´å€¾å‘äºæµ…å±‚çš„ç­–ç•¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å‹ï¼šä¸€ä¸ªå¼ºå¤§çš„é€šç”¨æ¨ç†å™¨ç”Ÿæˆå¤šæ ·çš„å­ç›®æ ‡å¼•ç†ï¼Œå¦ä¸€ä¸ªé«˜æ•ˆçš„è¯æ˜å™¨å¯¹å…¶è¿›è¡Œä¸¥æ ¼éªŒè¯ã€‚é€šè¿‡è¿™ç§æ¨¡å—åŒ–è®¾è®¡ï¼Œæˆ‘ä»¬æˆåŠŸè§£å†³äº†äº”ä¸ª2000å¹´åå›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIMOï¼‰é—®é¢˜ï¼Œå±•ç¤ºäº†åœ¨æå…·æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜ä¸Šå®ç°è‡ªåŠ¨æ¨ç†çš„é‡è¦è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.24044",
            "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
            "url": "https://huggingface.co/papers/2506.24044",
            "abstract": "This survey provides a comprehensive overview of Vision-Language-Action (VLA) paradigms and their adaptation for autonomous driving, detailing architectural components, evolution of models, datasets, and future challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at https://github.com/JohnsonJiang1996/Awesome-VLA4AD{SicongJiang/Awesome-VLA4AD}.",
            "score": 8,
            "issue_id": 4738,
            "pub_date": "2025-06-30",
            "pub_date_card": {
                "ru": "30 Ğ¸ÑĞ½Ñ",
                "en": "June 30",
                "zh": "6æœˆ30æ—¥"
            },
            "hash": "ae24de54097b310f",
            "authors": [
                "Sicong Jiang",
                "Zilin Huang",
                "Kangan Qian",
                "Ziang Luo",
                "Tianze Zhu",
                "Yang Zhong",
                "Yihong Tang",
                "Menglin Kong",
                "Yunlong Wang",
                "Siwen Jiao",
                "Hao Ye",
                "Zihao Sheng",
                "Xin Zhao",
                "Tuopu Wen",
                "Zheng Fu",
                "Sikai Chen",
                "Kun Jiang",
                "Diange Yang",
                "Seongjin Choi",
                "Lijun Sun"
            ],
            "affiliations": [
                "McGill University, Canada",
                "State Key Laboratory of Intelligent Green Vehicle and Mobility, Tsinghua University, China",
                "Tsinghua University, China",
                "University of Minnesota-Twin Cities, USA",
                "University of Wisconsin-Madison, USA",
                "Xiaomi Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.24044.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#architecture",
                    "#benchmark",
                    "#survey",
                    "#dataset",
                    "#multimodal",
                    "#alignment",
                    "#interpretability"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "VLA: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼ Vision-Language-Action (VLA) Ğ¸ Ğ¸Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ VLA Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 20 Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ VLA Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾ÑĞ²ĞµÑ‰Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Driving the Future: Integrating Vision, Language, and Action in Autonomous Vehicles",
                    "desc": "This paper surveys the integration of Vision-Language-Action (VLA) paradigms in the context of autonomous driving. It discusses how multimodal large language models can enhance vehicles' abilities to understand visual inputs and natural language commands for decision-making. The authors analyze the evolution of VLA models, compare various architectures, and consolidate datasets relevant to this field. They also identify key challenges such as robustness and real-time efficiency, providing a roadmap for future research in VLA for autonomous driving."
                },
                "zh": {
                    "title": "è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼šè‡ªåŠ¨é©¾é©¶çš„æœªæ¥ä¹‹è·¯",
                    "desc": "è¿™ç¯‡è°ƒæŸ¥è®ºæ–‡å…¨é¢æ¦‚è¿°äº†è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰èŒƒå¼åŠå…¶åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ï¼Œè¯¦ç»†ä»‹ç»äº†æ¶æ„ç»„ä»¶ã€æ¨¡å‹æ¼”å˜ã€æ•°æ®é›†å’Œæœªæ¥æŒ‘æˆ˜ã€‚éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼ŒVLAèŒƒå¼å°†è§†è§‰æ„ŸçŸ¥ã€è‡ªç„¶è¯­è¨€ç†è§£å’Œæ§åˆ¶æ•´åˆåœ¨ä¸€ä¸ªç­–ç•¥ä¸­ã€‚ç ”ç©¶äººå‘˜æ­£åœ¨ç§¯æå°†è¿™äº›æ–¹æ³•é€‚åº”äºè½¦è¾†é¢†åŸŸï¼Œä»¥å®ç°èƒ½å¤Ÿç†è§£é«˜å±‚æŒ‡ä»¤ã€æ¨ç†å¤æ‚äº¤é€šåœºæ™¯å¹¶è‡ªä¸»å†³ç­–çš„è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€‚è®ºæ–‡è¿˜æ€»ç»“äº†ç°æœ‰æ•°æ®é›†å’ŒåŸºå‡†ï¼Œå¼ºè°ƒäº†å…±åŒæµ‹é‡é©¾é©¶å®‰å…¨æ€§ã€å‡†ç¡®æ€§å’Œè§£é‡Šè´¨é‡çš„åè®®ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.06853",
            "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using\n  Diffusion Models",
            "url": "https://huggingface.co/papers/2507.06853",
            "abstract": "DiffSpectra uses diffusion models with SE(3)-equivariant architecture and SpecFormer spectral encoder to accurately infer both 2D and 3D molecular structures from multi-modal spectral data.  \t\t\t\t\tAI-generated summary \t\t\t\t Molecular structure elucidation from spectra is a foundational problem in chemistry, with profound implications for compound identification, synthesis, and drug development. Traditional methods rely heavily on expert interpretation and lack scalability. Pioneering machine learning methods have introduced retrieval-based strategies, but their reliance on finite libraries limits generalization to novel molecules. Generative models offer a promising alternative, yet most adopt autoregressive SMILES-based architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that directly infers both 2D and 3D molecular structures from multi-modal spectral data using diffusion models. DiffSpectra formulates structure elucidation as a conditional generation process. Its denoising network is parameterized by Diffusion Molecule Transformer, an SE(3)-equivariant architecture that integrates topological and geometric information. Conditioning is provided by SpecFormer, a transformer-based spectral encoder that captures intra- and inter-spectral dependencies from multi-modal spectra. Extensive experiments demonstrate that DiffSpectra achieves high accuracy in structure elucidation, recovering exact structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model benefits significantly from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. These results highlight the effectiveness of spectrum-conditioned diffusion modeling in addressing the challenge of molecular structure elucidation. To our knowledge, DiffSpectra is the first framework to unify multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.",
            "score": 4,
            "issue_id": 4741,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "06caa3223981eb7b",
            "authors": [
                "Liang Wang",
                "Yu Rong",
                "Tingyang Xu",
                "Zhenyi Zhong",
                "Zhiyuan Liu",
                "Pengju Wang",
                "Deli Zhao",
                "Qiang Liu",
                "Shu Wu",
                "Liang Wang"
            ],
            "affiliations": [
                "College of Intelligence and Computing, Tianjin University",
                "DAMO Academy, Alibaba Group",
                "Hupan Lab",
                "NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences",
                "National University of Singapore",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.06853.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#science",
                    "#multimodal",
                    "#data",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "DiffSpectra - ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ 2D Ğ¸ 3D ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ SE(3)-ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ SpecFormer. DiffSpectra Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞµÑ‚Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Molecular Structure Elucidation with Diffusion Models",
                    "desc": "DiffSpectra is a novel generative framework that utilizes diffusion models to infer both 2D and 3D molecular structures from various types of spectral data. It employs an SE(3)-equivariant architecture to effectively incorporate geometric and topological information, enhancing the accuracy of molecular structure elucidation. The model's conditioning is achieved through a transformer-based spectral encoder called SpecFormer, which captures complex dependencies within the spectral data. Experimental results show that DiffSpectra significantly outperforms traditional methods, achieving high accuracy in recovering molecular structures, thus addressing the limitations of previous approaches in the field."
                },
                "zh": {
                    "title": "DiffSpectraï¼šå¤šæ¨¡æ€å…‰è°±ä¸‹çš„åˆ†å­ç»“æ„æ¨æ–­æ–°æ–¹æ³•",
                    "desc": "DiffSpectra æ˜¯ä¸€ä¸ªä½¿ç”¨æ‰©æ•£æ¨¡å‹å’Œ SE(3) ç­‰å˜æ¶æ„çš„ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿä»å¤šæ¨¡æ€å…‰è°±æ•°æ®ä¸­å‡†ç¡®æ¨æ–­å‡ºäºŒç»´å’Œä¸‰ç»´åˆ†å­ç»“æ„ã€‚è¯¥æ–¹æ³•å°†ç»“æ„é˜æ˜è§†ä¸ºä¸€ä¸ªæ¡ä»¶ç”Ÿæˆè¿‡ç¨‹ï¼Œåˆ©ç”¨ Diffusion Molecule Transformer ä½œä¸ºå»å™ªç½‘ç»œï¼Œæ•´åˆäº†æ‹“æ‰‘å’Œå‡ ä½•ä¿¡æ¯ã€‚SpecFormer ä½œä¸ºå…‰è°±ç¼–ç å™¨ï¼Œæ•æ‰äº†å¤šæ¨¡æ€å…‰è°±ä¸­çš„å†…éƒ¨å’Œå¤–éƒ¨ä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffSpectra åœ¨ç»“æ„é˜æ˜æ–¹é¢å…·æœ‰é«˜å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å…‰è°±æ¡ä»¶æ‰©æ•£å»ºæ¨¡åœ¨åˆ†å­ç»“æ„é˜æ˜ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05455",
            "title": "ModelCitizens: Representing Community Voices in Online Safety",
            "url": "https://huggingface.co/papers/2507.05455",
            "abstract": "A new dataset and models for toxic language detection incorporate diverse community perspectives and conversational context, improving accuracy over existing tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it is a highly subjective task, with perceptions of toxic language shaped by community norms and lived experience. Existing toxicity detection models are typically trained on annotations that collapse diverse annotator perspectives into a single ground truth, erasing important context-specific notions of toxicity such as reclaimed language. To address this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K toxicity annotations across diverse identity groups. To capture the role of conversational context on toxicity, typical of social media posts, we augment MODELCITIZENS posts with LLM-generated conversational scenarios. State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API, GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on context-augmented posts. Finally, we release LLAMACITIZEN-8B and GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS, which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our findings highlight the importance of community-informed annotation and modeling for inclusive content moderation. The data, models and code are available at https://github.com/asuvarna31/modelcitizens.",
            "score": 3,
            "issue_id": 4743,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 Ğ¸ÑĞ»Ñ",
                "en": "July 7",
                "zh": "7æœˆ7æ—¥"
            },
            "hash": "e6b75294b5b90953",
            "authors": [
                "Ashima Suvarna",
                "Christina Chance",
                "Karolina Naranjo",
                "Hamid Palangi",
                "Sophie Hao",
                "Thomas Hartvigsen",
                "Saadia Gabriel"
            ],
            "affiliations": [
                "Google",
                "New York University",
                "University of California, Los Angeles",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05455.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#ethics",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ—¨ï¸",
                "ru": {
                    "title": "Ğ˜Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°: ÑƒÑ‡ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MODELCITIZENS Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ½Ğ¸Ğ·ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLAMACITIZEN-8B Ğ¸ GEMMACITIZEN-12B, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Toxicity Detection with Community Insights",
                    "desc": "This paper presents MODELCITIZENS, a new dataset designed for detecting toxic language in social media, which includes 6.8K posts and 40K annotations reflecting diverse community perspectives. Traditional models often overlook the nuances of toxicity shaped by individual experiences and community norms, leading to inaccuracies. By incorporating conversational context through LLM-generated scenarios, the study demonstrates that existing tools struggle with this enriched dataset. The authors also introduce two new models, LLAMACITIZEN-8B and GEMMACITIZEN-12B, which significantly improve performance on toxicity detection tasks, emphasizing the need for community-informed approaches in AI moderation."
                },
                "zh": {
                    "title": "ç¤¾åŒºè§†è§’æå‡æ¯’æ€§è¯­è¨€æ£€æµ‹å‡†ç¡®æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†MODELCITIZENSï¼Œç”¨äºæ£€æµ‹æœ‰æ¯’è¯­è¨€ï¼ŒåŒ…å«6800æ¡ç¤¾äº¤åª’ä½“å¸–å­å’Œ4ä¸‡æ¡æ¯’æ€§æ³¨é‡Šï¼Œæ¶µç›–äº†å¤šæ ·çš„èº«ä»½ç¾¤ä½“ã€‚ç°æœ‰çš„æ¯’æ€§æ£€æµ‹æ¨¡å‹é€šå¸¸åŸºäºå•ä¸€çš„æ³¨é‡Šæ ‡å‡†ï¼Œå¿½è§†äº†ç¤¾åŒºè§„èŒƒå’Œå…·ä½“è¯­å¢ƒå¯¹æ¯’æ€§è¯­è¨€çš„å½±å“ã€‚é€šè¿‡å¼•å…¥å¯¹è¯åœºæ™¯çš„å¢å¼ºï¼ŒMODELCITIZENSèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç¤¾äº¤åª’ä½“ä¸­æ¯’æ€§è¯­è¨€çš„å¤æ‚æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºMODELCITIZENSå¾®è°ƒçš„æ¨¡å‹åœ¨æ¯’æ€§æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å·¥å…·ï¼Œå¼ºè°ƒäº†ç¤¾åŒºå‚ä¸çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07024",
            "title": "FlexOlmo: Open Language Models for Flexible Data Use",
            "url": "https://huggingface.co/papers/2507.07024",
            "abstract": "FlexOlmo, a distributed and data-flexible language model using a mixture-of-experts architecture, achieves significant improvements in performance across diverse tasks while respecting data privacy and ownership.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference.",
            "score": 2,
            "issue_id": 4760,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "3f1e93f06a4d48f4",
            "authors": [
                "Weijia Shi",
                "Akshita Bhagia",
                "Kevin Farhat",
                "Niklas Muennighoff",
                "Pete Walsh",
                "Jacob Morrison",
                "Dustin Schwenk",
                "Shayne Longpre",
                "Jake Poznanski",
                "Allyson Ettinger",
                "Daogao Liu",
                "Margaret Li",
                "Dirk Groeneveld",
                "Mike Lewis",
                "Wen-tau Yih",
                "Luca Soldaini",
                "Kyle Lo",
                "Noah A. Smith",
                "Luke Zettlemoyer",
                "Pang Wei Koh",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Sewon Min"
            ],
            "affiliations": [
                "Allen Institute for AI",
                "MIT",
                "Stanford University",
                "University of California, Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07024.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#ethics",
                    "#training",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "FlexOlmo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. FlexOlmo Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ MoE Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°Ğ² ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Empowering Language Models with Privacy and Flexibility",
                    "desc": "FlexOlmo is a novel language model that utilizes a mixture-of-experts (MoE) architecture to enhance performance on various tasks while ensuring data privacy. It allows for distributed training without sharing data, meaning different parts of the model can learn from separate datasets without compromising data ownership. During inference, users can flexibly include or exclude data associated with model parameters, providing control over data usage without needing additional training. This approach not only improves model performance significantly but also respects the preferences of data owners, making it suitable for sensitive applications."
                },
                "zh": {
                    "title": "FlexOlmoï¼šå°Šé‡æ•°æ®éšç§çš„è¯­è¨€æ¨¡å‹",
                    "desc": "FlexOlmoæ˜¯ä¸€ç§æ–°å‹çš„è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨ä¸å…±äº«æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚å®ƒæ”¯æŒæ•°æ®çµæ´»æ¨ç†ï¼Œå…è®¸æ ¹æ®éœ€è¦çµæ´»åœ°åŒ…å«æˆ–æ’é™¤æ¨¡å‹æ¨ç†ä¸­çš„å‚æ•°å’Œæ•°æ®ã€‚é€šè¿‡ç‹¬ç«‹è®­ç»ƒçš„ä¸“å®¶ä¸å…¬å…±æ•°æ®è®­ç»ƒçš„é€šç”¨ä¸“å®¶ç›¸ç»“åˆï¼ŒFlexOlmoåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šå®ç°äº†41%çš„ç›¸å¯¹æ€§èƒ½æå‡ï¼ŒåŒæ—¶å°Šé‡æ•°æ®æ‰€æœ‰è€…çš„éšç§å’Œè®¸å¯è¦æ±‚ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨å—ç›‘ç®¡è¡Œä¸šä¸­å¤„ç†æ•æ„Ÿæ•°æ®çš„ç ”ç©¶äººå‘˜å’Œæ•°æ®æ‰€æœ‰è€…æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.06607",
            "title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long\n  Generation",
            "url": "https://huggingface.co/papers/2507.06607",
            "abstract": "Gated Memory Units improve memory sharing in hybrid decoder architectures, enhancing efficiency and performance in language modeling tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence modeling. While hybrid architectures such as Samba and the decoder-decoder architecture, YOCO, have shown promising performance gains over Transformers, prior works have not investigated the efficiency potential of representation sharing between SSM layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet effective mechanism for efficient memory sharing across layers. We apply it to create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in the cross-decoder to share memory readout states from a Samba-based self-decoder. SambaY significantly enhances decoding efficiency, preserves linear pre-filling time complexity, and boosts long-context performance, all while eliminating the need for explicit positional encoding. Through extensive scaling experiments, we demonstrate that our model exhibits a significantly lower irreducible loss compared to a strong YOCO baseline, indicating superior performance scalability under large-scale compute regimes. Our largest model enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves significantly better performance than Phi4-mini-Reasoning on reasoning tasks such as Math500, AIME24/25, and GPQA Diamond without any reinforcement learning, while delivering up to 10x higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM inference framework. We release our training codebase on open-source data at https://github.com/microsoft/ArchScale.",
            "score": 2,
            "issue_id": 4757,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "203f70f6ea5f8c5c",
            "authors": [
                "Liliang Ren",
                "Congcong Chen",
                "Haoran Xu",
                "Young Jin Kim",
                "Adam Atkinson",
                "Zheng Zhan",
                "Jiankai Sun",
                "Baolin Peng",
                "Liyuan Liu",
                "Shuohang Wang",
                "Hao Cheng",
                "Jianfeng Gao",
                "Weizhu Chen",
                "Yelong Shen"
            ],
            "affiliations": [
                "Microsoft",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.06607.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization",
                    "#long_context",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "GMU: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ¼ĞµĞ½Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Gated Memory Unit (GMU) - Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ² Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ GMU Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ SambaY - Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SambaY Ğ¸Ğ¼ĞµĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ½ĞµÑƒÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ YOCO, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞšÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ², Phi4-mini-Flash-Reasoning, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Phi4-mini-Reasoning Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ¾ 10 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Efficiency with Gated Memory Units in Language Models",
                    "desc": "This paper presents Gated Memory Units (GMUs) as a novel mechanism to improve memory sharing in hybrid decoder architectures for language modeling. By integrating GMUs into the SambaY architecture, the authors enhance decoding efficiency and performance while maintaining linear pre-filling time complexity. The model demonstrates superior scalability and lower irreducible loss compared to existing architectures like YOCO, particularly in long-context tasks. Additionally, the enhanced model achieves significant improvements in reasoning tasks without the need for reinforcement learning, showcasing its effectiveness in handling large-scale computations."
                },
                "zh": {
                    "title": "é—¨æ§è®°å¿†å•å…ƒæå‡æ··åˆè§£ç å™¨æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æœºåˆ¶ï¼Œç§°ä¸ºé—¨æ§è®°å¿†å•å…ƒï¼ˆGMUï¼‰ï¼Œç”¨äºåœ¨æ··åˆè§£ç å™¨æ¶æ„ä¸­æé«˜è®°å¿†å…±äº«çš„æ•ˆç‡ã€‚é€šè¿‡å°†GMUåº”ç”¨äºSambaYæ¶æ„ï¼Œç ”ç©¶è€…ä»¬å®ç°äº†è·¨è§£ç å™¨çš„è®°å¿†è¯»å–çŠ¶æ€å…±äº«ï¼Œä»è€Œæ˜¾è‘—æå‡äº†è§£ç æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSambaYåœ¨é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”åœ¨å¤§è§„æ¨¡è®¡ç®—ç¯å¢ƒä¸‹å…·æœ‰æ›´å¥½çš„æ€§èƒ½å¯æ‰©å±•æ€§ã€‚è¯¥æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè§£ç ååé‡æé«˜äº†10å€ï¼Œä¸”æ— éœ€å¼ºåŒ–å­¦ä¹ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.06485",
            "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for\n  Efficient and Enhanced Video Reasoning",
            "url": "https://huggingface.co/papers/2507.06485",
            "abstract": "Video-RTS enhances video reasoning efficiency and accuracy through pure RL training and adaptive test-time scaling, reducing data and computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advances in reinforcement learning (RL)-based video reasoning with large language models (LLMs), data collection and finetuning remain significant challenges. These methods often rely on large-scale supervised fine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT) annotations, making them costly and hard to scale. To address this, we present Video-RTS, a new approach to improve video reasoning capability with drastically improved data efficiency by combining data-efficient RL with a video-adaptive test-time scaling (TTS) strategy. Based on observations about the data scaling of RL samples, we skip the resource-intensive SFT step and employ efficient pure-RL training with output-based rewards, requiring no additional annotations or extensive fine-tuning. Furthermore, to utilize computational resources more efficiently, we introduce a sparse-to-dense video TTS strategy that improves inference by iteratively adding frames based on output consistency. We validate our approach on multiple video reasoning benchmarks, showing that Video-RTS surpasses existing video reasoning models by an average of 2.4% in accuracy using only 3.6% training samples. For example, Video-RTS achieves a 4.2% improvement on Video-Holmes, a recent and challenging video reasoning benchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and adaptive video TTS offer complementary strengths, enabling Video-RTS's strong reasoning performance.",
            "score": 2,
            "issue_id": 4752,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "209daa331e7041bd",
            "authors": [
                "Ziyang Wang",
                "Jaehong Yoon",
                "Shoubin Yu",
                "Md Mohaiminul Islam",
                "Gedas Bertasius",
                "Mohit Bansal"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2507.06485.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#video",
                    "#rl",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Video-RTS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ drastically Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ğµ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾. Video-RTS Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ñ€ĞµÑÑƒÑ€ÑĞ¾ĞµĞ¼ĞºĞ¸Ğ¹ ÑÑ‚Ğ°Ğ¿ SFT Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ‡Ğ¸ÑÑ‚Ğ¾Ğµ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 2.4% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 3.6% Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Efficient Video Reasoning with Pure RL and Adaptive Scaling",
                    "desc": "Video-RTS is a novel approach that enhances video reasoning by utilizing pure reinforcement learning (RL) training and an adaptive test-time scaling (TTS) strategy. This method significantly reduces the need for large-scale supervised fine-tuning and extensive video data, making it more efficient and cost-effective. By focusing on output-based rewards and avoiding additional annotations, Video-RTS improves data efficiency and accuracy in video reasoning tasks. The results demonstrate that it outperforms existing models on various benchmarks, achieving higher accuracy with fewer training samples."
                },
                "zh": {
                    "title": "è§†é¢‘æ¨ç†çš„æ–°çªç ´ï¼šé«˜æ•ˆä¸å‡†ç¡®å¹¶å­˜",
                    "desc": "Video-RTSæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒå’Œè‡ªé€‚åº”æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰ç­–ç•¥ï¼Œæé«˜è§†é¢‘æ¨ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é¿å…äº†ä¼ ç»Ÿçš„å¤§è§„æ¨¡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå‡å°‘äº†æ•°æ®å’Œè®¡ç®—æˆæœ¬ã€‚Video-RTSé€šè¿‡è¾“å‡ºå¥–åŠ±è¿›è¡Œé«˜æ•ˆçš„çº¯RLè®­ç»ƒï¼Œæ— éœ€é¢å¤–çš„æ³¨é‡Šæˆ–å¹¿æ³›çš„å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-RTSåœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.06415",
            "title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning",
            "url": "https://huggingface.co/papers/2507.06415",
            "abstract": "PERK, a scalable approach using parameter-efficient adapters, enhances long-context reasoning by encoding contexts into a lightweight model at test time, achieving significant performance improvements over prompt-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-context reasoning requires accurately identifying relevant information in extensive, noisy input contexts. Previous research shows that using test-time learning to encode context directly into model parameters can effectively enable reasoning over noisy information. However, meta-learning methods for enabling test-time learning are prohibitively memory-intensive, preventing their application to long context settings. In this work, we propose PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for learning to encode long input contexts using gradient updates to a lightweight model adapter at test time. Specifically, PERK employs two nested optimization loops in a meta-training phase. The inner loop rapidly encodes contexts into a low-rank adapter (LoRA) that serves as a parameter-efficient memory module for the base model. Concurrently, the outer loop learns to use the updated adapter to accurately recall and reason over relevant information from the encoded long context. Our evaluations on several long-context reasoning tasks show that PERK significantly outperforms the standard prompt-based long-context baseline, achieving average absolute performance gains of up to 90% for smaller models (GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In general, PERK is more robust to reasoning complexity, length extrapolation, and the locations of relevant information in contexts. Finally, we show that while PERK is memory-intensive during training, it scales more efficiently at inference time than prompt-based long-context inference.",
            "score": 2,
            "issue_id": 4755,
            "pub_date": "2025-07-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ»Ñ",
                "en": "July 8",
                "zh": "7æœˆ8æ—¥"
            },
            "hash": "a3f686298107206f",
            "authors": [
                "Zeming Chen",
                "Angelika Romanou",
                "Gail Weiss",
                "Antoine Bosselut"
            ],
            "affiliations": [
                "Department of Computer and Communication Science EPFL Lausanne, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.06415.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#small_models",
                    "#reasoning",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "PERK: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ",
                    "desc": "PERK - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². PERK Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ñ†Ğ¸ĞºĞ»Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ LoRA, Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PERK Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹."
                },
                "en": {
                    "title": "Enhancing Long-Context Reasoning with Efficient Adapters",
                    "desc": "PERK (Parameter Efficient Reasoning over Knowledge) is a novel approach that enhances long-context reasoning by using lightweight model adapters during test time. It effectively encodes extensive input contexts into a low-rank adapter, allowing for efficient memory usage and improved reasoning capabilities. The method employs two optimization loops: one for quickly adapting the model to the context and another for refining the reasoning process. Evaluations demonstrate that PERK significantly outperforms traditional prompt-based methods, achieving substantial performance gains across various long-context reasoning tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆé•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "PERKï¼ˆå‚æ•°é«˜æ•ˆçŸ¥è¯†æ¨ç†ï¼‰æ˜¯ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨æµ‹è¯•æ—¶ä½¿ç”¨è½»é‡çº§æ¨¡å‹é€‚é…å™¨æ¥å¢å¼ºé•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¢¯åº¦æ›´æ–°å¿«é€Ÿå°†ä¸Šä¸‹æ–‡ç¼–ç åˆ°ä½ç§©é€‚é…å™¨ä¸­ï¼Œä½œä¸ºåŸºç¡€æ¨¡å‹çš„é«˜æ•ˆè®°å¿†æ¨¡å—ã€‚PERKåœ¨å¤šä¸ªé•¿ä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡ä¸­çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºæç¤ºçš„æ–¹æ³•ï¼Œå°¤å…¶åœ¨å°å‹æ¨¡å‹ï¼ˆå¦‚GPT-2ï¼‰ä¸Šï¼Œæ€§èƒ½æå‡å¯è¾¾90%ã€‚å°½ç®¡åœ¨è®­ç»ƒæ—¶å†…å­˜æ¶ˆè€—è¾ƒå¤§ï¼Œä½†åœ¨æ¨ç†æ—¶ï¼ŒPERKçš„æ•ˆç‡ä¼˜äºåŸºäºæç¤ºçš„é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.06260",
            "title": "Evaluating the Critical Risks of Amazon's Nova Premier under the\n  Frontier Model Safety Framework",
            "url": "https://huggingface.co/papers/2507.06260",
            "abstract": "Nova Premier is Amazon's most capable multimodal foundation model and teacher for model distillation. It processes text, images, and video with a one-million-token context window, enabling analysis of large codebases, 400-page documents, and 90-minute videos in a single prompt. We present the first comprehensive evaluation of Nova Premier's critical risk profile under the Frontier Model Safety Framework. Evaluations target three high-risk domains -- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber Operations, and Automated AI R&D -- and combine automated benchmarks, expert red-teaming, and uplift studies to determine whether the model exceeds release thresholds. We summarize our methodology and report core findings. Based on this evaluation, we find that Nova Premier is safe for public release as per our commitments made at the 2025 Paris AI Safety Summit. We will continue to enhance our safety evaluation and mitigation pipelines as new risks and capabilities associated with frontier models are identified.",
            "score": 2,
            "issue_id": 4738,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 Ğ¸ÑĞ»Ñ",
                "en": "July 7",
                "zh": "7æœˆ7æ—¥"
            },
            "hash": "e5d9ffe28d4a0466",
            "authors": [
                "Satyapriya Krishna",
                "Ninareh Mehrabi",
                "Abhinav Mohanty",
                "Matteo Memelli",
                "Vincent Ponzo",
                "Payal Motwani",
                "Rahul Gupta"
            ],
            "affiliations": [
                "Amazon Nova Responsible AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.06260.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#benchmark",
                    "#multimodal",
                    "#alignment",
                    "#security"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Nova Premier: Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ˜Ğ˜ Ğ¾Ñ‚ Amazon",
                    "desc": "Amazon Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ° Nova Premier - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ² Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Frontier Model Safety Framework Ğ² Ñ‚Ñ€ĞµÑ… ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ¥Ğ‘Ğ Ğ¯, ĞºĞ¸Ğ±ĞµÑ€Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ˜Ğ˜-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ²ÑˆĞ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ»Ğ¸Ğ·Ğ°. Amazon Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Nova Premier: A Safe Multimodal Model for Complex Analysis",
                    "desc": "Nova Premier is a powerful multimodal foundation model developed by Amazon that can understand and analyze text, images, and videos all at once. It has a large context window of one million tokens, allowing it to handle extensive inputs like long documents and videos in a single prompt. The paper presents a thorough evaluation of Nova Premier's safety using the Frontier Model Safety Framework, focusing on high-risk areas such as CBRN and Offensive Cyber Operations. The findings indicate that Nova Premier meets safety standards for public use, and the team plans to continuously improve safety measures as new challenges arise."
                },
                "zh": {
                    "title": "Nova Premierï¼šå®‰å…¨çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹",
                    "desc": "Nova Premieræ˜¯äºšé©¬é€Šæœ€å¼ºå¤§çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ï¼Œå…·æœ‰ä¸€ç™¾ä¸‡ä¸ªæ ‡è®°çš„ä¸Šä¸‹æ–‡çª—å£ã€‚è¿™ä½¿å¾—å®ƒèƒ½å¤Ÿåœ¨å•ä¸ªæç¤ºä¸­åˆ†æå¤§å‹ä»£ç åº“ã€400é¡µæ–‡æ¡£å’Œ90åˆ†é’Ÿè§†é¢‘ã€‚æˆ‘ä»¬é¦–æ¬¡å…¨é¢è¯„ä¼°äº†Nova Premieråœ¨å‰æ²¿æ¨¡å‹å®‰å…¨æ¡†æ¶ä¸‹çš„å…³é”®é£é™©ç‰¹å¾ï¼Œé‡ç‚¹å…³æ³¨åŒ–å­¦ã€ç”Ÿç‰©ã€æ”¾å°„æ€§å’Œæ ¸ï¼ˆCBRNï¼‰ã€è¿›æ”»æ€§ç½‘ç»œæ“ä½œå’Œè‡ªåŠ¨åŒ–äººå·¥æ™ºèƒ½ç ”å‘ç­‰é«˜é£é™©é¢†åŸŸã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒNova Premierç¬¦åˆ2025å¹´å·´é»äººå·¥æ™ºèƒ½å®‰å…¨å³°ä¼šçš„æ‰¿è¯ºï¼Œé€‚åˆå…¬å¼€å‘å¸ƒï¼Œå¹¶å°†ç»§ç»­å¢å¼ºå®‰å…¨è¯„ä¼°å’Œé£é™©ç¼“è§£æµç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.10251",
            "title": "SRT-H: A Hierarchical Framework for Autonomous Surgery via Language\n  Conditioned Imitation Learning",
            "url": "https://huggingface.co/papers/2505.10251",
            "abstract": "A hierarchical framework combining high-level task planning and low-level trajectory generation enables autonomous surgical procedures with high success rates in ex vivo experiments.  \t\t\t\t\tAI-generated summary \t\t\t\t Research on autonomous surgery has largely focused on simple task automation in controlled environments. However, real-world surgical applications demand dexterous manipulation over extended durations and generalization to the inherent variability of human tissue. These challenges remain difficult to address using existing logic-based or conventional end-to-end learning approaches. To address this gap, we propose a hierarchical framework for performing dexterous, long-horizon surgical steps. Our approach utilizes a high-level policy for task planning and a low-level policy for generating robot trajectories. The high-level planner plans in language space, generating task-level or corrective instructions that guide the robot through the long-horizon steps and correct for the low-level policy's errors. We validate our framework through ex vivo experiments on cholecystectomy, a commonly-practiced minimally invasive procedure, and conduct ablation studies to evaluate key components of the system. Our method achieves a 100\\% success rate across eight unseen ex vivo gallbladders, operating fully autonomously without human intervention. This work demonstrates step-level autonomy in a surgical procedure, marking a milestone toward clinical deployment of autonomous surgical systems.",
            "score": 2,
            "issue_id": 4749,
            "pub_date": "2025-05-15",
            "pub_date_card": {
                "ru": "15 Ğ¼Ğ°Ñ",
                "en": "May 15",
                "zh": "5æœˆ15æ—¥"
            },
            "hash": "2d7e1a44cbd9022b",
            "authors": [
                "Ji Woong Kim",
                "Juo-Tung Chen",
                "Pascal Hansen",
                "Lucy X. Shi",
                "Antony Goldenberg",
                "Samuel Schmidgall",
                "Paul Maria Scheikl",
                "Anton Deguet",
                "Brandon M. White",
                "De Ru Tsai",
                "Richard Cha",
                "Jeffrey Jopling",
                "Chelsea Finn",
                "Axel Krieger"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "Optosurgical",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.10251.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#agents",
                    "#optimization",
                    "#science",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ: Ğ¾Ñ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… ex vivo Ğ¿Ğ¾ Ñ…Ğ¾Ğ»ĞµÑ†Ğ¸ÑÑ‚ÑĞºÑ‚Ğ¾Ğ¼Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 100% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ±ĞµĞ· Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Achieving Autonomous Surgery with Hierarchical Planning",
                    "desc": "This paper presents a hierarchical framework designed for autonomous surgical procedures, combining high-level task planning with low-level trajectory generation. The high-level policy generates task instructions in language space, while the low-level policy focuses on executing these tasks through precise robot movements. This approach addresses the complexities of real-world surgeries, such as dexterous manipulation and variability in human tissue, which traditional methods struggle to manage. The framework was validated through successful ex vivo experiments on gallbladder removal, achieving a 100% success rate without human intervention, indicating a significant advancement towards autonomous surgical systems."
                },
                "zh": {
                    "title": "è‡ªä¸»å¤–ç§‘æ‰‹æœ¯çš„åˆ†å±‚æ¡†æ¶",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ†å±‚æ¡†æ¶ï¼Œç»“åˆäº†é«˜å±‚ä»»åŠ¡è§„åˆ’å’Œä½å±‚è½¨è¿¹ç”Ÿæˆï¼Œä»¥å®ç°è‡ªä¸»å¤–ç§‘æ‰‹æœ¯ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å¤æ‚çš„äººä½“ç»„ç»‡ç¯å¢ƒä¸­è¿›è¡Œçµæ´»çš„æ“ä½œï¼Œå¹¶ä¸”åœ¨é•¿æ—¶é—´çš„æ‰‹æœ¯è¿‡ç¨‹ä¸­ä¿æŒé«˜æˆåŠŸç‡ã€‚é€šè¿‡åœ¨èƒ†å›Šåˆ‡é™¤æœ¯çš„å®éªŒä¸­éªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…«ä¸ªæœªè§çš„æ ·æœ¬ä¸Šå®ç°äº†100%çš„æˆåŠŸç‡ï¼Œå®Œå…¨è‡ªä¸»æ“ä½œï¼Œæ— éœ€äººç±»å¹²é¢„ã€‚æ­¤é¡¹å·¥ä½œæ ‡å¿—ç€è‡ªä¸»å¤–ç§‘ç³»ç»Ÿå‘ä¸´åºŠåº”ç”¨è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07106",
            "title": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware\n  Feature Extractor",
            "url": "https://huggingface.co/papers/2507.07106",
            "abstract": "Text-to-image diffusion models enhance image-based question-answering by providing semantically rich and instruction-aware visual encodings, complementing CLIP and improving spatial and compositional reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have enabled image-based question-answering capabilities. However, a key limitation is the use of CLIP as the visual encoder; while it can capture coarse global information, it often can miss fine-grained details that are relevant to the input query. To address these shortcomings, this work studies whether pre-trained text-to-image diffusion models can serve as instruction-aware visual encoders. Through an analysis of their internal representations, we find diffusion features are both rich in semantics and can encode strong image-text alignment. Moreover, we find that we can leverage text conditioning to focus the model on regions relevant to the input question. We then investigate how to align these features with large language models and uncover a leakage phenomenon, where the LLM can inadvertently recover information from the original diffusion prompt. We analyze the causes of this leakage and propose a mitigation strategy. Based on these insights, we explore a simple fusion strategy that utilizes both CLIP and conditional diffusion features. We evaluate our approach on both general VQA and specialized MLLM benchmarks, demonstrating the promise of diffusion models for visual understanding, particularly in vision-centric tasks that require spatial and compositional reasoning. Our project page can be found https://vatsalag99.github.io/mustafar/.",
            "score": 1,
            "issue_id": 4751,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "bfb7b389a6a985bd",
            "authors": [
                "Vatsal Agarwal",
                "Matthew Gwilliam",
                "Gefen Kohavi",
                "Eshan Verma",
                "Daniel Ulbricht",
                "Abhinav Shrivastava"
            ],
            "affiliations": [
                "Apple",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07106.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#leakage",
                    "#reasoning"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² CLIP Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ·Ñ€ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Enhancing Image Question-Answering with Diffusion Models",
                    "desc": "This paper explores the use of text-to-image diffusion models as visual encoders for image-based question-answering tasks. Unlike CLIP, which captures broad visual information, diffusion models provide detailed semantic representations that enhance image-text alignment. The authors demonstrate that these models can focus on relevant image regions based on the input question, improving spatial and compositional reasoning. They also address a leakage issue where large language models can unintentionally access original diffusion prompts, proposing a fusion strategy that combines the strengths of both CLIP and diffusion features."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹æå‡å›¾åƒé—®ç­”èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒé—®ç­”ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æå‡è§†è§‰ç¼–ç çš„è¯­ä¹‰ä¸°å¯Œæ€§å’ŒæŒ‡ä»¤æ„ŸçŸ¥èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„CLIPè§†è§‰ç¼–ç å™¨è™½ç„¶èƒ½å¤Ÿæ•æ‰å…¨å±€ä¿¡æ¯ï¼Œä½†å¸¸å¸¸å¿½ç•¥ä¸è¾“å…¥æŸ¥è¯¢ç›¸å…³çš„ç»†èŠ‚ã€‚é€šè¿‡åˆ†ææ‰©æ•£æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºï¼Œå‘ç°å…¶åœ¨è¯­ä¹‰ä¸Šéå¸¸ä¸°å¯Œï¼Œå¹¶ä¸”èƒ½å¤Ÿå¼ºæœ‰åŠ›åœ°ç¼–ç å›¾åƒä¸æ–‡æœ¬çš„å¯¹é½ã€‚ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ç®€å•çš„èåˆç­–ç•¥ï¼Œç»“åˆäº†CLIPå’Œæ¡ä»¶æ‰©æ•£ç‰¹å¾ï¼Œä»¥æé«˜è§†è§‰ç†è§£èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç©ºé—´å’Œç»„åˆæ¨ç†çš„ä»»åŠ¡ä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05980",
            "title": "RabakBench: Scaling Human Annotations to Construct Localized\n  Multilingual Safety Benchmarks for Low-Resource Languages",
            "url": "https://huggingface.co/papers/2507.05980",
            "abstract": "Large language models (LLMs) and their safety classifiers often perform poorly on low-resource languages due to limited training data and evaluation benchmarks. This paper introduces RabakBench, a new multilingual safety benchmark localized to Singapore's unique linguistic context, covering Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a scalable three-stage pipeline: (i) Generate - adversarial example generation by augmenting real Singlish web content with LLM-driven red teaming; (ii) Label - semi-automated multi-label safety annotation using majority-voted LLM labelers aligned with human judgments; and (iii) Translate - high-fidelity translation preserving linguistic nuance and toxicity across languages. The final dataset comprises over 5,000 safety-labeled examples across four languages and six fine-grained safety categories with severity levels. Evaluations of 11 popular open-source and closed-source guardrail classifiers reveal significant performance degradation. RabakBench not only enables robust safety evaluation in Southeast Asian multilingual settings but also offers a reproducible framework for building localized safety datasets in low-resource environments. The benchmark dataset, including the human-verified translations, and evaluation code are publicly available.",
            "score": 1,
            "issue_id": 4760,
            "pub_date": "2025-07-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ»Ñ",
                "en": "July 8",
                "zh": "7æœˆ8æ—¥"
            },
            "hash": "de40adc01f68a673",
            "authors": [
                "Gabriel Chua",
                "Leanne Tan",
                "Ziyu Ge",
                "Roy Ka-Wei Lee"
            ],
            "affiliations": [
                "GovTech, Singapore",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05980.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multilingual",
                    "#low_resource",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ´Ğ»Ñ Ğ®Ğ³Ğ¾-Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞĞ·Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RabakBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¡Ğ¸Ğ½Ğ³Ğ°Ğ¿ÑƒÑ€Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑĞ·Ñ‹ĞºĞ°. RabakBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 5000 Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° 11 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "RabakBench: Enhancing Safety for Multilingual LLMs in Low-Resource Settings",
                    "desc": "This paper addresses the challenges faced by large language models (LLMs) in ensuring safety for low-resource languages. It introduces RabakBench, a multilingual safety benchmark tailored to Singapore's linguistic diversity, including Singlish, Chinese, Malay, and Tamil. The benchmark is created through a three-stage process: generating adversarial examples, semi-automated safety labeling, and translating content while maintaining linguistic nuances. The resulting dataset, with over 5,000 examples across multiple languages and safety categories, highlights the performance issues of existing classifiers and provides a framework for future localized safety evaluations."
                },
                "zh": {
                    "title": "æå‡ä½èµ„æºè¯­è¨€çš„å®‰å…¨æ€§è¯„ä¼°",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†RabakBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ–°åŠ å¡ç‹¬ç‰¹è¯­è¨€ç¯å¢ƒçš„å¤šè¯­è¨€å®‰å…¨åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„è¡¨ç°ä¸ä½³é—®é¢˜ã€‚RabakBenché€šè¿‡ä¸€ä¸ªå¯æ‰©å±•çš„ä¸‰é˜¶æ®µæµç¨‹æ„å»ºï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€åŠè‡ªåŠ¨åŒ–å¤šæ ‡ç­¾å®‰å…¨æ³¨é‡Šå’Œé«˜ä¿çœŸç¿»è¯‘ã€‚æœ€ç»ˆæ•°æ®é›†åŒ…å«è¶…è¿‡5000ä¸ªå®‰å…¨æ ‡è®°çš„ç¤ºä¾‹ï¼Œæ¶µç›–å››ç§è¯­è¨€å’Œå…­ä¸ªç»†åˆ†çš„å®‰å…¨ç±»åˆ«ã€‚è¯¥åŸºå‡†ä¸ä»…æ”¯æŒä¸œå—äºšå¤šè¯­è¨€ç¯å¢ƒä¸­çš„å®‰å…¨è¯„ä¼°ï¼Œè¿˜æä¾›äº†åœ¨ä½èµ„æºç¯å¢ƒä¸­æ„å»ºæœ¬åœ°åŒ–å®‰å…¨æ•°æ®é›†çš„å¯é‡å¤æ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.01702",
            "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large\n  Language Models on Harmfulness",
            "url": "https://huggingface.co/papers/2507.01702",
            "abstract": "AdamMeme, an adaptive agent-based framework, evaluates multimodal Large Language Models' understanding of harmful memes through iterative updates and multi-agent collaboration, revealing model-specific weaknesses.  \t\t\t\t\tAI-generated summary \t\t\t\t The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at https://github.com/Lbotirx/AdamMeme.",
            "score": 1,
            "issue_id": 4745,
            "pub_date": "2025-07-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ»Ñ",
                "en": "July 2",
                "zh": "7æœˆ2æ—¥"
            },
            "hash": "6465ab2f7869c041",
            "authors": [
                "Zixin Chen",
                "Hongzhan Lin",
                "Kaixin Li",
                "Ziyang Luo",
                "Zhen Ye",
                "Guang Chen",
                "Zhiyong Huang",
                "Jing Ma"
            ],
            "affiliations": [
                "BUPT",
                "HKBU",
                "HKUST",
                "NUS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.01702.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#multimodal",
                    "#benchmark",
                    "#reasoning",
                    "#interpretability",
                    "#alignment"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "AdamMeme: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ğ² Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¼ĞµĞ¼Ğ¾Ğ²",
                    "desc": "AdamMeme - ÑÑ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¼ĞµĞ¼Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ¼ĞµÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, AdamMeme Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ¼Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸."
                },
                "en": {
                    "title": "Unveiling the Weaknesses of mLLMs in Understanding Harmful Memes",
                    "desc": "AdamMeme is a new framework designed to evaluate how well multimodal Large Language Models (mLLMs) understand harmful memes. It uses an adaptive, agent-based approach that allows for continuous updates and collaboration among multiple agents. This method helps identify specific weaknesses in different mLLMs by testing them with evolving meme data. The framework provides a more dynamic and thorough assessment of mLLMs compared to traditional static benchmarks."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤šæ¨¡æ€æ¨¡å‹çš„æœ‰å®³æ€§ç†è§£å¼±ç‚¹",
                    "desc": "AdamMemeæ˜¯ä¸€ä¸ªè‡ªé€‚åº”çš„åŸºäºä»£ç†çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¯¹æœ‰å®³è¡¨æƒ…åŒ…çš„ç†è§£èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡è¿­ä»£æ›´æ–°å’Œå¤šä»£ç†åä½œï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹çš„ç‰¹å®šå¼±ç‚¹ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¾èµ–äºé™æ€æ•°æ®é›†ï¼Œæ— æ³•åŠæ—¶åæ˜ åœ¨çº¿è¡¨æƒ…åŒ…çš„åŠ¨æ€æ¼”å˜ã€‚é€šè¿‡æŒ‘æˆ˜æ€§æ ·æœ¬çš„æ›´æ–°ï¼ŒAdamMemeèƒ½å¤Ÿå…¨é¢è¯„ä¼°æ¨¡å‹åœ¨è§£è¯»æœ‰å®³æ€§æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-10.html",
    "link_next": "2025-07-14.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "10.07",
        "en": "07/10",
        "zh": "7æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "14.07",
        "en": "07/14",
        "zh": "7æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 3,
        "#benchmark": 11,
        "#agents": 4,
        "#cv": 3,
        "#rl": 5,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 6,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 2,
        "#training": 11,
        "#robotics": 2,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 2,
        "#reasoning": 9,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 1,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 7,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 2
    }
}