{
    "date": {
        "ru": "11 Ğ¸ÑĞ»Ñ",
        "en": "July 11",
        "zh": "7æœˆ11æ—¥"
    },
    "time_utc": "2025-07-11 23:11",
    "weekday": 4,
    "issue_id": 4782,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.07966",
            "title": "Scaling RL to Long Videos",
            "url": "https://huggingface.co/papers/2507.07966",
            "abstract": "A framework for scaling vision-language models to long videos using reinforcement learning, achieving strong performance on various reasoning tasks with a specialized training infrastructure.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks a firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens).",
            "score": 89,
            "issue_id": 4761,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ»Ñ",
                "en": "July 10",
                "zh": "7æœˆ10æ—¥"
            },
            "hash": "4ab23da398f0e8d8",
            "authors": [
                "Yukang Chen",
                "Wei Huang",
                "Baifeng Shi",
                "Qinghao Hu",
                "Hanrong Ye",
                "Ligeng Zhu",
                "Zhijian Liu",
                "Pavlo Molchanov",
                "Jan Kautz",
                "Xiaojuan Qi",
                "Sifei Liu",
                "Hongxu Yin",
                "Yao Lu",
                "Song Han"
            ],
            "affiliations": [
                "HKU",
                "MIT",
                "NVIDIA",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07966.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#reasoning",
                    "#multimodal",
                    "#rl",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LongVideo-Reason Ñ 52 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ RL, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ MR-SP. ĞœĞ¾Ğ´ĞµĞ»ÑŒ LongVILA-R1-7B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Scaling Vision-Language Models for Long Video Reasoning",
                    "desc": "This paper presents a framework designed to enhance vision-language models (VLMs) for reasoning tasks involving long videos. It introduces a large dataset called LongVideo-Reason, which contains 52,000 question-answer pairs related to long videos, facilitating high-quality reasoning across various domains. The framework employs a two-stage training process that combines chain-of-thought supervised fine-tuning with reinforcement learning, optimizing the model's performance. Additionally, it features a specialized training infrastructure, Multi-modal Reinforcement Sequence Parallelism, which significantly accelerates the training process for long video reasoning tasks."
                },
                "zh": {
                    "title": "é•¿è§†é¢‘æ¨ç†çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œç”¨äºé€šè¿‡å¼ºåŒ–å­¦ä¹ å°†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ‰©å±•åˆ°é•¿è§†é¢‘æ¨ç†ã€‚æˆ‘ä»¬æ•´åˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€ä¸ªåŒ…å«52Ké•¿è§†é¢‘é—®ç­”å¯¹çš„å¤§è§„æ¨¡æ•°æ®é›†LongVideo-Reasonï¼Œä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œä»¥åŠä¸€ä¸ªåä¸ºå¤šæ¨¡æ€å¼ºåŒ–åºåˆ—å¹¶è¡Œï¼ˆMR-SPï¼‰çš„è®­ç»ƒåŸºç¡€è®¾æ–½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLongVILA-R1-7Båœ¨é•¿è§†é¢‘é—®ç­”åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨æ—¶é—´æ¨ç†ã€ç›®æ ‡å’Œç›®çš„æ¨ç†ã€ç©ºé—´æ¨ç†ç­‰æ–¹é¢è¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨é•¿è§†é¢‘å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­å®ç°äº†é«˜è¾¾2.1å€çš„åŠ é€Ÿï¼Œæ ‡å¿—ç€åœ¨VLMsä¸­è¿›è¡Œé•¿è§†é¢‘æ¨ç†çš„åšå®ä¸€æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05964",
            "title": "T-LoRA: Single Image Diffusion Model Customization Without Overfitting",
            "url": "https://huggingface.co/papers/2507.05964",
            "abstract": "T-LoRA, a timestep-dependent low-rank adaptation framework, enhances diffusion model personalization with a dynamic fine-tuning strategy and orthogonal initialization, achieving better concept fidelity and text alignment in data-limited settings.  \t\t\t\t\tAI-generated summary \t\t\t\t While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve a superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available at https://github.com/ControlGenAI/T-LoRA.",
            "score": 81,
            "issue_id": 4766,
            "pub_date": "2025-07-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ»Ñ",
                "en": "July 8",
                "zh": "7æœˆ8æ—¥"
            },
            "hash": "8df9f241b664bc80",
            "authors": [
                "Vera Soboleva",
                "Aibek Alanov",
                "Andrey Kuznetsov",
                "Konstantin Sobolev"
            ],
            "affiliations": [
                "AIRI, HSE University",
                "AIRI, MSU",
                "AIRI, Sber, Innopolis",
                "HSE University, AIRI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05964.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#optimization",
                    "#training",
                    "#low_resource"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "T-LoRA: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸",
                    "desc": "T-LoRA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ°, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ÑƒÑ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ°. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ñƒ. T-LoRA Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ T-LoRA Ğ½Ğ°Ğ´ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Personalizing Diffusion Models with T-LoRA: Dynamic Fine-Tuning for Better Results",
                    "desc": "T-LoRA is a framework designed to improve the personalization of diffusion models, particularly when only a limited amount of data is available. It introduces a dynamic fine-tuning strategy that adapts to different diffusion timesteps, addressing the issue of overfitting that often occurs with higher timesteps. Additionally, T-LoRA employs orthogonal initialization to maintain independence among adapter components, enhancing the model's ability to generate accurate outputs. Through extensive testing, T-LoRA demonstrates superior performance in balancing concept fidelity and text alignment compared to traditional methods."
                },
                "zh": {
                    "title": "T-LoRAï¼šæ‰©æ•£æ¨¡å‹ä¸ªæ€§åŒ–çš„æ–°çªç ´",
                    "desc": "T-LoRAæ˜¯ä¸€ç§æ—¶é—´æ­¥ä¾èµ–çš„ä½ç§©é€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€å¾®è°ƒç­–ç•¥å’Œæ­£äº¤åˆå§‹åŒ–æ¥å¢å¼ºæ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨æ ·æœ¬æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ‰©æ•£æ¨¡å‹å¾®è°ƒå¸¸å¸¸å‡ºç°çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œä»è€Œæé«˜äº†æ¦‚å¿µçš„ä¿çœŸåº¦å’Œæ–‡æœ¬å¯¹é½èƒ½åŠ›ã€‚T-LoRAçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶åŠ¨æ€å¾®è°ƒç­–ç•¥å’Œæƒé‡å‚æ•°åŒ–æŠ€æœ¯ï¼Œä½¿å¾—é€‚é…å™¨ç»„ä»¶ä¹‹é—´ä¿æŒç‹¬ç«‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT-LoRAåœ¨æ•°æ®æœ‰é™å’Œèµ„æºå—é™çš„æƒ…å†µä¸‹ï¼Œä¼˜äºæ ‡å‡†çš„LoRAå’Œå…¶ä»–ä¸ªæ€§åŒ–æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07999",
            "title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology",
            "url": "https://huggingface.co/papers/2507.07999",
            "abstract": "TreeBench evaluates visual grounded reasoning through subtle target detection, traceable evidence, and second-order reasoning, while TreeVGR enhances this with joint localization and reasoning using reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically referencing visual regions, just like human \"thinking with images\". However, no benchmark exists to evaluate these capabilities holistically. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark built on three principles: (1) focused visual perception of subtle targets in complex scenes, (2) traceable evidence via bounding box evaluation, and (3) second-order reasoning to test object interactions and spatial hierarchies beyond simple object localization. Prioritizing images with dense objects, we initially sample 1K high-quality images from SA-1B, and incorporate eight LMM experts to manually annotate questions, candidate options, and answers for each image. After three stages of quality control, TreeBench consists of 405 challenging visual question-answering pairs, even the most advanced models struggle with this benchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to supervise localization and reasoning jointly with reinforcement learning, enabling accurate localizations and explainable reasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is key to advancing vision-grounded reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.",
            "score": 36,
            "issue_id": 4761,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ»Ñ",
                "en": "July 10",
                "zh": "7æœˆ10æ—¥"
            },
            "hash": "e52c2296896d713c",
            "authors": [
                "Haochen Wang",
                "Xiangtai Li",
                "Zilong Huang",
                "Anran Wang",
                "Jiacong Wang",
                "Tao Zhang",
                "Jiani Zheng",
                "Sule Bai",
                "Zijian Kang",
                "Jiashi Feng",
                "Zhuochen Wang",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "ByteDance",
                "NLPR, MAIS, CASIA",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07999.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#training",
                    "#interpretability",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞÑ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "TreeBench - ÑÑ‚Ğ¾ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ…: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ°. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 405 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ¼. TreeVGR - ÑÑ‚Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen2.5-VL-7B, Ğ¾Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Visual Grounded Reasoning with TreeBench and TreeVGR",
                    "desc": "This paper introduces TreeBench, a benchmark designed to evaluate visual grounded reasoning by focusing on subtle target detection, traceable evidence, and second-order reasoning. It highlights the need for a comprehensive assessment tool as existing models struggle with complex visual tasks, achieving less than 60% accuracy on the benchmark. TreeVGR is proposed as an enhancement that uses reinforcement learning to improve joint localization and reasoning, demonstrating significant performance gains over existing models. The research emphasizes the importance of traceability in developing advanced visual reasoning capabilities."
                },
                "zh": {
                    "title": "æå‡è§†è§‰æ¨ç†çš„å¯è¿½æº¯æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†TreeBenchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰åŸºç¡€æ¨ç†çš„åŸºå‡†ï¼Œä¾§é‡äºå¤æ‚åœºæ™¯ä¸­å¾®å¦™ç›®æ ‡çš„æ£€æµ‹ã€å¯è¿½æº¯è¯æ®å’ŒäºŒé˜¶æ¨ç†ã€‚TreeVGRåˆ™é€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºäº†è¿™ä¸€è¿‡ç¨‹ï¼Œå®ç°äº†å®šä½å’Œæ¨ç†çš„è”åˆè®­ç»ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„å…ˆè¿›æ¨¡å‹åœ¨TreeBenchåŸºå‡†ä¸Šè¡¨ç°ä¸ä½³ï¼Œå‡†ç¡®ç‡æœªè¶…è¿‡60%ã€‚é€šè¿‡å¼•å…¥å¯è¿½æº¯æ€§ï¼ŒTreeVGRæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨è§†è§‰åŸºç¡€æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07984",
            "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding",
            "url": "https://huggingface.co/papers/2507.07984",
            "abstract": "OST-Bench evaluates multimodal large language models in online spatio-temporal reasoning tasks, revealing challenges in handling complex spatial cues and long-term memory in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have shown remarkable capabilities in integrating vision and language for complex reasoning. While most existing benchmarks evaluate models under offline settings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a benchmark designed to evaluate Online Spatio-Temporal understanding from the perspective of an agent actively exploring a scene. The Online aspect emphasizes the need to process and reason over incrementally acquired observations, while the Spatio-Temporal component requires integrating current visual inputs with historical memory to support dynamic spatial reasoning. OST-Bench better reflects the challenges of real-world embodied perception. Built on an efficient data collection pipeline, OST-Bench consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that they fall short on tasks requiring complex spatio-temporal reasoning. Under the online setting, their accuracy declines as the exploration horizon extends and the memory grows. Through further experimental analysis, we identify common error patterns across models and find that both complex clue-based spatial reasoning demands and long-term memory retrieval requirements significantly drop model performance along two separate axes, highlighting the core challenges that must be addressed to improve online embodied reasoning. To foster further research and development in the field, our codes, dataset, and benchmark are available. Our project page is: https://rbler1234.github.io/OSTBench.github.io/",
            "score": 29,
            "issue_id": 4765,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ»Ñ",
                "en": "July 10",
                "zh": "7æœˆ10æ—¥"
            },
            "hash": "044eb90ac618689f",
            "authors": [
                "JingLi Lin",
                "Chenming Zhu",
                "Runsen Xu",
                "Xiaohan Mao",
                "Xihui Liu",
                "Tai Wang",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07984.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#multimodal",
                    "#dataset",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ˜Ğ˜: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "OST-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1,4 Ñ‚Ñ‹Ñ. ÑÑ†ĞµĞ½ Ğ¸ 10 Ñ‚Ñ‹Ñ. Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… 3D-ÑĞºĞ°Ğ½Ğ¾Ğ² Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ."
                },
                "en": {
                    "title": "Evaluating MLLMs in Real-World Spatio-Temporal Reasoning",
                    "desc": "OST-Bench is a new benchmark that tests multimodal large language models (MLLMs) on their ability to understand and reason about space and time while interacting with real-world environments. Unlike traditional benchmarks that use fixed inputs, OST-Bench evaluates models in an online setting where they must process information as they explore scenes. The study reveals that current MLLMs struggle with complex spatial reasoning and long-term memory tasks, especially as the amount of information increases. By identifying common errors, the research highlights key areas for improvement in online embodied perception and reasoning."
                },
                "zh": {
                    "title": "åœ¨çº¿æ—¶ç©ºæ¨ç†çš„æ–°æŒ‘æˆ˜",
                    "desc": "OST-Benchæ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åœ¨çº¿æ—¶ç©ºæ¨ç†ä»»åŠ¡ä¸­çš„åŸºå‡†ã€‚å®ƒå¼ºè°ƒäº†åœ¨åŠ¨æ€åœºæ™¯ä¸­å¤„ç†å¤æ‚ç©ºé—´çº¿ç´¢å’Œé•¿æœŸè®°å¿†çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹1.4åƒä¸ªåœºæ™¯å’Œ1ä¸‡å¯¹é—®ç­”çš„è¯„ä¼°ï¼Œå‘ç°ç°æœ‰æ¨¡å‹åœ¨å¤æ‚æ—¶ç©ºæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ¢ç´¢èŒƒå›´æ‰©å¤§å’Œè®°å¿†å¢é•¿æ—¶å‡†ç¡®ç‡ä¸‹é™ã€‚è¯¥åŸºå‡†æ—¨åœ¨æ¨åŠ¨åœ¨çº¿å…·èº«æ¨ç†çš„ç ”ç©¶ä¸å‘å±•ï¼Œæä¾›äº†æ•°æ®é›†å’Œä»£ç ä»¥ä¾›è¿›ä¸€æ­¥æ¢ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07990",
            "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
            "url": "https://huggingface.co/papers/2507.07990",
            "abstract": "A spatio-temporal token merging method improves video LLM efficiency by exploiting redundancy, achieving significant speed-ups with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Video large language models (LLMs) achieve strong video understanding by leveraging a large number of spatio-temporal tokens, but suffer from quadratic computational scaling with token count. To address this, we propose a training-free spatio-temporal token merging method, named STTM. Our key insight is to exploit local spatial and temporal redundancy in video data which has been overlooked in prior work. STTM first transforms each frame into multi-granular spatial tokens using a coarse-to-fine search over a quadtree structure, then performs directed pairwise merging across the temporal dimension. This decomposed merging approach outperforms existing token reduction methods across six video QA benchmarks. Notably, STTM achieves a 2times speed-up with only a 0.5% accuracy drop under a 50% token budget, and a 3times speed-up with just a 2% drop under a 30% budget. Moreover, STTM is query-agnostic, allowing KV cache reuse across different questions for the same video. The project page is available at https://www.jshyun.me/projects/sttm.",
            "score": 24,
            "issue_id": 4766,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ»Ñ",
                "en": "July 10",
                "zh": "7æœˆ10æ—¥"
            },
            "hash": "eff3cb8ac467d2a7",
            "authors": [
                "Jeongseok Hyun",
                "Sukjun Hwang",
                "Su Ho Han",
                "Taeoh Kim",
                "Inwoong Lee",
                "Dongyoon Wee",
                "Joon-Young Lee",
                "Seon Joo Kim",
                "Minho Shim"
            ],
            "affiliations": [
                "Adobe Research",
                "Carnegie Mellon University",
                "NAVER Cloud",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07990.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² (STTM) Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. STTM Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ĞºĞ°Ğ´Ñ€ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ²Ğ°Ğ´Ñ€Ğ¾Ğ´ĞµÑ€ĞµĞ²Ğ°, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-QA. STTM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ 50% Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Boosting Video LLM Efficiency with Smart Token Merging",
                    "desc": "This paper introduces a new method called Spatio-Temporal Token Merging (STTM) to enhance the efficiency of video large language models (LLMs). The method reduces the number of tokens used in video processing by merging them based on local spatial and temporal redundancies, which helps to maintain performance while speeding up computations. STTM achieves significant speed improvements, allowing for up to three times faster processing with minimal accuracy loss. Additionally, it is designed to be query-agnostic, enabling the reuse of cached information across different questions about the same video."
                },
                "zh": {
                    "title": "æ—¶ç©ºä»¤ç‰Œåˆå¹¶ï¼Œæå‡è§†é¢‘æ¨¡å‹æ•ˆç‡ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ—¶ç©ºä»¤ç‰Œåˆå¹¶æ–¹æ³•ï¼ˆSTTMï¼‰ï¼Œæ—¨åœ¨æé«˜è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨è§†é¢‘æ•°æ®ä¸­çš„å±€éƒ¨ç©ºé—´å’Œæ—¶é—´å†—ä½™ï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚STTMé¦–å…ˆå°†æ¯å¸§å›¾åƒè½¬åŒ–ä¸ºå¤šç²’åº¦çš„ç©ºé—´ä»¤ç‰Œï¼Œç„¶ååœ¨æ—¶é—´ç»´åº¦ä¸Šè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„é…å¯¹åˆå¹¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTTMåœ¨å¤šä¸ªè§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„ä»¤ç‰Œå‡å°‘æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07998",
            "title": "PyVision: Agentic Vision with Dynamic Tooling",
            "url": "https://huggingface.co/papers/2507.07998",
            "abstract": "PyVision, an interactive framework, enables LLMs to autonomously create and refine Python-based tools for visual reasoning, achieving significant performance improvements across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning.",
            "score": 20,
            "issue_id": 4761,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ»Ñ",
                "en": "July 10",
                "zh": "7æœˆ10æ—¥"
            },
            "hash": "ab8504b49800fe67",
            "authors": [
                "Shitian Zhao",
                "Haoquan Zhang",
                "Shaoheng Lin",
                "Ming Li",
                "Qilong Wu",
                "Kaipeng Zhang",
                "Chen Wei"
            ],
            "affiliations": [
                "CUHK",
                "NUS",
                "Rice University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07998.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#interpretability",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "PyVision: LLM ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°",
                    "desc": "PyVision - ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Python Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ. PyVision Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LLM Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ GPT-4.1 Ğ½Ğ° 7.8% Ğ² V* Ğ¸ Claude-4.0-Sonnet Ğ½Ğ° 31.1% Ğ² VLMsAreBlind-mini. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑˆĞ°Ğ³ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¼Ñƒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering LLMs with Dynamic Tool Creation for Visual Reasoning",
                    "desc": "PyVision is an innovative framework that empowers large language models (LLMs) to autonomously create and improve Python tools for visual reasoning tasks. Unlike previous methods that relied on fixed workflows, PyVision allows for dynamic tool generation and execution, enhancing flexibility in problem-solving. The framework has been evaluated across various benchmarks, showing significant performance improvements, such as a 7.8% increase for GPT-4.1 and a 31.1% boost for Claude-4.0-Sonnet. This advancement signifies a shift towards more agentic capabilities in visual reasoning, where models can not only utilize existing tools but also invent new ones."
                },
                "zh": {
                    "title": "åŠ¨æ€å·¥å…·ï¼Œæ™ºèƒ½æ¨ç†çš„æ–°çºªå…ƒ",
                    "desc": "PyVisionæ˜¯ä¸€ä¸ªäº¤äº’å¼æ¡†æ¶ï¼Œå…è®¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªä¸»åˆ›å»ºå’Œæ”¹è¿›åŸºäºPythonçš„è§†è§‰æ¨ç†å·¥å…·ã€‚ä¸ä»¥å¾€çš„é™æ€å·¥å…·é›†ä¸åŒï¼ŒPyVisionæ”¯æŒå¤šè½®äº¤äº’ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å…·ä½“ä»»åŠ¡çµæ´»ç”Ÿæˆå’Œæ‰§è¡Œå·¥å…·ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒPyVisionåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œä¾‹å¦‚GPT-4.1åœ¨V*ä¸Šæå‡äº†7.8%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŠ¨æ€å·¥å…·çš„ä½¿ç”¨ä½¿æ¨¡å‹ä¸ä»…èƒ½å¤Ÿä½¿ç”¨å·¥å…·ï¼Œè¿˜èƒ½å‘æ˜æ–°å·¥å…·ï¼Œæ¨åŠ¨è§†è§‰æ¨ç†çš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07982",
            "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling",
            "url": "https://huggingface.co/papers/2507.07982",
            "abstract": "Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, a simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the model's intermediate representations toward geometry-aware structure by aligning them with features from a pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera view-conditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods. Project page: https://GeometryForcing.github.io.",
            "score": 18,
            "issue_id": 4762,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ»Ñ",
                "en": "July 10",
                "zh": "7æœˆ10æ—¥"
            },
            "hash": "fbe6e1954d8e9c30",
            "authors": [
                "Haoyu Wu",
                "Diankun Wu",
                "Tianyu He",
                "Junliang Guo",
                "Yang Ye",
                "Yueqi Duan",
                "Jiang Bian"
            ],
            "affiliations": [
                "Microsoft Research",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07982.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#video",
                    "#diffusion",
                    "#3d"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "Geometry Forcing: Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Geometry Forcing Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ğ²Ğ¸Ğ´Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ: ÑƒĞ³Ğ»Ğ¾Ğ²Ğ¾Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Geometry Forcing Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ 3D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Bridging 2D Videos to 3D Understanding with Geometry Forcing",
                    "desc": "This paper addresses the limitations of video diffusion models that do not effectively capture the 3D structure of the world from 2D video data. The authors introduce a technique called Geometry Forcing, which helps these models learn geometric representations by aligning their intermediate features with those from a pretrained geometric foundation model. They propose two alignment objectives: Angular Alignment, which ensures directional consistency, and Scale Alignment, which maintains scale information. The results show that Geometry Forcing significantly enhances the visual quality and 3D consistency of generated videos compared to existing methods."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘æ¨¡å‹çš„å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºå‡ ä½•å¼ºåˆ¶ï¼ˆGeometry Forcingï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨å­¦ä¹ è¡¨ç¤ºæ—¶å¯¹ä¸‰ç»´å‡ ä½•ç»“æ„çš„æ•æ‰èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…ä½¿ç”¨åŸå§‹è§†é¢‘æ•°æ®è®­ç»ƒçš„æ¨¡å‹å¾€å¾€æ— æ³•æœ‰æ•ˆæ•æ‰åˆ°æœ‰æ„ä¹‰çš„å‡ ä½•ä¿¡æ¯ã€‚é€šè¿‡å°†æ¨¡å‹çš„ä¸­é—´è¡¨ç¤ºä¸é¢„è®­ç»ƒçš„å‡ ä½•åŸºç¡€æ¨¡å‹çš„ç‰¹å¾å¯¹é½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªäº’è¡¥çš„å¯¹é½ç›®æ ‡ï¼šè§’åº¦å¯¹é½å’Œå°ºåº¦å¯¹é½ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‡ ä½•å¼ºåˆ¶æ–¹æ³•åœ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†è§†è§‰è´¨é‡å’Œä¸‰ç»´ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07136",
            "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+\n  FPS",
            "url": "https://huggingface.co/papers/2507.07136",
            "abstract": "LangSplatV2 enhances 3D text querying speed and accuracy by replacing the heavyweight decoder with a sparse coefficient field and efficient CUDA optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images, providing a 42 times speedup and a 47 times boost over LangSplat respectively, along with improved query accuracy. LangSplat employs Gaussian Splatting to embed 2D CLIP language features into 3D, significantly enhancing speed and learning a precise 3D language field with SAM semantics. Such advancements in 3D language fields are crucial for applications that require language interaction within complex scenes. However, LangSplat does not yet achieve real-time inference performance (8.2 FPS), even with advanced A100 GPUs, severely limiting its broader application. In this paper, we first conduct a detailed time analysis of LangSplat, identifying the heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2 assumes that each Gaussian acts as a sparse code within a global dictionary, leading to the learning of a 3D sparse coefficient field that entirely eliminates the need for a heavyweight decoder. By leveraging this sparsity, we further propose an efficient sparse coefficient splatting method with CUDA optimization, rendering high-dimensional feature maps at high quality while incurring only the time cost of splatting an ultra-low-dimensional feature. Our experimental results demonstrate that LangSplatV2 not only achieves better or competitive query accuracy but is also significantly faster. Codes and demos are available at our project page: https://langsplat-v2.github.io.",
            "score": 18,
            "issue_id": 4763,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "f35bfc7d12aabb90",
            "authors": [
                "Wanhua Li",
                "Yujie Zhao",
                "Minghan Qin",
                "Yang Liu",
                "Yuanhao Cai",
                "Chuang Gan",
                "Hanspeter Pfister"
            ],
            "affiliations": [
                "Harvard University",
                "Johns Hopkins University",
                "MIT-IBM Watson AI Lab",
                "Tsinghua University",
                "UMass Amherst",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07136.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#inference",
                    "#data"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "LangSplatV2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 3D Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²",
                    "desc": "LangSplatV2 - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½Ğ° Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚ÑĞ¶ĞµĞ»Ğ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğµ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ CUDA. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ 476.2 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ´Ğ»Ñ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ 384.6 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. LangSplatV2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹."
                },
                "en": {
                    "title": "Speeding Up 3D Text Querying with LangSplatV2",
                    "desc": "LangSplatV2 is a machine learning model that improves the speed and accuracy of 3D text querying by replacing the traditional heavyweight decoder with a more efficient sparse coefficient field. It achieves impressive performance metrics, processing high-dimensional features at 476.2 frames per second (FPS) and 3D text queries at 384.6 FPS, marking a significant speedup compared to its predecessor, LangSplat. The model utilizes Gaussian Splatting to effectively integrate 2D language features into a 3D context, enhancing the precision of language interactions in complex scenes. Despite these advancements, LangSplatV2 still struggles to reach real-time inference speeds, which limits its practical applications in dynamic environments."
                },
                "zh": {
                    "title": "LangSplatV2ï¼šæå‡ 3D æ–‡æœ¬æŸ¥è¯¢é€Ÿåº¦ä¸å‡†ç¡®æ€§",
                    "desc": "LangSplatV2 æ˜¯ä¸€ç§æ–°å‹çš„ 3D æ–‡æœ¬æŸ¥è¯¢æ–¹æ³•ï¼Œé€šè¿‡ç”¨ç¨€ç–ç³»æ•°åœºæ›¿ä»£ä¼ ç»Ÿçš„é‡å‹è§£ç å™¨ï¼Œæ˜¾è‘—æé«˜äº†æŸ¥è¯¢é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸Šå®ç°äº†æ¯ç§’ 476.2 å¸§çš„é«˜ç»´ç‰¹å¾å–·æº…å’Œæ¯ç§’ 384.6 å¸§çš„ 3D å¼€æ”¾è¯æ±‡æ–‡æœ¬æŸ¥è¯¢ï¼Œé€Ÿåº¦æå‡è¾¾ 42 å€ï¼Œå‡†ç¡®æ€§ä¹Ÿæœ‰æ˜¾è‘—æé«˜ã€‚LangSplatV2 åˆ©ç”¨é«˜æ–¯å–·æº…æŠ€æœ¯å°† 2D CLIP è¯­è¨€ç‰¹å¾åµŒå…¥ 3Dï¼Œå­¦ä¹ ç²¾ç¡®çš„ 3D è¯­è¨€åœºï¼Œé€‚ç”¨äºå¤æ‚åœºæ™¯ä¸­çš„è¯­è¨€äº¤äº’åº”ç”¨ã€‚å°½ç®¡ LangSplatV2 æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œä½†åœ¨å®æ—¶æ¨ç†æ–¹é¢ä»æœ‰å¾…æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07202",
            "title": "A Survey on Long-Video Storytelling Generation: Architectures,\n  Consistency, and Cinematic Quality",
            "url": "https://huggingface.co/papers/2507.07202",
            "abstract": "Despite the significant progress that has been made in video generative models, existing state-of-the-art methods can only produce videos lasting 5-16 seconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16 seconds struggle to maintain consistent character appearances and scene layouts throughout the narrative. In particular, multi-subject long videos still fail to preserve character consistency and motion coherence. While some methods can generate videos up to 150 seconds long, they often suffer from frame redundancy and low temporal diversity. Recent work has attempted to produce long-form videos featuring multiple characters, narrative coherence, and high-fidelity detail. We comprehensively studied 32 papers on video generation to identify key architectural components and training strategies that consistently yield these qualities. We also construct a comprehensive novel taxonomy of existing methods and present comparative tables that categorize papers by their architectural designs and performance characteristics.",
            "score": 16,
            "issue_id": 4762,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "c7dc5888e8a06c13",
            "authors": [
                "Mohamed Elmoghany",
                "Ryan Rossi",
                "Seunghyun Yoon",
                "Subhojyoti Mukherjee",
                "Eslam Bakr",
                "Puneet Mathur",
                "Gang Wu",
                "Viet Dac Lai",
                "Nedim Lipka",
                "Ruiyi Zhang",
                "Varun Manjunatha",
                "Chien Nguyen",
                "Daksh Dangi",
                "Abel Salinas",
                "Mohammad Taesiri",
                "Hongjie Chen",
                "Xiaolei Huang",
                "Joe Barrow",
                "Nesreen Ahmed",
                "Hoda Eldardiry",
                "Namyong Park",
                "Yu Wang",
                "Jaemin Cho",
                "Anh Totti Nguyen",
                "Zhengzhong Tu",
                "Thien Nguyen",
                "Dinesh Manocha",
                "Mohamed Elhoseiny",
                "Franck Dernoncourt"
            ],
            "affiliations": [
                "Adobe Research",
                "Auburn University",
                "Cisco",
                "Dolby Labs",
                "Independent Researcher",
                "KAUST",
                "Meta AI",
                "Pattern Data",
                "Texas A&M University",
                "UNC Chapel Hill",
                "University of Maryland, College Park",
                "University of Memphis",
                "University of Oregon",
                "University of Southern California",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07202.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#architecture",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ 32 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¼ ÑÑĞ¶ĞµÑ‚Ğ¾Ğ¼. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 5-16 ÑĞµĞºÑƒĞ½Ğ´, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ ÑÑ†ĞµĞ½. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ¸Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unlocking the Future of Long-Form Video Generation",
                    "desc": "This paper reviews the current state of video generative models, highlighting their limitations in producing long-form videos that exceed 16 seconds. It identifies issues such as character consistency and motion coherence, particularly in videos featuring multiple subjects. The authors analyze 32 existing studies to pinpoint effective architectural components and training strategies that enhance video quality. Additionally, they propose a new taxonomy to classify these methods based on their designs and performance metrics, aiming to guide future research in this area."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç”Ÿæˆçš„è¿è´¯æ€§ä¸å¤šæ ·æ€§",
                    "desc": "å°½ç®¡è§†é¢‘ç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•åªèƒ½ç”ŸæˆæŒç»­5åˆ°16ç§’çš„è§†é¢‘ï¼Œé€šå¸¸è¢«ç§°ä¸ºâ€œé•¿è§†é¢‘â€ã€‚è¶…è¿‡16ç§’çš„è§†é¢‘åœ¨è§’è‰²å¤–è§‚å’Œåœºæ™¯å¸ƒå±€çš„ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯å¤šè§’è‰²é•¿è§†é¢‘åœ¨è§’è‰²ä¸€è‡´æ€§å’Œè¿åŠ¨è¿è´¯æ€§æ–¹é¢ä»ç„¶å­˜åœ¨é—®é¢˜ã€‚è™½ç„¶ä¸€äº›æ–¹æ³•å¯ä»¥ç”Ÿæˆé•¿è¾¾150ç§’çš„è§†é¢‘ï¼Œä½†å®ƒä»¬å¾€å¾€é¢ä¸´å¸§å†—ä½™å’Œæ—¶é—´å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚æˆ‘ä»¬å¯¹32ç¯‡è§†é¢‘ç”Ÿæˆè®ºæ–‡è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œè¯†åˆ«å‡ºå…³é”®çš„æ¶æ„ç»„ä»¶å’Œè®­ç»ƒç­–ç•¥ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ–°çš„ç°æœ‰æ–¹æ³•åˆ†ç±»æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07996",
            "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs",
            "url": "https://huggingface.co/papers/2507.07996",
            "abstract": "A method using chain-of-layers (CoLa) derived from a pretrained large language model allows for dynamic architecture adaptation, improving efficiency and accuracy across diverse tasks through selective layer manipulation and Monte Carlo Tree Search optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Can a pretrained neural network adapt its architecture to different inputs without any finetuning? Do we need all layers for simple tasks, and are they adequate for challenging tasks? We found that the layers of a pretrained large language model (LLM) can be manipulated as separate modules to build a better and even shallower model customized for each test sample. In particular, each layer from the pretrained model can be skipped/pruned or repeated multiple times as recurrent neural networks (RNN), and stacked with others in arbitrary orders, yielding a chain-of-layers (CoLa) per sample. This compositional space greatly expands the scope of existing works on looped/recurrent pretrained modules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree Search (MCTS) protocol to explore and identify the optimal CoLa for each sample from math and commonsense reasoning benchmarks. Compared to a static model of a fixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same layer(s) (slow thinking), and combining both, offering more flexible, dynamic architectures for different inputs. We conduct an extensive analysis of the MCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples with correct predictions by the original LLM, we can find shorter CoLa, suggesting a large space for improving inference efficiency; (2) For >60% of samples with originally incorrect predictions, we can identify CoLa achieving correct predictions, suggesting a large space of performance enhancement. Our results highlight the shortcomings of using a fixed architecture of pre-trained LLMs for inference on different samples and pave the way to unlock the generalization power of test-time depth adaptation.",
            "score": 13,
            "issue_id": 4765,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ»Ñ",
                "en": "July 10",
                "zh": "7æœˆ10æ—¥"
            },
            "hash": "c861c1e4c9288d18",
            "authors": [
                "Ziyue Li",
                "Yang Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Department of Computer Science, University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07996.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ ÑĞ»Ğ¾ĞµĞ² (CoLa), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ. CoLa Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ, Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ."
                },
                "en": {
                    "title": "Dynamic Layer Adaptation for Enhanced Model Efficiency",
                    "desc": "This paper introduces a novel method called chain-of-layers (CoLa) that utilizes a pretrained large language model (LLM) to dynamically adapt its architecture for various tasks. By manipulating individual layers, the model can skip, repeat, or rearrange them, allowing for a more efficient and tailored approach to processing different inputs. The authors employ Monte Carlo Tree Search (MCTS) to optimize the selection of layers for each sample, enhancing both accuracy and efficiency. The findings demonstrate that CoLa can significantly improve performance and reduce inference time compared to traditional static models, highlighting the potential for adaptive architectures in machine learning."
                },
                "zh": {
                    "title": "åŠ¨æ€æ¶æ„é€‚åº”ï¼Œæå‡æ¨¡å‹æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„å±‚é“¾ï¼ˆCoLaï¼‰æ–¹æ³•ï¼Œèƒ½å¤ŸåŠ¨æ€è°ƒæ•´æ¨¡å‹æ¶æ„ä»¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡é€‰æ‹©æ€§åœ°æ“ä½œå±‚å’Œä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ä¼˜åŒ–ï¼ŒCoLaå¯ä»¥ä¸ºæ¯ä¸ªæµ‹è¯•æ ·æœ¬æ„å»ºæ›´ä¼˜çš„æ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„å±‚å¯ä»¥ä½œä¸ºç‹¬ç«‹æ¨¡å—è¿›è¡Œæ“ä½œï¼Œä»è€Œå®ç°æ›´çµæ´»çš„æ¶æ„é€‚åº”ä¸åŒè¾“å…¥ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoLaåœ¨æé«˜æ¨ç†æ•ˆç‡å’Œæ€§èƒ½æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ä¸åŒæ ·æœ¬æ—¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.06543",
            "title": "Token Bottleneck: One Token to Remember Dynamics",
            "url": "https://huggingface.co/papers/2507.06543",
            "abstract": "ToBo is a self-supervised learning method that creates compact, temporally aware visual representations for sequential scene understanding tasks, outperforming baselines in both simulated and real-world environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Deriving compact and temporally aware visual representations from dynamic scenes is essential for successful execution of sequential scene understanding tasks such as visual tracking and robotic manipulation. In this paper, we introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised learning pipeline that squeezes a scene into a bottleneck token and predicts the subsequent scene using minimal patches as hints. The ToBo pipeline facilitates the learning of sequential scene representations by conservatively encoding the reference scene into a compact bottleneck token during the squeeze step. In the expansion step, we guide the model to capture temporal dynamics by predicting the target scene using the bottleneck token along with few target patches as hints. This design encourages the vision backbone to embed temporal dependencies, thereby enabling understanding of dynamic transitions across scenes. Extensive experiments in diverse sequential tasks, including video label propagation and robot manipulation in simulated environments demonstrate the superiority of ToBo over baselines. Moreover, deploying our pre-trained model on physical robots confirms its robustness and effectiveness in real-world environments. We further validate the scalability of ToBo across different model scales.",
            "score": 11,
            "issue_id": 4765,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "5dcb21845afc4bb6",
            "authors": [
                "Taekyung Kim",
                "Dongyoon Han",
                "Byeongho Heo",
                "Jeongeun Park",
                "Sangdoo Yun"
            ],
            "affiliations": [
                "Korea University",
                "NAVER AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.06543.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#transfer_learning",
                    "#cv",
                    "#robotics",
                    "#video"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ToBo: ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "ToBo - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ToBo ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ ÑÑ†ĞµĞ½Ñƒ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ÑƒÑ ÑÑ†ĞµĞ½Ñƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Compact and Temporal: Revolutionizing Scene Understanding with ToBo",
                    "desc": "ToBo is a self-supervised learning method that focuses on creating compact visual representations for understanding dynamic scenes. It works by encoding a scene into a small bottleneck token and then predicting the next scene using minimal visual hints. This approach helps the model learn temporal relationships between scenes, which is crucial for tasks like visual tracking and robotic manipulation. The effectiveness of ToBo is demonstrated through experiments in both simulated and real-world environments, showing its ability to outperform existing methods."
                },
                "zh": {
                    "title": "ToBoï¼šç´§å‡‘çš„æ—¶é—´æ„ŸçŸ¥è§†è§‰è¡¨ç¤º",
                    "desc": "ToBoæ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä¸ºé¡ºåºåœºæ™¯ç†è§£ä»»åŠ¡åˆ›å»ºç´§å‡‘ä¸”å…·æœ‰æ—¶é—´æ„ŸçŸ¥çš„è§†è§‰è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡å°†åœºæ™¯å‹ç¼©ä¸ºç“¶é¢ˆæ ‡è®°ï¼Œå¹¶åˆ©ç”¨æœ€å°çš„è¡¥ä¸ä½œä¸ºæç¤ºæ¥é¢„æµ‹åç»­åœºæ™¯ï¼Œä»è€Œæé«˜äº†è§†è§‰è·Ÿè¸ªå’Œæœºå™¨äººæ“ä½œç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚ToBoçš„è®¾è®¡é¼“åŠ±æ¨¡å‹æ•æ‰æ—¶é—´åŠ¨æ€ï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£åœºæ™¯ä¹‹é—´çš„åŠ¨æ€è¿‡æ¸¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒToBoåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07484",
            "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in\n  Large Language Models",
            "url": "https://huggingface.co/papers/2507.07484",
            "abstract": "Machine bullshit, characterized by LLMs' indifference to truth, is quantified and analyzed through a new framework, revealing that RLHF and CoT prompting exacerbate certain bullshit forms.  \t\t\t\t\tAI-generated summary \t\t\t\t Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to statements made without regard to their truth value. While previous work has explored large language model (LLM) hallucination and sycophancy, we propose machine bullshit as an overarching conceptual framework that can allow researchers to characterize the broader phenomenon of emergent loss of truthfulness in LLMs and shed light on its underlying mechanisms. We introduce the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and propose a complementary taxonomy analyzing four qualitative forms of bullshit: empty rhetoric, paltering, weasel words, and unverified claims. We conduct empirical evaluations on the Marketplace dataset, the Political Neutrality dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI assistants) explicitly designed to evaluate machine bullshit. Our results demonstrate that model fine-tuning with reinforcement learning from human feedback (RLHF) significantly exacerbates bullshit and inference-time chain-of-thought (CoT) prompting notably amplify specific bullshit forms, particularly empty rhetoric and paltering. We also observe prevalent machine bullshit in political contexts, with weasel words as the dominant strategy. Our findings highlight systematic challenges in AI alignment and provide new insights toward more truthful LLM behavior.",
            "score": 4,
            "issue_id": 4763,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ»Ñ",
                "en": "July 10",
                "zh": "7æœˆ10æ—¥"
            },
            "hash": "c35c3791aec951d3",
            "authors": [
                "Kaiqu Liang",
                "Haimin Hu",
                "Xuandong Zhao",
                "Dawn Song",
                "Thomas L. Griffiths",
                "Jaime FernÃ¡ndez Fisac"
            ],
            "affiliations": [
                "Princeton University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07484.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#ethics",
                    "#benchmark",
                    "#hallucinations",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ² LLM: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ÑƒĞ»Ğ»ÑˆĞ¸Ñ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ \"Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ÑƒĞ»Ğ»ÑˆĞ¸Ñ‚Ğ°\", ĞºĞ¾Ğ³Ğ´Ğ° LLMs Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡ĞµÑ‚Ğ° Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ±ÑƒĞ»Ğ»ÑˆĞ¸Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ±ĞµĞ·Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğµ LLMs Ğº Ğ¿Ñ€Ğ°Ğ²Ğ´Ğµ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ±ÑƒĞ»Ğ»ÑˆĞ¸Ñ‚Ğ°: Ğ¿ÑƒÑÑ‚Ğ°Ñ Ñ€Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞºĞ°, ÑƒĞ²Ğ¸Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑĞ»Ğ¾Ğ²Ğ°-Ğ»Ğ°Ğ·ĞµĞ¹ĞºĞ¸ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ±ÑƒĞ»Ğ»ÑˆĞ¸Ñ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ AI Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ LLM."
                },
                "en": {
                    "title": "Quantifying Machine Bullshit: A New Framework for LLM Truthfulness",
                    "desc": "This paper introduces the concept of 'machine bullshit' to describe how large language models (LLMs) can generate statements without regard for their truthfulness. It presents a new framework that includes the Bullshit Index, a metric designed to quantify this indifference to truth. The authors analyze four types of machine bullshit: empty rhetoric, paltering, weasel words, and unverified claims, and evaluate these through various datasets. The findings indicate that techniques like reinforcement learning from human feedback (RLHF) and chain-of-thought prompting can worsen the generation of certain types of bullshit, particularly in political contexts."
                },
                "zh": {
                    "title": "æ­ç¤ºæœºå™¨èƒ¡è¯´çš„çœŸç›¸",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶æ¥é‡åŒ–å’Œåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå†…å®¹æ—¶å¯¹çœŸç›¸çš„æ¼ è§†ï¼Œç§°ä¹‹ä¸ºâ€œæœºå™¨èƒ¡è¯´â€ã€‚æˆ‘ä»¬å¼•å…¥äº†â€œèƒ¡è¯´æŒ‡æ•°â€ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–LLMå¯¹çœŸç›¸çš„æ— åŠ¨äºè¡·ï¼Œå¹¶åˆ†æäº†å››ç§èƒ¡è¯´çš„å®šæ€§å½¢å¼ï¼šç©ºæ´ä¿®è¾ã€æ¨¡æ£±ä¸¤å¯ã€ç‹¡çŒ¾ç”¨è¯å’Œæœªç»éªŒè¯çš„å£°æ˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¼šæ˜¾è‘—åŠ å‰§èƒ¡è¯´ç°è±¡ï¼Œè€Œæ¨ç†æ—¶çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºåˆ™ç‰¹åˆ«æ”¾å¤§äº†ç©ºæ´ä¿®è¾å’Œæ¨¡æ£±ä¸¤å¯çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„å‘ç°æ­ç¤ºäº†AIå¯¹é½ä¸­çš„ç³»ç»Ÿæ€§æŒ‘æˆ˜ï¼Œå¹¶ä¸ºå®ç°æ›´çœŸå®çš„LLMè¡Œä¸ºæä¾›äº†æ–°çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07955",
            "title": "Dynamic Chunking for End-to-End Hierarchical Sequence Modeling",
            "url": "https://huggingface.co/papers/2507.07955",
            "abstract": "Hierarchical networks replace traditional tokenization pipelines by dynamically learning segmentation strategies, achieving better performance and scalability across various languages and modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite incredible progress in language models (LMs) in recent years, largely resulting from moving away from specialized models designed for specific tasks to general models based on powerful architectures (e.g. the Transformer) that learn everything from raw data, pre-processing steps such as tokenization remain a barrier to true end-to-end foundation models. We introduce a collection of new techniques that enable a dynamic chunking mechanism which automatically learns content -- and context -- dependent segmentation strategies learned jointly with the rest of the model. Incorporating this into an explicit hierarchical network (H-Net) allows replacing the (implicitly hierarchical) tokenization-LM-detokenization pipeline with a single model learned fully end-to-end. When compute- and data- matched, an H-Net with one stage of hierarchy operating at the byte level outperforms a strong Transformer language model operating over BPE tokens. Iterating the hierarchy to multiple stages further increases its performance by modeling multiple levels of abstraction, demonstrating significantly better scaling with data and matching a token-based Transformer of twice its size. H-Nets pretrained on English show significantly increased character-level robustness, and qualitatively learn meaningful data-dependent chunking strategies without any heuristics or explicit supervision. Finally, the H-Net's improvement over tokenized pipelines is further increased in languages and modalities with weaker tokenization heuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement in data efficiency over baselines), showing the potential of true end-to-end models that learn and scale better from unprocessed data.",
            "score": 3,
            "issue_id": 4779,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ»Ñ",
                "en": "July 10",
                "zh": "7æœˆ10æ—¥"
            },
            "hash": "e7521540f9f39040",
            "authors": [
                "Sukjun Hwang",
                "Brandon Wang",
                "Albert Gu"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Cartesia AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07955.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#transfer_learning",
                    "#long_context",
                    "#multilingual",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸: ĞºĞ¾Ğ½ĞµÑ† ÑÑ€Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµÑ‚ÑĞ¼Ğ¸ (H-Net). Ğ­Ñ‚Ğ¸ ÑĞµÑ‚Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. H-Net Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ BPE, Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ… Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¾ ÑĞ»Ğ°Ğ±Ñ‹Ğ¼Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ”ĞĞš."
                },
                "en": {
                    "title": "Dynamic Segmentation for Enhanced Language Modeling",
                    "desc": "This paper presents Hierarchical Networks (H-Nets) that improve upon traditional tokenization methods by dynamically learning how to segment data based on its content and context. By integrating this dynamic chunking mechanism into a hierarchical structure, H-Nets can operate as a single end-to-end model, eliminating the need for separate tokenization and detokenization processes. The results show that H-Nets outperform conventional Transformer models, especially in languages and modalities where tokenization is less effective, such as Chinese and DNA sequences. This approach not only enhances performance but also increases data efficiency, demonstrating the advantages of learning directly from raw data without relying on predefined tokenization strategies."
                },
                "zh": {
                    "title": "å±‚æ¬¡ç½‘ç»œï¼šæ‰“ç ´æ ‡è®°åŒ–çš„ç•Œé™",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å±‚æ¬¡ç½‘ç»œï¼ˆH-Netï¼‰ï¼Œå®ƒé€šè¿‡åŠ¨æ€å­¦ä¹ åˆ†å‰²ç­–ç•¥æ¥æ›¿ä»£ä¼ ç»Ÿçš„æ ‡è®°åŒ–æµç¨‹ï¼Œä»è€Œåœ¨å¤šç§è¯­è¨€å’Œæ¨¡æ€ä¸­å®ç°æ›´å¥½çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚H-Netèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜¾å¼æ ‡è®°åŒ–çš„æƒ…å†µä¸‹ï¼Œè‡ªåŠ¨å­¦ä¹ å†…å®¹å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„åˆ†å‰²ç­–ç•¥ï¼Œå¹¶ä¸æ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†å…±åŒè®­ç»ƒã€‚ç ”ç©¶è¡¨æ˜ï¼ŒH-Netåœ¨å­—èŠ‚çº§åˆ«çš„å•å±‚æ¬¡ç»“æ„ä¸Šè¶…è¶Šäº†å¼ºå¤§çš„Transformerè¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”é€šè¿‡å¤šå±‚æ¬¡çš„è¿­ä»£è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨æ ‡è®°åŒ–å¯å‘å¼è¾ƒå¼±çš„è¯­è¨€ï¼ˆå¦‚ä¸­æ–‡ï¼‰ä¸­ï¼ŒH-Netæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼Œå±•ç¤ºäº†ä»æœªå¤„ç†æ•°æ®ä¸­å­¦ä¹ å’Œæ‰©å±•çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07574",
            "title": "Beyond the Linear Separability Ceiling",
            "url": "https://huggingface.co/papers/2507.07574",
            "abstract": "The study identifies a linear reasoning bottleneck in Visual-Language Models and proposes the Linear Separability Ceiling as a metric to evaluate it, suggesting targeted alignment rather than improved representation learning as a solution.  \t\t\t\t\tAI-generated summary \t\t\t\t Most state-of-the-art Visual-Language Models (VLMs) are seemingly limited by the linear separabilty of their visual embeddings on abstract reasoning tasks. This work investigates this \"linear reasoning bottleneck\" by introducing the Linear Separability Ceiling (LSC), the performance of a simple linear classifier on a VLM's visual embeddings. We find this bottleneck is widespread and stems not from poor perception, but from failures in the language model's reasoning pathways. We demonstrate this is a solvable alignment issue. The required intervention, however, is task-dependent: activating existing pathways suffices for semantic concepts, while complex relational reasoning requires adapting core model weights. Using postfix tuning as a methodological control, we find strong evidence for powerful, dormant reasoning pathways within VLMs. However, for complex relational tasks requiring deeper adaptation, explicitly improving representation quality causes the model to fail on new prompt formats despite its embeddings remaining well separated. Ultimately, this work provides a new lens for VLM analysis, showing that robust reasoning is a matter of targeted alignment, not simply improved representation learning.",
            "score": 3,
            "issue_id": 4767,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ»Ñ",
                "en": "July 10",
                "zh": "7æœˆ10æ—¥"
            },
            "hash": "facb529d60e86e84",
            "authors": [
                "Enrico Vompa",
                "Tanel Tammet",
                "Mohit Vaishnav"
            ],
            "affiliations": [
                "Applied Artificial Intelligence Group, Tallinn University of Technology, Estonia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07574.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#alignment",
                    "#training",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ 'ĞŸĞ¾Ñ‚Ğ¾Ğ»Ğ¾Ğº Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸' (LSC) Ğ´Ğ»Ñ ĞµĞ³Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ Ğ½Ğµ Ñ Ğ¿Ğ»Ğ¾Ñ…Ğ¸Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼, Ğ° Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ°Ğ¼Ğ¸ Ğ² Ğ¿ÑƒÑ‚ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ VLM ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ½ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unlocking Reasoning in Visual-Language Models",
                    "desc": "This paper explores a limitation in Visual-Language Models (VLMs) known as the linear reasoning bottleneck, which affects their performance on abstract reasoning tasks. The authors introduce the Linear Separability Ceiling (LSC) as a metric to evaluate how well a simple linear classifier can perform on the visual embeddings produced by VLMs. They find that the bottleneck arises not from the models' perception abilities but from issues in the reasoning pathways of the language model. The study suggests that improving alignment, rather than just enhancing representation learning, is key to overcoming this bottleneck, with different strategies needed for various types of reasoning tasks."
                },
                "zh": {
                    "title": "è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹çš„çº¿æ€§æ¨ç†ç“¶é¢ˆ",
                    "desc": "æœ¬ç ”ç©¶å‘ç°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æŠ½è±¡æ¨ç†ä»»åŠ¡ä¸­å­˜åœ¨çº¿æ€§æ¨ç†ç“¶é¢ˆï¼Œå¹¶æå‡ºäº†çº¿æ€§å¯åˆ†æ€§ä¸Šé™ï¼ˆLinear Separability Ceilingï¼ŒLSCï¼‰ä½œä¸ºè¯„ä¼°è¯¥ç“¶é¢ˆçš„æŒ‡æ ‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ä¸€ç“¶é¢ˆæ™®éå­˜åœ¨ï¼Œä¸»è¦æºäºè¯­è¨€æ¨¡å‹æ¨ç†è·¯å¾„çš„å¤±è´¥ï¼Œè€Œéæ„ŸçŸ¥èƒ½åŠ›çš„ä¸è¶³ã€‚è§£å†³è¿™ä¸€é—®é¢˜éœ€è¦é’ˆå¯¹æ€§åœ°è¿›è¡Œå¯¹é½ï¼Œè€Œä¸æ˜¯å•çº¯æé«˜è¡¨ç¤ºå­¦ä¹ çš„è´¨é‡ã€‚å¯¹äºè¯­ä¹‰æ¦‚å¿µï¼Œæ¿€æ´»ç°æœ‰è·¯å¾„å³å¯ï¼Œè€Œå¤æ‚çš„å…³ç³»æ¨ç†åˆ™éœ€è¦è°ƒæ•´æ ¸å¿ƒæ¨¡å‹æƒé‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07867",
            "title": "Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders",
            "url": "https://huggingface.co/papers/2507.07867",
            "abstract": "A Re-Bottleneck framework modifies pre-trained autoencoders to enforce specific latent structures, improving performance in diverse downstream applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural audio codecs and autoencoders have emerged as versatile models for audio compression, transmission, feature-extraction, and latent-space generation. However, a key limitation is that most are trained to maximize reconstruction fidelity, often neglecting the specific latent structure necessary for optimal performance in diverse downstream applications. We propose a simple, post-hoc framework to address this by modifying the bottleneck of a pre-trained autoencoder. Our method introduces a \"Re-Bottleneck\", an inner bottleneck trained exclusively through latent space losses to instill user-defined structure. We demonstrate the framework's effectiveness in three experiments. First, we enforce an ordering on latent channels without sacrificing reconstruction quality. Second, we align latents with semantic embeddings, analyzing the impact on downstream diffusion modeling. Third, we introduce equivariance, ensuring that a filtering operation on the input waveform directly corresponds to a specific transformation in the latent space. Ultimately, our Re-Bottleneck framework offers a flexible and efficient way to tailor representations of neural audio models, enabling them to seamlessly meet the varied demands of different applications with minimal additional training.",
            "score": 2,
            "issue_id": 4774,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ»Ñ",
                "en": "July 10",
                "zh": "7æœˆ10æ—¥"
            },
            "hash": "72ee0206287ff7a0",
            "authors": [
                "Dimitrios Bralios",
                "Jonah Casebeer",
                "Paris Smaragdis"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07867.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'Re-Bottleneck' Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ñ Ñ†ĞµĞ»ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ, Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğµ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ², Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing Autoencoders with Re-Bottleneck for Tailored Latent Structures",
                    "desc": "This paper presents a new framework called 'Re-Bottleneck' that modifies pre-trained autoencoders to enhance their latent structures for better performance in various applications. Traditional autoencoders focus on reconstructing input data accurately but often overlook the specific latent structures needed for different tasks. The Re-Bottleneck approach introduces an additional bottleneck that is trained to optimize latent space representations based on user-defined criteria. Through experiments, the authors show that this method can enforce order in latent channels, align them with semantic meanings, and ensure that changes in input lead to predictable transformations in the latent space."
                },
                "zh": {
                    "title": "Re-Bottleneckæ¡†æ¶ï¼šä¼˜åŒ–æ½œåœ¨ç»“æ„ä»¥æå‡æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œRe-Bottleneckâ€çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¿®æ”¹é¢„è®­ç»ƒè‡ªç¼–ç å™¨çš„ç“¶é¢ˆéƒ¨åˆ†ï¼Œæ¥å¢å¼ºæ½œåœ¨ç»“æ„çš„ç‰¹å®šæ€§ã€‚è¿™ç§æ–¹æ³•é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´æŸå¤±ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¥å¼•å…¥ç”¨æˆ·å®šä¹‰çš„ç»“æ„ï¼Œä»è€Œæé«˜åœ¨ä¸åŒä¸‹æ¸¸åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå®éªŒä¸­éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬å¯¹æ½œåœ¨é€šé“çš„æ’åºã€ä¸è¯­ä¹‰åµŒå…¥çš„å¯¹é½ä»¥åŠå¼•å…¥ç­‰å˜æ€§ã€‚æœ€ç»ˆï¼ŒRe-Bottleneckæ¡†æ¶ä¸ºç¥ç»éŸ³é¢‘æ¨¡å‹çš„è¡¨ç¤ºæä¾›äº†ä¸€ç§çµæ´»é«˜æ•ˆçš„å®šåˆ¶æ–¹å¼ï¼Œèƒ½å¤Ÿæ»¡è¶³ä¸åŒåº”ç”¨çš„å¤šæ ·åŒ–éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07129",
            "title": "Growing Transformers: Modular Composition and Layer-wise Expansion on a\n  Frozen Substrate",
            "url": "https://huggingface.co/papers/2507.07129",
            "abstract": "A novel approach to scaling large language models through modular composition and layer-wise growth using fixed embeddings enhances performance and flexibility.  \t\t\t\t\tAI-generated summary \t\t\t\t The prevailing paradigm for scaling large language models (LLMs) involves monolithic, end-to-end training, a resource-intensive process that lacks flexibility. This paper explores an alternative, constructive approach to model development, built upon the foundation of non-trainable, deterministic input embeddings. In prior [1], we established that high-level semantic reasoning can emerge in Transformers using frozen embeddings derived from the visual structure of Unicode glyphs. Here, we demonstrate that this fixed representational substrate acts as a universal \"docking port,\" enabling two powerful and efficient scaling paradigms: seamless modular composition and progressive layer-wise growth.   First, we show that specialist models trained on disparate datasets (e.g., Russian and Chinese text) can be merged into a single, more capable Mixture-of-Experts (MoE) model, post-training, with zero architectural modification. This is achieved by simply averaging their output logits. The resulting MoE model exhibits immediate performance improvements on reasoning benchmarks like MMLU, surpassing its constituent experts without catastrophic forgetting. Second, we introduce a layer-wise constructive training methodology, where a deep Transformer is \"grown\" by progressively stacking and training one layer at a time. This method demonstrates stable convergence and a clear correlation between model depth and the emergence of complex reasoning abilities, such as those required for SQuAD.   Our findings suggest a paradigm shift from monolithic optimization towards a more biological or constructive model of AI development, where complexity is built incrementally and modules can be composed freely. This opens new avenues for resource-efficient scaling, continual learning, and a more democratized ecosystem for building powerful AI systems. We release all code and models to facilitate further research.",
            "score": 2,
            "issue_id": 4773,
            "pub_date": "2025-07-08",
            "pub_date_card": {
                "ru": "8 Ğ¸ÑĞ»Ñ",
                "en": "July 8",
                "zh": "7æœˆ8æ—¥"
            },
            "hash": "a5414a289d5b5913",
            "authors": [
                "A. Bochkov"
            ],
            "affiliations": [
                "Moscow Institute of Physics and Technology (MIPT), Moscow, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07129.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞšĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¾Ñ€ Ğ˜Ğ˜: ÑĞ±Ğ¾Ñ€ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ ĞºĞ¸Ñ€Ğ¿Ğ¸Ñ‡Ğ¸ĞºĞ°Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ñ‹ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Mixture-of-Experts Ğ±ĞµĞ· Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ 'Ğ²Ñ‹Ñ€Ğ°Ñ‰Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ' Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğ¾Ñ‚ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Modular Growth: A New Era for Language Models",
                    "desc": "This paper presents a new method for scaling large language models (LLMs) that focuses on modular composition and layer-wise growth instead of traditional end-to-end training. By using fixed, non-trainable embeddings, the authors show that different specialized models can be combined into a more powerful Mixture-of-Experts (MoE) model without changing their architecture. Additionally, they introduce a technique for gradually adding layers to a Transformer model, which leads to better performance and stability during training. This approach promotes a more flexible and efficient way to develop AI systems, allowing for continual learning and easier integration of new capabilities."
                },
                "zh": {
                    "title": "æ¨¡å—åŒ–ç»„åˆä¸é€å±‚å¢é•¿ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡æ¨¡å—åŒ–ç»„åˆå’Œé€å±‚å¢é•¿æ¥æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä½¿ç”¨å›ºå®šçš„åµŒå…¥å¢å¼ºäº†æ¨¡å‹çš„æ€§èƒ½å’Œçµæ´»æ€§ã€‚ä¸ä¼ ç»Ÿçš„æ•´ä½“è®­ç»ƒæ–¹æ³•ä¸åŒï¼Œè¿™ç§æ–¹æ³•åˆ©ç”¨ä¸å¯è®­ç»ƒçš„ç¡®å®šæ€§è¾“å…¥åµŒå…¥ï¼Œå…è®¸åœ¨ä¸ä¿®æ”¹æ¶æ„çš„æƒ…å†µä¸‹å°†ä¸åŒæ•°æ®é›†è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ªæ›´å¼ºå¤§çš„æ··åˆä¸“å®¶æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§é€å±‚æ„å»ºçš„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡é€æ­¥å †å å’Œè®­ç»ƒæ¯ä¸€å±‚ï¼Œå±•ç¤ºäº†æ¨¡å‹æ·±åº¦ä¸å¤æ‚æ¨ç†èƒ½åŠ›ä¹‹é—´çš„æ˜ç¡®å…³è”ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒAIå¼€å‘å¯ä»¥ä»å•ä¸€ä¼˜åŒ–è½¬å‘æ›´å…·ç”Ÿç‰©å­¦ç‰¹å¾çš„æ„å»ºæ¨¡å‹ï¼Œä¿ƒè¿›èµ„æºé«˜æ•ˆæ‰©å±•å’ŒæŒç»­å­¦ä¹ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05241",
            "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I.\n  X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
            "url": "https://huggingface.co/papers/2507.05241",
            "abstract": "The rapid advancements of AI agents have ignited the long-held ambition of leveraging them to accelerate scientific discovery. Achieving this goal requires a deep understanding of the frontiers of human knowledge. As such, Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for evaluating scientific AI agents. In this work, we aim to construct the foundational architecture for general-purpose agents and validate the capabilities through leading performance on HLE. To achieve this, we introduce X-Master, a tool-augmented reasoning agent designed to emulate human researchers by interacting flexibly with external tools during its reasoning process. This agent, guided by the conceptualization of code as an interaction language, can flexibly leverage built-in Python libraries and our customized tools to augment the reasoning. We further scale its capabilities through X-Masters, a scattered-and-stacked agentic workflow that systematically enhances breadth and depth of reasoning. Our open-source solution, X-Masters, sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to exceed the 30% threshold. This work allows us to gain a deeper understanding of complex task-solving and accumulates valuable experience that can inform future advancements, guiding subsequent model training.",
            "score": 2,
            "issue_id": 4765,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 Ğ¸ÑĞ»Ñ",
                "en": "July 7",
                "zh": "7æœˆ7æ—¥"
            },
            "hash": "6f1b7fc6f47b4db5",
            "authors": [
                "Jingyi Chai",
                "Shuo Tang",
                "Rui Ye",
                "Yuwen Du",
                "Xinyu Zhu",
                "Mengcheng Zhou",
                "Yanfeng Wang",
                "Weinan E",
                "Yuzhi Zhang",
                "Linfeng Zhang",
                "Siheng Chen"
            ],
            "affiliations": [
                "DP Technology",
                "School of Artificial Intelligence, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05241.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#agi",
                    "#training",
                    "#science",
                    "#reasoning",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "X-Master: Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ X-Master - Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹-Ğ»ÑĞ´ĞµĞ¹. X-Master Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ Python Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ X-Masters - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¾Ñ‚Ñ‹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. X-Masters Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ² 32.1% Ğ² Ñ‚ĞµÑÑ‚Ğµ Humanity's Last Exam, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ OpenAI Ğ¸ Google."
                },
                "en": {
                    "title": "Empowering AI for Scientific Breakthroughs with X-Master",
                    "desc": "This paper discusses the development of a new AI agent called X-Master, which is designed to enhance scientific discovery by mimicking human researchers. X-Master utilizes external tools and Python libraries to improve its reasoning capabilities, allowing it to tackle complex tasks more effectively. The authors introduce a novel workflow called X-Masters, which enhances the agent's reasoning breadth and depth. Their approach has achieved a new record score of 32.1% on the Humanity's Last Exam, outperforming previous benchmarks set by other leading AI models."
                },
                "zh": {
                    "title": "åˆ©ç”¨AIåŠ é€Ÿç§‘å­¦å‘ç°çš„æ–°æ—¶ä»£",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨äººå·¥æ™ºèƒ½ä»£ç†åŠ é€Ÿç§‘å­¦å‘ç°ï¼Œæå‡ºäº†äººç±»æœ€åè€ƒè¯•ï¼ˆHLEï¼‰ä½œä¸ºè¯„ä¼°ç§‘å­¦AIä»£ç†çš„æ ‡å‡†ã€‚æˆ‘ä»¬æ„å»ºäº†é€šç”¨ä»£ç†çš„åŸºç¡€æ¶æ„ï¼Œå¹¶é€šè¿‡X-Masterå·¥å…·å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œæ¨¡æ‹Ÿäººç±»ç ”ç©¶è€…çš„çµæ´»æ€§ã€‚X-Masterèƒ½å¤Ÿä¸å¤–éƒ¨å·¥å…·äº’åŠ¨ï¼Œåˆ©ç”¨å†…ç½®çš„Pythonåº“å’Œå®šåˆ¶å·¥å…·æ¥å¢å¼ºæ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„å¼€æºè§£å†³æ–¹æ¡ˆX-Mastersåœ¨HLEä¸Šå–å¾—äº†32.1%çš„æ–°çºªå½•ï¼Œè¶…è¶Šäº†OpenAIå’Œè°·æ­Œçš„æ·±åº¦ç ”ç©¶ï¼Œé¦–æ¬¡çªç ´30%çš„é—¨æ§›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04886",
            "title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations",
            "url": "https://huggingface.co/papers/2507.04886",
            "abstract": "Transformer models equipped with fixed, visually derived embeddings outperform those with trainable embeddings on a reasoning benchmark, challenging the traditional role of embeddings in LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding the locus of semantic representation in large language models (LLMs) is crucial for interpretability and architectural innovation. The dominant paradigm posits that trainable input embeddings serve as foundational \"meaning vectors.\" This paper challenges that view. We construct Transformer models where the embedding layer is entirely frozen, with vectors derived not from data, but from the visual structure of Unicode glyphs. These non-semantic, precomputed visual embeddings are fixed throughout training. Our method is compatible with any tokenizer, including a novel Unicode-centric tokenizer we introduce to ensure universal text coverage. Despite the absence of trainable, semantically initialized embeddings, our models converge, generate coherent text, and, critically, outperform architecturally identical models with trainable embeddings on the MMLU reasoning benchmark. We attribute this to \"representational interference\" in conventional models, where the embedding layer is burdened with learning both structural and semantic features. Our results indicate that high-level semantics are not inherent to input embeddings but are an emergent property of the Transformer's compositional architecture and data scale. This reframes the role of embeddings from meaning containers to structural primitives. We release all code and models to foster further research.",
            "score": 1,
            "issue_id": 4773,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 Ğ¸ÑĞ»Ñ",
                "en": "July 7",
                "zh": "7æœˆ7æ—¥"
            },
            "hash": "fc80c81b93b3402d",
            "authors": [
                "A. Bochkov"
            ],
            "affiliations": [
                "Moscow Institute of Physics and Technology (MIPT), Moscow, Russia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04886.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#open_source",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ»Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Unicode. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ emergent-ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ğ° Ğ½Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ÑÑ Ğ² ÑĞ°Ğ¼Ğ¸Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ñ…."
                },
                "en": {
                    "title": "Rethinking Embeddings: Structure Over Semantics in Transformers",
                    "desc": "This paper investigates the role of embeddings in large language models (LLMs) by using fixed, visually derived embeddings instead of trainable ones. The authors demonstrate that their Transformer models, which utilize precomputed visual embeddings from Unicode glyphs, outperform traditional models with trainable embeddings on reasoning tasks. They argue that the conventional view of embeddings as essential meaning vectors is flawed, as high-level semantics emerge from the Transformer's architecture and the scale of data rather than from the embeddings themselves. This research suggests a shift in understanding embeddings as structural components rather than semantic containers, paving the way for new approaches in model design."
                },
                "zh": {
                    "title": "åµŒå…¥çš„è§’è‰²é‡å¡‘ï¼šä»è¯­ä¹‰å®¹å™¨åˆ°ç»“æ„åŸè¯­",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼ŒåµŒå…¥å±‚çš„ä¼ ç»Ÿè§’è‰²ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å›ºå®šçš„è§†è§‰æ´¾ç”ŸåµŒå…¥çš„Transformeræ¨¡å‹åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä½¿ç”¨å¯è®­ç»ƒåµŒå…¥çš„æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åµŒå…¥å±‚ä¿æŒä¸å˜ï¼Œä½¿ç”¨æ¥è‡ªUnicodeå­—å½¢çš„è§†è§‰ç»“æ„å‘é‡ï¼Œè€Œéæ•°æ®ç”Ÿæˆçš„å‘é‡ã€‚ç»“æœè¡¨æ˜ï¼Œé«˜çº§è¯­ä¹‰å¹¶éåµŒå…¥çš„å›ºæœ‰ç‰¹æ€§ï¼Œè€Œæ˜¯Transformerçš„ç»„åˆæ¶æ„å’Œæ•°æ®è§„æ¨¡çš„æ¶Œç°å±æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-10.html",
    "link_next": "2025-07-14.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "10.07",
        "en": "07/10",
        "zh": "7æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "14.07",
        "en": "07/14",
        "zh": "7æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 2,
        "#cv": 5,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 1,
        "#video": 5,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 6,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 1,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 3,
        "#reasoning": 9,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 7,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 3,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    }
}