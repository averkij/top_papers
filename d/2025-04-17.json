{
    "date": {
        "ru": "17 –∞–ø—Ä–µ–ª—è",
        "en": "April 17",
        "zh": "4Êúà17Êó•"
    },
    "time_utc": "2025-04-17 21:10",
    "weekday": 3,
    "issue_id": 3299,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.10514",
            "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
            "url": "https://huggingface.co/papers/2504.10514",
            "abstract": "Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.",
            "score": 29,
            "issue_id": 3281,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 –∞–ø—Ä–µ–ª—è",
                "en": "April 10",
                "zh": "4Êúà10Êó•"
            },
            "hash": "c786e69f24be2f9e",
            "authors": [
                "Yijun Liang",
                "Ming Li",
                "Chenrui Fan",
                "Ziyue Li",
                "Dang Nguyen",
                "Kwesi Cobbina",
                "Shweta Bhardwaj",
                "Jiuhai Chen",
                "Fuxiao Liu",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10514.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "üåà",
                "ru": {
                    "title": "ColorBench: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ü–≤–µ—Ç–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç ColorBench - –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å –∏ –ø–æ–Ω–∏–º–∞—Ç—å —Ü–≤–µ—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 32 –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º —Ü–≤–µ—Ç–∞, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å—é. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ö–æ—Ç—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –ª—É—á—à–µ, —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–±–æ–ª—å—à–æ–π, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ü–≤–µ—Ç–∞ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∏—Å—Ç–µ–º–∞—Ö. ColorBench –º–æ–∂–µ—Ç —Å–ª—É–∂–∏—Ç—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ü–≤–µ—Ç–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–µ–ª–æ–≤–µ–∫–∞."
                },
                "en": {
                    "title": "Enhancing AI's Color Comprehension with ColorBench",
                    "desc": "This paper presents ColorBench, a benchmark designed to evaluate how vision-language models (VLMs) understand and utilize color in visual reasoning. It assesses various aspects of color perception, reasoning, and robustness through a series of real-world scenarios. The study finds that while larger models generally perform better, the existing VLMs show limited capabilities in color understanding, indicating a gap in their training. Additionally, the research highlights that while VLMs can use color cues effectively, they can also be misled by them, emphasizing the need for improved color comprehension in AI models."
                },
                "zh": {
                    "title": "ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ¢úËâ≤ÁêÜËß£ËÉΩÂäõ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜColorBenchÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®ËØÑ‰º∞ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®È¢úËâ≤ÁêÜËß£ÊñπÈù¢ËÉΩÂäõÁöÑÂü∫ÂáÜ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°Êõ¥Â§ßÁöÑÊ®°ÂûãÂú®ColorBench‰∏äË°®Áé∞Êõ¥Â•ΩÔºå‰ΩÜËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰ΩúÁî®ÊØîËßÜËßâÁºñÁ†ÅÂô®Êõ¥‰∏∫ÈáçË¶Å„ÄÇÁé∞ÊúâÁöÑVLMsÂú®È¢úËâ≤ÁêÜËß£ÊñπÈù¢ÁöÑË°®Áé∞Â∑ÆË∑ùËæÉÂ∞èÔºåË°®ÊòéËøô‰∏ÄÈ¢ÜÂüüÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÈáçËßÜ„ÄÇÊ≠§Â§ñÔºåÂ∞ΩÁÆ°VLMsËÉΩÂ§üÂà©Áî®È¢úËâ≤Á∫øÁ¥¢Ôºå‰ΩÜÂú®Êüê‰∫õ‰ªªÂä°‰∏≠‰πüÂèØËÉΩ‰ºöÂèóÂà∞ËØØÂØº„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12285",
            "title": "BitNet b1.58 2B4T Technical Report",
            "url": "https://huggingface.co/papers/2504.12285",
            "abstract": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.",
            "score": 27,
            "issue_id": 3281,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 –∞–ø—Ä–µ–ª—è",
                "en": "April 16",
                "zh": "4Êúà16Êó•"
            },
            "hash": "cf67f70d9f122792",
            "authors": [
                "Shuming Ma",
                "Hongyu Wang",
                "Shaohan Huang",
                "Xingxing Zhang",
                "Ying Hu",
                "Ting Song",
                "Yan Xia",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12285.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#science",
                    "#architecture",
                    "#training",
                    "#inference"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: 1-–±–∏—Ç–Ω–∞—è LLM –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–ª–Ω–æ—Ç–æ—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω BitNet b1.58 2B4T - –ø–µ—Ä–≤–∞—è –æ—Ç–∫—Ä—ã—Ç–∞—è 1-–±–∏—Ç–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (LLM) —Å 2 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –∏–∑ 4 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ—Ü–µ–Ω–µ–Ω–∞ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º, –≤–∫–ª—é—á–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ. BitNet b1.58 2B4T –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤–µ–¥—É—â–∏—Ö –ø–æ–ª–Ω–æ—Ç–æ—á–Ω—ã—Ö LLM –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –Ω–æ —Å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞–º–∏ –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ —á–µ—Ä–µ–∑ Hugging Face –≤–º–µ—Å—Ç–µ —Å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è GPU –∏ CPU."
                },
                "en": {
                    "title": "Efficient Language Understanding with BitNet: The 1-Bit Revolution",
                    "desc": "BitNet b1.58 2B4T is a groundbreaking 1-bit Large Language Model (LLM) with 2 billion parameters, making it the first of its kind to be open-source. It has been trained on an extensive dataset of 4 trillion tokens and evaluated on various benchmarks, showcasing its capabilities in language understanding, mathematical reasoning, coding, and conversation. Remarkably, BitNet achieves performance comparable to other leading full-precision LLMs while being more efficient in terms of memory usage, energy consumption, and decoding speed. The model's weights and inference implementations are made available on Hugging Face, promoting further research and practical applications."
                },
                "zh": {
                    "title": "ÂºÄÊ∫êÈ´òÊïàÁöÑ1‰ΩçÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜBitNet b1.58 2B4TÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÂºÄÊ∫êÁöÑ„ÄÅÂéüÁîüÁöÑ1‰ΩçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÂèÇÊï∞ËßÑÊ®°ËææÂà∞20‰∫ø„ÄÇËØ•Ê®°ÂûãÂú®4‰∏á‰∫ø‰∏™Ê†áËÆ∞ÁöÑËØ≠ÊñôÂ∫ì‰∏äËøõË°åËÆ≠ÁªÉÔºåÂπ∂Âú®ËØ≠Ë®ÄÁêÜËß£„ÄÅÊï∞Â≠¶Êé®ÁêÜ„ÄÅÁºñÁ®ãËÉΩÂäõÂíåÂØπËØùËÉΩÂäõÁ≠âÂü∫ÂáÜÊµãËØï‰∏≠ËøõË°å‰∫Ü‰∏•Ê†ºËØÑ‰º∞„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåBitNet b1.58 2B4TÂú®ÊÄßËÉΩ‰∏ä‰∏éÂêåÁ±ªËßÑÊ®°ÁöÑÈ¢ÜÂÖàÂºÄÊ∫êÂÖ®Á≤æÂ∫¶Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁõ∏ÂΩìÔºåÂêåÊó∂Âú®ËÆ°ÁÆóÊïàÁéá‰∏äÂÖ∑ÊúâÊòæËëó‰ºòÂäøÔºåÂåÖÊã¨ÊòæËëóÂáèÂ∞ëÁöÑÂÜÖÂ≠òÂç†Áî®„ÄÅËÉΩËÄóÂíåËß£Á†ÅÂª∂Ëøü„ÄÇ‰∏∫‰∫Ü‰øÉËøõËøõ‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂ÂíåÂ∫îÁî®ÔºåËØ•Ê®°ÂûãÁöÑÊùÉÈáçÈÄöËøáHugging FaceÂèëÂ∏ÉÔºåÂπ∂Êèê‰æõ‰∫ÜÈÄÇÁî®‰∫éGPUÂíåCPUÊû∂ÊûÑÁöÑÂºÄÊ∫êÊé®ÁêÜÂÆûÁé∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11536",
            "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
            "url": "https://huggingface.co/papers/2504.11536",
            "abstract": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems.",
            "score": 23,
            "issue_id": 3288,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 –∞–ø—Ä–µ–ª—è",
                "en": "April 15",
                "zh": "4Êúà15Êó•"
            },
            "hash": "1402eef5411f1416",
            "authors": [
                "Jiazhan Feng",
                "Shijue Huang",
                "Xingwei Qu",
                "Ge Zhang",
                "Yujia Qin",
                "Baoquan Zhong",
                "Chengquan Jiang",
                "Jinxin Chi",
                "Wanjun Zhong"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.11536.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#rl",
                    "#math",
                    "#synthetic",
                    "#reasoning",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "ReTool: –£—Å–∏–ª–µ–Ω–∏–µ –ò–ò-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ ReTool, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø—É—Ç–µ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä—ã –∫–æ–¥–∞. ReTool –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –∫–æ–≥–¥–∞ –∏ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å–ª–æ–∂–Ω–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ MATH Olympiad AIME –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ ReTool –Ω–∞–¥ –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "ReTool: Bridging Reasoning and Computation for Enhanced Learning",
                    "desc": "This paper introduces ReTool, a novel approach that enhances long-form reasoning in machine learning models by integrating real-time code execution into the reasoning process. It addresses the limitations of existing models in structured problem-solving tasks, such as geometric reasoning and complex computations, by employing a reinforcement learning framework that allows models to learn when and how to use computational tools effectively. ReTool's training involves generating synthetic data to fine-tune models and using task outcomes as rewards to improve tool invocation strategies. Experimental results show that ReTool significantly outperforms traditional text-based reinforcement learning models, demonstrating its potential for advancing complex mathematical reasoning and hybrid neuro-symbolic systems."
                },
                "zh": {
                    "title": "ReToolÔºöÊèêÂçáÂ§çÊùÇÊé®ÁêÜÁöÑÂ∑•ÂÖ∑ÈõÜÊàêÂ≠¶‰π†",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ReToolÁöÑÊ®°ÂûãÔºåÊó®Âú®ÊèêÂçáÈïøÊñáÊú¨Êé®ÁêÜËÉΩÂäõÔºåÁâπÂà´ÊòØÂú®Âá†‰ΩïÊé®ÁêÜÂíåÂ§çÊùÇÊñπÁ®ãÊ±ÇËß£Á≠âÁªìÊûÑÂåñÈóÆÈ¢ò‰∏ä„ÄÇReToolÁªìÂêà‰∫ÜÂÆûÊó∂‰ª£Á†ÅÊâßË°å‰∏éËá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜÁöÑÂä®ÊÄÅ‰∫§ÊõøÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËá™Âä®ÂåñÁ≠ñÁï•Êù•‰ºòÂåñÂ∑•ÂÖ∑ÁöÑ‰ΩøÁî®„ÄÇÈÄöËøáÂêàÊàêÂÜ∑ÂêØÂä®Êï∞ÊçÆÁîüÊàê‰ª£Á†ÅÂ¢ûÂº∫ÁöÑÊé®ÁêÜËΩ®ËøπÔºåReToolÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠‰∏çÊñ≠Ë∞ÉÊï¥Ê®°ÂûãÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®Á≠ñÁï•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåReToolÂú®MATHÂ••ÊûóÂåπÂÖãÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12240",
            "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
            "url": "https://huggingface.co/papers/2504.12240",
            "abstract": "The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.",
            "score": 17,
            "issue_id": 3280,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 –∞–ø—Ä–µ–ª—è",
                "en": "April 16",
                "zh": "4Êúà16Êó•"
            },
            "hash": "a237e12792a9a0c8",
            "authors": [
                "Junhao Zhuang",
                "Lingen Li",
                "Xuan Ju",
                "Zhaoyang Zhang",
                "Chun Yuan",
                "Ying Shan"
            ],
            "affiliations": [
                "Tencent ARC Lab, China",
                "The Chinese University of Hong Kong, China",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12240.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#cv",
                    "#open_source",
                    "#architecture",
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "üé®",
                "ru": {
                    "title": "Cobra: –ë—ã—Å—Ç—Ä–∞—è –∏ —Ç–æ—á–Ω–∞—è –∫–æ–ª–æ—Ä–∏–∑–∞—Ü–∏—è –∫–æ–º–∏–∫—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Cobra –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∫–æ–ª–æ—Ä–∏–∑–∞—Ü–∏–∏ –∫–æ–º–∏–∫—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Causal Sparse DiT —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±—ã—Å—Ç—Ä–æ –∏ —Ç–æ—á–Ω–æ —Ä–∞—Å–∫—Ä–∞—à–∏–≤–∞—Ç—å –ª–∏–Ω–µ–π–Ω—ã–µ —Ä–∏—Å—É–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º –±–æ–ª–µ–µ 200 —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–≤. Cobra –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –æ—Ç–≤–µ—á–∞—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –∫–æ–º–∏–∫—Å–æ–≤."
                },
                "en": {
                    "title": "Cobra: Revolutionizing Line Art Colorization with Contextual Efficiency",
                    "desc": "This paper presents Cobra, a novel method for line art colorization in the comic production industry, which requires high accuracy and efficiency. Cobra utilizes a Causal Sparse DiT architecture that incorporates advanced techniques like causal sparse attention and positional encodings to handle over 200 reference images effectively. The method addresses challenges such as slow inference times and the need for flexible control, ensuring color identity consistency across diverse characters and backgrounds. Experimental results show that Cobra significantly improves the quality and speed of line art colorization, making it a valuable tool for artists."
                },
                "zh": {
                    "title": "CobraÔºöÈ´òÊïàÁÅµÊ¥ªÁöÑÁ∫øÊù°Ëâ∫ÊúØ‰∏äËâ≤Ëß£ÂÜ≥ÊñπÊ°à",
                    "desc": "Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫CobraÁöÑÈ´òÊïàÁ∫øÊù°Ëâ∫ÊúØ‰∏äËâ≤ÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥Êº´ÁîªÂà∂‰ΩúË°å‰∏ö‰∏≠ÂØπÈ´òÂáÜÁ°ÆÊÄßÂíåÁÅµÊ¥ªÊéßÂà∂ÁöÑÈúÄÊ±Ç„ÄÇCobraËÉΩÂ§üÂ§ÑÁêÜË∂ÖËøá200Âº†ÂèÇËÄÉÂõæÂÉèÔºåÂπ∂‰øùÊåÅ‰ΩéÂª∂ËøüÔºåÈÄÇÂ∫îÂ§çÊùÇÁöÑËßíËâ≤ÂíåËÉåÊôØ„ÄÇËØ•ÊñπÊ≥ïÈááÁî®‰∫ÜÂõ†ÊûúÁ®ÄÁñèDiTÊû∂ÊûÑÔºåÂà©Áî®ÁâπÊÆäËÆæËÆ°ÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÂíåÂõ†ÊûúÁ®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÊúâÊïàÁÆ°ÁêÜÈïø‰∏ä‰∏ãÊñáÂèÇËÄÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCobraÂú®‰∏äËâ≤Ë¥®ÈáèÂíåÊé®ÁêÜÈÄüÂ∫¶‰∏äÂùáÊúâÊòæËëóÊèêÂçáÔºåÊª°Ë∂≥‰∫ÜÂ∑•‰∏öÁïåÁöÑÂÖ≥ÈîÆÈúÄÊ±Ç„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10326",
            "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
            "url": "https://huggingface.co/papers/2504.10326",
            "abstract": "AlayaDB is a cutting-edge vector database system natively architected for efficient and effective long-context inference for Large Language Models (LLMs) at AlayaDB AI. Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into a novel vector database system. For the Model as a Service providers (MaaS), AlayaDB consumes fewer hardware resources and offers higher generation quality for various workloads with different kinds of Service Level Objectives (SLOs), when comparing with the existing alternative solutions (e.g., KV cache disaggregation, retrieval-based sparse attention). The crux of AlayaDB is that it abstracts the attention computation and cache management for LLM inference into a query processing procedure, and optimizes the performance via a native query optimizer. In this work, we demonstrate the effectiveness of AlayaDB via (i) three use cases from our industry partners, and (ii) extensive experimental results on LLM inference benchmarks.",
            "score": 17,
            "issue_id": 3286,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 –∞–ø—Ä–µ–ª—è",
                "en": "April 14",
                "zh": "4Êúà14Êó•"
            },
            "hash": "030a9ba5449806fb",
            "authors": [
                "Yangshen Deng",
                "Zhengxin You",
                "Long Xiang",
                "Qilong Li",
                "Peiqi Yuan",
                "Zhaoyang Hong",
                "Yitao Zheng",
                "Wanting Li",
                "Runzhong Li",
                "Haotian Liu",
                "Kyriakos Mouratidis",
                "Man Lung Yiu",
                "Huan Li",
                "Qiaomu Shen",
                "Rui Mao",
                "Bo Tang"
            ],
            "affiliations": [
                "AlayaDB AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10326.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#long_context",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "üöÄ",
                "ru": {
                    "title": "AlayaDB: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "AlayaDB - —ç—Ç–æ –ø–µ—Ä–µ–¥–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∞ –æ—Ç–¥–µ–ª—è–µ—Ç KV-–∫—ç—à –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –æ—Ç —Å–∏—Å—Ç–µ–º –≤—ã–≤–æ–¥–∞ LLM, –∏–Ω–∫–∞–ø—Å—É–ª–∏—Ä—É—è –∏—Ö –≤ –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö. AlayaDB –ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç –º–µ–Ω—å—à–µ –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –Ω–∞–≥—Ä—É–∑–æ–∫ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏. –ö–ª—é—á–µ–≤—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º AlayaDB —è–≤–ª—è–µ—Ç—Å—è –∞–±—Å—Ç—Ä–∞–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫—ç—à–µ–º –¥–ª—è –≤—ã–≤–æ–¥–∞ LLM –≤ –ø—Ä–æ—Ü–µ–¥—É—Ä—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –Ω–∞—Ç–∏–≤–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤."
                },
                "en": {
                    "title": "Revolutionizing LLM Inference with AlayaDB",
                    "desc": "AlayaDB is an innovative vector database designed to enhance long-context inference for Large Language Models (LLMs). It separates key-value (KV) caching and attention computation from the LLM inference process, streamlining these functions into a dedicated database system. This architecture allows Model as a Service (MaaS) providers to use fewer hardware resources while achieving superior generation quality across various workloads. The system's core feature is its ability to treat attention computation and cache management as a query processing task, which is further optimized by a specialized query optimizer."
                },
                "zh": {
                    "title": "AlayaDBÔºöÈ´òÊïàÁöÑÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜËß£ÂÜ≥ÊñπÊ°à",
                    "desc": "AlayaDBÊòØ‰∏ÄÁßçÂÖàËøõÁöÑÂêëÈáèÊï∞ÊçÆÂ∫ìÁ≥ªÁªüÔºå‰∏ì‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËÆæËÆ°ÔºåÊó®Âú®ÊèêÈ´òÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜÁöÑÊïàÁéáÂíåÊïàÊûú„ÄÇÂÆÉÂ∞ÜÈîÆÂÄºÁºìÂ≠òÂíåÊ≥®ÊÑèÂäõËÆ°ÁÆó‰ªéLLMÊé®ÁêÜÁ≥ªÁªü‰∏≠Ëß£ËÄ¶ÔºåÂπ∂Â∞ÜÂÖ∂Â∞ÅË£ÖÂà∞‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂêëÈáèÊï∞ÊçÆÂ∫ìÁ≥ªÁªü‰∏≠„ÄÇ‰∏éÁé∞ÊúâËß£ÂÜ≥ÊñπÊ°àÁõ∏ÊØîÔºåAlayaDBÂú®Á°¨‰ª∂ËµÑÊ∫êÊ∂àËÄóÂíåÁîüÊàêË¥®ÈáèÊñπÈù¢Ë°®Áé∞Êõ¥‰Ω≥ÔºåÈÄÇÁî®‰∫é‰∏çÂêåÊúçÂä°Ê∞¥Âπ≥ÁõÆÊ†áÔºàSLOÔºâÁöÑÂ§öÁßçÂ∑•‰ΩúË¥üËΩΩ„ÄÇËØ•Á≥ªÁªüÈÄöËøáÂ∞ÜÊ≥®ÊÑèÂäõËÆ°ÁÆóÂíåÁºìÂ≠òÁÆ°ÁêÜÊäΩË±°‰∏∫Êü•ËØ¢Â§ÑÁêÜËøáÁ®ãÔºåÂπ∂ÈÄöËøáÊú¨Âú∞Êü•ËØ¢‰ºòÂåñÂô®‰ºòÂåñÊÄßËÉΩÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09081",
            "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning",
            "url": "https://huggingface.co/papers/2504.09081",
            "abstract": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing a diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, a benchmark dataset specifically designed to evaluate the instruction-following capabilities of speech-text LLMs.",
            "score": 12,
            "issue_id": 3287,
            "pub_date": "2025-04-12",
            "pub_date_card": {
                "ru": "12 –∞–ø—Ä–µ–ª—è",
                "en": "April 12",
                "zh": "4Êúà12Êó•"
            },
            "hash": "4c17c8d0a36c365d",
            "authors": [
                "Prabhat Pandey",
                "Rupak Vignesh Swaminathan",
                "K V Vijay Girish",
                "Arunasish Sen",
                "Jian Xie",
                "Grant P. Strimel",
                "Andreas Schwarz"
            ],
            "affiliations": [
                "Amazon AGI",
                "Apple Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09081.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#audio",
                    "#transfer_learning",
                    "#multilingual",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "üó£Ô∏è",
                "ru": {
                    "title": "SIFT: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Ä–µ—á–µ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "SIFT (Speech Instruction Fine-Tuning) - —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 50 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å —Ä–µ—á—å—é –∏ —Ç–µ–∫—Å—Ç–æ–º. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—É–±–ª–∏—á–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ—á–µ–≤—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤ –∏ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –ø—è—Ç—å —è–∑—ã–∫–æ–≤, –≤–∫–ª—é—á–∞—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –ø–æ–Ω–∏–º–∞–Ω–∏—é –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏. –ú–æ–¥–µ–ª—å SIFT-LLM, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —ç—Ç–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—á–µ–≤—ã–µ –Ø–ú –≤ –∑–∞–¥–∞—á–∞—Ö —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç EvalSIFT - –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–µ—á–µ–≤—ã—Ö –Ø–ú —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º."
                },
                "en": {
                    "title": "Empowering Speech Models with SIFT: Fine-Tuning for Instruction Mastery",
                    "desc": "The paper presents SIFT (Speech Instruction Fine-Tuning), a large dataset containing 50 million examples for enhancing speech-text large language models (LLMs). It is constructed from 14,000 hours of publicly available speech data across five languages, focusing on both speech understanding and generation tasks. The authors train a model called SIFT-LLM, which shows superior performance on instruction-following benchmarks compared to existing models, while also being competitive in foundational speech tasks. Additionally, they introduce EvalSIFT, a specialized benchmark for assessing the instruction-following abilities of speech-text LLMs."
                },
                "zh": {
                    "title": "SIFTÔºöÊèêÂçáËØ≠Èü≥Êåá‰ª§ÁêÜËß£ÁöÑÂàõÊñ∞Êï∞ÊçÆÈõÜ",
                    "desc": "Êàë‰ª¨‰ªãÁªç‰∫ÜSIFTÔºàËØ≠Èü≥Êåá‰ª§ÂæÆË∞ÉÔºâÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´5000‰∏áÊù°Á§∫‰æãÁöÑÊï∞ÊçÆÈõÜÔºåÊó®Âú®Áî®‰∫éËØ≠Èü≥-ÊñáÊú¨Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊåá‰ª§ÂæÆË∞ÉÂíåÈ¢ÑËÆ≠ÁªÉ„ÄÇSIFT-50MÂü∫‰∫éÂÖ¨ÂºÄÁöÑËØ≠Èü≥ËØ≠ÊñôÂ∫ìÊûÑÂª∫ÔºåÂåÖÂê´14000Â∞èÊó∂ÁöÑËØ≠Èü≥ÔºåÂà©Áî®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÁé∞ÊàêÁöÑ‰∏ìÂÆ∂Ê®°Âûã„ÄÇËØ•Êï∞ÊçÆÈõÜÊ∂µÁõñ‰∫îÁßçËØ≠Ë®ÄÔºåÂåÖÂê´Â§öÊ†∑ÁöÑËØ≠Èü≥ÁêÜËß£ÂíåÂèØÊéßÁöÑËØ≠Èü≥ÁîüÊàêÊåá‰ª§„ÄÇ‰ΩøÁî®SIFT-50MÔºåÊàë‰ª¨ËÆ≠ÁªÉ‰∫ÜSIFT-LLMÔºåÂÖ∂Âú®Êåá‰ª§Ë∑üÈöèÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑËØ≠Èü≥-ÊñáÊú¨Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÂêåÊó∂Âú®Âü∫Á°ÄËØ≠Èü≥‰ªªÂä°‰∏ä‰πüË°®Áé∞Âá∫Á´û‰∫âÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10483",
            "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers",
            "url": "https://huggingface.co/papers/2504.10483",
            "abstract": "In this paper we tackle a fundamental question: \"Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?\" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, we observe that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, our approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256 x 256. Code is available at https://end2end-diffusion.github.io.",
            "score": 9,
            "issue_id": 3285,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 –∞–ø—Ä–µ–ª—è",
                "en": "April 14",
                "zh": "4Êúà14Êó•"
            },
            "hash": "f46935088154b083",
            "authors": [
                "Xingjian Leng",
                "Jaskirat Singh",
                "Yunzhong Hou",
                "Zhenchang Xing",
                "Saining Xie",
                "Liang Zheng"
            ],
            "affiliations": [
                "Australian National University",
                "Data61 CSIRO",
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10483.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "üöÄ",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: —Å–æ–≤–º–µ—Å—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å VAE",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ (VAE) –≤ —Å–∫–≤–æ–∑–Ω–æ–º —Ä–µ–∂–∏–º–µ. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è —Ç–∞–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –Ω–æ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π (REPA). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ (REPA-E) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —É–ª—É—á—à–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ VAE. –ü–æ–¥—Ö–æ–¥ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–π state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–µ FID –Ω–∞ ImageNet 256x256."
                },
                "en": {
                    "title": "Unlocking End-to-End Training with REPA for Latent Diffusion Models",
                    "desc": "This paper explores the possibility of training latent diffusion models alongside a variational auto-encoder (VAE) in an end-to-end fashion. It highlights that traditional methods using standard diffusion loss are ineffective and can even harm performance. The authors introduce a new loss function called representation-alignment (REPA) loss, which enables effective joint training of the VAE and diffusion model. Their proposed training method, REPA-E, significantly accelerates training and enhances the quality of generated outputs, achieving state-of-the-art results on ImageNet."
                },
                "zh": {
                    "title": "Á´ØÂà∞Á´ØËÆ≠ÁªÉÁöÑÁ™ÅÁ†¥ÔºöREPAÊçüÂ§±ÁöÑÂäõÈáè",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏Ä‰∏™Âü∫Êú¨ÈóÆÈ¢òÔºöËÉΩÂê¶Â∞ÜÊΩúÂú®Êâ©Êï£Ê®°Âûã‰∏éÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÊ†áËÆ∞Âô®‰∏ÄËµ∑ËøõË°åÁ´ØÂà∞Á´ØËÆ≠ÁªÉÔºü‰º†ÁªüÁöÑÊ∑±Â∫¶Â≠¶‰π†ÁêÜËÆ∫ËÆ§‰∏∫ÔºåÂ∞ΩÂèØËÉΩËøõË°åÁ´ØÂà∞Á´ØËÆ≠ÁªÉÊòØÊõ¥‰ºòÁöÑÈÄâÊã©„ÄÇÁÑ∂ËÄåÔºåÂØπ‰∫éÊΩúÂú®Êâ©Êï£ÂèòÊç¢Âô®Ôºå‰ΩøÁî®Ê†áÂáÜÊâ©Êï£ÊçüÂ§±ËøõË°åÁ´ØÂà∞Á´ØËÆ≠ÁªÉ‰ºöÂØºËá¥ÊïàÊûú‰∏ç‰Ω≥ÔºåÁîöËá≥Èôç‰ΩéÊúÄÁªàÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÈÄöËøáË°®Á§∫ÂØπÈΩêÊçüÂ§±ÔºàREPAÊçüÂ§±ÔºâÊù•Ëß£ÈîÅÁ´ØÂà∞Á´ØËÆ≠ÁªÉÔºå‰ΩøÂæóVAEÂíåÊâ©Êï£Ê®°ÂûãËÉΩÂ§üÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂÖ±ÂêåË∞ÉÊï¥ÔºåÊúÄÁªàÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11468",
            "title": "SFT or RL? An Early Investigation into Training R1-Like Reasoning Large\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2504.11468",
            "abstract": "This work revisits the dominant supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm for training Large Vision-Language Models (LVLMs), and reveals a key finding: SFT can significantly undermine subsequent RL by inducing ``pseudo reasoning paths'' imitated from expert models. While these paths may resemble the native reasoning paths of RL models, they often involve prolonged, hesitant, less informative steps, and incorrect reasoning. To systematically study this effect, we introduce VLAA-Thinking, a new multimodal dataset designed to support reasoning in LVLMs. Constructed via a six-step pipeline involving captioning, reasoning distillation, answer rewrite and verification, VLAA-Thinking comprises high-quality, step-by-step visual reasoning traces for SFT, along with a more challenging RL split from the same data source. Using this dataset, we conduct extensive experiments comparing SFT, RL and their combinations. Results show that while SFT helps models learn reasoning formats, it often locks aligned models into imitative, rigid reasoning modes that impede further learning. In contrast, building on the Group Relative Policy Optimization (GRPO) with a novel mixed reward module integrating both perception and cognition signals, our RL approach fosters more genuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on Qwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard (https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard) among 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope our findings provide valuable insights in developing reasoning-capable LVLMs and can inform future research in this area.",
            "score": 7,
            "issue_id": 3294,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 –∞–ø—Ä–µ–ª—è",
                "en": "April 10",
                "zh": "4Êúà10Êó•"
            },
            "hash": "550fb98da92065ed",
            "authors": [
                "Hardy Chen",
                "Haoqin Tu",
                "Fali Wang",
                "Hui Liu",
                "Xianfeng Tang",
                "Xinya Du",
                "Yuyin Zhou",
                "Cihang Xie"
            ],
            "affiliations": [
                "Amazon Research",
                "The Pennsylvania State University",
                "University of California, Santa Cruz",
                "University of Texas at Dallas"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11468.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#rl",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ü–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç –∏–º–∏—Ç–∞—Ü–∏–∏ –∫ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é",
                    "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—é—â–∏–π —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (SFT), –∞ –∑–∞—Ç–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL), –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ SFT –º–æ–∂–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å '–ø—Å–µ–≤–¥–æ-–ø—É—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', –∫–æ—Ç–æ—Ä—ã–µ –º–µ—à–∞—é—Ç –¥–∞–ª—å–Ω–µ–π—à–µ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–º–æ—â—å—é RL. –î–ª—è –∏–∑—É—á–µ–Ω–∏—è —ç—Ç–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞ –±—ã–ª —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç VLAA-Thinking, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ—à–∞–≥–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ RL —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Group Relative Policy Optimization (GRPO) –∏ –Ω–æ–≤—ã–º –º–æ–¥—É–ª–µ–º —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —Ä–∞–∑–≤–∏–≤–∞—Ç—å –±–æ–ª–µ–µ –≥–∏–±–∫–∏–µ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Unlocking Genuine Reasoning in LVLMs",
                    "desc": "This paper examines the training process of Large Vision-Language Models (LVLMs) using supervised fine-tuning (SFT) followed by reinforcement learning (RL). It finds that SFT can create misleading reasoning patterns that hinder the effectiveness of subsequent RL, leading to less informative and incorrect reasoning. To address this, the authors introduce VLAA-Thinking, a new dataset that supports better reasoning in LVLMs through a structured six-step process. Their experiments show that while SFT can help with learning reasoning formats, it can also restrict models to rigid reasoning, whereas their RL approach encourages more flexible and adaptive reasoning capabilities."
                },
                "zh": {
                    "title": "ÊâìÁ†¥Ê®°‰ªøÔºå‰øÉËøõÁúüÂÆûÊé®ÁêÜÁöÑLVLMËÆ≠ÁªÉ",
                    "desc": "Êú¨ÊñáÈáçÊñ∞ÂÆ°ËßÜ‰∫ÜÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÁöÑÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâËÆ≠ÁªÉËåÉÂºèÔºåÂèëÁé∞SFTÂèØËÉΩ‰ºöÈÄöËøáÂºïÂÖ•‚Äú‰º™Êé®ÁêÜË∑ØÂæÑ‚ÄùÊù•ÊòæËëóÂâäÂº±ÂêéÁª≠ÁöÑRL„ÄÇËøô‰∫õ‰º™Ë∑ØÂæÑËôΩÁÑ∂Áúã‰ºº‰∏éRLÊ®°ÂûãÁöÑÂéüÁîüÊé®ÁêÜË∑ØÂæÑÁõ∏‰ººÔºå‰ΩÜÂæÄÂæÄÊ∂âÂèäÂÜóÈïø„ÄÅÁäπË±´‰∏î‰ø°ÊÅØÈáè‰∏çË∂≥ÁöÑÊ≠•È™§ÔºåÁîöËá≥ÈîôËØØÊé®ÁêÜ„ÄÇ‰∏∫Á≥ªÁªüÁ†îÁ©∂Ëøô‰∏ÄÁé∞Ë±°ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜVLAA-ThinkingÔºå‰∏Ä‰∏™Êñ∞ÂûãÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜÔºåÊó®Âú®ÊîØÊåÅLVLMsÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÈÄöËøáÂÖ≠Ê≠•ÊµÅÁ®ãÊûÑÂª∫ÁöÑVLAA-ThinkingÊèê‰æõ‰∫ÜÈ´òË¥®ÈáèÁöÑÈÄêÊ≠•ËßÜËßâÊé®ÁêÜËΩ®ËøπÔºåÂπ∂ÈÄöËøáÂÆûÈ™åË°®ÊòéÔºåSFTËôΩÁÑ∂ÊúâÂä©‰∫éÊ®°ÂûãÂ≠¶‰π†Êé®ÁêÜÊ†ºÂºèÔºå‰ΩÜ‰ºö‰ΩøÊ®°ÂûãÈô∑ÂÖ•Ê®°‰ªøÂíåÂÉµÂåñÁöÑÊé®ÁêÜÊ®°ÂºèÔºåÂ¶®Á¢çËøõ‰∏ÄÊ≠•Â≠¶‰π†„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12264",
            "title": "Towards Learning to Complete Anything in Lidar",
            "url": "https://huggingface.co/papers/2504.12264",
            "abstract": "We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion in-the-wild. This is closely related to Lidar-based semantic/panoptic scene completion. However, contemporary methods can only complete and recognize objects from a closed vocabulary labeled in existing Lidar datasets. Different to that, our zero-shot approach leverages the temporal context from multi-modal sensor sequences to mine object shapes and semantic features of observed objects. These are then distilled into a Lidar-only instance-level completion and recognition model. Although we only mine partial shape completions, we find that our distilled model learns to infer full object shapes from multiple such partial observations across the dataset. We show that our model can be prompted on standard benchmarks for Semantic and Panoptic Scene Completion, localize objects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class vocabularies. Our project page is https://research.nvidia.com/labs/dvl/projects/complete-anything-lidar",
            "score": 4,
            "issue_id": 3292,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 –∞–ø—Ä–µ–ª—è",
                "en": "April 16",
                "zh": "4Êúà16Êó•"
            },
            "hash": "171045e898ed54a7",
            "authors": [
                "Ayca Takmaz",
                "Cristiano Saltori",
                "Neehar Peri",
                "Tim Meinhardt",
                "Riccardo de Lutio",
                "Laura Leal-Taix√©",
                "Aljo≈°a O≈°ep"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "ETH Zurich",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12264.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "üöó",
                "ru": {
                    "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ñ–æ—Ä–º –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ –¥–∞–Ω–Ω—ã–º –ª–∏–¥–∞—Ä–∞",
                    "desc": "CAL (Complete Anything in Lidar) - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é —Ñ–æ—Ä–º –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –ª–∏–¥–∞—Ä–∞ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –∑–∞—Ä–∞–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º —Å–ª–æ–≤–∞—Ä–µ–º –æ–±—ä–µ–∫—Ç–æ–≤, CAL –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–µ–Ω—Å–æ—Ä–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–æ—Ä–º –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞–±–ª—é–¥–∞–µ–º—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤. –≠—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞—Ç–µ–º –¥–∏—Å—Ç–∏–ª–ª–∏—Ä—É–µ—Ç—Å—è –≤ –º–æ–¥–µ–ª—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â—É—é —Ç–æ–ª—å–∫–æ —Å –¥–∞–Ω–Ω—ã–º–∏ –ª–∏–¥–∞—Ä–∞. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –ª–∏—à—å —á–∞—Å—Ç–∏—á–Ω—ã–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ñ–æ—Ä–º, –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø–æ–ª–Ω—ã–µ —Ñ–æ—Ä–º—ã –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ç–∞–∫–∏—Ö —á–∞—Å—Ç–∏—á–Ω—ã—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö."
                },
                "en": {
                    "title": "Complete Any Object with Lidar: Beyond Fixed Classes!",
                    "desc": "The paper introduces CAL (Complete Anything in Lidar), a novel approach for completing shapes using Lidar data in real-world scenarios. Unlike traditional methods that rely on a fixed set of labeled objects, CAL employs a zero-shot learning technique that utilizes temporal context from multi-modal sensor sequences to extract object shapes and semantic features. This information is then distilled into a model that can complete and recognize objects at the instance level, even when only partial shapes are available. The results demonstrate that CAL can effectively infer full object shapes and perform well on standard benchmarks for scene completion and object localization, extending recognition beyond predefined class categories."
                },
                "zh": {
                    "title": "ÊøÄÂÖâÈõ∑Ëææ‰∏≠ÁöÑÂΩ¢Áä∂Ë°•ÂÖ®Êñ∞ÊñπÊ≥ï",
                    "desc": "Êàë‰ª¨ÊèêÂá∫‰∫ÜCALÔºàÂú®ÊøÄÂÖâÈõ∑Ëææ‰∏≠ÂÆåÊàê‰ªª‰ΩïÂΩ¢Áä∂ÔºâÔºåÁî®‰∫éÂú®ÂÆûÈôÖÁéØÂ¢É‰∏≠ËøõË°åÊøÄÂÖâÈõ∑ËææÂü∫Á°ÄÁöÑÂΩ¢Áä∂Ë°•ÂÖ®„ÄÇËøôÁßçÊñπÊ≥ï‰∏éÊøÄÂÖâÈõ∑ËææÂü∫Á°ÄÁöÑËØ≠‰πâ/ÂÖ®ÊôØÂú∫ÊôØË°•ÂÖ®ÂØÜÂàáÁõ∏ÂÖ≥Ôºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂè™ËÉΩÂú®Âõ∫ÂÆöÁöÑÊ†áÁ≠æËØçÊ±á‰∏≠ÂÆåÊàêÂíåËØÜÂà´ÂØπË±°„ÄÇÊàë‰ª¨ÁöÑÈõ∂Ê†∑Êú¨ÊñπÊ≥ïÂà©Áî®Â§öÊ®°ÊÄÅ‰º†ÊÑüÂô®Â∫èÂàó‰∏≠ÁöÑÊó∂Èó¥‰∏ä‰∏ãÊñáÔºåÊåñÊéòËßÇÂØüÂà∞ÂØπË±°ÁöÑÂΩ¢Áä∂ÂíåËØ≠‰πâÁâπÂæÅÔºåÂπ∂Â∞ÜÂÖ∂ÊèêÁÇº‰∏∫‰ªÖÂü∫‰∫éÊøÄÂÖâÈõ∑ËææÁöÑÂÆû‰æãÁ∫ßË°•ÂÖ®ÂíåËØÜÂà´Ê®°Âûã„ÄÇÂ∞ΩÁÆ°Êàë‰ª¨Âè™ÊåñÊéòÈÉ®ÂàÜÂΩ¢Áä∂Ë°•ÂÖ®Ôºå‰ΩÜÊàë‰ª¨ÁöÑÊ®°ÂûãËÉΩÂ§ü‰ªéÂ§ö‰∏™ÈÉ®ÂàÜËßÇÂØü‰∏≠Êé®Êñ≠Âá∫ÂÆåÊï¥ÁöÑÂØπË±°ÂΩ¢Áä∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11952",
            "title": "Robust and Fine-Grained Detection of AI Generated Texts",
            "url": "https://huggingface.co/papers/2504.11952",
            "abstract": "An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.",
            "score": 4,
            "issue_id": 3280,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 –∞–ø—Ä–µ–ª—è",
                "en": "April 16",
                "zh": "4Êúà16Êó•"
            },
            "hash": "bdea465fe17b9401",
            "authors": [
                "Ram Mohan Rao Kadiyala",
                "Siddartha Pullakhandam",
                "Kanwal Mehreen",
                "Drishti Sharma",
                "Siddhant Gupta",
                "Jebish Purbey",
                "Ashay Srivastava",
                "Subhasya TippaReddy",
                "Arvind Reddy Bobbili",
                "Suraj Telugara Chandrashekhar",
                "Modabbir Adeeb",
                "Srinadh Vura",
                "Hamza Farooq"
            ],
            "affiliations": [
                "Cohere for AI Community",
                "IISc Bangalore",
                "IIT Roorkee",
                "M2ai.in",
                "Pulchowk Campus",
                "Stanford University",
                "Traversaal.ai",
                "University of California, Los Angeles",
                "University of Houston",
                "University of Maryland, College Park",
                "University of South Florida",
                "Vantager"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11952.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#hallucinations",
                    "#dataset",
                    "#benchmark",
                    "#multilingual",
                    "#security",
                    "#data"
                ],
                "emoji": "üïµÔ∏è",
                "ru": {
                    "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –ò–ò-—Ç–µ–∫—Å—Ç–æ–≤: –æ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ —è–∑—ã–∫–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é —Ç–µ–∫—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ–±—à–∏—Ä–Ω–æ–º –Ω–∞–±–æ—Ä–µ —Ç–µ–∫—Å—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Å–æ–≤–º–µ—Å—Ç–Ω–æ —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –ò–ò. –ú–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö –∏–∑ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π, –æ—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏ –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö —Å —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 2,4 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ 23 —è–∑—ã–∫–∞—Ö, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –≤ —Å–æ–∞–≤—Ç–æ—Ä—Å—Ç–≤–µ —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏."
                },
                "en": {
                    "title": "Advancing Detection of Human-LLM Co-Authored Texts",
                    "desc": "This paper addresses the challenge of detecting machine-generated content, particularly in cases where texts are co-authored by humans and language models (LLMs). The authors developed a set of token classification models trained on a large dataset of 2.4 million co-authored texts, which allows for better detection across various domains and generators. The models demonstrated strong performance even with adversarial inputs and texts from non-native speakers. Additionally, the paper provides insights into how the models perform based on text length and characteristics compared to purely human-authored content."
                },
                "zh": {
                    "title": "ÊûÑÂª∫È´òÊïàÁöÑÊú∫Âô®ÁîüÊàêÂÜÖÂÆπÊ£ÄÊµãÁ≥ªÁªü",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁêÜÊÉ≥ÁöÑÊ£ÄÊµãÁ≥ªÁªüÔºåÊó®Âú®ÊúâÊïàËØÜÂà´Êú∫Âô®ÁîüÊàêÁöÑÂÜÖÂÆπÔºåÂ∞§ÂÖ∂ÊòØÂú®Áü≠ÊñáÊú¨‰∏≠„ÄÇÁé∞ÊúâÁ≥ªÁªüÂú®ËØÜÂà´AIÁîüÊàêÂÜÖÂÆπÊó∂Â∏∏Â∏∏Èù¢‰∏¥ÊåëÊàòÔºåÂõ†Ê≠§Êàë‰ª¨‰∏ìÊ≥®‰∫é‰∫∫Á±ª‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂÖ±ÂêåÂàõ‰ΩúÁöÑÊñáÊú¨„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏ÄÁ≥ªÂàóÁî®‰∫éÊ†áËÆ∞ÂàÜÁ±ªÁöÑÊ®°ÂûãÔºåËøô‰∫õÊ®°ÂûãÂú®Â§ßÈáè‰∫∫Êú∫ÂÖ±ÂàõÊñáÊú¨‰∏äËøõË°åËÆ≠ÁªÉÔºåÂπ∂Âú®Êú™ËßÅÈ¢ÜÂüüÂíåÁîüÊàêÂô®ÁöÑÊñáÊú¨‰∏äË°®Áé∞ËâØÂ•Ω„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÂåÖÂê´240‰∏áÊù°ÊñáÊú¨ÁöÑÊñ∞Êï∞ÊçÆÈõÜÔºå‰∏ªË¶ÅÁî±Â§öÁßçÊµÅË°åÁöÑ‰∏ìÊúâLLMÂÖ±ÂêåÂàõ‰ΩúÔºåÊ∂µÁõñ23ÁßçËØ≠Ë®Ä„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11092",
            "title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video\n  Inpainting",
            "url": "https://huggingface.co/papers/2504.11092",
            "abstract": "Reconstructing 4D dynamic scenes from casually captured monocular videos is valuable but highly challenging, as each timestamp is observed from a single viewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular video synthesis by augmenting observation views - synthesizing multi-view videos from a monocular input. Unlike existing methods that either solely leverage geometric priors for supervision or use generative priors while overlooking geometry, we integrate both. This reformulates view augmentation as a video inpainting task, where observed views are warped into new viewpoints based on monocular depth priors. To achieve this, we train a video inpainting model on unposed web videos with synthetically generated masks that mimic warping occlusions, ensuring spatially and temporally consistent completion of missing regions. To further mitigate inaccuracies in monocular depth priors, we introduce an iterative view augmentation strategy and a robust reconstruction loss. Experiments demonstrate that our method effectively improves monocular 4D scene reconstruction and completion.",
            "score": 4,
            "issue_id": 3287,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 –∞–ø—Ä–µ–ª—è",
                "en": "April 15",
                "zh": "4Êúà15Êó•"
            },
            "hash": "a6e019e03ced5592",
            "authors": [
                "Jiaxin Huang",
                "Sheng Miao",
                "BangBnag Yang",
                "Yuewen Ma",
                "Yiyi Liao"
            ],
            "affiliations": [
                "ByteDance PICO",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11092.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#video"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "Vivid4D: –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 3D-—Å—Ü–µ–Ω –∏–∑ –æ–±—ã—á–Ω–æ–≥–æ –≤–∏–¥–µ–æ",
                    "desc": "Vivid4D - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É 4D-–≤–∏–¥–µ–æ –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞–∫ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ, —Ç–∞–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–æ—Ä—ã –¥–ª—è augment–∞—Ü–∏–∏ –Ω–∞–±–ª—é–¥–∞–µ–º—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –ó–∞–¥–∞—á–∞ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ –Ω–µ–ø–æ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–±-–≤–∏–¥–µ–æ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è augment–∞—Ü–∏–∏ —Ä–∞–∫—É—Ä—Å–æ–≤ –∏ —É—Å—Ç–æ–π—á–∏–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω."
                },
                "en": {
                    "title": "Enhancing 4D Scene Reconstruction with Vivid4D",
                    "desc": "The paper presents Vivid4D, a new method for reconstructing 4D dynamic scenes from single-view videos. It synthesizes multi-view videos by augmenting the observation views, addressing the challenges of limited viewpoints. The approach combines geometric and generative priors, treating view augmentation as a video inpainting task to fill in missing regions. By training on unposed web videos and using iterative strategies, Vivid4D enhances the accuracy of monocular depth priors and improves scene reconstruction quality."
                },
                "zh": {
                    "title": "Vivid4DÔºö‰ªéÂçïÁõÆËßÜÈ¢ëÈáçÂª∫ÂõõÁª¥Âä®ÊÄÅÂú∫ÊôØÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Vivid4DÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫é‰ªéÂçïÁõÆËßÜÈ¢ë‰∏≠ÈáçÂª∫ÂõõÁª¥Âä®ÊÄÅÂú∫ÊôØ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂêàÊàêÂ§öËßÜËßíËßÜÈ¢ëÊù•Â¢ûÂº∫ÂçïÁõÆËßÜÈ¢ëÂêàÊàêÔºåÂÖãÊúç‰∫Ü‰ªÖ‰ªéÂçï‰∏ÄËßÜËßíËßÇÂØüÁöÑÈôêÂà∂„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ï‰∏çÂêåÔºåVivid4DÂêåÊó∂Âà©Áî®Âá†‰ΩïÂÖàÈ™åÂíåÁîüÊàêÂÖàÈ™åÔºåÂ∞ÜËßÜËßíÂ¢ûÂº∫ÈáçÊñ∞ÂÆö‰πâ‰∏∫ËßÜÈ¢ë‰øÆÂ§ç‰ªªÂä°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÊúâÊïàÊèêÈ´ò‰∫ÜÂçïÁõÆÂõõÁª¥Âú∫ÊôØÁöÑÈáçÂª∫ÂíåË°•ÂÖ®ÊïàÊûú„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09566",
            "title": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution",
            "url": "https://huggingface.co/papers/2504.09566",
            "abstract": "Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors. However, complex tasks with vast solution spaces and vague constraints often exceed the capacity of a single reasoning chain. Inspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic geometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends CoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper logical dependencies, enabling more robust and structured problem-solving. MFR decomposes a module into a sequence of free modules with minimal rank, providing a structured analytical approach to complex systems. This method introduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\", \"Exactness\" and \"Minimality\", enabling the systematic decomposition of the original complex problem into logically complete minimal subproblems while preserving key problem features and reducing reasoning length. We tested SoT across diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini, Qwen2.5), achieving inference accuracy that matches or surpasses mainstream CoTs standards. Additionally, by aligning the sampling process with algebraic constraints, our approach enhances the scalability of inference time in LLMs, ensuring both transparent reasoning and high performance. Our code will be publicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.",
            "score": 1,
            "issue_id": 3290,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 –∞–ø—Ä–µ–ª—è",
                "en": "April 13",
                "zh": "4Êúà13Êó•"
            },
            "hash": "24032586ed61676f",
            "authors": [
                "Chenghao Li",
                "Chaoning Zhang",
                "Yi Lu",
                "Jiaquan Zhang",
                "Qigan Sun",
                "Xudong Wang",
                "Jiwei Wei",
                "Guoqing Wang",
                "Yang Yang",
                "Heng Tao Shen"
            ],
            "affiliations": [
                "Capital Normal University, Beijing, China",
                "Kyung Hee University, Yongin-si, Republic of Korea",
                "Tongji University, Shanghai, China",
                "University of Electronic Science and Technology of China, Chengdu, China",
                "University of Liverpool, Liverpool, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09566.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#inference",
                    "#reasoning",
                    "#training",
                    "#dataset"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "Syzygy of Thoughts: –ù–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Syzygy of Thoughts (SoT), —Ä–∞—Å—à–∏—Ä—è—é—â–∏–π Chain-of-Thought (CoT) –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. SoT –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω –∫–æ–Ω—Ü–µ–ø—Ü–∏–µ–π –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏–∑ –∫–æ–º–º—É—Ç–∞—Ç–∏–≤–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã –∏ –≤–≤–æ–¥–∏—Ç –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø—É—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ SoT –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª—è—Ö –ø–æ–∫–∞–∑–∞–ª–æ —Ç–æ—á–Ω–æ—Å—Ç—å –≤—ã–≤–æ–¥–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â—É—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã CoT."
                },
                "en": {
                    "title": "Enhancing Reasoning with Syzygy of Thoughts",
                    "desc": "This paper introduces a new framework called Syzygy of Thoughts (SoT) that builds on Chain-of-Thought (CoT) prompting to improve the reasoning capabilities of large language models (LLMs). SoT addresses the limitations of single reasoning chains by incorporating multiple interrelated reasoning paths, which helps in tackling complex tasks with broad solution spaces. By drawing inspiration from concepts in commutative algebra, such as Minimal Free Resolution, SoT systematically breaks down complex problems into simpler, manageable subproblems while maintaining essential features. The framework has been tested on various datasets and models, demonstrating improved inference accuracy and efficiency compared to traditional CoT methods."
                },
                "zh": {
                    "title": "ÊÄùÁª¥ÁöÑSyzygyÔºöÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞Ê°ÜÊû∂",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÊÄùÁª¥ÁöÑSyzygyÔºàSoTÔºâÁöÑÊñ∞Ê°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇSoTÈÄöËøáÂºïÂÖ•ËæÖÂä©ÁöÑ„ÄÅÁõ∏‰∫íÂÖ≥ËÅîÁöÑÊé®ÁêÜË∑ØÂæÑÔºåÊâ©Â±ï‰∫ÜÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÔºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÂ§ÑÁêÜÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°„ÄÇËØ•ÊñπÊ≥ïÂÄüÈâ¥‰∫Ü‰∫§Êç¢‰ª£Êï∞Âíå‰ª£Êï∞Âá†‰Ωï‰∏≠ÁöÑÊúÄÂ∞èËá™Áî±ÂàÜËß£ÔºàMFRÔºâÔºåÈÄöËøáÁ≥ªÁªüÂú∞Â∞ÜÂ§çÊùÇÈóÆÈ¢òÂàÜËß£‰∏∫ÈÄªËæë‰∏äÂÆåÊï¥ÁöÑÊúÄÂ∞èÂ≠êÈóÆÈ¢òÔºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSoTÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜÂíåÊ®°Âûã‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÊé®ÁêÜÂáÜÁ°ÆÁéáËææÂà∞ÊàñË∂ÖËøá‰∫Ü‰∏ªÊµÅÁöÑÈìæÂºèÊÄùÁª¥Ê†áÂáÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09346",
            "title": "\"It's not a representation of me\": Examining Accent Bias and Digital\n  Exclusion in Synthetic AI Voice Services",
            "url": "https://huggingface.co/papers/2504.09346",
            "abstract": "Recent advances in artificial intelligence (AI) speech generation and voice cloning technologies have produced naturalistic speech and accurate voice replication, yet their influence on sociotechnical systems across diverse accents and linguistic traits is not fully understood. This study evaluates two synthetic AI voice services (Speechify and ElevenLabs) through a mixed methods approach using surveys and interviews to assess technical performance and uncover how users' lived experiences influence their perceptions of accent variations in these speech technologies. Our findings reveal technical performance disparities across five regional, English-language accents and demonstrate how current speech generation technologies may inadvertently reinforce linguistic privilege and accent-based discrimination, potentially creating new forms of digital exclusion. Overall, our study highlights the need for inclusive design and regulation by providing actionable insights for developers, policymakers, and organizations to ensure equitable and socially responsible AI speech technologies.",
            "score": 1,
            "issue_id": 3294,
            "pub_date": "2025-04-12",
            "pub_date_card": {
                "ru": "12 –∞–ø—Ä–µ–ª—è",
                "en": "April 12",
                "zh": "4Êúà12Êó•"
            },
            "hash": "c5df8b667242e0e1",
            "authors": [
                "Shira Michel",
                "Sufi Kaur",
                "Sarah Elizabeth Gillespie",
                "Jeffrey Gleason",
                "Christo Wilson",
                "Avijit Ghosh"
            ],
            "affiliations": [
                "Hugging Face and University of Connecticut, USA",
                "Northeastern University, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09346.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#healthcare",
                    "#ethics",
                    "#synthetic",
                    "#audio"
                ],
                "emoji": "üó£Ô∏è",
                "ru": {
                    "title": "–ò–Ω–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç—å –∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å –≤ –ò–ò-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ —Å–µ—Ä–≤–∏—Å–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥–æ–ª–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ò–ò (Speechify –∏ ElevenLabs) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ä–æ—Å–æ–≤ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ –≤–∞—Ä–∏–∞—Ü–∏–π –∞–∫—Ü–µ–Ω—Ç–æ–≤ –≤ —ç—Ç–∏—Ö —Ä–µ—á–µ–≤—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö. –í—ã—è–≤–ª–µ–Ω—ã —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—è—Ç–∏ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –∞–∫—Ü–µ–Ω—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏ –º–æ–≥—É—Ç –Ω–µ–ø—Ä–µ–¥–Ω–∞–º–µ—Ä–µ–Ω–Ω–æ —É—Å–∏–ª–∏–≤–∞—Ç—å —è–∑—ã–∫–æ–≤—ã–µ –ø—Ä–∏–≤–∏–ª–µ–≥–∏–∏ –∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—é –ø–æ –∞–∫—Ü–µ–Ω—Ç—É."
                },
                "en": {
                    "title": "Ensuring Fairness in AI Speech: Addressing Accent Bias and Digital Exclusion",
                    "desc": "This paper investigates the impact of AI speech generation and voice cloning technologies on social systems, focusing on how different accents are represented. It evaluates two AI voice services, Speechify and ElevenLabs, using surveys and interviews to understand user experiences and perceptions of accent variations. The study finds that there are significant differences in technical performance across various English accents, which may lead to accent-based discrimination and reinforce existing linguistic privileges. The authors emphasize the importance of inclusive design and regulation to promote fairness and accessibility in AI speech technologies."
                },
                "zh": {
                    "title": "Á°Æ‰øùAIËØ≠Èü≥ÊäÄÊúØÁöÑÂÖ¨Âπ≥‰∏éÂåÖÂÆπ",
                    "desc": "Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫Ü‰∫∫Â∑•Êô∫ËÉΩËØ≠Èü≥ÁîüÊàêÂíåÂ£∞Èü≥ÂÖãÈöÜÊäÄÊúØÂØπÁ§æ‰ºöÊäÄÊúØÁ≥ªÁªüÁöÑÂΩ±ÂìçÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏çÂêåÂè£Èü≥ÂíåËØ≠Ë®ÄÁâπÂæÅÊñπÈù¢„ÄÇÊàë‰ª¨ËØÑ‰º∞‰∫Ü‰∏§ÁßçÂêàÊàêAIËØ≠Èü≥ÊúçÂä°ÔºàSpeechifyÂíåElevenLabsÔºâÔºåÈÄöËøáË∞ÉÊü•ÂíåËÆøË∞àÁöÑÊñπÊ≥ïÊù•ÂàÜÊûêÂÖ∂ÊäÄÊúØÊÄßËÉΩÔºåÂπ∂‰∫ÜËß£Áî®Êà∑ÁöÑÁîüÊ¥ªÁªèÂéÜÂ¶Ç‰ΩïÂΩ±Âìç‰ªñ‰ª¨ÂØπÂè£Èü≥ÂèòÂåñÁöÑÁúãÊ≥ï„ÄÇÁ†îÁ©∂ÂèëÁé∞Ôºå‰∏çÂêåÂú∞Âå∫ÁöÑËã±ËØ≠Âè£Èü≥Âú®ÊäÄÊúØÊÄßËÉΩ‰∏äÂ≠òÂú®Â∑ÆÂºÇÔºåËøôÂèØËÉΩÊó†ÊÑè‰∏≠Âä†Ââß‰∫ÜËØ≠Ë®ÄÁâπÊùÉÂíåÂü∫‰∫éÂè£Èü≥ÁöÑÊ≠ßËßÜÔºåÂØºËá¥Êñ∞ÁöÑÊï∞Â≠óÊéíÊñ•Áé∞Ë±°„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåÁ†îÁ©∂Âº∫Ë∞É‰∫ÜÂåÖÂÆπÊÄßËÆæËÆ°ÂíåÁõëÁÆ°ÁöÑÂøÖË¶ÅÊÄßÔºå‰∏∫ÂºÄÂèëËÄÖ„ÄÅÊîøÁ≠ñÂà∂ÂÆöËÄÖÂíåÁªÑÁªáÊèê‰æõ‰∫ÜÂèØË°åÁöÑËßÅËß£Ôºå‰ª•Á°Æ‰øùAIËØ≠Èü≥ÊäÄÊúØÁöÑÂÖ¨Âπ≥ÊÄßÂíåÁ§æ‰ºöË¥£‰ªª„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09048",
            "title": "BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via\n  Adaptive Block-Based Gaussian Splatting",
            "url": "https://huggingface.co/papers/2504.09048",
            "abstract": "The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated remarkable potential in novel view synthesis tasks. The divide-and-conquer paradigm has enabled large-scale scene reconstruction, but significant challenges remain in scene partitioning, optimization, and merging processes. This paper introduces BlockGaussian, a novel framework incorporating a content-aware scene partition strategy and visibility-aware block optimization to achieve efficient and high-quality large-scale scene reconstruction. Specifically, our approach considers the content-complexity variation across different regions and balances computational load during scene partitioning, enabling efficient scene reconstruction. To tackle the supervision mismatch issue during independent block optimization, we introduce auxiliary points during individual block optimization to align the ground-truth supervision, which enhances the reconstruction quality. Furthermore, we propose a pseudo-view geometry constraint that effectively mitigates rendering degradation caused by airspace floaters during block merging. Extensive experiments on large-scale scenes demonstrate that our approach achieves state-of-the-art performance in both reconstruction efficiency and rendering quality, with a 5x speedup in optimization and an average PSNR improvement of 1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces computational requirements, enabling large-scale scene reconstruction on a single 24GB VRAM device. The project page is available at https://github.com/SunshineWYC/BlockGaussian",
            "score": 1,
            "issue_id": 3291,
            "pub_date": "2025-04-12",
            "pub_date_card": {
                "ru": "12 –∞–ø—Ä–µ–ª—è",
                "en": "April 12",
                "zh": "4Êúà12Êó•"
            },
            "hash": "3f4a6ef28e699ddc",
            "authors": [
                "Yongchang Wu",
                "Zipeng Qi",
                "Zhenwei Shi",
                "Zhengxia Zou"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.09048.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark"
                ],
                "emoji": "üßä",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö —Å—Ü–µ–Ω —Å –ø–æ–º–æ—â—å—é –±–ª–æ—á–Ω–æ–≥–æ –≥–∞—É—Å—Å–æ–≤–∞ —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç BlockGaussian - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö —Å—Ü–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 3D –≥–∞—É—Å—Å–æ–≤–∞ —Å–ø–ª–∞—Ç—Ç–∏–Ω–≥–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å—Ü–µ–Ω—ã —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –±–ª–æ–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –≤–∏–¥–∏–º–æ—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –í–≤–µ–¥–µ–Ω–∏–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø—Å–µ–≤–¥–æ-–≤–∏–¥–æ–≤ –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ—à–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —Å–ª–∏—è–Ω–∏–∏ –±–ª–æ–∫–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç 5-–∫—Ä–∞—Ç–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —É–ª—É—á—à–µ–Ω–∏–µ PSNR –Ω–∞ 1.21 –¥–ë –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "Efficient Large-Scale Scene Reconstruction with BlockGaussian",
                    "desc": "This paper presents BlockGaussian, a new framework for improving large-scale scene reconstruction using 3D Gaussian Splatting. It introduces a content-aware partitioning strategy that considers the complexity of different scene regions, allowing for better optimization and merging of blocks. The framework also addresses supervision mismatch by using auxiliary points to align with ground-truth data, which enhances the overall reconstruction quality. Experimental results show that BlockGaussian achieves faster optimization and better rendering quality, making it feasible to perform large-scale reconstructions on devices with limited memory."
                },
                "zh": {
                    "title": "È´òÊïàÂ§ßËßÑÊ®°Âú∫ÊôØÈáçÂª∫ÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫BlockGaussianÁöÑÊñ∞Ê°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßËßÑÊ®°Âú∫ÊôØÈáçÂª∫ÁöÑÊïàÁéáÂíåË¥®Èáè„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®‰∫ÜÂÜÖÂÆπÊÑüÁü•ÁöÑÂú∫ÊôØÂàÜÂå∫Á≠ñÁï•ÂíåÂèØËßÅÊÄßÊÑüÁü•ÁöÑÂùó‰ºòÂåñÊñπÊ≥ïÔºå‰ª•Â∫îÂØπÂú∫ÊôØÂàÜÂå∫„ÄÅ‰ºòÂåñÂíåÂêàÂπ∂ËøáÁ®ã‰∏≠ÁöÑÊåëÊàò„ÄÇÈÄöËøáÂºïÂÖ•ËæÖÂä©ÁÇπÊù•Ëß£ÂÜ≥Áã¨Á´ãÂùó‰ºòÂåñ‰∏≠ÁöÑÁõëÁù£‰∏çÂåπÈÖçÈóÆÈ¢òÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÈáçÂª∫Ë¥®Èáè„ÄÇÊ≠§Â§ñÔºåÊèêÂá∫ÁöÑ‰º™ËßÜÂõæÂá†‰ΩïÁ∫¶ÊùüÊúâÊïàÂáèÂ∞ë‰∫ÜÂùóÂêàÂπ∂ËøáÁ®ã‰∏≠Âõ†Á©∫Ê∞îÊµÆÂä®ÈÄ†ÊàêÁöÑÊ∏≤ÊüìÈôçÁ∫ß„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-04-16.html",
    "link_next": "2025-04-18.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "16.04",
        "en": "04/16",
        "zh": "4Êúà16Êó•"
    },
    "short_date_next": {
        "ru": "18.04",
        "en": "04/18",
        "zh": "4Êúà18Êó•"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 8,
        "#agents": 0,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 3,
        "#audio": 2,
        "#video": 1,
        "#multimodal": 4,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "This article discusses the role of color in how humans and vision-language models (VLMs) perceive and understand the world. It introduces ColorBench, a new benchmark for testing VLMs' color understanding skills. The study finds that while larger models perform better, current VLMs generally neglect color understanding. It also shows that using Chain of Thought (CoT) reasoning can improve color task accuracy. The authors hope ColorBench will help advance research in this area.",
        "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
        "pinyin": "Sure, here is the pinyin transcription for the text:\n\nZh√® w√©nzhƒÅng t«éol√πn zh√® zh«íng y«éns√® z√†i r√©nl√®i h√© sh√¨y«én y«îy√°n m√≥x√≠ng (VLMs) zhƒ´ji√†n h√© l«êjiƒõ sh√¨ji√® zh≈çng de zu√≤y√≤ng. TƒÅ ji√®sh√†o Y«éns√® Bƒõnch, yƒ´g√® xƒ´n de bƒõnchmark y«ê c√®sh√¨ VLMs de y«éns√® l«êjiƒõ j√¨n√©ng. Y√°nji≈´ fƒÅxi√†n zh«êy«íu d√†x√≠ng m√≥x√≠ng bi«éoxi√†n g√®ng h«éo, dƒÅngqi√°n VLMs yƒ´bƒÅn sh≈´ hu«éng y«éns√® l«êjiƒõ. TƒÅ yƒõ shu≈çm√≠ng y√≤ng Li√°n de Sƒ´xi«éng (CoT) tu«êl«ê n√©ng g«éish√†n y«éns√® r√®nw√π de zh«înqu√®du. Zh√®xiƒì zu√≤zhƒõ xƒ´w√†ng Y«éns√® Bƒõnch hu√¨ bƒÅngzh√π tuƒ´j√¨n zh√® ge l«êngy√π de y√°nji≈´.\n\nPlease note that the pinyin transcription is based on the pronunciation of the Chinese characters that would be used to translate the English text. The actual translation might vary slightly depending on the context and specific word choices.",
        "vocab": "[\n    {\"word\": \"perceive\", \"pinyin\": \"p…ôrÀàsiÀêv\", \"trans\": \"ÊÑüÁü•\"},\n    {\"word\": \"vision-language models\", \"pinyin\": \"Ààv…™ í…ôn Ààl√¶≈ã…°w…™d í m…íd…ôlz\", \"trans\": \"ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã\"},\n    {\"word\": \"benchmark\", \"pinyin\": \"Ààb…õn ßm…ëÀêk\", \"trans\": \"Âü∫ÂáÜ\"},\n    {\"word\": \"neglect\", \"pinyin\": \"n…™Àà…°l…õkt\", \"trans\": \"ÂøΩËßÜ\"},\n    {\"word\": \"Chain of Thought\", \"pinyin\": \" ße…™n …ív Œ∏…îÀêt\", \"trans\": \"ÊÄùÁª¥Èìæ\"},\n    {\"word\": \"reasoning\", \"pinyin\": \"ÀàriÀêz(…ô)n…™≈ã\", \"trans\": \"Êé®ÁêÜ\"},\n    {\"word\": \"accuracy\", \"pinyin\": \"Àà√¶kj…ôr…ôsi\", \"trans\": \"ÂáÜÁ°ÆÊÄß\"},\n    {\"word\": \"advance\", \"pinyin\": \"√¶dÀàv…ëÀêns\", \"trans\": \"Êé®Ëøõ\"}\n]",
        "trans": "This article explores the role of color in how both humans and vision-language models (VLMs) perceive and understand the world. It introduces ColorBench, a new benchmark designed to evaluate VLMs' ability to understand color. The study reveals that while larger models tend to perform better, current VLMs generally overlook color understanding. Additionally, the research demonstrates that employing Chain of Thought (CoT) reasoning can enhance the accuracy of color-related tasks. The authors express hope that ColorBench will contribute to further advancements in this field of research.",
        "update_ts": "2025-04-17 09:12"
    }
}