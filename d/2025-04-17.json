{
    "date": {
        "ru": "17 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 17",
        "zh": "4æœˆ17æ—¥"
    },
    "time_utc": "2025-04-17 21:10",
    "weekday": 3,
    "issue_id": 3299,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.10514",
            "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
            "url": "https://huggingface.co/papers/2504.10514",
            "abstract": "Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.",
            "score": 29,
            "issue_id": 3281,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "c786e69f24be2f9e",
            "authors": [
                "Yijun Liang",
                "Ming Li",
                "Chenrui Fan",
                "Ziyue Li",
                "Dang Nguyen",
                "Kwesi Cobbina",
                "Shweta Bhardwaj",
                "Jiuhai Chen",
                "Fuxiao Liu",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10514.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸŒˆ",
                "ru": {
                    "title": "ColorBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ColorBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ†Ğ²ĞµÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 32 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ñ†Ğ²ĞµÑ‚Ğ°, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ†Ğ²ĞµÑ‚Ğ° Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ColorBench Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ†Ğ²ĞµÑ‚Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Enhancing AI's Color Comprehension with ColorBench",
                    "desc": "This paper presents ColorBench, a benchmark designed to evaluate how vision-language models (VLMs) understand and utilize color in visual reasoning. It assesses various aspects of color perception, reasoning, and robustness through a series of real-world scenarios. The study finds that while larger models generally perform better, the existing VLMs show limited capabilities in color understanding, indicating a gap in their training. Additionally, the research highlights that while VLMs can use color cues effectively, they can also be misled by them, emphasizing the need for improved color comprehension in AI models."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„é¢œè‰²ç†è§£èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ColorBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é¢œè‰²ç†è§£æ–¹é¢èƒ½åŠ›çš„åŸºå‡†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡æ›´å¤§çš„æ¨¡å‹åœ¨ColorBenchä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†è¯­è¨€æ¨¡å‹çš„ä½œç”¨æ¯”è§†è§‰ç¼–ç å™¨æ›´ä¸ºé‡è¦ã€‚ç°æœ‰çš„VLMsåœ¨é¢œè‰²ç†è§£æ–¹é¢çš„è¡¨ç°å·®è·è¾ƒå°ï¼Œè¡¨æ˜è¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†é‡è§†ã€‚æ­¤å¤–ï¼Œå°½ç®¡VLMsèƒ½å¤Ÿåˆ©ç”¨é¢œè‰²çº¿ç´¢ï¼Œä½†åœ¨æŸäº›ä»»åŠ¡ä¸­ä¹Ÿå¯èƒ½ä¼šå—åˆ°è¯¯å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12285",
            "title": "BitNet b1.58 2B4T Technical Report",
            "url": "https://huggingface.co/papers/2504.12285",
            "abstract": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.",
            "score": 27,
            "issue_id": 3281,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "cf67f70d9f122792",
            "authors": [
                "Shuming Ma",
                "Hongyu Wang",
                "Shaohan Huang",
                "Xingxing Zhang",
                "Ying Hu",
                "Ting Song",
                "Yan Xia",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12285.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#science",
                    "#architecture",
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸: 1-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ LLM Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ BitNet b1.58 2B4T - Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM) Ñ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 4 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ° Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ°, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. BitNet b1.58 2B4T Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… LLM Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ½Ğ¾ Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ· Hugging Face Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ GPU Ğ¸ CPU."
                },
                "en": {
                    "title": "Efficient Language Understanding with BitNet: The 1-Bit Revolution",
                    "desc": "BitNet b1.58 2B4T is a groundbreaking 1-bit Large Language Model (LLM) with 2 billion parameters, making it the first of its kind to be open-source. It has been trained on an extensive dataset of 4 trillion tokens and evaluated on various benchmarks, showcasing its capabilities in language understanding, mathematical reasoning, coding, and conversation. Remarkably, BitNet achieves performance comparable to other leading full-precision LLMs while being more efficient in terms of memory usage, energy consumption, and decoding speed. The model's weights and inference implementations are made available on Hugging Face, promoting further research and practical applications."
                },
                "zh": {
                    "title": "å¼€æºé«˜æ•ˆçš„1ä½å¤§å‹è¯­è¨€æ¨¡å‹",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†BitNet b1.58 2B4Tï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¼€æºçš„ã€åŸç”Ÿçš„1ä½å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡è¾¾åˆ°20äº¿ã€‚è¯¥æ¨¡å‹åœ¨4ä¸‡äº¿ä¸ªæ ‡è®°çš„è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨è¯­è¨€ç†è§£ã€æ•°å­¦æ¨ç†ã€ç¼–ç¨‹èƒ½åŠ›å’Œå¯¹è¯èƒ½åŠ›ç­‰åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒBitNet b1.58 2B4Tåœ¨æ€§èƒ½ä¸Šä¸åŒç±»è§„æ¨¡çš„é¢†å…ˆå¼€æºå…¨ç²¾åº¦å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬æ˜¾è‘—å‡å°‘çš„å†…å­˜å ç”¨ã€èƒ½è€—å’Œè§£ç å»¶è¿Ÿã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåº”ç”¨ï¼Œè¯¥æ¨¡å‹çš„æƒé‡é€šè¿‡Hugging Faceå‘å¸ƒï¼Œå¹¶æä¾›äº†é€‚ç”¨äºGPUå’ŒCPUæ¶æ„çš„å¼€æºæ¨ç†å®ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11536",
            "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs",
            "url": "https://huggingface.co/papers/2504.11536",
            "abstract": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems.",
            "score": 23,
            "issue_id": 3288,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "1402eef5411f1416",
            "authors": [
                "Jiazhan Feng",
                "Shijue Huang",
                "Xingwei Qu",
                "Ge Zhang",
                "Yujia Qin",
                "Baoquan Zhong",
                "Chengquan Jiang",
                "Jinxin Chi",
                "Wanjun Zhong"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.11536.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#rl",
                    "#math",
                    "#synthetic",
                    "#reasoning",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ReTool: Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ReTool, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ´Ğ°. ReTool Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ ĞºĞ°Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MATH Olympiad AIME Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ReTool Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "ReTool: Bridging Reasoning and Computation for Enhanced Learning",
                    "desc": "This paper introduces ReTool, a novel approach that enhances long-form reasoning in machine learning models by integrating real-time code execution into the reasoning process. It addresses the limitations of existing models in structured problem-solving tasks, such as geometric reasoning and complex computations, by employing a reinforcement learning framework that allows models to learn when and how to use computational tools effectively. ReTool's training involves generating synthetic data to fine-tune models and using task outcomes as rewards to improve tool invocation strategies. Experimental results show that ReTool significantly outperforms traditional text-based reinforcement learning models, demonstrating its potential for advancing complex mathematical reasoning and hybrid neuro-symbolic systems."
                },
                "zh": {
                    "title": "ReToolï¼šæå‡å¤æ‚æ¨ç†çš„å·¥å…·é›†æˆå­¦ä¹ ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºReToolçš„æ¨¡å‹ï¼Œæ—¨åœ¨æå‡é•¿æ–‡æœ¬æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡ ä½•æ¨ç†å’Œå¤æ‚æ–¹ç¨‹æ±‚è§£ç­‰ç»“æ„åŒ–é—®é¢˜ä¸Šã€‚ReToolç»“åˆäº†å®æ—¶ä»£ç æ‰§è¡Œä¸è‡ªç„¶è¯­è¨€æ¨ç†çš„åŠ¨æ€äº¤æ›¿ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è‡ªåŠ¨åŒ–ç­–ç•¥æ¥ä¼˜åŒ–å·¥å…·çš„ä½¿ç”¨ã€‚é€šè¿‡åˆæˆå†·å¯åŠ¨æ•°æ®ç”Ÿæˆä»£ç å¢å¼ºçš„æ¨ç†è½¨è¿¹ï¼ŒReToolåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­è°ƒæ•´æ¨¡å‹çš„å·¥å…·è°ƒç”¨ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReToolåœ¨MATHå¥¥æ—åŒ¹å…‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12240",
            "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
            "url": "https://huggingface.co/papers/2504.12240",
            "abstract": "The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.",
            "score": 17,
            "issue_id": 3280,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "a237e12792a9a0c8",
            "authors": [
                "Junhao Zhuang",
                "Lingen Li",
                "Xuan Ju",
                "Zhaoyang Zhang",
                "Chun Yuan",
                "Ying Shan"
            ],
            "affiliations": [
                "Tencent ARC Lab, China",
                "The Chinese University of Hong Kong, China",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12240.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#cv",
                    "#open_source",
                    "#architecture",
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Cobra: Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Cobra Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Causal Sparse DiT Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€Ğ°ÑĞºÑ€Ğ°ÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ 200 Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ². Cobra Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Cobra: Revolutionizing Line Art Colorization with Contextual Efficiency",
                    "desc": "This paper presents Cobra, a novel method for line art colorization in the comic production industry, which requires high accuracy and efficiency. Cobra utilizes a Causal Sparse DiT architecture that incorporates advanced techniques like causal sparse attention and positional encodings to handle over 200 reference images effectively. The method addresses challenges such as slow inference times and the need for flexible control, ensuring color identity consistency across diverse characters and backgrounds. Experimental results show that Cobra significantly improves the quality and speed of line art colorization, making it a valuable tool for artists."
                },
                "zh": {
                    "title": "Cobraï¼šé«˜æ•ˆçµæ´»çš„çº¿æ¡è‰ºæœ¯ä¸Šè‰²è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCobraçš„é«˜æ•ˆçº¿æ¡è‰ºæœ¯ä¸Šè‰²æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ¼«ç”»åˆ¶ä½œè¡Œä¸šä¸­å¯¹é«˜å‡†ç¡®æ€§å’Œçµæ´»æ§åˆ¶çš„éœ€æ±‚ã€‚Cobraèƒ½å¤Ÿå¤„ç†è¶…è¿‡200å¼ å‚è€ƒå›¾åƒï¼Œå¹¶ä¿æŒä½å»¶è¿Ÿï¼Œé€‚åº”å¤æ‚çš„è§’è‰²å’ŒèƒŒæ™¯ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†å› æœç¨€ç–DiTæ¶æ„ï¼Œåˆ©ç”¨ç‰¹æ®Šè®¾è®¡çš„ä½ç½®ç¼–ç å’Œå› æœç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆç®¡ç†é•¿ä¸Šä¸‹æ–‡å‚è€ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCobraåœ¨ä¸Šè‰²è´¨é‡å’Œæ¨ç†é€Ÿåº¦ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œæ»¡è¶³äº†å·¥ä¸šç•Œçš„å…³é”®éœ€æ±‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10326",
            "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
            "url": "https://huggingface.co/papers/2504.10326",
            "abstract": "AlayaDB is a cutting-edge vector database system natively architected for efficient and effective long-context inference for Large Language Models (LLMs) at AlayaDB AI. Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into a novel vector database system. For the Model as a Service providers (MaaS), AlayaDB consumes fewer hardware resources and offers higher generation quality for various workloads with different kinds of Service Level Objectives (SLOs), when comparing with the existing alternative solutions (e.g., KV cache disaggregation, retrieval-based sparse attention). The crux of AlayaDB is that it abstracts the attention computation and cache management for LLM inference into a query processing procedure, and optimizes the performance via a native query optimizer. In this work, we demonstrate the effectiveness of AlayaDB via (i) three use cases from our industry partners, and (ii) extensive experimental results on LLM inference benchmarks.",
            "score": 17,
            "issue_id": 3286,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "030a9ba5449806fb",
            "authors": [
                "Yangshen Deng",
                "Zhengxin You",
                "Long Xiang",
                "Qilong Li",
                "Peiqi Yuan",
                "Zhaoyang Hong",
                "Yitao Zheng",
                "Wanting Li",
                "Runzhong Li",
                "Haotian Liu",
                "Kyriakos Mouratidis",
                "Man Lung Yiu",
                "Huan Li",
                "Qiaomu Shen",
                "Rui Mao",
                "Bo Tang"
            ],
            "affiliations": [
                "AlayaDB AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10326.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#long_context",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "AlayaDB: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "AlayaDB - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ KV-ĞºÑÑˆ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM, Ğ¸Ğ½ĞºĞ°Ğ¿ÑÑƒĞ»Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ² Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. AlayaDB Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ½Ğ°Ğ³Ñ€ÑƒĞ·Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ AlayaDB ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ°Ğ±ÑÑ‚Ñ€Ğ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑÑˆĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° LLM Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing LLM Inference with AlayaDB",
                    "desc": "AlayaDB is an innovative vector database designed to enhance long-context inference for Large Language Models (LLMs). It separates key-value (KV) caching and attention computation from the LLM inference process, streamlining these functions into a dedicated database system. This architecture allows Model as a Service (MaaS) providers to use fewer hardware resources while achieving superior generation quality across various workloads. The system's core feature is its ability to treat attention computation and cache management as a query processing task, which is further optimized by a specialized query optimizer."
                },
                "zh": {
                    "title": "AlayaDBï¼šé«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡æ¨ç†è§£å†³æ–¹æ¡ˆ",
                    "desc": "AlayaDBæ˜¯ä¸€ç§å…ˆè¿›çš„å‘é‡æ•°æ®åº“ç³»ç»Ÿï¼Œä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®¾è®¡ï¼Œæ—¨åœ¨æé«˜é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ•ˆç‡å’Œæ•ˆæœã€‚å®ƒå°†é”®å€¼ç¼“å­˜å’Œæ³¨æ„åŠ›è®¡ç®—ä»LLMæ¨ç†ç³»ç»Ÿä¸­è§£è€¦ï¼Œå¹¶å°†å…¶å°è£…åˆ°ä¸€ä¸ªæ–°é¢–çš„å‘é‡æ•°æ®åº“ç³»ç»Ÿä¸­ã€‚ä¸ç°æœ‰è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒAlayaDBåœ¨ç¡¬ä»¶èµ„æºæ¶ˆè€—å’Œç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œé€‚ç”¨äºä¸åŒæœåŠ¡æ°´å¹³ç›®æ ‡ï¼ˆSLOï¼‰çš„å¤šç§å·¥ä½œè´Ÿè½½ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†æ³¨æ„åŠ›è®¡ç®—å’Œç¼“å­˜ç®¡ç†æŠ½è±¡ä¸ºæŸ¥è¯¢å¤„ç†è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡æœ¬åœ°æŸ¥è¯¢ä¼˜åŒ–å™¨ä¼˜åŒ–æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09081",
            "title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction\n  Fine-Tuning",
            "url": "https://huggingface.co/papers/2504.09081",
            "abstract": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing a diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, a benchmark dataset specifically designed to evaluate the instruction-following capabilities of speech-text LLMs.",
            "score": 12,
            "issue_id": 3287,
            "pub_date": "2025-04-12",
            "pub_date_card": {
                "ru": "12 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 12",
                "zh": "4æœˆ12æ—¥"
            },
            "hash": "4c17c8d0a36c365d",
            "authors": [
                "Prabhat Pandey",
                "Rupak Vignesh Swaminathan",
                "K V Vijay Girish",
                "Arunasish Sen",
                "Jian Xie",
                "Grant P. Strimel",
                "Andreas Schwarz"
            ],
            "affiliations": [
                "Amazon AGI",
                "Apple Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09081.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#audio",
                    "#transfer_learning",
                    "#multilingual",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "SIFT: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "SIFT (Speech Instruction Fine-Tuning) - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 50 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ñ€ĞµÑ‡ÑŒÑ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ SIFT-LLM, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¯Ğœ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ EvalSIFT - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¯Ğœ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼."
                },
                "en": {
                    "title": "Empowering Speech Models with SIFT: Fine-Tuning for Instruction Mastery",
                    "desc": "The paper presents SIFT (Speech Instruction Fine-Tuning), a large dataset containing 50 million examples for enhancing speech-text large language models (LLMs). It is constructed from 14,000 hours of publicly available speech data across five languages, focusing on both speech understanding and generation tasks. The authors train a model called SIFT-LLM, which shows superior performance on instruction-following benchmarks compared to existing models, while also being competitive in foundational speech tasks. Additionally, they introduce EvalSIFT, a specialized benchmark for assessing the instruction-following abilities of speech-text LLMs."
                },
                "zh": {
                    "title": "SIFTï¼šæå‡è¯­éŸ³æŒ‡ä»¤ç†è§£çš„åˆ›æ–°æ•°æ®é›†",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†SIFTï¼ˆè¯­éŸ³æŒ‡ä»¤å¾®è°ƒï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«5000ä¸‡æ¡ç¤ºä¾‹çš„æ•°æ®é›†ï¼Œæ—¨åœ¨ç”¨äºè¯­éŸ³-æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒå’Œé¢„è®­ç»ƒã€‚SIFT-50MåŸºäºå…¬å¼€çš„è¯­éŸ³è¯­æ–™åº“æ„å»ºï¼ŒåŒ…å«14000å°æ—¶çš„è¯­éŸ³ï¼Œåˆ©ç”¨äº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œç°æˆçš„ä¸“å®¶æ¨¡å‹ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº”ç§è¯­è¨€ï¼ŒåŒ…å«å¤šæ ·çš„è¯­éŸ³ç†è§£å’Œå¯æ§çš„è¯­éŸ³ç”ŸæˆæŒ‡ä»¤ã€‚ä½¿ç”¨SIFT-50Mï¼Œæˆ‘ä»¬è®­ç»ƒäº†SIFT-LLMï¼Œå…¶åœ¨æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„è¯­éŸ³-æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶åœ¨åŸºç¡€è¯­éŸ³ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10483",
            "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion\n  Transformers",
            "url": "https://huggingface.co/papers/2504.10483",
            "abstract": "In this paper we tackle a fundamental question: \"Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?\" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, we observe that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, our approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256 x 256. Code is available at https://end2end-diffusion.github.io.",
            "score": 9,
            "issue_id": 3285,
            "pub_date": "2025-04-14",
            "pub_date_card": {
                "ru": "14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 14",
                "zh": "4æœˆ14æ—¥"
            },
            "hash": "f46935088154b083",
            "authors": [
                "Xingjian Leng",
                "Jaskirat Singh",
                "Yunzhong Hou",
                "Zhenchang Xing",
                "Saining Xie",
                "Liang Zheng"
            ],
            "affiliations": [
                "Australian National University",
                "Data61 CSIRO",
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10483.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ VAE",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° (VAE) Ğ² ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ»Ñ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ (REPA). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ (REPA-E) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° VAE. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FID Ğ½Ğ° ImageNet 256x256."
                },
                "en": {
                    "title": "Unlocking End-to-End Training with REPA for Latent Diffusion Models",
                    "desc": "This paper explores the possibility of training latent diffusion models alongside a variational auto-encoder (VAE) in an end-to-end fashion. It highlights that traditional methods using standard diffusion loss are ineffective and can even harm performance. The authors introduce a new loss function called representation-alignment (REPA) loss, which enables effective joint training of the VAE and diffusion model. Their proposed training method, REPA-E, significantly accelerates training and enhances the quality of generated outputs, achieving state-of-the-art results on ImageNet."
                },
                "zh": {
                    "title": "ç«¯åˆ°ç«¯è®­ç»ƒçš„çªç ´ï¼šREPAæŸå¤±çš„åŠ›é‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šèƒ½å¦å°†æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ ‡è®°å™¨ä¸€èµ·è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Ÿä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ ç†è®ºè®¤ä¸ºï¼Œå°½å¯èƒ½è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒæ˜¯æ›´ä¼˜çš„é€‰æ‹©ã€‚ç„¶è€Œï¼Œå¯¹äºæ½œåœ¨æ‰©æ•£å˜æ¢å™¨ï¼Œä½¿ç”¨æ ‡å‡†æ‰©æ•£æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒä¼šå¯¼è‡´æ•ˆæœä¸ä½³ï¼Œç”šè‡³é™ä½æœ€ç»ˆæ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†é€šè¿‡è¡¨ç¤ºå¯¹é½æŸå¤±ï¼ˆREPAæŸå¤±ï¼‰æ¥è§£é”ç«¯åˆ°ç«¯è®­ç»ƒï¼Œä½¿å¾—VAEå’Œæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å…±åŒè°ƒæ•´ï¼Œæœ€ç»ˆå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11468",
            "title": "SFT or RL? An Early Investigation into Training R1-Like Reasoning Large\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2504.11468",
            "abstract": "This work revisits the dominant supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm for training Large Vision-Language Models (LVLMs), and reveals a key finding: SFT can significantly undermine subsequent RL by inducing ``pseudo reasoning paths'' imitated from expert models. While these paths may resemble the native reasoning paths of RL models, they often involve prolonged, hesitant, less informative steps, and incorrect reasoning. To systematically study this effect, we introduce VLAA-Thinking, a new multimodal dataset designed to support reasoning in LVLMs. Constructed via a six-step pipeline involving captioning, reasoning distillation, answer rewrite and verification, VLAA-Thinking comprises high-quality, step-by-step visual reasoning traces for SFT, along with a more challenging RL split from the same data source. Using this dataset, we conduct extensive experiments comparing SFT, RL and their combinations. Results show that while SFT helps models learn reasoning formats, it often locks aligned models into imitative, rigid reasoning modes that impede further learning. In contrast, building on the Group Relative Policy Optimization (GRPO) with a novel mixed reward module integrating both perception and cognition signals, our RL approach fosters more genuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on Qwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard (https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard) among 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope our findings provide valuable insights in developing reasoning-capable LVLMs and can inform future research in this area.",
            "score": 7,
            "issue_id": 3294,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "550fb98da92065ed",
            "authors": [
                "Hardy Chen",
                "Haoqin Tu",
                "Fali Wang",
                "Hui Liu",
                "Xianfeng Tang",
                "Xinya Du",
                "Yuyin Zhou",
                "Cihang Xie"
            ],
            "affiliations": [
                "Amazon Research",
                "The Pennsylvania State University",
                "University of California, Santa Cruz",
                "University of Texas at Dallas"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11468.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#rl",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT), Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL), Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SFT Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ 'Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼ĞµÑˆĞ°ÑÑ‚ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL. Ğ”Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VLAA-Thinking, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RL Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Group Relative Policy Optimization (GRPO) Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¸Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Genuine Reasoning in LVLMs",
                    "desc": "This paper examines the training process of Large Vision-Language Models (LVLMs) using supervised fine-tuning (SFT) followed by reinforcement learning (RL). It finds that SFT can create misleading reasoning patterns that hinder the effectiveness of subsequent RL, leading to less informative and incorrect reasoning. To address this, the authors introduce VLAA-Thinking, a new dataset that supports better reasoning in LVLMs through a structured six-step process. Their experiments show that while SFT can help with learning reasoning formats, it can also restrict models to rigid reasoning, whereas their RL approach encourages more flexible and adaptive reasoning capabilities."
                },
                "zh": {
                    "title": "æ‰“ç ´æ¨¡ä»¿ï¼Œä¿ƒè¿›çœŸå®æ¨ç†çš„LVLMè®­ç»ƒ",
                    "desc": "æœ¬æ–‡é‡æ–°å®¡è§†äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒèŒƒå¼ï¼Œå‘ç°SFTå¯èƒ½ä¼šé€šè¿‡å¼•å…¥â€œä¼ªæ¨ç†è·¯å¾„â€æ¥æ˜¾è‘—å‰Šå¼±åç»­çš„RLã€‚è¿™äº›ä¼ªè·¯å¾„è™½ç„¶çœ‹ä¼¼ä¸RLæ¨¡å‹çš„åŸç”Ÿæ¨ç†è·¯å¾„ç›¸ä¼¼ï¼Œä½†å¾€å¾€æ¶‰åŠå†—é•¿ã€çŠ¹è±«ä¸”ä¿¡æ¯é‡ä¸è¶³çš„æ­¥éª¤ï¼Œç”šè‡³é”™è¯¯æ¨ç†ã€‚ä¸ºç³»ç»Ÿç ”ç©¶è¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†VLAA-Thinkingï¼Œä¸€ä¸ªæ–°å‹çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒLVLMsçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å…­æ­¥æµç¨‹æ„å»ºçš„VLAA-Thinkingæä¾›äº†é«˜è´¨é‡çš„é€æ­¥è§†è§‰æ¨ç†è½¨è¿¹ï¼Œå¹¶é€šè¿‡å®éªŒè¡¨æ˜ï¼ŒSFTè™½ç„¶æœ‰åŠ©äºæ¨¡å‹å­¦ä¹ æ¨ç†æ ¼å¼ï¼Œä½†ä¼šä½¿æ¨¡å‹é™·å…¥æ¨¡ä»¿å’ŒåƒµåŒ–çš„æ¨ç†æ¨¡å¼ï¼Œå¦¨ç¢è¿›ä¸€æ­¥å­¦ä¹ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12264",
            "title": "Towards Learning to Complete Anything in Lidar",
            "url": "https://huggingface.co/papers/2504.12264",
            "abstract": "We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion in-the-wild. This is closely related to Lidar-based semantic/panoptic scene completion. However, contemporary methods can only complete and recognize objects from a closed vocabulary labeled in existing Lidar datasets. Different to that, our zero-shot approach leverages the temporal context from multi-modal sensor sequences to mine object shapes and semantic features of observed objects. These are then distilled into a Lidar-only instance-level completion and recognition model. Although we only mine partial shape completions, we find that our distilled model learns to infer full object shapes from multiple such partial observations across the dataset. We show that our model can be prompted on standard benchmarks for Semantic and Panoptic Scene Completion, localize objects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class vocabularies. Our project page is https://research.nvidia.com/labs/dvl/projects/complete-anything-lidar",
            "score": 4,
            "issue_id": 3292,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "171045e898ed54a7",
            "authors": [
                "Ayca Takmaz",
                "Cristiano Saltori",
                "Neehar Peri",
                "Tim Meinhardt",
                "Riccardo de Lutio",
                "Laura Leal-TaixÃ©",
                "AljoÅ¡a OÅ¡ep"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "ETH Zurich",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12264.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ»Ğ¸Ğ´Ğ°Ñ€Ğ°",
                    "desc": "CAL (Complete Anything in Lidar) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ¸Ğ´Ğ°Ñ€Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², CAL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ğ´Ğ°Ñ€Ğ°. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ÑÑ Ğ»Ğ¸ÑˆÑŒ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Complete Any Object with Lidar: Beyond Fixed Classes!",
                    "desc": "The paper introduces CAL (Complete Anything in Lidar), a novel approach for completing shapes using Lidar data in real-world scenarios. Unlike traditional methods that rely on a fixed set of labeled objects, CAL employs a zero-shot learning technique that utilizes temporal context from multi-modal sensor sequences to extract object shapes and semantic features. This information is then distilled into a model that can complete and recognize objects at the instance level, even when only partial shapes are available. The results demonstrate that CAL can effectively infer full object shapes and perform well on standard benchmarks for scene completion and object localization, extending recognition beyond predefined class categories."
                },
                "zh": {
                    "title": "æ¿€å…‰é›·è¾¾ä¸­çš„å½¢çŠ¶è¡¥å…¨æ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬æå‡ºäº†CALï¼ˆåœ¨æ¿€å…‰é›·è¾¾ä¸­å®Œæˆä»»ä½•å½¢çŠ¶ï¼‰ï¼Œç”¨äºåœ¨å®é™…ç¯å¢ƒä¸­è¿›è¡Œæ¿€å…‰é›·è¾¾åŸºç¡€çš„å½¢çŠ¶è¡¥å…¨ã€‚è¿™ç§æ–¹æ³•ä¸æ¿€å…‰é›·è¾¾åŸºç¡€çš„è¯­ä¹‰/å…¨æ™¯åœºæ™¯è¡¥å…¨å¯†åˆ‡ç›¸å…³ï¼Œä½†ç°æœ‰æ–¹æ³•åªèƒ½åœ¨å›ºå®šçš„æ ‡ç­¾è¯æ±‡ä¸­å®Œæˆå’Œè¯†åˆ«å¯¹è±¡ã€‚æˆ‘ä»¬çš„é›¶æ ·æœ¬æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€ä¼ æ„Ÿå™¨åºåˆ—ä¸­çš„æ—¶é—´ä¸Šä¸‹æ–‡ï¼ŒæŒ–æ˜è§‚å¯Ÿåˆ°å¯¹è±¡çš„å½¢çŠ¶å’Œè¯­ä¹‰ç‰¹å¾ï¼Œå¹¶å°†å…¶æç‚¼ä¸ºä»…åŸºäºæ¿€å…‰é›·è¾¾çš„å®ä¾‹çº§è¡¥å…¨å’Œè¯†åˆ«æ¨¡å‹ã€‚å°½ç®¡æˆ‘ä»¬åªæŒ–æ˜éƒ¨åˆ†å½¢çŠ¶è¡¥å…¨ï¼Œä½†æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿä»å¤šä¸ªéƒ¨åˆ†è§‚å¯Ÿä¸­æ¨æ–­å‡ºå®Œæ•´çš„å¯¹è±¡å½¢çŠ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11952",
            "title": "Robust and Fine-Grained Detection of AI Generated Texts",
            "url": "https://huggingface.co/papers/2504.11952",
            "abstract": "An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.",
            "score": 4,
            "issue_id": 3280,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "bdea465fe17b9401",
            "authors": [
                "Ram Mohan Rao Kadiyala",
                "Siddartha Pullakhandam",
                "Kanwal Mehreen",
                "Drishti Sharma",
                "Siddhant Gupta",
                "Jebish Purbey",
                "Ashay Srivastava",
                "Subhasya TippaReddy",
                "Arvind Reddy Bobbili",
                "Suraj Telugara Chandrashekhar",
                "Modabbir Adeeb",
                "Srinadh Vura",
                "Hamza Farooq"
            ],
            "affiliations": [
                "Cohere for AI Community",
                "IISc Bangalore",
                "IIT Roorkee",
                "M2ai.in",
                "Pulchowk Campus",
                "Stanford University",
                "Traversaal.ai",
                "University of California, Los Angeles",
                "University of Houston",
                "University of Maryland, College Park",
                "University of South Florida",
                "Vantager"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11952.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#hallucinations",
                    "#dataset",
                    "#benchmark",
                    "#multilingual",
                    "#security",
                    "#data"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ˜Ğ˜-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ˜Ğ˜. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ¸Ğ· Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, Ğ¾Ñ‚ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 2,4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° 23 ÑĞ·Ñ‹ĞºĞ°Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ¾Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğµ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Advancing Detection of Human-LLM Co-Authored Texts",
                    "desc": "This paper addresses the challenge of detecting machine-generated content, particularly in cases where texts are co-authored by humans and language models (LLMs). The authors developed a set of token classification models trained on a large dataset of 2.4 million co-authored texts, which allows for better detection across various domains and generators. The models demonstrated strong performance even with adversarial inputs and texts from non-native speakers. Additionally, the paper provides insights into how the models perform based on text length and characteristics compared to purely human-authored content."
                },
                "zh": {
                    "title": "æ„å»ºé«˜æ•ˆçš„æœºå™¨ç”Ÿæˆå†…å®¹æ£€æµ‹ç³»ç»Ÿ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç†æƒ³çš„æ£€æµ‹ç³»ç»Ÿï¼Œæ—¨åœ¨æœ‰æ•ˆè¯†åˆ«æœºå™¨ç”Ÿæˆçš„å†…å®¹ï¼Œå°¤å…¶æ˜¯åœ¨çŸ­æ–‡æœ¬ä¸­ã€‚ç°æœ‰ç³»ç»Ÿåœ¨è¯†åˆ«AIç”Ÿæˆå†…å®¹æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå› æ­¤æˆ‘ä»¬ä¸“æ³¨äºäººç±»ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…±åŒåˆ›ä½œçš„æ–‡æœ¬ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ç³»åˆ—ç”¨äºæ ‡è®°åˆ†ç±»çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤§é‡äººæœºå…±åˆ›æ–‡æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æœªè§é¢†åŸŸå’Œç”Ÿæˆå™¨çš„æ–‡æœ¬ä¸Šè¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåŒ…å«240ä¸‡æ¡æ–‡æœ¬çš„æ–°æ•°æ®é›†ï¼Œä¸»è¦ç”±å¤šç§æµè¡Œçš„ä¸“æœ‰LLMå…±åŒåˆ›ä½œï¼Œæ¶µç›–23ç§è¯­è¨€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11092",
            "title": "Vivid4D: Improving 4D Reconstruction from Monocular Video by Video\n  Inpainting",
            "url": "https://huggingface.co/papers/2504.11092",
            "abstract": "Reconstructing 4D dynamic scenes from casually captured monocular videos is valuable but highly challenging, as each timestamp is observed from a single viewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular video synthesis by augmenting observation views - synthesizing multi-view videos from a monocular input. Unlike existing methods that either solely leverage geometric priors for supervision or use generative priors while overlooking geometry, we integrate both. This reformulates view augmentation as a video inpainting task, where observed views are warped into new viewpoints based on monocular depth priors. To achieve this, we train a video inpainting model on unposed web videos with synthetically generated masks that mimic warping occlusions, ensuring spatially and temporally consistent completion of missing regions. To further mitigate inaccuracies in monocular depth priors, we introduce an iterative view augmentation strategy and a robust reconstruction loss. Experiments demonstrate that our method effectively improves monocular 4D scene reconstruction and completion.",
            "score": 4,
            "issue_id": 3287,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "a6e019e03ced5592",
            "authors": [
                "Jiaxin Huang",
                "Sheng Miao",
                "BangBnag Yang",
                "Yuewen Ma",
                "Yiyi Liao"
            ],
            "affiliations": [
                "ByteDance PICO",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11092.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Vivid4D: Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Vivid4D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ 4D-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ augmentĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ½ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ augmentĞ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Enhancing 4D Scene Reconstruction with Vivid4D",
                    "desc": "The paper presents Vivid4D, a new method for reconstructing 4D dynamic scenes from single-view videos. It synthesizes multi-view videos by augmenting the observation views, addressing the challenges of limited viewpoints. The approach combines geometric and generative priors, treating view augmentation as a video inpainting task to fill in missing regions. By training on unposed web videos and using iterative strategies, Vivid4D enhances the accuracy of monocular depth priors and improves scene reconstruction quality."
                },
                "zh": {
                    "title": "Vivid4Dï¼šä»å•ç›®è§†é¢‘é‡å»ºå››ç»´åŠ¨æ€åœºæ™¯çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVivid4Dçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å•ç›®è§†é¢‘ä¸­é‡å»ºå››ç»´åŠ¨æ€åœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆæˆå¤šè§†è§’è§†é¢‘æ¥å¢å¼ºå•ç›®è§†é¢‘åˆæˆï¼Œå…‹æœäº†ä»…ä»å•ä¸€è§†è§’è§‚å¯Ÿçš„é™åˆ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒVivid4DåŒæ—¶åˆ©ç”¨å‡ ä½•å…ˆéªŒå’Œç”Ÿæˆå…ˆéªŒï¼Œå°†è§†è§’å¢å¼ºé‡æ–°å®šä¹‰ä¸ºè§†é¢‘ä¿®å¤ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆæé«˜äº†å•ç›®å››ç»´åœºæ™¯çš„é‡å»ºå’Œè¡¥å…¨æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09566",
            "title": "Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution",
            "url": "https://huggingface.co/papers/2504.09566",
            "abstract": "Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors. However, complex tasks with vast solution spaces and vague constraints often exceed the capacity of a single reasoning chain. Inspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic geometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends CoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper logical dependencies, enabling more robust and structured problem-solving. MFR decomposes a module into a sequence of free modules with minimal rank, providing a structured analytical approach to complex systems. This method introduces the concepts of \"Module\", \"Betti numbers\",\"Freeness\", \"Mapping\", \"Exactness\" and \"Minimality\", enabling the systematic decomposition of the original complex problem into logically complete minimal subproblems while preserving key problem features and reducing reasoning length. We tested SoT across diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini, Qwen2.5), achieving inference accuracy that matches or surpasses mainstream CoTs standards. Additionally, by aligning the sampling process with algebraic constraints, our approach enhances the scalability of inference time in LLMs, ensuring both transparent reasoning and high performance. Our code will be publicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.",
            "score": 1,
            "issue_id": 3290,
            "pub_date": "2025-04-13",
            "pub_date_card": {
                "ru": "13 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 13",
                "zh": "4æœˆ13æ—¥"
            },
            "hash": "24032586ed61676f",
            "authors": [
                "Chenghao Li",
                "Chaoning Zhang",
                "Yi Lu",
                "Jiaquan Zhang",
                "Qigan Sun",
                "Xudong Wang",
                "Jiwei Wei",
                "Guoqing Wang",
                "Yang Yang",
                "Heng Tao Shen"
            ],
            "affiliations": [
                "Capital Normal University, Beijing, China",
                "Kyung Hee University, Yongin-si, Republic of Korea",
                "Tongji University, Shanghai, China",
                "University of Electronic Science and Technology of China, Chengdu, China",
                "University of Liverpool, Liverpool, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09566.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#inference",
                    "#reasoning",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Syzygy of Thoughts: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Syzygy of Thoughts (SoT), Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‰Ğ¸Ğ¹ Chain-of-Thought (CoT) Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. SoT Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ĞµĞ¹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¼ÑƒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ»Ğ³ĞµĞ±Ñ€Ñ‹ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ SoT Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ CoT."
                },
                "en": {
                    "title": "Enhancing Reasoning with Syzygy of Thoughts",
                    "desc": "This paper introduces a new framework called Syzygy of Thoughts (SoT) that builds on Chain-of-Thought (CoT) prompting to improve the reasoning capabilities of large language models (LLMs). SoT addresses the limitations of single reasoning chains by incorporating multiple interrelated reasoning paths, which helps in tackling complex tasks with broad solution spaces. By drawing inspiration from concepts in commutative algebra, such as Minimal Free Resolution, SoT systematically breaks down complex problems into simpler, manageable subproblems while maintaining essential features. The framework has been tested on various datasets and models, demonstrating improved inference accuracy and efficiency compared to traditional CoT methods."
                },
                "zh": {
                    "title": "æ€ç»´çš„Syzygyï¼šæå‡æ¨ç†èƒ½åŠ›çš„æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ€ç»´çš„Syzygyï¼ˆSoTï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚SoTé€šè¿‡å¼•å…¥è¾…åŠ©çš„ã€ç›¸äº’å…³è”çš„æ¨ç†è·¯å¾„ï¼Œæ‰©å±•äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†äº¤æ¢ä»£æ•°å’Œä»£æ•°å‡ ä½•ä¸­çš„æœ€å°è‡ªç”±åˆ†è§£ï¼ˆMFRï¼‰ï¼Œé€šè¿‡ç³»ç»Ÿåœ°å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºé€»è¾‘ä¸Šå®Œæ•´çš„æœ€å°å­é—®é¢˜ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSoTåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ¨ç†å‡†ç¡®ç‡è¾¾åˆ°æˆ–è¶…è¿‡äº†ä¸»æµçš„é“¾å¼æ€ç»´æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09346",
            "title": "\"It's not a representation of me\": Examining Accent Bias and Digital\n  Exclusion in Synthetic AI Voice Services",
            "url": "https://huggingface.co/papers/2504.09346",
            "abstract": "Recent advances in artificial intelligence (AI) speech generation and voice cloning technologies have produced naturalistic speech and accurate voice replication, yet their influence on sociotechnical systems across diverse accents and linguistic traits is not fully understood. This study evaluates two synthetic AI voice services (Speechify and ElevenLabs) through a mixed methods approach using surveys and interviews to assess technical performance and uncover how users' lived experiences influence their perceptions of accent variations in these speech technologies. Our findings reveal technical performance disparities across five regional, English-language accents and demonstrate how current speech generation technologies may inadvertently reinforce linguistic privilege and accent-based discrimination, potentially creating new forms of digital exclusion. Overall, our study highlights the need for inclusive design and regulation by providing actionable insights for developers, policymakers, and organizations to ensure equitable and socially responsible AI speech technologies.",
            "score": 1,
            "issue_id": 3294,
            "pub_date": "2025-04-12",
            "pub_date_card": {
                "ru": "12 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 12",
                "zh": "4æœˆ12æ—¥"
            },
            "hash": "c5df8b667242e0e1",
            "authors": [
                "Shira Michel",
                "Sufi Kaur",
                "Sarah Elizabeth Gillespie",
                "Jeffrey Gleason",
                "Christo Wilson",
                "Avijit Ghosh"
            ],
            "affiliations": [
                "Hugging Face and University of Connecticut, USA",
                "Northeastern University, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.09346.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#healthcare",
                    "#ethics",
                    "#synthetic",
                    "#audio"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ˜Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ² Ğ˜Ğ˜-Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑÑ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ÑĞµÑ€Ğ²Ğ¸ÑĞ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ˜Ğ˜ (Speechify Ğ¸ ElevenLabs) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ. ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ‚Ğ¸Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑÑ…. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿ÑÑ‚Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ñƒ."
                },
                "en": {
                    "title": "Ensuring Fairness in AI Speech: Addressing Accent Bias and Digital Exclusion",
                    "desc": "This paper investigates the impact of AI speech generation and voice cloning technologies on social systems, focusing on how different accents are represented. It evaluates two AI voice services, Speechify and ElevenLabs, using surveys and interviews to understand user experiences and perceptions of accent variations. The study finds that there are significant differences in technical performance across various English accents, which may lead to accent-based discrimination and reinforce existing linguistic privileges. The authors emphasize the importance of inclusive design and regulation to promote fairness and accessibility in AI speech technologies."
                },
                "zh": {
                    "title": "ç¡®ä¿AIè¯­éŸ³æŠ€æœ¯çš„å…¬å¹³ä¸åŒ…å®¹",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½è¯­éŸ³ç”Ÿæˆå’Œå£°éŸ³å…‹éš†æŠ€æœ¯å¯¹ç¤¾ä¼šæŠ€æœ¯ç³»ç»Ÿçš„å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒå£éŸ³å’Œè¯­è¨€ç‰¹å¾æ–¹é¢ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§åˆæˆAIè¯­éŸ³æœåŠ¡ï¼ˆSpeechifyå’ŒElevenLabsï¼‰ï¼Œé€šè¿‡è°ƒæŸ¥å’Œè®¿è°ˆçš„æ–¹æ³•æ¥åˆ†æå…¶æŠ€æœ¯æ€§èƒ½ï¼Œå¹¶äº†è§£ç”¨æˆ·çš„ç”Ÿæ´»ç»å†å¦‚ä½•å½±å“ä»–ä»¬å¯¹å£éŸ³å˜åŒ–çš„çœ‹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒåœ°åŒºçš„è‹±è¯­å£éŸ³åœ¨æŠ€æœ¯æ€§èƒ½ä¸Šå­˜åœ¨å·®å¼‚ï¼Œè¿™å¯èƒ½æ— æ„ä¸­åŠ å‰§äº†è¯­è¨€ç‰¹æƒå’ŒåŸºäºå£éŸ³çš„æ­§è§†ï¼Œå¯¼è‡´æ–°çš„æ•°å­—æ’æ–¥ç°è±¡ã€‚æ€»ä½“è€Œè¨€ï¼Œç ”ç©¶å¼ºè°ƒäº†åŒ…å®¹æ€§è®¾è®¡å’Œç›‘ç®¡çš„å¿…è¦æ€§ï¼Œä¸ºå¼€å‘è€…ã€æ”¿ç­–åˆ¶å®šè€…å’Œç»„ç»‡æä¾›äº†å¯è¡Œçš„è§è§£ï¼Œä»¥ç¡®ä¿AIè¯­éŸ³æŠ€æœ¯çš„å…¬å¹³æ€§å’Œç¤¾ä¼šè´£ä»»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.09048",
            "title": "BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via\n  Adaptive Block-Based Gaussian Splatting",
            "url": "https://huggingface.co/papers/2504.09048",
            "abstract": "The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated remarkable potential in novel view synthesis tasks. The divide-and-conquer paradigm has enabled large-scale scene reconstruction, but significant challenges remain in scene partitioning, optimization, and merging processes. This paper introduces BlockGaussian, a novel framework incorporating a content-aware scene partition strategy and visibility-aware block optimization to achieve efficient and high-quality large-scale scene reconstruction. Specifically, our approach considers the content-complexity variation across different regions and balances computational load during scene partitioning, enabling efficient scene reconstruction. To tackle the supervision mismatch issue during independent block optimization, we introduce auxiliary points during individual block optimization to align the ground-truth supervision, which enhances the reconstruction quality. Furthermore, we propose a pseudo-view geometry constraint that effectively mitigates rendering degradation caused by airspace floaters during block merging. Extensive experiments on large-scale scenes demonstrate that our approach achieves state-of-the-art performance in both reconstruction efficiency and rendering quality, with a 5x speedup in optimization and an average PSNR improvement of 1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces computational requirements, enabling large-scale scene reconstruction on a single 24GB VRAM device. The project page is available at https://github.com/SunshineWYC/BlockGaussian",
            "score": 1,
            "issue_id": 3291,
            "pub_date": "2025-04-12",
            "pub_date_card": {
                "ru": "12 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 12",
                "zh": "4æœˆ12æ—¥"
            },
            "hash": "3f4a6ef28e699ddc",
            "authors": [
                "Yongchang Wu",
                "Zipeng Qi",
                "Zhenwei Shi",
                "Zhengxia Zou"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.09048.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ° ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BlockGaussian - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ° ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 5-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ PSNR Ğ½Ğ° 1.21 Ğ´Ğ‘ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Efficient Large-Scale Scene Reconstruction with BlockGaussian",
                    "desc": "This paper presents BlockGaussian, a new framework for improving large-scale scene reconstruction using 3D Gaussian Splatting. It introduces a content-aware partitioning strategy that considers the complexity of different scene regions, allowing for better optimization and merging of blocks. The framework also addresses supervision mismatch by using auxiliary points to align with ground-truth data, which enhances the overall reconstruction quality. Experimental results show that BlockGaussian achieves faster optimization and better rendering quality, making it feasible to perform large-scale reconstructions on devices with limited memory."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¤§è§„æ¨¡åœºæ™¯é‡å»ºçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBlockGaussiançš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡åœºæ™¯é‡å»ºçš„æ•ˆç‡å’Œè´¨é‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†å†…å®¹æ„ŸçŸ¥çš„åœºæ™¯åˆ†åŒºç­–ç•¥å’Œå¯è§æ€§æ„ŸçŸ¥çš„å—ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥åº”å¯¹åœºæ™¯åˆ†åŒºã€ä¼˜åŒ–å’Œåˆå¹¶è¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¼•å…¥è¾…åŠ©ç‚¹æ¥è§£å†³ç‹¬ç«‹å—ä¼˜åŒ–ä¸­çš„ç›‘ç£ä¸åŒ¹é…é—®é¢˜ï¼Œè¿›ä¸€æ­¥æå‡äº†é‡å»ºè´¨é‡ã€‚æ­¤å¤–ï¼Œæå‡ºçš„ä¼ªè§†å›¾å‡ ä½•çº¦æŸæœ‰æ•ˆå‡å°‘äº†å—åˆå¹¶è¿‡ç¨‹ä¸­å› ç©ºæ°”æµ®åŠ¨é€ æˆçš„æ¸²æŸ“é™çº§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-16.html",
    "link_next": "2025-04-18.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "16.04",
        "en": "04/16",
        "zh": "4æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "18.04",
        "en": "04/18",
        "zh": "4æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 8,
        "#agents": 0,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 3,
        "#audio": 2,
        "#video": 1,
        "#multimodal": 4,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 3,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "This article discusses the role of color in how humans and vision-language models (VLMs) perceive and understand the world. It introduces ColorBench, a new benchmark for testing VLMs' color understanding skills. The study finds that while larger models perform better, current VLMs generally neglect color understanding. It also shows that using Chain of Thought (CoT) reasoning can improve color task accuracy. The authors hope ColorBench will help advance research in this area.",
        "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
        "pinyin": "Sure, here is the pinyin transcription for the text:\n\nZhÃ¨ wÃ©nzhÄng tÇolÃ¹n zhÃ¨ zhÇ’ng yÇnsÃ¨ zÃ i rÃ©nlÃ¨i hÃ© shÃ¬yÇn yÇ”yÃ¡n mÃ³xÃ­ng (VLMs) zhÄ«jiÃ n hÃ© lÇjiÄ› shÃ¬jiÃ¨ zhÅng de zuÃ²yÃ²ng. TÄ jiÃ¨shÃ o YÇnsÃ¨ BÄ›nch, yÄ«gÃ¨ xÄ«n de bÄ›nchmark yÇ cÃ¨shÃ¬ VLMs de yÇnsÃ¨ lÇjiÄ› jÃ¬nÃ©ng. YÃ¡njiÅ« fÄxiÃ n zhÇyÇ’u dÃ xÃ­ng mÃ³xÃ­ng biÇoxiÃ n gÃ¨ng hÇo, dÄngqiÃ¡n VLMs yÄ«bÄn shÅ« huÇng yÇnsÃ¨ lÇjiÄ›. TÄ yÄ› shuÅmÃ­ng yÃ²ng LiÃ¡n de SÄ«xiÇng (CoT) tuÇlÇ nÃ©ng gÇishÃ n yÇnsÃ¨ rÃ¨nwÃ¹ de zhÇ”nquÃ¨du. ZhÃ¨xiÄ“ zuÃ²zhÄ› xÄ«wÃ ng YÇnsÃ¨ BÄ›nch huÃ¬ bÄngzhÃ¹ tuÄ«jÃ¬n zhÃ¨ ge lÇngyÃ¹ de yÃ¡njiÅ«.\n\nPlease note that the pinyin transcription is based on the pronunciation of the Chinese characters that would be used to translate the English text. The actual translation might vary slightly depending on the context and specific word choices.",
        "vocab": "[\n    {\"word\": \"perceive\", \"pinyin\": \"pÉ™rËˆsiËv\", \"trans\": \"æ„ŸçŸ¥\"},\n    {\"word\": \"vision-language models\", \"pinyin\": \"ËˆvÉªÊ’É™n ËˆlÃ¦Å‹É¡wÉªdÊ’ mÉ’dÉ™lz\", \"trans\": \"è§†è§‰-è¯­è¨€æ¨¡å‹\"},\n    {\"word\": \"benchmark\", \"pinyin\": \"ËˆbÉ›nÊ§mÉ‘Ëk\", \"trans\": \"åŸºå‡†\"},\n    {\"word\": \"neglect\", \"pinyin\": \"nÉªËˆÉ¡lÉ›kt\", \"trans\": \"å¿½è§†\"},\n    {\"word\": \"Chain of Thought\", \"pinyin\": \"Ê§eÉªn É’v Î¸É”Ët\", \"trans\": \"æ€ç»´é“¾\"},\n    {\"word\": \"reasoning\", \"pinyin\": \"ËˆriËz(É™)nÉªÅ‹\", \"trans\": \"æ¨ç†\"},\n    {\"word\": \"accuracy\", \"pinyin\": \"ËˆÃ¦kjÉ™rÉ™si\", \"trans\": \"å‡†ç¡®æ€§\"},\n    {\"word\": \"advance\", \"pinyin\": \"Ã¦dËˆvÉ‘Ëns\", \"trans\": \"æ¨è¿›\"}\n]",
        "trans": "This article explores the role of color in how both humans and vision-language models (VLMs) perceive and understand the world. It introduces ColorBench, a new benchmark designed to evaluate VLMs' ability to understand color. The study reveals that while larger models tend to perform better, current VLMs generally overlook color understanding. Additionally, the research demonstrates that employing Chain of Thought (CoT) reasoning can enhance the accuracy of color-related tasks. The authors express hope that ColorBench will contribute to further advancements in this field of research.",
        "update_ts": "2025-04-17 09:12"
    }
}