{
    "date": {
        "ru": "17 апреля",
        "en": "April 17",
        "zh": "4月17日"
    },
    "time_utc": "2025-04-17 03:30",
    "weekday": 3,
    "issue_id": 3281,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.12240",
            "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
            "url": "https://huggingface.co/papers/2504.12240",
            "abstract": "The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.",
            "score": 12,
            "issue_id": 3280,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "a237e12792a9a0c8",
            "authors": [
                "Junhao Zhuang",
                "Lingen Li",
                "Xuan Ju",
                "Zhaoyang Zhang",
                "Chun Yuan",
                "Ying Shan"
            ],
            "affiliations": [
                "Tencent ARC Lab, China",
                "The Chinese University of Hong Kong, China",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12240.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#cv",
                    "#open_source",
                    "#architecture",
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Cobra: Быстрая и точная колоризация комиксов с помощью контекстных референсов",
                    "desc": "В статье представлен метод Cobra для эффективной колоризации комиксов на основе множества эталонных изображений. Используется архитектура Causal Sparse DiT с позиционным кодированием и разреженным вниманием для обработки длинного контекста. Метод позволяет быстро и точно раскрашивать линейные рисунки с учетом более 200 референсов. Cobra демонстрирует высокую производительность и интерактивность, отвечая требованиям индустрии комиксов."
                },
                "en": {
                    "title": "Cobra: Revolutionizing Line Art Colorization with Contextual Efficiency",
                    "desc": "This paper presents Cobra, a novel method for line art colorization in the comic production industry, which requires high accuracy and efficiency. Cobra utilizes a Causal Sparse DiT architecture that incorporates advanced techniques like causal sparse attention and positional encodings to handle over 200 reference images effectively. The method addresses challenges such as slow inference times and the need for flexible control, ensuring color identity consistency across diverse characters and backgrounds. Experimental results show that Cobra significantly improves the quality and speed of line art colorization, making it a valuable tool for artists."
                },
                "zh": {
                    "title": "Cobra：高效灵活的线条艺术上色解决方案",
                    "desc": "本论文介绍了一种名为Cobra的高效线条艺术上色方法，旨在解决漫画制作行业中对高准确性和灵活控制的需求。Cobra能够处理超过200张参考图像，并保持低延迟，适应复杂的角色和背景。该方法采用了因果稀疏DiT架构，利用特殊设计的位置编码和因果稀疏注意力机制，有效管理长上下文参考。实验结果表明，Cobra在上色质量和推理速度上均有显著提升，满足了工业界的关键需求。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.10514",
            "title": "ColorBench: Can VLMs See and Understand the Colorful World? A\n  Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
            "url": "https://huggingface.co/papers/2504.10514",
            "abstract": "Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.",
            "score": 12,
            "issue_id": 3281,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "c786e69f24be2f9e",
            "authors": [
                "Yijun Liang",
                "Ming Li",
                "Chenrui Fan",
                "Ziyue Li",
                "Dang Nguyen",
                "Kwesi Cobbina",
                "Shweta Bhardwaj",
                "Jiuhai Chen",
                "Fuxiao Liu",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.10514.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "🌈",
                "ru": {
                    "title": "ColorBench: новый рубеж в понимании цвета искусственным интеллектом",
                    "desc": "Эта статья представляет ColorBench - новый бенчмарк для оценки способностей мультимодальных моделей воспринимать и понимать цвета. Авторы провели обширное тестирование 32 моделей на различных сценариях, связанных с восприятием цвета, рассуждением и устойчивостью. Результаты показали, что хотя более крупные модели справляются лучше, разрыв в производительности между моделями относительно небольшой, что указывает на недостаточное внимание к пониманию цвета в существующих системах. ColorBench может служить фундаментальным инструментом для развития исследований в области понимания цвета искусственным интеллектом на уровне человека."
                },
                "en": {
                    "title": "Enhancing AI's Color Comprehension with ColorBench",
                    "desc": "This paper presents ColorBench, a benchmark designed to evaluate how vision-language models (VLMs) understand and utilize color in visual reasoning. It assesses various aspects of color perception, reasoning, and robustness through a series of real-world scenarios. The study finds that while larger models generally perform better, the existing VLMs show limited capabilities in color understanding, indicating a gap in their training. Additionally, the research highlights that while VLMs can use color cues effectively, they can also be misled by them, emphasizing the need for improved color comprehension in AI models."
                },
                "zh": {
                    "title": "提升视觉语言模型的颜色理解能力",
                    "desc": "本文介绍了ColorBench，这是一个专门评估视觉语言模型（VLMs）在颜色理解方面能力的基准。研究表明，尽管更大的模型在ColorBench上表现更好，但语言模型的作用比视觉编码器更为重要。现有的VLMs在颜色理解方面的表现差距较小，表明这一领域尚未得到充分重视。此外，尽管VLMs能够利用颜色线索，但在某些任务中也可能会受到误导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.12285",
            "title": "BitNet b1.58 2B4T Technical Report",
            "url": "https://huggingface.co/papers/2504.12285",
            "abstract": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.",
            "score": 2,
            "issue_id": 3281,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "cf67f70d9f122792",
            "authors": [
                "Shuming Ma",
                "Hongyu Wang",
                "Shaohan Huang",
                "Xingxing Zhang",
                "Ying Hu",
                "Ting Song",
                "Yan Xia",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.12285.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#benchmark",
                    "#science",
                    "#architecture",
                    "#training",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Революция в эффективности: 1-битная LLM на уровне полноточных моделей",
                    "desc": "Представлен BitNet b1.58 2B4T - первая открытая 1-битная большая языковая модель (LLM) с 2 миллиардами параметров. Модель обучена на корпусе из 4 триллионов токенов и оценена по различным критериям, включая понимание языка, математические рассуждения и кодирование. BitNet b1.58 2B4T показывает производительность на уровне ведущих полноточных LLM аналогичного размера, но с преимуществами в эффективности вычислений. Модель доступна через Hugging Face вместе с реализациями для GPU и CPU."
                },
                "en": {
                    "title": "Efficient Language Understanding with BitNet: The 1-Bit Revolution",
                    "desc": "BitNet b1.58 2B4T is a groundbreaking 1-bit Large Language Model (LLM) with 2 billion parameters, making it the first of its kind to be open-source. It has been trained on an extensive dataset of 4 trillion tokens and evaluated on various benchmarks, showcasing its capabilities in language understanding, mathematical reasoning, coding, and conversation. Remarkably, BitNet achieves performance comparable to other leading full-precision LLMs while being more efficient in terms of memory usage, energy consumption, and decoding speed. The model's weights and inference implementations are made available on Hugging Face, promoting further research and practical applications."
                },
                "zh": {
                    "title": "开源高效的1位大型语言模型",
                    "desc": "我们介绍了BitNet b1.58 2B4T，这是第一个开源的、原生的1位大型语言模型，参数规模达到20亿。该模型在4万亿个标记的语料库上进行训练，并在语言理解、数学推理、编程能力和对话能力等基准测试中进行了严格评估。我们的结果表明，BitNet b1.58 2B4T在性能上与同类规模的领先开源全精度大型语言模型相当，同时在计算效率上具有显著优势，包括显著减少的内存占用、能耗和解码延迟。为了促进进一步的研究和应用，该模型的权重通过Hugging Face发布，并提供了适用于GPU和CPU架构的开源推理实现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.11952",
            "title": "Robust and Fine-Grained Detection of AI Generated Texts",
            "url": "https://huggingface.co/papers/2504.11952",
            "abstract": "An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.",
            "score": 2,
            "issue_id": 3280,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 апреля",
                "en": "April 16",
                "zh": "4月16日"
            },
            "hash": "bdea465fe17b9401",
            "authors": [
                "Ram Mohan Rao Kadiyala",
                "Siddartha Pullakhandam",
                "Kanwal Mehreen",
                "Drishti Sharma",
                "Siddhant Gupta",
                "Jebish Purbey",
                "Ashay Srivastava",
                "Subhasya TippaReddy",
                "Arvind Reddy Bobbili",
                "Suraj Telugara Chandrashekhar",
                "Modabbir Adeeb",
                "Srinadh Vura",
                "Hamza Farooq"
            ],
            "affiliations": [
                "Cohere for AI Community",
                "IISc Bangalore",
                "IIT Roorkee",
                "M2ai.in",
                "Pulchowk Campus",
                "Stanford University",
                "Traversaal.ai",
                "University of California, Los Angeles",
                "University of Houston",
                "University of Maryland, College Park",
                "University of South Florida",
                "Vantager"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.11952.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#hallucinations",
                    "#dataset",
                    "#benchmark",
                    "#multilingual",
                    "#security",
                    "#data"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Универсальный детектор ИИ-текстов: от токенов до языков",
                    "desc": "Статья представляет новый подход к обнаружению текстов, созданных искусственным интеллектом. Авторы разработали модели для классификации токенов, обученные на обширном наборе текстов, созданных совместно человеком и ИИ. Модели показали хорошие результаты на текстах из неизвестных областей, от неизвестных генераторов и на текстах с состязательными входными данными. Исследователи также представили новый датасет из 2,4 миллиона текстов на 23 языках, созданных в соавторстве с популярными проприетарными языковыми моделями."
                },
                "en": {
                    "title": "Advancing Detection of Human-LLM Co-Authored Texts",
                    "desc": "This paper addresses the challenge of detecting machine-generated content, particularly in cases where texts are co-authored by humans and language models (LLMs). The authors developed a set of token classification models trained on a large dataset of 2.4 million co-authored texts, which allows for better detection across various domains and generators. The models demonstrated strong performance even with adversarial inputs and texts from non-native speakers. Additionally, the paper provides insights into how the models perform based on text length and characteristics compared to purely human-authored content."
                },
                "zh": {
                    "title": "构建高效的机器生成内容检测系统",
                    "desc": "本文提出了一种理想的检测系统，旨在有效识别机器生成的内容，尤其是在短文本中。现有系统在识别AI生成内容时常常面临挑战，因此我们专注于人类与大型语言模型（LLM）共同创作的文本。我们构建了一系列用于标记分类的模型，这些模型在大量人机共创文本上进行训练，并在未见领域和生成器的文本上表现良好。我们还引入了一个包含240万条文本的新数据集，主要由多种流行的专有LLM共同创作，涵盖23种语言。"
                }
            }
        }
    ],
    "link_prev": "2025-04-16.html",
    "link_next": "2025-04-18.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "16.04",
        "en": "04/16",
        "zh": "4月16日"
    },
    "short_date_next": {
        "ru": "18.04",
        "en": "04/18",
        "zh": "4月18日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了提升大型语言模型（LLM）推理能力的兴趣。目前的方法依赖监督信号，存在可扩展性和高标注成本问题。作者提出了一种无监督的自训练框架，名为Genius。Genius通过步进式预测重采样策略和优势校准优化（ACO）损失函数，实现了无需外部监督的LLM推理能力提升。代码将在https://github.com/xufangzhi/Genius发布。",
        "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
        "pinyin": "这篇文章讨论了提升大型语言模型（LLM）推理能力的兴趣。\nZhè piān wénzhāng tǎolùn le tíshēng dàxíng yǔyán móxíng (LLM) tuīlǐ nénglì de xìngqù.\n\n目前的方法依赖监督信号，存在可扩展性和高标注成本问题。\nMùqián de fāngfǎ yīlài jiàndū xìnhà",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"提升\", \"pinyin\": \"tí shēng\", \"trans\": \"improve\"},\n    {\"word\": \"大型\", \"pinyin\": \"dà xíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"语言模型\", \"pinyin\": \"yǔ yán mó xíng\", \"trans\": \"language model\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"兴趣\", \"pinyin\": \"xìng qù\", \"trans\": \"interest\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"depend on\"},\n    {\"word\": \"监督\", \"pinyin\": \"jiàn dū\", \"trans\": \"supervised\"},\n    {\"word\": \"信号\", \"pinyin\": \"xìn hào\", \"trans\": \"signal\"},\n    {\"word\": \"可扩展性\", \"pinyin\": \"kě kuò zhǎn xìng\", \"trans\": \"scalability\"},\n    {\"word\": \"高\", \"pinyin\": \"gāo\", \"trans\": \"high\"},\n    {\"word\": \"标注\", \"pinyin\": \"biāo zhù\", \"trans\": \"annotation\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéng běn\", \"trans\": \"cost\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"无监督\", \"pinyin\": \"wú jiàn dū\", \"trans\": \"unsupervised\"},\n    {\"word\": \"自训练\", \"pinyin\": \"zì xùn liàn\", \"trans\": \"self-training\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"名为\", \"pinyin\": \"míng wéi\", \"trans\": \"named\"},\n    {\"word\": \"步进式\", \"pinyin\": \"bù jìn shì\", \"trans\": \"stepwise\"},\n    {\"word\": \"预测\", \"pinyin\": \"yù cè\", \"trans\": \"prediction\"},\n    {\"word\": \"重采样\", \"pinyin\": \"chóng cǎi yàng\", \"trans\": \"resampling\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"优势\", \"pinyin\": \"yōu shì\", \"trans\": \"advantage\"},\n    {\"word\": \"校准\", \"pinyin\": \"jiào zhǔn\", \"trans\": \"calibration\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimization\"},\n    {\"word\": \"损失函数\", \"pinyin\": \"sǔn shī hán shù\", \"trans\": \"loss function\"},\n    {\"word\": \"实现\", \"pinyin\": \"shí xiàn\", \"trans\": \"achieve\"},\n    {\"word\": \"外部\", \"pinyin\": \"wài bù\", \"trans\": \"external\"},\n    {\"word\": \"发布\", \"pinyin\": \"fā bù\", \"trans\": \"release\"}\n]",
        "trans": "This article discusses the interest in enhancing the reasoning capabilities of large language models (LLMs). Current methods rely on supervised signals, which present issues with scalability and high annotation costs. The authors",
        "update_ts": "2025-04-16 09:12"
    }
}