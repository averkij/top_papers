{
    "date": {
        "ru": "17 Ğ¸ÑĞ½Ñ",
        "en": "June 17",
        "zh": "6æœˆ17æ—¥"
    },
    "time_utc": "2025-06-17 04:21",
    "weekday": 1,
    "issue_id": 4326,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.13585",
            "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention",
            "url": "https://huggingface.co/papers/2506.13585",
            "abstract": "A hybrid-attention reasoning model called MiniMax-M1, featuring a Mixture-of-Experts architecture and lightning attention mechanism, is introduced for efficient long-input processing and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.",
            "score": 111,
            "issue_id": 4324,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ½Ñ",
                "en": "June 16",
                "zh": "6æœˆ16æ—¥"
            },
            "hash": "05163c188bd37051",
            "authors": [
                "MiniMax",
                ":",
                "Aili Chen",
                "Aonian Li",
                "Bangwei Gong",
                "Binyang Jiang",
                "Bo Fei",
                "Bo Yang",
                "Boji Shan",
                "Changqing Yu",
                "Chao Wang",
                "Cheng Zhu",
                "Chengjun Xiao",
                "Chengyu Du",
                "Chi Zhang",
                "Chu Qiao",
                "Chunhao Zhang",
                "Chunhui Du",
                "Congchao Guo",
                "Da Chen",
                "Deming Ding",
                "Dianjun Sun",
                "Dong Li",
                "Enwei Jiao",
                "Haigang Zhou",
                "Haimo Zhang",
                "Han Ding",
                "Haohai Sun",
                "Haoyu Feng",
                "Huaiguang Cai",
                "Haichao Zhu",
                "Jian Sun",
                "Jiaqi Zhuang",
                "Jiaren Cai",
                "Jiayuan Song",
                "Jin Zhu",
                "Jingyang Li",
                "Jinhao Tian",
                "Jinli Liu",
                "Junhao Xu",
                "Junjie Yan",
                "Junteng Liu",
                "Junxian He",
                "Kaiyi Feng",
                "Ke Yang",
                "Kecheng Xiao",
                "Le Han",
                "Leyang Wang",
                "Lianfei Yu",
                "Liheng Feng",
                "Lin Li",
                "Lin Zheng",
                "Linge Du",
                "Lingyu Yang",
                "Lunbin Zeng",
                "Minghui Yu",
                "Mingliang Tao",
                "Mingyuan Chi",
                "Mozhi Zhang",
                "Mujie Lin",
                "Nan Hu",
                "Nongyu Di",
                "Peng Gao",
                "Pengfei Li",
                "Pengyu Zhao",
                "Qibing Ren",
                "Qidi Xu",
                "Qile Li",
                "Qin Wang",
                "Rong Tian",
                "Ruitao Leng",
                "Shaoxiang Chen",
                "Shaoyu Chen",
                "Shengmin Shi",
                "Shitong Weng",
                "Shuchang Guan",
                "Shuqi Yu",
                "Sichen Li",
                "Songquan Zhu",
                "Tengfei Li",
                "Tianchi Cai",
                "Tianrun Liang",
                "Weiyu Cheng",
                "Weize Kong",
                "Wenkai Li",
                "Xiancai Chen",
                "Xiangjun Song",
                "Xiao Luo",
                "Xiao Su",
                "Xiaobo Li",
                "Xiaodong Han",
                "Xinzhu Hou",
                "Xuan Lu",
                "Xun Zou",
                "Xuyang Shen",
                "Yan Gong",
                "Yan Ma",
                "Yang Wang",
                "Yiqi Shi",
                "Yiran Zhong",
                "Yonghong Duan",
                "Yongxiang Fu",
                "Yongyi Hu",
                "Yu Gao",
                "Yuanxiang Fan",
                "Yufeng Yang",
                "Yuhao Li",
                "Yulin Hu",
                "Yunan Huang",
                "Yunji Li",
                "Yunzhi Xu",
                "Yuxin Mao",
                "Yuxuan Shi",
                "Yuze Wenren",
                "Zehan Li",
                "Zelin Li",
                "Zhanxu Tian",
                "Zhengmao Zhu",
                "Zhenhua Fan",
                "Zhenzhen Wu",
                "Zhichao Xu",
                "Zhihang Yu",
                "Zhiheng Lyu",
                "Zhuo Jiang",
                "Zibo Gao",
                "Zijia Wu",
                "Zijian Song",
                "Zijun Sun"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.13585.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#open_source",
                    "#architecture",
                    "#reasoning",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MiniMax-M1: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "MiniMax-M1 - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¼Ğ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. MiniMax-M1 Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Long-Input Processing with MiniMax-M1",
                    "desc": "MiniMax-M1 is a groundbreaking hybrid-attention reasoning model that utilizes a Mixture-of-Experts architecture and a lightning attention mechanism to efficiently handle long input sequences. With 456 billion parameters and the ability to process up to 1 million tokens, it significantly outperforms previous models in context length. The model is trained using a novel reinforcement learning algorithm called CISPO, which enhances training efficiency by clipping importance sampling weights. MiniMax-M1 demonstrates superior performance in complex tasks, particularly in software engineering and long-context applications, making it a valuable tool for various AI challenges."
                },
                "zh": {
                    "title": "é«˜æ•ˆé•¿è¾“å…¥å¤„ç†çš„æ··åˆæ³¨æ„åŠ›æ¨¡å‹",
                    "desc": "MiniMax-M1æ˜¯ä¸€ç§æ··åˆæ³¨æ„åŠ›æ¨ç†æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„å’Œé—ªç”µæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨é«˜æ•ˆå¤„ç†é•¿è¾“å…¥å’Œå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åŸºäºä¹‹å‰çš„MiniMax-Text-01æ¨¡å‹ï¼Œå…·æœ‰4560äº¿ä¸ªå‚æ•°ï¼Œå¹¶æ”¯æŒé«˜è¾¾100ä¸‡ä¸ªtokençš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚MiniMax-M1åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨è½¯ä»¶å·¥ç¨‹å’Œå·¥å…·åˆ©ç”¨æ–¹é¢ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•CISPOï¼Œè¿›ä¸€æ­¥æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.11763",
            "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
            "url": "https://huggingface.co/papers/2506.11763",
            "abstract": "DeepResearch Bench offers a benchmark framework to evaluate the capabilities of Deep Research Agents in terms of research quality and information retrieval accuracy across multiple fields.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.",
            "score": 11,
            "issue_id": 4324,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ½Ñ",
                "en": "June 13",
                "zh": "6æœˆ13æ—¥"
            },
            "hash": "197213635094ee83",
            "authors": [
                "Mingxuan Du",
                "Benfeng Xu",
                "Chiwei Zhu",
                "Xiaorui Wang",
                "Zhendong Mao"
            ],
            "affiliations": [
                "MetastoneTechnology, Beijing, China",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.11763.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#science",
                    "#agents",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "DeepResearch Bench - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 100 Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ PhD Ğ¿Ğ¾ 22 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‰Ğ¸ĞµÑÑ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼: Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. DeepResearch Bench Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Benchmarking Deep Research Agents for Superior Research Quality",
                    "desc": "DeepResearch Bench is a new framework designed to evaluate the performance of Deep Research Agents (DRAs) in generating high-quality research outputs. It includes 100 PhD-level tasks across 22 fields, allowing for a comprehensive assessment of DRAs' capabilities in information retrieval and report synthesis. The framework introduces two innovative evaluation methods: one that uses reference-based criteria to judge report quality, and another that measures citation effectiveness and accuracy. By providing these tools, DeepResearch Bench aims to enhance the development of LLM-based agents and improve their research efficiency."
                },
                "zh": {
                    "title": "è¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†çš„æ–°åŸºå‡†",
                    "desc": "DeepResearch Benchæ˜¯ä¸€ä¸ªåŸºå‡†æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†åœ¨ç ”ç©¶è´¨é‡å’Œä¿¡æ¯æ£€ç´¢å‡†ç¡®æ€§æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…å«100ä¸ªç”±é¢†åŸŸä¸“å®¶ç²¾å¿ƒè®¾è®¡çš„åšå£«çº§ç ”ç©¶ä»»åŠ¡ï¼Œæ¶µç›–22ä¸ªä¸åŒé¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•æ¥è¯„ä¼°è¿™äº›ä»£ç†çš„èƒ½åŠ›ï¼Œä¸€ç§æ˜¯åŸºäºå‚è€ƒçš„è¯„ä¼°æ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯è¯„ä¼°ä¿¡æ¯æ£€ç´¢å’Œå¼•ç”¨å‡†ç¡®æ€§çš„æ¡†æ¶ã€‚é€šè¿‡å¼€æºDeepResearch BenchåŠå…¶å…³é”®ç»„ä»¶ï¼Œæˆ‘ä»¬å¸Œæœ›åŠ é€ŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13759",
            "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
            "url": "https://huggingface.co/papers/2506.13759",
            "abstract": "Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs) enable parallel generation and faster inference compared to autoregressive models through denoising-based strategies and full attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed.   The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025.   In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment.   Paper collection: https://github.com/LiQiiiii/DLLM-Survey",
            "score": 10,
            "issue_id": 4324,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ½Ñ",
                "en": "June 16",
                "zh": "6æœˆ16æ—¥"
            },
            "hash": "0a523ab9b7563360",
            "authors": [
                "Runpeng Yu",
                "Qi Li",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13759.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#diffusion",
                    "#inference",
                    "#multimodal",
                    "#survey"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dLLMs) Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dMLLMs). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, dLLMs Ğ¸ dMLLMs Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¸ Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹, ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ dLLMs Ğ¸ dMLLMs Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Accelerating Language Generation with Discrete Diffusion Models",
                    "desc": "This paper surveys Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs), highlighting their advantages over traditional autoregressive models. dLLMs and dMLLMs utilize a parallel decoding approach with full attention and denoising strategies, allowing for faster generation and improved output control. The paper reviews the historical development, mathematical foundations, and key techniques for training these models, as well as their applications in various domains. It also discusses the future potential of dLLMs and dMLLMs in advancing machine learning research and deployment."
                },
                "zh": {
                    "title": "ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼šåŠ é€Ÿç”Ÿæˆä¸æ§åˆ¶çš„æœªæ¥",
                    "desc": "æœ¬æ–‡ç³»ç»Ÿæ€§åœ°è°ƒæŸ¥äº†ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰å’Œç¦»æ•£æ‰©æ•£å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆdMLLMsï¼‰ã€‚ä¸è‡ªå›å½’æ¨¡å‹ä¸åŒï¼ŒdLLMså’ŒdMLLMsé‡‡ç”¨å¤šæ ‡è®°å¹¶è¡Œè§£ç çš„èŒƒå¼ï¼Œåˆ©ç”¨å…¨æ³¨æ„åŠ›æœºåˆ¶å’Œå»å™ªç”Ÿæˆç­–ç•¥ï¼Œä»è€Œå®ç°å¹¶è¡Œç”Ÿæˆå’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚è¿™ç§æ–°æ–¹æ³•ä½¿å¾—ç»†ç²’åº¦çš„è¾“å‡ºæ§åˆ¶å’ŒåŠ¨æ€å“åº”æ„ŸçŸ¥æˆä¸ºå¯èƒ½ï¼Œè¿™åœ¨è‡ªå›å½’æ¨¡å‹ä¸­æ˜¯éš¾ä»¥å®ç°çš„ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒdLLMså’ŒdMLLMsåœ¨æ¨ç†é€Ÿåº¦ä¸Šå¯å®ç°é«˜è¾¾10å€çš„åŠ é€Ÿï¼Œä¸”åœ¨å¤šä¸ªé¢†åŸŸçš„åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12915",
            "title": "PersonaFeedback: A Large-scale Human-annotated Benchmark For\n  Personalization",
            "url": "https://huggingface.co/papers/2506.12915",
            "abstract": "A new benchmark, PersonaFeedback, evaluates Large Language Models' ability to generate personalized responses given explicit user personas, revealing limitations in current systems.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid improvement in the general capabilities of LLMs, LLM personalization, i.e., how to build LLM systems that can generate personalized responses or services that are tailored to distinct user personas, has become an increasingly important research and engineering problem. However, unlike many new challenging benchmarks being released for evaluating the general/reasoning capabilities, the lack of high-quality benchmarks for evaluating LLM personalization greatly hinders progress in this field. To address this, we introduce PersonaFeedback, a new benchmark that directly evaluates LLMs' ability to provide personalized responses given pre-defined user personas and queries. Unlike existing benchmarks that require models to infer implicit user personas from historical interactions, PersonaFeedback decouples persona inference from personalization, focusing on evaluating the model's ability to generate responses tailored to explicit personas. PersonaFeedback consists of 8298 human-annotated test cases, which are categorized into easy, medium, and hard tiers based on the contextual complexity of the user personas and the difficulty in distinguishing subtle differences between two personalized responses. We conduct comprehensive evaluations across a wide range of models. The empirical results reveal that even state-of-the-art LLMs that can solve complex real-world reasoning tasks could fall short on the hard tier of PersonaFeedback where even human evaluators may find the distinctions challenging. Furthermore, we conduct an in-depth analysis of failure modes across various types of systems, demonstrating that the current retrieval-augmented framework should not be seen as a de facto solution for personalization tasks. All benchmark data, annotation protocols, and the evaluation pipeline will be publicly available to facilitate future research on LLM personalization.",
            "score": 10,
            "issue_id": 4325,
            "pub_date": "2025-06-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ½Ñ",
                "en": "June 15",
                "zh": "6æœˆ15æ—¥"
            },
            "hash": "7f12645dbf1aa58d",
            "authors": [
                "Meiling Tao",
                "Chenghao Zhu",
                "Dongyi Ding",
                "Tiannan Wang",
                "Yuchen Eleanor Jiang",
                "Wangchunshu Zhou"
            ],
            "affiliations": [
                "OPPO",
                "South China Agricultural University",
                "The Chinese University of Hong Kong, Shenzhen",
                "University of Electronic Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12915.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#interpretability",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "PersonaFeedback: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PersonaFeedback Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²Ğ½Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 8298 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ»ĞµĞ³ĞºĞ¸Ğµ, ÑÑ€ĞµĞ´Ğ½Ğ¸Ğµ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ PersonaFeedback. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ retrieval-augmented Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "Enhancing Personalization in LLMs with PersonaFeedback",
                    "desc": "The paper introduces PersonaFeedback, a new benchmark designed to assess the ability of Large Language Models (LLMs) to generate personalized responses based on explicit user personas. This benchmark addresses the gap in evaluating LLM personalization, which has been overlooked compared to general reasoning capabilities. PersonaFeedback includes 8,298 human-annotated test cases categorized by complexity, revealing that even advanced LLMs struggle with nuanced personalization tasks. The findings highlight the limitations of current models and emphasize the need for improved frameworks in LLM personalization."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–ç”Ÿæˆçš„æ–°åŸºå‡†ï¼šPersonaFeedback",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸ºPersonaFeedbackï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆä¸ªæ€§åŒ–å“åº”çš„èƒ½åŠ›ã€‚éšç€LLMèƒ½åŠ›çš„å¿«é€Ÿæå‡ï¼Œä¸ªæ€§åŒ–ç”Ÿæˆå·²æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶é—®é¢˜ï¼Œä½†ç¼ºä¹é«˜è´¨é‡çš„åŸºå‡†æµ‹è¯•é™åˆ¶äº†è¿™ä¸€é¢†åŸŸçš„è¿›å±•ã€‚PersonaFeedbacké€šè¿‡æä¾›é¢„å®šä¹‰çš„ç”¨æˆ·è§’è‰²å’ŒæŸ¥è¯¢ï¼Œç›´æ¥è¯„ä¼°æ¨¡å‹ç”Ÿæˆé’ˆå¯¹ç‰¹å®šç”¨æˆ·è§’è‰²çš„å“åº”èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„LLMåœ¨å¤„ç†å¤æ‚çš„ä¸ªæ€§åŒ–ä»»åŠ¡æ—¶ä¹Ÿå¯èƒ½è¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†å½“å‰æ¨¡å‹åœ¨ä¸ªæ€§åŒ–ç”Ÿæˆæ–¹é¢çš„å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.08343",
            "title": "Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves\n  Reasoning Efficiency",
            "url": "https://huggingface.co/papers/2506.08343",
            "abstract": "NoWait suppresses explicit self-reflection tokens during inference to enhance efficiency in multimodal reasoning without reducing model utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as \"Wait\" and \"Hmm\", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning.",
            "score": 9,
            "issue_id": 4324,
            "pub_date": "2025-06-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ½Ñ",
                "en": "June 10",
                "zh": "6æœˆ10æ—¥"
            },
            "hash": "bc2b3e7cb2a8d002",
            "authors": [
                "Chenlong Wang",
                "Yuanning Feng",
                "Dongping Chen",
                "Zhaoyang Chu",
                "Ranjay Krishna",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "University College London",
                "University of Maryland",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.08343.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… ÑĞ»Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ NoWait, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ²Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 27-51% Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ´ĞµÑÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. NoWait Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…."
                },
                "en": {
                    "title": "NoWait: Streamlining Multimodal Reasoning for Efficiency",
                    "desc": "The paper introduces NoWait, a method that improves the efficiency of multimodal reasoning models by suppressing explicit self-reflection tokens during inference. These tokens, like 'Wait' and 'Hmm', often lead to unnecessary verbosity and can slow down the reasoning process. By removing these tokens, NoWait significantly shortens the reasoning paths while maintaining the effectiveness of the model. The results from various benchmarks demonstrate that this approach can reduce the length of reasoning trajectories by up to 51%, making it a valuable enhancement for large reasoning models."
                },
                "zh": {
                    "title": "NoWaitï¼šæå‡å¤šæ¨¡æ€æ¨ç†æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "NoWaitæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­æŠ‘åˆ¶æ˜¾å¼è‡ªæˆ‘åæ€çš„æ ‡è®°ï¼ˆå¦‚â€œç­‰ä¸€ä¸‹â€å’Œâ€œå—¯â€ï¼‰ï¼Œæ¥æé«˜å¤šæ¨¡æ€æ¨ç†çš„æ•ˆç‡ï¼Œè€Œä¸é™ä½æ¨¡å‹çš„å®ç”¨æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„æ¨ç†æ¨¡å‹åœ¨å¤æ‚æ¨ç†æ—¶å¸¸å¸¸ä¼šå‡ºç°è¿‡åº¦æ€è€ƒï¼Œå¯¼è‡´è¾“å‡ºå†—é•¿ä¸”é‡å¤ï¼Œå½±å“æ•ˆç‡ã€‚é€šè¿‡åœ¨åä¸ªåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œå¹¿æ³›å®éªŒï¼ŒNoWaitèƒ½å¤Ÿå°†æ€ç»´é“¾çš„é•¿åº¦å‡å°‘27%åˆ°51%ã€‚å› æ­¤ï¼ŒNoWaitä¸ºé«˜æ•ˆä¸”ä¿æŒå®ç”¨æ€§çš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03968",
            "title": "From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding",
            "url": "https://huggingface.co/papers/2506.03968",
            "abstract": "The paper presents a method for generating diverse and complex instruction data for large language models using attributed grounding, achieving top performance on benchmarks with a large synthesized dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.",
            "score": 8,
            "issue_id": 4324,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "991991c3f686afa8",
            "authors": [
                "Chiwei Zhu",
                "Benfeng Xu",
                "Xiaorui Wang",
                "Zhendong Mao"
            ],
            "affiliations": [
                "Metastone Technology",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03968.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#alignment",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SynthQuestions, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ Ğ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²ĞµĞ±-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Harnessing Attributed Grounding for Diverse Instruction Generation",
                    "desc": "This paper introduces a novel method for generating diverse and complex instruction data for large language models (LLMs) through a technique called attributed grounding. The approach combines a top-down attribution process, which connects real-world instructions to specific user contexts, with a bottom-up synthesis process that creates meaningful instructions from web documents. By leveraging this framework, the authors successfully produce a large dataset of 1 million synthesized instructions, named SynthQuestions, which significantly enhances the performance of LLMs on various benchmarks. The results indicate that as more web data is utilized, the effectiveness of the models continues to improve, showcasing the importance of rich and varied instruction data for model alignment."
                },
                "zh": {
                    "title": "é€šè¿‡å±æ€§åŸºç¡€ç”Ÿæˆå¤æ‚æŒ‡ä»¤æ•°æ®ï¼Œæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å±æ€§åŸºç¡€ç”Ÿæˆå¤šæ ·åŒ–å’Œå¤æ‚çš„æŒ‡ä»¤æ•°æ®çš„æ–¹æ³•ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è‡ªä¸Šè€Œä¸‹çš„å½’å› è¿‡ç¨‹å’Œè‡ªä¸‹è€Œä¸Šçš„åˆæˆè¿‡ç¨‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»çœŸå®æŒ‡ä»¤å’Œç½‘ç»œæ–‡æ¡£ä¸­ç”Ÿæˆæœ‰æ„ä¹‰çš„æŒ‡ä»¤ã€‚é€šè¿‡è¿™ç§æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«100ä¸‡ä¸ªæŒ‡ä»¤çš„æ•°æ®é›†SynthQuestionsï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆçš„è¡¨ç°ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨ä¸°å¯Œçš„ç½‘ç»œæ–‡æ¡£å¯ä»¥å¤§è§„æ¨¡æ”¶é›†å¤æ‚çš„æŒ‡ä»¤ï¼Œä»è€Œæå‡æ¨¡å‹çš„å¯¹é½èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13654",
            "title": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning",
            "url": "https://huggingface.co/papers/2506.13654",
            "abstract": "Ego-R1, a reinforcement learning-based framework, uses a structured tool-augmented chain-of-thought process to reason over ultra-long egocentric videos, achieving better performance than existing methods by extending time coverage to a week.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e., in days and weeks) egocentric videos, which leverages a structured Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained via reinforcement learning (RL). Inspired by human problem-solving strategies, CoTT decomposes complex reasoning into modular steps, with the RL agent invoking specific tools, one per step, to iteratively and collaboratively answer sub-questions tackling such tasks as temporal retrieval and multi-modal understanding. We design a two-stage training paradigm involving supervised finetuning (SFT) of a pretrained language model using CoTT data and RL to enable our agent to dynamically propose step-by-step tools for long-range reasoning. To facilitate training, we construct a dataset called Ego-R1 Data, which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark, Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources. Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of understanding ultra-long egocentric videos, significantly extending the time coverage from few hours to a week.",
            "score": 6,
            "issue_id": 4326,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ½Ñ",
                "en": "June 16",
                "zh": "6æœˆ16æ—¥"
            },
            "hash": "52b4ddc8a62e646b",
            "authors": [
                "Shulin Tian",
                "Ruiqi Wang",
                "Hongming Guo",
                "Penghao Wu",
                "Yuhao Dong",
                "Xiuying Wang",
                "Jingkang Yang",
                "Hao Zhang",
                "Hongyuan Zhu",
                "Ziwei Liu"
            ],
            "affiliations": [
                "A*STAR, Singapore",
                "S-Lab, Nanyang Technological University",
                "Shanghai AI Lab",
                "Simon Fraser University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13654.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#training",
                    "#multimodal",
                    "#long_context",
                    "#rl"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ego-R1: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ego-R1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ (Chain-of-Tool-Thought). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸. Ego-R1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ…Ğ²Ğ°Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‡Ğ°ÑĞ¾Ğ² Ğ´Ğ¾ Ğ½ĞµĞ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Video Understanding with Ego-R1",
                    "desc": "Ego-R1 is a new framework designed to analyze ultra-long egocentric videos, which can last for days or even weeks. It employs a structured Chain-of-Tool-Thought (CoTT) process, allowing the system to break down complex reasoning tasks into manageable steps. The framework is powered by a reinforcement learning agent that learns to select and use specific tools for each step, enhancing its ability to answer questions about the video content. By training on a specially created dataset and evaluating on a new benchmark, Ego-R1 demonstrates improved performance in understanding long-duration videos compared to existing methods."
                },
                "zh": {
                    "title": "Ego-R1ï¼šè¶…é•¿è§†é¢‘æ¨ç†çš„æ–°çªç ´",
                    "desc": "Ego-R1æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¤„ç†è¶…é•¿çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§ç»“æ„åŒ–çš„å·¥å…·å¢å¼ºæ€ç»´é“¾ï¼ˆCoTTï¼‰è¿‡ç¨‹ï¼Œå°†å¤æ‚çš„æ¨ç†åˆ†è§£ä¸ºæ¨¡å—åŒ–æ­¥éª¤ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„Ego-R1ä»£ç†èƒ½å¤ŸåŠ¨æ€åœ°æå‡ºé€æ­¥å·¥å…·ï¼Œä»¥åº”å¯¹é•¿æ—¶é—´èŒƒå›´å†…çš„æ¨ç†ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEgo-R1åœ¨ç†è§£è¶…é•¿è§†é¢‘æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ—¶é—´è¦†ç›–èŒƒå›´ä»å‡ å°æ—¶æ‰©å±•åˆ°ä¸€å‘¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.07961",
            "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning\n  with Vision-Language Models",
            "url": "https://huggingface.co/papers/2506.07961",
            "abstract": "BridgeVLA is a 3D vision-language-action model that projects 3D inputs to 2D images and uses 2D heatmaps for efficient and effective action prediction, outperforming baselines in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, leveraging pre-trained vision-language models (VLMs) for building vision-language-action (VLA) models has emerged as a promising approach to effective robot manipulation learning. However, only few methods incorporate 3D signals into VLMs for action prediction, and they do not fully leverage the spatial structure inherent in 3D data, leading to low sample efficiency. In this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D inputs to multiple 2D images, ensuring input alignment with the VLM backbone, and (2) utilizes 2D heatmaps for action prediction, unifying the input and output spaces within a consistent 2D image space. In addition, we propose a scalable pre-training method that equips the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning. Extensive experiments show the proposed method is able to learn 3D manipulation efficiently and effectively. BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks. In RLBench, it improves the average success rate from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better performance in challenging generalization settings, boosting the average success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It generalizes robustly in multiple out-of-distribution settings, including visual disturbances and unseen instructions. Remarkably, it is able to achieve a success rate of 96.8% on 10+ tasks with only 3 trajectories per task, highlighting its extraordinary sample efficiency. Project Website:https://bridgevla.github.io/",
            "score": 6,
            "issue_id": 4324,
            "pub_date": "2025-06-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ½Ñ",
                "en": "June 9",
                "zh": "6æœˆ9æ—¥"
            },
            "hash": "3fcf8d6329af3962",
            "authors": [
                "Peiyan Li",
                "Yixiang Chen",
                "Hongtao Wu",
                "Xiao Ma",
                "Xiangnan Wu",
                "Yan Huang",
                "Liang Wang",
                "Tao Kong",
                "Tieniu Tan"
            ],
            "affiliations": [
                "ByteDance Seed",
                "CASIA",
                "FiveAges",
                "NJU",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.07961.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#3d",
                    "#games",
                    "#optimization",
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "BridgeVLA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ 3D Ğ² 2D",
                    "desc": "BridgeVLA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ 3D-Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ½Ğ° Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµÑ‚ 3D-Ğ²Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 2D-Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ²Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. BridgeVLA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "BridgeVLA: Bridging 3D Vision and Action with 2D Heatmaps",
                    "desc": "BridgeVLA is a novel model that integrates 3D vision with language and action prediction by projecting 3D inputs into 2D images. This approach allows it to utilize 2D heatmaps for more efficient action prediction, enhancing the model's performance in robot manipulation tasks. The model is pre-trained to predict these heatmaps, which helps it learn effectively from fewer samples. Extensive testing shows that BridgeVLA significantly outperforms existing methods in various benchmarks, demonstrating its robustness and efficiency in real-world applications."
                },
                "zh": {
                    "title": "BridgeVLAï¼šé«˜æ•ˆçš„3Dè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹",
                    "desc": "BridgeVLAæ˜¯ä¸€ç§3Dè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œå®ƒå°†3Dè¾“å…¥æŠ•å½±åˆ°2Då›¾åƒï¼Œå¹¶åˆ©ç”¨2Dçƒ­å›¾è¿›è¡Œé«˜æ•ˆçš„åŠ¨ä½œé¢„æµ‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†3Dä¿¡å·æ•´åˆåˆ°è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­ï¼Œå……åˆ†åˆ©ç”¨äº†3Dæ•°æ®çš„ç©ºé—´ç»“æ„ï¼Œä»è€Œæé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œä½¿å¾—è§†è§‰-è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸‹æ¸¸ç­–ç•¥å­¦ä¹ ä¹‹å‰é¢„æµ‹2Dçƒ­å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBridgeVLAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå±•ç°äº†å“è¶Šçš„å­¦ä¹ æ•ˆç‡å’Œæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13750",
            "title": "Test3R: Learning to Reconstruct 3D at Test Time",
            "url": "https://huggingface.co/papers/2506.13750",
            "abstract": "Test3R, a test-time learning technique for 3D reconstruction, enhances geometric accuracy by optimizing network consistency using self-supervised learning on image triplets.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense matching methods like DUSt3R regress pairwise pointmaps for 3D reconstruction. However, the reliance on pairwise prediction and the limited generalization capability inherently restrict the global geometric consistency. In this work, we introduce Test3R, a surprisingly simple test-time learning technique that significantly boosts geometric accuracy. Using image triplets (I_1,I_2,I_3), Test3R generates reconstructions from pairs (I_1,I_2) and (I_1,I_3). The core idea is to optimize the network at test time via a self-supervised objective: maximizing the geometric consistency between these two reconstructions relative to the common image I_1. This ensures the model produces cross-pair consistent outputs, regardless of the inputs. Extensive experiments demonstrate that our technique significantly outperforms previous state-of-the-art methods on the 3D reconstruction and multi-view depth estimation tasks. Moreover, it is universally applicable and nearly cost-free, making it easily applied to other models and implemented with minimal test-time training overhead and parameter footprint. Code is available at https://github.com/nopQAQ/Test3R.",
            "score": 5,
            "issue_id": 4324,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ½Ñ",
                "en": "June 16",
                "zh": "6æœˆ16æ—¥"
            },
            "hash": "68d521856e78273a",
            "authors": [
                "Yuheng Yuan",
                "Qiuhong Shen",
                "Shizun Wang",
                "Xingyi Yang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13750.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "Test3R: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Test3R - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Test3R Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹."
                },
                "en": {
                    "title": "Boosting 3D Reconstruction Accuracy with Test3R",
                    "desc": "Test3R is a novel test-time learning approach designed to improve the accuracy of 3D reconstruction by leveraging self-supervised learning on image triplets. It addresses the limitations of traditional dense matching methods that rely on pairwise predictions, which can lead to inconsistencies in global geometry. By optimizing the network's output during testing, Test3R ensures that reconstructions from different image pairs maintain geometric consistency relative to a common reference image. This technique not only enhances performance on 3D reconstruction tasks but is also easy to implement and applicable to various models with minimal additional training requirements."
                },
                "zh": {
                    "title": "Test3Rï¼šæå‡3Dé‡å»ºç²¾åº¦çš„ç®€å•æ–¹æ³•",
                    "desc": "Test3Ræ˜¯ä¸€ç§ç”¨äº3Dé‡å»ºçš„æµ‹è¯•æ—¶å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ ä¼˜åŒ–ç½‘ç»œä¸€è‡´æ€§ï¼Œä»è€Œæé«˜å‡ ä½•ç²¾åº¦ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å›¾åƒä¸‰å…ƒç»„ç”Ÿæˆé‡å»ºï¼Œç¡®ä¿ä¸åŒå›¾åƒå¯¹ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚Test3Rçš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æµ‹è¯•æ—¶æœ€å¤§åŒ–é‡å»ºä¹‹é—´çš„å‡ ä½•ä¸€è‡´æ€§ï¼Œç¡®ä¿æ¨¡å‹è¾“å‡ºåœ¨ä¸åŒè¾“å…¥ä¸‹ä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTest3Råœ¨3Dé‡å»ºå’Œå¤šè§†è§’æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¸”é€‚ç”¨æ€§å¹¿æ³›ï¼Œå‡ ä¹ä¸å¢åŠ æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10521",
            "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via\n  Perception, Understanding, and Reasoning",
            "url": "https://huggingface.co/papers/2506.10521",
            "abstract": "Scientists' First Exam (SFE) benchmark assesses scientific cognitive capacities of Multimodal Large Language Models through perception, understanding, and comparative reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries.",
            "score": 5,
            "issue_id": 4325,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 Ğ¸ÑĞ½Ñ",
                "en": "June 12",
                "zh": "6æœˆ12æ—¥"
            },
            "hash": "3e2672b026127b5e",
            "authors": [
                "Yuhao Zhou",
                "Yiheng Wang",
                "Xuming He",
                "Ruoyao Xiao",
                "Zhiwei Li",
                "Qiantai Feng",
                "Zijie Guo",
                "Yuejin Yang",
                "Hao Wu",
                "Wenxuan Huang",
                "Jiaqi Wei",
                "Dan Si",
                "Xiuqi Yao",
                "Jia Bu",
                "Haiwen Huang",
                "Tianfan Fu",
                "Shixiang Tang",
                "Ben Fei",
                "Dongzhan Zhou",
                "Fenghua Ling",
                "Yan Lu",
                "Siqi Sun",
                "Chenhui Li",
                "Guanjie Zheng",
                "Jiancheng Lv",
                "Wenlong Zhang",
                "Lei Bai"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10521.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#science",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ£Ñ‡Ñ‘Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Scientists' First Exam (SFE) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). SFE Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 830 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 66 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¿ÑÑ‚Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4 Ğ¸ InternVL-3 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 34.08% Ğ¸ 26.52% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° SFE, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ MLLM Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ."
                },
                "en": {
                    "title": "Enhancing Scientific Discovery with MLLMs: The SFE Benchmark",
                    "desc": "The Scientists' First Exam (SFE) benchmark evaluates the cognitive abilities of Multimodal Large Language Models (MLLMs) in scientific contexts. It focuses on three key areas: perception of scientific signals, understanding of scientific attributes, and comparative reasoning. The benchmark includes 830 expert-verified visual question-answering pairs across various multimodal tasks in five important scientific disciplines. Results show that leading models like GPT-o3 and InternVL-3 perform below expectations, indicating a need for improvement in their scientific reasoning capabilities."
                },
                "zh": {
                    "title": "ç§‘å­¦è®¤çŸ¥èƒ½åŠ›çš„æ–°è¯„ä¼°æ ‡å‡†",
                    "desc": "ç§‘å­¦å®¶é¦–æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†æµ‹è¯•è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ï¼Œä¸»è¦é€šè¿‡æ„ŸçŸ¥ã€ç†è§£å’Œæ¯”è¾ƒæ¨ç†ä¸‰ä¸ªæ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚å½“å‰çš„ç§‘å­¦åŸºå‡†ä¸»è¦å…³æ³¨MLLMsçš„çŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œæœªèƒ½å……åˆ†è¯„ä¼°å…¶æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚SFEåŸºå‡†åŒ…å«830ä¸ªç»è¿‡ä¸“å®¶éªŒè¯çš„è§†è§‰é—®ç­”å¯¹ï¼Œæ¶µç›–66ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ¶‰åŠäº”ä¸ªé«˜ä»·å€¼å­¦ç§‘ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹åœ¨SFEä¸Šçš„è¡¨ç°ä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ï¼Œè¡¨æ˜MLLMsåœ¨ç§‘å­¦é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å·¨å¤§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12953",
            "title": "Forecasting Time Series with LLMs via Patch-Based Prompting and\n  Decomposition",
            "url": "https://huggingface.co/papers/2506.12953",
            "abstract": "PatchInstruct enhances LLM forecasting quality through specialized prompting methods that include time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions.",
            "score": 1,
            "issue_id": 4324,
            "pub_date": "2025-06-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ½Ñ",
                "en": "June 15",
                "zh": "6æœˆ15æ—¥"
            },
            "hash": "fb2789e38592ff5a",
            "authors": [
                "Mayank Bumb",
                "Anshul Vemulapalli",
                "Sri Harsha Vardhan Prasad Jella",
                "Anish Gupta",
                "An La",
                "Ryan A. Rossi",
                "Hongjie Chen",
                "Franck Dernoncourt",
                "Nesreen K. Ahmed",
                "Yu Wang"
            ],
            "affiliations": [
                "Adobe",
                "Dolby Labs",
                "Intel",
                "University of Massachusetts Amherst",
                "University of Oregon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12953.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ“ˆ",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ PatchInstruct Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ², Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹. PatchInstruct Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing LLM Forecasting with Simple Prompting Techniques",
                    "desc": "PatchInstruct is a method that improves the forecasting abilities of Large Language Models (LLMs) by using innovative prompting techniques. It incorporates time series decomposition to break down data into manageable parts, patch-based tokenization to efficiently handle input, and similarity-based neighbor augmentation to enhance predictions by considering related data points. This approach allows LLMs to perform time series forecasting without the need for extensive fine-tuning or complex architectures. Overall, PatchInstruct simplifies the process while boosting the accuracy of predictions in time series analysis."
                },
                "zh": {
                    "title": "PatchInstructï¼šç®€åŒ–æ—¶é—´åºåˆ—é¢„æµ‹çš„æœ‰æ•ˆæ–¹æ³•",
                    "desc": "PatchInstructæ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶é—´åºåˆ—é¢„æµ‹è´¨é‡çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡ä¸“é—¨çš„æç¤ºç­–ç•¥ï¼Œå¦‚æ—¶é—´åºåˆ—åˆ†è§£ã€åŸºäºè¡¥ä¸çš„æ ‡è®°åŒ–å’Œç›¸ä¼¼æ€§é‚»å±…å¢å¼ºï¼Œæ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚ä¸ä»¥å¾€éœ€è¦å¤§é‡å¾®è°ƒçš„æ–¹æ³•ä¸åŒï¼ŒPatchInstructèƒ½å¤Ÿåœ¨ä¸å¤æ‚é‡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œçµæ´»åœ°è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚è¯¥æ–¹æ³•ä¿æŒäº†ç®€å•æ€§ï¼Œå¹¶ä¸”å¯¹æ•°æ®çš„é¢„å¤„ç†è¦æ±‚æœ€ä½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12623",
            "title": "MS4UI: A Dataset for Multi-modal Summarization of User Interface\n  Instructional Videos",
            "url": "https://huggingface.co/papers/2506.12623",
            "abstract": "A novel benchmark and dataset are proposed for multi-modal summarization of UI instructional videos, addressing the need for step-by-step executable instructions and key video frames.  \t\t\t\t\tAI-generated summary \t\t\t\t We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose a novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect a dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization.",
            "score": 1,
            "issue_id": 4324,
            "pub_date": "2025-06-14",
            "pub_date_card": {
                "ru": "14 Ğ¸ÑĞ½Ñ",
                "en": "June 14",
                "zh": "6æœˆ14æ—¥"
            },
            "hash": "ef83eb4ade9dc4bf",
            "authors": [
                "Yuan Zang",
                "Hao Tan",
                "Seunghyun Yoon",
                "Franck Dernoncourt",
                "Jiuxiang Gu",
                "Kushal Kafle",
                "Chen Sun",
                "Trung Bui"
            ],
            "affiliations": [
                "Adobe Research",
                "Brown University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12623.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑƒĞ¼Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ UI",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¾Ğ±Ñ€Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 2413 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ¾Ğ»ĞµĞµ 167 Ñ‡Ğ°ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Learning with UI Video Summarization",
                    "desc": "This paper introduces a new benchmark and dataset specifically designed for multi-modal summarization of user interface (UI) instructional videos. The goal is to create efficient summaries that include step-by-step text instructions and key video frames, which are essential for effective learning. The authors highlight that existing benchmarks are inadequate for this purpose, as they focus on general video summarization rather than instructional content. Through extensive experiments on their dataset of 2,413 annotated UI instructional videos, they demonstrate that current multi-modal summarization techniques are not effective for this specific type of video, indicating a need for improved methods."
                },
                "zh": {
                    "title": "æå‡UIæ•™å­¦è§†é¢‘çš„å¤šæ¨¡æ€æ€»ç»“èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†å’Œæ•°æ®é›†ï¼Œç”¨äºå¤šæ¨¡æ€æ€»ç»“ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰æ•™å­¦è§†é¢‘ï¼Œæ—¨åœ¨æä¾›é€æ­¥å¯æ‰§è¡Œçš„æŒ‡ä»¤å’Œå…³é”®è§†é¢‘å¸§ã€‚ç°æœ‰çš„åŸºå‡†ä¸»è¦å…³æ³¨ä¸€èˆ¬çš„è¯­ä¹‰çº§è§†é¢‘æ€»ç»“ï¼Œæ— æ³•æ»¡è¶³æ•™å­¦è§†é¢‘ä¸­å¯¹é€æ­¥æŒ‡ä»¤å’Œæ’å›¾çš„éœ€æ±‚ã€‚æˆ‘ä»¬æ”¶é›†äº†2413ä¸ªUIæ•™å­¦è§†é¢‘çš„æ•°æ®é›†ï¼Œæ‰‹åŠ¨æ ‡æ³¨äº†è§†é¢‘åˆ†å‰²ã€æ–‡æœ¬æ€»ç»“å’Œè§†é¢‘æ€»ç»“ï¼Œä»¥ä¾¿è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ€»ç»“æ–¹æ³•åœ¨UIè§†é¢‘æ€»ç»“ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†å¼€å‘æ–°æ–¹æ³•çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12450",
            "title": "Language Surgery in Multilingual Large Language Models",
            "url": "https://huggingface.co/papers/2506.12450",
            "abstract": "Research confirms natural representation alignment in large language models and introduces Inference-Time Language Control to enhance cross-lingual performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and languages, revolutionizing natural language processing. This paper investigates the naturally emerging representation alignment in LLMs, particularly in the middle layers, and its implications for disentangling language-specific and language-agnostic information. We empirically confirm the existence of this alignment, analyze its behavior in comparison to explicitly designed alignment models, and demonstrate its potential for language-specific manipulation without semantic degradation. Building on these findings, we propose Inference-Time Language Control (ITLC), a novel method that leverages latent injection to enable precise cross-lingual language control and mitigate language confusion in LLMs. Our experiments highlight ITLC's strong cross-lingual control capabilities while preserving semantic integrity in target languages. Furthermore, we demonstrate its effectiveness in alleviating the cross-lingual language confusion problem, which persists even in current large-scale LLMs, leading to inconsistent language generation. This work advances our understanding of representation alignment in LLMs and introduces a practical solution for enhancing their cross-lingual performance.",
            "score": 1,
            "issue_id": 4325,
            "pub_date": "2025-06-14",
            "pub_date_card": {
                "ru": "14 Ğ¸ÑĞ½Ñ",
                "en": "June 14",
                "zh": "6æœˆ14æ—¥"
            },
            "hash": "6c05e4a1a8b705dc",
            "authors": [
                "Joanito Agili Lopo",
                "Muhammad Ravi Shulthan Habibi",
                "Tack Hwa Wong",
                "Muhammad Ilham Ghozali",
                "Fajri Koto",
                "Genta Indra Winata",
                "Peerat Limkonchotiwat",
                "Alham Fikri Aji",
                "Samuel Cahyawijaya"
            ],
            "affiliations": [
                "AI Singapore",
                "Capital One",
                "Cohere",
                "Kreasof AI",
                "MBZUAI",
                "SEACrowd",
                "Universitas Indonesia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12450.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#machine_translation",
                    "#inference",
                    "#multilingual"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM), Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Inference-Time Language Control (ITLC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ¶ÑŠÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ITLC Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LLM Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ¸Ñ†Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ITLC Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Cross-Lingual Performance with Natural Representation Alignment",
                    "desc": "This paper explores how large language models (LLMs) naturally align their representations across different languages, particularly in their middle layers. It confirms that this alignment allows for the separation of language-specific and language-agnostic information, which can be manipulated without losing meaning. The authors introduce a new technique called Inference-Time Language Control (ITLC) that uses latent injection to improve control over language generation in cross-lingual contexts. Their experiments show that ITLC effectively reduces language confusion while maintaining semantic integrity, enhancing the overall performance of LLMs in multilingual tasks."
                },
                "zh": {
                    "title": "æå‡è·¨è¯­è¨€æ€§èƒ½çš„æ¨ç†æ—¶è¯­è¨€æ§åˆ¶",
                    "desc": "æœ¬ç ”ç©¶ç¡®è®¤äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­è‡ªç„¶å‡ºç°çš„è¡¨ç¤ºå¯¹é½ç°è±¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­é—´å±‚çš„è¡¨ç°ã€‚æˆ‘ä»¬å®è¯éªŒè¯äº†è¿™ç§å¯¹é½çš„å­˜åœ¨ï¼Œå¹¶åˆ†æäº†å…¶ä¸æ˜¾å¼è®¾è®¡çš„å¯¹é½æ¨¡å‹çš„è¡Œä¸ºæ¯”è¾ƒã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€”â€”æ¨ç†æ—¶è¯­è¨€æ§åˆ¶ï¼ˆITLCï¼‰ï¼Œå®ƒåˆ©ç”¨æ½œåœ¨æ³¨å…¥æŠ€æœ¯å®ç°ç²¾ç¡®çš„è·¨è¯­è¨€æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒITLCåœ¨ä¿æŒç›®æ ‡è¯­è¨€è¯­ä¹‰å®Œæ•´æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†è·¨è¯­è¨€æ§åˆ¶èƒ½åŠ›ï¼Œè§£å†³äº†å½“å‰å¤§å‹LLMsä¸­å­˜åœ¨çš„è¯­è¨€æ··æ·†é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.12189",
            "title": "Supernova Event Dataset: Interpreting Large Language Model's Personality\n  through Critical Event Analysis",
            "url": "https://huggingface.co/papers/2506.12189",
            "abstract": "The study evaluates various LLMs on diverse text tasks using a new dataset, revealing distinct personality traits and improving model interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications.",
            "score": 1,
            "issue_id": 4324,
            "pub_date": "2025-06-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ½Ñ",
                "en": "June 13",
                "zh": "6æœˆ13æ—¥"
            },
            "hash": "952c0d68aa23cbda",
            "authors": [
                "Pranav Agarwal",
                "Ioana CiucÄƒ"
            ],
            "affiliations": [
                "Google Deep Research",
                "Institute",
                "Mila",
                "Quebec AI",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.12189.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#small_models",
                    "#reasoning",
                    "#long_context",
                    "#interpretability",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¸Ñ… Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ñ‡ĞµÑ€Ñ‚Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Supernova Event Dataset, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ 'Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸' ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´ĞµĞ»Ğ°Ñ Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unveiling LLM Personalities for Better Interpretability",
                    "desc": "This study investigates how different Large Language Models (LLMs) perform on various text tasks using a new dataset called the Supernova Event Dataset. The dataset includes a wide range of articles, allowing for the evaluation of LLMs in extracting and ranking key events, which requires complex reasoning and understanding of context. The research reveals distinct personality traits among the models, such as emotional reasoning in Orca 2 and strategic thinking in Qwen 2.5, enhancing our understanding of their decision-making processes. By using another LLM as a judge to assess these traits, the study improves the interpretability of models, making them more accessible for diverse applications."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§ç‰¹å¾",
                    "desc": "æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒæ–‡æœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ã€‚é€šè¿‡åˆ†ææ¨¡å‹çš„é€‰æ‹©å’Œåˆ†ç±»äº‹ä»¶ï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ¨¡å‹çš„ä¸ªæ€§ç‰¹å¾ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬æå‡ºçš„è¶…æ–°æ˜Ÿäº‹ä»¶æ•°æ®é›†åŒ…å«å¤šæ ·çš„æ–‡ç« ï¼Œå¸®åŠ©æˆ‘ä»¬åŸºå‡†æµ‹è¯•æ¨¡å‹åœ¨æå–å’Œæ’åºå…³é”®äº‹ä»¶æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹åœ¨å¤„ç†æƒ…æ„Ÿæ¨ç†ã€æˆ˜ç•¥åˆ†æå’Œå› æœæ¨ç†ç­‰æ–¹é¢è¡¨ç°å‡ºæ˜æ˜¾çš„ä¸ªæ€§å·®å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06366",
            "title": "AI Agent Behavioral Science",
            "url": "https://huggingface.co/papers/2506.06366",
            "abstract": "A new field, AI Agent Behavioral Science, is proposed to systematically study the behaviors of AI agents in diverse contexts, emphasizing external factors and their interactions, and addressing responsible AI aspects.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled the development of AI agents that exhibit increasingly human-like behaviors, including planning, adaptation, and social dynamics across diverse, interactive, and open-ended scenarios. These behaviors are not solely the product of the internal architectures of the underlying models, but emerge from their integration into agentic systems operating within specific contexts, where environmental factors, social cues, and interaction feedbacks shape behavior over time. This evolution necessitates a new scientific perspective: AI Agent Behavioral Science. Rather than focusing only on internal mechanisms, this perspective emphasizes the systematic observation of behavior, design of interventions to test hypotheses, and theory-guided interpretation of how AI agents act, adapt, and interact over time. We systematize a growing body of research across individual agent, multi-agent, and human-agent interaction settings, and further demonstrate how this perspective informs responsible AI by treating fairness, safety, interpretability, accountability, and privacy as behavioral properties. By unifying recent findings and laying out future directions, we position AI Agent Behavioral Science as a necessary complement to traditional model-centric approaches, providing essential tools for understanding, evaluating, and governing the real-world behavior of increasingly autonomous AI systems.",
            "score": 1,
            "issue_id": 4326,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "683be64d015db51c",
            "authors": [
                "Lin Chen",
                "Yunke Zhang",
                "Jie Feng",
                "Haoye Chai",
                "Honglin Zhang",
                "Bingbing Fan",
                "Yibo Ma",
                "Shiyuan Zhang",
                "Nian Li",
                "Tianhui Liu",
                "Nicholas Sukiennik",
                "Keyu Zhao",
                "Yu Li",
                "Ziyi Liu",
                "Fengli Xu",
                "Yong Li"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",
                "Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06366.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#healthcare",
                    "#ethics",
                    "#multimodal",
                    "#agents",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ - Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ°Ñ Ğ½Ğ°ÑƒĞºĞ° Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…, ÑƒĞ´ĞµĞ»ÑÑ Ğ¾ÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼ Ğ¸ Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ."
                },
                "en": {
                    "title": "Understanding AI Behavior: A New Scientific Approach",
                    "desc": "The paper introduces AI Agent Behavioral Science, a new field focused on studying the behaviors of AI agents in various contexts. It highlights that these behaviors arise not just from the AI's internal design but also from interactions with their environment and social dynamics. The approach emphasizes systematic observation, hypothesis testing, and theory-driven analysis to understand how AI agents adapt and interact over time. This perspective also addresses responsible AI considerations, such as fairness and accountability, making it a vital complement to traditional model-centric methods."
                },
                "zh": {
                    "title": "æ¢ç´¢äººå·¥æ™ºèƒ½ä»£ç†çš„è¡Œä¸ºç§‘å­¦",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„é¢†åŸŸâ€”â€”äººå·¥æ™ºèƒ½ä»£ç†è¡Œä¸ºç§‘å­¦ï¼Œæ—¨åœ¨ç³»ç»Ÿç ”ç©¶äººå·¥æ™ºèƒ½ä»£ç†åœ¨ä¸åŒç¯å¢ƒä¸­çš„è¡Œä¸ºã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼ŒAIä»£ç†å±•ç°å‡ºè¶Šæ¥è¶Šäººæ€§åŒ–çš„è¡Œä¸ºï¼Œå¦‚è§„åˆ’ã€é€‚åº”å’Œç¤¾äº¤åŠ¨æ€ã€‚è¿™äº›è¡Œä¸ºä¸ä»…æºäºæ¨¡å‹çš„å†…éƒ¨ç»“æ„ï¼Œè¿˜å—åˆ°ç¯å¢ƒå› ç´ ã€ç¤¾äº¤çº¿ç´¢å’Œäº’åŠ¨åé¦ˆçš„å½±å“ã€‚è¯¥é¢†åŸŸå¼ºè°ƒå¯¹è¡Œä¸ºçš„ç³»ç»Ÿè§‚å¯Ÿå’Œå¹²é¢„è®¾è®¡ï¼Œä»¥ä¿ƒè¿›å¯¹AIä»£ç†è¡Œä¸ºçš„ç†è§£å’Œè´Ÿè´£ä»»çš„åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.13752",
            "title": "Steering LLM Thinking with Budget Guidance",
            "url": "https://huggingface.co/papers/2506.13752",
            "abstract": "Budget guidance is a method that steers LLM reasoning within a targeted budget without fine-tuning and achieves improved efficiency and performance on math benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, a simple yet effective method for steering the reasoning process of LLMs toward a target budget without requiring any LLM fine-tuning. Our approach introduces a lightweight predictor that models a Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in a soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to a 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance.",
            "score": 0,
            "issue_id": 4325,
            "pub_date": "2025-06-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ½Ñ",
                "en": "June 16",
                "zh": "6æœˆ16æ—¥"
            },
            "hash": "c3d9b714e91736d6",
            "authors": [
                "Junyan Li",
                "Wenshuo Zhao",
                "Yang Zhang",
                "Chuang Gan"
            ],
            "affiliations": [
                "MIT-IBM Watson AI Lab",
                "UMass Amherst",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.13752.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training",
                    "#reasoning",
                    "#math"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ˜Ğ˜ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ 'Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°' Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³Ğ°Ğ¼Ğ¼Ğ°-Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞµĞ¹ÑÑ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Steering LLM Reasoning with Budget Guidance for Efficiency and Performance",
                    "desc": "This paper introduces a method called budget guidance, which helps large language models (LLMs) reason effectively within a specified budget of thinking tokens. By using a lightweight predictor that models a Gamma distribution, the method controls the reasoning length during the generation of each token without needing to fine-tune the LLM. This approach not only improves efficiency but also enhances performance on math benchmarks, achieving significant accuracy gains while using fewer tokens. Additionally, budget guidance shows versatility across different tasks and can even estimate the difficulty of questions."
                },
                "zh": {
                    "title": "é¢„ç®—å¼•å¯¼ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "é¢„ç®—å¼•å¯¼æ˜¯ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç›®æ ‡é¢„ç®—å†…è¿›è¡Œæ¨ç†ï¼Œä»è€Œæé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ä¸ªè½»é‡çº§é¢„æµ‹å™¨ï¼Œå»ºæ¨¡å‰©ä½™æ€è€ƒé•¿åº¦çš„ä¼½é©¬åˆ†å¸ƒï¼Œæ¥æ§åˆ¶æ¨ç†é•¿åº¦ã€‚é¢„ç®—å¼•å¯¼ç¡®ä¿ç”Ÿæˆè¿‡ç¨‹éµå¾ªæŒ‡å®šçš„æ€è€ƒé¢„ç®—ï¼ŒåŒæ—¶åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†ä»¤ç‰Œæ•ˆç‡ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨ç´§å¼ é¢„ç®—ä¸‹ï¼Œé¢„ç®—å¼•å¯¼åœ¨MATH-500åŸºå‡†ä¸Šå®ç°äº†é«˜è¾¾26%çš„å‡†ç¡®ç‡æå‡ï¼ŒåŒæ—¶ä»…ä½¿ç”¨å…¨æ€è€ƒæ¨¡å‹63%çš„æ€è€ƒä»¤ç‰Œã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-16.html",
    "link_next": "2025-06-18.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "16.06",
        "en": "06/16",
        "zh": "6æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "18.06",
        "en": "06/18",
        "zh": "6æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 9,
        "#agents": 3,
        "#cv": 0,
        "#rl": 3,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 8,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 6,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 3,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 4,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    }
}