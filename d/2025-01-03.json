{
    "date": {
        "ru": "3 января",
        "en": "January 3",
        "zh": "1月3日"
    },
    "time_utc": "2025-01-03 00:45",
    "weekday": 4,
    "issue_id": 1471,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.19723",
            "title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis",
            "url": "https://huggingface.co/papers/2412.19723",
            "abstract": "Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at https://qiushisun.github.io/OS-Genesis-Home/{OS-Genesis Homepage}.",
            "score": 51,
            "issue_id": 1455,
            "pub_date": "2025-12-27",
            "pub_date_card": {
                "ru": "27 декабря",
                "en": "December 27",
                "zh": "12月27日"
            },
            "hash": "b331198d09aa8650",
            "authors": [
                "Qiushi Sun",
                "Kanzhi Cheng",
                "Zichen Ding",
                "Chuanyang Jin",
                "Yian Wang",
                "Fangzhi Xu",
                "Zhenyu Wu",
                "Chengyou Jia",
                "Liheng Chen",
                "Zhoumianze Liu",
                "Ben Kao",
                "Guohao Li",
                "Junxian He",
                "Yu Qiao",
                "Zhiyong Wu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Johns Hopkins University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The University of Hong Kong",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.19723.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#synthetic",
                    "#dataset",
                    "#optimization",
                    "#training",
                    "#data",
                    "#agents"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Революция в обучении ИИ-агентов: от заданий к исследованию",
                    "desc": "Статья представляет OS-Genesis - новый метод синтеза данных для обучения ИИ-агентов взаимодействию с графическим интерфейсом. Вместо предопределенных задач, агенты сначала исследуют среду и выполняют пошаговые действия, а затем ретроспективно формируют качественные траектории. Используется модель вознаграждения для обеспечения качества сгенерированных траекторий. Результаты показывают значительное улучшение производительности агентов на сложных онлайн-бенчмарках по сравнению с существующими методами."
                },
                "en": {
                    "title": "Revolutionizing GUI Agent Training with OS-Genesis",
                    "desc": "This paper introduces OS-Genesis, a new method for generating high-quality trajectory data for training GUI agents using Vision-Language Models (VLMs). Unlike traditional methods that rely on human supervision or predefined tasks, OS-Genesis allows agents to first interact with their environment and then derive tasks retrospectively. This approach enhances data diversity and quality by enabling agents to explore and learn from real-world interactions. The results show that GUI agents trained with OS-Genesis perform significantly better on challenging benchmarks, demonstrating the effectiveness of this novel data synthesis pipeline."
                },
                "zh": {
                    "title": "OS-Genesis：提升GUI代理性能的新方法",
                    "desc": "本论文提出了一种名为OS-Genesis的新型图形用户界面（GUI）数据合成管道，旨在解决高质量轨迹数据收集的瓶颈。传统方法依赖于人类监督或合成数据生成，往往资源消耗大且数据质量难以保证。OS-Genesis通过让代理先感知环境并进行逐步交互，随后回溯生成高质量任务，从而实现轨迹级探索。实验结果表明，使用OS-Genesis训练的GUI代理在复杂的在线基准测试中表现显著提升，且其数据质量和多样性优于现有合成方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.19638",
            "title": "Xmodel-2 Technical Report",
            "url": "https://huggingface.co/papers/2412.19638",
            "abstract": "Xmodel-2 is a 1.2-billion-parameter large language model designed specifically for reasoning tasks. Its architecture enables different model scales to share a unified set of hyperparameters, allowing for extensive experimentation on smaller models and seamless transfer of optimal configurations to larger models. To maximize training efficiency and stability, Xmodel-2 employs the WSD learning rate scheduler from MiniCPM. Pretrained on 1.5 trillion tokens from diverse sources, Xmodel-2 achieves state-of-the-art performance in complex reasoning and agent-based tasks, while maintaining low training costs. These results highlight the potential of efficient model design and training strategies in advancing reasoning capabilities. Model checkpoints and code are publicly available on GitHub at https://github.com/XiaoduoAILab/Xmodel-2",
            "score": 12,
            "issue_id": 1453,
            "pub_date": "2025-12-27",
            "pub_date_card": {
                "ru": "27 декабря",
                "en": "December 27",
                "zh": "12月27日"
            },
            "hash": "4707dc8ac5a87e66",
            "authors": [
                "Wang Qun",
                "Liu Yang",
                "Lin Qingquan",
                "Qu Zhijiu",
                "Jiang Ling"
            ],
            "affiliations": [
                "AI Lab, Xiaodu Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.19638.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#small_models",
                    "#reasoning",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное рассуждение с Xmodel-2: мощь в компактности",
                    "desc": "Xmodel-2 - это языковая модель с 1,2 миллиардами параметров, специализирующаяся на задачах рассуждения. Её архитектура позволяет разным масштабам модели использовать единый набор гиперпараметров, что облегчает эксперименты и перенос оптимальных конфигураций. Модель использует планировщик скорости обучения WSD из MiniCPM для повышения эффективности и стабильности. Предобученная на 1,5 триллионах токенов, Xmodel-2 достигает передовых результатов в сложных задачах рассуждения, сохраняя низкие затраты на обучение."
                },
                "en": {
                    "title": "Unlocking Reasoning Power with Efficient Model Design",
                    "desc": "Xmodel-2 is a large language model with 1.2 billion parameters, specifically built for reasoning tasks. It features a flexible architecture that allows different model sizes to use the same hyperparameters, facilitating experimentation and optimization across scales. The model utilizes the WSD learning rate scheduler to enhance training efficiency and stability. With pretraining on 1.5 trillion tokens, Xmodel-2 demonstrates superior performance in complex reasoning tasks while keeping training costs low, showcasing the benefits of efficient model design."
                },
                "zh": {
                    "title": "高效推理能力的模型设计与训练策略",
                    "desc": "Xmodel-2 是一个拥有 12 亿参数的大型语言模型，专门设计用于推理任务。它的架构允许不同规模的模型共享统一的超参数，从而可以在较小的模型上进行广泛实验，并将最佳配置无缝转移到更大的模型上。为了最大化训练效率和稳定性，Xmodel-2 采用了 MiniCPM 的 WSD 学习率调度器。经过在 1.5 万亿个来自多样化来源的标记上进行预训练，Xmodel-2 在复杂推理和基于代理的任务中达到了最先进的性能，同时保持了较低的训练成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.20735",
            "title": "HUNYUANPROVER: A Scalable Data Synthesis Framework and Guided Tree Search for Automated Theorem Proving",
            "url": "https://huggingface.co/papers/2412.20735",
            "abstract": "We introduce HunyuanProver, an language model finetuned from the Hunyuan 7B for interactive automatic theorem proving with LEAN4. To alleviate the data sparsity issue, we design a scalable framework to iterative synthesize data with low cost. Besides, guided tree search algorithms are designed to enable effective ``system 2 thinking`` of the prover. HunyuanProver achieves state-of-the-art (SOTA) performances on major benchmarks. Specifically, it achieves a pass of 68.4% on the miniF2F-test compared to 65.9%, the current SOTA results. It proves 4 IMO statements (imo_1960_p2, imo_1962_p2}, imo_1964_p2 and imo_1983_p6) in miniF2F-test. To benefit the community, we will open-source a dataset of 30k synthesized instances, where each instance contains the original question in natural language, the converted statement by autoformalization, and the proof by HunyuanProver.",
            "score": 3,
            "issue_id": 1464,
            "pub_date": "2025-12-30",
            "pub_date_card": {
                "ru": "30 декабря",
                "en": "December 30",
                "zh": "12月30日"
            },
            "hash": "18d70581e862bf86",
            "authors": [
                "Yang Li",
                "Dong Du",
                "Linfeng Song",
                "Chen Li",
                "Weikang Wang",
                "Tao Yang",
                "Haitao Mi"
            ],
            "affiliations": [
                "Tencent",
                "Tencent Hunyuan Teams"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.20735.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#synthetic",
                    "#data",
                    "#benchmark",
                    "#reasoning",
                    "#open_source",
                    "#training",
                    "#math"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прорыв в автоматическом доказательстве теорем с помощью ИИ",
                    "desc": "HunyuanProver - это языковая модель, настроенная для автоматического доказательства теорем с использованием LEAN4. Модель использует масштабируемую структуру для итеративного синтеза данных и алгоритмы направленного поиска по дереву для эффективного 'системного мышления'. HunyuanProver достигает лучших результатов на основных бенчмарках, включая 68.4% прохождения на miniF2F-test. Авторы планируют открыть доступ к набору данных из 30 тысяч синтезированных примеров для пользы сообщества."
                },
                "en": {
                    "title": "HunyuanProver: Advancing Theorem Proving with AI",
                    "desc": "HunyuanProver is a language model specifically fine-tuned for interactive automatic theorem proving using LEAN4. To address the challenge of data sparsity, the authors developed a scalable framework that allows for the iterative synthesis of data at a low cost. They also implemented guided tree search algorithms to enhance the reasoning capabilities of the prover, enabling it to perform complex logical deductions. HunyuanProver has achieved state-of-the-art performance on key benchmarks, including a notable pass rate of 68.4% on the miniF2F-test, surpassing previous results and proving several significant mathematical statements."
                },
                "zh": {
                    "title": "HunyuanProver：自动定理证明的新突破",
                    "desc": "本文介绍了HunyuanProver，这是一个基于Hunyuan 7B微调的语言模型，旨在与LEAN4进行交互式自动定理证明。为了缓解数据稀疏问题，我们设计了一个可扩展的框架，以低成本迭代合成数据。此外，我们还设计了引导树搜索算法，以实现证明者的有效“系统2思维”。HunyuanProver在主要基准测试中达到了最先进的性能，特别是在miniF2F-test中取得了68.4%的通过率，超越了当前的65.9%最先进结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.20800",
            "title": "VMix: Improving Text-to-Image Diffusion Model with Cross-Attention Mixing Control",
            "url": "https://huggingface.co/papers/2412.20800",
            "abstract": "While diffusion models show extraordinary talents in text-to-image generation, they may still fail to generate highly aesthetic images. More specifically, there is still a gap between the generated images and the real-world aesthetic images in finer-grained dimensions including color, lighting, composition, etc. In this paper, we propose Cross-Attention Value Mixing Control (VMix) Adapter, a plug-and-play aesthetics adapter, to upgrade the quality of generated images while maintaining generality across visual concepts by (1) disentangling the input text prompt into the content description and aesthetic description by the initialization of aesthetic embedding, and (2) integrating aesthetic conditions into the denoising process through value-mixed cross-attention, with the network connected by zero-initialized linear layers. Our key insight is to enhance the aesthetic presentation of existing diffusion models by designing a superior condition control method, all while preserving the image-text alignment. Through our meticulous design, VMix is flexible enough to be applied to community models for better visual performance without retraining. To validate the effectiveness of our method, we conducted extensive experiments, showing that VMix outperforms other state-of-the-art methods and is compatible with other community modules (e.g., LoRA, ControlNet, and IPAdapter) for image generation. The project page is https://vmix-diffusion.github.io/VMix/.",
            "score": 2,
            "issue_id": 1471,
            "pub_date": "2025-12-30",
            "pub_date_card": {
                "ru": "30 декабря",
                "en": "December 30",
                "zh": "12月30日"
            },
            "hash": "3389fb4f56a0a59a",
            "authors": [
                "Shaojin Wu",
                "Fei Ding",
                "Mengqi Huang",
                "Wei Liu",
                "Qian He"
            ],
            "affiliations": [
                "ByteDance Inc",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.20800.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#diffusion",
                    "#multimodal"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Улучшение эстетики генеративных моделей без переобучения",
                    "desc": "Статья представляет новый метод под названием Cross-Attention Value Mixing Control (VMix) Adapter для улучшения эстетического качества изображений, генерируемых диффузионными моделями. VMix разделяет текстовый запрос на описание содержания и эстетические характеристики, а затем интегрирует эстетические условия в процесс шумоподавления через смешанное кросс-внимание. Метод может быть применен к существующим моделям без переобучения и совместим с другими модулями, такими как LoRA, ControlNet и IPAdapter. Эксперименты показали, что VMix превосходит другие современные методы в улучшении визуального качества генерируемых изображений."
                },
                "en": {
                    "title": "Enhancing Aesthetic Quality in Diffusion Models with VMix",
                    "desc": "This paper introduces the Cross-Attention Value Mixing Control (VMix) Adapter, which aims to improve the aesthetic quality of images generated by diffusion models. The VMix Adapter works by separating the content and aesthetic descriptions from the input text prompt and incorporating aesthetic conditions into the denoising process. This method enhances the visual appeal of generated images while ensuring that the alignment between images and text prompts is maintained. Extensive experiments demonstrate that VMix outperforms existing methods and can be easily integrated with other community models without the need for retraining."
                },
                "zh": {
                    "title": "提升扩散模型图像美学的创新适配器",
                    "desc": "本论文提出了一种名为Cross-Attention Value Mixing Control (VMix)的美学适配器，旨在提升扩散模型生成图像的美学质量。我们通过将输入文本提示分解为内容描述和美学描述，并在去噪过程中整合美学条件，来实现这一目标。VMix能够在不重新训练的情况下，灵活地应用于社区模型，提升视觉表现。实验结果表明，VMix在图像生成方面优于其他先进方法，并与其他社区模块兼容。"
                }
            }
        }
    ],
    "link_prev": "2025-01-02.html",
    "link_next": "2025-01-06.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "02.01",
        "en": "01/02",
        "zh": "1月2日"
    },
    "short_date_next": {
        "ru": "06.01",
        "en": "01/06",
        "zh": "1月6日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的图形用户界面（GUI）数据合成方法，称为OS-Genesis。它通过让代理首先感知环境并进行步进交互，然后逆向推导出高质量任务，从而解决了传统方法中数据收集困难的问题。OS-Genesis 使用一种轨迹奖励模型来确保生成轨迹的质量。实验结果表明，使用OS-Genesis训练的GUI代理在挑战性在线基准上表现更好。进一步分析验证了OS-Genesis在数据质量和多样性方面的优越性。",
        "title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis",
        "pinyin": "这篇文章介绍了一种新的图形用户界面（GUI）数据合成方法，称为OS-Genesis。它通过让代理首先感知环境并进行步进交互，然后逆向推导出高质量任务，从而解决了传统方法中数据收集困难的问题。OS-Genesis 使用一种轨迹奖励模型来确保生成轨迹的质量。实验结果表明，使用OS-Genesis训练的GUI代理在挑战性在线基准上表现更好。进一步分析验证了OS-Genesis在数据质量和多样性方面的优越性。\n\nZhè piān wénzhāng jièshào le yī zhǒng xīn de túxíng yònghù jiēmiàn (GUI) shùjù héchéng fāngfǎ, chēngwéi OS-Genesis. Tā tōngguò ràng dàilǐ shǒuxiān gǎnzhī huánjìng bìng jìnxíng bùjìn jiāohù, ránhòu nìxiàng tuīdǎo chū gāo zhìliàng rènwù, cóng'ér jiějué le chuántǒng fāngfǎ zhōng shùjù shōucuì kùnnán de wèntí. OS-Genesis shǐyòng yī zhǒng guǐjī jiǎnglì móxíng lái quèbǎo shēngchéng guǐjī de zhìliàng. Shíyàn jiéguǒ biǎomíng, shǐyòng OS-Genesis xùnliàn de GUI dàilǐ zài tiǎozhànxìng zàixiàn jīzhǔn shàng biǎoxiàn gèng hǎo. Jìn yībù fēnxi yànzhèng le OS-Genesis zài shùjù zhìliàng hé duōyàngxìng fāngmiàn de yōuyuèxìng.",
        "vocab": "[{'word': '图形用户界面', 'pinyin': 'tú xíng yòng hù jiē miàn', 'trans': 'graphical user interface'},\n{'word': '数据合成', 'pinyin': 'shù jù hé chéng', 'trans': 'data synthesis'},\n{'word': '代理', 'pinyin': 'dài lǐ', 'trans': 'agent'},\n{'word': '感知', 'pinyin': 'gǎn zhī', 'trans': 'perceive'},\n{'word': '步进', 'pinyin': 'bù jìn', 'trans': 'stepwise'},\n{'word': '逆向', 'pinyin': 'nì xiàng', 'trans': 'reverse'},\n{'word': '推导', 'pinyin': 'tuī dǎo', 'trans': 'deduce'},\n{'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'},\n{'word': '轨迹', 'pinyin': 'guǐ jì', 'trans': 'trajectory'},\n{'word': '奖励', 'pinyin': 'jiǎng lì', 'trans': 'reward'},\n{'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'},\n{'word': '确保', 'pinyin': 'què bǎo', 'trans': 'ensure'},\n{'word': '挑战性', 'pinyin': 'tiǎo zhàn xìng', 'trans': 'challenging'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '多样性', 'pinyin': 'duō yàng xìng', 'trans': 'diversity'},\n{'word': '优越性', 'pinyin': 'yōu yuè xìng', 'trans': 'superiority'}]",
        "trans": "This article introduces a new method for synthesizing Graphical User Interface (GUI) data, called OS-Genesis. It addresses the challenges of data collection in traditional methods by having agents first perceive the environment and engage in step-by-step interactions, then retroactively deduce high-quality tasks. OS-Genesis employs a trajectory reward model to ensure the quality of the generated trajectories. Experimental results show that GUI agents trained using OS-Genesis perform better on challenging online benchmarks. Further analysis validates the superiority of OS-Genesis in terms of data quality and diversity.",
        "update_ts": "2025-01-02 09:10"
    }
}