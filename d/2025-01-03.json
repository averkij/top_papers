{
    "date": {
        "ru": "3 января",
        "en": "January 3",
        "zh": "1月3日"
    },
    "time_utc": "2025-01-03 03:15",
    "weekday": 4,
    "issue_id": 1473,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.01264",
            "title": "ProgCo: Program Helps Self-Correction of Large Language Models",
            "url": "https://huggingface.co/papers/2501.01264",
            "abstract": "Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then, program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools.",
            "score": 5,
            "issue_id": 1473,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "bda3f96e83319526",
            "authors": [
                "Xiaoshuai Song",
                "Yanan Wu",
                "Weixun Wang",
                "Jiaheng Liu",
                "Wenbo Su",
                "Bo Zheng"
            ],
            "affiliations": [
                "Taobao & Tmall Group of Alibaba"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01264.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#reasoning",
                    "#interpretability",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "ProgCo: Самокоррекция языковых моделей через программно-управляемую верификацию и уточнение",
                    "desc": "Эта статья представляет новый подход к самокоррекции больших языковых моделей (LLM) под названием Program-driven Self-Correction (ProgCo). Метод включает в себя программно-управляемую верификацию (ProgVe), которая использует самогенерируемые и самовыполняющиеся псевдопрограммы для сложной логики проверки. Затем программно-управляемое уточнение (ProgRe) проводит двойную рефлексию и улучшение как ответов, так и программ верификации. Эксперименты показали, что ProgCo эффективен в самокоррекции и может дополнительно улучшить производительность при комбинировании с реальными программными инструментами."
                },
                "en": {
                    "title": "Empowering LLMs with Program-Driven Self-Correction",
                    "desc": "This paper introduces Program-driven Self-Correction (ProgCo) to improve the self-verification and self-refinement capabilities of large language models (LLMs). It addresses the common issue where LLMs struggle to provide accurate feedback, which can lead to incorrect refinements, particularly in complex reasoning tasks. ProgCo utilizes program-driven verification (ProgVe) to create self-executing verification pseudo-programs that enhance the verification process. Additionally, program-driven refinement (ProgRe) allows the model to reflect on and refine both its responses and the verification programs, leading to more reliable self-correction outcomes."
                },
                "zh": {
                    "title": "基于程序的自我纠正：提升语言模型的自我验证能力",
                    "desc": "自我纠正旨在使大型语言模型（LLMs）能够在没有外部反馈的情况下自我验证和自我完善其初始响应。然而，LLMs往往无法有效自我验证并生成正确的反馈，这会进一步误导其完善过程，尤其是在复杂推理任务中。本文提出了基于程序的自我纠正（ProgCo），通过自生成、自执行的验证伪程序实现复杂的验证逻辑和广泛的验证。实验结果表明，ProgCo在三个指令遵循和数学基准测试中实现了有效的自我纠正，并且与真实程序工具结合时可以进一步提升性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01423",
            "title": "Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models",
            "url": "https://huggingface.co/papers/2501.01423",
            "abstract": "Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochs--representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: https://github.com/hustvl/LightningDiT.",
            "score": 0,
            "issue_id": 1473,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 января",
                "en": "January 2",
                "zh": "1月2日"
            },
            "hash": "173fa21b6e47d04c",
            "authors": [
                "Jingfeng Yao",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01423.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Революция в латентных диффузионных моделях: быстрее, лучше, эффективнее",
                    "desc": "Статья представляет новый подход к улучшению латентных диффузионных моделей с архитектурой Трансформер для генерации изображений высокого качества. Авторы предлагают метод VA-VAE, который выравнивает латентное пространство с предобученными моделями компьютерного зрения. Это позволяет значительно расширить границы реконструкции-генерации и ускорить сходимость Диффузионных Трансформеров в высокоразмерных латентных пространствах. На основе VA-VAE авторы создали улучшенную модель LightningDiT, достигающую современного уровня производительности на задаче генерации изображений ImageNet 256x256."
                },
                "en": {
                    "title": "Accelerating Image Generation with Aligned Latent Spaces",
                    "desc": "This paper discusses the challenges faced by latent diffusion models, particularly when using Transformer architectures for image generation. It highlights an optimization issue where increasing the feature dimensions in visual tokenizers can lead to larger models and longer training times, often resulting in sub-optimal image quality. The authors propose a solution by aligning the latent space with pre-trained vision models, introducing a new framework called VA-VAE to enhance the training process. Their improved model, LightningDiT, achieves state-of-the-art performance in image generation while significantly speeding up the training process."
                },
                "zh": {
                    "title": "优化潜在扩散模型，提升图像生成效率",
                    "desc": "本论文探讨了潜在扩散模型与变换器架构在生成高质量图像时的优化困境。研究表明，虽然增加视觉标记器中的每个标记特征维度可以提高重建质量，但这也导致需要更大的扩散模型和更多的训练迭代。为了解决这一问题，作者提出将潜在空间与预训练的视觉基础模型对齐，从而提高训练效率。最终，提出的VA-VAE模型显著提升了潜在扩散模型的重建生成能力，并在ImageNet数据集上实现了最先进的性能。"
                }
            }
        }
    ],
    "link_prev": "2025-01-02.html",
    "link_next": "2025-01-06.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "02.01",
        "en": "01/02",
        "zh": "1月2日"
    },
    "short_date_next": {
        "ru": "06.01",
        "en": "01/06",
        "zh": "1月6日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的图形用户界面（GUI）数据合成方法，称为OS-Genesis。它通过让代理首先感知环境并进行步进交互，然后逆向推导出高质量任务，从而解决了传统方法中数据收集困难的问题。OS-Genesis 使用一种轨迹奖励模型来确保生成轨迹的质量。实验结果表明，使用OS-Genesis训练的GUI代理在挑战性在线基准上表现更好。进一步分析验证了OS-Genesis在数据质量和多样性方面的优越性。",
        "title": "OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis",
        "pinyin": "这篇文章介绍了一种新的图形用户界面（GUI）数据合成方法，称为OS-Genesis。它通过让代理首先感知环境并进行步进交互，然后逆向推导出高质量任务，从而解决了传统方法中数据收集困难的问题。OS-Genesis 使用一种轨迹奖励模型来确保生成轨迹的质量。实验结果表明，使用OS-Genesis训练的GUI代理在挑战性在线基准上表现更好。进一步分析验证了OS-Genesis在数据质量和多样性方面的优越性。\n\nZhè piān wénzhāng jièshào le yī zhǒng xīn de túxíng yònghù jiēmiàn (GUI) shùjù héchéng fāngfǎ, chēngwéi OS-Genesis. Tā tōngguò ràng dàilǐ shǒuxiān gǎnzhī huánjìng bìng jìnxíng bùjìn jiāohù, ránhòu nìxiàng tuīdǎo chū gāo zhìliàng rènwù, cóng'ér jiějué le chuántǒng fāngfǎ zhōng shùjù shōucuì kùnnán de wèntí. OS-Genesis shǐyòng yī zhǒng guǐjī jiǎnglì móxíng lái quèbǎo shēngchéng guǐjī de zhìliàng. Shíyàn jiéguǒ biǎomíng, shǐyòng OS-Genesis xùnliàn de GUI dàilǐ zài tiǎozhànxìng zàixiàn jīzhǔn shàng biǎoxiàn gèng hǎo. Jìn yībù fēnxi yànzhèng le OS-Genesis zài shùjù zhìliàng hé duōyàngxìng fāngmiàn de yōuyuèxìng.",
        "vocab": "[{'word': '图形用户界面', 'pinyin': 'tú xíng yòng hù jiē miàn', 'trans': 'graphical user interface'},\n{'word': '数据合成', 'pinyin': 'shù jù hé chéng', 'trans': 'data synthesis'},\n{'word': '代理', 'pinyin': 'dài lǐ', 'trans': 'agent'},\n{'word': '感知', 'pinyin': 'gǎn zhī', 'trans': 'perceive'},\n{'word': '步进', 'pinyin': 'bù jìn', 'trans': 'stepwise'},\n{'word': '逆向', 'pinyin': 'nì xiàng', 'trans': 'reverse'},\n{'word': '推导', 'pinyin': 'tuī dǎo', 'trans': 'deduce'},\n{'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'},\n{'word': '轨迹', 'pinyin': 'guǐ jì', 'trans': 'trajectory'},\n{'word': '奖励', 'pinyin': 'jiǎng lì', 'trans': 'reward'},\n{'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'},\n{'word': '确保', 'pinyin': 'què bǎo', 'trans': 'ensure'},\n{'word': '挑战性', 'pinyin': 'tiǎo zhàn xìng', 'trans': 'challenging'},\n{'word': '基准', 'pinyin': 'jī zhǔn', 'trans': 'benchmark'},\n{'word': '多样性', 'pinyin': 'duō yàng xìng', 'trans': 'diversity'},\n{'word': '优越性', 'pinyin': 'yōu yuè xìng', 'trans': 'superiority'}]",
        "trans": "This article introduces a new method for synthesizing Graphical User Interface (GUI) data, called OS-Genesis. It addresses the challenges of data collection in traditional methods by having agents first perceive the environment and engage in step-by-step interactions, then retroactively deduce high-quality tasks. OS-Genesis employs a trajectory reward model to ensure the quality of the generated trajectories. Experimental results show that GUI agents trained using OS-Genesis perform better on challenging online benchmarks. Further analysis validates the superiority of OS-Genesis in terms of data quality and diversity.",
        "update_ts": "2025-01-02 09:10"
    }
}