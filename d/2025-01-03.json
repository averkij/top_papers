{
    "date": {
        "ru": "3 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 3",
        "zh": "1æœˆ3æ—¥"
    },
    "time_utc": "2025-01-03 13:16",
    "weekday": 4,
    "issue_id": 1483,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2501.00958",
            "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining",
            "url": "https://huggingface.co/papers/2501.00958",
            "abstract": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality multimodal textbook corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving~Our code are available at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}.",
            "score": 32,
            "issue_id": 1475,
            "pub_date": "2025-01-01",
            "pub_date_card": {
                "ru": "1 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 1",
                "zh": "1æœˆ1æ—¥"
            },
            "hash": "b10f0cd62f6334fc",
            "authors": [
                "Wenqi Zhang",
                "Hang Zhang",
                "Xin Li",
                "Jiashuo Sun",
                "Yongliang Shen",
                "Weiming Lu",
                "Deli Zhao",
                "Yueting Zhuang",
                "Lidong Bing"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University",
                "DAMO Academy, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00958.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#cv",
                    "#video"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ‡ĞµĞ±Ğ½Ğ¸Ğº: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ VLM",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 22 000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Harnessing Instructional Videos for Superior Vision-Language Model Training",
                    "desc": "This paper presents a new approach to training Vision-Language Models (VLMs) using a multimodal textbook corpus derived from instructional videos. Unlike traditional datasets that often suffer from low knowledge density and weak image-text relationships, this corpus offers a richer and more coherent context for VLM pretraining. The authors systematically extract visual, audio, and textual information from over 22,000 hours of instructional content, enhancing the alignment between images and text. Experiments show that VLMs trained on this video-centric dataset perform significantly better on knowledge-intensive tasks, demonstrating improved reasoning and context awareness."
                },
                "zh": {
                    "title": "è§†é¢‘æ•™æï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜è´¨é‡çš„å¤šæ¨¡æ€æ•™æè¯­æ–™åº“ï¼Œæ—¨åœ¨ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æä¾›æ›´ä¸°å¯Œçš„åŸºç¡€çŸ¥è¯†ã€‚è¯¥è¯­æ–™åº“æ”¶é›†äº†è¶…è¿‡2.5å¹´çš„æ•™å­¦è§†é¢‘ï¼Œæ€»è®¡22,000å°æ—¶ï¼Œç³»ç»Ÿæ€§åœ°æå–äº†è§†é¢‘ä¸­çš„è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬çŸ¥è¯†ã€‚ä¸ç°æœ‰çš„æ•°æ®é›†ç›¸æ¯”ï¼Œè¿™ç§è§†é¢‘ä¸­å¿ƒçš„æ•™ææä¾›äº†æ›´è¿è´¯çš„ä¸Šä¸‹æ–‡ã€æ›´ä¸°å¯Œçš„çŸ¥è¯†å’Œæ›´å¥½çš„å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºè¯¥æ•™æé¢„è®­ç»ƒçš„VLMåœ¨çŸ¥è¯†å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨ScienceQAå’ŒMathVistaç­‰ä»»åŠ¡ä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01427",
            "title": "VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control",
            "url": "https://huggingface.co/papers/2501.01427",
            "abstract": "Despite significant advancements in video generation, inserting a given object into videos remains a challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motions at the same time. In this paper, we propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from a text-to-video model, we utilize an ID extractor to inject the global identity and leverage a box sequence to control the overall motion. To preserve the detailed appearance and meanwhile support fine-grained motion control, we design a pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating the motion trajectories. In addition, we propose a training strategy involving both videos and static images with a reweight reconstruction loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications (e.g., talking head generation, video virtual try-on, multi-region editing) without task-specific fine-tuning.",
            "score": 28,
            "issue_id": 1474,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 2",
                "zh": "1æœˆ2æ—¥"
            },
            "hash": "4c67f688775a3eca",
            "authors": [
                "Yuanpeng Tu",
                "Hao Luo",
                "Xi Chen",
                "Sihui Ji",
                "Xiang Bai",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "HUST",
                "Hupan Lab",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01427.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#games",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ VideoAnydoor - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¿ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Seamless Object Insertion in Videos with VideoAnydoor",
                    "desc": "This paper introduces VideoAnydoor, a novel framework for zero-shot video object insertion that excels in maintaining high-fidelity details and precise motion control. The approach begins with a text-to-video model and incorporates an ID extractor to ensure consistent object identity while using a box sequence for motion management. A key innovation is the pixel warper, which adjusts pixel details based on key-point trajectories, enhancing both detail preservation and user control over motion. The proposed training strategy, which combines videos and static images with a reweighted reconstruction loss, significantly improves the quality of object insertion, making VideoAnydoor versatile for various applications without needing specific fine-tuning."
                },
                "zh": {
                    "title": "é«˜ä¿çœŸè§†é¢‘å¯¹è±¡æ’å…¥çš„æ–°çªç ´",
                    "desc": "å°½ç®¡è§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†ç‰¹å®šå¯¹è±¡æ’å…¥è§†é¢‘ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†VideoAnydoorï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶-shotè§†é¢‘å¯¹è±¡æ’å…¥æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜ä¿çœŸåœ°ä¿ç•™ç»†èŠ‚å¹¶ç²¾ç¡®æ§åˆ¶è¿åŠ¨ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åƒç´ å˜å½¢å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®å…³é”®ç‚¹è½¨è¿¹æ‰­æ›²åƒç´ ç»†èŠ‚ï¼Œå¹¶ä¸æ‰©æ•£U-Netèåˆï¼Œä»è€Œæé«˜ç»†èŠ‚ä¿ç•™èƒ½åŠ›ã€‚VideoAnydooråœ¨ç°æœ‰æ–¹æ³•ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶æ”¯æŒå¤šç§ä¸‹æ¸¸åº”ç”¨ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01257",
            "title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings",
            "url": "https://huggingface.co/papers/2501.01257",
            "abstract": "With the increasing code reasoning capabilities of existing large language models (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3, there is a growing need to develop more challenging and comprehensive benchmarks that effectively test their sophisticated competition-level coding abilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to the unavailability of private test cases, lack of support for special judges, and misaligned execution environments. To bridge this gap, we introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. We compile the recent six months of contest problems on CodeForces with detailed information such as contest divisions, problem difficulty ratings, and problem algorithm tags. We introduce a unique judging method in which problems are submitted directly to the platform and develop a reliable Elo rating calculation system that aligns with the platform and is comparable with human participants but has lower variance. By testing on our CodeElo, we provide the Elo ratings of 30 existing popular open-source and 3 proprietary LLMs for the first time. The results show that o1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of 1578 and 1261, respectively, while other models struggle even with the easiest problems, placing in the lowest 20 percent among all human participants. Detailed analysis experiments are also conducted to provide insights into performance across algorithms and comparisons between using C++ and Python, which can suggest directions for future studies.",
            "score": 24,
            "issue_id": 1475,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 2",
                "zh": "1æœˆ2æ—¥"
            },
            "hash": "e31430bb6ba5dfc8",
            "authors": [
                "Shanghaoran Quan",
                "Jiaxi Yang",
                "Bowen Yu",
                "Bo Zheng",
                "Dayiheng Liu",
                "An Yang",
                "Xuancheng Ren",
                "Bofei Gao",
                "Yibo Miao",
                "Yunlong Feng",
                "Zekun Wang",
                "Jian Yang",
                "Zeyu Cui",
                "Yang Fan",
                "Yichang Zhang",
                "Binyuan Hui",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01257.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#reasoning",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸ†",
                "ru": {
                    "title": "CodeElo: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CodeElo Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. CodeElo Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ CodeForces Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞ³Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ° Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ­Ğ»Ğ¾, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ñ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 33 LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ o1-mini Ğ¸ QwQ-32B-Preview Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² 1578 Ğ¸ 1261 ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "CodeElo: Elevating Code Generation Benchmarks for LLMs",
                    "desc": "This paper presents CodeElo, a new benchmark designed to evaluate the coding abilities of large language models (LLMs) in a competitive setting. Unlike existing benchmarks, CodeElo addresses limitations such as the lack of private test cases and misaligned execution environments by utilizing the CodeForces platform. The benchmark includes a unique judging method and an Elo rating system that allows for fair comparisons between LLMs and human participants. Results indicate that certain models, like o1-mini, perform significantly better than others, highlighting the varying capabilities of LLMs in code generation tasks."
                },
                "zh": {
                    "title": "CodeEloï¼šæå‡ä»£ç ç”Ÿæˆèƒ½åŠ›çš„æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç æ¨ç†èƒ½åŠ›ä¸Šçš„æå‡ï¼Œå¼€å‘æ›´å…·æŒ‘æˆ˜æ€§å’Œå…¨é¢æ€§çš„åŸºå‡†æµ‹è¯•å˜å¾—æ„ˆå‘é‡è¦ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¦‚LiveCodeBenchå’ŒUSACOå­˜åœ¨ä¸€äº›ä¸è¶³ï¼Œä¾‹å¦‚ç¼ºä¹ç§æœ‰æµ‹è¯•ç”¨ä¾‹å’Œç‰¹æ®Šè¯„åˆ¤æ”¯æŒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CodeEloï¼Œè¿™æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–çš„ç«èµ›çº§ä»£ç ç”ŸæˆåŸºå‡†ï¼Œé¦–æ¬¡æœ‰æ•ˆåº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚é€šè¿‡åœ¨CodeForceså¹³å°ä¸Šç¼–è¯‘æœ€è¿‘å…­ä¸ªæœˆçš„ç«èµ›é—®é¢˜ï¼Œæˆ‘ä»¬ä¸º30ä¸ªæµè¡Œçš„å¼€æºå’Œ3ä¸ªä¸“æœ‰LLMsæä¾›äº†Eloè¯„åˆ†ï¼Œç»“æœæ˜¾ç¤ºo1-miniå’ŒQwQ-32B-Previewè¡¨ç°çªå‡ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00599",
            "title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM",
            "url": "https://huggingface.co/papers/2501.00599",
            "abstract": "Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding. However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and a comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent data engine to meticulously curate a large-scale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips a versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create a VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of a Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities.",
            "score": 24,
            "issue_id": 1474,
            "pub_date": "2025-12-31",
            "pub_date_card": {
                "ru": "31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 31",
                "zh": "12æœˆ31æ—¥"
            },
            "hash": "daee687ce36ef3db",
            "authors": [
                "Yuqian Yuan",
                "Hang Zhang",
                "Wentong Li",
                "Zesen Cheng",
                "Boqiang Zhang",
                "Long Li",
                "Xin Li",
                "Deli Zhao",
                "Wenqiao Zhang",
                "Yueting Zhuang",
                "Jianke Zhu",
                "Lidong Bing"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00599.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#optimization",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VideoRefer Suite",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoRefer Suite - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VideoRefer-700K Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VideoRefer Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VideoRefer-Bench, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Empowering Video LLMs for Fine-Grained Understanding",
                    "desc": "This paper introduces the VideoRefer Suite, which enhances Video Large Language Models (Video LLMs) for better understanding of videos by focusing on fine-grained spatial and temporal details. It addresses the limitations of existing models that primarily focus on overall comprehension and lack high-quality object-level instruction data. The suite includes a new dataset called VideoRefer-700K, a specialized VideoRefer model with a spatial-temporal object encoder, and a benchmark for evaluating video understanding capabilities. Experimental results show that the VideoRefer model significantly improves performance on video referring tasks while also enhancing general video comprehension."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç†è§£ï¼Œç»†è‡´æ•æ‰ç©ºé—´ä¸æ—¶é—´",
                    "desc": "è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å±•ç°äº†å‡ºè‰²çš„èƒ½åŠ›ï¼Œä½†åœ¨æ•æ‰ç»†ç²’åº¦çš„ç©ºé—´å’Œæ—¶é—´ç»†èŠ‚ä¸Šå­˜åœ¨å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†VideoRefer Suiteï¼Œä»¥å¢å¼ºè§†é¢‘LLMåœ¨ç©ºé—´-æ—¶é—´è§†é¢‘ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šä»£ç†æ•°æ®å¼•æ“ï¼Œåˆ›å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„å¯¹è±¡çº§è§†é¢‘æŒ‡ä»¤æ•°æ®é›†VideoRefer-700Kï¼Œå¹¶æå‡ºäº†VideoReferæ¨¡å‹ï¼Œé…å¤‡äº†å¤šåŠŸèƒ½çš„ç©ºé—´-æ—¶é—´å¯¹è±¡ç¼–ç å™¨ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºäº†VideoRefer-Benchï¼Œä»¥å…¨é¢è¯„ä¼°è§†é¢‘LLMçš„ç©ºé—´-æ—¶é—´ç†è§£èƒ½åŠ›ï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹åœ¨è§†é¢‘å¼•ç”¨åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01423",
            "title": "Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models",
            "url": "https://huggingface.co/papers/2501.01423",
            "abstract": "Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochs--representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: https://github.com/hustvl/LightningDiT.",
            "score": 22,
            "issue_id": 1473,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 2",
                "zh": "1æœˆ2æ—¥"
            },
            "hash": "173fa21b6e47d04c",
            "authors": [
                "Jingfeng Yao",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01423.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ğ»ÑƒÑ‡ÑˆĞµ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ VA-VAE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VA-VAE Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LightningDiT, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰ÑƒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ImageNet 256x256."
                },
                "en": {
                    "title": "Accelerating Image Generation with Aligned Latent Spaces",
                    "desc": "This paper discusses the challenges faced by latent diffusion models, particularly when using Transformer architectures for image generation. It highlights an optimization issue where increasing the feature dimensions in visual tokenizers can lead to larger models and longer training times, often resulting in sub-optimal image quality. The authors propose a solution by aligning the latent space with pre-trained vision models, introducing a new framework called VA-VAE to enhance the training process. Their improved model, LightningDiT, achieves state-of-the-art performance in image generation while significantly speeding up the training process."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œæå‡å›¾åƒç”Ÿæˆæ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸å˜æ¢å™¨æ¶æ„åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ—¶çš„ä¼˜åŒ–å›°å¢ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œè™½ç„¶å¢åŠ è§†è§‰æ ‡è®°å™¨ä¸­çš„æ¯ä¸ªæ ‡è®°ç‰¹å¾ç»´åº¦å¯ä»¥æé«˜é‡å»ºè´¨é‡ï¼Œä½†è¿™ä¹Ÿå¯¼è‡´éœ€è¦æ›´å¤§çš„æ‰©æ•£æ¨¡å‹å’Œæ›´å¤šçš„è®­ç»ƒè¿­ä»£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºå°†æ½œåœ¨ç©ºé—´ä¸é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚æœ€ç»ˆï¼Œæå‡ºçš„VA-VAEæ¨¡å‹æ˜¾è‘—æå‡äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é‡å»ºç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶åœ¨ImageNetæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01264",
            "title": "ProgCo: Program Helps Self-Correction of Large Language Models",
            "url": "https://huggingface.co/papers/2501.01264",
            "abstract": "Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then, program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools.",
            "score": 16,
            "issue_id": 1473,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 2",
                "zh": "1æœˆ2æ—¥"
            },
            "hash": "bda3f96e83319526",
            "authors": [
                "Xiaoshuai Song",
                "Yanan Wu",
                "Weixun Wang",
                "Jiaheng Liu",
                "Wenbo Su",
                "Bo Zheng"
            ],
            "affiliations": [
                "Taobao & Tmall Group of Alibaba"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01264.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#reasoning",
                    "#interpretability",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ProgCo: Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Program-driven Self-Correction (ProgCo). ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ (ProgVe), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸ĞµÑÑ Ğ¿ÑĞµĞ²Ğ´Ğ¾Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ (ProgRe) Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ProgCo ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering LLMs with Program-Driven Self-Correction",
                    "desc": "This paper introduces Program-driven Self-Correction (ProgCo) to improve the self-verification and self-refinement capabilities of large language models (LLMs). It addresses the common issue where LLMs struggle to provide accurate feedback, which can lead to incorrect refinements, particularly in complex reasoning tasks. ProgCo utilizes program-driven verification (ProgVe) to create self-executing verification pseudo-programs that enhance the verification process. Additionally, program-driven refinement (ProgRe) allows the model to reflect on and refine both its responses and the verification programs, leading to more reliable self-correction outcomes."
                },
                "zh": {
                    "title": "åŸºäºç¨‹åºçš„è‡ªæˆ‘çº æ­£ï¼šæå‡è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘éªŒè¯èƒ½åŠ›",
                    "desc": "è‡ªæˆ‘çº æ­£æ—¨åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿåœ¨æ²¡æœ‰å¤–éƒ¨åé¦ˆçš„æƒ…å†µä¸‹è‡ªæˆ‘éªŒè¯å’Œè‡ªæˆ‘å®Œå–„å…¶åˆå§‹å“åº”ã€‚ç„¶è€Œï¼ŒLLMså¾€å¾€æ— æ³•æœ‰æ•ˆè‡ªæˆ‘éªŒè¯å¹¶ç”Ÿæˆæ­£ç¡®çš„åé¦ˆï¼Œè¿™ä¼šè¿›ä¸€æ­¥è¯¯å¯¼å…¶å®Œå–„è¿‡ç¨‹ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºç¨‹åºçš„è‡ªæˆ‘çº æ­£ï¼ˆProgCoï¼‰ï¼Œé€šè¿‡è‡ªç”Ÿæˆã€è‡ªæ‰§è¡Œçš„éªŒè¯ä¼ªç¨‹åºå®ç°å¤æ‚çš„éªŒè¯é€»è¾‘å’Œå¹¿æ³›çš„éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProgCoåœ¨ä¸‰ä¸ªæŒ‡ä»¤éµå¾ªå’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ‰æ•ˆçš„è‡ªæˆ‘çº æ­£ï¼Œå¹¶ä¸”ä¸çœŸå®ç¨‹åºå·¥å…·ç»“åˆæ—¶å¯ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01149",
            "title": "A3: Android Agent Arena for Mobile GUI Agents",
            "url": "https://huggingface.co/papers/2501.01149",
            "abstract": "AI agents have become increasingly prevalent in recent years, driven by significant advancements in the field of large language models (LLMs). Mobile GUI agents, a subset of AI agents, are designed to autonomously perform tasks on mobile devices. While numerous studies have introduced agents, datasets, and benchmarks to advance mobile GUI agent research, many existing datasets focus on static frame evaluations and fail to provide a comprehensive platform for assessing performance on real-world, in-the-wild tasks. To address this gap, we present Android Agent Arena (A3), a novel evaluation platform. Unlike existing in-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as real-time online information retrieval and operational instructions; (2) a larger, more flexible action space, enabling compatibility with agents trained on any dataset; and (3) automated business-level LLM-based evaluation process. A3 includes 21 widely used general third-party apps and 201 tasks representative of common user scenarios, providing a robust foundation for evaluating mobile GUI agents in real-world situations and a new autonomous evaluation process for less human labor and coding expertise. The project is available at https://yuxiangchai.github.io/Android-Agent-Arena/.",
            "score": 12,
            "issue_id": 1474,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 2",
                "zh": "1æœˆ2æ—¥"
            },
            "hash": "050f155aa526c100",
            "authors": [
                "Yuxiang Chai",
                "Hanhao Li",
                "Jiayu Zhang",
                "Liang Liu",
                "Guozhi Wang",
                "Shuai Ren",
                "Siyuan Huang",
                "Hongsheng Li"
            ],
            "affiliations": [
                "EE department @ CUHK",
                "MMLab @ CUHK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01149.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "A3: ĞÑ€ĞµĞ½Ğ° Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Android Agent Arena (A3). A3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 21 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ 201 Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰ÑƒÑ Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸. A3 Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ ĞµÑ‘ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Mobile GUI Agent Evaluation with A3",
                    "desc": "This paper introduces the Android Agent Arena (A3), a new evaluation platform for mobile GUI agents that addresses limitations in existing datasets. A3 focuses on real-world tasks, providing a larger action space that accommodates agents trained on various datasets. It features 21 popular third-party apps and 201 tasks that reflect common user scenarios, enhancing the assessment of agent performance. Additionally, A3 incorporates an automated evaluation process using large language models, reducing the need for extensive human involvement and coding skills."
                },
                "zh": {
                    "title": "Android Agent Arenaï¼šç§»åŠ¨GUIä»£ç†çš„æ–°è¯„ä¼°å¹³å°",
                    "desc": "è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½ä»£ç†çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œå°¤å…¶æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸçš„è¿›æ­¥æ¨åŠ¨ä¸‹ã€‚ç§»åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ˜¯äººå·¥æ™ºèƒ½ä»£ç†çš„ä¸€ç§ï¼Œæ—¨åœ¨è‡ªä¸»æ‰§è¡Œç§»åŠ¨è®¾å¤‡ä¸Šçš„ä»»åŠ¡ã€‚ç°æœ‰çš„ç ”ç©¶è™½ç„¶æå‡ºäº†è®¸å¤šä»£ç†ã€æ•°æ®é›†å’ŒåŸºå‡†ï¼Œä½†å¤§å¤šæ•°æ•°æ®é›†ä»…å…³æ³¨é™æ€æ¡†æ¶è¯„ä¼°ï¼Œæ— æ³•å…¨é¢è¯„ä¼°çœŸå®ä¸–ç•Œä¸­çš„ä»»åŠ¡è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Android Agent Arenaï¼ˆA3ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„è¯„ä¼°å¹³å°ï¼Œæä¾›äº†å®é™…çš„ä»»åŠ¡å’Œæ›´çµæ´»çš„æ“ä½œç©ºé—´ï¼Œæ”¯æŒåŸºäºLLMçš„è‡ªåŠ¨åŒ–è¯„ä¼°è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00316",
            "title": "MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models",
            "url": "https://huggingface.co/papers/2501.00316",
            "abstract": "Recent advancements in foundation models have enhanced AI systems' capabilities in autonomous tool usage and reasoning. However, their ability in location or map-based reasoning - which improves daily life by optimizing navigation, facilitating resource discovery, and streamlining logistics - has not been systematically studied. To bridge this gap, we introduce MapEval, a benchmark designed to assess diverse and complex map-based user queries with geo-spatial reasoning. MapEval features three task types (textual, API-based, and visual) that require collecting world information via map tools, processing heterogeneous geo-spatial contexts (e.g., named entities, travel distances, user reviews or ratings, images), and compositional reasoning, which all state-of-the-art foundation models find challenging. Comprising 700 unique multiple-choice questions about locations across 180 cities and 54 countries, MapEval evaluates foundation models' ability to handle spatial relationships, map infographics, travel planning, and navigation challenges. Using MapEval, we conducted a comprehensive evaluation of 28 prominent foundation models. While no single model excelled across all tasks, Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro achieved competitive performance overall. However, substantial performance gaps emerged, particularly in MapEval, where agents with Claude-3.5-Sonnet outperformed GPT-4o and Gemini-1.5-Pro by 16% and 21%, respectively, and the gaps became even more amplified when compared to open-source LLMs. Our detailed analyses provide insights into the strengths and weaknesses of current models, though all models still fall short of human performance by more than 20% on average, struggling with complex map images and rigorous geo-spatial reasoning. This gap highlights MapEval's critical role in advancing general-purpose foundation models with stronger geo-spatial understanding.",
            "score": 11,
            "issue_id": 1477,
            "pub_date": "2025-12-31",
            "pub_date_card": {
                "ru": "31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 31",
                "zh": "12æœˆ31æ—¥"
            },
            "hash": "a4e45c6bd9d30ff4",
            "authors": [
                "Mahir Labib Dihan",
                "Md Tanvir Hassan",
                "Md Tanvir Parvez",
                "Md Hasebul Hasan",
                "Md Almash Alam",
                "Muhammad Aamir Cheema",
                "Mohammed Eunus Ali",
                "Md Rizwan Parvez"
            ],
            "affiliations": [
                "Bangladesh Computer Council (BCC)",
                "Department of Computer Science and Engineering Bangladesh University of Engineering and Technology (BUET)",
                "Monash University",
                "Qatar Computing Research Institute (QCRI)",
                "Statistics, Islamic University Bangladesh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00316.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#survey"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "MapEval: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MapEval - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸. MapEval Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 700 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 180 Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ² Ğ¸ 54 ÑÑ‚Ñ€Ğ°Ğ½Ñ‹, Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ½Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ ĞºĞ°Ñ€Ñ‚, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 28 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 20%. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ MapEval Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing AI's Geo-Spatial Reasoning with MapEval",
                    "desc": "This paper introduces MapEval, a benchmark designed to evaluate the performance of foundation models in map-based reasoning tasks. It focuses on assessing how well these models can handle complex geo-spatial queries, which are essential for navigation and resource discovery. The benchmark includes various task types that require models to process diverse information, such as travel distances and user reviews, and perform compositional reasoning. The evaluation reveals that while some models perform competitively, they still lag behind human capabilities, indicating a need for further advancements in geo-spatial understanding within AI systems."
                },
                "zh": {
                    "title": "æå‡åœ°å›¾æ¨ç†èƒ½åŠ›çš„åŸºå‡†è¯„ä¼°",
                    "desc": "æœ€è¿‘åŸºç¡€æ¨¡å‹çš„è¿›å±•æå‡äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨è‡ªä¸»å·¥å…·ä½¿ç”¨å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŸºäºä½ç½®æˆ–åœ°å›¾çš„æ¨ç†èƒ½åŠ›ä¸Šå°šæœªå¾—åˆ°ç³»ç»Ÿç ”ç©¶ï¼Œè¿™å¯¹äºä¼˜åŒ–å¯¼èˆªã€èµ„æºå‘ç°å’Œç‰©æµç®¡ç†è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MapEvalï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤æ‚åœ°å›¾ç”¨æˆ·æŸ¥è¯¢çš„åŸºå‡†ï¼Œæ¶‰åŠåœ°ç†ç©ºé—´æ¨ç†ã€‚MapEvalåŒ…å«700ä¸ªå…³äº180ä¸ªåŸå¸‚å’Œ54ä¸ªå›½å®¶çš„ç‹¬ç‰¹å¤šé¡¹é€‰æ‹©é¢˜ï¼Œè¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨å¤„ç†ç©ºé—´å…³ç³»ã€åœ°å›¾ä¿¡æ¯ã€æ—…è¡Œè§„åˆ’å’Œå¯¼èˆªæŒ‘æˆ˜æ–¹é¢çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01054",
            "title": "Dynamic Scaling of Unit Tests for Code Reward Modeling",
            "url": "https://huggingface.co/papers/2501.01054",
            "abstract": "Current large language models (LLMs) often struggle to produce accurate responses on the first attempt for complex reasoning tasks like code generation. Prior research tackles this challenge by generating multiple candidate solutions and validating them with LLM-generated unit tests. The execution results of unit tests serve as reward signals to identify correct solutions. As LLMs always confidently make mistakes, these unit tests are not reliable, thereby diminishing the quality of reward signals. Motivated by the observation that scaling the number of solutions improves LLM performance, we explore the impact of scaling unit tests to enhance reward signal quality. Our pioneer experiment reveals a positive correlation between the number of unit tests and reward signal quality, with greater benefits observed in more challenging problems. Based on these insights, we propose CodeRM-8B, a lightweight yet effective unit test generator that enables efficient and high-quality unit test scaling. Additionally, we implement a dynamic scaling mechanism that adapts the number of unit tests based on problem difficulty, further improving efficiency. Experimental results show that our approach significantly improves performance across various models on three benchmarks (e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on HumanEval Plus).",
            "score": 10,
            "issue_id": 1474,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 2",
                "zh": "1æœˆ2æ—¥"
            },
            "hash": "33b9590f2acb0e48",
            "authors": [
                "Zeyao Ma",
                "Xiaokang Zhang",
                "Jing Zhang",
                "Jifan Yu",
                "Sijia Luo",
                "Jie Tang"
            ],
            "affiliations": [
                "Key Laboratory of Data Engineering and Knowledge Engineering, Beijing, China",
                "School of Information, Renmin University of China",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01054.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#training",
                    "#small_models",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ½Ğ¸Ñ‚-Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ½Ğ¸Ñ‚-Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ ÑĞ½Ğ¸Ñ‚-Ñ‚ĞµÑÑ‚Ğ¾Ğ² CodeRM-8B Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing LLM Performance through Scaled Unit Testing",
                    "desc": "This paper addresses the limitations of large language models (LLMs) in generating accurate responses for complex tasks like code generation. It highlights the issue of unreliable reward signals from LLM-generated unit tests, which can lead to incorrect solutions. The authors propose a novel approach, CodeRM-8B, which generates a larger number of unit tests to improve the quality of these reward signals. Their experiments demonstrate that scaling unit tests enhances LLM performance, particularly for more challenging problems, leading to significant improvements across various models."
                },
                "zh": {
                    "title": "æå‡å•å…ƒæµ‹è¯•è´¨é‡ï¼Œå¢å¼ºæ¨¡å‹æ€§èƒ½",
                    "desc": "å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆï¼‰ä¸­ï¼Œå¾€å¾€éš¾ä»¥åœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶äº§ç”Ÿå‡†ç¡®çš„å“åº”ã€‚ä»¥å¾€çš„ç ”ç©¶é€šè¿‡ç”Ÿæˆå¤šä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆå¹¶ä½¿ç”¨LLMç”Ÿæˆçš„å•å…ƒæµ‹è¯•è¿›è¡ŒéªŒè¯æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚å•å…ƒæµ‹è¯•çš„æ‰§è¡Œç»“æœä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œç”¨äºè¯†åˆ«æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºLLMså¸¸å¸¸è‡ªä¿¡åœ°çŠ¯é”™ï¼Œè¿™äº›å•å…ƒæµ‹è¯•çš„å¯é æ€§ä¸è¶³ï¼Œä»è€Œé™ä½äº†å¥–åŠ±ä¿¡å·çš„è´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†CodeRM-8Bï¼Œä¸€ä¸ªè½»é‡çº§ä¸”æœ‰æ•ˆçš„å•å…ƒæµ‹è¯•ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°æ‰©å±•å•å…ƒæµ‹è¯•ï¼Œå¹¶æ ¹æ®é—®é¢˜çš„éš¾åº¦åŠ¨æ€è°ƒæ•´å•å…ƒæµ‹è¯•çš„æ•°é‡ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00192",
            "title": "MLLM-as-a-Judge for Image Safety without Human Labeling",
            "url": "https://huggingface.co/papers/2501.00192",
            "abstract": "Image content safety has become a significant challenge with the rise of visual media on online platforms. Meanwhile, in the age of AI-generated content (AIGC), many image generation models are capable of producing harmful content, such as images containing sexual or violent material. Thus, it becomes crucial to identify such unsafe images based on established safety rules. Pre-trained Multimodal Large Language Models (MLLMs) offer potential in this regard, given their strong pattern recognition abilities. Existing approaches typically fine-tune MLLMs with human-labeled datasets, which however brings a series of drawbacks. First, relying on human annotators to label data following intricate and detailed guidelines is both expensive and labor-intensive. Furthermore, users of safety judgment systems may need to frequently update safety rules, making fine-tuning on human-based annotation more challenging. This raises the research question: Can we detect unsafe images by querying MLLMs in a zero-shot setting using a predefined safety constitution (a set of safety rules)? Our research showed that simply querying pre-trained MLLMs does not yield satisfactory results. This lack of effectiveness stems from factors such as the subjectivity of safety rules, the complexity of lengthy constitutions, and the inherent biases in the models. To address these challenges, we propose a MLLM-based method includes objectifying safety rules, assessing the relevance between rules and images, making quick judgments based on debiased token probabilities with logically complete yet simplified precondition chains for safety rules, and conducting more in-depth reasoning with cascaded chain-of-thought processes if necessary. Experiment results demonstrate that our method is highly effective for zero-shot image safety judgment tasks.",
            "score": 9,
            "issue_id": 1474,
            "pub_date": "2025-12-31",
            "pub_date_card": {
                "ru": "31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 31",
                "zh": "12æœˆ31æ—¥"
            },
            "hash": "2a62bcbb87c1b7a5",
            "authors": [
                "Zhenting Wang",
                "Shuming Hu",
                "Shiyu Zhao",
                "Xiaowen Lin",
                "Felix Juefei-Xu",
                "Zhuowei Li",
                "Ligong Han",
                "Harihar Subramanyam",
                "Li Chen",
                "Jianfa Chen",
                "Nan Jiang",
                "Lingjuan Lyu",
                "Shiqing Ma",
                "Dimitris N. Metaxas",
                "Ankit Jain"
            ],
            "affiliations": [
                "GenAI @ Meta",
                "Rutgers University",
                "UMass Amherst",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00192.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#ethics",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ°: Zero-shot Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ MLLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞ±Ğ¸Ğ°ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Zero-Shot Image Safety Detection with MLLMs",
                    "desc": "This paper addresses the challenge of identifying unsafe images in the context of AI-generated content using Multimodal Large Language Models (MLLMs). The authors propose a novel approach that allows for zero-shot detection of harmful images by utilizing predefined safety rules without the need for extensive human labeling. They highlight the limitations of traditional methods, such as the subjectivity of safety rules and the biases present in models. The proposed method enhances safety judgment by objectifying rules, assessing their relevance to images, and employing a reasoning process that simplifies complex safety guidelines."
                },
                "zh": {
                    "title": "åˆ©ç”¨MLLMså®ç°é›¶æ ·æœ¬å›¾åƒå®‰å…¨åˆ¤æ–­",
                    "desc": "éšç€åœ¨çº¿å¹³å°è§†è§‰åª’ä½“çš„å…´èµ·ï¼Œå›¾åƒå†…å®¹å®‰å…¨æˆä¸ºä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚è®¸å¤šå›¾åƒç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿæœ‰å®³å†…å®¹ï¼Œå› æ­¤è¯†åˆ«ä¸å®‰å…¨å›¾åƒå˜å¾—è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡æŸ¥è¯¢è¿™äº›æ¨¡å‹æ¥æ£€æµ‹ä¸å®‰å…¨å›¾åƒï¼Œè€Œæ— éœ€ä¾èµ–äººå·¥æ ‡æ³¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é›¶æ ·æœ¬å›¾åƒå®‰å…¨åˆ¤æ–­ä»»åŠ¡ä¸­éå¸¸æœ‰æ•ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.21015",
            "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
            "url": "https://huggingface.co/papers/2412.21015",
            "abstract": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet Maps, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, a web application that streamlines the creation of reproducible, traceable map-based QA datasets. With its plug-and-play architecture, MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
            "score": 4,
            "issue_id": 1477,
            "pub_date": "2025-12-30",
            "pub_date_card": {
                "ru": "30 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 30",
                "zh": "12æœˆ30æ—¥"
            },
            "hash": "0d1081756b5bc4f7",
            "authors": [
                "Mahir Labib Dihan",
                "Mohammed Eunus Ali",
                "Md Rizwan Parvez"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering Bangladesh University of Engineering and Technology (BUET)",
                "Qatar Computing Research Institute (QCRI)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.21015.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#science",
                    "#reasoning",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "MapQaTor: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "MapQaTor - ÑÑ‚Ğ¾ Ğ²ĞµĞ±-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ñ€Ñ‚. ĞĞ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ»ÑĞ±Ñ‹Ğ¼ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ API Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². MapQaTor ĞºÑÑˆĞ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ API, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ ÑĞ±Ğ¾Ñ€Ğ°, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² 30 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Streamlining Geospatial QA with MapQaTor",
                    "desc": "This paper presents MapQaTor, a web application designed to facilitate the creation of geospatial question answering (QA) datasets using map services. It leverages recent advancements in Large Language Models (LLMs) to improve the handling of natural language queries related to locations. The platform features a plug-and-play architecture that integrates with various maps APIs, allowing users to efficiently gather, annotate, and visualize geospatial data. By caching API responses, MapQaTor ensures consistent and reliable data, significantly speeding up the annotation process and enhancing the evaluation of LLM-based geospatial reasoning capabilities."
                },
                "zh": {
                    "title": "MapQaTorï¼šæå‡åœ°å›¾é—®ç­”æ•°æ®é›†åˆ›å»ºæ•ˆç‡çš„åˆ©å™¨",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†MapQaTorï¼Œä¸€ä¸ªç”¨äºåˆ›å»ºåœ°å›¾é—®ç­”æ•°æ®é›†çš„ç½‘ç»œåº”ç”¨ç¨‹åºã€‚å®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œç®€åŒ–äº†ä»åœ°å›¾æœåŠ¡ç”Ÿæˆå¯é‡å¤å’Œå¯è¿½æº¯çš„æ•°æ®é›†çš„è¿‡ç¨‹ã€‚MapQaToræ”¯æŒä¸ä»»ä½•åœ°å›¾APIçš„æ— ç¼é›†æˆï¼Œå¹¶é€šè¿‡ç¼“å­˜APIå“åº”æ¥ç¡®ä¿æ•°æ®çš„ä¸€è‡´æ€§ã€‚è¯¥å¹³å°æ˜¾è‘—æé«˜äº†æ•°æ®æ ‡æ³¨çš„æ•ˆç‡ï¼Œå±•ç¤ºäº†åœ¨åœ°ç†ç©ºé—´æ¨ç†æ–¹é¢çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.00658",
            "title": "Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing",
            "url": "https://huggingface.co/papers/2501.00658",
            "abstract": "Structured State Space Models (SSMs) have emerged as alternatives to transformers. While SSMs are often regarded as effective in capturing long-sequence dependencies, we rigorously demonstrate that they are inherently limited by strong recency bias. Our empirical studies also reveal that this bias impairs the models' ability to recall distant information and introduces robustness issues. Our scaling experiments then discovered that deeper structures in SSMs can facilitate the learning of long contexts. However, subsequent theoretical analysis reveals that as SSMs increase in depth, they exhibit another inevitable tendency toward over-smoothing, e.g., token representations becoming increasingly indistinguishable. This fundamental dilemma between recency and over-smoothing hinders the scalability of existing SSMs. Inspired by our theoretical findings, we propose to polarize two channels of the state transition matrices in SSMs, setting them to zero and one, respectively, simultaneously addressing recency bias and over-smoothing. Experiments demonstrate that our polarization technique consistently enhances the associative recall accuracy of long-range tokens and unlocks SSMs to benefit further from deeper architectures. All source codes are released at https://github.com/VITA-Group/SSM-Bottleneck.",
            "score": 4,
            "issue_id": 1476,
            "pub_date": "2025-12-31",
            "pub_date_card": {
                "ru": "31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 31",
                "zh": "12æœˆ31æ—¥"
            },
            "hash": "253304ea64defbe0",
            "authors": [
                "Peihao Wang",
                "Ruisi Cai",
                "Yuehao Wang",
                "Jiajun Zhu",
                "Pragya Srivastava",
                "Zhangyang Wang",
                "Pan Li"
            ],
            "affiliations": [
                "Georgia Tech",
                "Google DeepMind",
                "University of Texas at Austin",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.00658.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#long_context",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ SSM: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM) Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ğ¼ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ SSM Ğ¸Ğ¼ĞµÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ´Ğµ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğº Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ´Ğ°Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ SSM ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ»ÑŒĞ½Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Balancing Recency and Over-Smoothing in SSMs",
                    "desc": "This paper discusses Structured State Space Models (SSMs) as alternatives to transformers, highlighting their limitations due to strong recency bias. This bias affects the models' ability to remember distant information and creates robustness issues. The authors propose a solution by polarizing the state transition matrices, which helps mitigate both recency bias and over-smoothing that occurs with deeper architectures. Their experiments show that this new approach improves the accuracy of recalling long-range tokens, allowing SSMs to effectively utilize deeper structures."
                },
                "zh": {
                    "title": "è§£å†³è¿‘æœŸåè§ä¸è¿‡å¹³æ»‘çš„åŒé‡æŒ‘æˆ˜",
                    "desc": "ç»“æ„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä½œä¸ºå˜æ¢å™¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œè™½ç„¶åœ¨æ•æ‰é•¿åºåˆ—ä¾èµ–æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨å¼ºçƒˆçš„è¿‘æœŸåè§é™åˆ¶ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§åè§å½±å“äº†æ¨¡å‹å¯¹è¿œç¨‹ä¿¡æ¯çš„å›å¿†èƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†é²æ£’æ€§é—®é¢˜ã€‚é€šè¿‡æ‰©å±•å®éªŒï¼Œæˆ‘ä»¬å‘ç°SSMsçš„æ·±å±‚ç»“æ„å¯ä»¥ä¿ƒè¿›é•¿ä¸Šä¸‹æ–‡çš„å­¦ä¹ ï¼Œä½†ç†è®ºåˆ†ææ˜¾ç¤ºï¼Œéšç€æ·±åº¦å¢åŠ ï¼Œæ¨¡å‹ä¼šå‡ºç°è¿‡å¹³æ»‘çš„è¶‹åŠ¿ï¼Œä½¿å¾—æ ‡è®°è¡¨ç¤ºå˜å¾—éš¾ä»¥åŒºåˆ†ã€‚æˆ‘ä»¬æå‡ºçš„æåŒ–æŠ€æœ¯é€šè¿‡å°†çŠ¶æ€è½¬ç§»çŸ©é˜µçš„ä¸¤ä¸ªé€šé“è®¾ç½®ä¸ºé›¶å’Œä¸€ï¼Œè§£å†³äº†è¿‘æœŸåè§å’Œè¿‡å¹³æ»‘çš„é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†é•¿è·ç¦»æ ‡è®°çš„å…³è”å›å¿†å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01320",
            "title": "SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration",
            "url": "https://huggingface.co/papers/2501.01320",
            "abstract": "Video restoration poses non-trivial challenges in maintaining fidelity while recovering temporally consistent details from unknown degradations in the wild. Despite recent advances in diffusion-based restoration, these methods often face limitations in generation capability and sampling efficiency. In this work, we present SeedVR, a diffusion transformer designed to handle real-world video restoration with arbitrary length and resolution. The core design of SeedVR lies in the shifted window attention that facilitates effective restoration on long video sequences. SeedVR further supports variable-sized windows near the boundary of both spatial and temporal dimensions, overcoming the resolution constraints of traditional window attention. Equipped with contemporary practices, including causal video autoencoder, mixed image and video training, and progressive training, SeedVR achieves highly-competitive performance on both synthetic and real-world benchmarks, as well as AI-generated videos. Extensive experiments demonstrate SeedVR's superiority over existing methods for generic video restoration.",
            "score": 2,
            "issue_id": 1479,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 2",
                "zh": "1æœˆ2æ—¥"
            },
            "hash": "fa277e5baed864a4",
            "authors": [
                "Jianyi Wang",
                "Zhijie Lin",
                "Meng Wei",
                "Yang Zhao",
                "Ceyuan Yang",
                "Chen Change Loy",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01320.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#long_context",
                    "#video",
                    "#training",
                    "#diffusion",
                    "#synthetic"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "SeedVR: Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "SeedVR - ÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğµ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. SeedVR Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾ĞºĞ½Ğ° Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, SeedVR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…."
                },
                "en": {
                    "title": "SeedVR: Revolutionizing Video Restoration with Diffusion Transformers",
                    "desc": "This paper introduces SeedVR, a novel diffusion transformer aimed at improving video restoration by effectively managing long sequences and varying resolutions. It utilizes shifted window attention to enhance the restoration process, allowing for better handling of temporal consistency and fidelity in videos. SeedVR incorporates advanced techniques such as causal video autoencoders and mixed training strategies to boost its performance on both synthetic and real-world datasets. The results show that SeedVR outperforms existing video restoration methods, making it a significant advancement in the field."
                },
                "zh": {
                    "title": "SeedVRï¼šé«˜æ•ˆçš„è§†é¢‘ä¿®å¤æ–°æ–¹æ³•",
                    "desc": "è§†é¢‘ä¿®å¤é¢ä¸´ç€åœ¨æ¢å¤æœªçŸ¥é€€åŒ–çš„åŒæ—¶ä¿æŒç»†èŠ‚ä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚å°½ç®¡åŸºäºæ‰©æ•£çš„ä¿®å¤æ–¹æ³•æœ‰æ‰€è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨ç”Ÿæˆèƒ½åŠ›å’Œé‡‡æ ·æ•ˆç‡ä¸Šä»å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºäº†SeedVRï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå¤„ç†ä»»æ„é•¿åº¦å’Œåˆ†è¾¨ç‡çš„çœŸå®è§†é¢‘ä¿®å¤è€Œè®¾è®¡çš„æ‰©æ•£å˜æ¢å™¨ã€‚SeedVRé€šè¿‡ç§»åŠ¨çª—å£æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆåœ°å¤„ç†é•¿è§†é¢‘åºåˆ—ï¼Œå¹¶åœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦çš„è¾¹ç•Œé™„è¿‘æ”¯æŒå¯å˜å¤§å°çš„çª—å£ï¼Œå…‹æœäº†ä¼ ç»Ÿçª—å£æ³¨æ„åŠ›çš„åˆ†è¾¨ç‡é™åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2501.01245",
            "title": "SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization",
            "url": "https://huggingface.co/papers/2501.01245",
            "abstract": "Human action understanding is crucial for the advancement of multimodal systems. While recent developments, driven by powerful large language models (LLMs), aim to be general enough to cover a wide range of categories, they often overlook the need for more specific capabilities. In this work, we address the more challenging task of Fine-grained Action Recognition (FAR), which focuses on detailed semantic labels within shorter temporal duration (e.g., \"salto backward tucked with 1 turn\"). Given the high costs of annotating fine-grained labels and the substantial data needed for fine-tuning LLMs, we propose to adopt semi-supervised learning (SSL). Our framework, SeFAR, incorporates several innovative designs to tackle these challenges. Specifically, to capture sufficient visual details, we construct Dual-level temporal elements as more effective representations, based on which we design a new strong augmentation strategy for the Teacher-Student learning paradigm through involving moderate temporal perturbation. Furthermore, to handle the high uncertainty within the teacher model's predictions for FAR, we propose the Adaptive Regulation to stabilize the learning process. Experiments show that SeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and FineDiving, across various data scopes. It also outperforms other semi-supervised methods on two classical coarse-grained datasets, UCF101 and HMDB51. Further analysis and ablation studies validate the effectiveness of our designs. Additionally, we show that the features extracted by our SeFAR could largely promote the ability of multimodal foundation models to understand fine-grained and domain-specific semantics.",
            "score": 2,
            "issue_id": 1475,
            "pub_date": "2025-01-02",
            "pub_date_card": {
                "ru": "2 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 2",
                "zh": "1æœˆ2æ—¥"
            },
            "hash": "30d94590a5c78569",
            "authors": [
                "Yongle Huang",
                "Haodong Chen",
                "Zhenbang Xu",
                "Zihan Jia",
                "Haozhou Sun",
                "Dian Shao"
            ],
            "affiliations": [
                "School of Automation, Northwestern Polytechnical University, Xian, China",
                "School of Computer Science, Northwestern Polytechnical University, Xian, China",
                "School of Software, Northwestern Polytechnical University, Xian, China",
                "Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2501.01245.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#transfer_learning",
                    "#multimodal",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ¤¸",
                "ru": {
                    "title": "SeFAR: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ»Ñƒ-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ (Fine-grained Action Recognition, FAR) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»Ñƒ-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SeFAR, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. SeFAR Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SeFAR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FAR Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹."
                },
                "en": {
                    "title": "SeFAR: Elevating Fine-grained Action Recognition with Semi-supervised Learning",
                    "desc": "This paper focuses on improving Fine-grained Action Recognition (FAR), which identifies specific actions in short time frames. The authors introduce a semi-supervised learning framework called SeFAR, which uses innovative techniques to enhance the learning process despite the challenges of limited labeled data. They develop Dual-level temporal elements for better visual representation and implement a strong augmentation strategy within a Teacher-Student learning setup. The results demonstrate that SeFAR achieves top performance on FAR datasets and enhances multimodal models' understanding of detailed actions."
                },
                "zh": {
                    "title": "ç»†ç²’åº¦åŠ¨ä½œè¯†åˆ«çš„æ–°çªç ´",
                    "desc": "äººç±»åŠ¨ä½œç†è§£å¯¹å¤šæ¨¡æ€ç³»ç»Ÿçš„å‘å±•è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶SeFARï¼Œä¸“æ³¨äºç»†ç²’åº¦åŠ¨ä½œè¯†åˆ«ï¼ˆFARï¼‰ï¼Œæ—¨åœ¨å¤„ç†çŸ­æ—¶é—´å†…çš„è¯¦ç»†è¯­ä¹‰æ ‡ç­¾ã€‚æˆ‘ä»¬é‡‡ç”¨åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¥å‡å°‘å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ï¼Œå¹¶é€šè¿‡æ„å»ºåŒå±‚æ—¶é—´å…ƒç´ å’Œæ–°çš„å¼ºå¢å¼ºç­–ç•¥æ¥æé«˜æ¨¡å‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSeFARåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†æˆ‘ä»¬è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-01-02.html",
    "link_next": "2025-01-06.html",
    "link_month": "2025-01.html",
    "short_date_prev": {
        "ru": "02.01",
        "en": "01/02",
        "zh": "1æœˆ2æ—¥"
    },
    "short_date_next": {
        "ru": "06.01",
        "en": "01/06",
        "zh": "1æœˆ6æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 7,
        "#agents": 1,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 8,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œç”¨äºæ”¹è¿›è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é¢„è®­ç»ƒã€‚ä¸ç°æœ‰çš„å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ç›¸æ¯”ï¼Œè¿™ä¸ªæ•°æ®é›†ä»æ•™å­¦è§†é¢‘ä¸­æå–ä¿¡æ¯ï¼ŒåŒ…å«æ›´ä¸°å¯Œçš„åŸºç¡€çŸ¥è¯†å’Œæ›´å¥½çš„å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚æ•°æ®é›†æ”¶é›†äº†è¶…è¿‡2.5å¹´çš„æ•™å­¦è§†é¢‘ï¼Œæ€»è®¡22,000è¯¾æ—¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¿™ä¸ªæ•°æ®é›†é¢„è®­ç»ƒçš„VLMsåœ¨çŸ¥è¯†å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å±•ç¤ºäº†å‡ºè‰²çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚",
        "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œç”¨äºæ”¹è¿›è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é¢„è®­ç»ƒã€‚ä¸ç°æœ‰çš„å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ç›¸æ¯”ï¼Œè¿™ä¸ªæ•°æ®é›†ä»æ•™å­¦è§†é¢‘ä¸­æå–ä¿¡æ¯ï¼ŒåŒ…å«æ›´ä¸°å¯Œçš„åŸºç¡€çŸ¥è¯†å’Œæ›´å¥½çš„å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚æ•°æ®é›†æ”¶é›†äº†è¶…è¿‡2.5å¹´çš„æ•™å­¦è§†é¢‘ï¼Œæ€»è®¡22,000è¯¾æ—¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¿™ä¸ªæ•°æ®é›†é¢„è®­ç»ƒçš„VLMsåœ¨çŸ¥è¯†å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å±•ç¤ºäº†å‡ºè‰²çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng xÄ«n de duÅ mÃ³ tÃ i shÃ¹ jÃ­, yÃ²ng yÃº gÇi jÃ¬n shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng (VLMs) de yÃ¹ xÃ¹n liÃ n. yÇ” xiÃ n yÇ’u de tÃº xiÃ ng wÃ©n bÄ›n shÃ¹ jÃ­ xiÄng bÇ, zhÃ¨ ge shÃ¹ jÃ­ cÃ³ng jiÃ o xuÃ© shÃ¬ pÃ­n zhÅng tÃ­ qÇ” xÃ¬n xÄ«, bÄo hÃ¡n gÃ¨ng fÄ“ng fÃ¹ de jÄ« chÇ” zhÄ« shi hÃ© gÃ¨ng hÇo de tÃº xiÃ ng wÃ©n bÄ›n duÃ¬ qÃ­. shÃ¹ jÃ­ shÅu jÃ­ le chÄo guÃ² 2.5 niÃ¡n de jiÃ o xuÃ© shÃ¬ pÃ­n, zÇ’ng jÃ¬ 22,000 kÃ¨ shÃ­. shÃ­ yÃ n jiÃ© guÇ’ xiÇn shÃ¬, shÇ yÃ²ng zhÃ¨ ge shÃ¹ jÃ­ yÃ¹ xÃ¹n liÃ n de VLMs zÃ i zhÄ« shi hÃ© tuÄ« lÇ mÃ¬ jÄ« xÃ­ng rÃ¨n wÃ¹ zhÅng biÇo xiÃ n yÅu yÃ¡n, bÃ¬ng zhÇn shÃ¬ le chÅ« sÃ¨ de shÃ ng xiÃ  wÃ©n juÃ© nÃ©ng lÃ¬. dÃ i mÇ yÇ zÃ i GitHub shÃ ng gÅng kÄi.",
        "vocab": "[{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'æ”¹è¿›', 'pinyin': 'gÇi jÃ¬n', 'trans': 'improve'},\n{'word': 'è§†è§‰-è¯­è¨€æ¨¡å‹', 'pinyin': 'shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'vision-language models'},\n{'word': 'é¢„è®­ç»ƒ', 'pinyin': 'yÃ¹ xÃ¹n liÃ n', 'trans': 'pre-training'},\n{'word': 'ç°æœ‰', 'pinyin': 'xiÃ n yÇ’u', 'trans': 'existing'},\n{'word': 'å›¾åƒ-æ–‡æœ¬', 'pinyin': 'tÃº xiÃ ng wÃ©n bÄ›n', 'trans': 'image-text'},\n{'word': 'æå–', 'pinyin': 'tÃ­ quÌ„', 'trans': 'extract'},\n{'word': 'ä¿¡æ¯', 'pinyin': 'xÃ¬n xÄ«', 'trans': 'information'},\n{'word': 'æ•™å­¦', 'pinyin': 'jiÃ o xuÃ©', 'trans': 'teaching'},\n{'word': 'è§†é¢‘', 'pinyin': 'shÃ¬ pÃ­n', 'trans': 'video'},\n{'word': 'åŸºç¡€çŸ¥è¯†', 'pinyin': 'jÄ« chÇ” zhÄ« shÃ¬', 'trans': 'foundational knowledge'},\n{'word': 'å¯¹é½', 'pinyin': 'duÃ¬ qÃ­', 'trans': 'alignment'},\n{'word': 'æ”¶é›†', 'pinyin': 'shÅu jÃ­', 'trans': 'collect'},\n{'word': 'è¯¾æ—¶', 'pinyin': 'kÃ¨ shÃ­', 'trans': 'class hours'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'},\n{'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'},\n{'word': 'ä¼˜å¼‚', 'pinyin': 'yÅu yÃ¬', 'trans': 'excellent'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'},\n{'word': 'å¯†é›†å‹', 'pinyin': 'mÃ¬ jÃ­ xÃ­ng', 'trans': 'intensive'},\n{'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wÃ¹', 'trans': 'task'},\n{'word': 'ä¸Šä¸‹æ–‡', 'pinyin': 'shÃ ng xiÃ  wÃ©n', 'trans': 'context'},\n{'word': 'æ„ŸçŸ¥', 'pinyin': 'gÇn zhÄ«', 'trans': 'perception'},\n{'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'},\n{'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'},\n{'word': 'å…¬å¼€', 'pinyin': 'gÅng kÄi', 'trans': 'public'}]",
        "trans": "This article introduces a new multimodal dataset aimed at improving the pretraining of Vision-Language Models (VLMs). Unlike existing image-text datasets, this dataset extracts information from educational videos, containing richer foundational knowledge and better image-text alignment. The dataset comprises over 2.5 years of educational videos, totaling 22,000 hours. Experimental results demonstrate that VLMs pretrained with this dataset perform exceptionally well in knowledge-intensive and reasoning-intensive tasks, showcasing excellent context-aware capabilities. The code is publicly available on GitHub.",
        "update_ts": "2025-01-03 09:10"
    }
}