{
    "date": {
        "ru": "14 ноября",
        "en": "November 14",
        "zh": "11月14日"
    },
    "time_utc": "2024-11-14 08:13",
    "weekday": 3,
    "issue_id": 568,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.08147",
            "title": "Large Language Models Can Self-Improve in Long-context Reasoning",
            "url": "https://huggingface.co/papers/2411.08147",
            "abstract": "Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus restricting further advancements. To address this issue, we investigate the potential for LLMs to self-improve in long-context reasoning and propose SEALONG, an approach specifically designed for this purpose. This approach is straightforward: we sample multiple outputs for each question, score them with Minimum Bayes Risk, and then apply supervised fine-tuning or preference optimization based on these outputs. Extensive experiments on several leading LLMs demonstrate the effectiveness of SEALONG, with an absolute improvement of 4.2 points for Llama-3.1-8B-Instruct. Furthermore, SEALONG achieves superior performance compared to prior approaches that depend on data produced by human experts or advanced models. We anticipate that this work will open new avenues for self-improvement techniques in long-context scenarios, which are essential for the continual advancement of LLMs.",
            "score": 15,
            "issue_id": 567,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "ea4232d9ddd5ef31",
            "data": {
                "categories": [
                    "#training",
                    "#synthetic",
                    "#long_context",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самообучение LLMs для длинных контекстов",
                    "desc": "В статье рассматривается проблема улучшения способности больших языковых моделей (LLMs) к рассуждению в длинных контекстах. Авторы предлагают метод SEALONG, который позволяет моделям самостоятельно улучшаться без необходимости в данных от экспертов или продвинутых моделей. Метод заключается в генерации нескольких ответов на вопрос, их оценке с помощью минимального байесовского риска и последующей оптимизации. Эксперименты показывают, что SEALONG значительно улучшает производительность моделей, таких как Llama-3.1-8B-Instruct."
                },
                "en": {
                    "title": "Empowering LLMs to Self-Improve Long-Context Reasoning",
                    "desc": "This paper addresses the challenges that large language models (LLMs) face in reasoning over long contexts. The authors propose a new method called SEALONG, which allows LLMs to enhance their long-context reasoning capabilities without relying on human-annotated data. Instead, the approach involves generating multiple outputs for each question, scoring them using Minimum Bayes Risk, and then fine-tuning the model based on these scores. Experimental results show that SEALONG significantly improves performance on Llama-3.1-8B-Instruct, outperforming previous methods that depend on expert-generated data."
                },
                "zh": {
                    "title": "让大型语言模型自我提升长文本推理能力",
                    "desc": "大型语言模型（LLMs）在处理长文本方面取得了显著进展，但在长文本推理上仍然存在困难。现有的方法通常依赖于人工专家或先进模型（如GPT-4）提供的合成数据进行微调，这限制了进一步的发展。为了解决这个问题，我们提出了一种名为SEALONG的方法，旨在让LLMs在长文本推理中自我改进。通过对每个问题采样多个输出，并使用最小贝叶斯风险进行评分，我们可以基于这些输出进行监督微调或偏好优化，从而显著提高模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.08380",
            "title": "EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation",
            "url": "https://huggingface.co/papers/2411.08380",
            "abstract": "Video generation has emerged as a promising tool for world simulation, leveraging visual data to replicate real-world environments. Within this context, egocentric video generation, which centers on the human perspective, holds significant potential for enhancing applications in virtual reality, augmented reality, and gaming. However, the generation of egocentric videos presents substantial challenges due to the dynamic nature of egocentric viewpoints, the intricate diversity of actions, and the complex variety of scenes encountered. Existing datasets are inadequate for addressing these challenges effectively. To bridge this gap, we present EgoVid-5M, the first high-quality dataset specifically curated for egocentric video generation. EgoVid-5M encompasses 5 million egocentric video clips and is enriched with detailed action annotations, including fine-grained kinematic control and high-level textual descriptions. To ensure the integrity and usability of the dataset, we implement a sophisticated data cleaning pipeline designed to maintain frame consistency, action coherence, and motion smoothness under egocentric conditions. Furthermore, we introduce EgoDreamer, which is capable of generating egocentric videos driven simultaneously by action descriptions and kinematic control signals. The EgoVid-5M dataset, associated action annotations, and all data cleansing metadata will be released for the advancement of research in egocentric video generation.",
            "score": 7,
            "issue_id": 565,
            "pub_date": "2024-11-13",
            "pub_date_card": {
                "ru": "13 ноября",
                "en": "November 13",
                "zh": "11月13日"
            },
            "hash": "7c82b1fc5a785fe0",
            "data": {
                "categories": [
                    "#data",
                    "#synthetic",
                    "#games",
                    "#video",
                    "#dataset"
                ],
                "emoji": "👀",
                "ru": {
                    "title": "EgoVid-5M: революция в генерации эгоцентрических видео",
                    "desc": "Статья представляет EgoVid-5M - первый крупномасштабный датасет для генерации эгоцентрических видео, содержащий 5 миллионов клипов с подробными аннотациями действий. Авторы разработали сложный процесс очистки данных для обеспечения целостности и удобства использования датасета. Также представлена модель EgoDreamer, способная генерировать эгоцентрические видео на основе описаний действий и кинематических сигналов управления. Датасет, аннотации и метаданные будут опубликованы для продвижения исследований в области генерации эгоцентрических видео."
                },
                "en": {
                    "title": "EgoVid-5M: Revolutionizing Egocentric Video Generation",
                    "desc": "This paper introduces EgoVid-5M, a new dataset designed for generating egocentric videos, which are videos captured from a first-person perspective. The dataset contains 5 million video clips with detailed annotations that describe the actions and movements occurring in each clip. The authors also present EgoDreamer, a model that can create egocentric videos based on both action descriptions and kinematic controls. This work aims to address the challenges of existing datasets and improve the quality and usability of egocentric video generation for applications in virtual and augmented reality."
                },
                "zh": {
                    "title": "EgoVid-5M：自我视角视频生成的新突破",
                    "desc": "视频生成是一种有前景的世界模拟工具，可以利用视觉数据复制现实环境。以自我视角为中心的视频生成在虚拟现实、增强现实和游戏应用中具有重要潜力。然而，自我视角视频的生成面临着动态视角、复杂动作和多样场景的挑战。为了解决这些问题，我们提出了EgoVid-5M，这是第一个专门为自我视角视频生成而创建的高质量数据集，包含500万个视频片段和详细的动作注释。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07618",
            "title": "Direct Preference Optimization Using Sparse Feature-Level Constraints",
            "url": "https://huggingface.co/papers/2411.07618",
            "abstract": "The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often introduce computational inefficiencies and training instability. In this paper, we propose Feature-level constrained Preference Optimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves a 5.08% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.",
            "score": 3,
            "issue_id": 565,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "b7ab0d7ebab29360",
            "data": {
                "categories": [
                    "#alignment",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Эффективное выравнивание языковых моделей с помощью разреженных признаков",
                    "desc": "Статья представляет новый метод для выравнивания больших языковых моделей (LLM) с человеческими предпочтениями - Feature-level constrained Preference Optimization (FPO). FPO использует предобученные разреженные автоэнкодеры и вводит ограничения на уровне признаков для эффективного выравнивания. Метод показывает улучшение на 5.08% по сравнению с современными базовыми методами при значительно меньших вычислительных затратах. FPO предлагает перспективное решение для эффективного и контролируемого выравнивания LLM."
                },
                "en": {
                    "title": "Efficient Alignment of Language Models with Human Preferences",
                    "desc": "This paper addresses the challenge of aligning large language models (LLMs) with human preferences. It introduces a new method called Feature-level constrained Preference Optimization (FPO), which aims to improve the alignment process while maintaining stability and efficiency. FPO utilizes pre-trained Sparse Autoencoders (SAEs) and applies feature-level constraints to enhance the alignment without incurring high computational costs. Experimental results show that FPO outperforms existing methods, achieving a significant improvement in performance while being more resource-efficient."
                },
                "zh": {
                    "title": "高效稳定的语言模型对齐新方法",
                    "desc": "本文提出了一种新的方法，称为特征级约束偏好优化（FPO），旨在简化大型语言模型（LLM）与人类偏好的对齐过程。FPO利用预训练的稀疏自编码器（SAE）并引入特征级约束，从而实现高效且稳定的对齐。通过激活稀疏特征和使用特征级离线参考，FPO在计算成本上显著降低，同时提高了模型的性能。实验结果表明，FPO在基准数据集上相较于最先进的基线方法，赢率提高了5.08%。"
                }
            }
        }
    ],
    "link_prev": "2024-11-13.html",
    "link_next": "2024-11-15.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "13.11",
        "en": "11/13",
        "zh": "11月13日"
    },
    "short_date_next": {
        "ru": "15.11",
        "en": "11/15",
        "zh": "11月15日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了3D部件分割的重要性及其在机器人、3D生成和3D编辑中的应用。现有方法利用视觉语言模型进行2D到3D的知识蒸馏，实现零样本3D部件分割，但依赖文本提示，限制了扩展性和处理部件歧义的灵活性。文章介绍了SAMPart3D，一个可扩展的零样本3D部件分割框架，不需要预定义的部件标签集。它使用与文本无关的视觉基础模型进行蒸馏，并提取多尺度的部件感知3D特征。实验显示，SAMPart3D在大规模3D数据集上表现优异，并超越现有方法。",
        "title": "SAMPart3D: Segment Any Part in 3D Objects",
        "pinyin": "这篇文章讨论了3D部件分割的重要性及其在机器人、3D生成和3D编辑中的应用。现有方法利用视觉语言模型进行2D到3D的知识蒸馏，实现零样本3D部件分割，但依赖文本提示，限制了扩展性和处理部件歧义的灵活性。文章介绍了SAMPart3D，一个可扩展的零样本3D部件分割框架，不需要预定义的部件标签集。它使用与文本无关的视觉基础模型进行蒸馏，并提取多尺度的部件感知3D特征。实验显示，SAMPart3D在大规模3D数据集上表现优异，并超越现有方法。\n\nPinyin transcription:\n\nZhè piān wénzhāng tǎolùn le 3D bùjiàn fēngé de zhòngyàoxìng jí qí zài jīqìrén, 3D shēngchéng hé 3D biānjí zhōng de yìngyòng. Xiànyǒu fāngfǎ lìyòng shìjué yǔyán móxíng jìnxíng 2D dào 3D de zhīshi zhēngliú, shíxiàn líng yàngběn 3D bùjiàn fēngé, dàn yīlài wénběn tíshì, xiànzhì le kuòzhǎnxìng hé chǔlǐ bùjiàn jíyì de línghuóxìng. Wénzhāng jièshào le SAMPart3D, yīgè kě kuòzhǎn de líng yàngběn 3D bùjiàn fēngé kuàngjià, bù xūyào yùdìngyì de bùjiàn biǎoqián jí. Tā shǐyòng yǔ wénběn wúguān de shìjué jīchǔ móxíng jìnxíng zhēngliú, bìng tīquǎn duō chǐdù de bùjiàn gǎnjué 3D tèzhēng. Shíyàn xiǎnshì, SAMPart3D zài dà guīmó 3D shùjùjí shàng biǎoxiàn yōuyuè, bìng chāoyué xiànyǒu fāngfǎ.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"3D部件分割\", \"pinyin\": \"3D bù jiàn fēn gē\", \"trans\": \"3D part segmentation\"},\n    {\"word\": \"重要性\", \"pinyin\": \"zhòng yào xìng\", \"trans\": \"importance\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"application\"},\n    {\"word\": \"现有方法\", \"pinyin\": \"xiàn yǒu fāng fǎ\", \"trans\": \"existing methods\"},\n    {\"word\": \"视觉语言模型\", \"pinyin\": \"shì jué yǔ yán mó xíng\", \"trans\": \"vision-language model\"},\n    {\"word\": \"知识蒸馏\", \"pinyin\": \"zhī shi zhēng liú\", \"trans\": \"knowledge distillation\"},\n    {\"word\": \"零样本\", \"pinyin\": \"líng yàng běn\", \"trans\": \"zero-shot\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"rely on\"},\n    {\"word\": \"文本提示\", \"pinyin\": \"wén běn tí shì\", \"trans\": \"textual prompts\"},\n    {\"word\": \"限制\", \"pinyin\": \"xiàn zhì\", \"trans\": \"limit\"},\n    {\"word\": \"扩展性\", \"pinyin\": \"kuò zhǎn xìng\", \"trans\": \"scalability\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"handle\"},\n    {\"word\": \"部件歧义\", \"pinyin\": \"bù jiàn qí yì\", \"trans\": \"part ambiguity\"},\n    {\"word\": \"灵活性\", \"pinyin\": \"líng huó xìng\", \"trans\": \"flexibility\"},\n    {\"word\": \"SAMPart3D\", \"pinyin\": \"SAMPart3D\", \"trans\": \"SAMPart3D\"},\n    {\"word\": \"框架\", \"pinyin\": \"kuàng jià\", \"trans\": \"framework\"},\n    {\"word\": \"预定义\", \"pinyin\": \"yù dìng yì\", \"trans\": \"predefined\"},\n    {\"word\": \"标签集\", \"pinyin\": \"biāo qiān jí\", \"trans\": \"label set\"},\n    {\"word\": \"视觉基础模型\", \"pinyin\": \"shì jué jī chǔ mó xíng\", \"trans\": \"vision foundation model\"},\n    {\"word\": \"多尺度\", \"pinyin\": \"duō chǐ dù\", \"trans\": \"multi-scale\"},\n    {\"word\": \"部件感知\", \"pinyin\": \"bù jiàn gǎn zhī\", \"trans\": \"part-aware\"},\n    {\"word\": \"3D特征\", \"pinyin\": \"3D tè zhēng\", \"trans\": \"3D features\"},\n    {\"word\": \"实验\", \"pinyin\": \"shí yàn\", \"trans\": \"experiment\"},\n    {\"word\": \"大规模\", \"pinyin\": \"dà guī mó\", \"trans\": \"large-scale\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"优异\", \"pinyin\": \"yōu yì\", \"trans\": \"excellent\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"}\n]",
        "trans": "This article discusses the importance of 3D part segmentation and its applications in robotics, 3D generation, and 3D editing. Existing methods utilize vision-language models for knowledge distillation from 2D to 3D, achieving zero-shot 3D part segmentation, but they rely on textual prompts, which limits their scalability and flexibility in handling part ambiguities. The article introduces SAMPart3D, a scalable zero-shot 3D part segmentation framework that does not require predefined part labels. It uses a text-agnostic visual foundation model for distillation and extracts multi-scale part-aware 3D features. Experiments show that SAMPart3D performs excellently on large-scale 3D datasets and outperforms existing methods.",
        "update_ts": "2024-11-13 09:10"
    }
}