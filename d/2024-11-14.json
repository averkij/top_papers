{
    "date": {
        "ru": "14 ноября",
        "en": "November 14",
        "zh": "11月14日"
    },
    "time_utc": "2024-11-14 22:09",
    "weekday": 3,
    "issue_id": 582,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.08147",
            "title": "Large Language Models Can Self-Improve in Long-context Reasoning",
            "url": "https://huggingface.co/papers/2411.08147",
            "abstract": "Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus restricting further advancements. To address this issue, we investigate the potential for LLMs to self-improve in long-context reasoning and propose SEALONG, an approach specifically designed for this purpose. This approach is straightforward: we sample multiple outputs for each question, score them with Minimum Bayes Risk, and then apply supervised fine-tuning or preference optimization based on these outputs. Extensive experiments on several leading LLMs demonstrate the effectiveness of SEALONG, with an absolute improvement of 4.2 points for Llama-3.1-8B-Instruct. Furthermore, SEALONG achieves superior performance compared to prior approaches that depend on data produced by human experts or advanced models. We anticipate that this work will open new avenues for self-improvement techniques in long-context scenarios, which are essential for the continual advancement of LLMs.",
            "score": 39,
            "issue_id": 567,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "ea4232d9ddd5ef31",
            "data": {
                "categories": [
                    "#training",
                    "#synthetic",
                    "#long_context",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самообучение LLMs для длинных контекстов",
                    "desc": "В статье рассматривается проблема улучшения способности больших языковых моделей (LLMs) к рассуждению в длинных контекстах. Авторы предлагают метод SEALONG, который позволяет моделям самостоятельно улучшаться без необходимости в данных от экспертов или продвинутых моделей. Метод заключается в генерации нескольких ответов на вопрос, их оценке с помощью минимального байесовского риска и последующей оптимизации. Эксперименты показывают, что SEALONG значительно улучшает производительность моделей, таких как Llama-3.1-8B-Instruct."
                },
                "en": {
                    "title": "Empowering LLMs to Self-Improve Long-Context Reasoning",
                    "desc": "This paper addresses the challenges that large language models (LLMs) face in reasoning over long contexts. The authors propose a new method called SEALONG, which allows LLMs to enhance their long-context reasoning capabilities without relying on human-annotated data. Instead, the approach involves generating multiple outputs for each question, scoring them using Minimum Bayes Risk, and then fine-tuning the model based on these scores. Experimental results show that SEALONG significantly improves performance on Llama-3.1-8B-Instruct, outperforming previous methods that depend on expert-generated data."
                },
                "zh": {
                    "title": "让大型语言模型自我提升长文本推理能力",
                    "desc": "大型语言模型（LLMs）在处理长文本方面取得了显著进展，但在长文本推理上仍然存在困难。现有的方法通常依赖于人工专家或先进模型（如GPT-4）提供的合成数据进行微调，这限制了进一步的发展。为了解决这个问题，我们提出了一种名为SEALONG的方法，旨在让LLMs在长文本推理中自我改进。通过对每个问题采样多个输出，并使用最小贝叶斯风险进行评分，我们可以基于这些输出进行监督微调或偏好优化，从而显著提高模型性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.08380",
            "title": "EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation",
            "url": "https://huggingface.co/papers/2411.08380",
            "abstract": "Video generation has emerged as a promising tool for world simulation, leveraging visual data to replicate real-world environments. Within this context, egocentric video generation, which centers on the human perspective, holds significant potential for enhancing applications in virtual reality, augmented reality, and gaming. However, the generation of egocentric videos presents substantial challenges due to the dynamic nature of egocentric viewpoints, the intricate diversity of actions, and the complex variety of scenes encountered. Existing datasets are inadequate for addressing these challenges effectively. To bridge this gap, we present EgoVid-5M, the first high-quality dataset specifically curated for egocentric video generation. EgoVid-5M encompasses 5 million egocentric video clips and is enriched with detailed action annotations, including fine-grained kinematic control and high-level textual descriptions. To ensure the integrity and usability of the dataset, we implement a sophisticated data cleaning pipeline designed to maintain frame consistency, action coherence, and motion smoothness under egocentric conditions. Furthermore, we introduce EgoDreamer, which is capable of generating egocentric videos driven simultaneously by action descriptions and kinematic control signals. The EgoVid-5M dataset, associated action annotations, and all data cleansing metadata will be released for the advancement of research in egocentric video generation.",
            "score": 14,
            "issue_id": 565,
            "pub_date": "2024-11-13",
            "pub_date_card": {
                "ru": "13 ноября",
                "en": "November 13",
                "zh": "11月13日"
            },
            "hash": "7c82b1fc5a785fe0",
            "data": {
                "categories": [
                    "#data",
                    "#synthetic",
                    "#games",
                    "#video",
                    "#dataset"
                ],
                "emoji": "👀",
                "ru": {
                    "title": "EgoVid-5M: революция в генерации эгоцентрических видео",
                    "desc": "Статья представляет EgoVid-5M - первый крупномасштабный датасет для генерации эгоцентрических видео, содержащий 5 миллионов клипов с подробными аннотациями действий. Авторы разработали сложный процесс очистки данных для обеспечения целостности и удобства использования датасета. Также представлена модель EgoDreamer, способная генерировать эгоцентрические видео на основе описаний действий и кинематических сигналов управления. Датасет, аннотации и метаданные будут опубликованы для продвижения исследований в области генерации эгоцентрических видео."
                },
                "en": {
                    "title": "EgoVid-5M: Revolutionizing Egocentric Video Generation",
                    "desc": "This paper introduces EgoVid-5M, a new dataset designed for generating egocentric videos, which are videos captured from a first-person perspective. The dataset contains 5 million video clips with detailed annotations that describe the actions and movements occurring in each clip. The authors also present EgoDreamer, a model that can create egocentric videos based on both action descriptions and kinematic controls. This work aims to address the challenges of existing datasets and improve the quality and usability of egocentric video generation for applications in virtual and augmented reality."
                },
                "zh": {
                    "title": "EgoVid-5M：自我视角视频生成的新突破",
                    "desc": "视频生成是一种有前景的世界模拟工具，可以利用视觉数据复制现实环境。以自我视角为中心的视频生成在虚拟现实、增强现实和游戏应用中具有重要潜力。然而，自我视角视频的生成面临着动态视角、复杂动作和多样场景的挑战。为了解决这些问题，我们提出了EgoVid-5M，这是第一个专门为自我视角视频生成而创建的高质量数据集，包含500万个视频片段和详细的动作注释。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.07618",
            "title": "Direct Preference Optimization Using Sparse Feature-Level Constraints",
            "url": "https://huggingface.co/papers/2411.07618",
            "abstract": "The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often introduce computational inefficiencies and training instability. In this paper, we propose Feature-level constrained Preference Optimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves a 5.08% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.",
            "score": 10,
            "issue_id": 565,
            "pub_date": "2024-11-12",
            "pub_date_card": {
                "ru": "12 ноября",
                "en": "November 12",
                "zh": "11月12日"
            },
            "hash": "b7ab0d7ebab29360",
            "data": {
                "categories": [
                    "#alignment",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Эффективное выравнивание языковых моделей с помощью разреженных признаков",
                    "desc": "Статья представляет новый метод для выравнивания больших языковых моделей (LLM) с человеческими предпочтениями - Feature-level constrained Preference Optimization (FPO). FPO использует предобученные разреженные автоэнкодеры и вводит ограничения на уровне признаков для эффективного выравнивания. Метод показывает улучшение на 5.08% по сравнению с современными базовыми методами при значительно меньших вычислительных затратах. FPO предлагает перспективное решение для эффективного и контролируемого выравнивания LLM."
                },
                "en": {
                    "title": "Efficient Alignment of Language Models with Human Preferences",
                    "desc": "This paper addresses the challenge of aligning large language models (LLMs) with human preferences. It introduces a new method called Feature-level constrained Preference Optimization (FPO), which aims to improve the alignment process while maintaining stability and efficiency. FPO utilizes pre-trained Sparse Autoencoders (SAEs) and applies feature-level constraints to enhance the alignment without incurring high computational costs. Experimental results show that FPO outperforms existing methods, achieving a significant improvement in performance while being more resource-efficient."
                },
                "zh": {
                    "title": "高效稳定的语言模型对齐新方法",
                    "desc": "本文提出了一种新的方法，称为特征级约束偏好优化（FPO），旨在简化大型语言模型（LLM）与人类偏好的对齐过程。FPO利用预训练的稀疏自编码器（SAE）并引入特征级约束，从而实现高效且稳定的对齐。通过激活稀疏特征和使用特征级离线参考，FPO在计算成本上显著降低，同时提高了模型的性能。实验结果表明，FPO在基准数据集上相较于最先进的基线方法，赢率提高了5.08%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.08868",
            "title": "CamemBERT 2.0: A Smarter French Language Model Aged to Perfection",
            "url": "https://huggingface.co/papers/2411.08868",
            "abstract": "French language models, such as CamemBERT, have been widely adopted across industries for natural language processing (NLP) tasks, with models like CamemBERT seeing over 4 million downloads per month. However, these models face challenges due to temporal concept drift, where outdated training data leads to a decline in performance, especially when encountering new topics and terminology. This issue emphasizes the need for updated models that reflect current linguistic trends. In this paper, we introduce two new versions of the CamemBERT base model-CamemBERTav2 and CamemBERTv2-designed to address these challenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes use of the Replaced Token Detection (RTD) objective for better contextual understanding, while CamemBERTv2 is built on RoBERTa, which uses the Masked Language Modeling (MLM) objective. Both models are trained on a significantly larger and more recent dataset with longer context length and an updated tokenizer that enhances tokenization performance for French. We evaluate the performance of these models on both general-domain NLP tasks and domain-specific applications, such as medical field tasks, demonstrating their versatility and effectiveness across a range of use cases. Our results show that these updated models vastly outperform their predecessors, making them valuable tools for modern NLP systems. All our new models, as well as intermediate checkpoints, are made openly available on Huggingface.",
            "score": 8,
            "issue_id": 569,
            "pub_date": "2024-11-13",
            "pub_date_card": {
                "ru": "13 ноября",
                "en": "November 13",
                "zh": "11月13日"
            },
            "hash": "6be618c24defa5fb",
            "data": {
                "categories": [
                    "#training",
                    "#multilingual",
                    "#healthcare",
                    "#long_context",
                    "#architecture",
                    "#dataset",
                    "#low_resource",
                    "#open_source"
                ],
                "emoji": "🇫🇷",
                "ru": {
                    "title": "Обновленные французские языковые модели для современных задач NLP",
                    "desc": "В статье представлены две новые версии французской языковой модели CamemBERT - CamemBERTav2 и CamemBERTv2. Эти модели призваны решить проблему временного дрейфа концепций, из-за которого устаревшие данные для обучения приводят к снижению производительности. CamemBERTav2 основана на архитектуре DeBERTaV3 и использует метод Replaced Token Detection, а CamemBERTv2 построена на RoBERTa с применением Masked Language Modeling. Обе модели обучены на более обширном и современном наборе данных с увеличенной длиной контекста и улучшенным токенизатором для французского языка."
                },
                "en": {
                    "title": "Revamping CamemBERT for Modern NLP Challenges",
                    "desc": "This paper presents two updated versions of the French language model CamemBERT, named CamemBERTav2 and CamemBERTv2, to tackle the issue of temporal concept drift in natural language processing. CamemBERTav2 utilizes the DeBERTaV3 architecture with a Replaced Token Detection objective, while CamemBERTv2 is based on RoBERTa and employs Masked Language Modeling. Both models are trained on a larger, more recent dataset, improving their contextual understanding and tokenization performance for French. The evaluation shows that these models significantly outperform previous versions in various NLP tasks, including general and domain-specific applications."
                },
                "zh": {
                    "title": "更新模型，提升法语NLP性能",
                    "desc": "本文介绍了两种新的法语语言模型CamemBERTav2和CamemBERTv2，旨在解决由于过时训练数据导致的性能下降问题。这些模型基于DeBERTaV3和RoBERTa架构，分别采用了替换标记检测（RTD）和掩蔽语言建模（MLM）目标，以提高上下文理解能力。它们在更大且更新的数据集上进行训练，具有更长的上下文长度和改进的分词器，提升了法语的分词性能。实验结果表明，这些更新的模型在通用和特定领域的自然语言处理任务中表现优异，成为现代NLP系统的重要工具。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.08790",
            "title": "Can sparse autoencoders be used to decompose and interpret steering vectors?",
            "url": "https://huggingface.co/papers/2411.08790",
            "abstract": "Steering vectors are a promising approach to control the behaviour of large language models. However, their underlying mechanisms remain poorly understood. While sparse autoencoders (SAEs) may offer a potential method to interpret steering vectors, recent findings show that SAE-reconstructed vectors often lack the steering properties of the original vectors. This paper investigates why directly applying SAEs to steering vectors yields misleading decompositions, identifying two reasons: (1) steering vectors fall outside the input distribution for which SAEs are designed, and (2) steering vectors can have meaningful negative projections in feature directions, which SAEs are not designed to accommodate. These limitations hinder the direct use of SAEs for interpreting steering vectors.",
            "score": 4,
            "issue_id": 570,
            "pub_date": "2024-11-13",
            "pub_date_card": {
                "ru": "13 ноября",
                "en": "November 13",
                "zh": "11月13日"
            },
            "hash": "68e55764db274419",
            "data": {
                "categories": [
                    "#architecture",
                    "#interpretability",
                    "#training"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "Ограничения автоэнкодеров в расшифровке векторов управления ИИ",
                    "desc": "Статья исследует проблемы применения разреженных автоэнкодеров (SAE) для интерпретации векторов управления в больших языковых моделях. Авторы выявляют два ключевых ограничения: несоответствие векторов управления входному распределению SAE и наличие значимых отрицательных проекций. Эти факторы препятствуют прямому использованию SAE для анализа векторов управления. Исследование подчеркивает необходимость разработки новых методов для понимания механизмов работы векторов управления в языковых моделях."
                },
                "en": {
                    "title": "Understanding Steering Vectors: Limitations of Sparse Autoencoders",
                    "desc": "This paper explores the use of steering vectors in controlling large language models and highlights the challenges in understanding their mechanisms. It specifically examines the limitations of sparse autoencoders (SAEs) when applied to these steering vectors. The authors identify that steering vectors do not fit the input distribution that SAEs are designed for, and they can have significant negative projections that SAEs cannot handle. These findings suggest that using SAEs for interpreting steering vectors may lead to inaccurate results, indicating a need for alternative methods."
                },
                "zh": {
                    "title": "揭示引导向量的奥秘",
                    "desc": "本论文探讨了引导向量在控制大型语言模型行为中的应用潜力，但其机制尚不清楚。研究发现，稀疏自编码器（SAE）在解释引导向量时存在问题，重构的向量往往缺乏原始向量的引导特性。论文指出，直接应用SAE于引导向量会导致误导性的分解，原因包括引导向量超出了SAE设计的输入分布，以及引导向量在特征方向上可能具有有意义的负投影。由于这些限制，SAE在解释引导向量时的直接应用受到阻碍。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.08307",
            "title": "PerceiverS: A Multi-Scale Perceiver with Effective Segmentation for Long-Term Expressive Symbolic Music Generation",
            "url": "https://huggingface.co/papers/2411.08307",
            "abstract": "Music generation has progressed significantly, especially in the domain of audio generation. However, generating symbolic music that is both long-structured and expressive remains a significant challenge. In this paper, we propose PerceiverS (Segmentation and Scale), a novel architecture designed to address this issue by leveraging both Effective Segmentation and Multi-Scale attention mechanisms. Our approach enhances symbolic music generation by simultaneously learning long-term structural dependencies and short-term expressive details. By combining cross-attention and self-attention in a Multi-Scale setting, PerceiverS captures long-range musical structure while preserving performance nuances. The proposed model, evaluated on datasets like Maestro, demonstrates improvements in generating coherent and diverse music with both structural consistency and expressive variation. The project demos and the generated music samples can be accessed through the link: https://perceivers.github.io.",
            "score": 1,
            "issue_id": 575,
            "pub_date": "2024-11-13",
            "pub_date_card": {
                "ru": "13 ноября",
                "en": "November 13",
                "zh": "11月13日"
            },
            "hash": "1f1b5e7081062b6f",
            "data": {
                "categories": [
                    "#audio",
                    "#architecture",
                    "#story_generation"
                ],
                "emoji": "🎼",
                "ru": {
                    "title": "PerceiverS: Новый подход к генерации структурированной и выразительной музыки",
                    "desc": "Статья представляет PerceiverS - новую архитектуру для генерации символической музыки. Модель использует механизмы эффективной сегментации и многомасштабного внимания для улучшения структуры и выразительности генерируемой музыки. PerceiverS сочетает кросс-внимание и самовнимание для захвата как долгосрочных музыкальных структур, так и кратковременных нюансов исполнения. Эксперименты на наборах данных вроде Maestro показывают улучшение в генерации связной и разнообразной музыки."
                },
                "en": {
                    "title": "Enhancing Symbolic Music Generation with PerceiverS",
                    "desc": "This paper introduces PerceiverS, a new architecture aimed at improving symbolic music generation. It utilizes Effective Segmentation and Multi-Scale attention mechanisms to learn both long-term structures and short-term expressive details in music. By integrating cross-attention and self-attention, the model effectively captures the overall musical structure while maintaining nuanced performance elements. Evaluations on datasets like Maestro show that PerceiverS generates more coherent and diverse music, achieving better structural consistency and expressive variation."
                },
                "zh": {
                    "title": "PerceiverS：生成富有表现力的符号音乐新方法",
                    "desc": "本文提出了一种新的音乐生成架构，称为PerceiverS（分段与尺度），旨在解决生成长结构和富有表现力的符号音乐的挑战。该模型结合了有效的分段和多尺度注意力机制，能够同时学习长期结构依赖和短期表现细节。通过在多尺度设置中结合交叉注意力和自注意力，PerceiverS能够捕捉长距离的音乐结构，同时保留演奏的细微差别。实验结果表明，该模型在生成连贯且多样化的音乐方面表现出色，具有结构一致性和表现变化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.08328",
            "title": "Motion Control for Enhanced Complex Action Video Generation",
            "url": "https://huggingface.co/papers/2411.08328",
            "abstract": "Existing text-to-video (T2V) models often struggle with generating videos with sufficiently pronounced or complex actions. A key limitation lies in the text prompt's inability to precisely convey intricate motion details. To address this, we propose a novel framework, MVideo, designed to produce long-duration videos with precise, fluid actions. MVideo overcomes the limitations of text prompts by incorporating mask sequences as an additional motion condition input, providing a clearer, more accurate representation of intended actions. Leveraging foundational vision models such as GroundingDINO and SAM2, MVideo automatically generates mask sequences, enhancing both efficiency and robustness. Our results demonstrate that, after training, MVideo effectively aligns text prompts with motion conditions to produce videos that simultaneously meet both criteria. This dual control mechanism allows for more dynamic video generation by enabling alterations to either the text prompt or motion condition independently, or both in tandem. Furthermore, MVideo supports motion condition editing and composition, facilitating the generation of videos with more complex actions. MVideo thus advances T2V motion generation, setting a strong benchmark for improved action depiction in current video diffusion models. Our project page is available at https://mvideo-v1.github.io/.",
            "score": 0,
            "issue_id": 575,
            "pub_date": "2024-11-13",
            "pub_date_card": {
                "ru": "13 ноября",
                "en": "November 13",
                "zh": "11月13日"
            },
            "hash": "ff99307fadb40120",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#video",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Точные движения в генерируемых видео с помощью масок",
                    "desc": "MVideo - это новая система для генерации видео по текстовому описанию, которая решает проблему недостаточно выраженных или сложных действий в существующих моделях. Она использует последовательности масок в качестве дополнительного входа для более точного представления движений. MVideo автоматически генерирует эти маски с помощью моделей компьютерного зрения, что повышает эффективность и надежность. Система позволяет независимо изменять текстовое описание и условия движения, а также поддерживает редактирование и композицию условий движения."
                },
                "en": {
                    "title": "MVideo: Revolutionizing Text-to-Video with Precise Motion Control",
                    "desc": "The paper introduces MVideo, a new framework for text-to-video (T2V) generation that enhances the depiction of complex actions in videos. It addresses the limitations of traditional text prompts by using mask sequences as an additional input, which provides clearer motion details. MVideo utilizes advanced vision models to automatically create these mask sequences, improving the efficiency and accuracy of video generation. The framework allows for independent or combined adjustments to text prompts and motion conditions, enabling the creation of more dynamic and intricate videos."
                },
                "zh": {
                    "title": "MVideo：提升文本到视频生成的动态性与精确性",
                    "desc": "现有的文本到视频（T2V）模型在生成复杂动作的视频时常常面临挑战。主要问题在于文本提示无法准确传达复杂的运动细节。为了解决这个问题，我们提出了一种新框架MVideo，旨在生成具有精确流畅动作的长时视频。MVideo通过引入掩码序列作为额外的运动条件输入，克服了文本提示的局限性，从而实现更动态的视频生成。"
                }
            }
        }
    ],
    "link_prev": "2024-11-13.html",
    "link_next": "2024-11-15.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "13.11",
        "en": "11/13",
        "zh": "11月13日"
    },
    "short_date_next": {
        "ru": "15.11",
        "en": "11/15",
        "zh": "11月15日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了大型语言模型（LLMs）在处理长上下文时的挑战。现有方法依赖人类专家或先进模型的标注数据，限制了进一步发展。作者提出了一种新方法，称为SEALONG，使LLMs能够在长上下文推理中自我提升。该方法通过采样多个输出，用最小贝叶斯风险评分，然后进行监督微调或偏好优化。实验证明，SEALONG在多个领先LLMs上有效，并超越了依赖人类专家或先进模型数据的方法。",
        "title": "Large Language Models Can Self-Improve in Long-context Reasoning",
        "pinyin": "这篇文章讨论了大型语言模型（LLMs）在处理长上下文时的挑战。现有方法依赖人类专家或先进模型的标注数据，限制了进一步发展。作者提出了一种新方法，称为SEALONG，使LLMs能够在长上下文推理中自我提升。该方法通过采样多个输出，用最小贝叶斯风险评分，然后进行监督微调或偏好优化。实验证明，SEALONG在多个领先LLMs上有效，并超越了依赖人类专家或先进模型数据的方法。\n\nzhè piān wén zhāng tǎo lùn le dà xíng yǔ yán mó xíng (LLMs) zài chǔ lǐ cháng shàng xià wěn shí de tiǎo zhàn. xiàn yǒu fāng fǎ yī lài rén lèi zhuān jiā huò xiān jìn mó xíng de biāo zhù shù jiù, xiàn zhì le jìn yì bù fā zhǎn. zuò zhě tí chū le yī zhǒng xīn fāng fǎ, chēng wéi SEALONG, shǐ LLMs néng gòu zài cháng shàng xià wěn tuī lǐ zhōng zì wǒ tí shēng. gāi fāng fǎ tōng guò cǎi yàng duō gè shū rù, yòng zuì xiǎo bèi lái sī fēng xiǎn píng fēn, rán hòu jìn xíng jiān dū wēi tiáo huò piàn hào yōu huà. shí yàn zhèng míng, SEALONG zài duō gè lǐng xiān LLMs shàng yǒu xiào, bìng chāo yuè le yī lài rén lèi zhuān jiā huò xiān jìn mó xíng shù jiù de fāng fǎ.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎo zhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"现有\", \"pinyin\": \"xiàn yǒu\", \"trans\": \"existing\"},\n    {\"word\": \"依赖\", \"pinyin\": \"yī lài\", \"trans\": \"rely on\"},\n    {\"word\": \"专家\", \"pinyin\": \"zhuān jiā\", \"trans\": \"expert\"},\n    {\"word\": \"标注\", \"pinyin\": \"biāo zhù\", \"trans\": \"annotate\"},\n    {\"word\": \"数据\", \"pinyin\": \"shù jù\", \"trans\": \"data\"},\n    {\"word\": \"限制\", \"pinyin\": \"xiàn zhì\", \"trans\": \"limit\"},\n    {\"word\": \"进一步\", \"pinyin\": \"jìn yī bù\", \"trans\": \"further\"},\n    {\"word\": \"发展\", \"pinyin\": \"fā zhǎn\", \"trans\": \"develop\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāng fǎ\", \"trans\": \"method\"},\n    {\"word\": \"称为\", \"pinyin\": \"chēng wéi\", \"trans\": \"called\"},\n    {\"word\": \"自我\", \"pinyin\": \"zì wǒ\", \"trans\": \"self\"},\n    {\"word\": \"提升\", \"pinyin\": \"tí shēng\", \"trans\": \"improve\"},\n    {\"word\": \"采样\", \"pinyin\": \"cǎi yàng\", \"trans\": \"sample\"},\n    {\"word\": \"输出\", \"pinyin\": \"shū chū\", \"trans\": \"output\"},\n    {\"word\": \"贝叶斯\", \"pinyin\": \"bèi yè sī\", \"trans\": \"Bayesian\"},\n    {\"word\": \"风险\", \"pinyin\": \"fēng xiǎn\", \"trans\": \"risk\"},\n    {\"word\": \"评分\", \"pinyin\": \"píng fēn\", \"trans\": \"score\"},\n    {\"word\": \"监督\", \"pinyin\": \"jiān dū\", \"trans\": \"supervise\"},\n    {\"word\": \"微调\", \"pinyin\": \"wēi tiáo\", \"trans\": \"fine-tune\"},\n    {\"word\": \"偏好\", \"pinyin\": \"piān hào\", \"trans\": \"preference\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimize\"},\n    {\"word\": \"证明\", \"pinyin\": \"zhèng míng\", \"trans\": \"prove\"},\n    {\"word\": \"领先\", \"pinyin\": \"lǐng xiān\", \"trans\": \"leading\"},\n    {\"word\": \"超越\", \"pinyin\": \"chāo yuè\", \"trans\": \"surpass\"}\n]",
        "trans": "This article discusses the challenges faced by large language models (LLMs) in handling long contexts. Existing methods rely on data annotated by human experts or advanced models, which limits further development. The authors propose a new method, called SEALONG, that enables LLMs to self-improve in long context reasoning. This method involves sampling multiple outputs, using minimum Bayes risk scoring, and then performing supervised fine-tuning or preference optimization. Experiments demonstrate that SEALONG is effective across multiple leading LLMs and outperforms methods that rely on data from human experts or advanced models.",
        "update_ts": "2024-11-14 09:10"
    }
}