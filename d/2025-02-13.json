{
    "date": {
        "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 13",
        "zh": "2æœˆ13æ—¥"
    },
    "time_utc": "2025-02-13 04:12",
    "weekday": 3,
    "issue_id": 2187,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.08639",
            "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
            "url": "https://huggingface.co/papers/2502.08639",
            "abstract": "In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. Project page: https://cinemaster-dev.github.io/.",
            "score": 13,
            "issue_id": 2186,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "3d3890b6b6bf7904",
            "authors": [
                "Qinghe Wang",
                "Yawen Luo",
                "Xiaoyu Shi",
                "Xu Jia",
                "Huchuan Lu",
                "Tianfan Xue",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Kuaishou Technology",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08639.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#diffusion",
                    "#3d",
                    "#games"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "CineMaster: Ğ ĞµĞ¶Ğ¸ÑÑĞ¸Ñ€ÑƒĞ¹Ñ‚Ğµ ÑĞ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² 3D",
                    "desc": "CineMaster - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² ÑÑ†ĞµĞ½Ğµ, Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ 3D-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ 3D-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "Empowering Video Creation with 3D Control",
                    "desc": "CineMaster is a new framework designed for generating videos from text while allowing users to control 3D elements like a film director. It operates in two stages: first, users create 3D-aware signals by placing objects and defining camera movements, and second, these signals guide a text-to-video diffusion model to produce the desired video. The framework also includes an automated data annotation system to gather necessary 3D motion and camera data from existing videos. Experiments show that CineMaster outperforms current methods in generating 3D-aware videos from text descriptions."
                },
                "zh": {
                    "title": "CineMasterï¼šè®©è§†é¢‘ç”Ÿæˆå¦‚å¯¼æ¼”èˆ¬å¯æ§",
                    "desc": "CineMasteræ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰3Dæ„ŸçŸ¥å’Œå¯æ§æ€§çš„æ–‡æœ¬åˆ°è§†é¢‘ã€‚å®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿåƒä¸“ä¸šç”µå½±å¯¼æ¼”ä¸€æ ·ç²¾ç¡®æ§åˆ¶åœºæ™¯ä¸­çš„ç‰©ä½“ä½ç½®ã€çµæ´»æ“ä½œ3Dç©ºé—´ä¸­çš„ç‰©ä½“å’Œç›¸æœºï¼Œå¹¶ç›´è§‚åœ°å¸ƒå±€æ¸²æŸ“å¸§ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡äº¤äº’å¼å·¥ä½œæµç¨‹æ„å»º3Dæ„ŸçŸ¥çš„æ¡ä»¶ä¿¡å·ï¼Œç¬¬äºŒé˜¶æ®µåˆ©ç”¨è¿™äº›ä¿¡å·æŒ‡å¯¼æ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆç”¨æˆ·æ‰€éœ€çš„è§†é¢‘å†…å®¹ã€‚æ­¤å¤–ï¼ŒCineMasterè¿˜å»ºç«‹äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ•°æ®æ³¨é‡Šç®¡é“ï¼Œä»¥è§£å†³ç¼ºä¹3Dç‰©ä½“è¿åŠ¨å’Œç›¸æœºå§¿æ€æ ‡æ³¨çš„æ•°æ®é›†é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08127",
            "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
            "url": "https://huggingface.co/papers/2502.08127",
            "abstract": "Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving. Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains. Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks. To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension. All our datasets, models, and codes are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models.",
            "score": 11,
            "issue_id": 2186,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "fa3f08993ba529cd",
            "authors": [
                "Lingfei Qian",
                "Weipeng Zhou",
                "Yan Wang",
                "Xueqing Peng",
                "Jimin Huang",
                "Qianqian Xie"
            ],
            "affiliations": [
                "TheFinAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08127.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#rl",
                    "#open_source",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ’¹",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - ĞºĞ»ÑÑ‡ Ğº ÑƒÑĞ¿ĞµÑ…Ñƒ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ 16 Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Llama-3.1-8B-Instruct, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Financial Reasoning in LLMs with Domain-Specific Adaptations",
                    "desc": "This paper investigates the performance of large language models (LLMs) in financial reasoning tasks, which include interpreting financial text, analyzing tabular data, and solving equations. The authors evaluate 16 advanced LLMs and find that while improved datasets and pretraining enhance financial reasoning, general techniques like Chain-of-Thought (CoT) fine-tuning do not consistently improve results. They propose a new model, enhanced with CoT fine-tuning and reinforcement learning, which shows a significant performance boost of 10% across various financial tasks. The study emphasizes the importance of domain-specific adaptations for better performance in financial reasoning and introduces a leaderboard for future benchmarking."
                },
                "zh": {
                    "title": "é‡‘èæ¨ç†æ¨¡å‹çš„åˆ›æ–°ä¸æå‡",
                    "desc": "æœ¬ç ”ç©¶è¯„ä¼°äº†16ç§å¼ºå¤§çš„è¯­è¨€æ¨¡å‹åœ¨é‡‘èæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬é‡‘èæ–‡æœ¬ã€è¡¨æ ¼æ•°æ®å’Œæ–¹ç¨‹å¼ï¼Œæ¶‰åŠæ•°å€¼æ¨ç†ã€è¡¨æ ¼è§£è¯»å’Œé‡‘èæœ¯è¯­ç†è§£ç­‰æ–¹é¢ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ›´å¥½çš„æ•°æ®é›†å’Œé¢„è®­ç»ƒå¯ä»¥æå‡é‡‘èæ¨ç†èƒ½åŠ›ï¼Œä½†é€šç”¨çš„å¢å¼ºæ–¹æ³•å¦‚é“¾å¼æ¨ç†å¾®è°ƒå¹¶ä¸æ€»æ˜¯æœ‰æ•ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºLlama-3.1-8B-Instructçš„é‡‘èæ¨ç†å¢å¼ºæ¨¡å‹ï¼Œç»è¿‡å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ åï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†10%çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08168",
            "title": "SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation",
            "url": "https://huggingface.co/papers/2502.08168",
            "abstract": "In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.",
            "score": 7,
            "issue_id": 2186,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "1ca2b8d38e35b203",
            "authors": [
                "Zhiming Ma",
                "Xiayang Xiao",
                "Sihao Dong",
                "Peidong Wang",
                "HaiPeng Wang",
                "Qingyun Pan"
            ],
            "affiliations": [
                "China Mobile Group Guangdong Co., Ltd. Guangzhou Branch, Guangzhou, China",
                "China Mobile Internet Company Ltd., Guangzhou, China",
                "School of Computer Science and Engineering, Northeastern University, Shenyang, China",
                "The Key Laboratory for Information Science of Electromagnetic Waves (Ministry of Education), School of Information Science and Technology, Fudan University, Shanghai, China",
                "The School of Automation and Electrical Engineering, Inner Mongolia University of Science and Technology, Baotou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08168.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "ğŸ›°ï¸",
                "ru": {
                    "title": "SARChat-2M: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ SAR Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ SAR, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ SARChat-2M. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° 16 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… VLM, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ SAR."
                },
                "en": {
                    "title": "Empowering SAR Image Interpretation with SARChat-2M!",
                    "desc": "This paper introduces SARChat-2M, a large-scale multimodal dialogue dataset specifically designed for synthetic aperture radar (SAR) images. It contains around 2 million image-text pairs that cover various scenarios and include detailed annotations for effective visual understanding and object detection. The dataset serves as a benchmark for evaluating vision language models (VLMs) in the SAR domain, demonstrating their capabilities in interpreting SAR images. By conducting experiments on 16 popular VLMs, the study establishes a new multi-task dialogue benchmark, paving the way for advancements in remote sensing applications."
                },
                "zh": {
                    "title": "æ¨åŠ¨SARå›¾åƒè§£è¯»çš„å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†",
                    "desc": "åœ¨åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰é¥æ„Ÿå›¾åƒè§£è¯»é¢†åŸŸï¼Œå°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†ï¼Œå…¶åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡åˆ›æ–°æ€§åœ°æå‡ºäº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„SARå›¾åƒå¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†SARChat-2Mï¼ŒåŒ…å«çº¦200ä¸‡å¯¹é«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬é…å¯¹ï¼Œæ¶µç›–äº†å¤šç§åœºæ™¯å’Œè¯¦ç»†çš„ç›®æ ‡æ³¨é‡Šã€‚è¯¥æ•°æ®é›†ä¸ä»…æ”¯æŒè§†è§‰ç†è§£å’Œç›®æ ‡æ£€æµ‹ç­‰å…³é”®ä»»åŠ¡ï¼Œè¿˜å¼€å‘äº†SARé¢†åŸŸçš„è§†è§‰è¯­è¨€æ•°æ®é›†å’ŒåŸºå‡†ï¼Œè¯„ä¼°VLMsåœ¨SARå›¾åƒè§£è¯»ä¸­çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹16ä¸ªä¸»æµVLMçš„å®éªŒéªŒè¯ï¼Œè¯¥æ•°æ®é›†çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†å……åˆ†è¯æ˜ï¼Œå¹¶æˆåŠŸå»ºç«‹äº†SARé¢†åŸŸçš„ç¬¬ä¸€ä¸ªå¤šä»»åŠ¡å¯¹è¯åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07864",
            "title": "TransMLA: Multi-head Latent Attention Is All You Need",
            "url": "https://huggingface.co/papers/2502.07864",
            "abstract": "Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1.",
            "score": 4,
            "issue_id": 2186,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "dbff84dafe8c2312",
            "authors": [
                "Fanxu Meng",
                "Zengwei Yao",
                "Muhan Zhang"
            ],
            "affiliations": [
                "Institute for Artificial Intelligence, Peking University",
                "State Key Laboratory of General Artificial Intelligence, Peking University",
                "Xiaomi Corp., Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07864.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#inference",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: MLA Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Multi-head Latent Attention (MLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑƒĞ·ĞºĞ¸Ñ… Ğ¼ĞµÑÑ‚ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. MLA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ° Ğ² ÑĞ»Ğ¾ÑÑ… ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ KV. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ TransMLA Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Group Query Attention (GQA) Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MLA. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ KV-ĞºÑÑˆĞ° Ğ¸ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Transforming Attention: From GQA to Efficient MLA",
                    "desc": "This paper introduces Multi-head Latent Attention (MLA) as a solution to communication bottlenecks in large language models (LLMs) caused by hardware limitations. MLA utilizes low-rank matrices in the key-value (KV) layers to compress KV states, significantly reducing cache size and improving inference speed. The authors demonstrate that Group Query Attention (GQA) can be represented by MLA without increasing KV cache overhead, while MLA offers greater flexibility. To facilitate the adoption of MLA, they propose TransMLA, a method for converting GQA-based models into MLA-based ones, allowing for enhanced expressiveness without additional cache size."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹æ•ˆç‡çš„å…³é”®ï¼šå¤šå¤´æ½œåœ¨æ³¨æ„åŠ›",
                    "desc": "ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å½“å‰ç¡¬ä»¶ä¸Šå¸¸å¸¸é¢ä¸´é€šä¿¡ç“¶é¢ˆï¼Œè€Œä¸ä»…ä»…æ˜¯è®¡ç®—é™åˆ¶ã€‚å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰é€šè¿‡åœ¨é”®å€¼ï¼ˆKVï¼‰å±‚ä¸­ä½¿ç”¨ä½ç§©çŸ©é˜µæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»è€Œå…è®¸å‹ç¼©çš„æ½œåœ¨KVçŠ¶æ€è¢«ç¼“å­˜ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—å‡å°‘äº†KVç¼“å­˜çš„å¤§å°ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„å¤šå¤´æ³¨æ„åŠ›ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«ã€‚æ­¤å¤–ï¼ŒMLAä½¿ç”¨ä¸ŠæŠ•å½±çŸ©é˜µæ¥å¢åŠ è¡¨è¾¾èƒ½åŠ›ï¼Œä»¥é¢å¤–çš„è®¡ç®—æ¢å–å‡å°‘çš„é€šä¿¡å¼€é”€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07737",
            "title": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling",
            "url": "https://huggingface.co/papers/2502.07737",
            "abstract": "Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed. In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. By uniformly decomposing video content into equal-sized blocks (e.g., rows or frames), we shift the generation unit from individual tokens to blocks, allowing each token in the current block to simultaneously predict the corresponding token in the next block. Unlike traditional AR modeling, our framework employs bidirectional attention within each block, enabling tokens to capture more robust spatial dependencies. By predicting multiple tokens in parallel, NBP models significantly reduce the number of generation steps, leading to faster and more efficient inference. Our model achieves FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4. Furthermore, thanks to the reduced number of inference steps, the NBP model generates 8.89 frames (128x128 resolution) per second, achieving an 11x speedup. We also explored model scales ranging from 700M to 3B parameters, observing significant improvements in generation quality, with FVD scores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600, demonstrating the scalability of our approach.",
            "score": 1,
            "issue_id": 2186,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "8038af0ecacc031f",
            "authors": [
                "Shuhuai Ren",
                "Shuming Ma",
                "Xu Sun",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07737.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#video",
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "NBP: Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Next-Block Prediction (NBP). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Next-Token Prediction, NBP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµrÑ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ NBP Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FVD Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… UCF101 Ğ¸ K600, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Next-Block Prediction",
                    "desc": "This paper introduces a new method for generating videos called Next-Block Prediction (NBP), which improves upon the traditional Next-Token Prediction (NTP) approach. NBP changes the way video content is generated by using blocks instead of individual tokens, allowing for simultaneous predictions within each block. This method utilizes bidirectional attention to enhance the understanding of spatial relationships in the video, leading to better quality outputs. As a result, NBP not only speeds up the generation process significantly but also achieves superior performance metrics compared to the standard NTP model."
                },
                "zh": {
                    "title": "è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´ï¼šä¸‹ä¸€å—é¢„æµ‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠè‡ªå›å½’æ¡†æ¶ï¼Œç§°ä¸ºä¸‹ä¸€å—é¢„æµ‹ï¼ˆNBPï¼‰ï¼Œç”¨äºè§†é¢‘ç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ–¹æ³•ä¸åŒï¼ŒNBPé€šè¿‡å°†è§†é¢‘å†…å®¹å‡åŒ€åˆ†è§£ä¸ºç›¸ç­‰å¤§å°çš„å—ï¼Œä½¿å¾—æ¯ä¸ªå—å†…çš„æ ‡è®°å¯ä»¥åŒæ—¶é¢„æµ‹ä¸‹ä¸€ä¸ªå—çš„å¯¹åº”æ ‡è®°ï¼Œä»è€Œæ•æ‰æ›´å¼ºçš„ç©ºé—´ä¾èµ–æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¹¶è¡Œé¢„æµ‹å¤šä¸ªæ ‡è®°ï¼Œæ˜¾è‘—å‡å°‘äº†ç”Ÿæˆæ­¥éª¤ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œè¾¾åˆ°äº†æ¯ç§’ç”Ÿæˆ8.89å¸§çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNBPåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ä¸Šçš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07599",
            "title": "DPO-Shift: Shifting the Distribution of Direct Preference Optimization",
            "url": "https://huggingface.co/papers/2502.07599",
            "abstract": "Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \\method to controllably shift the distribution of the chosen probability. Then, we show that \\method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \\method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.",
            "score": 1,
            "issue_id": 2186,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "85d178e03a57421f",
            "authors": [
                "Xiliang Yang",
                "Feng Jiang",
                "Qianen Zhang",
                "Lei Zhao",
                "Xiao Li"
            ],
            "affiliations": [
                "Institute of Translational Medicine and National Center for Translational Medicine, Shanghai Jiao Tong University",
                "School of Data Science, The Chinese University of Hong Kong, Shenzhen",
                "School of Mathematics, South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07599.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ DPO-Shift Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Direct Preference Optimization (DPO). DPO-Shift Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ ÑĞ¼ĞµÑ‰Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DPO-Shift Ğ½Ğ°Ğ´ DPO Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MT-Bench."
                },
                "en": {
                    "title": "Mitigating Likelihood Displacement in Language Model Training",
                    "desc": "This paper addresses the issue of likelihood displacement in Direct Preference Optimization (DPO) for aligning language models with human preferences. The authors introduce a new method, referred to as \textit{method}, which aims to control the distribution of chosen response probabilities during training. They highlight a trade-off between enhancing the chosen probability and the reward margin, supported by both theoretical insights and experimental results. The findings indicate that \textit{method} outperforms DPO in various downstream tasks, providing a promising solution to the challenges posed by likelihood displacement."
                },
                "zh": {
                    "title": "è§£å†³é€‰æ‹©æ¦‚ç‡ä¸‹é™çš„æœ‰æ•ˆæ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•\textit{method}ï¼Œæ—¨åœ¨è§£å†³ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸­å‡ºç°çš„é€‰æ‹©æ¦‚ç‡ä¸‹é™é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¯¹é€‰æ‹©å“åº”çš„æ¦‚ç‡å¾€å¾€ä¼šé™ä½ï¼Œè¿™è¢«ç§°ä¸ºä¼¼ç„¶ä½ç§»ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ§åˆ¶é€‰æ‹©æ¦‚ç‡çš„åˆ†å¸ƒï¼Œä»è€Œæ”¹å–„æ¨¡å‹çš„è¡¨ç°ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæˆ‘ä»¬è¯æ˜äº†\textit{method}åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»Ÿçš„DPOæ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-12.html",
    "link_next": "2025-02-14.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "12.02",
        "en": "02/12",
        "zh": "2æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "14.02",
        "en": "02/14",
        "zh": "2æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥æ˜¾è‘—æå‡å¤æ‚ç¼–ç¨‹å’Œæ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ä¸ªé€šç”¨æ¨ç†æ¨¡å‹ - OpenAI o1 å’Œ o3 çš„æ—©æœŸç‰ˆæœ¬ï¼Œä»¥åŠä¸€ä¸ªé’ˆå¯¹2024å¹´å›½é™…ä¿¡æ¯å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIOIï¼‰è®¾è®¡çš„ç‰¹å®šé¢†åŸŸç³»ç»Ÿ o1-ioiã€‚æˆ‘ä»¬åœ¨IOI 2024ä¸Šä½¿ç”¨ o1-ioi å‚èµ›ï¼Œå¹¶åœ¨æ”¾å®½çš„æ¯”èµ›çº¦æŸä¸‹è·å¾—äº†é‡‘ç‰Œã€‚ç„¶è€Œï¼Œåç»­æ¨¡å‹å¦‚ o3 åœ¨æ²¡æœ‰æ‰‹å·¥åˆ¶å®šçš„ç‰¹å®šé¢†åŸŸç­–ç•¥æˆ–æ”¾å®½çº¦æŸçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è·å¾—é‡‘ç‰Œã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œè™½ç„¶ä¸“é—¨çš„æµæ°´çº¿å¦‚ o1-ioi å¸¦æ¥äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä½†æ‰©å±•çš„é€šç”¨ o3 æ¨¡å‹åœ¨ä¸ä¾èµ–æ‰‹å·¥åˆ¶å®šçš„æ¨ç†å¯å‘å¼çš„æƒ…å†µä¸‹è¶…è¶Šäº†è¿™äº›ç»“æœã€‚",
        "title": "Competitive Programming with Large Reasoning Models",
        "pinyin": "WÇ’men zhÇnshÃ¬le qiÃ¡ng huÃ  xuÃ©xÃ­ yÃ¬ngyÃ²ng yÃº dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) kÄ›yÇ xiÇnzhÃ¹ tÃ­shÄ“ng fÃ¹zÃ¡ bÇan chÃ©ng yÇ” tuÄ«lÇ rÃ¨nwÃ¹ de biÇoxiÃ n. WÇ’men bÇjiÃ ole liÇng gÃ¨ tÅngyÃ²ng tuÄ«lÇ mÃ³xÃ­ng - OpenAI o1 hÃ© o3 de zÇoqÄ« bÇnbÄ›n, yÇjiÃ  yÄ«gÃ¨ zhÇduÃ¬ 2024 niÃ¡n guÃ³jÃ¬ xÃ¬nxÄ« xuÃ© Ã olÃ­npÇkÃ¨ (IOI) shÃ¨jÃ¬ de tÃ¨dÃ¬ng yÃ¹yÃ­ xÃ¬tÇ’ng o1-ioi. WÇ’men zÃ i IOI 2024 shÃ ng shÇyÃ²ng o1-ioi cÄnsÃ i, bÃ¬ng zÃ i fÃ ngkuÄn de bÇsÃ i yuÄ“shÃ¹ xiÃ  huÃ²dÃ©le jÄ«npÃ¡i. RÃ¡n'Ã©r, hÃ²uxÃ¹ mÃ³xÃ­ng rÃº o3 zÃ i mÃ©iyÇ’u shÇ’ugÅng zhÃ¬dÃ¬ng de tÃ¨dÃ¬ng yÃ¹yÃ­ cÃ¨lÃ¼Ã¨ huÃ² fÃ ngkuÄn yuÄ“shÃ¹ de qÃ­ngkuÃ ng xiÃ  yÄ› nÃ©ng huÃ²dÃ© jÄ«npÃ¡i. WÇ’men de fÄxiÃ n biÇomÃ­ng, suÄ«rÃ¡n zhuÄnmÃ©n de liÃºshuÇxiÃ n rÃº o1-ioi dÃ ilÃ¡i le xiÇnzhÃ¹ de gÇijÃ¬n, dÃ n kuÃ²zhÇn de tÅngyÃ²ng o3 mÃ³xÃ­ng zÃ i bÃ¹ yÄ«lÃ i shÇ’ugÅng zhÃ¬dÃ¬ng de tuÄ«lÇ qÇfÇshÃ¬ de qÃ­ngkuÃ ng xiÃ  chÄoyuÃ¨le zhÃ¨xiÄ“ jiÃ©guÇ’.",
        "vocab": "[{'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'display'}, {'word': 'å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'qiÃ¡ngâ€‹huÃ â€‹xuÃ©â€‹xÃ­', 'trans': 'reinforcement learning'}, {'word': 'åº”ç”¨äº', 'pinyin': 'yÃ¬ngâ€‹yÃ²ngâ€‹yÃº', 'trans': 'apply to'}, {'word': 'å¤§å‹è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ â€‹xÃ­ngâ€‹yÇ”â€‹yÃ¡nâ€‹mÃ³â€‹xÃ­ng', 'trans': 'large language model'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnâ€‹zhÃ¹', 'trans': 'significant'}, {'word': 'æå‡', 'pinyin': 'tÃ­â€‹shÄ“ng', 'trans': 'improve'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹â€‹zÃ¡', 'trans': 'complex'}, {'word': 'ç¼–ç¨‹', 'pinyin': 'biÄnâ€‹chÃ©ng', 'trans': 'programming'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ«â€‹lÇ', 'trans': 'reasoning'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇoâ€‹xiÃ n', 'trans': 'performance'}, {'word': 'æ¯”è¾ƒ', 'pinyin': 'bÇâ€‹jiÃ o', 'trans': 'compare'}, {'word': 'é€šç”¨', 'pinyin': 'tÅngâ€‹yÃ²ng', 'trans': 'general-purpose'}, {'word': 'é’ˆå¯¹', 'pinyin': 'zhÄ“nâ€‹duÃ¬', 'trans': 'target'}, {'word': 'ç‰¹å®šé¢†åŸŸ', 'pinyin': 'tÃ¨â€‹dÃ¬ngâ€‹lÇngâ€‹yÃ¹', 'trans': 'specific domain'}, {'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬â€‹tÇ’ng', 'trans': 'system'}, {'word': 'å›½é™…ä¿¡æ¯å­¦å¥¥æ—åŒ¹å…‹', 'pinyin': 'guÃ³â€‹jÃ¬â€‹xÃ¬nâ€‹xÄ«â€‹xuÃ©â€‹Ã oâ€‹lÃ­nâ€‹pÇâ€‹kÃ¨', 'trans': 'International Olympiad in Informatics'}, {'word': 'è®¾è®¡', 'pinyin': 'shÃ¨â€‹jÃ¬', 'trans': 'design'}, {'word': 'å‚èµ›', 'pinyin': 'cÄnâ€‹sÃ i', 'trans': 'compete'}, {'word': 'æ”¾å®½', 'pinyin': 'fÃ ngâ€‹kuÄn', 'trans': 'relax'}, {'word': 'çº¦æŸ', 'pinyin': 'yuÄ“â€‹shÃ¹', 'trans': 'constraint'}, {'word': 'è·å¾—', 'pinyin': 'huÃ²â€‹dÃ©', 'trans': 'obtain'}, {'word': 'é‡‘ç‰Œ', 'pinyin': 'jÄ«nâ€‹pÃ¡i', 'trans': 'gold medal'}, {'word': 'åç»­', 'pinyin': 'hÃ²uâ€‹xÃ¹', 'trans': 'subsequent'}, {'word': 'æ‰‹å·¥åˆ¶å®š', 'pinyin': 'shÇ’uâ€‹gÅngâ€‹zhÃ¬â€‹dÃ¬ng', 'trans': 'manually specified'}, {'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨â€‹lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'å¯å‘å¼', 'pinyin': 'qÇâ€‹fÄâ€‹shÃ¬', 'trans': 'heuristic'}, {'word': 'ä¾èµ–', 'pinyin': 'yÄ«â€‹lÃ i', 'trans': 'rely on'}, {'word': 'æ‰©å±•', 'pinyin': 'kuÃ²â€‹zhÇn', 'trans': 'extend'}, {'word': 'è¶…è¶Š', 'pinyin': 'chÄoâ€‹yuÃ¨', 'trans': 'surpass'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ©â€‹guÇ’', 'trans': 'result'}]",
        "trans": "We demonstrated that applying reinforcement learning to large language models (LLMs) can significantly enhance performance in complex programming and reasoning tasks. We compared two general reasoning modelsâ€”early versions of OpenAI o1 and o3â€”and a domain-specific system, o1-ioi, designed for the 2024 International Olympiad in Informatics (IOI). We competed in IOI 2024 using o1-ioi and won a gold medal under relaxed competition constraints. However, subsequent models like o3 were able to achieve gold medals without handcrafted domain-specific strategies or relaxed constraints. Our findings indicate that while specialized pipelines like o1-ioi brought significant improvements, the expanded general o3 model surpassed these results without relying on handcrafted reasoning heuristics.",
        "update_ts": "2025-02-12 09:11"
    }
}