{
    "date": {
        "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 13",
        "zh": "2æœˆ13æ—¥"
    },
    "time_utc": "2025-02-13 07:10",
    "weekday": 3,
    "issue_id": 2190,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.08590",
            "title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
            "url": "https://huggingface.co/papers/2502.08590",
            "abstract": "Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video relighting datasets. A simple application of image relighting models on a frame-by-frame basis leads to several issues: lighting source inconsistency and relighted appearance inconsistency, resulting in flickers in the generated videos. In this work, we propose Light-A-Video, a training-free approach to achieve temporally smooth video relighting. Adapted from image relighting models, Light-A-Video introduces two key techniques to enhance lighting consistency. First, we design a Consistent Light Attention (CLA) module, which enhances cross-frame interactions within the self-attention layers to stabilize the generation of the background lighting source. Second, leveraging the physical principle of light transport independence, we apply linear blending between the source video's appearance and the relighted appearance, using a Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions in illumination. Experiments show that Light-A-Video improves the temporal consistency of relighted video while maintaining the image quality, ensuring coherent lighting transitions across frames. Project page: https://bujiazi.github.io/light-a-video.github.io/.",
            "score": 22,
            "issue_id": 2188,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "dcc03282320e88b1",
            "authors": [
                "Yujie Zhou",
                "Jiazi Bu",
                "Pengyang Ling",
                "Pan Zhang",
                "Tong Wu",
                "Qidong Huang",
                "Jinsong Li",
                "Xiaoyi Dong",
                "Yuhang Zang",
                "Yuhang Cao",
                "Anyi Rao",
                "Jiaqi Wang",
                "Li Niu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08590.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "ĞŸĞ»Ğ°Ğ²Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Light-A-Video - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Consistent Light Attention (CLA) Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° ÑĞ²ĞµÑ‚Ğ° Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Progressive Light Fusion (PLF) Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. Light-A-Video Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° ÑĞ²ĞµÑ‚Ğ° Ğ¸ Ğ¼ĞµÑ€Ñ†Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Achieving Smooth Video Relighting with Light-A-Video",
                    "desc": "This paper presents Light-A-Video, a novel approach for video relighting that addresses the challenges of lighting consistency and flickering in generated videos. Unlike traditional methods that apply image relighting on a frame-by-frame basis, Light-A-Video utilizes a training-free strategy to enhance temporal smoothness. It introduces a Consistent Light Attention (CLA) module to improve interactions between frames and stabilize background lighting. Additionally, the Progressive Light Fusion (PLF) technique is employed to blend the original and relighted appearances, ensuring coherent lighting transitions across video frames."
                },
                "zh": {
                    "title": "å®ç°è§†é¢‘é‡å…‰çš„ä¸€è‡´æ€§ä¸å¹³æ»‘æ€§",
                    "desc": "æœ€è¿‘ï¼Œå›¾åƒé‡å…‰æ¨¡å‹çš„è¿›å±•å¾—ç›Šäºå¤§è§„æ¨¡æ•°æ®é›†å’Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œä½¿å¾—ä¸€è‡´çš„å…‰ç…§æ•ˆæœå¾—ä»¥å®ç°ã€‚ç„¶è€Œï¼Œè§†é¢‘é‡å…‰ä»ç„¶æ»åï¼Œä¸»è¦æ˜¯ç”±äºè®­ç»ƒæˆæœ¬é«˜å’Œç¼ºä¹å¤šæ ·åŒ–çš„é«˜è´¨é‡è§†é¢‘é‡å…‰æ•°æ®é›†ã€‚ç®€å•åœ°å°†å›¾åƒé‡å…‰æ¨¡å‹é€å¸§åº”ç”¨ä¼šå¯¼è‡´å…‰æºä¸ä¸€è‡´å’Œé‡å…‰å¤–è§‚ä¸ä¸€è‡´ï¼Œä»è€Œåœ¨ç”Ÿæˆçš„è§†é¢‘ä¸­äº§ç”Ÿé—ªçƒç°è±¡ã€‚æˆ‘ä»¬æå‡ºäº†Light-A-Videoï¼Œè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°æ—¶é—´ä¸Šå¹³æ»‘çš„è§†é¢‘é‡å…‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07870",
            "title": "TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation",
            "url": "https://huggingface.co/papers/2502.07870",
            "abstract": "Text-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text and visuals is essential for conveying complex information. However, despite these advances, the generation of images containing long-form text remains a persistent challenge, largely due to the limitations of existing datasets, which often focus on shorter and simpler text. To address this gap, we introduce TextAtlas5M, a novel dataset specifically designed to evaluate long-text rendering in text-conditioned image generation. Our dataset consists of 5 million long-text generated and collected images across diverse data types, enabling comprehensive evaluation of large-scale generative models on long-text image generation. We further curate 3000 human-improved test set TextAtlasEval across 3 data domains, establishing one of the most extensive benchmarks for text-conditioned generation. Evaluations suggest that the TextAtlasEval benchmarks present significant challenges even for the most advanced proprietary models (e.g. GPT4o with DallE-3), while their open-source counterparts show an even larger performance gap. These evidences position TextAtlas5M as a valuable dataset for training and evaluating future-generation text-conditioned image generation models.",
            "score": 20,
            "issue_id": 2188,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "5b552b320e2e69f0",
            "authors": [
                "Alex Jinpeng Wang",
                "Dongxing Mao",
                "Jiawei Zhang",
                "Weiming Han",
                "Zhuobai Dong",
                "Linjie Li",
                "Yiqi Lin",
                "Zhengyuan Yang",
                "Libo Qin",
                "Fuwei Zhang",
                "Lijuan Wang",
                "Min Li"
            ],
            "affiliations": [
                "Central South University",
                "Microsoft",
                "National University of Singapore",
                "North University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07870.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#long_context",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "TextAtlas5M: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ TextAtlas5M Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ TextAtlasEval Ğ¸Ğ· 3000 ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, GPT4o Ñ DallE-3) ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑĞ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ÑÑ‚Ğ¸Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Empowering Long-Text Image Generation with TextAtlas5M",
                    "desc": "This paper introduces TextAtlas5M, a new dataset aimed at improving text-conditioned image generation, particularly for long-form text. Current models struggle with generating images that include dense and intricate text, as existing datasets primarily focus on shorter text. TextAtlas5M contains 5 million images with long text, allowing for better evaluation of generative models. The paper also presents TextAtlasEval, a curated test set that highlights the challenges faced by both proprietary and open-source models in this area."
                },
                "zh": {
                    "title": "é•¿æ–‡æœ¬å›¾åƒç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†TextAtlas5Mï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬æ¡ä»¶ä¸‹å›¾åƒç”Ÿæˆä¸­é•¿æ–‡æœ¬æ¸²æŸ“çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ•°æ®é›†é€šå¸¸åªå…³æ³¨çŸ­æ–‡æœ¬ï¼Œé™åˆ¶äº†ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ã€‚TextAtlas5MåŒ…å«500ä¸‡å¼ é•¿æ–‡æœ¬ç”Ÿæˆçš„å›¾åƒï¼Œè¦†ç›–å¤šç§æ•°æ®ç±»å‹ï¼Œä¸ºå¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°æä¾›äº†åŸºç¡€ã€‚é€šè¿‡å»ºç«‹3000ä¸ªç»è¿‡äººå·¥æ”¹è¿›çš„æµ‹è¯•é›†TextAtlasEvalï¼Œæœ¬æ–‡ä¸ºæ–‡æœ¬æ¡ä»¶ç”Ÿæˆæä¾›äº†ä¸€ä¸ªå¹¿æ³›çš„åŸºå‡†ï¼Œå¸®åŠ©æœªæ¥çš„ç ”ç©¶å’Œæ¨¡å‹è®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08639",
            "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
            "url": "https://huggingface.co/papers/2502.08639",
            "abstract": "In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. Project page: https://cinemaster-dev.github.io/.",
            "score": 19,
            "issue_id": 2186,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "3d3890b6b6bf7904",
            "authors": [
                "Qinghe Wang",
                "Yawen Luo",
                "Xiaoyu Shi",
                "Xu Jia",
                "Huchuan Lu",
                "Tianfan Xue",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Kuaishou Technology",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08639.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#diffusion",
                    "#3d",
                    "#games"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "CineMaster: Ğ ĞµĞ¶Ğ¸ÑÑĞ¸Ñ€ÑƒĞ¹Ñ‚Ğµ ÑĞ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² 3D",
                    "desc": "CineMaster - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² ÑÑ†ĞµĞ½Ğµ, Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ 3D-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ 3D-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "Empowering Video Creation with 3D Control",
                    "desc": "CineMaster is a new framework designed for generating videos from text while allowing users to control 3D elements like a film director. It operates in two stages: first, users create 3D-aware signals by placing objects and defining camera movements, and second, these signals guide a text-to-video diffusion model to produce the desired video. The framework also includes an automated data annotation system to gather necessary 3D motion and camera data from existing videos. Experiments show that CineMaster outperforms current methods in generating 3D-aware videos from text descriptions."
                },
                "zh": {
                    "title": "CineMasterï¼šè®©è§†é¢‘ç”Ÿæˆå¦‚å¯¼æ¼”èˆ¬å¯æ§",
                    "desc": "CineMasteræ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰3Dæ„ŸçŸ¥å’Œå¯æ§æ€§çš„æ–‡æœ¬åˆ°è§†é¢‘ã€‚å®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿåƒä¸“ä¸šç”µå½±å¯¼æ¼”ä¸€æ ·ç²¾ç¡®æ§åˆ¶åœºæ™¯ä¸­çš„ç‰©ä½“ä½ç½®ã€çµæ´»æ“ä½œ3Dç©ºé—´ä¸­çš„ç‰©ä½“å’Œç›¸æœºï¼Œå¹¶ç›´è§‚åœ°å¸ƒå±€æ¸²æŸ“å¸§ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡äº¤äº’å¼å·¥ä½œæµç¨‹æ„å»º3Dæ„ŸçŸ¥çš„æ¡ä»¶ä¿¡å·ï¼Œç¬¬äºŒé˜¶æ®µåˆ©ç”¨è¿™äº›ä¿¡å·æŒ‡å¯¼æ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆç”¨æˆ·æ‰€éœ€çš„è§†é¢‘å†…å®¹ã€‚æ­¤å¤–ï¼ŒCineMasterè¿˜å»ºç«‹äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ•°æ®æ³¨é‡Šç®¡é“ï¼Œä»¥è§£å†³ç¼ºä¹3Dç‰©ä½“è¿åŠ¨å’Œç›¸æœºå§¿æ€æ ‡æ³¨çš„æ•°æ®é›†é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07563",
            "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
            "url": "https://huggingface.co/papers/2502.07563",
            "abstract": "Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.",
            "score": 15,
            "issue_id": 2188,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "f5a4cfd0a0d018ae",
            "authors": [
                "Weigao Sun",
                "Disen Lan",
                "Yiran Zhong",
                "Xiaoye Qu",
                "Yu Cheng"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "South China University of Technology",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07563.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑĞ²ĞµÑ€Ñ…Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ LASP-2 Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…. LASP-2 Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ñƒ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ AllGather Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ LASP-2H Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Linear-Llama3 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LASP-2 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Boosting Efficiency in Linear Attention with LASP-2",
                    "desc": "This paper presents LASP-2, a new sequence parallelism (SP) method designed to improve the efficiency of training linear attention transformer models with very long input sequences. LASP-2 optimizes communication and computation parallelism by minimizing the communication requirements for linear attention layers, allowing for a single AllGather operation on intermediate memory states. This approach leads to significant enhancements in both communication and computation parallelism, enabling better scalability in distributed systems. Additionally, LASP-2 is extended to LASP-2H, which applies similar optimizations to standard attention modules, providing an efficient solution for hybrid models that utilize both linear and standard attention."
                },
                "zh": {
                    "title": "LASP-2ï¼šæå‡çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹çš„å¹¶è¡Œæ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åºåˆ—å¹¶è¡Œæ–¹æ³•LASP-2ï¼Œæ—¨åœ¨æé«˜çº¿æ€§æ³¨æ„åŠ›å˜æ¢å™¨æ¨¡å‹åœ¨å¤„ç†éå¸¸é•¿è¾“å…¥åºåˆ—æ—¶çš„é€šä¿¡å’Œè®¡ç®—å¹¶è¡Œæ€§ã€‚ä¸ä¹‹å‰çš„LASPæ–¹æ³•ç›¸æ¯”ï¼ŒLASP-2é‡æ–°æ€è€ƒäº†çº¿æ€§æ³¨æ„åŠ›å±‚çš„æœ€å°é€šä¿¡éœ€æ±‚ï¼Œå¹¶é‡æ–°ç»„ç»‡äº†é€šä¿¡å’Œè®¡ç®—çš„å·¥ä½œæµç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒLASP-2åªéœ€åœ¨ä¸­é—´å†…å­˜çŠ¶æ€ä¸Šè¿›è¡Œä¸€æ¬¡AllGatheré›†ä½“é€šä¿¡ï¼Œæ˜¾è‘—æé«˜äº†é€šä¿¡å’Œè®¡ç®—çš„å¹¶è¡Œæ€§åŠå…¶é‡å ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒLASP-2åœ¨è®­ç»ƒé€Ÿåº¦ä¸Šæ¯”LASPæé«˜äº†15.2%ï¼Œæ¯”ç¯å½¢æ³¨æ„åŠ›æé«˜äº†36.6%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08047",
            "title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation",
            "url": "https://huggingface.co/papers/2502.08047",
            "abstract": "Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or the interface not being in its default state-often lead to planning errors. This issue is widespread in real user scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation.",
            "score": 12,
            "issue_id": 2190,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "0f83dccb05181f21",
            "authors": [
                "Henry Hengyuan Zhao",
                "Difei Gao",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08047.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GUI",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WorldGUI - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº GUI-Thinker, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ GUI. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GUI-Thinker Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Claude-3.5 (Computer Use) Ğ¿Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² WorldGUI. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸, Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GUI."
                },
                "en": {
                    "title": "Enhancing GUI Automation with WorldGUI and GUI-Thinker",
                    "desc": "This paper addresses the challenges faced by current GUI agents in planning tasks due to their sensitivity to the initial state of the environment. It introduces WorldGUI, a new benchmark that simulates real user interactions by incorporating various initial states across multiple software applications. To tackle the complexities of dynamic GUI automation, the authors propose GUI-Thinker, a framework that utilizes a critique mechanism to improve decision-making in unpredictable scenarios. Experimental results show that GUI-Thinker outperforms existing models, highlighting its effectiveness in enhancing the success rate of GUI automation tasks."
                },
                "zh": {
                    "title": "æå‡GUIè‡ªåŠ¨åŒ–çš„å…³é”®æ€ç»´æ¡†æ¶",
                    "desc": "å½“å‰çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨å…ƒç´ å®šä½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§„åˆ’æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹ç¯å¢ƒåˆå§‹çŠ¶æ€çš„æ•æ„Ÿæ€§ã€‚åˆå§‹çŠ¶æ€çš„å¾®å°å·®å¼‚ï¼Œä¾‹å¦‚ç›®æ ‡è½¯ä»¶æœªæ‰“å¼€æˆ–ç•Œé¢ä¸åœ¨é»˜è®¤çŠ¶æ€ï¼Œå¸¸å¸¸å¯¼è‡´è§„åˆ’é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†WorldGUIï¼Œä¸€ä¸ªæ–°é¢–çš„GUIåŸºå‡†ï¼Œè®¾è®¡äº†å…·æœ‰å¤šç§åˆå§‹çŠ¶æ€çš„GUIä»»åŠ¡ï¼Œä»¥æ¨¡æ‹ŸçœŸå®çš„è®¡ç®—æœºç”¨æˆ·äº¤äº’ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†GUI-Thinkerï¼Œä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œé€šè¿‡æ‰¹åˆ¤æœºåˆ¶æœ‰æ•ˆç®¡ç†GUIäº¤äº’çš„ä¸å¯é¢„æµ‹æ€§å’Œå¤æ‚æ€§ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨WorldGUIä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æ¯”Claude-3.5é«˜å‡º14.9%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08127",
            "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
            "url": "https://huggingface.co/papers/2502.08127",
            "abstract": "Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving. Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains. Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks. To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension. All our datasets, models, and codes are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models.",
            "score": 12,
            "issue_id": 2186,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "fa3f08993ba529cd",
            "authors": [
                "Lingfei Qian",
                "Weipeng Zhou",
                "Yan Wang",
                "Xueqing Peng",
                "Jimin Huang",
                "Qianqian Xie"
            ],
            "affiliations": [
                "TheFinAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08127.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#rl",
                    "#open_source",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ’¹",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - ĞºĞ»ÑÑ‡ Ğº ÑƒÑĞ¿ĞµÑ…Ñƒ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ 16 Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Llama-3.1-8B-Instruct, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Financial Reasoning in LLMs with Domain-Specific Adaptations",
                    "desc": "This paper investigates the performance of large language models (LLMs) in financial reasoning tasks, which include interpreting financial text, analyzing tabular data, and solving equations. The authors evaluate 16 advanced LLMs and find that while improved datasets and pretraining enhance financial reasoning, general techniques like Chain-of-Thought (CoT) fine-tuning do not consistently improve results. They propose a new model, enhanced with CoT fine-tuning and reinforcement learning, which shows a significant performance boost of 10% across various financial tasks. The study emphasizes the importance of domain-specific adaptations for better performance in financial reasoning and introduces a leaderboard for future benchmarking."
                },
                "zh": {
                    "title": "é‡‘èæ¨ç†æ¨¡å‹çš„åˆ›æ–°ä¸æå‡",
                    "desc": "æœ¬ç ”ç©¶è¯„ä¼°äº†16ç§å¼ºå¤§çš„è¯­è¨€æ¨¡å‹åœ¨é‡‘èæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬é‡‘èæ–‡æœ¬ã€è¡¨æ ¼æ•°æ®å’Œæ–¹ç¨‹å¼ï¼Œæ¶‰åŠæ•°å€¼æ¨ç†ã€è¡¨æ ¼è§£è¯»å’Œé‡‘èæœ¯è¯­ç†è§£ç­‰æ–¹é¢ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ›´å¥½çš„æ•°æ®é›†å’Œé¢„è®­ç»ƒå¯ä»¥æå‡é‡‘èæ¨ç†èƒ½åŠ›ï¼Œä½†é€šç”¨çš„å¢å¼ºæ–¹æ³•å¦‚é“¾å¼æ¨ç†å¾®è°ƒå¹¶ä¸æ€»æ˜¯æœ‰æ•ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºLlama-3.1-8B-Instructçš„é‡‘èæ¨ç†å¢å¼ºæ¨¡å‹ï¼Œç»è¿‡å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ åï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†10%çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07864",
            "title": "TransMLA: Multi-head Latent Attention Is All You Need",
            "url": "https://huggingface.co/papers/2502.07864",
            "abstract": "Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1.",
            "score": 10,
            "issue_id": 2186,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "dbff84dafe8c2312",
            "authors": [
                "Fanxu Meng",
                "Zengwei Yao",
                "Muhan Zhang"
            ],
            "affiliations": [
                "Institute for Artificial Intelligence, Peking University",
                "State Key Laboratory of General Artificial Intelligence, Peking University",
                "Xiaomi Corp., Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07864.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#inference",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: MLA Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Multi-head Latent Attention (MLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑƒĞ·ĞºĞ¸Ñ… Ğ¼ĞµÑÑ‚ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. MLA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ° Ğ² ÑĞ»Ğ¾ÑÑ… ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ KV. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ TransMLA Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Group Query Attention (GQA) Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MLA. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ KV-ĞºÑÑˆĞ° Ğ¸ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Transforming Attention: From GQA to Efficient MLA",
                    "desc": "This paper introduces Multi-head Latent Attention (MLA) as a solution to communication bottlenecks in large language models (LLMs) caused by hardware limitations. MLA utilizes low-rank matrices in the key-value (KV) layers to compress KV states, significantly reducing cache size and improving inference speed. The authors demonstrate that Group Query Attention (GQA) can be represented by MLA without increasing KV cache overhead, while MLA offers greater flexibility. To facilitate the adoption of MLA, they propose TransMLA, a method for converting GQA-based models into MLA-based ones, allowing for enhanced expressiveness without additional cache size."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹æ•ˆç‡çš„å…³é”®ï¼šå¤šå¤´æ½œåœ¨æ³¨æ„åŠ›",
                    "desc": "ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å½“å‰ç¡¬ä»¶ä¸Šå¸¸å¸¸é¢ä¸´é€šä¿¡ç“¶é¢ˆï¼Œè€Œä¸ä»…ä»…æ˜¯è®¡ç®—é™åˆ¶ã€‚å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰é€šè¿‡åœ¨é”®å€¼ï¼ˆKVï¼‰å±‚ä¸­ä½¿ç”¨ä½ç§©çŸ©é˜µæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»è€Œå…è®¸å‹ç¼©çš„æ½œåœ¨KVçŠ¶æ€è¢«ç¼“å­˜ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—å‡å°‘äº†KVç¼“å­˜çš„å¤§å°ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„å¤šå¤´æ³¨æ„åŠ›ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«ã€‚æ­¤å¤–ï¼ŒMLAä½¿ç”¨ä¸ŠæŠ•å½±çŸ©é˜µæ¥å¢åŠ è¡¨è¾¾èƒ½åŠ›ï¼Œä»¥é¢å¤–çš„è®¡ç®—æ¢å–å‡å°‘çš„é€šä¿¡å¼€é”€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08168",
            "title": "SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation",
            "url": "https://huggingface.co/papers/2502.08168",
            "abstract": "In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.",
            "score": 7,
            "issue_id": 2186,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "1ca2b8d38e35b203",
            "authors": [
                "Zhiming Ma",
                "Xiayang Xiao",
                "Sihao Dong",
                "Peidong Wang",
                "HaiPeng Wang",
                "Qingyun Pan"
            ],
            "affiliations": [
                "China Mobile Group Guangdong Co., Ltd. Guangzhou Branch, Guangzhou, China",
                "China Mobile Internet Company Ltd., Guangzhou, China",
                "School of Computer Science and Engineering, Northeastern University, Shenyang, China",
                "The Key Laboratory for Information Science of Electromagnetic Waves (Ministry of Education), School of Information Science and Technology, Fudan University, Shanghai, China",
                "The School of Automation and Electrical Engineering, Inner Mongolia University of Science and Technology, Baotou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08168.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "ğŸ›°ï¸",
                "ru": {
                    "title": "SARChat-2M: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ SAR Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ SAR, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ SARChat-2M. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ†ĞµĞ»ĞµĞ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° 16 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… VLM, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ SAR."
                },
                "en": {
                    "title": "Empowering SAR Image Interpretation with SARChat-2M!",
                    "desc": "This paper introduces SARChat-2M, a large-scale multimodal dialogue dataset specifically designed for synthetic aperture radar (SAR) images. It contains around 2 million image-text pairs that cover various scenarios and include detailed annotations for effective visual understanding and object detection. The dataset serves as a benchmark for evaluating vision language models (VLMs) in the SAR domain, demonstrating their capabilities in interpreting SAR images. By conducting experiments on 16 popular VLMs, the study establishes a new multi-task dialogue benchmark, paving the way for advancements in remote sensing applications."
                },
                "zh": {
                    "title": "æ¨åŠ¨SARå›¾åƒè§£è¯»çš„å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†",
                    "desc": "åœ¨åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰é¥æ„Ÿå›¾åƒè§£è¯»é¢†åŸŸï¼Œå°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹ä¸“ä¸šé¢†åŸŸçš„çŸ¥è¯†ï¼Œå…¶åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡åˆ›æ–°æ€§åœ°æå‡ºäº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„SARå›¾åƒå¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†SARChat-2Mï¼ŒåŒ…å«çº¦200ä¸‡å¯¹é«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬é…å¯¹ï¼Œæ¶µç›–äº†å¤šç§åœºæ™¯å’Œè¯¦ç»†çš„ç›®æ ‡æ³¨é‡Šã€‚è¯¥æ•°æ®é›†ä¸ä»…æ”¯æŒè§†è§‰ç†è§£å’Œç›®æ ‡æ£€æµ‹ç­‰å…³é”®ä»»åŠ¡ï¼Œè¿˜å¼€å‘äº†SARé¢†åŸŸçš„è§†è§‰è¯­è¨€æ•°æ®é›†å’ŒåŸºå‡†ï¼Œè¯„ä¼°VLMsåœ¨SARå›¾åƒè§£è¯»ä¸­çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹16ä¸ªä¸»æµVLMçš„å®éªŒéªŒè¯ï¼Œè¯¥æ•°æ®é›†çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†å……åˆ†è¯æ˜ï¼Œå¹¶æˆåŠŸå»ºç«‹äº†SARé¢†åŸŸçš„ç¬¬ä¸€ä¸ªå¤šä»»åŠ¡å¯¹è¯åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08606",
            "title": "Distillation Scaling Laws",
            "url": "https://huggingface.co/papers/2502.08606",
            "abstract": "We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design.",
            "score": 5,
            "issue_id": 2188,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "774eeded4c92c597",
            "authors": [
                "Dan Busbridge",
                "Amitis Shidani",
                "Floris Weers",
                "Jason Ramapuram",
                "Etai Littwin",
                "Russ Webb"
            ],
            "affiliations": [
                "Apple",
                "University of Oxford, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08606.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ¸ ĞµĞ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ñ‹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ´Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ Ñ€Ğ°ÑÑ‚ĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Maximizing Student Performance through Optimal Distillation Strategies",
                    "desc": "This paper introduces a distillation scaling law that helps predict how well a distilled model will perform based on the available computing resources and how those resources are divided between the teacher and student models. The authors demonstrate that by optimizing the compute allocation, the performance of the student model can be significantly improved, especially when multiple students are distilled from a pre-trained teacher. They also provide guidelines for when to use distillation versus supervised learning, depending on whether a teacher model is available and whether it requires training. Overall, the study enhances our understanding of model distillation and offers practical strategies for effective implementation in machine learning tasks."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è’¸é¦æ¨¡å‹æ€§èƒ½çš„è®¡ç®—æ³•åˆ™",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è’¸é¦ç¼©æ”¾æ³•åˆ™ï¼Œç”¨äºæ ¹æ®è®¡ç®—é¢„ç®—å’Œåœ¨å­¦ç”Ÿä¸æ•™å¸ˆä¹‹é—´çš„åˆ†é…æ¥ä¼°è®¡è’¸é¦æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœé™ä½äº†å¤§è§„æ¨¡ä½¿ç”¨è’¸é¦çš„é£é™©ï¼Œèƒ½å¤Ÿä¼˜åŒ–æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹çš„è®¡ç®—åˆ†é…ï¼Œä»¥æœ€å¤§åŒ–å­¦ç”Ÿçš„è¡¨ç°ã€‚æˆ‘ä»¬æä¾›äº†è®¡ç®—æœ€ä¼˜çš„è’¸é¦æ–¹æ¡ˆï¼Œé€‚ç”¨äºå·²æœ‰æ•™å¸ˆæˆ–éœ€è¦è®­ç»ƒæ•™å¸ˆçš„æƒ…å†µã€‚é€šè¿‡å¤§è§„æ¨¡ç ”ç©¶ï¼Œæˆ‘ä»¬å¢åŠ äº†å¯¹è’¸é¦çš„ç†è§£ï¼Œå¹¶ä¸ºå®éªŒè®¾è®¡æä¾›äº†æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06872",
            "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey",
            "url": "https://huggingface.co/papers/2502.06872",
            "abstract": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact.",
            "score": 2,
            "issue_id": 2188,
            "pub_date": "2025-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "f454782ce3101c66",
            "authors": [
                "Bo Ni",
                "Zheyuan Liu",
                "Leyao Wang",
                "Yongjia Lei",
                "Yuying Zhao",
                "Xueqi Cheng",
                "Qingkai Zeng",
                "Luna Dong",
                "Yinglong Xia",
                "Krishnaram Kenthapadi",
                "Ryan Rossi",
                "Franck Dernoncourt",
                "Md Mehrab Tanjim",
                "Nesreen Ahmed",
                "Xiaorui Liu",
                "Wenqi Fan",
                "Erik Blasch",
                "Yu Wang",
                "Meng Jiang",
                "Tyler Derr"
            ],
            "affiliations": [
                "Adobe Research",
                "Air Force Research Lab",
                "Cisco AI Research",
                "Meta",
                "North Carolina State University",
                "Oracle Health AI",
                "The Hong Kong Polytechnic University",
                "University of Notre Dame",
                "University of Oregon",
                "Vanderbilt University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06872.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#interpretability",
                    "#security",
                    "#survey",
                    "#rag",
                    "#ethics"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑƒÑ‚ÑŒ Ğº Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜: ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ (RAG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ¸ÑĞºĞ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ RAG, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¿ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ². Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾ÑĞ²ĞµÑ‰Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."
                },
                "en": {
                    "title": "Building Trust in Retrieval-Augmented Generation Systems",
                    "desc": "This paper discusses Retrieval-Augmented Generation (RAG), a technique that enhances AI-generated content by incorporating external knowledge for better accuracy and relevance. While RAG improves content generation, it also brings new challenges such as robustness, privacy, and accountability issues that need to be addressed. The authors propose a comprehensive framework that focuses on five key areas: reliability, privacy, safety, fairness, and explainability, to guide future research and development of trustworthy RAG systems. By providing a structured approach, the paper aims to foster innovation and highlight the importance of trust in RAG applications."
                },
                "zh": {
                    "title": "æ„å»ºå¯ä¿¡èµ–çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ",
                    "desc": "æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§å…ˆè¿›çš„æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚é€šè¿‡å°†ä¸Šä¸‹æ–‡æ£€ç´¢ä¸å†…å®¹ç”Ÿæˆç›¸ç»“åˆï¼ŒRAG æä¾›å¯é ä¸”æœ€æ–°çš„å¤–éƒ¨çŸ¥è¯†ï¼Œå‡å°‘å¹»è§‰ç°è±¡ï¼Œå¹¶ç¡®ä¿åœ¨å„ç§ä»»åŠ¡ä¸­ä¿æŒç›¸å…³ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œå°½ç®¡ RAG å–å¾—äº†æˆåŠŸï¼Œä½†æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯¥èŒƒå¼ä¹Ÿå¼•å…¥äº†æ–°çš„é£é™©ï¼ŒåŒ…æ‹¬é²æ£’æ€§é—®é¢˜ã€éšç§é—®é¢˜ã€å¯¹æŠ—æ€§æ”»å‡»å’Œé—®è´£é—®é¢˜ã€‚æœ¬æ–‡æ—¨åœ¨æä¾›ä¸€ä¸ªå…¨é¢çš„è·¯çº¿å›¾ï¼Œä»¥å¼€å‘å¯ä¿¡èµ–çš„ RAG ç³»ç»Ÿï¼Œå›´ç»•å¯é æ€§ã€éšç§ã€å®‰å…¨æ€§ã€å…¬å¹³æ€§ã€å¯è§£é‡Šæ€§å’Œé—®è´£æ€§ç­‰äº”ä¸ªå…³é”®è§†è§’è¿›è¡Œè®¨è®ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05167",
            "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
            "url": "https://huggingface.co/papers/2502.05167",
            "abstract": "Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\" (relevant information) from a \"haystack\" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.",
            "score": 2,
            "issue_id": 2188,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "4ff0f34526efea9f",
            "authors": [
                "Ali Modarressi",
                "Hanieh Deilamsalehy",
                "Franck Dernoncourt",
                "Trung Bui",
                "Ryan A. Rossi",
                "Seunghyun Yoon",
                "Hinrich SchÃ¼tze"
            ],
            "affiliations": [
                "Adobe Research",
                "Center for Information and Language Processing"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05167.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "NoLiMa: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº NoLiMa Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², NoLiMa Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ 12 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… LLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "NoLiMa: Challenging LLMs Beyond Literal Matches",
                    "desc": "This paper introduces NoLiMa, a new benchmark designed to evaluate the capabilities of large language models (LLMs) in retrieving relevant information from extensive contexts. Unlike traditional methods that allow models to rely on literal matches, NoLiMa requires models to infer deeper associations between questions and answers with minimal lexical overlap. The study evaluates 12 popular LLMs, revealing that while they perform well in shorter contexts, their performance significantly declines as context length increases, particularly beyond 1K tokens. The findings indicate that the attention mechanism struggles to effectively retrieve relevant information in longer contexts when direct matches are not available."
                },
                "zh": {
                    "title": "é•¿ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯æ£€ç´¢æŒ‘æˆ˜",
                    "desc": "æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ”¯æŒé•¿è¾¾128Kåˆ°1Mçš„ä¸Šä¸‹æ–‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•NoLiMaï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„é’ˆåœ¨å¹²è‰å †ï¼ˆNIAHï¼‰æµ‹è¯•ä¸åŒï¼ŒNoLiMaè®¾è®¡äº†æœ€å°è¯æ±‡é‡å çš„é’ˆé›†ï¼Œè¦æ±‚æ¨¡å‹æ¨æ–­æ½œåœ¨å…³è”ä»¥æ‰¾åˆ°é’ˆã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå°½ç®¡è¿™äº›æ¨¡å‹åœ¨çŸ­ä¸Šä¸‹æ–‡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹å­—é¢åŒ¹é…çš„æƒ…å†µä¸‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08524",
            "title": "LLM Pretraining with Continuous Concepts",
            "url": "https://huggingface.co/papers/2502.08524",
            "abstract": "Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the model's internal reasoning process.",
            "score": 2,
            "issue_id": 2188,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "99ad370e7cd11e3c",
            "authors": [
                "Jihoon Tack",
                "Jack Lanchantin",
                "Jane Yu",
                "Andrew Cohen",
                "Ilia Kulikov",
                "Janice Lan",
                "Shibo Hao",
                "Yuandong Tian",
                "Jason Weston",
                "Xian Li"
            ],
            "affiliations": [
                "FAIR at Meta",
                "KAIST",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08524.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#interpretability",
                    "#training",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CoCoMix: ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Continuous Concept Mixing (CoCoMix). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, CoCoMix ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CoCoMix Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, CoCoMix ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Language Models with Continuous Concept Mixing",
                    "desc": "This paper introduces Continuous Concept Mixing (CoCoMix), a new framework for pretraining large language models. Unlike traditional methods that focus solely on predicting the next token, CoCoMix integrates continuous concepts derived from a sparse autoencoder into the model's hidden states. This approach improves sample efficiency and enhances performance on various tasks, including language modeling and reasoning. Additionally, CoCoMix allows for better interpretability and control over the model's reasoning by enabling direct manipulation of the predicted concepts."
                },
                "zh": {
                    "title": "è¿ç»­æ¦‚å¿µæ··åˆï¼šæå‡è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ä¸å¯è§£é‡Šæ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºè¿ç»­æ¦‚å¿µæ··åˆï¼ˆCoCoMixï¼‰ï¼Œå®ƒç»“åˆäº†ç¦»æ•£çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œè¿ç»­æ¦‚å¿µã€‚CoCoMixé€šè¿‡å°†ä»é¢„è®­ç»ƒç¨€ç–è‡ªç¼–ç å™¨ä¸­å­¦ä¹ çš„è¿ç»­æ¦‚å¿µä¸æ ‡è®°çš„éšè—è¡¨ç¤ºäº¤é”™æ··åˆï¼Œæ¥ä¼˜åŒ–æ¨¡å‹çš„éšè—çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoCoMixåœ¨æ ·æœ¬æ•ˆç‡ä¸Šè¡¨ç°æ›´ä½³ï¼Œå¹¶ä¸”åœ¨è¯­è¨€å»ºæ¨¡å’Œæ¨ç†ä»»åŠ¡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’ŒçŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚è¯¥æ–¹æ³•è¿˜å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯å¼•å¯¼æ€§ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥ç›´æ¥æ£€æŸ¥å’Œä¿®æ”¹é¢„æµ‹çš„æ¦‚å¿µï¼Œä»è€Œé€æ˜åœ°å¼•å¯¼æ¨¡å‹çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07737",
            "title": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling",
            "url": "https://huggingface.co/papers/2502.07737",
            "abstract": "Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed. In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. By uniformly decomposing video content into equal-sized blocks (e.g., rows or frames), we shift the generation unit from individual tokens to blocks, allowing each token in the current block to simultaneously predict the corresponding token in the next block. Unlike traditional AR modeling, our framework employs bidirectional attention within each block, enabling tokens to capture more robust spatial dependencies. By predicting multiple tokens in parallel, NBP models significantly reduce the number of generation steps, leading to faster and more efficient inference. Our model achieves FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4. Furthermore, thanks to the reduced number of inference steps, the NBP model generates 8.89 frames (128x128 resolution) per second, achieving an 11x speedup. We also explored model scales ranging from 700M to 3B parameters, observing significant improvements in generation quality, with FVD scores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600, demonstrating the scalability of our approach.",
            "score": 2,
            "issue_id": 2186,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "8038af0ecacc031f",
            "authors": [
                "Shuhuai Ren",
                "Shuming Ma",
                "Xu Sun",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07737.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#video",
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "NBP: Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Next-Block Prediction (NBP). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Next-Token Prediction, NBP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµrÑ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ NBP Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ FVD Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… UCF101 Ğ¸ K600, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Next-Block Prediction",
                    "desc": "This paper introduces a new method for generating videos called Next-Block Prediction (NBP), which improves upon the traditional Next-Token Prediction (NTP) approach. NBP changes the way video content is generated by using blocks instead of individual tokens, allowing for simultaneous predictions within each block. This method utilizes bidirectional attention to enhance the understanding of spatial relationships in the video, leading to better quality outputs. As a result, NBP not only speeds up the generation process significantly but also achieves superior performance metrics compared to the standard NTP model."
                },
                "zh": {
                    "title": "è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´ï¼šä¸‹ä¸€å—é¢„æµ‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠè‡ªå›å½’æ¡†æ¶ï¼Œç§°ä¸ºä¸‹ä¸€å—é¢„æµ‹ï¼ˆNBPï¼‰ï¼Œç”¨äºè§†é¢‘ç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„è‡ªå›å½’æ–¹æ³•ä¸åŒï¼ŒNBPé€šè¿‡å°†è§†é¢‘å†…å®¹å‡åŒ€åˆ†è§£ä¸ºç›¸ç­‰å¤§å°çš„å—ï¼Œä½¿å¾—æ¯ä¸ªå—å†…çš„æ ‡è®°å¯ä»¥åŒæ—¶é¢„æµ‹ä¸‹ä¸€ä¸ªå—çš„å¯¹åº”æ ‡è®°ï¼Œä»è€Œæ•æ‰æ›´å¼ºçš„ç©ºé—´ä¾èµ–æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¹¶è¡Œé¢„æµ‹å¤šä¸ªæ ‡è®°ï¼Œæ˜¾è‘—å‡å°‘äº†ç”Ÿæˆæ­¥éª¤ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œè¾¾åˆ°äº†æ¯ç§’ç”Ÿæˆ8.89å¸§çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNBPåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨ç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ä¸Šçš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07599",
            "title": "DPO-Shift: Shifting the Distribution of Direct Preference Optimization",
            "url": "https://huggingface.co/papers/2502.07599",
            "abstract": "Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \\method to controllably shift the distribution of the chosen probability. Then, we show that \\method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \\method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.",
            "score": 2,
            "issue_id": 2186,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "85d178e03a57421f",
            "authors": [
                "Xiliang Yang",
                "Feng Jiang",
                "Qianen Zhang",
                "Lei Zhao",
                "Xiao Li"
            ],
            "affiliations": [
                "Institute of Translational Medicine and National Center for Translational Medicine, Shanghai Jiao Tong University",
                "School of Data Science, The Chinese University of Hong Kong, Shenzhen",
                "School of Mathematics, South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07599.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ DPO-Shift Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Direct Preference Optimization (DPO). DPO-Shift Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ ÑĞ¼ĞµÑ‰Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DPO-Shift Ğ½Ğ°Ğ´ DPO Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MT-Bench."
                },
                "en": {
                    "title": "Mitigating Likelihood Displacement in Language Model Training",
                    "desc": "This paper addresses the issue of likelihood displacement in Direct Preference Optimization (DPO) for aligning language models with human preferences. The authors introduce a new method, referred to as \textit{method}, which aims to control the distribution of chosen response probabilities during training. They highlight a trade-off between enhancing the chosen probability and the reward margin, supported by both theoretical insights and experimental results. The findings indicate that \textit{method} outperforms DPO in various downstream tasks, providing a promising solution to the challenges posed by likelihood displacement."
                },
                "zh": {
                    "title": "è§£å†³é€‰æ‹©æ¦‚ç‡ä¸‹é™çš„æœ‰æ•ˆæ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•\textit{method}ï¼Œæ—¨åœ¨è§£å†³ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸­å‡ºç°çš„é€‰æ‹©æ¦‚ç‡ä¸‹é™é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¯¹é€‰æ‹©å“åº”çš„æ¦‚ç‡å¾€å¾€ä¼šé™ä½ï¼Œè¿™è¢«ç§°ä¸ºä¼¼ç„¶ä½ç§»ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ§åˆ¶é€‰æ‹©æ¦‚ç‡çš„åˆ†å¸ƒï¼Œä»è€Œæ”¹å–„æ¨¡å‹çš„è¡¨ç°ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæˆ‘ä»¬è¯æ˜äº†\textit{method}åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»Ÿçš„DPOæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07985",
            "title": "MetaSC: Test-Time Safety Specification Optimization for Language Models",
            "url": "https://huggingface.co/papers/2502.07985",
            "abstract": "We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code to be released at https://github.com/vicgalle/meta-self-critique.git .",
            "score": 1,
            "issue_id": 2190,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "68244a5483bfc513",
            "authors": [
                "VÃ­ctor Gallego"
            ],
            "affiliations": [
                "Komorebi AI, Madrid, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07985.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#security",
                    "#alignment",
                    "#inference"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ¼ĞµÑ‚Ğ°-ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ğ°. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Dynamic Safety for Language Models: Adapting Prompts for Better Protection",
                    "desc": "This paper introduces a dynamic safety framework designed to enhance the safety of language models during inference without altering their underlying weights. It utilizes a meta-critique mechanism that adaptively updates safety prompts, known as specifications, to improve the model's ability to handle adversarial inputs and general safety tasks. The approach shows significant improvements in safety performance, particularly in avoiding moral harm and ensuring honest responses. Empirical results indicate that the dynamically optimized safety prompts outperform traditional fixed prompts and static defenses in various scenarios."
                },
                "zh": {
                    "title": "åŠ¨æ€ä¼˜åŒ–ï¼Œæå‡è¯­è¨€æ¨¡å‹å®‰å…¨æ€§ï¼",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŠ¨æ€å®‰å…¨æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶çš„å®‰å…¨æ€§æ¨ç†ï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹æƒé‡ã€‚è¯¥æ–¹æ³•åŸºäºè‡ªæˆ‘æ‰¹è¯„æ–¹æ³•çš„æœ€æ–°è¿›å±•ï¼Œåˆ©ç”¨å…ƒæ‰¹è¯„æœºåˆ¶è¿­ä»£æ›´æ–°å®‰å…¨æç¤ºï¼ˆç§°ä¸ºè§„èŒƒï¼‰ï¼Œä»¥è‡ªé€‚åº”åœ°æ¨åŠ¨æ‰¹è¯„å’Œä¿®è®¢è¿‡ç¨‹ã€‚æ­¤æµ‹è¯•æ—¶ä¼˜åŒ–ä¸ä»…æé«˜äº†å¯¹æŠ—æ€§è¶Šç‹±è¯·æ±‚çš„æ€§èƒ½ï¼Œè¿˜åœ¨é¿å…é“å¾·ä¼¤å®³å’Œè¿½æ±‚è¯šå®å›åº”ç­‰å¤šç§å®‰å…¨ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒåŠ¨æ€ä¼˜åŒ–çš„å®‰å…¨æç¤ºç›¸æ¯”äºå›ºå®šç³»ç»Ÿæç¤ºå’Œé™æ€è‡ªæˆ‘æ‰¹è¯„é˜²å¾¡ï¼Œæ˜¾è‘—æé«˜äº†å®‰å…¨è¯„åˆ†ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-12.html",
    "link_next": "2025-02-14.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "12.02",
        "en": "02/12",
        "zh": "2æœˆ12æ—¥"
    },
    "short_date_next": {
        "ru": "14.02",
        "en": "02/14",
        "zh": "2æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 4,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥æ˜¾è‘—æå‡å¤æ‚ç¼–ç¨‹å’Œæ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ä¸ªé€šç”¨æ¨ç†æ¨¡å‹ - OpenAI o1 å’Œ o3 çš„æ—©æœŸç‰ˆæœ¬ï¼Œä»¥åŠä¸€ä¸ªé’ˆå¯¹2024å¹´å›½é™…ä¿¡æ¯å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIOIï¼‰è®¾è®¡çš„ç‰¹å®šé¢†åŸŸç³»ç»Ÿ o1-ioiã€‚æˆ‘ä»¬åœ¨IOI 2024ä¸Šä½¿ç”¨ o1-ioi å‚èµ›ï¼Œå¹¶åœ¨æ”¾å®½çš„æ¯”èµ›çº¦æŸä¸‹è·å¾—äº†é‡‘ç‰Œã€‚ç„¶è€Œï¼Œåç»­æ¨¡å‹å¦‚ o3 åœ¨æ²¡æœ‰æ‰‹å·¥åˆ¶å®šçš„ç‰¹å®šé¢†åŸŸç­–ç•¥æˆ–æ”¾å®½çº¦æŸçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è·å¾—é‡‘ç‰Œã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œè™½ç„¶ä¸“é—¨çš„æµæ°´çº¿å¦‚ o1-ioi å¸¦æ¥äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä½†æ‰©å±•çš„é€šç”¨ o3 æ¨¡å‹åœ¨ä¸ä¾èµ–æ‰‹å·¥åˆ¶å®šçš„æ¨ç†å¯å‘å¼çš„æƒ…å†µä¸‹è¶…è¶Šäº†è¿™äº›ç»“æœã€‚",
        "title": "Competitive Programming with Large Reasoning Models",
        "pinyin": "WÇ’men zhÇnshÃ¬le qiÃ¡ng huÃ  xuÃ©xÃ­ yÃ¬ngyÃ²ng yÃº dÃ xÃ­ng yÇ”yÃ¡n mÃ³xÃ­ng (LLMs) kÄ›yÇ xiÇnzhÃ¹ tÃ­shÄ“ng fÃ¹zÃ¡ bÇan chÃ©ng yÇ” tuÄ«lÇ rÃ¨nwÃ¹ de biÇoxiÃ n. WÇ’men bÇjiÃ ole liÇng gÃ¨ tÅngyÃ²ng tuÄ«lÇ mÃ³xÃ­ng - OpenAI o1 hÃ© o3 de zÇoqÄ« bÇnbÄ›n, yÇjiÃ  yÄ«gÃ¨ zhÇduÃ¬ 2024 niÃ¡n guÃ³jÃ¬ xÃ¬nxÄ« xuÃ© Ã olÃ­npÇkÃ¨ (IOI) shÃ¨jÃ¬ de tÃ¨dÃ¬ng yÃ¹yÃ­ xÃ¬tÇ’ng o1-ioi. WÇ’men zÃ i IOI 2024 shÃ ng shÇyÃ²ng o1-ioi cÄnsÃ i, bÃ¬ng zÃ i fÃ ngkuÄn de bÇsÃ i yuÄ“shÃ¹ xiÃ  huÃ²dÃ©le jÄ«npÃ¡i. RÃ¡n'Ã©r, hÃ²uxÃ¹ mÃ³xÃ­ng rÃº o3 zÃ i mÃ©iyÇ’u shÇ’ugÅng zhÃ¬dÃ¬ng de tÃ¨dÃ¬ng yÃ¹yÃ­ cÃ¨lÃ¼Ã¨ huÃ² fÃ ngkuÄn yuÄ“shÃ¹ de qÃ­ngkuÃ ng xiÃ  yÄ› nÃ©ng huÃ²dÃ© jÄ«npÃ¡i. WÇ’men de fÄxiÃ n biÇomÃ­ng, suÄ«rÃ¡n zhuÄnmÃ©n de liÃºshuÇxiÃ n rÃº o1-ioi dÃ ilÃ¡i le xiÇnzhÃ¹ de gÇijÃ¬n, dÃ n kuÃ²zhÇn de tÅngyÃ²ng o3 mÃ³xÃ­ng zÃ i bÃ¹ yÄ«lÃ i shÇ’ugÅng zhÃ¬dÃ¬ng de tuÄ«lÇ qÇfÇshÃ¬ de qÃ­ngkuÃ ng xiÃ  chÄoyuÃ¨le zhÃ¨xiÄ“ jiÃ©guÇ’.",
        "vocab": "[{'word': 'å±•ç¤º', 'pinyin': 'zhÇnshÃ¬', 'trans': 'display'}, {'word': 'å¼ºåŒ–å­¦ä¹ ', 'pinyin': 'qiÃ¡ngâ€‹huÃ â€‹xuÃ©â€‹xÃ­', 'trans': 'reinforcement learning'}, {'word': 'åº”ç”¨äº', 'pinyin': 'yÃ¬ngâ€‹yÃ²ngâ€‹yÃº', 'trans': 'apply to'}, {'word': 'å¤§å‹è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ â€‹xÃ­ngâ€‹yÇ”â€‹yÃ¡nâ€‹mÃ³â€‹xÃ­ng', 'trans': 'large language model'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnâ€‹zhÃ¹', 'trans': 'significant'}, {'word': 'æå‡', 'pinyin': 'tÃ­â€‹shÄ“ng', 'trans': 'improve'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹â€‹zÃ¡', 'trans': 'complex'}, {'word': 'ç¼–ç¨‹', 'pinyin': 'biÄnâ€‹chÃ©ng', 'trans': 'programming'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ«â€‹lÇ', 'trans': 'reasoning'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇoâ€‹xiÃ n', 'trans': 'performance'}, {'word': 'æ¯”è¾ƒ', 'pinyin': 'bÇâ€‹jiÃ o', 'trans': 'compare'}, {'word': 'é€šç”¨', 'pinyin': 'tÅngâ€‹yÃ²ng', 'trans': 'general-purpose'}, {'word': 'é’ˆå¯¹', 'pinyin': 'zhÄ“nâ€‹duÃ¬', 'trans': 'target'}, {'word': 'ç‰¹å®šé¢†åŸŸ', 'pinyin': 'tÃ¨â€‹dÃ¬ngâ€‹lÇngâ€‹yÃ¹', 'trans': 'specific domain'}, {'word': 'ç³»ç»Ÿ', 'pinyin': 'xÃ¬â€‹tÇ’ng', 'trans': 'system'}, {'word': 'å›½é™…ä¿¡æ¯å­¦å¥¥æ—åŒ¹å…‹', 'pinyin': 'guÃ³â€‹jÃ¬â€‹xÃ¬nâ€‹xÄ«â€‹xuÃ©â€‹Ã oâ€‹lÃ­nâ€‹pÇâ€‹kÃ¨', 'trans': 'International Olympiad in Informatics'}, {'word': 'è®¾è®¡', 'pinyin': 'shÃ¨â€‹jÃ¬', 'trans': 'design'}, {'word': 'å‚èµ›', 'pinyin': 'cÄnâ€‹sÃ i', 'trans': 'compete'}, {'word': 'æ”¾å®½', 'pinyin': 'fÃ ngâ€‹kuÄn', 'trans': 'relax'}, {'word': 'çº¦æŸ', 'pinyin': 'yuÄ“â€‹shÃ¹', 'trans': 'constraint'}, {'word': 'è·å¾—', 'pinyin': 'huÃ²â€‹dÃ©', 'trans': 'obtain'}, {'word': 'é‡‘ç‰Œ', 'pinyin': 'jÄ«nâ€‹pÃ¡i', 'trans': 'gold medal'}, {'word': 'åç»­', 'pinyin': 'hÃ²uâ€‹xÃ¹', 'trans': 'subsequent'}, {'word': 'æ‰‹å·¥åˆ¶å®š', 'pinyin': 'shÇ’uâ€‹gÅngâ€‹zhÃ¬â€‹dÃ¬ng', 'trans': 'manually specified'}, {'word': 'ç­–ç•¥', 'pinyin': 'cÃ¨â€‹lÃ¼Ã¨', 'trans': 'strategy'}, {'word': 'å¯å‘å¼', 'pinyin': 'qÇâ€‹fÄâ€‹shÃ¬', 'trans': 'heuristic'}, {'word': 'ä¾èµ–', 'pinyin': 'yÄ«â€‹lÃ i', 'trans': 'rely on'}, {'word': 'æ‰©å±•', 'pinyin': 'kuÃ²â€‹zhÇn', 'trans': 'extend'}, {'word': 'è¶…è¶Š', 'pinyin': 'chÄoâ€‹yuÃ¨', 'trans': 'surpass'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ©â€‹guÇ’', 'trans': 'result'}]",
        "trans": "We demonstrated that applying reinforcement learning to large language models (LLMs) can significantly enhance performance in complex programming and reasoning tasks. We compared two general reasoning modelsâ€”early versions of OpenAI o1 and o3â€”and a domain-specific system, o1-ioi, designed for the 2024 International Olympiad in Informatics (IOI). We competed in IOI 2024 using o1-ioi and won a gold medal under relaxed competition constraints. However, subsequent models like o3 were able to achieve gold medals without handcrafted domain-specific strategies or relaxed constraints. Our findings indicate that while specialized pipelines like o1-ioi brought significant improvements, the expanded general o3 model surpassed these results without relying on handcrafted reasoning heuristics.",
        "update_ts": "2025-02-12 09:11"
    }
}