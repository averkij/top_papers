{
    "date": {
        "ru": "13 февраля",
        "en": "February 13",
        "zh": "2月13日"
    },
    "time_utc": "2025-02-13 07:10",
    "weekday": 3,
    "issue_id": 2190,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.08590",
            "title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
            "url": "https://huggingface.co/papers/2502.08590",
            "abstract": "Recent advancements in image relighting models, driven by large-scale datasets and pre-trained diffusion models, have enabled the imposition of consistent lighting. However, video relighting still lags, primarily due to the excessive training costs and the scarcity of diverse, high-quality video relighting datasets. A simple application of image relighting models on a frame-by-frame basis leads to several issues: lighting source inconsistency and relighted appearance inconsistency, resulting in flickers in the generated videos. In this work, we propose Light-A-Video, a training-free approach to achieve temporally smooth video relighting. Adapted from image relighting models, Light-A-Video introduces two key techniques to enhance lighting consistency. First, we design a Consistent Light Attention (CLA) module, which enhances cross-frame interactions within the self-attention layers to stabilize the generation of the background lighting source. Second, leveraging the physical principle of light transport independence, we apply linear blending between the source video's appearance and the relighted appearance, using a Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions in illumination. Experiments show that Light-A-Video improves the temporal consistency of relighted video while maintaining the image quality, ensuring coherent lighting transitions across frames. Project page: https://bujiazi.github.io/light-a-video.github.io/.",
            "score": 22,
            "issue_id": 2188,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "dcc03282320e88b1",
            "authors": [
                "Yujie Zhou",
                "Jiazi Bu",
                "Pengyang Ling",
                "Pan Zhang",
                "Tong Wu",
                "Qidong Huang",
                "Jinsong Li",
                "Xiaoyi Dong",
                "Yuhang Zang",
                "Yuhang Cao",
                "Anyi Rao",
                "Jiaqi Wang",
                "Li Niu"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08590.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "Плавное переосвещение видео без обучения",
                    "desc": "Эта статья представляет Light-A-Video - подход к переосвещению видео без дополнительного обучения. Авторы предлагают два ключевых метода для улучшения согласованности освещения: модуль Consistent Light Attention (CLA) для стабилизации генерации фонового источника света и стратегию Progressive Light Fusion (PLF) для плавных переходов освещения между кадрами. Light-A-Video адаптирует модели переосвещения изображений для видео, решая проблемы несогласованности источника света и мерцания. Эксперименты показывают, что метод улучшает временную согласованность переосвещенного видео, сохраняя качество изображения."
                },
                "en": {
                    "title": "Achieving Smooth Video Relighting with Light-A-Video",
                    "desc": "This paper presents Light-A-Video, a novel approach for video relighting that addresses the challenges of lighting consistency and flickering in generated videos. Unlike traditional methods that apply image relighting on a frame-by-frame basis, Light-A-Video utilizes a training-free strategy to enhance temporal smoothness. It introduces a Consistent Light Attention (CLA) module to improve interactions between frames and stabilize background lighting. Additionally, the Progressive Light Fusion (PLF) technique is employed to blend the original and relighted appearances, ensuring coherent lighting transitions across video frames."
                },
                "zh": {
                    "title": "实现视频重光的一致性与平滑性",
                    "desc": "最近，图像重光模型的进展得益于大规模数据集和预训练的扩散模型，使得一致的光照效果得以实现。然而，视频重光仍然滞后，主要是由于训练成本高和缺乏多样化的高质量视频重光数据集。简单地将图像重光模型逐帧应用会导致光源不一致和重光外观不一致，从而在生成的视频中产生闪烁现象。我们提出了Light-A-Video，这是一种无训练的方法，旨在实现时间上平滑的视频重光。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07870",
            "title": "TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation",
            "url": "https://huggingface.co/papers/2502.07870",
            "abstract": "Text-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text and visuals is essential for conveying complex information. However, despite these advances, the generation of images containing long-form text remains a persistent challenge, largely due to the limitations of existing datasets, which often focus on shorter and simpler text. To address this gap, we introduce TextAtlas5M, a novel dataset specifically designed to evaluate long-text rendering in text-conditioned image generation. Our dataset consists of 5 million long-text generated and collected images across diverse data types, enabling comprehensive evaluation of large-scale generative models on long-text image generation. We further curate 3000 human-improved test set TextAtlasEval across 3 data domains, establishing one of the most extensive benchmarks for text-conditioned generation. Evaluations suggest that the TextAtlasEval benchmarks present significant challenges even for the most advanced proprietary models (e.g. GPT4o with DallE-3), while their open-source counterparts show an even larger performance gap. These evidences position TextAtlas5M as a valuable dataset for training and evaluating future-generation text-conditioned image generation models.",
            "score": 20,
            "issue_id": 2188,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "5b552b320e2e69f0",
            "authors": [
                "Alex Jinpeng Wang",
                "Dongxing Mao",
                "Jiawei Zhang",
                "Weiming Han",
                "Zhuobai Dong",
                "Linjie Li",
                "Yiqi Lin",
                "Zhengyuan Yang",
                "Libo Qin",
                "Fuwei Zhang",
                "Lijuan Wang",
                "Min Li"
            ],
            "affiliations": [
                "Central South University",
                "Microsoft",
                "National University of Singapore",
                "North University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07870.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#long_context",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "TextAtlas5M: Новый горизонт в генерации изображений с длинным текстом",
                    "desc": "Статья представляет новый датасет TextAtlas5M для оценки генерации изображений с длинным текстом. Датасет содержит 5 миллионов изображений с разнообразными типами данных, что позволяет всесторонне оценивать крупномасштабные генеративные модели. Авторы также создали тестовый набор TextAtlasEval из 3000 улучшенных человеком изображений. Оценка показала, что даже самые продвинутые проприетарные модели (например, GPT4o с DallE-3) сталкиваются со значительными трудностями при работе с этим набором данных."
                },
                "en": {
                    "title": "Empowering Long-Text Image Generation with TextAtlas5M",
                    "desc": "This paper introduces TextAtlas5M, a new dataset aimed at improving text-conditioned image generation, particularly for long-form text. Current models struggle with generating images that include dense and intricate text, as existing datasets primarily focus on shorter text. TextAtlas5M contains 5 million images with long text, allowing for better evaluation of generative models. The paper also presents TextAtlasEval, a curated test set that highlights the challenges faced by both proprietary and open-source models in this area."
                },
                "zh": {
                    "title": "长文本图像生成的新突破",
                    "desc": "本文介绍了一个新的数据集TextAtlas5M，旨在解决文本条件下图像生成中长文本渲染的挑战。现有的数据集通常只关注短文本，限制了生成模型的能力。TextAtlas5M包含500万张长文本生成的图像，覆盖多种数据类型，为大规模生成模型的评估提供了基础。通过建立3000个经过人工改进的测试集TextAtlasEval，本文为文本条件生成提供了一个广泛的基准，帮助未来的研究和模型训练。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08639",
            "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
            "url": "https://huggingface.co/papers/2502.08639",
            "abstract": "In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware conditional signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals--comprising rendered depth maps, camera trajectories and object class labels--serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation. Project page: https://cinemaster-dev.github.io/.",
            "score": 19,
            "issue_id": 2186,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "3d3890b6b6bf7904",
            "authors": [
                "Qinghe Wang",
                "Yawen Luo",
                "Xiaoyu Shi",
                "Xu Jia",
                "Huchuan Lu",
                "Tianfan Xue",
                "Xintao Wang",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Kuaishou Technology",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08639.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#video",
                    "#diffusion",
                    "#3d",
                    "#games"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "CineMaster: Режиссируйте свое видео в 3D",
                    "desc": "CineMaster - это новая система для создания 3D-ориентированного и контролируемого видео на основе текста. Она позволяет пользователям точно размещать объекты в сцене, гибко манипулировать объектами и камерой в 3D-пространстве. Система работает в два этапа: сначала пользователь интерактивно создает 3D-сигналы управления, затем эти сигналы используются для управления диффузионной моделью генерации видео. Для обучения была разработана автоматизированная система аннотации видеоданных с извлечением 3D-ограничивающих рамок и траекторий камеры."
                },
                "en": {
                    "title": "Empowering Video Creation with 3D Control",
                    "desc": "CineMaster is a new framework designed for generating videos from text while allowing users to control 3D elements like a film director. It operates in two stages: first, users create 3D-aware signals by placing objects and defining camera movements, and second, these signals guide a text-to-video diffusion model to produce the desired video. The framework also includes an automated data annotation system to gather necessary 3D motion and camera data from existing videos. Experiments show that CineMaster outperforms current methods in generating 3D-aware videos from text descriptions."
                },
                "zh": {
                    "title": "CineMaster：让视频生成如导演般可控",
                    "desc": "CineMaster是一个新颖的框架，用于生成具有3D感知和可控性的文本到视频。它使用户能够像专业电影导演一样精确控制场景中的物体位置、灵活操作3D空间中的物体和相机，并直观地布局渲染帧。该框架分为两个阶段：第一阶段通过交互式工作流程构建3D感知的条件信号，第二阶段利用这些信号指导文本到视频的扩散模型生成用户所需的视频内容。此外，CineMaster还建立了一个自动化数据注释管道，以解决缺乏3D物体运动和相机姿态标注的数据集问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07563",
            "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
            "url": "https://huggingface.co/papers/2502.07563",
            "abstract": "Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.",
            "score": 15,
            "issue_id": 2188,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "f5a4cfd0a0d018ae",
            "authors": [
                "Weigao Sun",
                "Disen Lan",
                "Yiran Zhong",
                "Xiaoye Qu",
                "Yu Cheng"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "South China University of Technology",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07563.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение обучения трансформеров с линейным вниманием на сверхдлинных последовательностях",
                    "desc": "Статья представляет новый метод параллелизма последовательностей LASP-2 для обучения моделей трансформеров с линейным вниманием на очень длинных входных последовательностях. LASP-2 оптимизирует коммуникацию и вычислительный параллелизм, требуя только одну операцию AllGather для промежуточных состояний памяти, размер которых не зависит от длины последовательности. Авторы также предлагают расширение LASP-2H для гибридных моделей, сочетающих линейное и стандартное внимание. Эксперименты на модели Linear-Llama3 показывают, что LASP-2 превосходит предыдущие методы по скорости обучения на длинных последовательностях при распределенных вычислениях."
                },
                "en": {
                    "title": "Boosting Efficiency in Linear Attention with LASP-2",
                    "desc": "This paper presents LASP-2, a new sequence parallelism (SP) method designed to improve the efficiency of training linear attention transformer models with very long input sequences. LASP-2 optimizes communication and computation parallelism by minimizing the communication requirements for linear attention layers, allowing for a single AllGather operation on intermediate memory states. This approach leads to significant enhancements in both communication and computation parallelism, enabling better scalability in distributed systems. Additionally, LASP-2 is extended to LASP-2H, which applies similar optimizations to standard attention modules, providing an efficient solution for hybrid models that utilize both linear and standard attention."
                },
                "zh": {
                    "title": "LASP-2：提升线性注意力模型的并行性",
                    "desc": "本文介绍了一种新的序列并行方法LASP-2，旨在提高线性注意力变换器模型在处理非常长输入序列时的通信和计算并行性。与之前的LASP方法相比，LASP-2重新思考了线性注意力层的最小通信需求，并重新组织了通信和计算的工作流程。通过这种方式，LASP-2只需在中间内存状态上进行一次AllGather集体通信，显著提高了通信和计算的并行性及其重叠。我们的评估表明，LASP-2在训练速度上比LASP提高了15.2%，比环形注意力提高了36.6%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08047",
            "title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation",
            "url": "https://huggingface.co/papers/2502.08047",
            "abstract": "Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or the interface not being in its default state-often lead to planning errors. This issue is widespread in real user scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation.",
            "score": 12,
            "issue_id": 2190,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "0f83dccb05181f21",
            "authors": [
                "Henry Hengyuan Zhao",
                "Difei Gao",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08047.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Критическое мышление для улучшения автоматизации GUI",
                    "desc": "Статья представляет WorldGUI - новый бенчмарк для оценки графических интерфейсов, который симулирует реальные взаимодействия пользователей с компьютером в различных начальных состояниях. Авторы также предлагают фреймворк GUI-Thinker, использующий механизм критического мышления для эффективного управления непредсказуемостью и сложностью взаимодействий с GUI. Эксперименты показывают, что GUI-Thinker значительно превосходит Claude-3.5 (Computer Use) по показателю успешности выполнения задач в WorldGUI. Это исследование подчеркивает эффективность подхода, основанного на критическом мышлении, для улучшения автоматизации GUI."
                },
                "en": {
                    "title": "Enhancing GUI Automation with WorldGUI and GUI-Thinker",
                    "desc": "This paper addresses the challenges faced by current GUI agents in planning tasks due to their sensitivity to the initial state of the environment. It introduces WorldGUI, a new benchmark that simulates real user interactions by incorporating various initial states across multiple software applications. To tackle the complexities of dynamic GUI automation, the authors propose GUI-Thinker, a framework that utilizes a critique mechanism to improve decision-making in unpredictable scenarios. Experimental results show that GUI-Thinker outperforms existing models, highlighting its effectiveness in enhancing the success rate of GUI automation tasks."
                },
                "zh": {
                    "title": "提升GUI自动化的关键思维框架",
                    "desc": "当前的图形用户界面（GUI）代理在元素定位方面表现出色，但在规划方面仍然面临挑战，尤其是对环境初始状态的敏感性。初始状态的微小差异，例如目标软件未打开或界面不在默认状态，常常导致规划错误。为了解决这个问题，本文提出了WorldGUI，一个新颖的GUI基准，设计了具有多种初始状态的GUI任务，以模拟真实的计算机用户交互。我们还提出了GUI-Thinker，一个全面的框架，通过批判机制有效管理GUI交互的不可预测性和复杂性，实验结果显示其在WorldGUI任务上的成功率比Claude-3.5高出14.9%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08127",
            "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
            "url": "https://huggingface.co/papers/2502.08127",
            "abstract": "Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving. Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains. Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks. To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension. All our datasets, models, and codes are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models.",
            "score": 12,
            "issue_id": 2186,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "fa3f08993ba529cd",
            "authors": [
                "Lingfei Qian",
                "Weipeng Zhou",
                "Yan Wang",
                "Xueqing Peng",
                "Jimin Huang",
                "Qianqian Xie"
            ],
            "affiliations": [
                "TheFinAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08127.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#rl",
                    "#open_source",
                    "#training",
                    "#long_context"
                ],
                "emoji": "💹",
                "ru": {
                    "title": "Специализация языковых моделей - ключ к успеху в финансовом анализе",
                    "desc": "В этом исследовании оценивается эффективность 16 мощных языковых моделей в решении сложных финансовых задач. Авторы обнаружили, что улучшение наборов данных и предварительное обучение повышают способности моделей к финансовым рассуждениям. Они разработали специализированную модель на основе Llama-3.1-8B-Instruct, которая превзошла даже более крупные модели в финансовых задачах. Исследование подчеркивает необходимость адаптации моделей к специфике финансовой области."
                },
                "en": {
                    "title": "Enhancing Financial Reasoning in LLMs with Domain-Specific Adaptations",
                    "desc": "This paper investigates the performance of large language models (LLMs) in financial reasoning tasks, which include interpreting financial text, analyzing tabular data, and solving equations. The authors evaluate 16 advanced LLMs and find that while improved datasets and pretraining enhance financial reasoning, general techniques like Chain-of-Thought (CoT) fine-tuning do not consistently improve results. They propose a new model, enhanced with CoT fine-tuning and reinforcement learning, which shows a significant performance boost of 10% across various financial tasks. The study emphasizes the importance of domain-specific adaptations for better performance in financial reasoning and introduces a leaderboard for future benchmarking."
                },
                "zh": {
                    "title": "金融推理模型的创新与提升",
                    "desc": "本研究评估了16种强大的语言模型在金融推理任务中的表现。这些任务包括金融文本、表格数据和方程式，涉及数值推理、表格解读和金融术语理解等方面。研究结果表明，尽管更好的数据集和预训练可以提升金融推理能力，但通用的增强方法如链式推理微调并不总是有效。为了解决这些问题，我们开发了一种基于Llama-3.1-8B-Instruct的金融推理增强模型，经过微调和强化学习后，在多个任务上实现了10%的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07864",
            "title": "TransMLA: Multi-head Latent Attention Is All You Need",
            "url": "https://huggingface.co/papers/2502.07864",
            "abstract": "Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1.",
            "score": 10,
            "issue_id": 2186,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "dbff84dafe8c2312",
            "authors": [
                "Fanxu Meng",
                "Zengwei Yao",
                "Muhan Zhang"
            ],
            "affiliations": [
                "Institute for Artificial Intelligence, Peking University",
                "State Key Laboratory of General Artificial Intelligence, Peking University",
                "Xiaomi Corp., Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07864.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#inference",
                    "#training",
                    "#long_context"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Революция в архитектуре внимания: MLA для эффективных языковых моделей",
                    "desc": "Статья представляет новый метод под названием Multi-head Latent Attention (MLA), который решает проблему коммуникационных узких мест в больших языковых моделях. MLA использует матрицы низкого ранга в слоях ключ-значение, что позволяет сжимать и кэшировать латентные состояния KV. Авторы также предлагают метод TransMLA для преобразования предобученных моделей на основе Group Query Attention (GQA) в модели на основе MLA. Этот подход позволяет значительно уменьшить размер KV-кэша и ускорить вывод, сохраняя при этом выразительность модели."
                },
                "en": {
                    "title": "Transforming Attention: From GQA to Efficient MLA",
                    "desc": "This paper introduces Multi-head Latent Attention (MLA) as a solution to communication bottlenecks in large language models (LLMs) caused by hardware limitations. MLA utilizes low-rank matrices in the key-value (KV) layers to compress KV states, significantly reducing cache size and improving inference speed. The authors demonstrate that Group Query Attention (GQA) can be represented by MLA without increasing KV cache overhead, while MLA offers greater flexibility. To facilitate the adoption of MLA, they propose TransMLA, a method for converting GQA-based models into MLA-based ones, allowing for enhanced expressiveness without additional cache size."
                },
                "zh": {
                    "title": "提升语言模型效率的关键：多头潜在注意力",
                    "desc": "现代大型语言模型（LLMs）在当前硬件上常常面临通信瓶颈，而不仅仅是计算限制。多头潜在注意力（MLA）通过在键值（KV）层中使用低秩矩阵来解决这个问题，从而允许压缩的潜在KV状态被缓存。这种方法显著减少了KV缓存的大小，相比传统的多头注意力，推理速度更快。此外，MLA使用上投影矩阵来增加表达能力，以额外的计算换取减少的通信开销。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08168",
            "title": "SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation",
            "url": "https://huggingface.co/papers/2502.08168",
            "abstract": "In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.",
            "score": 7,
            "issue_id": 2186,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "1ca2b8d38e35b203",
            "authors": [
                "Zhiming Ma",
                "Xiayang Xiao",
                "Sihao Dong",
                "Peidong Wang",
                "HaiPeng Wang",
                "Qingyun Pan"
            ],
            "affiliations": [
                "China Mobile Group Guangdong Co., Ltd. Guangzhou Branch, Guangzhou, China",
                "China Mobile Internet Company Ltd., Guangzhou, China",
                "School of Computer Science and Engineering, Northeastern University, Shenyang, China",
                "The Key Laboratory for Information Science of Electromagnetic Waves (Ministry of Education), School of Information Science and Technology, Fudan University, Shanghai, China",
                "The School of Automation and Electrical Engineering, Inner Mongolia University of Science and Technology, Baotou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08168.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "🛰️",
                "ru": {
                    "title": "SARChat-2M: Революция в интерпретации изображений SAR с помощью мультимодальных диалоговых моделей",
                    "desc": "Статья представляет первый крупномасштабный мультимодальный диалоговый датасет для изображений SAR, названный SARChat-2M. Он содержит около 2 миллионов пар изображение-текст высокого качества и охватывает различные сценарии с детальными аннотациями целей. Датасет поддерживает ключевые задачи, такие как визуальное понимание и обнаружение объектов, а также предоставляет основу для создания мультимодальных датасетов в различных областях дистанционного зондирования. Эффективность датасета была подтверждена экспериментами на 16 основных VLM, что позволило создать первый многозадачный диалоговый бенчмарк в области SAR."
                },
                "en": {
                    "title": "Empowering SAR Image Interpretation with SARChat-2M!",
                    "desc": "This paper introduces SARChat-2M, a large-scale multimodal dialogue dataset specifically designed for synthetic aperture radar (SAR) images. It contains around 2 million image-text pairs that cover various scenarios and include detailed annotations for effective visual understanding and object detection. The dataset serves as a benchmark for evaluating vision language models (VLMs) in the SAR domain, demonstrating their capabilities in interpreting SAR images. By conducting experiments on 16 popular VLMs, the study establishes a new multi-task dialogue benchmark, paving the way for advancements in remote sensing applications."
                },
                "zh": {
                    "title": "推动SAR图像解读的多模态对话数据集",
                    "desc": "在合成孔径雷达（SAR）遥感图像解读领域，尽管视觉语言模型（VLMs）在自然语言处理和图像理解方面取得了显著进展，但由于缺乏专业领域的知识，其应用仍然有限。本文创新性地提出了第一个大规模的SAR图像多模态对话数据集SARChat-2M，包含约200万对高质量的图像-文本配对，涵盖了多种场景和详细的目标注释。该数据集不仅支持视觉理解和目标检测等关键任务，还开发了SAR领域的视觉语言数据集和基准，评估VLMs在SAR图像解读中的能力。通过对16个主流VLM的实验验证，该数据集的有效性得到了充分证明，并成功建立了SAR领域的第一个多任务对话基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08606",
            "title": "Distillation Scaling Laws",
            "url": "https://huggingface.co/papers/2502.08606",
            "abstract": "We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design.",
            "score": 5,
            "issue_id": 2188,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "774eeded4c92c597",
            "authors": [
                "Dan Busbridge",
                "Amitis Shidani",
                "Floris Weers",
                "Jason Ramapuram",
                "Etai Littwin",
                "Russ Webb"
            ],
            "affiliations": [
                "Apple",
                "University of Oxford, UK"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08606.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Масштабируемая дистилляция: оптимизация обучения моделей",
                    "desc": "Эта статья представляет закон масштабирования дистилляции, который оценивает производительность дистиллированной модели на основе вычислительного бюджета и его распределения между студентом и учителем. Авторы предоставляют оптимальные рецепты дистилляции для различных сценариев, включая случаи с существующим учителем или необходимостью его обучения. Исследование показывает, что дистилляция превосходит обычное предобучение с учителем до определенного уровня вычислений, который предсказуемо растет с размером модели-студента. Работа также предоставляет ценные insights о процессе дистилляции, улучшающие понимание этого метода и информирующие дизайн экспериментов."
                },
                "en": {
                    "title": "Maximizing Student Performance through Optimal Distillation Strategies",
                    "desc": "This paper introduces a distillation scaling law that helps predict how well a distilled model will perform based on the available computing resources and how those resources are divided between the teacher and student models. The authors demonstrate that by optimizing the compute allocation, the performance of the student model can be significantly improved, especially when multiple students are distilled from a pre-trained teacher. They also provide guidelines for when to use distillation versus supervised learning, depending on whether a teacher model is available and whether it requires training. Overall, the study enhances our understanding of model distillation and offers practical strategies for effective implementation in machine learning tasks."
                },
                "zh": {
                    "title": "优化蒸馏模型性能的计算法则",
                    "desc": "本文提出了一种蒸馏缩放法则，用于根据计算预算和在学生与教师之间的分配来估计蒸馏模型的性能。研究结果降低了大规模使用蒸馏的风险，能够优化教师和学生模型的计算分配，以最大化学生的表现。我们提供了计算最优的蒸馏方案，适用于已有教师或需要训练教师的情况。通过大规模研究，我们增加了对蒸馏的理解，并为实验设计提供了指导。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06872",
            "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey",
            "url": "https://huggingface.co/papers/2502.06872",
            "abstract": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact.",
            "score": 2,
            "issue_id": 2188,
            "pub_date": "2025-02-08",
            "pub_date_card": {
                "ru": "8 февраля",
                "en": "February 8",
                "zh": "2月8日"
            },
            "hash": "f454782ce3101c66",
            "authors": [
                "Bo Ni",
                "Zheyuan Liu",
                "Leyao Wang",
                "Yongjia Lei",
                "Yuying Zhao",
                "Xueqi Cheng",
                "Qingkai Zeng",
                "Luna Dong",
                "Yinglong Xia",
                "Krishnaram Kenthapadi",
                "Ryan Rossi",
                "Franck Dernoncourt",
                "Md Mehrab Tanjim",
                "Nesreen Ahmed",
                "Xiaorui Liu",
                "Wenqi Fan",
                "Erik Blasch",
                "Yu Wang",
                "Meng Jiang",
                "Tyler Derr"
            ],
            "affiliations": [
                "Adobe Research",
                "Air Force Research Lab",
                "Cisco AI Research",
                "Meta",
                "North Carolina State University",
                "Oracle Health AI",
                "The Hong Kong Polytechnic University",
                "University of Notre Dame",
                "University of Oregon",
                "Vanderbilt University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06872.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#interpretability",
                    "#security",
                    "#survey",
                    "#rag",
                    "#ethics"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Путь к надёжному ИИ: Преодоление рисков в генерации с дополнением извлечённой информацией",
                    "desc": "Статья посвящена методу генерации с дополнением извлечённой информацией (RAG), который улучшает генерацию контента искусственным интеллектом. Авторы рассматривают риски, связанные с RAG, включая проблемы надёжности, конфиденциальности и безопасности. Предлагается комплексная дорожная карта для разработки надёжных RAG-систем с акцентом на пять ключевых аспектов. Статья также освещает применение надёжных RAG-систем в различных прикладных областях."
                },
                "en": {
                    "title": "Building Trust in Retrieval-Augmented Generation Systems",
                    "desc": "This paper discusses Retrieval-Augmented Generation (RAG), a technique that enhances AI-generated content by incorporating external knowledge for better accuracy and relevance. While RAG improves content generation, it also brings new challenges such as robustness, privacy, and accountability issues that need to be addressed. The authors propose a comprehensive framework that focuses on five key areas: reliability, privacy, safety, fairness, and explainability, to guide future research and development of trustworthy RAG systems. By providing a structured approach, the paper aims to foster innovation and highlight the importance of trust in RAG applications."
                },
                "zh": {
                    "title": "构建可信赖的检索增强生成系统",
                    "desc": "检索增强生成（RAG）是一种先进的技术，旨在解决人工智能生成内容（AIGC）面临的挑战。通过将上下文检索与内容生成相结合，RAG 提供可靠且最新的外部知识，减少幻觉现象，并确保在各种任务中保持相关上下文。然而，尽管 RAG 取得了成功，但最近的研究表明，该范式也引入了新的风险，包括鲁棒性问题、隐私问题、对抗性攻击和问责问题。本文旨在提供一个全面的路线图，以开发可信赖的 RAG 系统，围绕可靠性、隐私、安全性、公平性、可解释性和问责性等五个关键视角进行讨论。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05167",
            "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
            "url": "https://huggingface.co/papers/2502.05167",
            "abstract": "Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\" (relevant information) from a \"haystack\" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information.",
            "score": 2,
            "issue_id": 2188,
            "pub_date": "2025-02-07",
            "pub_date_card": {
                "ru": "7 февраля",
                "en": "February 7",
                "zh": "2月7日"
            },
            "hash": "4ff0f34526efea9f",
            "authors": [
                "Ali Modarressi",
                "Hanieh Deilamsalehy",
                "Franck Dernoncourt",
                "Trung Bui",
                "Ryan A. Rossi",
                "Seunghyun Yoon",
                "Hinrich Schütze"
            ],
            "affiliations": [
                "Adobe Research",
                "Center for Information and Language Processing"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05167.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "NoLiMa: Новый вызов для больших языковых моделей в работе с длинным контекстом",
                    "desc": "Статья представляет новый бенчмарк NoLiMa для оценки способности больших языковых моделей (LLM) работать с длинным контекстом. В отличие от существующих тестов, NoLiMa требует от моделей выявления скрытых ассоциаций между вопросом и релевантной информацией в тексте. Исследование показало, что производительность 12 популярных LLM значительно снижается при увеличении длины контекста. Результаты указывают на трудности механизма внимания в обработке длинных последовательностей при отсутствии прямых лексических совпадений."
                },
                "en": {
                    "title": "NoLiMa: Challenging LLMs Beyond Literal Matches",
                    "desc": "This paper introduces NoLiMa, a new benchmark designed to evaluate the capabilities of large language models (LLMs) in retrieving relevant information from extensive contexts. Unlike traditional methods that allow models to rely on literal matches, NoLiMa requires models to infer deeper associations between questions and answers with minimal lexical overlap. The study evaluates 12 popular LLMs, revealing that while they perform well in shorter contexts, their performance significantly declines as context length increases, particularly beyond 1K tokens. The findings indicate that the attention mechanism struggles to effectively retrieve relevant information in longer contexts when direct matches are not available."
                },
                "zh": {
                    "title": "长上下文中的信息检索挑战",
                    "desc": "最近的大型语言模型（LLMs）支持长达128K到1M的上下文。本文提出了一种新的基准测试NoLiMa，旨在评估模型在长上下文中检索相关信息的能力。与传统的针在干草堆（NIAH）测试不同，NoLiMa设计了最小词汇重叠的针集，要求模型推断潜在关联以找到针。我们的评估显示，尽管这些模型在短上下文中表现良好，但在长上下文中性能显著下降，尤其是在缺乏字面匹配的情况下。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.08524",
            "title": "LLM Pretraining with Continuous Concepts",
            "url": "https://huggingface.co/papers/2502.08524",
            "abstract": "Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the model's internal reasoning process.",
            "score": 2,
            "issue_id": 2188,
            "pub_date": "2025-02-12",
            "pub_date_card": {
                "ru": "12 февраля",
                "en": "February 12",
                "zh": "2月12日"
            },
            "hash": "99ad370e7cd11e3c",
            "authors": [
                "Jihoon Tack",
                "Jack Lanchantin",
                "Jane Yu",
                "Andrew Cohen",
                "Ilia Kulikov",
                "Janice Lan",
                "Shibo Hao",
                "Yuandong Tian",
                "Jason Weston",
                "Xian Li"
            ],
            "affiliations": [
                "FAIR at Meta",
                "KAIST",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.08524.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#interpretability",
                    "#training",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "CoCoMix: смешивание концепций для улучшения языковых моделей",
                    "desc": "В этой статье представлен новый метод предобучения языковых моделей под названием Continuous Concept Mixing (CoCoMix). В отличие от стандартного подхода предсказания следующего токена, CoCoMix сочетает дискретное предсказание токенов с непрерывными концепциями, полученными из предобученного разреженного автоэнкодера. Эксперименты показывают, что CoCoMix превосходит стандартные методы по эффективности обучения и качеству на различных задачах. Кроме того, CoCoMix улучшает интерпретируемость и управляемость модели, позволяя напрямую анализировать и модифицировать предсказанные концепции."
                },
                "en": {
                    "title": "Revolutionizing Language Models with Continuous Concept Mixing",
                    "desc": "This paper introduces Continuous Concept Mixing (CoCoMix), a new framework for pretraining large language models. Unlike traditional methods that focus solely on predicting the next token, CoCoMix integrates continuous concepts derived from a sparse autoencoder into the model's hidden states. This approach improves sample efficiency and enhances performance on various tasks, including language modeling and reasoning. Additionally, CoCoMix allows for better interpretability and control over the model's reasoning by enabling direct manipulation of the predicted concepts."
                },
                "zh": {
                    "title": "连续概念混合：提升语言模型的效率与可解释性",
                    "desc": "本文提出了一种新的预训练框架，称为连续概念混合（CoCoMix），它结合了离散的下一个标记预测和连续概念。CoCoMix通过将从预训练稀疏自编码器中学习的连续概念与标记的隐藏表示交错混合，来优化模型的隐藏状态。实验结果表明，CoCoMix在样本效率上表现更佳，并且在语言建模和推理任务上均优于传统的下一个标记预测和知识蒸馏方法。该方法还增强了模型的可解释性和可引导性，使得用户可以直接检查和修改预测的概念，从而透明地引导模型的内部推理过程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07737",
            "title": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling",
            "url": "https://huggingface.co/papers/2502.07737",
            "abstract": "Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed. In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. By uniformly decomposing video content into equal-sized blocks (e.g., rows or frames), we shift the generation unit from individual tokens to blocks, allowing each token in the current block to simultaneously predict the corresponding token in the next block. Unlike traditional AR modeling, our framework employs bidirectional attention within each block, enabling tokens to capture more robust spatial dependencies. By predicting multiple tokens in parallel, NBP models significantly reduce the number of generation steps, leading to faster and more efficient inference. Our model achieves FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4. Furthermore, thanks to the reduced number of inference steps, the NBP model generates 8.89 frames (128x128 resolution) per second, achieving an 11x speedup. We also explored model scales ranging from 700M to 3B parameters, observing significant improvements in generation quality, with FVD scores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600, demonstrating the scalability of our approach.",
            "score": 2,
            "issue_id": 2186,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "8038af0ecacc031f",
            "authors": [
                "Shuhuai Ren",
                "Shuming Ma",
                "Xu Sun",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07737.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#video",
                    "#inference",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "NBP: Быстрая и качественная генерация видео блоками",
                    "desc": "Статья представляет новый подход к генерации видео под названием Next-Block Prediction (NBP). В отличие от традиционного метода Next-Token Prediction, NBP использует полуавтореrрессивную модель, разбивая видео на блоки и предсказывая их параллельно. Это позволяет значительно ускорить процесс генерации и улучшить качество результатов. Модель NBP превзошла базовые методы по метрике FVD на датасетах UCF101 и K600, демонстрируя масштабируемость и эффективность подхода."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Next-Block Prediction",
                    "desc": "This paper introduces a new method for generating videos called Next-Block Prediction (NBP), which improves upon the traditional Next-Token Prediction (NTP) approach. NBP changes the way video content is generated by using blocks instead of individual tokens, allowing for simultaneous predictions within each block. This method utilizes bidirectional attention to enhance the understanding of spatial relationships in the video, leading to better quality outputs. As a result, NBP not only speeds up the generation process significantly but also achieves superior performance metrics compared to the standard NTP model."
                },
                "zh": {
                    "title": "视频生成的新突破：下一块预测",
                    "desc": "本文提出了一种新的半自回归框架，称为下一块预测（NBP），用于视频生成。与传统的自回归方法不同，NBP通过将视频内容均匀分解为相等大小的块，使得每个块内的标记可以同时预测下一个块的对应标记，从而捕捉更强的空间依赖性。该方法通过并行预测多个标记，显著减少了生成步骤，提高了推理速度，达到了每秒生成8.89帧的效果。实验结果表明，NBP在多个数据集上表现优于传统模型，展示了其在生成质量和速度上的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07599",
            "title": "DPO-Shift: Shifting the Distribution of Direct Preference Optimization",
            "url": "https://huggingface.co/papers/2502.07599",
            "abstract": "Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \\method to controllably shift the distribution of the chosen probability. Then, we show that \\method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \\method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.",
            "score": 2,
            "issue_id": 2186,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "85d178e03a57421f",
            "authors": [
                "Xiliang Yang",
                "Feng Jiang",
                "Qianen Zhang",
                "Lei Zhao",
                "Xiao Li"
            ],
            "affiliations": [
                "Institute of Translational Medicine and National Center for Translational Medicine, Shanghai Jiao Tong University",
                "School of Data Science, The Chinese University of Hong Kong, Shenzhen",
                "School of Mathematics, South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07599.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Контролируемое смещение вероятностей для улучшения обучения языковых моделей",
                    "desc": "Статья представляет новый метод под названием DPO-Shift для улучшения обучения языковых моделей с учетом человеческих предпочтений. Авторы решают проблему смещения вероятности выбранных ответов, которая возникает при использовании метода Direct Preference Optimization (DPO). DPO-Shift позволяет контролируемо смещать распределение вероятности выбранных ответов. Экспериментальные результаты показывают превосходство DPO-Shift над DPO на ряде задач, включая MT-Bench."
                },
                "en": {
                    "title": "Mitigating Likelihood Displacement in Language Model Training",
                    "desc": "This paper addresses the issue of likelihood displacement in Direct Preference Optimization (DPO) for aligning language models with human preferences. The authors introduce a new method, referred to as \textit{method}, which aims to control the distribution of chosen response probabilities during training. They highlight a trade-off between enhancing the chosen probability and the reward margin, supported by both theoretical insights and experimental results. The findings indicate that \textit{method} outperforms DPO in various downstream tasks, providing a promising solution to the challenges posed by likelihood displacement."
                },
                "zh": {
                    "title": "解决选择概率下降的有效方法",
                    "desc": "本文介绍了一种新的方法\textit{method}，旨在解决直接偏好优化（DPO）中出现的选择概率下降问题。研究表明，在训练过程中，模型对选择响应的概率往往会降低，这被称为似然位移。我们的方法可以控制选择概率的分布，从而改善模型的表现。通过理论分析和实验验证，我们证明了\textit{method}在下游任务中优于传统的DPO方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.07985",
            "title": "MetaSC: Test-Time Safety Specification Optimization for Language Models",
            "url": "https://huggingface.co/papers/2502.07985",
            "abstract": "We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code to be released at https://github.com/vicgalle/meta-self-critique.git .",
            "score": 1,
            "issue_id": 2190,
            "pub_date": "2025-02-11",
            "pub_date_card": {
                "ru": "11 февраля",
                "en": "February 11",
                "zh": "2月11日"
            },
            "hash": "68244a5483bfc513",
            "authors": [
                "Víctor Gallego"
            ],
            "affiliations": [
                "Komorebi AI, Madrid, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.07985.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#security",
                    "#alignment",
                    "#inference"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Динамическая оптимизация безопасности языковых моделей без переобучения",
                    "desc": "Авторы предлагают новую динамическую систему безопасности для оптимизации рассуждений языковых моделей во время вывода без изменения весов модели. Подход основан на механизме мета-критики, который итеративно обновляет промпты безопасности для адаптивного управления процессом критики и пересмотра. Оптимизация во время тестирования улучшает защиту от попыток обхода ограничений и повышает безопасность в различных задачах. Эмпирические оценки показывают, что динамически оптимизированные промпты безопасности дают значительно более высокие показатели безопасности по сравнению с фиксированными системными промптами."
                },
                "en": {
                    "title": "Dynamic Safety for Language Models: Adapting Prompts for Better Protection",
                    "desc": "This paper introduces a dynamic safety framework designed to enhance the safety of language models during inference without altering their underlying weights. It utilizes a meta-critique mechanism that adaptively updates safety prompts, known as specifications, to improve the model's ability to handle adversarial inputs and general safety tasks. The approach shows significant improvements in safety performance, particularly in avoiding moral harm and ensuring honest responses. Empirical results indicate that the dynamically optimized safety prompts outperform traditional fixed prompts and static defenses in various scenarios."
                },
                "zh": {
                    "title": "动态优化，提升语言模型安全性！",
                    "desc": "我们提出了一种新颖的动态安全框架，旨在优化语言模型在推理时的安全性推理，而无需修改模型权重。该方法基于自我批评方法的最新进展，利用元批评机制迭代更新安全提示（称为规范），以自适应地推动批评和修订过程。此测试时优化不仅提高了对抗性越狱请求的性能，还在避免道德伤害和追求诚实回应等多种安全相关任务中表现出色。我们的实证评估显示，动态优化的安全提示相比于固定系统提示和静态自我批评防御，显著提高了安全评分。"
                }
            }
        }
    ],
    "link_prev": "2025-02-12.html",
    "link_next": "2025-02-14.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "12.02",
        "en": "02/12",
        "zh": "2月12日"
    },
    "short_date_next": {
        "ru": "14.02",
        "en": "02/14",
        "zh": "2月14日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 0,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 6,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 4,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们展示了强化学习应用于大型语言模型（LLMs）可以显著提升复杂编程和推理任务的表现。我们比较了两个通用推理模型 - OpenAI o1 和 o3 的早期版本，以及一个针对2024年国际信息学奥林匹克（IOI）设计的特定领域系统 o1-ioi。我们在IOI 2024上使用 o1-ioi 参赛，并在放宽的比赛约束下获得了金牌。然而，后续模型如 o3 在没有手工制定的特定领域策略或放宽约束的情况下也能获得金牌。我们的发现表明，虽然专门的流水线如 o1-ioi 带来了显著的改进，但扩展的通用 o3 模型在不依赖手工制定的推理启发式的情况下超越了这些结果。",
        "title": "Competitive Programming with Large Reasoning Models",
        "pinyin": "Wǒmen zhǎnshìle qiáng huà xuéxí yìngyòng yú dàxíng yǔyán móxíng (LLMs) kěyǐ xiǎnzhù tíshēng fùzá bǐan chéng yǔ tuīlǐ rènwù de biǎoxiàn. Wǒmen bǐjiàole liǎng gè tōngyòng tuīlǐ móxíng - OpenAI o1 hé o3 de zǎoqī bǎnběn, yǐjià yīgè zhǐduì 2024 nián guójì xìnxī xué àolínpǐkè (IOI) shèjì de tèdìng yùyí xìtǒng o1-ioi. Wǒmen zài IOI 2024 shàng shǐyòng o1-ioi cānsài, bìng zài fàngkuān de bǐsài yuēshù xià huòdéle jīnpái. Rán'ér, hòuxù móxíng rú o3 zài méiyǒu shǒugōng zhìdìng de tèdìng yùyí cèlüè huò fàngkuān yuēshù de qíngkuàng xià yě néng huòdé jīnpái. Wǒmen de fāxiàn biǎomíng, suīrán zhuānmén de liúshuǐxiàn rú o1-ioi dàilái le xiǎnzhù de gǎijìn, dàn kuòzhǎn de tōngyòng o3 móxíng zài bù yīlài shǒugōng zhìdìng de tuīlǐ qǐfǎshì de qíngkuàng xià chāoyuèle zhèxiē jiéguǒ.",
        "vocab": "[{'word': '展示', 'pinyin': 'zhǎnshì', 'trans': 'display'}, {'word': '强化学习', 'pinyin': 'qiáng​huà​xué​xí', 'trans': 'reinforcement learning'}, {'word': '应用于', 'pinyin': 'yìng​yòng​yú', 'trans': 'apply to'}, {'word': '大型语言模型', 'pinyin': 'dà​xíng​yǔ​yán​mó​xíng', 'trans': 'large language model'}, {'word': '显著', 'pinyin': 'xiǎn​zhù', 'trans': 'significant'}, {'word': '提升', 'pinyin': 'tí​shēng', 'trans': 'improve'}, {'word': '复杂', 'pinyin': 'fù​zá', 'trans': 'complex'}, {'word': '编程', 'pinyin': 'biān​chéng', 'trans': 'programming'}, {'word': '推理', 'pinyin': 'tuī​lǐ', 'trans': 'reasoning'}, {'word': '表现', 'pinyin': 'biǎo​xiàn', 'trans': 'performance'}, {'word': '比较', 'pinyin': 'bǐ​jiào', 'trans': 'compare'}, {'word': '通用', 'pinyin': 'tōng​yòng', 'trans': 'general-purpose'}, {'word': '针对', 'pinyin': 'zhēn​duì', 'trans': 'target'}, {'word': '特定领域', 'pinyin': 'tè​dìng​lǐng​yù', 'trans': 'specific domain'}, {'word': '系统', 'pinyin': 'xì​tǒng', 'trans': 'system'}, {'word': '国际信息学奥林匹克', 'pinyin': 'guó​jì​xìn​xī​xué​ào​lín​pǐ​kè', 'trans': 'International Olympiad in Informatics'}, {'word': '设计', 'pinyin': 'shè​jì', 'trans': 'design'}, {'word': '参赛', 'pinyin': 'cān​sài', 'trans': 'compete'}, {'word': '放宽', 'pinyin': 'fàng​kuān', 'trans': 'relax'}, {'word': '约束', 'pinyin': 'yuē​shù', 'trans': 'constraint'}, {'word': '获得', 'pinyin': 'huò​dé', 'trans': 'obtain'}, {'word': '金牌', 'pinyin': 'jīn​pái', 'trans': 'gold medal'}, {'word': '后续', 'pinyin': 'hòu​xù', 'trans': 'subsequent'}, {'word': '手工制定', 'pinyin': 'shǒu​gōng​zhì​dìng', 'trans': 'manually specified'}, {'word': '策略', 'pinyin': 'cè​lüè', 'trans': 'strategy'}, {'word': '启发式', 'pinyin': 'qǐ​fā​shì', 'trans': 'heuristic'}, {'word': '依赖', 'pinyin': 'yī​lài', 'trans': 'rely on'}, {'word': '扩展', 'pinyin': 'kuò​zhǎn', 'trans': 'extend'}, {'word': '超越', 'pinyin': 'chāo​yuè', 'trans': 'surpass'}, {'word': '结果', 'pinyin': 'jié​guǒ', 'trans': 'result'}]",
        "trans": "We demonstrated that applying reinforcement learning to large language models (LLMs) can significantly enhance performance in complex programming and reasoning tasks. We compared two general reasoning models—early versions of OpenAI o1 and o3—and a domain-specific system, o1-ioi, designed for the 2024 International Olympiad in Informatics (IOI). We competed in IOI 2024 using o1-ioi and won a gold medal under relaxed competition constraints. However, subsequent models like o3 were able to achieve gold medals without handcrafted domain-specific strategies or relaxed constraints. Our findings indicate that while specialized pipelines like o1-ioi brought significant improvements, the expanded general o3 model surpassed these results without relying on handcrafted reasoning heuristics.",
        "update_ts": "2025-02-12 09:11"
    }
}