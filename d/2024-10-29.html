
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. October 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 октября</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-28.html">⬅️ <span id="prev-date">28.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-30.html">➡️ <span id="next-date">30.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'};
        let feedDateNext = {'ru': '30.10', 'en': '10/30', 'zh': '10月30日'};
        let feedDatePrev = {'ru': '28.10', 'en': '10/28', 'zh': '10月28日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.21276', 'title': 'GPT-4o System Card', 'url': 'https://huggingface.co/papers/2410.21276', 'abstract': "GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.", 'score': 79, 'issue_id': 324, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': '15d57a2a0852d31e', 'authors': ['OpenAI', ':', 'Aaron Hurst', 'Adam Lerer', 'Adam P. Goucher', 'Adam Perelman', 'Aditya Ramesh', 'Aidan Clark', 'AJ Ostrow', 'Akila Welihinda', 'Alan Hayes', 'Alec Radford', 'Aleksander Mądry', 'Alex Baker-Whitcomb', 'Alex Beutel', 'Alex Borzunov', 'Alex Carney', 'Alex Chow', 'Alex Kirillov', 'Alex Nichol', 'Alex Paino', 'Alex Renzin', 'Alex Tachard Passos', 'Alexander Kirillov', 'Alexi Christakis', 'Alexis Conneau', 'Ali Kamali', 'Allan Jabri', 'Allison Moyer', 'Allison Tam', 'Amadou Crookes', 'Amin Tootoochian', 'Amin Tootoonchian', 'Ananya Kumar', 'Andrea Vallone', 'Andrej Karpathy', 'Andrew Braunstein', 'Andrew Cann', 'Andrew Codispoti', 'Andrew Galu', 'Andrew Kondrich', 'Andrew Tulloch', 'Andrey Mishchenko', 'Angela Baek', 'Angela Jiang', 'Antoine Pelisse', 'Antonia Woodford', 'Anuj Gosalia', 'Arka Dhar', 'Ashley Pantuliano', 'Avi Nayak', 'Avital Oliver', 'Barret Zoph', 'Behrooz Ghorbani', 'Ben Leimberger', 'Ben Rossen', 'Ben Sokolowsky', 'Ben Wang', 'Benjamin Zweig', 'Beth Hoover', 'Blake Samic', 'Bob McGrew', 'Bobby Spero', 'Bogo Giertler', 'Bowen Cheng', 'Brad Lightcap', 'Brandon Walkin', 'Brendan Quinn', 'Brian Guarraci', 'Brian Hsu', 'Bright Kellogg', 'Brydon Eastman', 'Camillo Lugaresi', 'Carroll Wainwright', 'Cary Bassin', 'Cary Hudson', 'Casey Chu', 'Chad Nelson', 'Chak Li', 'Chan Jun Shern', 'Channing Conger', 'Charlotte Barette', 'Chelsea Voss', 'Chen Ding', 'Cheng Lu', 'Chong Zhang', 'Chris Beaumont', 'Chris Hallacy', 'Chris Koch', 'Christian Gibson', 'Christina Kim', 'Christine Choi', 'Christine McLeavey', 'Christopher Hesse', 'Claudia Fischer', 'Clemens Winter', 'Coley Czarnecki', 'Colin Jarvis', 'Colin Wei', 'Constantin Koumouzelis', 'Dane Sherburn', 'Daniel Kappler', 'Daniel Levin', 'Daniel Levy', 'David Carr', 'David Farhi', 'David Mely', 'David Robinson', 'David Sasaki', 'Denny Jin', 'Dev Valladares', 'Dimitris Tsipras', 'Doug Li', 'Duc Phong Nguyen', 'Duncan Findlay', 'Edede Oiwoh', 'Edmund Wong', 'Ehsan Asdar', 'Elizabeth Proehl', 'Elizabeth Yang', 'Eric Antonow', 'Eric Kramer', 'Eric Peterson', 'Eric Sigler', 'Eric Wallace', 'Eugene Brevdo', 'Evan Mays', 'Farzad Khorasani', 'Felipe Petroski Such', 'Filippo Raso', 'Francis Zhang', 'Fred von Lohmann', 'Freddie Sulit', 'Gabriel Goh', 'Gene Oden', 'Geoff Salmon', 'Giulio Starace', 'Greg Brockman', 'Hadi Salman', 'Haiming Bao', 'Haitang Hu', 'Hannah Wong', 'Haoyu Wang', 'Heather Schmidt', 'Heather Whitney', 'Heewoo Jun', 'Hendrik Kirchner', 'Henrique Ponde de Oliveira Pinto', 'Hongyu Ren', 'Huiwen Chang', 'Hyung Won Chung', 'Ian Kivlichan', "Ian O'Connell", "Ian O'Connell", 'Ian Osband', 'Ian Silber', 'Ian Sohl', 'Ibrahim Okuyucu', 'Ikai Lan', 'Ilya Kostrikov', 'Ilya Sutskever', 'Ingmar Kanitscheider', 'Ishaan Gulrajani', 'Jacob Coxon', 'Jacob Menick', 'Jakub Pachocki', 'James Aung', 'James Betker', 'James Crooks', 'James Lennon', 'Jamie Kiros', 'Jan Leike', 'Jane Park', 'Jason Kwon', 'Jason Phang', 'Jason Teplitz', 'Jason Wei', 'Jason Wolfe', 'Jay Chen', 'Jeff Harris', 'Jenia Varavva', 'Jessica Gan Lee', 'Jessica Shieh', 'Ji Lin', 'Jiahui Yu', 'Jiayi Weng', 'Jie Tang', 'Jieqi Yu', 'Joanne Jang', 'Joaquin Quinonero Candela', 'Joe Beutler', 'Joe Landers', 'Joel Parish', 'Johannes Heidecke', 'John Schulman', 'Jonathan Lachman', 'Jonathan McKay', 'Jonathan Uesato', 'Jonathan Ward', 'Jong Wook Kim', 'Joost Huizinga', 'Jordan Sitkin', 'Jos Kraaijeveld', 'Josh Gross', 'Josh Kaplan', 'Josh Snyder', 'Joshua Achiam', 'Joy Jiao', 'Joyce Lee', 'Juntang Zhuang', 'Justyn Harriman', 'Kai Fricke', 'Kai Hayashi', 'Karan Singhal', 'Katy Shi', 'Kavin Karthik', 'Kayla Wood', 'Kendra Rimbach', 'Kenny Hsu', 'Kenny Nguyen', 'Keren Gu-Lemberg', 'Kevin Button', 'Kevin Liu', 'Kiel Howe', 'Krithika Muthukumar', 'Kyle Luther', 'Lama Ahmad', 'Larry Kai', 'Lauren Itow', 'Lauren Workman', 'Leher Pathak', 'Leo Chen', 'Li Jing', 'Lia Guy', 'Liam Fedus', 'Liang Zhou', 'Lien Mamitsuka', 'Lilian Weng', 'Lindsay McCallum', 'Lindsey Held', 'Long Ouyang', 'Louis Feuvrier', 'Lu Zhang', 'Lukas Kondraciuk', 'Lukasz Kaiser', 'Luke Hewitt', 'Luke Metz', 'Lyric Doshi', 'Mada Aflak', 'Maddie Simens', 'Madelaine Boyd', 'Madeleine Thompson', 'Marat Dukhan', 'Mark Chen', 'Mark Gray', 'Mark Hudnall', 'Marvin Zhang', 'Marwan Aljubeh', 'Mateusz Litwin', 'Matthew Zeng', 'Max Johnson', 'Maya Shetty', 'Mayank Gupta', 'Meghan Shah', 'Mehmet Yatbaz', 'Meng Jia Yang', 'Mengchao Zhong', 'Mia Glaese', 'Mianna Chen', 'Michael Janner', 'Michael Lampe', 'Michael Petrov', 'Michael Wu', 'Michele Wang', 'Michelle Fradin', 'Michelle Pokrass', 'Miguel Castro', 'Miguel Oom Temudo de Castro', 'Mikhail Pavlov', 'Miles Brundage', 'Miles Wang', 'Minal Khan', 'Mira Murati', 'Mo Bavarian', 'Molly Lin', 'Murat Yesildal', 'Nacho Soto', 'Natalia Gimelshein', 'Natalie Cone', 'Natalie Staudacher', 'Natalie Summers', 'Natan LaFontaine', 'Neil Chowdhury', 'Nick Ryder', 'Nick Stathas', 'Nick Turley', 'Nik Tezak', 'Niko Felix', 'Nithanth Kudige', 'Nitish Keskar', 'Noah Deutsch', 'Noel Bundick', 'Nora Puckett', 'Ofir Nachum', 'Ola Okelola', 'Oleg Boiko', 'Oleg Murk', 'Oliver Jaffe', 'Olivia Watkins', 'Olivier Godement', 'Owen Campbell-Moore', 'Patrick Chao', 'Paul McMillan', 'Pavel Belov', 'Peng Su', 'Peter Bak', 'Peter Bakkum', 'Peter Deng', 'Peter Dolan', 'Peter Hoeschele', 'Peter Welinder', 'Phil Tillet', 'Philip Pronin', 'Philippe Tillet', 'Prafulla Dhariwal', 'Qiming Yuan', 'Rachel Dias', 'Rachel Lim', 'Rahul Arora', 'Rajan Troll', 'Randall Lin', 'Rapha Gontijo Lopes', 'Raul Puri', 'Reah Miyara', 'Reimar Leike', 'Renaud Gaubert', 'Reza Zamani', 'Ricky Wang', 'Rob Donnelly', 'Rob Honsby', 'Rocky Smith', 'Rohan Sahai', 'Rohit Ramchandani', 'Romain Huet', 'Rory Carmichael', 'Rowan Zellers', 'Roy Chen', 'Ruby Chen', 'Ruslan Nigmatullin', 'Ryan Cheu', 'Saachi Jain', 'Sam Altman', 'Sam Schoenholz', 'Sam Toizer', 'Samuel Miserendino', 'Sandhini Agarwal', 'Sara Culver', 'Scott Ethersmith', 'Scott Gray', 'Sean Grove', 'Sean Metzger', 'Shamez Hermani', 'Shantanu Jain', 'Shengjia Zhao', 'Sherwin Wu', 'Shino Jomoto', 'Shirong Wu', 'Shuaiqi', 'Xia', 'Sonia Phene', 'Spencer Papay', 'Srinivas Narayanan', 'Steve Coffey', 'Steve Lee', 'Stewart Hall', 'Suchir Balaji', 'Tal Broda', 'Tal Stramer', 'Tao Xu', 'Tarun Gogineni', 'Taya Christianson', 'Ted Sanders', 'Tejal Patwardhan', 'Thomas Cunninghman', 'Thomas Degry', 'Thomas Dimson', 'Thomas Raoux', 'Thomas Shadwell', 'Tianhao Zheng', 'Todd Underwood', 'Todor Markov', 'Toki Sherbakov', 'Tom Rubin', 'Tom Stasi', 'Tomer Kaftan', 'Tristan Heywood', 'Troy Peterson', 'Tyce Walters', 'Tyna Eloundou', 'Valerie Qi', 'Veit Moeller', 'Vinnie Monaco', 'Vishal Kuo', 'Vlad Fomenko', 'Wayne Chang', 'Weiyi Zheng', 'Wenda Zhou', 'Wesam Manassra', 'Will Sheu', 'Wojciech Zaremba', 'Yash Patil', 'Yilei Qian', 'Yongjik Kim', 'Youlong Cheng', 'Yu Zhang', 'Yuchen He', 'Yuchen Zhang', 'Yujia Jin', 'Yunxing Dai', 'Yury Malkov'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21276.jpg', 'data': {'categories': ['#synthetic', '#multilingual', '#cv', '#video', '#multimodal', '#ethics', '#training', '#open_source', '#audio', '#security', '#architecture', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'GPT-4o: Революция в мультимодальном ИИ', 'desc': 'GPT-4o - это мультимодальная модель, способная обрабатывать и генерировать текст, аудио, изображения и видео. Она обучена сквозным образом на различных типах данных, что позволяет единой нейронной сети обрабатывать все виды входных и выходных данных. Модель демонстрирует высокую скорость отклика на аудиовходы, сравнимую с человеческой, и превосходит существующие модели в понимании визуальной и аудиоинформации. GPT-4o также показывает улучшенную производительность для неанглийских языков при сохранении высокого качества работы с английским текстом и кодом.'}, 'en': {'title': 'GPT-4o: The All-in-One AI for Text, Audio, and Vision', 'desc': 'GPT-4o is a versatile autoregressive model that can process and generate various types of media, including text, audio, images, and video. It utilizes an end-to-end training approach, allowing it to handle all input and output modalities through a single neural network. The model demonstrates rapid response times comparable to human conversation and shows enhanced performance in understanding non-English text, audio, and visual data. Additionally, GPT-4o prioritizes safety and alignment, providing a comprehensive System Card that outlines its capabilities, limitations, and societal implications.'}, 'zh': {'title': '全能模型，快速响应，安全可靠', 'desc': 'GPT-4o是一种自回归的全能模型，能够处理文本、音频、图像和视频的任意组合，并生成相应的输出。它通过一个统一的神经网络进行端到端训练，确保所有输入和输出的高效处理。GPT-4o在响应音频输入时的速度与人类对话相似，且在非英语文本处理上有显著提升，同时在视觉和音频理解方面表现优于现有模型。该模型的系统卡片详细介绍了其能力、局限性和安全评估，确保其在社会影响和潜在危险能力方面的透明性。'}}}, {'id': 'https://huggingface.co/papers/2410.18565', 'title': 'Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation', 'url': 'https://huggingface.co/papers/2410.18565', 'abstract': 'We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for Polish language processing. Trained on curated Polish corpora, this model addresses key challenges in language model development through innovative techniques. These include Weighted Instruction Cross-Entropy Loss, which balances the learning of different instruction types, and Adaptive Learning Rate, which dynamically adjusts the learning rate based on training progress. To evaluate performance, we created the Open PL LLM Leaderboard and Polish MT-Bench, novel frameworks assessing various NLP tasks and conversational abilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9 percentage point increase in average score compared to Mistral-7B-v0.1 on the RAG Reader task. It also excels in the Polish MT-Bench, particularly in Reasoning (6.15/10) and Role-playing (7.83/10) categories. This model represents a substantial advancement in Polish language AI, offering a powerful tool for diverse linguistic applications and setting new benchmarks in the field.', 'score': 42, 'issue_id': 325, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'c68d615cedcd651e', 'authors': ['Krzysztof Ociepa', 'Łukasz Flis', 'Krzysztof Wróbel', 'Adrian Gwoździej', 'Remigiusz Kinas'], 'affiliations': ['ACK Cyfronet AGH', 'Azurro', 'Enelpol', 'Jagiellonian University', 'SpeakLeash'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18565.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#benchmark', '#multilingual', '#optimization', '#training', '#dataset', '#transfer_learning', '#open_source', '#machine_translation'], 'emoji': '🇵🇱', 'ru': {'title': 'Прорыв в обработке польского языка: мощная 7B-модель Bielik', 'desc': 'Представлен Bielik 7B v0.1 - языковая модель для польского языка с 7 миллиардами параметров. Модель использует инновационные техники обучения, включая взвешенную кросс-энтропийную функцию потерь для инструкций и адаптивную скорость обучения. Для оценки созданы Open PL LLM Leaderboard и Polish MT-Bench, тестирующие различные задачи обработки естественного языка. Bielik 7B v0.1 демонстрирует значительное улучшение производительности по сравнению с предыдущими моделями, особенно в задачах рассуждения и ролевых игр.'}, 'en': {'title': 'Bielik 7B: Advancing Polish Language AI with 7 Billion Parameters', 'desc': "Bielik 7B v0.1 is a generative text model designed specifically for processing the Polish language, featuring 7 billion parameters. It employs advanced techniques like Weighted Instruction Cross-Entropy Loss to enhance learning across various instruction types and an Adaptive Learning Rate that adjusts based on the model's training progress. The model's performance is evaluated using new frameworks such as the Open PL LLM Leaderboard and Polish MT-Bench, which assess its capabilities in natural language processing tasks. Bielik 7B v0.1 shows significant improvements over previous models, particularly in reasoning and role-playing tasks, marking a major step forward in Polish language AI applications."}, 'zh': {'title': 'Bielik 7B v0.1：波兰语处理的新标杆', 'desc': 'Bielik 7B v0.1 是一个用于波兰语处理的生成文本模型，拥有70亿个参数。该模型通过创新技术解决了语言模型开发中的关键挑战，包括加权指令交叉熵损失和自适应学习率。我们还创建了开放的PL LLM排行榜和波兰MT-Bench，以评估模型在各种自然语言处理任务和对话能力上的表现。Bielik 7B v0.1 在RAG Reader任务上比Mistral-7B-v0.1提高了9个百分点，特别在推理和角色扮演类别中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2410.20011', 'title': 'A Survey of Small Language Models', 'url': 'https://huggingface.co/papers/2410.20011', 'abstract': 'Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device, mobile, edge devices, among many others. In this article, we present a comprehensive survey on SLMs, focusing on their architectures, training techniques, and model compression techniques. We propose a novel taxonomy for categorizing the methods used to optimize SLMs, including model compression, pruning, and quantization techniques. We summarize the benchmark datasets that are useful for benchmarking SLMs along with the evaluation metrics commonly used. Additionally, we highlight key open challenges that remain to be addressed. Our survey aims to serve as a valuable resource for researchers and practitioners interested in developing and deploying small yet efficient language models.', 'score': 38, 'issue_id': 321, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': 'bde2fa0e4317a316', 'authors': ['Chien Van Nguyen', 'Xuan Shen', 'Ryan Aponte', 'Yu Xia', 'Samyadeep Basu', 'Zhengmian Hu', 'Jian Chen', 'Mihir Parmar', 'Sasidhar Kunapuli', 'Joe Barrow', 'Junda Wu', 'Ashish Singh', 'Yu Wang', 'Jiuxiang Gu', 'Franck Dernoncourt', 'Nesreen K. Ahmed', 'Nedim Lipka', 'Ruiyi Zhang', 'Xiang Chen', 'Tong Yu', 'Sungchul Kim', 'Hanieh Deilamsalehy', 'Namyong Park', 'Mike Rimer', 'Zhehao Zhang', 'Huanrui Yang', 'Ryan A. Rossi', 'Thien Huu Nguyen'], 'affiliations': ['Adobe Research', 'Arizona State University', 'Carnegie Mellon University', 'Dartmouth College', 'Intel AI Research', 'Meta AI', 'Northeastern University', 'State University of New York at Buffalo', 'University of Arizona', 'University of California, San Diego', 'University of Maryland, College Park', 'University of Massachusetts Amherst', 'University of Oregon'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20011.jpg', 'data': {'categories': ['#small_models', '#benchmark', '#inference', '#optimization', '#training', '#survey', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Малые языковые модели: эффективность в компактности', 'desc': 'Эта статья представляет собой всесторонний обзор малых языковых моделей (SLM), которые становятся все более важными из-за их эффективности и производительности при минимальных вычислительных ресурсах. Авторы предлагают новую таксономию для категоризации методов оптимизации SLM, включая сжатие модели, прунинг и квантизацию. В работе обобщаются наборы данных для бенчмаркинга SLM и метрики оценки. Статья также освещает ключевые нерешенные проблемы в области малых языковых моделей.'}, 'en': {'title': 'Optimizing Small Language Models for Efficiency and Performance', 'desc': 'This paper provides a detailed overview of Small Language Models (SLMs), which are designed to perform language tasks efficiently with low computational requirements. It introduces a new classification system for the various optimization methods used in SLMs, such as model compression, pruning, and quantization. The authors also compile important benchmark datasets and evaluation metrics that are essential for assessing the performance of SLMs. Furthermore, the paper discusses ongoing challenges in the field, aiming to assist researchers and practitioners in advancing the development of efficient language models.'}, 'zh': {'title': '小型语言模型：高效语言处理的未来', 'desc': '小型语言模型（SLMs）因其高效性和性能而变得越来越重要，能够在资源有限的情况下执行各种语言任务，非常适合在设备、移动和边缘设备等环境中使用。本文对SLMs进行了全面的调查，重点介绍了它们的架构、训练技术和模型压缩技术。我们提出了一种新的分类法，用于对优化SLMs的方法进行分类，包括模型压缩、剪枝和量化技术。我们总结了对SLMs进行基准测试的有用数据集以及常用的评估指标，并强调了仍需解决的关键开放挑战。'}}}, {'id': 'https://huggingface.co/papers/2410.18603', 'title': 'AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant', 'url': 'https://huggingface.co/papers/2410.18603', 'abstract': "Digital agents capable of automating complex computer tasks have attracted considerable attention due to their immense potential to enhance human-computer interaction. However, existing agent methods exhibit deficiencies in their generalization and specialization capabilities, especially in handling open-ended computer tasks in real-world environments. Inspired by the rich functionality of the App store, we present AgentStore, a scalable platform designed to dynamically integrate heterogeneous agents for automating computer tasks. AgentStore empowers users to integrate third-party agents, allowing the system to continuously enrich its capabilities and adapt to rapidly evolving operating systems. Additionally, we propose a novel core MetaAgent with the AgentToken strategy to efficiently manage diverse agents and utilize their specialized and generalist abilities for both domain-specific and system-wide tasks. Extensive experiments on three challenging benchmarks demonstrate that AgentStore surpasses the limitations of previous systems with narrow capabilities, particularly achieving a significant improvement from 11.21\\% to 23.85\\% on the OSWorld benchmark, more than doubling the previous results. Comprehensive quantitative and qualitative results further demonstrate AgentStore's ability to enhance agent systems in both generalization and specialization, underscoring its potential for developing the specialized generalist computer assistant. All our codes will be made publicly available in https://chengyou-jia.github.io/AgentStore-Home.", 'score': 30, 'issue_id': 324, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '9d62252fde1756e0', 'authors': ['Chengyou Jia', 'Minnan Luo', 'Zhuohang Dang', 'Qiushi Sun', 'Fangzhi Xu', 'Junlin Hu', 'Tianbao Xie', 'Zhiyong Wu'], 'affiliations': ['Shanghai AI Lab', 'The University of Hong Kong', 'Xian Jiaotong University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18603.jpg', 'data': {'categories': ['#benchmark', '#agi', '#open_source', '#agents', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'AgentStore: Универсальный помощник для автоматизации компьютерных задач', 'desc': 'AgentStore - это новая масштабируемая платформа для автоматизации компьютерных задач, вдохновленная функциональностью магазина приложений. Она позволяет интегрировать сторонних агентов, постоянно расширяя возможности системы и адаптируясь к быстро меняющимся операционным системам. В основе платформы лежит MetaAgent с AgentToken стратегией для эффективного управления разнообразными агентами. Эксперименты показали значительное улучшение результатов по сравнению с предыдущими системами, особенно на бенчмарке OSWorld.'}, 'en': {'title': 'AgentStore: Empowering Dynamic Integration of Digital Agents for Enhanced Automation', 'desc': 'This paper introduces AgentStore, a platform that allows for the integration of various digital agents to automate complex computer tasks. It addresses the limitations of existing agent systems in generalization and specialization, particularly in real-world applications. The core innovation is the MetaAgent with the AgentToken strategy, which efficiently manages diverse agents to leverage their strengths for both specific and broad tasks. Experimental results show that AgentStore significantly improves performance on benchmarks, demonstrating its effectiveness in enhancing human-computer interaction.'}, 'zh': {'title': 'AgentStore：提升计算机任务自动化的智能代理平台', 'desc': '本文介绍了一种名为AgentStore的平台，旨在通过动态集成异构代理来自动化复杂的计算机任务。现有的代理方法在处理开放式计算机任务时存在泛化和专业化能力不足的问题。AgentStore允许用户集成第三方代理，从而不断丰富系统的功能，适应快速变化的操作系统。通过在多个基准测试上的实验，AgentStore在泛化和专业化方面表现出色，显著提高了系统的能力。'}}}, {'id': 'https://huggingface.co/papers/2410.21169', 'title': 'Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction', 'url': 'https://huggingface.co/papers/2410.21169', 'abstract': 'Document parsing is essential for converting unstructured and semi-structured documents-such as contracts, academic papers, and invoices-into structured, machine-readable data. Document parsing extract reliable structured data from unstructured inputs, providing huge convenience for numerous applications. Especially with recent achievements in Large Language Models, document parsing plays an indispensable role in both knowledge base construction and training data generation. This survey presents a comprehensive review of the current state of document parsing, covering key methodologies, from modular pipeline systems to end-to-end models driven by large vision-language models. Core components such as layout detection, content extraction (including text, tables, and mathematical expressions), and multi-modal data integration are examined in detail. Additionally, this paper discusses the challenges faced by modular document parsing systems and vision-language models in handling complex layouts, integrating multiple modules, and recognizing high-density text. It emphasizes the importance of developing larger and more diverse datasets and outlines future research directions.', 'score': 29, 'issue_id': 325, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'b5541dbb322ac8c4', 'authors': ['Qintong Zhang', 'Victor Shea-Jay Huang', 'Bin Wang', 'Junyuan Zhang', 'Zhengren Wang', 'Hao Liang', 'Shawn Wang', 'Matthieu Lin', 'Conghui He', 'Wentao Zhang'], 'affiliations': ['Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21169.jpg', 'data': {'categories': ['#science', '#cv', '#multimodal', '#data', '#dataset', '#survey', '#architecture'], 'emoji': '📄', 'ru': {'title': 'Парсинг документов: от модульных систем к мультимодальным моделям', 'desc': 'Эта статья представляет собой обзор современного состояния парсинга документов. Рассматриваются ключевые методологии, от модульных систем до end-to-end моделей, основанных на крупных мультимодальных языковых моделях. Обсуждаются основные компоненты, такие как обнаружение макета, извлечение контента и интеграция мультимодальных данных. Статья также анализирует проблемы, с которыми сталкиваются системы парсинга документов, и намечает направления будущих исследований.'}, 'en': {'title': 'Transforming Documents: From Chaos to Clarity with Parsing', 'desc': 'This paper reviews the process of document parsing, which transforms unstructured documents into structured data that machines can understand. It highlights the advancements made possible by Large Language Models, which enhance the efficiency of knowledge base construction and training data generation. The survey covers various methodologies, including modular pipeline systems and end-to-end models, focusing on essential tasks like layout detection and content extraction. It also addresses the challenges in parsing complex layouts and the need for larger datasets to improve model performance.'}, 'zh': {'title': '文档解析：从非结构到结构的关键技术', 'desc': '文档解析是将非结构化和半结构化文档（如合同、学术论文和发票）转换为结构化、机器可读数据的重要过程。通过文档解析，可以从非结构化输入中提取可靠的结构化数据，为许多应用提供极大的便利。尤其是在大型语言模型取得的最新进展下，文档解析在知识库构建和训练数据生成中发挥着不可或缺的作用。本文综述了文档解析的现状，涵盖了从模块化管道系统到由大型视觉-语言模型驱动的端到端模型的关键方法。'}}}, {'id': 'https://huggingface.co/papers/2410.20280', 'title': 'MarDini: Masked Autoregressive Diffusion for Video Generation at Scale', 'url': 'https://huggingface.co/papers/2410.20280', 'abstract': "We introduce MarDini, a new family of video diffusion models that integrate the advantages of masked auto-regression (MAR) into a unified diffusion model (DM) framework. Here, MAR handles temporal planning, while DM focuses on spatial generation in an asymmetric network design: i) a MAR-based planning model containing most of the parameters generates planning signals for each masked frame using low-resolution input; ii) a lightweight generation model uses these signals to produce high-resolution frames via diffusion de-noising. MarDini's MAR enables video generation conditioned on any number of masked frames at any frame positions: a single model can handle video interpolation (e.g., masking middle frames), image-to-video generation (e.g., masking from the second frame onward), and video expansion (e.g., masking half the frames). The efficient design allocates most of the computational resources to the low-resolution planning model, making computationally expensive but important spatio-temporal attention feasible at scale. MarDini sets a new state-of-the-art for video interpolation; meanwhile, within few inference steps, it efficiently generates videos on par with those of much more expensive advanced image-to-video models.", 'score': 21, 'issue_id': 322, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '33b4a79a3995aa46', 'authors': ['Haozhe Liu', 'Shikun Liu', 'Zijian Zhou', 'Mengmeng Xu', 'Yanping Xie', 'Xiao Han', 'Juan C. Pérez', 'Ding Liu', 'Kumara Kahatapitiya', 'Menglin Jia', 'Jui-Chieh Wu', 'Sen He', 'Tao Xiang', 'Jürgen Schmidhuber', 'Juan-Manuel Pérez-Rúa'], 'affiliations': ['KAUST', 'Meta AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20280.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#inference', '#video', '#optimization', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'MarDini: Гибкая генерация видео с маскированной авторегрессией и диффузией', 'desc': 'MarDini - это новое семейство моделей диффузии видео, объединяющее преимущества маскированной авторегрессии (MAR) и диффузионных моделей (DM). MAR используется для временного планирования, а DM - для пространственной генерации в асимметричной архитектуре сети. Модель планирования на основе MAR генерирует сигналы для каждого маскированного кадра, используя входные данные низкого разрешения. Легковесная модель генерации использует эти сигналы для создания кадров высокого разрешения посредством диффузионного шумоподавления.'}, 'en': {'title': 'MarDini: Revolutionizing Video Generation with Efficient Diffusion Models', 'desc': 'MarDini is a novel family of video diffusion models that combines masked auto-regression (MAR) with a unified diffusion model (DM) framework. The MAR component is responsible for planning the sequence of frames, while the DM focuses on generating high-quality spatial outputs. This model can handle various tasks such as video interpolation, image-to-video generation, and video expansion by conditioning on different masked frames. By optimizing the computational resources, MarDini achieves state-of-the-art performance in video interpolation and efficiently generates videos comparable to more complex models.'}, 'zh': {'title': 'MarDini：视频生成的新突破', 'desc': 'MarDini是一种新的视频扩散模型家族，它将掩蔽自回归（MAR）的优势与统一的扩散模型（DM）框架结合在一起。MAR负责时间规划，而DM则专注于空间生成，采用不对称网络设计。该模型能够根据任意数量的掩蔽帧生成视频，支持视频插值、图像到视频生成和视频扩展等多种任务。MarDini在视频插值方面设定了新的最先进水平，并且在少量推理步骤内高效生成与更昂贵的图像到视频模型相当的视频。'}}}, {'id': 'https://huggingface.co/papers/2410.19313', 'title': 'COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training', 'url': 'https://huggingface.co/papers/2410.19313', 'abstract': "FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54x compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43x end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine's speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT.", 'score': 18, 'issue_id': 323, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': '9c2301bebb36909b', 'authors': ['Haocheng Xi', 'Han Cai', 'Ligeng Zhu', 'Yao Lu', 'Kurt Keutzer', 'Jianfei Chen', 'Song Han'], 'affiliations': ['MIT', 'NVIDIA', 'Tsinghua University', 'University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.19313.jpg', 'data': {'categories': ['#small_models', '#inference', '#optimization', '#training', '#open_source'], 'emoji': '🚀', 'ru': {'title': 'COAT: Эффективное обучение больших нейросетей с минимальными затратами памяти', 'desc': 'COAT - это новая система обучения нейросетей в формате FP8, которая значительно сокращает использование памяти при обучении больших моделей. Она вводит две ключевые инновации: динамическое расширение диапазона для оптимизации состояний оптимизатора и смешанную грануляцию квантования активаций. Эксперименты показывают, что COAT уменьшает общий объем используемой памяти в 1,54 раза по сравнению с BF16, сохраняя при этом производительность на различных задачах. Система также достигает ускорения обучения в 1,43 раза по сравнению с BF16, что делает ее практичным решением для масштабирования обучения крупных моделей.'}, 'en': {'title': 'COAT: Revolutionizing FP8 Training for Large Models', 'desc': 'This paper presents COAT, a new framework for FP8 training that enhances memory efficiency during the training of large machine learning models. COAT introduces two main innovations: Dynamic Range Expansion to minimize quantization errors in optimizer states, and Mixed-Granularity Activation Quantization to optimize memory usage for activations. The framework achieves a 1.54x reduction in memory footprint and a 1.43x speedup in training compared to traditional BF16 methods, while maintaining high performance across various tasks. By enabling efficient training on fewer GPUs and allowing larger batch sizes, COAT provides a scalable solution for large-scale model training.'}, 'zh': {'title': 'COAT：高效的FP8训练内存优化方案', 'desc': 'FP8训练是一种提高训练效率的新方法。现有框架在使用FP8计算时，优化器状态和激活仍保持高精度，未能充分优化内存使用。本文提出了COAT框架，通过动态范围扩展和混合粒度激活量化，显著减少大模型训练的内存占用。实验表明，COAT在多种任务中实现了接近无损的性能，同时内存占用减少了1.54倍，训练速度提升了1.43倍。'}}}, {'id': 'https://huggingface.co/papers/2410.18666', 'title': 'DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation', 'url': 'https://huggingface.co/papers/2410.18666', 'abstract': "Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model. GenIR, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models. GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation & filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction. The result is a large-scale dataset of one million high-quality images. Our second contribution, DreamClear, is a DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address. Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration. Code and pre-trained models will be available at: https://github.com/shallowdream204/DreamClear.", 'score': 18, 'issue_id': 322, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'ebdde1eca3a0b04c', 'authors': ['Yuang Ai', 'Xiaoqiang Zhou', 'Huaibo Huang', 'Xiaotian Han', 'Zhengyu Chen', 'Quanzeng You', 'Hongxia Yang'], 'affiliations': ['ByteDance, Inc', 'MAIS & NLPR, Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18666.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#multimodal', '#data', '#dataset', '#open_source', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Революция в восстановлении изображений: от генерации данных до фотореалистичных результатов', 'desc': 'Статья представляет новый подход к восстановлению изображений в реальных условиях. Авторы предлагают GenIR - инновационный конвейер для создания датасетов, и DreamClear - модель восстановления изображений на основе Diffusion Transformer. GenIR использует двойное обучение с подсказками для создания масштабного датасета из миллиона высококачественных изображений. DreamClear применяет генеративные приоры диффузионных моделей и перцептивные способности мультимодальных языковых моделей для фотореалистичного восстановления.'}, 'en': {'title': 'Revolutionizing Image Restoration with GenIR and DreamClear', 'desc': 'This paper addresses the challenges of image restoration (IR) in real-world applications by introducing two key innovations: GenIR and DreamClear. GenIR is a data curation pipeline that creates a large-scale dataset of one million high-quality images through a dual-prompt learning approach, enhancing the generalizability of models. DreamClear is a Diffusion Transformer-based model that leverages generative priors from text-to-image diffusion models and integrates adaptive modulation to handle various image degradations effectively. The results demonstrate that this dual strategy significantly improves the performance of image restoration tasks in practical scenarios.'}, 'zh': {'title': '双重策略提升图像恢复能力', 'desc': '本文提出了一种针对现实场景中图像恢复（IR）问题的双重策略，包括GenIR数据策划管道和基于扩散变换器（DiT）的DreamClear图像恢复模型。GenIR通过构建图像-文本对、双提示微调和数据生成与过滤，克服了现有数据集的局限性，提供了一个包含一百万高质量图像的大规模数据集。DreamClear模型利用文本到图像扩散模型的生成先验和多模态大语言模型的感知能力，实现了逼真的图像恢复。通过引入自适应调制混合器（MoAM），该模型能够动态整合多种恢复专家，增强了对各种现实世界退化的适应能力。'}}}, {'id': 'https://huggingface.co/papers/2410.21252', 'title': 'LongReward: Improving Long-context Large Language Models with AI Feedback', 'url': 'https://huggingface.co/papers/2410.21252', 'abstract': "Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinforcement learning (RL) with appropriate reward signals can further enhance models' capacities. However, how to obtain reliable rewards in long-context scenarios remains unexplored. To this end, we propose LongReward, a novel method that utilizes an off-the-shelf LLM to provide rewards for long-context model responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, each with a carefully designed assessment pipeline. By combining LongReward and offline RL algorithm DPO, we are able to effectively improve long-context SFT models. Our experiments indicate that LongReward not only significantly improves models' long-context performance but also enhances their ability to follow short instructions. We also find that long-context DPO with LongReward and conventional short-context DPO can be used together without hurting either one's performance.", 'score': 16, 'issue_id': 321, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '0ff5d39896cdfbbe', 'authors': ['Jiajie Zhang', 'Zhongni Hou', 'Xin Lv', 'Shulin Cao', 'Zhenyu Hou', 'Yilin Niu', 'Lei Hou', 'Yuxiao Dong', 'Ling Feng', 'Juanzi Li'], 'affiliations': ['Tsinghua University', 'University of Chinese Academy of Sciences', 'Zhipu AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21252.jpg', 'data': {'categories': ['#rl', '#rlhf', '#optimization', '#training', '#long_context', '#alignment'], 'emoji': '📏', 'ru': {'title': 'LongReward: Улучшение языковых моделей для работы с длинным контекстом', 'desc': 'Статья представляет новый метод LongReward для улучшения работы языковых моделей с длинным контекстом. Метод использует готовую большую языковую модель для оценки ответов по четырем параметрам: полезность, логичность, точность и полнота. LongReward применяется вместе с алгоритмом обучения с подкреплением DPO для улучшения моделей, обученных на длинных текстах. Эксперименты показывают, что метод значительно улучшает работу с длинным контекстом и короткими инструкциями.'}, 'en': {'title': 'Enhancing Long-Context Performance with LongReward', 'desc': 'This paper addresses the challenges faced by long-context large language models (LLMs) in generating high-quality data for supervised fine-tuning (SFT). It introduces LongReward, a method that leverages an existing LLM to provide rewards based on four key dimensions: helpfulness, logicality, faithfulness, and completeness. By integrating LongReward with the offline reinforcement learning algorithm DPO, the authors demonstrate significant improvements in the long-context performance of SFT models. The findings suggest that LongReward enhances both long-context and short instruction-following capabilities without compromising performance across different contexts.'}, 'zh': {'title': '提升长上下文模型性能的新方法', 'desc': '本文提出了一种名为LongReward的新方法，旨在提高长上下文大语言模型（LLM）的性能。通过利用现成的LLM，从四个维度（有用性、逻辑性、可信性和完整性）为长上下文模型的响应提供奖励信号。结合LongReward和离线强化学习算法DPO，我们能够有效提升长上下文的监督微调模型的表现。实验结果表明，LongReward不仅显著改善了模型的长上下文性能，还增强了其执行短指令的能力。'}}}, {'id': 'https://huggingface.co/papers/2410.20474', 'title': 'GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation', 'url': 'https://huggingface.co/papers/2410.20474', 'abstract': 'We introduce a novel training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT). Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation. However, prior training-free approaches often rely on updating the noisy image during the reverse diffusion process via backpropagation from custom loss functions, which frequently struggle to provide precise control over individual bounding boxes. In this work, we leverage the flexibility of the Transformer architecture, demonstrating that DiT can generate noisy patches corresponding to each bounding box, fully encoding the target object and allowing for fine-grained control over each region. Our approach builds on an intriguing property of DiT, which we refer to as semantic sharing. Due to semantic sharing, when a smaller patch is jointly denoised alongside a generatable-size image, the two become "semantic clones". Each patch is denoised in its own branch of the generation process and then transplanted into the corresponding region of the original noisy image at each timestep, resulting in robust spatial grounding for each bounding box. In our experiments on the HRS and DrawBench benchmarks, we achieve state-of-the-art performance compared to previous training-free spatial grounding approaches.', 'score': 13, 'issue_id': 323, 'pub_date': '2024-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': '354e97e3a32e2063', 'authors': ['Phillip Y. Lee', 'Taehoon Yoon', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20474.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#training', '#games', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Точная пространственная привязка в генерации изображений без дополнительного обучения', 'desc': "Представлен новый метод пространственной привязки для генерации изображений по тексту с использованием Diffusion Transformers (DiT) без дополнительного обучения. Метод использует гибкость архитектуры трансформеров для генерации шумовых патчей, соответствующих каждому ограничивающему прямоугольнику, что обеспечивает точный контроль над отдельными областями. Подход основан на свойстве DiT, называемом 'семантическим обменом', которое позволяет создавать 'семантические клоны' при совместном шумоподавлении маленького патча и полноразмерного изображения. Эксперименты на бенчмарках HRS и DrawBench показывают превосходство метода над существующими подходами без дополнительного обучения."}, 'en': {'title': 'Empowering Image Generation with Training-Free Spatial Grounding!', 'desc': "This paper presents a new method for spatial grounding in text-to-image generation using Diffusion Transformers (DiT) without the need for prior training. The technique allows for better user control by generating specific image patches that correspond to defined bounding boxes. Unlike previous methods that struggled with precise control, this approach utilizes the unique property of semantic sharing in DiT, enabling the generation of 'semantic clones' for each patch. The results show that this method outperforms existing training-free techniques on benchmark datasets, achieving state-of-the-art performance."}, 'zh': {'title': '无训练空间定位，精细控制图像生成', 'desc': '我们提出了一种新颖的无训练空间定位技术，用于文本到图像生成，采用扩散变换器（DiT）。这种空间定位方法通过边界框实现，因其简单性和多功能性而受到关注，增强了用户在图像生成中的控制能力。我们利用变换器架构的灵活性，展示了DiT能够生成与每个边界框对应的噪声补丁，从而实现对每个区域的精细控制。我们的实验表明，与之前的无训练空间定位方法相比，我们的方法在HRS和DrawBench基准测试中达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2410.20290', 'title': 'Fast Best-of-N Decoding via Speculative Rejection', 'url': 'https://huggingface.co/papers/2410.20290', 'abstract': "The safe and effective deployment of Large Language Models (LLMs) involves a critical step called alignment, which ensures that the model's responses are in accordance with human preferences. Prevalent alignment techniques, such as DPO, PPO and their variants, align LLMs by changing the pre-trained model weights during a phase called post-training. While predominant, these post-training methods add substantial complexity before LLMs can be deployed. Inference-time alignment methods avoid the complex post-training step and instead bias the generation towards responses that are aligned with human preferences. The best-known inference-time alignment method, called Best-of-N, is as effective as the state-of-the-art post-training procedures. Unfortunately, Best-of-N requires vastly more resources at inference time than standard decoding strategies, which makes it computationally not viable. In this work, we introduce Speculative Rejection, a computationally-viable inference-time alignment algorithm. It generates high-scoring responses according to a given reward model, like Best-of-N does, while being between 16 to 32 times more computationally efficient.", 'score': 9, 'issue_id': 324, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': 'ca9ce39ec0390fc3', 'authors': ['Hanshi Sun', 'Momin Haider', 'Ruiqi Zhang', 'Huitao Yang', 'Jiahao Qiu', 'Ming Yin', 'Mengdi Wang', 'Peter Bartlett', 'Andrea Zanette'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Google DeepMind', 'Princeton University', 'UC Berkeley', 'University of Virginia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20290.jpg', 'data': {'categories': ['#inference', '#optimization', '#rlhf', '#alignment'], 'emoji': '🚀', 'ru': {'title': 'Эффективное выравнивание LLM без пост-обучения', 'desc': 'Статья представляет новый алгоритм выравнивания больших языковых моделей (LLM) во время вывода, называемый Speculative Rejection. Этот метод позволяет генерировать высококачественные ответы в соответствии с заданной моделью вознаграждения, аналогично методу Best-of-N. Однако Speculative Rejection в 16-32 раза более эффективен с точки зрения вычислительных ресурсов. Новый алгоритм устраняет необходимость в сложном этапе пост-обучения, делая процесс выравнивания LLM более простым и эффективным.'}, 'en': {'title': 'Efficient Alignment of Language Models at Inference Time', 'desc': "This paper discusses the importance of aligning Large Language Models (LLMs) with human preferences to ensure safe and effective deployment. Traditional alignment methods, such as DPO and PPO, require complex post-training adjustments to the model's weights, which can complicate deployment. In contrast, inference-time alignment methods, like Best-of-N, adjust responses during generation but are resource-intensive. The authors propose a new method called Speculative Rejection, which aligns LLMs efficiently at inference time, achieving similar performance to Best-of-N while being significantly more computationally efficient."}, 'zh': {'title': '高效对齐：投机拒绝算法的创新', 'desc': '本论文讨论了大型语言模型（LLMs）在安全有效部署中的对齐问题，确保模型的响应符合人类偏好。现有的对齐技术，如DPO和PPO，通常在后训练阶段通过改变预训练模型的权重来实现对齐，但这增加了复杂性。相较之下，推理时对齐方法避免了复杂的后训练步骤，直接偏向于生成符合人类偏好的响应。我们提出了一种名为“投机拒绝”的推理时对齐算法，其计算效率比现有的最佳选择方法高出16到32倍。'}}}, {'id': 'https://huggingface.co/papers/2410.21220', 'title': 'Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines', 'url': 'https://huggingface.co/papers/2410.21220', 'abstract': "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-language models (VLMs): if the model has not been exposed to the object depicted in an image, it struggles to generate reliable answers to the user's question regarding that image. Moreover, as new objects and events continuously emerge, frequently updating VLMs is impractical due to heavy computational burdens. To address this limitation, we propose Vision Search Assistant, a novel framework that facilitates collaboration between VLMs and web agents. This approach leverages VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web. By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system. Extensive experiments conducted on both open-set and closed-set QA benchmarks demonstrate that the Vision Search Assistant significantly outperforms the other models and can be widely applied to existing VLMs.", 'score': 8, 'issue_id': 321, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'bdb8b2a5fbb4c663', 'authors': ['Zhixin Zhang', 'Yiyuan Zhang', 'Xiaohan Ding', 'Xiangyu Yue'], 'affiliations': ['MMLab, CUHK', 'Shanghai AI Laboratory', 'Tencent'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21220.jpg', 'data': {'categories': ['#science', '#rag', '#reasoning', '#benchmark', '#cv', '#optimization', '#transfer_learning', '#games', '#agents', '#alignment'], 'emoji': '🔍', 'ru': {'title': 'Зрение ИИ: от незнания к пониманию через интернет', 'desc': 'В статье представлен Vision Search Assistant - новый подход к обработке визуальной информации. Он объединяет возможности больших визуально-языковых моделей (VLM) и веб-агентов для ответа на вопросы по незнакомым изображениям. Система использует генерацию с дополнением из интернета (Retrieval-Augmented Generation), чтобы получать актуальную информацию в реальном времени. Эксперименты показали значительное превосходство этого метода над существующими подходами.'}, 'en': {'title': 'Empowering Vision-Language Models with Real-Time Web Collaboration', 'desc': 'This paper introduces the Vision Search Assistant, a new framework that enhances the capabilities of vision-language models (VLMs) by enabling them to collaborate with web agents. Traditional VLMs struggle with unfamiliar visual content, especially when they encounter objects they have never seen before, leading to unreliable responses. The proposed framework allows VLMs to access real-time information from the web, facilitating open-world Retrieval-Augmented Generation. Experimental results show that the Vision Search Assistant significantly improves performance on both open-set and closed-set question-answering tasks, making it a valuable addition to existing VLMs.'}, 'zh': {'title': '视觉搜索助手：打破未知视觉内容的壁垒', 'desc': '本文提出了一种新的框架，称为视觉搜索助手（Vision Search Assistant），旨在解决传统视觉语言模型（VLMs）在处理未知视觉内容时的局限性。该框架通过结合VLMs的视觉理解能力和网络代理的实时信息访问，实现了开放世界的检索增强生成。这样，即使模型从未见过某个图像中的对象，也能提供准确的回答。实验结果表明，视觉搜索助手在开放集和封闭集的问答基准测试中显著优于其他模型，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2410.21264', 'title': 'LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior', 'url': 'https://huggingface.co/papers/2410.21264', 'abstract': "We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARP's strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs).", 'score': 8, 'issue_id': 321, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '20438897f3e9bc41', 'authors': ['Hanyu Wang', 'Saksham Suri', 'Yixuan Ren', 'Hao Chen', 'Abhinav Shrivastava'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21264.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#video', '#optimization', '#multimodal', '#games', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'LARP: Революционный подход к токенизации видео для генеративных моделей', 'desc': 'LARP - это новый токенизатор видео, разработанный для преодоления ограничений существующих методов токенизации в авторегрессионных генеративных моделях. В отличие от традиционных токенизаторов, LARP использует набор обученных холистических запросов для сбора информации из визуального контента, что позволяет захватывать более глобальные и семантические представления. LARP интегрирует облегченный AR-трансформер в качестве предварительной модели во время обучения, что оптимизирует латентное пространство для авторегрессионной генерации. Эксперименты показывают, что LARP достигает передовых результатов на бенчмарке UCF101 по условной генерации видео.'}, 'en': {'title': 'LARP: Revolutionizing Video Tokenization for Better Generative Models', 'desc': "LARP is a new video tokenizer that improves how videos are processed for autoregressive generative models. Instead of just breaking videos into small patches, LARP uses learned holistic queries to capture broader and more meaningful visual information. This method allows for flexible tokenization, adapting the number of tokens based on the task's needs. By integrating a lightweight autoregressive transformer during training, LARP optimizes the token space for better video generation, achieving top performance in benchmarks."}, 'zh': {'title': 'LARP：视频生成的新突破', 'desc': '本文介绍了一种新的视频标记器LARP，旨在克服当前自回归生成模型在视频标记方面的局限性。与传统的局部补丁标记器不同，LARP采用整体标记方案，通过学习的整体查询收集视觉内容的信息，从而捕捉更全球和语义化的表示。LARP支持任意数量的离散标记，能够根据任务的具体需求进行自适应和高效的标记。通过在训练过程中整合轻量级的自回归变换器，LARP优化了视频重建和自回归生成的潜在空间，确保在推理时实现更平滑和准确的生成。'}}}, {'id': 'https://huggingface.co/papers/2410.21271', 'title': 'EoRA: Training-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation', 'url': 'https://huggingface.co/papers/2410.21271', 'abstract': 'In this work, we re-formulate the model compression problem into the customized compensation problem: Given a compressed model, we aim to introduce residual low-rank paths to compensate for compression errors under customized requirements from users (e.g., tasks, compression ratios), resulting in greater flexibility in adjusting overall capacity without being constrained by specific compression formats. However, naively applying SVD to derive residual paths causes suboptimal utilization of the low-rank representation capacity. Instead, we propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method that directly minimizes compression-induced errors without requiring gradient-based training, achieving fast optimization in minutes using a small amount of calibration data. EoRA projects compression errors into the eigenspace of input activations, leveraging eigenvalues to effectively prioritize the reconstruction of high-importance error components. Moreover, EoRA can be seamlessly integrated with fine-tuning and quantization to further improve effectiveness and efficiency. EoRA consistently outperforms previous methods in compensating errors for compressed LLaMA2/3 models on various tasks, such as language generation, commonsense reasoning, and math reasoning tasks (e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and MathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4 sparsity). EoRA offers a scalable, training-free solution to compensate for compression errors, making it a powerful tool to deploy LLMs in various capacity and efficiency requirements.', 'score': 6, 'issue_id': 332, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '433d9a837f70f8da', 'authors': ['Shih-Yang Liu', 'Huck Yang', 'Chien-Yi Wang', 'Nai Chit Fung', 'Hongxu Yin', 'Charbel Sakr', 'Saurav Muralidharan', 'Kwang-Ting Cheng', 'Jan Kautz', 'Yu-Chiang Frank Wang', 'Pavlo Molchanov', 'Min-Hung Chen'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21271.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#inference', '#optimization', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'EoRA: эффективная компенсация ошибок сжатия языковых моделей без обучения', 'desc': 'В этой работе предлагается метод EoRA для компенсации ошибок сжатия языковых моделей. EoRA проецирует ошибки сжатия в собственное пространство входных активаций, используя собственные значения для эффективной реконструкции важных компонентов ошибок. Метод не требует обучения на градиентах и работает быстро на небольшом объеме калибровочных данных. EoRA превосходит предыдущие методы при компенсации ошибок сжатых моделей LLaMA2/3 на различных задачах, включая генерацию текста и рассуждения.'}, 'en': {'title': 'EoRA: Efficient Error Compensation for Compressed Models', 'desc': "This paper introduces a new approach to model compression called Training-free Eigenspace Low-Rank Approximation (EoRA). Instead of relying on traditional methods like Singular Value Decomposition (SVD), EoRA minimizes errors caused by compression without needing extensive training, making it faster and more efficient. It works by focusing on the most important components of the error, allowing for better reconstruction of the model's performance. EoRA has shown significant improvements in various tasks with compressed models, demonstrating its effectiveness in enhancing model deployment under different constraints."}, 'zh': {'title': '灵活的模型压缩补偿解决方案', 'desc': '本文将模型压缩问题重新定义为定制补偿问题：在给定压缩模型的情况下，我们旨在引入残差低秩路径，以补偿压缩误差，满足用户的定制需求（如任务、压缩比），从而在不受特定压缩格式限制的情况下，灵活调整整体容量。我们提出了一种无训练的特征空间低秩近似方法（EoRA），该方法直接最小化压缩引起的误差，无需基于梯度的训练，使用少量校准数据即可在几分钟内实现快速优化。EoRA将压缩误差投影到输入激活的特征空间中，利用特征值有效优先重建高重要性的误差成分。此外，EoRA可以与微调和量化无缝集成，进一步提高效果和效率。'}}}, {'id': 'https://huggingface.co/papers/2410.19100', 'title': 'VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks', 'url': 'https://huggingface.co/papers/2410.19100', 'abstract': 'Videos are often used to learn or extract the necessary information to complete tasks in ways different than what text and static imagery alone can provide. However, many existing agent benchmarks neglect long-context video understanding, instead focusing on text or static image inputs. To bridge this gap, we introduce VideoWebArena (VideoWA), a benchmark for evaluating the capabilities of long-context multimodal agents for video understanding. VideoWA consists of 2,021 web agent tasks based on manually crafted video tutorials, which total almost four hours of content. For our benchmark, we define a taxonomy of long-context video-based agent tasks with two main areas of focus: skill retention and factual retention. While skill retention tasks evaluate whether an agent can use a given human demonstration to complete a task efficiently, the factual retention task evaluates whether an agent can retrieve instruction-relevant information from a video to complete a task. We find that the best model achieves 13.3% success on factual retention tasks and 45.8% on factual retention QA pairs, far below human performance at 73.9% and 79.3%, respectively. On skill retention tasks, long-context models perform worse with tutorials than without, exhibiting a 5% performance decrease in WebArena tasks and a 10.3% decrease in VisualWebArena tasks. Our work highlights the need to improve the agentic abilities of long-context multimodal models and provides a testbed for future development with long-context video agents.', 'score': 6, 'issue_id': 330, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '82af04969ba93bff', 'authors': ['Lawrence Jang', 'Yinheng Li', 'Charles Ding', 'Justin Lin', 'Paul Pu Liang', 'Dan Zhao', 'Rogerio Bonatti', 'Kazuhito Koishida'], 'affiliations': ['Carnegie Mellon University', 'Massachusetts Institute of Technology', 'Microsoft', 'New York University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.19100.jpg', 'data': {'categories': ['#benchmark', '#video', '#multimodal', '#interpretability', '#games', '#agents', '#long_context'], 'emoji': '🎥', 'ru': {'title': 'VideoWebArena: новый рубеж в понимании видео для ИИ-агентов', 'desc': 'Статья представляет новый бенчмарк VideoWebArena (VideoWA) для оценки возможностей мультимодальных агентов с длинным контекстом в понимании видео. VideoWA включает 2,021 задачу веб-агента на основе видеоуроков, разделенных на две категории: сохранение навыков и сохранение фактов. Результаты показывают, что лучшая модель достигает лишь 13.3% успеха в задачах сохранения фактов, что значительно ниже человеческой производительности. Исследование подчеркивает необходимость улучшения способностей моделей с длинным контекстом в работе с видео.'}, 'en': {'title': 'Enhancing Long-Context Video Understanding for Agents', 'desc': 'This paper introduces VideoWebArena (VideoWA), a new benchmark designed to evaluate long-context multimodal agents specifically for video understanding. It consists of 2,021 tasks based on video tutorials, focusing on two main areas: skill retention and factual retention. The study reveals that current models struggle with these tasks, achieving significantly lower success rates compared to human performance. The findings emphasize the necessity for advancements in the capabilities of long-context video agents to enhance their effectiveness in real-world applications.'}, 'zh': {'title': '提升长上下文视频理解能力的基准测试', 'desc': '本论文介绍了VideoWebArena（VideoWA），这是一个用于评估长上下文多模态代理在视频理解能力的基准。VideoWA包含2021个基于手工制作视频教程的网络代理任务，总时长接近四小时。我们定义了长上下文视频代理任务的分类，主要关注技能保留和事实保留。研究发现，现有模型在事实保留任务上的成功率仅为13.3%，远低于人类的73.9%，这表明需要提升长上下文多模态模型的代理能力。'}}}, {'id': 'https://huggingface.co/papers/2410.20672', 'title': 'Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA', 'url': 'https://huggingface.co/papers/2410.20672', 'abstract': 'Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit "layer tying" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller "Recursive Transformers" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original "full-size" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.', 'score': 5, 'issue_id': 330, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'ca8b51b9816b76db', 'authors': ['Sangmin Bae', 'Adam Fisch', 'Hrayr Harutyunyan', 'Ziwei Ji', 'Seungyeon Kim', 'Tal Schuster'], 'affiliations': ['Google DeepMind', 'Google Research', 'KAIST AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20672.jpg', 'data': {'categories': ['#small_models', '#inference', '#optimization', '#training', '#transfer_learning', '#architecture'], 'emoji': '🔁', 'ru': {'title': 'Рекурсивные трансформеры: компактность без потери производительности', 'desc': 'Это исследование посвящено новому методу уменьшения размера и стоимости больших языковых моделей (LLM) с помощью рекурсивных трансформеров. Авторы предлагают эффективный способ инициализации рекурсивных трансформеров из стандартных предобученных моделей, используя один блок уникальных слоев, повторяемый несколько раз. Они также вводят концепцию релаксированных рекурсивных трансформеров, добавляя гибкость с помощью модулей LoRA. Результаты показывают, что рекурсивные модели превосходят как аналогичные по размеру стандартные модели, так и базовые линии дистилляции знаний.'}, 'en': {'title': 'Efficient LLMs through Recursive Transformers', 'desc': "This paper explores a method to reduce the size and cost of large language models (LLMs) by using parameter sharing through a technique called 'layer tying' in Transformers. The authors introduce 'Recursive Transformers', which utilize a single set of unique layers that are repeated, allowing for a more compact model with minimal performance loss. They enhance this approach with 'Relaxed Recursive Transformers' that incorporate low-rank adaptation modules, maintaining efficiency while improving performance. The results demonstrate that these recursive models outperform similarly sized models and can achieve performance close to larger models, while also introducing a new inference method that significantly boosts throughput."}, 'zh': {'title': '递归变换器：高效共享参数的创新之路', 'desc': '本文探讨了如何通过参数共享来减少大型语言模型（LLMs）的规模和成本。我们重新审视了在变换器中使用的“层绑定”技术，并提出了将现有LLMs转化为更小的“递归变换器”的新方法，这些变换器在层之间共享参数，且性能损失最小。我们还引入了放松递归变换器，通过深度低秩适应（LoRA）模块增加灵活性，同时保持模型的紧凑性。实验结果表明，我们的递归模型在性能上超越了同等规模的预训练模型，并能恢复大部分原始全尺寸模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2410.18481', 'title': 'Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction', 'url': 'https://huggingface.co/papers/2410.18481', 'abstract': 'Efficiently deriving structured workflows from unannotated dialogs remains an underexplored and formidable challenge in computational linguistics. Automating this process could significantly accelerate the manual design of workflows in new domains and enable the grounding of large language models in domain-specific flowcharts, enhancing transparency and controllability. In this paper, we introduce Dialog2Flow (D2F) embeddings, which differ from conventional sentence embeddings by mapping utterances to a latent space where they are grouped according to their communicative and informative functions (i.e., the actions they represent). D2F allows for modeling dialogs as continuous trajectories in a latent space with distinct action-related regions. By clustering D2F embeddings, the latent space is quantized, and dialogs can be converted into sequences of region/action IDs, facilitating the extraction of the underlying workflow. To pre-train D2F, we build a comprehensive dataset by unifying twenty task-oriented dialog datasets with normalized per-turn action annotations. We also introduce a novel soft contrastive loss that leverages the semantic information of these actions to guide the representation learning process, showing superior performance compared to standard supervised contrastive loss. Evaluation against various sentence embeddings, including dialog-specific ones, demonstrates that D2F yields superior qualitative and quantitative results across diverse domains.', 'score': 5, 'issue_id': 328, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '7ddeda6e7b60deaa', 'authors': ['Sergio Burdisso', 'Srikanth Madikeri', 'Petr Motlicek'], 'affiliations': ['Brno University of Technology, Brno, Czech Republic', 'Department of Computational Linguistics, University of Zurich, Zurich, Switzerland', 'Idiap Research Institute, Martigny, Switzerland'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18481.jpg', 'data': {'categories': ['#reasoning', '#multilingual', '#cv', '#optimization', '#data', '#training', '#dataset', '#transfer_learning'], 'emoji': '🗺️', 'ru': {'title': 'D2F: Автоматическое картирование диалогов в рабочие процессы', 'desc': 'Статья представляет новый подход к извлечению структурированных рабочих процессов из неаннотированных диалогов с помощью embeddings Dialog2Flow (D2F). D2F отображает высказывания в латентное пространство, группируя их по коммуникативным и информативным функциям. Авторы создали обширный датасет, объединив 20 наборов диалоговых данных с нормализованными аннотациями действий, и разработали новую функцию потерь soft contrastive loss. Эксперименты показали превосходство D2F над другими sentence embeddings в различных доменах.'}, 'en': {'title': 'Transforming Dialogs into Actionable Workflows with D2F Embeddings', 'desc': 'This paper addresses the challenge of creating structured workflows from unannotated dialogs in computational linguistics. It presents Dialog2Flow (D2F) embeddings, which uniquely map dialog utterances into a latent space based on their communicative functions. By clustering these embeddings, the authors can convert dialogs into sequences of action IDs, effectively extracting workflows. The study also introduces a new soft contrastive loss for better representation learning, showing that D2F outperforms existing sentence embeddings in various tasks.'}, 'zh': {'title': '从对话中提取工作流的创新方法', 'desc': '本文探讨了从未标注对话中高效提取结构化工作流的挑战。我们提出了Dialog2Flow (D2F) 嵌入，它通过将对话映射到潜在空间，按其交流和信息功能进行分组。D2F使得对话可以在潜在空间中建模为连续轨迹，并通过聚类D2F嵌入来量化潜在空间，从而提取底层工作流。我们还引入了一种新颖的软对比损失，利用动作的语义信息来指导表示学习过程，显示出优于传统监督对比损失的性能。'}}}, {'id': 'https://huggingface.co/papers/2410.20220', 'title': 'Neural Fields in Robotics: A Survey', 'url': 'https://huggingface.co/papers/2410.20220', 'abstract': "Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable rendering, Neural Fields encompass both continuous implicit and explicit neural representations enabling high-fidelity 3D reconstruction, integration of multi-modal sensor data, and generation of novel viewpoints. This survey explores their applications in robotics, emphasizing their potential to enhance perception, planning, and control. Their compactness, memory efficiency, and differentiability, along with seamless integration with foundation and generative models, make them ideal for real-time applications, improving robot adaptability and decision-making. This paper provides a thorough review of Neural Fields in robotics, categorizing applications across various domains and evaluating their strengths and limitations, based on over 200 papers. First, we present four key Neural Fields frameworks: Occupancy Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian Splatting. Second, we detail Neural Fields' applications in five major robotics domains: pose estimation, manipulation, navigation, physics, and autonomous driving, highlighting key works and discussing takeaways and open challenges. Finally, we outline the current limitations of Neural Fields in robotics and propose promising directions for future research. Project page: https://robonerf.github.io", 'score': 4, 'issue_id': 323, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '4666404a494d69c8', 'authors': ['Muhammad Zubair Irshad', 'Mauro Comi', 'Yen-Chen Lin', 'Nick Heppert', 'Abhinav Valada', 'Rares Ambrus', 'Zsolt Kira', 'Jonathan Tremblay'], 'affiliations': ['Georgia Institute of Technology', 'Nvidia', 'Toyota Research Institute', 'University of Bristol', 'University of Freiburg'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20220.jpg', 'data': {'categories': ['#science', '#cv', '#graphs', '#multimodal', '#robotics', '#3d', '#survey', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Нейронные поля: революция в 3D-восприятии роботов', 'desc': 'Нейронные поля - это новый подход к представлению 3D-сцен в компьютерном зрении и робототехнике. Они позволяют точно определять геометрию, 3D-семантику и динамику из 2D-данных с помощью дифференцируемого рендеринга. В статье рассматривается применение нейронных полей в робототехнике для улучшения восприятия, планирования и управления. Авторы анализируют более 200 работ, категоризируют применения в различных областях и оценивают сильные и слабые стороны этого подхода.'}, 'en': {'title': 'Revolutionizing Robotics with Neural Fields for 3D Understanding', 'desc': "Neural Fields are a new way to represent 3D scenes using data from 2D images, which helps in understanding the shape, meaning, and movement of objects in a scene. They use a technique called differentiable rendering to create detailed 3D models and can combine information from different types of sensors. This paper reviews how Neural Fields can improve robots' ability to see, plan, and act in their environments, making them more efficient and adaptable. It also discusses various frameworks and applications of Neural Fields in robotics, while identifying their current challenges and future research opportunities."}, 'zh': {'title': '神经场：提升机器人感知与决策的关键技术', 'desc': '神经场是一种新兴的3D场景表示方法，能够从2D数据中准确推断几何形状、3D语义和动态信息。通过可微渲染，神经场结合了连续隐式和显式神经表示，实现高保真度的3D重建和多模态传感器数据的整合。本文回顾了神经场在机器人领域的应用，强调其在感知、规划和控制方面的潜力。我们还讨论了神经场的局限性，并提出了未来研究的方向。'}}}, {'id': 'https://huggingface.co/papers/2406.10615', 'title': 'Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation', 'url': 'https://huggingface.co/papers/2406.10615', 'abstract': "Given the high cost of collecting robotic data in the real world, sample efficiency is a consistently compelling pursuit in robotics. In this paper, we introduce SGRv2, an imitation learning framework that enhances sample efficiency through improved visual and action representations. Central to the design of SGRv2 is the incorporation of a critical inductive bias-action locality, which posits that robot's actions are predominantly influenced by the target object and its interactions with the local environment. Extensive experiments in both simulated and real-world settings demonstrate that action locality is essential for boosting sample efficiency. SGRv2 excels in RLBench tasks with keyframe control using merely 5 demonstrations and surpasses the RVT baseline in 23 of 26 tasks. Furthermore, when evaluated on ManiSkill2 and MimicGen using dense control, SGRv2's success rate is 2.54 times that of SGR. In real-world environments, with only eight demonstrations, SGRv2 can perform a variety of tasks at a markedly higher success rate compared to baseline models. Project website: http://sgrv2-robot.github.io", 'score': 2, 'issue_id': 327, 'pub_date': '2024-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': 'cc1e4c7a45b1c9ab', 'authors': ['Tong Zhang', 'Yingdong Hu', 'Jiacheng You', 'Yang Gao'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Qi Zhi Institute', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2406.10615.jpg', 'data': {'categories': ['#synthetic', '#rl', '#benchmark', '#cv', '#optimization', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Локальность действий - ключ к эффективному обучению роботов', 'desc': 'SGRv2 - это фреймворк имитационного обучения для робототехники, повышающий эффективность использования данных. Он основан на концепции локальности действий, предполагающей, что действия робота в основном зависят от целевого объекта и его взаимодействия с ближайшим окружением. SGRv2 превосходит базовые модели в симулированных и реальных задачах, требуя всего 5-8 демонстраций. Эксперименты показывают, что локальность действий критически важна для повышения эффективности обучения.'}, 'en': {'title': 'Boosting Robotics with Sample-Efficient Imitation Learning', 'desc': "This paper presents SGRv2, an advanced imitation learning framework aimed at improving sample efficiency in robotics. It introduces a key concept called action locality, which suggests that a robot's actions are mainly determined by the target object and its local environment interactions. Through extensive testing in both simulated and real-world scenarios, the authors demonstrate that SGRv2 significantly outperforms existing models, achieving higher success rates with fewer demonstrations. The results indicate that SGRv2 is particularly effective in complex tasks, showcasing its potential for practical applications in robotics."}, 'zh': {'title': '提升样本效率的模仿学习新框架SGRv2', 'desc': '本论文介绍了一种名为SGRv2的模仿学习框架，旨在提高机器人数据收集的样本效率。SGRv2通过改进视觉和动作表示，结合了关键的归纳偏置——动作局部性，强调机器人动作主要受目标物体及其与环境的互动影响。实验结果表明，动作局部性对于提升样本效率至关重要，SGRv2在多项任务中表现优异，成功率显著高于基线模型。该框架在真实环境中仅需八个演示就能完成多种任务，展示了其强大的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2410.20636', 'title': 'Language Models And A Second Opinion Use Case: The Pocket Professional', 'url': 'https://huggingface.co/papers/2410.20636', 'abstract': "This research tests the role of Large Language Models (LLMs) as formal second opinion tools in professional decision-making, particularly focusing on complex medical cases where even experienced physicians seek peer consultation. The work analyzed 183 challenging medical cases from Medscape over a 20-month period, testing multiple LLMs' performance against crowd-sourced physician responses. A key finding was the high overall score possible in the latest foundational models (>80% accuracy compared to consensus opinion), which exceeds most human metrics reported on the same clinical cases (450 pages of patient profiles, test results). The study rates the LLMs' performance disparity between straightforward cases (>81% accuracy) and complex scenarios (43% accuracy), particularly in these cases generating substantial debate among human physicians. The research demonstrates that LLMs may be valuable as generators of comprehensive differential diagnoses rather than as primary diagnostic tools, potentially helping to counter cognitive biases in clinical decision-making, reduce cognitive loads, and thus remove some sources of medical error. The inclusion of a second comparative legal dataset (Supreme Court cases, N=21) provides added empirical context to the AI use to foster second opinions, though these legal challenges proved considerably easier for LLMs to analyze. In addition to the original contributions of empirical evidence for LLM accuracy, the research aggregated a novel benchmark for others to score highly contested question and answer reliability between both LLMs and disagreeing human practitioners. These results suggest that the optimal deployment of LLMs in professional settings may differ substantially from current approaches that emphasize automation of routine tasks.", 'score': 2, 'issue_id': 323, 'pub_date': '2024-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'b1a78251a22af319', 'authors': ['David Noever'], 'affiliations': ['PeopleTec, 4901-D Corporate Drive, Huntsville, AL, USA, 35805'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20636.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#multilingual', '#healthcare', '#dataset', '#alignment'], 'emoji': '🩺', 'ru': {'title': 'LLM как второе мнение: новый взгляд на медицинскую диагностику', 'desc': 'Исследование оценивает эффективность использования больших языковых моделей (LLM) в качестве формального инструмента для получения второго мнения в профессиональной медицинской практике. Анализ 183 сложных медицинских случаев показал, что современные LLM могут достигать точности более 80% по сравнению с консенсусным мнением врачей. Модели продемонстрировали высокую эффективность в простых случаях, но менее точны в сложных сценариях. Результаты указывают на потенциал LLM как инструмента для генерации дифференциальных диагнозов, способного помочь в борьбе с когнитивными искажениями и снижении когнитивной нагрузки врачей.'}, 'en': {'title': 'LLMs: Enhancing Medical Decision-Making with Second Opinions', 'desc': 'This research investigates how Large Language Models (LLMs) can serve as second opinion tools in complex medical decision-making. By analyzing 183 challenging medical cases, the study found that LLMs achieved over 80% accuracy, outperforming many human physicians. However, the models struggled with complex cases, showing only 43% accuracy, indicating their strength lies in generating differential diagnoses rather than making primary diagnoses. The findings suggest that LLMs could help reduce cognitive biases and errors in clinical settings, while also providing a new benchmark for evaluating AI performance against human practitioners.'}, 'zh': {'title': '大型语言模型：医疗决策中的第二意见助手', 'desc': '本研究探讨了大型语言模型（LLMs）在专业决策中的作用，特别是在复杂医疗案例中作为正式的第二意见工具。研究分析了183个具有挑战性的医疗案例，比较了多种LLMs的表现与医生的群体意见。结果显示，最新的基础模型在这些案例中的准确率超过80%，高于大多数人类医生的表现。研究表明，LLMs在生成全面的鉴别诊断方面可能更有价值，而不是作为主要的诊断工具，能够帮助减少临床决策中的认知偏差。'}}}, {'id': 'https://huggingface.co/papers/2410.01968', 'title': 'Bi-Level Motion Imitation for Humanoid Robots', 'url': 'https://huggingface.co/papers/2410.01968', 'abstract': 'Imitation learning from human motion capture (MoCap) data provides a promising way to train humanoid robots. However, due to differences in morphology, such as varying degrees of joint freedom and force limits, exact replication of human behaviors may not be feasible for humanoid robots. Consequently, incorporating physically infeasible MoCap data in training datasets can adversely affect the performance of the robot policy. To address this issue, we propose a bi-level optimization-based imitation learning framework that alternates between optimizing both the robot policy and the target MoCap data. Specifically, we first develop a generative latent dynamics model using a novel self-consistent auto-encoder, which learns sparse and structured motion representations while capturing desired motion patterns in the dataset. The dynamics model is then utilized to generate reference motions while the latent representation regularizes the bi-level motion imitation process. Simulations conducted with a realistic model of a humanoid robot demonstrate that our method enhances the robot policy by modifying reference motions to be physically consistent.', 'score': 1, 'issue_id': 331, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '61a874c3e9900dab', 'authors': ['Wenshuai Zhao', 'Yi Zhao', 'Joni Pajarinen', 'Michael Muehlebach'], 'affiliations': ['Aalto University, Finland', 'Max Planck Institute for Intelligent Systems, Germany'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.01968.jpg', 'data': {'categories': ['#rl', '#optimization', '#dataset', '#robotics', '#games', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Адаптивное имитационное обучение для реалистичных движений роботов', 'desc': 'Статья представляет новый подход к имитационному обучению гуманоидных роботов на основе данных захвата движения человека. Авторы предлагают двухуровневую оптимизацию, которая одновременно улучшает политику робота и целевые данные захвата движения. Ключевым элементом является генеративная модель латентной динамики, использующая самосогласованный автоэнкодер для получения разреженных и структурированных представлений движения. Симуляции показывают, что метод улучшает политику робота, модифицируя эталонные движения для физической согласованности.'}, 'en': {'title': 'Optimizing Robot Imitation with Physically Feasible Motion', 'desc': "This paper presents a new approach to train humanoid robots using imitation learning from human motion capture (MoCap) data. The challenge arises from the differences in robot and human body structures, which can make it difficult to directly replicate human movements. To solve this, the authors introduce a bi-level optimization framework that optimizes both the robot's movement policy and the MoCap data. By using a generative latent dynamics model, the framework ensures that the generated motions are physically feasible, improving the robot's ability to imitate human actions effectively."}, 'zh': {'title': '优化模仿学习，提升类人机器人表现', 'desc': '本论文提出了一种基于双层优化的模仿学习框架，用于训练类人机器人。该框架通过优化机器人策略和目标运动捕捉数据，解决了人类行为在机器人中无法精确复制的问题。我们开发了一种生成潜在动态模型，利用自一致性自编码器学习稀疏和结构化的运动表示。通过这种方法，生成的参考运动与潜在表示相结合，确保了机器人策略的物理一致性，从而提升了机器人的表现。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (3)', '#agi (1)', '#alignment (5)', '#architecture (11)', '#audio (1)', '#benchmark (10)', '#cv (8)', '#data (3)', '#dataset (6)', '#diffusion (4)', '#ethics (1)', '#games (5)', '#graphs (1)', '#hallucinations', '#healthcare (1)', '#inference (6)', '#interpretability (1)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation (1)', '#math (1)', '#multilingual (4)', '#multimodal (6)', '#open_source (5)', '#optimization (13)', '#plp', '#rag (1)', '#reasoning (5)', '#rl (3)', '#rlhf (2)', '#robotics (3)', '#science (4)', '#security (1)', '#small_models (5)', '#story_generation', '#survey (3)', '#synthetic (3)', '#training (9)', '#transfer_learning (4)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-29 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-29 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-29 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    