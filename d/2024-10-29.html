
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. October 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-28.html">â¬…ï¸ <span id="prev-date">28.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-30.html">â¡ï¸ <span id="next-date">30.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'};
        let feedDateNext = {'ru': '30.10', 'en': '10/30', 'zh': '10æœˆ30æ—¥'};
        let feedDatePrev = {'ru': '28.10', 'en': '10/28', 'zh': '10æœˆ28æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.21276', 'title': 'GPT-4o System Card', 'url': 'https://huggingface.co/papers/2410.21276', 'abstract': "GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.", 'score': 79, 'issue_id': 324, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 25', 'zh': '10æœˆ25æ—¥'}, 'hash': '15d57a2a0852d31e', 'authors': ['OpenAI', ':', 'Aaron Hurst', 'Adam Lerer', 'Adam P. Goucher', 'Adam Perelman', 'Aditya Ramesh', 'Aidan Clark', 'AJ Ostrow', 'Akila Welihinda', 'Alan Hayes', 'Alec Radford', 'Aleksander MÄ…dry', 'Alex Baker-Whitcomb', 'Alex Beutel', 'Alex Borzunov', 'Alex Carney', 'Alex Chow', 'Alex Kirillov', 'Alex Nichol', 'Alex Paino', 'Alex Renzin', 'Alex Tachard Passos', 'Alexander Kirillov', 'Alexi Christakis', 'Alexis Conneau', 'Ali Kamali', 'Allan Jabri', 'Allison Moyer', 'Allison Tam', 'Amadou Crookes', 'Amin Tootoochian', 'Amin Tootoonchian', 'Ananya Kumar', 'Andrea Vallone', 'Andrej Karpathy', 'Andrew Braunstein', 'Andrew Cann', 'Andrew Codispoti', 'Andrew Galu', 'Andrew Kondrich', 'Andrew Tulloch', 'Andrey Mishchenko', 'Angela Baek', 'Angela Jiang', 'Antoine Pelisse', 'Antonia Woodford', 'Anuj Gosalia', 'Arka Dhar', 'Ashley Pantuliano', 'Avi Nayak', 'Avital Oliver', 'Barret Zoph', 'Behrooz Ghorbani', 'Ben Leimberger', 'Ben Rossen', 'Ben Sokolowsky', 'Ben Wang', 'Benjamin Zweig', 'Beth Hoover', 'Blake Samic', 'Bob McGrew', 'Bobby Spero', 'Bogo Giertler', 'Bowen Cheng', 'Brad Lightcap', 'Brandon Walkin', 'Brendan Quinn', 'Brian Guarraci', 'Brian Hsu', 'Bright Kellogg', 'Brydon Eastman', 'Camillo Lugaresi', 'Carroll Wainwright', 'Cary Bassin', 'Cary Hudson', 'Casey Chu', 'Chad Nelson', 'Chak Li', 'Chan Jun Shern', 'Channing Conger', 'Charlotte Barette', 'Chelsea Voss', 'Chen Ding', 'Cheng Lu', 'Chong Zhang', 'Chris Beaumont', 'Chris Hallacy', 'Chris Koch', 'Christian Gibson', 'Christina Kim', 'Christine Choi', 'Christine McLeavey', 'Christopher Hesse', 'Claudia Fischer', 'Clemens Winter', 'Coley Czarnecki', 'Colin Jarvis', 'Colin Wei', 'Constantin Koumouzelis', 'Dane Sherburn', 'Daniel Kappler', 'Daniel Levin', 'Daniel Levy', 'David Carr', 'David Farhi', 'David Mely', 'David Robinson', 'David Sasaki', 'Denny Jin', 'Dev Valladares', 'Dimitris Tsipras', 'Doug Li', 'Duc Phong Nguyen', 'Duncan Findlay', 'Edede Oiwoh', 'Edmund Wong', 'Ehsan Asdar', 'Elizabeth Proehl', 'Elizabeth Yang', 'Eric Antonow', 'Eric Kramer', 'Eric Peterson', 'Eric Sigler', 'Eric Wallace', 'Eugene Brevdo', 'Evan Mays', 'Farzad Khorasani', 'Felipe Petroski Such', 'Filippo Raso', 'Francis Zhang', 'Fred von Lohmann', 'Freddie Sulit', 'Gabriel Goh', 'Gene Oden', 'Geoff Salmon', 'Giulio Starace', 'Greg Brockman', 'Hadi Salman', 'Haiming Bao', 'Haitang Hu', 'Hannah Wong', 'Haoyu Wang', 'Heather Schmidt', 'Heather Whitney', 'Heewoo Jun', 'Hendrik Kirchner', 'Henrique Ponde de Oliveira Pinto', 'Hongyu Ren', 'Huiwen Chang', 'Hyung Won Chung', 'Ian Kivlichan', "Ian O'Connell", "Ian O'Connell", 'Ian Osband', 'Ian Silber', 'Ian Sohl', 'Ibrahim Okuyucu', 'Ikai Lan', 'Ilya Kostrikov', 'Ilya Sutskever', 'Ingmar Kanitscheider', 'Ishaan Gulrajani', 'Jacob Coxon', 'Jacob Menick', 'Jakub Pachocki', 'James Aung', 'James Betker', 'James Crooks', 'James Lennon', 'Jamie Kiros', 'Jan Leike', 'Jane Park', 'Jason Kwon', 'Jason Phang', 'Jason Teplitz', 'Jason Wei', 'Jason Wolfe', 'Jay Chen', 'Jeff Harris', 'Jenia Varavva', 'Jessica Gan Lee', 'Jessica Shieh', 'Ji Lin', 'Jiahui Yu', 'Jiayi Weng', 'Jie Tang', 'Jieqi Yu', 'Joanne Jang', 'Joaquin Quinonero Candela', 'Joe Beutler', 'Joe Landers', 'Joel Parish', 'Johannes Heidecke', 'John Schulman', 'Jonathan Lachman', 'Jonathan McKay', 'Jonathan Uesato', 'Jonathan Ward', 'Jong Wook Kim', 'Joost Huizinga', 'Jordan Sitkin', 'Jos Kraaijeveld', 'Josh Gross', 'Josh Kaplan', 'Josh Snyder', 'Joshua Achiam', 'Joy Jiao', 'Joyce Lee', 'Juntang Zhuang', 'Justyn Harriman', 'Kai Fricke', 'Kai Hayashi', 'Karan Singhal', 'Katy Shi', 'Kavin Karthik', 'Kayla Wood', 'Kendra Rimbach', 'Kenny Hsu', 'Kenny Nguyen', 'Keren Gu-Lemberg', 'Kevin Button', 'Kevin Liu', 'Kiel Howe', 'Krithika Muthukumar', 'Kyle Luther', 'Lama Ahmad', 'Larry Kai', 'Lauren Itow', 'Lauren Workman', 'Leher Pathak', 'Leo Chen', 'Li Jing', 'Lia Guy', 'Liam Fedus', 'Liang Zhou', 'Lien Mamitsuka', 'Lilian Weng', 'Lindsay McCallum', 'Lindsey Held', 'Long Ouyang', 'Louis Feuvrier', 'Lu Zhang', 'Lukas Kondraciuk', 'Lukasz Kaiser', 'Luke Hewitt', 'Luke Metz', 'Lyric Doshi', 'Mada Aflak', 'Maddie Simens', 'Madelaine Boyd', 'Madeleine Thompson', 'Marat Dukhan', 'Mark Chen', 'Mark Gray', 'Mark Hudnall', 'Marvin Zhang', 'Marwan Aljubeh', 'Mateusz Litwin', 'Matthew Zeng', 'Max Johnson', 'Maya Shetty', 'Mayank Gupta', 'Meghan Shah', 'Mehmet Yatbaz', 'Meng Jia Yang', 'Mengchao Zhong', 'Mia Glaese', 'Mianna Chen', 'Michael Janner', 'Michael Lampe', 'Michael Petrov', 'Michael Wu', 'Michele Wang', 'Michelle Fradin', 'Michelle Pokrass', 'Miguel Castro', 'Miguel Oom Temudo de Castro', 'Mikhail Pavlov', 'Miles Brundage', 'Miles Wang', 'Minal Khan', 'Mira Murati', 'Mo Bavarian', 'Molly Lin', 'Murat Yesildal', 'Nacho Soto', 'Natalia Gimelshein', 'Natalie Cone', 'Natalie Staudacher', 'Natalie Summers', 'Natan LaFontaine', 'Neil Chowdhury', 'Nick Ryder', 'Nick Stathas', 'Nick Turley', 'Nik Tezak', 'Niko Felix', 'Nithanth Kudige', 'Nitish Keskar', 'Noah Deutsch', 'Noel Bundick', 'Nora Puckett', 'Ofir Nachum', 'Ola Okelola', 'Oleg Boiko', 'Oleg Murk', 'Oliver Jaffe', 'Olivia Watkins', 'Olivier Godement', 'Owen Campbell-Moore', 'Patrick Chao', 'Paul McMillan', 'Pavel Belov', 'Peng Su', 'Peter Bak', 'Peter Bakkum', 'Peter Deng', 'Peter Dolan', 'Peter Hoeschele', 'Peter Welinder', 'Phil Tillet', 'Philip Pronin', 'Philippe Tillet', 'Prafulla Dhariwal', 'Qiming Yuan', 'Rachel Dias', 'Rachel Lim', 'Rahul Arora', 'Rajan Troll', 'Randall Lin', 'Rapha Gontijo Lopes', 'Raul Puri', 'Reah Miyara', 'Reimar Leike', 'Renaud Gaubert', 'Reza Zamani', 'Ricky Wang', 'Rob Donnelly', 'Rob Honsby', 'Rocky Smith', 'Rohan Sahai', 'Rohit Ramchandani', 'Romain Huet', 'Rory Carmichael', 'Rowan Zellers', 'Roy Chen', 'Ruby Chen', 'Ruslan Nigmatullin', 'Ryan Cheu', 'Saachi Jain', 'Sam Altman', 'Sam Schoenholz', 'Sam Toizer', 'Samuel Miserendino', 'Sandhini Agarwal', 'Sara Culver', 'Scott Ethersmith', 'Scott Gray', 'Sean Grove', 'Sean Metzger', 'Shamez Hermani', 'Shantanu Jain', 'Shengjia Zhao', 'Sherwin Wu', 'Shino Jomoto', 'Shirong Wu', 'Shuaiqi', 'Xia', 'Sonia Phene', 'Spencer Papay', 'Srinivas Narayanan', 'Steve Coffey', 'Steve Lee', 'Stewart Hall', 'Suchir Balaji', 'Tal Broda', 'Tal Stramer', 'Tao Xu', 'Tarun Gogineni', 'Taya Christianson', 'Ted Sanders', 'Tejal Patwardhan', 'Thomas Cunninghman', 'Thomas Degry', 'Thomas Dimson', 'Thomas Raoux', 'Thomas Shadwell', 'Tianhao Zheng', 'Todd Underwood', 'Todor Markov', 'Toki Sherbakov', 'Tom Rubin', 'Tom Stasi', 'Tomer Kaftan', 'Tristan Heywood', 'Troy Peterson', 'Tyce Walters', 'Tyna Eloundou', 'Valerie Qi', 'Veit Moeller', 'Vinnie Monaco', 'Vishal Kuo', 'Vlad Fomenko', 'Wayne Chang', 'Weiyi Zheng', 'Wenda Zhou', 'Wesam Manassra', 'Will Sheu', 'Wojciech Zaremba', 'Yash Patil', 'Yilei Qian', 'Yongjik Kim', 'Youlong Cheng', 'Yu Zhang', 'Yuchen He', 'Yuchen Zhang', 'Yujia Jin', 'Yunxing Dai', 'Yury Malkov'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21276.jpg', 'data': {'categories': ['#synthetic', '#multilingual', '#cv', '#video', '#multimodal', '#ethics', '#training', '#open_source', '#audio', '#security', '#architecture', '#alignment'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'GPT-4o: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜', 'desc': 'GPT-4o - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚, Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° ÑĞºĞ²Ğ¾Ğ·Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ğ²Ğ¸Ğ´Ñ‹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ° Ğ½Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ñ…Ğ¾Ğ´Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. GPT-4o Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ½ĞµĞ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'GPT-4o: The All-in-One AI for Text, Audio, and Vision', 'desc': 'GPT-4o is a versatile autoregressive model that can process and generate various types of media, including text, audio, images, and video. It utilizes an end-to-end training approach, allowing it to handle all input and output modalities through a single neural network. The model demonstrates rapid response times comparable to human conversation and shows enhanced performance in understanding non-English text, audio, and visual data. Additionally, GPT-4o prioritizes safety and alignment, providing a comprehensive System Card that outlines its capabilities, limitations, and societal implications.'}, 'zh': {'title': 'å…¨èƒ½æ¨¡å‹ï¼Œå¿«é€Ÿå“åº”ï¼Œå®‰å…¨å¯é ', 'desc': 'GPT-4oæ˜¯ä¸€ç§è‡ªå›å½’çš„å…¨èƒ½æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘çš„ä»»æ„ç»„åˆï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„è¾“å‡ºã€‚å®ƒé€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„ç¥ç»ç½‘ç»œè¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œç¡®ä¿æ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºçš„é«˜æ•ˆå¤„ç†ã€‚GPT-4oåœ¨å“åº”éŸ³é¢‘è¾“å…¥æ—¶çš„é€Ÿåº¦ä¸äººç±»å¯¹è¯ç›¸ä¼¼ï¼Œä¸”åœ¨éè‹±è¯­æ–‡æœ¬å¤„ç†ä¸Šæœ‰æ˜¾è‘—æå‡ï¼ŒåŒæ—¶åœ¨è§†è§‰å’ŒéŸ³é¢‘ç†è§£æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹çš„ç³»ç»Ÿå¡ç‰‡è¯¦ç»†ä»‹ç»äº†å…¶èƒ½åŠ›ã€å±€é™æ€§å’Œå®‰å…¨è¯„ä¼°ï¼Œç¡®ä¿å…¶åœ¨ç¤¾ä¼šå½±å“å’Œæ½œåœ¨å±é™©èƒ½åŠ›æ–¹é¢çš„é€æ˜æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.18565', 'title': 'Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation', 'url': 'https://huggingface.co/papers/2410.18565', 'abstract': 'We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for Polish language processing. Trained on curated Polish corpora, this model addresses key challenges in language model development through innovative techniques. These include Weighted Instruction Cross-Entropy Loss, which balances the learning of different instruction types, and Adaptive Learning Rate, which dynamically adjusts the learning rate based on training progress. To evaluate performance, we created the Open PL LLM Leaderboard and Polish MT-Bench, novel frameworks assessing various NLP tasks and conversational abilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9 percentage point increase in average score compared to Mistral-7B-v0.1 on the RAG Reader task. It also excels in the Polish MT-Bench, particularly in Reasoning (6.15/10) and Role-playing (7.83/10) categories. This model represents a substantial advancement in Polish language AI, offering a powerful tool for diverse linguistic applications and setting new benchmarks in the field.', 'score': 42, 'issue_id': 325, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 24', 'zh': '10æœˆ24æ—¥'}, 'hash': 'c68d615cedcd651e', 'authors': ['Krzysztof Ociepa', 'Åukasz Flis', 'Krzysztof WrÃ³bel', 'Adrian GwoÅºdziej', 'Remigiusz Kinas'], 'affiliations': ['ACK Cyfronet AGH', 'Azurro', 'Enelpol', 'Jagiellonian University', 'SpeakLeash'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18565.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#benchmark', '#multilingual', '#optimization', '#training', '#dataset', '#transfer_learning', '#open_source', '#machine_translation'], 'emoji': 'ğŸ‡µğŸ‡±', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°: Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ 7B-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Bielik', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Bielik 7B v0.1 - ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½ÑƒÑ ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Open PL LLM Leaderboard Ğ¸ Polish MT-Bench, Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Bielik 7B v0.1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ²Ñ‹Ñ… Ğ¸Ğ³Ñ€.'}, 'en': {'title': 'Bielik 7B: Advancing Polish Language AI with 7 Billion Parameters', 'desc': "Bielik 7B v0.1 is a generative text model designed specifically for processing the Polish language, featuring 7 billion parameters. It employs advanced techniques like Weighted Instruction Cross-Entropy Loss to enhance learning across various instruction types and an Adaptive Learning Rate that adjusts based on the model's training progress. The model's performance is evaluated using new frameworks such as the Open PL LLM Leaderboard and Polish MT-Bench, which assess its capabilities in natural language processing tasks. Bielik 7B v0.1 shows significant improvements over previous models, particularly in reasoning and role-playing tasks, marking a major step forward in Polish language AI applications."}, 'zh': {'title': 'Bielik 7B v0.1ï¼šæ³¢å…°è¯­å¤„ç†çš„æ–°æ ‡æ†', 'desc': 'Bielik 7B v0.1 æ˜¯ä¸€ä¸ªç”¨äºæ³¢å…°è¯­å¤„ç†çš„ç”Ÿæˆæ–‡æœ¬æ¨¡å‹ï¼Œæ‹¥æœ‰70äº¿ä¸ªå‚æ•°ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ›æ–°æŠ€æœ¯è§£å†³äº†è¯­è¨€æ¨¡å‹å¼€å‘ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŠ æƒæŒ‡ä»¤äº¤å‰ç†µæŸå¤±å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†å¼€æ”¾çš„PL LLMæ’è¡Œæ¦œå’Œæ³¢å…°MT-Benchï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡å’Œå¯¹è¯èƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚Bielik 7B v0.1 åœ¨RAG Readerä»»åŠ¡ä¸Šæ¯”Mistral-7B-v0.1æé«˜äº†9ä¸ªç™¾åˆ†ç‚¹ï¼Œç‰¹åˆ«åœ¨æ¨ç†å’Œè§’è‰²æ‰®æ¼”ç±»åˆ«ä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.20011', 'title': 'A Survey of Small Language Models', 'url': 'https://huggingface.co/papers/2410.20011', 'abstract': 'Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device, mobile, edge devices, among many others. In this article, we present a comprehensive survey on SLMs, focusing on their architectures, training techniques, and model compression techniques. We propose a novel taxonomy for categorizing the methods used to optimize SLMs, including model compression, pruning, and quantization techniques. We summarize the benchmark datasets that are useful for benchmarking SLMs along with the evaluation metrics commonly used. Additionally, we highlight key open challenges that remain to be addressed. Our survey aims to serve as a valuable resource for researchers and practitioners interested in developing and deploying small yet efficient language models.', 'score': 38, 'issue_id': 321, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 25', 'zh': '10æœˆ25æ—¥'}, 'hash': 'bde2fa0e4317a316', 'authors': ['Chien Van Nguyen', 'Xuan Shen', 'Ryan Aponte', 'Yu Xia', 'Samyadeep Basu', 'Zhengmian Hu', 'Jian Chen', 'Mihir Parmar', 'Sasidhar Kunapuli', 'Joe Barrow', 'Junda Wu', 'Ashish Singh', 'Yu Wang', 'Jiuxiang Gu', 'Franck Dernoncourt', 'Nesreen K. Ahmed', 'Nedim Lipka', 'Ruiyi Zhang', 'Xiang Chen', 'Tong Yu', 'Sungchul Kim', 'Hanieh Deilamsalehy', 'Namyong Park', 'Mike Rimer', 'Zhehao Zhang', 'Huanrui Yang', 'Ryan A. Rossi', 'Thien Huu Nguyen'], 'affiliations': ['Adobe Research', 'Arizona State University', 'Carnegie Mellon University', 'Dartmouth College', 'Intel AI Research', 'Meta AI', 'Northeastern University', 'State University of New York at Buffalo', 'University of Arizona', 'University of California, San Diego', 'University of Maryland, College Park', 'University of Massachusetts Amherst', 'University of Oregon'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20011.jpg', 'data': {'categories': ['#small_models', '#benchmark', '#inference', '#optimization', '#training', '#survey', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°Ğ»Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (SLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ²ÑĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ SLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³ Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ğ° SLM Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾ÑĞ²ĞµÑ‰Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ½ĞµÑ€ĞµÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Optimizing Small Language Models for Efficiency and Performance', 'desc': 'This paper provides a detailed overview of Small Language Models (SLMs), which are designed to perform language tasks efficiently with low computational requirements. It introduces a new classification system for the various optimization methods used in SLMs, such as model compression, pruning, and quantization. The authors also compile important benchmark datasets and evaluation metrics that are essential for assessing the performance of SLMs. Furthermore, the paper discusses ongoing challenges in the field, aiming to assist researchers and practitioners in advancing the development of efficient language models.'}, 'zh': {'title': 'å°å‹è¯­è¨€æ¨¡å‹ï¼šé«˜æ•ˆè¯­è¨€å¤„ç†çš„æœªæ¥', 'desc': 'å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å› å…¶é«˜æ•ˆæ€§å’Œæ€§èƒ½è€Œå˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹æ‰§è¡Œå„ç§è¯­è¨€ä»»åŠ¡ï¼Œéå¸¸é€‚åˆåœ¨è®¾å¤‡ã€ç§»åŠ¨å’Œè¾¹ç¼˜è®¾å¤‡ç­‰ç¯å¢ƒä¸­ä½¿ç”¨ã€‚æœ¬æ–‡å¯¹SLMsè¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œé‡ç‚¹ä»‹ç»äº†å®ƒä»¬çš„æ¶æ„ã€è®­ç»ƒæŠ€æœ¯å’Œæ¨¡å‹å‹ç¼©æŠ€æœ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»æ³•ï¼Œç”¨äºå¯¹ä¼˜åŒ–SLMsçš„æ–¹æ³•è¿›è¡Œåˆ†ç±»ï¼ŒåŒ…æ‹¬æ¨¡å‹å‹ç¼©ã€å‰ªæå’Œé‡åŒ–æŠ€æœ¯ã€‚æˆ‘ä»¬æ€»ç»“äº†å¯¹SLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•çš„æœ‰ç”¨æ•°æ®é›†ä»¥åŠå¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶å¼ºè°ƒäº†ä»éœ€è§£å†³çš„å…³é”®å¼€æ”¾æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.18603', 'title': 'AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant', 'url': 'https://huggingface.co/papers/2410.18603', 'abstract': "Digital agents capable of automating complex computer tasks have attracted considerable attention due to their immense potential to enhance human-computer interaction. However, existing agent methods exhibit deficiencies in their generalization and specialization capabilities, especially in handling open-ended computer tasks in real-world environments. Inspired by the rich functionality of the App store, we present AgentStore, a scalable platform designed to dynamically integrate heterogeneous agents for automating computer tasks. AgentStore empowers users to integrate third-party agents, allowing the system to continuously enrich its capabilities and adapt to rapidly evolving operating systems. Additionally, we propose a novel core MetaAgent with the AgentToken strategy to efficiently manage diverse agents and utilize their specialized and generalist abilities for both domain-specific and system-wide tasks. Extensive experiments on three challenging benchmarks demonstrate that AgentStore surpasses the limitations of previous systems with narrow capabilities, particularly achieving a significant improvement from 11.21\\% to 23.85\\% on the OSWorld benchmark, more than doubling the previous results. Comprehensive quantitative and qualitative results further demonstrate AgentStore's ability to enhance agent systems in both generalization and specialization, underscoring its potential for developing the specialized generalist computer assistant. All our codes will be made publicly available in https://chengyou-jia.github.io/AgentStore-Home.", 'score': 30, 'issue_id': 324, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 24', 'zh': '10æœˆ24æ—¥'}, 'hash': '9d62252fde1756e0', 'authors': ['Chengyou Jia', 'Minnan Luo', 'Zhuohang Dang', 'Qiushi Sun', 'Fangzhi Xu', 'Junlin Hu', 'Tianbao Xie', 'Zhiyong Wu'], 'affiliations': ['Shanghai AI Lab', 'The University of Hong Kong', 'Xian Jiaotong University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18603.jpg', 'data': {'categories': ['#benchmark', '#agi', '#open_source', '#agents', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'AgentStore: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'AgentStore - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑÑŒ Ğº Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼ÑÑ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ»ĞµĞ¶Ğ¸Ñ‚ MetaAgent Ñ AgentToken ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OSWorld.'}, 'en': {'title': 'AgentStore: Empowering Dynamic Integration of Digital Agents for Enhanced Automation', 'desc': 'This paper introduces AgentStore, a platform that allows for the integration of various digital agents to automate complex computer tasks. It addresses the limitations of existing agent systems in generalization and specialization, particularly in real-world applications. The core innovation is the MetaAgent with the AgentToken strategy, which efficiently manages diverse agents to leverage their strengths for both specific and broad tasks. Experimental results show that AgentStore significantly improves performance on benchmarks, demonstrating its effectiveness in enhancing human-computer interaction.'}, 'zh': {'title': 'AgentStoreï¼šæå‡è®¡ç®—æœºä»»åŠ¡è‡ªåŠ¨åŒ–çš„æ™ºèƒ½ä»£ç†å¹³å°', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAgentStoreçš„å¹³å°ï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€é›†æˆå¼‚æ„ä»£ç†æ¥è‡ªåŠ¨åŒ–å¤æ‚çš„è®¡ç®—æœºä»»åŠ¡ã€‚ç°æœ‰çš„ä»£ç†æ–¹æ³•åœ¨å¤„ç†å¼€æ”¾å¼è®¡ç®—æœºä»»åŠ¡æ—¶å­˜åœ¨æ³›åŒ–å’Œä¸“ä¸šåŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚AgentStoreå…è®¸ç”¨æˆ·é›†æˆç¬¬ä¸‰æ–¹ä»£ç†ï¼Œä»è€Œä¸æ–­ä¸°å¯Œç³»ç»Ÿçš„åŠŸèƒ½ï¼Œé€‚åº”å¿«é€Ÿå˜åŒ–çš„æ“ä½œç³»ç»Ÿã€‚é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼ŒAgentStoreåœ¨æ³›åŒ–å’Œä¸“ä¸šåŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†ç³»ç»Ÿçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.21169', 'title': 'Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction', 'url': 'https://huggingface.co/papers/2410.21169', 'abstract': 'Document parsing is essential for converting unstructured and semi-structured documents-such as contracts, academic papers, and invoices-into structured, machine-readable data. Document parsing extract reliable structured data from unstructured inputs, providing huge convenience for numerous applications. Especially with recent achievements in Large Language Models, document parsing plays an indispensable role in both knowledge base construction and training data generation. This survey presents a comprehensive review of the current state of document parsing, covering key methodologies, from modular pipeline systems to end-to-end models driven by large vision-language models. Core components such as layout detection, content extraction (including text, tables, and mathematical expressions), and multi-modal data integration are examined in detail. Additionally, this paper discusses the challenges faced by modular document parsing systems and vision-language models in handling complex layouts, integrating multiple modules, and recognizing high-density text. It emphasizes the importance of developing larger and more diverse datasets and outlines future research directions.', 'score': 29, 'issue_id': 325, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'b5541dbb322ac8c4', 'authors': ['Qintong Zhang', 'Victor Shea-Jay Huang', 'Bin Wang', 'Junyuan Zhang', 'Zhengren Wang', 'Hao Liang', 'Shawn Wang', 'Matthieu Lin', 'Conghui He', 'Wentao Zhang'], 'affiliations': ['Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21169.jpg', 'data': {'categories': ['#science', '#cv', '#multimodal', '#data', '#dataset', '#survey', '#architecture'], 'emoji': 'ğŸ“„', 'ru': {'title': 'ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ¾ end-to-end Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ĞºĞµÑ‚Ğ°, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Transforming Documents: From Chaos to Clarity with Parsing', 'desc': 'This paper reviews the process of document parsing, which transforms unstructured documents into structured data that machines can understand. It highlights the advancements made possible by Large Language Models, which enhance the efficiency of knowledge base construction and training data generation. The survey covers various methodologies, including modular pipeline systems and end-to-end models, focusing on essential tasks like layout detection and content extraction. It also addresses the challenges in parsing complex layouts and the need for larger datasets to improve model performance.'}, 'zh': {'title': 'æ–‡æ¡£è§£æï¼šä»éç»“æ„åˆ°ç»“æ„çš„å…³é”®æŠ€æœ¯', 'desc': 'æ–‡æ¡£è§£ææ˜¯å°†éç»“æ„åŒ–å’ŒåŠç»“æ„åŒ–æ–‡æ¡£ï¼ˆå¦‚åˆåŒã€å­¦æœ¯è®ºæ–‡å’Œå‘ç¥¨ï¼‰è½¬æ¢ä¸ºç»“æ„åŒ–ã€æœºå™¨å¯è¯»æ•°æ®çš„é‡è¦è¿‡ç¨‹ã€‚é€šè¿‡æ–‡æ¡£è§£æï¼Œå¯ä»¥ä»éç»“æ„åŒ–è¾“å…¥ä¸­æå–å¯é çš„ç»“æ„åŒ–æ•°æ®ï¼Œä¸ºè®¸å¤šåº”ç”¨æä¾›æå¤§çš„ä¾¿åˆ©ã€‚å°¤å…¶æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å–å¾—çš„æœ€æ–°è¿›å±•ä¸‹ï¼Œæ–‡æ¡£è§£æåœ¨çŸ¥è¯†åº“æ„å»ºå’Œè®­ç»ƒæ•°æ®ç”Ÿæˆä¸­å‘æŒ¥ç€ä¸å¯æˆ–ç¼ºçš„ä½œç”¨ã€‚æœ¬æ–‡ç»¼è¿°äº†æ–‡æ¡£è§£æçš„ç°çŠ¶ï¼Œæ¶µç›–äº†ä»æ¨¡å—åŒ–ç®¡é“ç³»ç»Ÿåˆ°ç”±å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç«¯åˆ°ç«¯æ¨¡å‹çš„å…³é”®æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.20280', 'title': 'MarDini: Masked Autoregressive Diffusion for Video Generation at Scale', 'url': 'https://huggingface.co/papers/2410.20280', 'abstract': "We introduce MarDini, a new family of video diffusion models that integrate the advantages of masked auto-regression (MAR) into a unified diffusion model (DM) framework. Here, MAR handles temporal planning, while DM focuses on spatial generation in an asymmetric network design: i) a MAR-based planning model containing most of the parameters generates planning signals for each masked frame using low-resolution input; ii) a lightweight generation model uses these signals to produce high-resolution frames via diffusion de-noising. MarDini's MAR enables video generation conditioned on any number of masked frames at any frame positions: a single model can handle video interpolation (e.g., masking middle frames), image-to-video generation (e.g., masking from the second frame onward), and video expansion (e.g., masking half the frames). The efficient design allocates most of the computational resources to the low-resolution planning model, making computationally expensive but important spatio-temporal attention feasible at scale. MarDini sets a new state-of-the-art for video interpolation; meanwhile, within few inference steps, it efficiently generates videos on par with those of much more expensive advanced image-to-video models.", 'score': 21, 'issue_id': 322, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': '33b4a79a3995aa46', 'authors': ['Haozhe Liu', 'Shikun Liu', 'Zijian Zhou', 'Mengmeng Xu', 'Yanping Xie', 'Xiao Han', 'Juan C. PÃ©rez', 'Ding Liu', 'Kumara Kahatapitiya', 'Menglin Jia', 'Jui-Chieh Wu', 'Sen He', 'Tao Xiang', 'JÃ¼rgen Schmidhuber', 'Juan-Manuel PÃ©rez-RÃºa'], 'affiliations': ['KAUST', 'Meta AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20280.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#inference', '#video', '#optimization', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'MarDini: Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸ĞµĞ¹ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸ĞµĞ¹', 'desc': 'MarDini - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ (MAR) Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (DM). MAR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° DM - Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ ÑĞµÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MAR Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'MarDini: Revolutionizing Video Generation with Efficient Diffusion Models', 'desc': 'MarDini is a novel family of video diffusion models that combines masked auto-regression (MAR) with a unified diffusion model (DM) framework. The MAR component is responsible for planning the sequence of frames, while the DM focuses on generating high-quality spatial outputs. This model can handle various tasks such as video interpolation, image-to-video generation, and video expansion by conditioning on different masked frames. By optimizing the computational resources, MarDini achieves state-of-the-art performance in video interpolation and efficiently generates videos comparable to more complex models.'}, 'zh': {'title': 'MarDiniï¼šè§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'MarDiniæ˜¯ä¸€ç§æ–°çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å®¶æ—ï¼Œå®ƒå°†æ©è”½è‡ªå›å½’ï¼ˆMARï¼‰çš„ä¼˜åŠ¿ä¸ç»Ÿä¸€çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰æ¡†æ¶ç»“åˆåœ¨ä¸€èµ·ã€‚MARè´Ÿè´£æ—¶é—´è§„åˆ’ï¼Œè€ŒDMåˆ™ä¸“æ³¨äºç©ºé—´ç”Ÿæˆï¼Œé‡‡ç”¨ä¸å¯¹ç§°ç½‘ç»œè®¾è®¡ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä»»æ„æ•°é‡çš„æ©è”½å¸§ç”Ÿæˆè§†é¢‘ï¼Œæ”¯æŒè§†é¢‘æ’å€¼ã€å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆå’Œè§†é¢‘æ‰©å±•ç­‰å¤šç§ä»»åŠ¡ã€‚MarDiniåœ¨è§†é¢‘æ’å€¼æ–¹é¢è®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œå¹¶ä¸”åœ¨å°‘é‡æ¨ç†æ­¥éª¤å†…é«˜æ•ˆç”Ÿæˆä¸æ›´æ˜‚è´µçš„å›¾åƒåˆ°è§†é¢‘æ¨¡å‹ç›¸å½“çš„è§†é¢‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.19313', 'title': 'COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training', 'url': 'https://huggingface.co/papers/2410.19313', 'abstract': "FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54x compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43x end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine's speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT.", 'score': 18, 'issue_id': 323, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 25', 'zh': '10æœˆ25æ—¥'}, 'hash': '9c2301bebb36909b', 'authors': ['Haocheng Xi', 'Han Cai', 'Ligeng Zhu', 'Yao Lu', 'Kurt Keutzer', 'Jianfei Chen', 'Song Han'], 'affiliations': ['MIT', 'NVIDIA', 'Tsinghua University', 'University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.19313.jpg', 'data': {'categories': ['#small_models', '#inference', '#optimization', '#training', '#open_source'], 'emoji': 'ğŸš€', 'ru': {'title': 'COAT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'COAT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ FP8, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸: Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½ÑƒÑ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ†Ğ¸Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ COAT ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² 1,54 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ BF16, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² 1,43 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ BF16, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'COAT: Revolutionizing FP8 Training for Large Models', 'desc': 'This paper presents COAT, a new framework for FP8 training that enhances memory efficiency during the training of large machine learning models. COAT introduces two main innovations: Dynamic Range Expansion to minimize quantization errors in optimizer states, and Mixed-Granularity Activation Quantization to optimize memory usage for activations. The framework achieves a 1.54x reduction in memory footprint and a 1.43x speedup in training compared to traditional BF16 methods, while maintaining high performance across various tasks. By enabling efficient training on fewer GPUs and allowing larger batch sizes, COAT provides a scalable solution for large-scale model training.'}, 'zh': {'title': 'COATï¼šé«˜æ•ˆçš„FP8è®­ç»ƒå†…å­˜ä¼˜åŒ–æ–¹æ¡ˆ', 'desc': 'FP8è®­ç»ƒæ˜¯ä¸€ç§æé«˜è®­ç»ƒæ•ˆç‡çš„æ–°æ–¹æ³•ã€‚ç°æœ‰æ¡†æ¶åœ¨ä½¿ç”¨FP8è®¡ç®—æ—¶ï¼Œä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¿€æ´»ä»ä¿æŒé«˜ç²¾åº¦ï¼Œæœªèƒ½å……åˆ†ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€‚æœ¬æ–‡æå‡ºäº†COATæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€èŒƒå›´æ‰©å±•å’Œæ··åˆç²’åº¦æ¿€æ´»é‡åŒ–ï¼Œæ˜¾è‘—å‡å°‘å¤§æ¨¡å‹è®­ç»ƒçš„å†…å­˜å ç”¨ã€‚å®éªŒè¡¨æ˜ï¼ŒCOATåœ¨å¤šç§ä»»åŠ¡ä¸­å®ç°äº†æ¥è¿‘æ— æŸçš„æ€§èƒ½ï¼ŒåŒæ—¶å†…å­˜å ç”¨å‡å°‘äº†1.54å€ï¼Œè®­ç»ƒé€Ÿåº¦æå‡äº†1.43å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.18666', 'title': 'DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation', 'url': 'https://huggingface.co/papers/2410.18666', 'abstract': "Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model. GenIR, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models. GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation & filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction. The result is a large-scale dataset of one million high-quality images. Our second contribution, DreamClear, is a DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address. Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration. Code and pre-trained models will be available at: https://github.com/shallowdream204/DreamClear.", 'score': 18, 'issue_id': 322, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 24', 'zh': '10æœˆ24æ—¥'}, 'hash': 'ebdde1eca3a0b04c', 'authors': ['Yuang Ai', 'Xiaoqiang Zhou', 'Huaibo Huang', 'Xiaotian Han', 'Zhengyu Chen', 'Quanzeng You', 'Hongxia Yang'], 'affiliations': ['ByteDance, Inc', 'MAIS & NLPR, Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18666.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#multimodal', '#data', '#dataset', '#open_source', '#architecture'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ GenIR - Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ¸ DreamClear - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Diffusion Transformer. GenIR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸Ğ· Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. DreamClear Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Image Restoration with GenIR and DreamClear', 'desc': 'This paper addresses the challenges of image restoration (IR) in real-world applications by introducing two key innovations: GenIR and DreamClear. GenIR is a data curation pipeline that creates a large-scale dataset of one million high-quality images through a dual-prompt learning approach, enhancing the generalizability of models. DreamClear is a Diffusion Transformer-based model that leverages generative priors from text-to-image diffusion models and integrates adaptive modulation to handle various image degradations effectively. The results demonstrate that this dual strategy significantly improves the performance of image restoration tasks in practical scenarios.'}, 'zh': {'title': 'åŒé‡ç­–ç•¥æå‡å›¾åƒæ¢å¤èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ç°å®åœºæ™¯ä¸­å›¾åƒæ¢å¤ï¼ˆIRï¼‰é—®é¢˜çš„åŒé‡ç­–ç•¥ï¼ŒåŒ…æ‹¬GenIRæ•°æ®ç­–åˆ’ç®¡é“å’ŒåŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰çš„DreamClearå›¾åƒæ¢å¤æ¨¡å‹ã€‚GenIRé€šè¿‡æ„å»ºå›¾åƒ-æ–‡æœ¬å¯¹ã€åŒæç¤ºå¾®è°ƒå’Œæ•°æ®ç”Ÿæˆä¸è¿‡æ»¤ï¼Œå…‹æœäº†ç°æœ‰æ•°æ®é›†çš„å±€é™æ€§ï¼Œæä¾›äº†ä¸€ä¸ªåŒ…å«ä¸€ç™¾ä¸‡é«˜è´¨é‡å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚DreamClearæ¨¡å‹åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒå’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå®ç°äº†é€¼çœŸçš„å›¾åƒæ¢å¤ã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”è°ƒåˆ¶æ··åˆå™¨ï¼ˆMoAMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤ŸåŠ¨æ€æ•´åˆå¤šç§æ¢å¤ä¸“å®¶ï¼Œå¢å¼ºäº†å¯¹å„ç§ç°å®ä¸–ç•Œé€€åŒ–çš„é€‚åº”èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.21252', 'title': 'LongReward: Improving Long-context Large Language Models with AI Feedback', 'url': 'https://huggingface.co/papers/2410.21252', 'abstract': "Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinforcement learning (RL) with appropriate reward signals can further enhance models' capacities. However, how to obtain reliable rewards in long-context scenarios remains unexplored. To this end, we propose LongReward, a novel method that utilizes an off-the-shelf LLM to provide rewards for long-context model responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, each with a carefully designed assessment pipeline. By combining LongReward and offline RL algorithm DPO, we are able to effectively improve long-context SFT models. Our experiments indicate that LongReward not only significantly improves models' long-context performance but also enhances their ability to follow short instructions. We also find that long-context DPO with LongReward and conventional short-context DPO can be used together without hurting either one's performance.", 'score': 16, 'issue_id': 321, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '0ff5d39896cdfbbe', 'authors': ['Jiajie Zhang', 'Zhongni Hou', 'Xin Lv', 'Shulin Cao', 'Zhenyu Hou', 'Yilin Niu', 'Lei Hou', 'Yuxiao Dong', 'Ling Feng', 'Juanzi Li'], 'affiliations': ['Tsinghua University', 'University of Chinese Academy of Sciences', 'Zhipu AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21252.jpg', 'data': {'categories': ['#rl', '#rlhf', '#optimization', '#training', '#long_context', '#alignment'], 'emoji': 'ğŸ“', 'ru': {'title': 'LongReward: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ LongReward Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ²ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼: Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ°. LongReward Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ DPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Long-Context Performance with LongReward', 'desc': 'This paper addresses the challenges faced by long-context large language models (LLMs) in generating high-quality data for supervised fine-tuning (SFT). It introduces LongReward, a method that leverages an existing LLM to provide rewards based on four key dimensions: helpfulness, logicality, faithfulness, and completeness. By integrating LongReward with the offline reinforcement learning algorithm DPO, the authors demonstrate significant improvements in the long-context performance of SFT models. The findings suggest that LongReward enhances both long-context and short instruction-following capabilities without compromising performance across different contexts.'}, 'zh': {'title': 'æå‡é•¿ä¸Šä¸‹æ–‡æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLongRewardçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚é€šè¿‡åˆ©ç”¨ç°æˆçš„LLMï¼Œä»å››ä¸ªç»´åº¦ï¼ˆæœ‰ç”¨æ€§ã€é€»è¾‘æ€§ã€å¯ä¿¡æ€§å’Œå®Œæ•´æ€§ï¼‰ä¸ºé•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„å“åº”æä¾›å¥–åŠ±ä¿¡å·ã€‚ç»“åˆLongRewardå’Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•DPOï¼Œæˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆæå‡é•¿ä¸Šä¸‹æ–‡çš„ç›‘ç£å¾®è°ƒæ¨¡å‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLongRewardä¸ä»…æ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ï¼Œè¿˜å¢å¼ºäº†å…¶æ‰§è¡ŒçŸ­æŒ‡ä»¤çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.20474', 'title': 'GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation', 'url': 'https://huggingface.co/papers/2410.20474', 'abstract': 'We introduce a novel training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT). Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation. However, prior training-free approaches often rely on updating the noisy image during the reverse diffusion process via backpropagation from custom loss functions, which frequently struggle to provide precise control over individual bounding boxes. In this work, we leverage the flexibility of the Transformer architecture, demonstrating that DiT can generate noisy patches corresponding to each bounding box, fully encoding the target object and allowing for fine-grained control over each region. Our approach builds on an intriguing property of DiT, which we refer to as semantic sharing. Due to semantic sharing, when a smaller patch is jointly denoised alongside a generatable-size image, the two become "semantic clones". Each patch is denoised in its own branch of the generation process and then transplanted into the corresponding region of the original noisy image at each timestep, resulting in robust spatial grounding for each bounding box. In our experiments on the HRS and DrawBench benchmarks, we achieve state-of-the-art performance compared to previous training-free spatial grounding approaches.', 'score': 13, 'issue_id': 323, 'pub_date': '2024-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '354e97e3a32e2063', 'authors': ['Phillip Y. Lee', 'Taehoon Yoon', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20474.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#training', '#games', '#architecture'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Diffusion Transformers (DiT) Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¼Ñƒ Ğ¿Ñ€ÑĞ¼Ğ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºÑƒ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ DiT, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¼ 'ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¾Ğ¼', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ 'ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ»Ğ¾Ğ½Ñ‹' Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ‚Ñ‡Ğ° Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… HRS Ğ¸ DrawBench Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'Empowering Image Generation with Training-Free Spatial Grounding!', 'desc': "This paper presents a new method for spatial grounding in text-to-image generation using Diffusion Transformers (DiT) without the need for prior training. The technique allows for better user control by generating specific image patches that correspond to defined bounding boxes. Unlike previous methods that struggled with precise control, this approach utilizes the unique property of semantic sharing in DiT, enabling the generation of 'semantic clones' for each patch. The results show that this method outperforms existing training-free techniques on benchmark datasets, achieving state-of-the-art performance."}, 'zh': {'title': 'æ— è®­ç»ƒç©ºé—´å®šä½ï¼Œç²¾ç»†æ§åˆ¶å›¾åƒç”Ÿæˆ', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— è®­ç»ƒç©ºé—´å®šä½æŠ€æœ¯ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œé‡‡ç”¨æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰ã€‚è¿™ç§ç©ºé—´å®šä½æ–¹æ³•é€šè¿‡è¾¹ç•Œæ¡†å®ç°ï¼Œå› å…¶ç®€å•æ€§å’Œå¤šåŠŸèƒ½æ€§è€Œå—åˆ°å…³æ³¨ï¼Œå¢å¼ºäº†ç”¨æˆ·åœ¨å›¾åƒç”Ÿæˆä¸­çš„æ§åˆ¶èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ©ç”¨å˜æ¢å™¨æ¶æ„çš„çµæ´»æ€§ï¼Œå±•ç¤ºäº†DiTèƒ½å¤Ÿç”Ÿæˆä¸æ¯ä¸ªè¾¹ç•Œæ¡†å¯¹åº”çš„å™ªå£°è¡¥ä¸ï¼Œä»è€Œå®ç°å¯¹æ¯ä¸ªåŒºåŸŸçš„ç²¾ç»†æ§åˆ¶ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸ä¹‹å‰çš„æ— è®­ç»ƒç©ºé—´å®šä½æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨HRSå’ŒDrawBenchåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.20290', 'title': 'Fast Best-of-N Decoding via Speculative Rejection', 'url': 'https://huggingface.co/papers/2410.20290', 'abstract': "The safe and effective deployment of Large Language Models (LLMs) involves a critical step called alignment, which ensures that the model's responses are in accordance with human preferences. Prevalent alignment techniques, such as DPO, PPO and their variants, align LLMs by changing the pre-trained model weights during a phase called post-training. While predominant, these post-training methods add substantial complexity before LLMs can be deployed. Inference-time alignment methods avoid the complex post-training step and instead bias the generation towards responses that are aligned with human preferences. The best-known inference-time alignment method, called Best-of-N, is as effective as the state-of-the-art post-training procedures. Unfortunately, Best-of-N requires vastly more resources at inference time than standard decoding strategies, which makes it computationally not viable. In this work, we introduce Speculative Rejection, a computationally-viable inference-time alignment algorithm. It generates high-scoring responses according to a given reward model, like Best-of-N does, while being between 16 to 32 times more computationally efficient.", 'score': 9, 'issue_id': 324, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': 'ca9ce39ec0390fc3', 'authors': ['Hanshi Sun', 'Momin Haider', 'Ruiqi Zhang', 'Huitao Yang', 'Jiahao Qiu', 'Ming Yin', 'Mengdi Wang', 'Peter Bartlett', 'Andrea Zanette'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Google DeepMind', 'Princeton University', 'UC Berkeley', 'University of Virginia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20290.jpg', 'data': {'categories': ['#inference', '#optimization', '#rlhf', '#alignment'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Speculative Rejection. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ Best-of-N. ĞĞ´Ğ½Ğ°ĞºĞ¾ Speculative Rejection Ğ² 16-32 Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ´ĞµĞ»Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼.'}, 'en': {'title': 'Efficient Alignment of Language Models at Inference Time', 'desc': "This paper discusses the importance of aligning Large Language Models (LLMs) with human preferences to ensure safe and effective deployment. Traditional alignment methods, such as DPO and PPO, require complex post-training adjustments to the model's weights, which can complicate deployment. In contrast, inference-time alignment methods, like Best-of-N, adjust responses during generation but are resource-intensive. The authors propose a new method called Speculative Rejection, which aligns LLMs efficiently at inference time, achieving similar performance to Best-of-N while being significantly more computationally efficient."}, 'zh': {'title': 'é«˜æ•ˆå¯¹é½ï¼šæŠ•æœºæ‹’ç»ç®—æ³•çš„åˆ›æ–°', 'desc': 'æœ¬è®ºæ–‡è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®‰å…¨æœ‰æ•ˆéƒ¨ç½²ä¸­çš„å¯¹é½é—®é¢˜ï¼Œç¡®ä¿æ¨¡å‹çš„å“åº”ç¬¦åˆäººç±»åå¥½ã€‚ç°æœ‰çš„å¯¹é½æŠ€æœ¯ï¼Œå¦‚DPOå’ŒPPOï¼Œé€šå¸¸åœ¨åè®­ç»ƒé˜¶æ®µé€šè¿‡æ”¹å˜é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡æ¥å®ç°å¯¹é½ï¼Œä½†è¿™å¢åŠ äº†å¤æ‚æ€§ã€‚ç›¸è¾ƒä¹‹ä¸‹ï¼Œæ¨ç†æ—¶å¯¹é½æ–¹æ³•é¿å…äº†å¤æ‚çš„åè®­ç»ƒæ­¥éª¤ï¼Œç›´æ¥åå‘äºç”Ÿæˆç¬¦åˆäººç±»åå¥½çš„å“åº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œæŠ•æœºæ‹’ç»â€çš„æ¨ç†æ—¶å¯¹é½ç®—æ³•ï¼Œå…¶è®¡ç®—æ•ˆç‡æ¯”ç°æœ‰çš„æœ€ä½³é€‰æ‹©æ–¹æ³•é«˜å‡º16åˆ°32å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.21220', 'title': 'Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines', 'url': 'https://huggingface.co/papers/2410.21220', 'abstract': "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-language models (VLMs): if the model has not been exposed to the object depicted in an image, it struggles to generate reliable answers to the user's question regarding that image. Moreover, as new objects and events continuously emerge, frequently updating VLMs is impractical due to heavy computational burdens. To address this limitation, we propose Vision Search Assistant, a novel framework that facilitates collaboration between VLMs and web agents. This approach leverages VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web. By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system. Extensive experiments conducted on both open-set and closed-set QA benchmarks demonstrate that the Vision Search Assistant significantly outperforms the other models and can be widely applied to existing VLMs.", 'score': 8, 'issue_id': 321, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'bdb8b2a5fbb4c663', 'authors': ['Zhixin Zhang', 'Yiyuan Zhang', 'Xiaohan Ding', 'Xiangyu Yue'], 'affiliations': ['MMLab, CUHK', 'Shanghai AI Laboratory', 'Tencent'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21220.jpg', 'data': {'categories': ['#science', '#rag', '#reasoning', '#benchmark', '#cv', '#optimization', '#transfer_learning', '#games', '#agents', '#alignment'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ—Ñ€ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜: Ğ¾Ñ‚ Ğ½ĞµĞ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Vision Search Assistant - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ½ĞµĞ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° (Retrieval-Augmented Generation), Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering Vision-Language Models with Real-Time Web Collaboration', 'desc': 'This paper introduces the Vision Search Assistant, a new framework that enhances the capabilities of vision-language models (VLMs) by enabling them to collaborate with web agents. Traditional VLMs struggle with unfamiliar visual content, especially when they encounter objects they have never seen before, leading to unreliable responses. The proposed framework allows VLMs to access real-time information from the web, facilitating open-world Retrieval-Augmented Generation. Experimental results show that the Vision Search Assistant significantly improves performance on both open-set and closed-set question-answering tasks, making it a valuable addition to existing VLMs.'}, 'zh': {'title': 'è§†è§‰æœç´¢åŠ©æ‰‹ï¼šæ‰“ç ´æœªçŸ¥è§†è§‰å†…å®¹çš„å£å’', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºè§†è§‰æœç´¢åŠ©æ‰‹ï¼ˆVision Search Assistantï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†æœªçŸ¥è§†è§‰å†…å®¹æ—¶çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆVLMsçš„è§†è§‰ç†è§£èƒ½åŠ›å’Œç½‘ç»œä»£ç†çš„å®æ—¶ä¿¡æ¯è®¿é—®ï¼Œå®ç°äº†å¼€æ”¾ä¸–ç•Œçš„æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚è¿™æ ·ï¼Œå³ä½¿æ¨¡å‹ä»æœªè§è¿‡æŸä¸ªå›¾åƒä¸­çš„å¯¹è±¡ï¼Œä¹Ÿèƒ½æä¾›å‡†ç¡®çš„å›ç­”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§†è§‰æœç´¢åŠ©æ‰‹åœ¨å¼€æ”¾é›†å’Œå°é—­é›†çš„é—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.21264', 'title': 'LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior', 'url': 'https://huggingface.co/papers/2410.21264', 'abstract': "We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARP's strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs).", 'score': 8, 'issue_id': 321, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '20438897f3e9bc41', 'authors': ['Hanyu Wang', 'Saksham Suri', 'Yixuan Ren', 'Hao Chen', 'Abhinav Shrivastava'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21264.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#video', '#optimization', '#multimodal', '#games', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'LARP: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LARP - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², LARP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ…Ğ¾Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. LARP Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ AR-Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LARP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ UCF101 Ğ¿Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'LARP: Revolutionizing Video Tokenization for Better Generative Models', 'desc': "LARP is a new video tokenizer that improves how videos are processed for autoregressive generative models. Instead of just breaking videos into small patches, LARP uses learned holistic queries to capture broader and more meaningful visual information. This method allows for flexible tokenization, adapting the number of tokens based on the task's needs. By integrating a lightweight autoregressive transformer during training, LARP optimizes the token space for better video generation, achieving top performance in benchmarks."}, 'zh': {'title': 'LARPï¼šè§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘æ ‡è®°å™¨LARPï¼Œæ—¨åœ¨å…‹æœå½“å‰è‡ªå›å½’ç”Ÿæˆæ¨¡å‹åœ¨è§†é¢‘æ ‡è®°æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ä¼ ç»Ÿçš„å±€éƒ¨è¡¥ä¸æ ‡è®°å™¨ä¸åŒï¼ŒLARPé‡‡ç”¨æ•´ä½“æ ‡è®°æ–¹æ¡ˆï¼Œé€šè¿‡å­¦ä¹ çš„æ•´ä½“æŸ¥è¯¢æ”¶é›†è§†è§‰å†…å®¹çš„ä¿¡æ¯ï¼Œä»è€Œæ•æ‰æ›´å…¨çƒå’Œè¯­ä¹‰åŒ–çš„è¡¨ç¤ºã€‚LARPæ”¯æŒä»»æ„æ•°é‡çš„ç¦»æ•£æ ‡è®°ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡çš„å…·ä½“éœ€æ±‚è¿›è¡Œè‡ªé€‚åº”å’Œé«˜æ•ˆçš„æ ‡è®°ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ•´åˆè½»é‡çº§çš„è‡ªå›å½’å˜æ¢å™¨ï¼ŒLARPä¼˜åŒ–äº†è§†é¢‘é‡å»ºå’Œè‡ªå›å½’ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ï¼Œç¡®ä¿åœ¨æ¨ç†æ—¶å®ç°æ›´å¹³æ»‘å’Œå‡†ç¡®çš„ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.21271', 'title': 'EoRA: Training-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation', 'url': 'https://huggingface.co/papers/2410.21271', 'abstract': 'In this work, we re-formulate the model compression problem into the customized compensation problem: Given a compressed model, we aim to introduce residual low-rank paths to compensate for compression errors under customized requirements from users (e.g., tasks, compression ratios), resulting in greater flexibility in adjusting overall capacity without being constrained by specific compression formats. However, naively applying SVD to derive residual paths causes suboptimal utilization of the low-rank representation capacity. Instead, we propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method that directly minimizes compression-induced errors without requiring gradient-based training, achieving fast optimization in minutes using a small amount of calibration data. EoRA projects compression errors into the eigenspace of input activations, leveraging eigenvalues to effectively prioritize the reconstruction of high-importance error components. Moreover, EoRA can be seamlessly integrated with fine-tuning and quantization to further improve effectiveness and efficiency. EoRA consistently outperforms previous methods in compensating errors for compressed LLaMA2/3 models on various tasks, such as language generation, commonsense reasoning, and math reasoning tasks (e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and MathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4 sparsity). EoRA offers a scalable, training-free solution to compensate for compression errors, making it a powerful tool to deploy LLMs in various capacity and efficiency requirements.', 'score': 6, 'issue_id': 332, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '433d9a837f70f8da', 'authors': ['Shih-Yang Liu', 'Huck Yang', 'Chien-Yi Wang', 'Nai Chit Fung', 'Hongxu Yin', 'Charbel Sakr', 'Saurav Muralidharan', 'Kwang-Ting Cheng', 'Jan Kautz', 'Yu-Chiang Frank Wang', 'Pavlo Molchanov', 'Min-Hung Chen'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21271.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#inference', '#optimization', '#math', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'EoRA: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ EoRA Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. EoRA Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ² ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. EoRA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ LLaMA2/3 Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'EoRA: Efficient Error Compensation for Compressed Models', 'desc': "This paper introduces a new approach to model compression called Training-free Eigenspace Low-Rank Approximation (EoRA). Instead of relying on traditional methods like Singular Value Decomposition (SVD), EoRA minimizes errors caused by compression without needing extensive training, making it faster and more efficient. It works by focusing on the most important components of the error, allowing for better reconstruction of the model's performance. EoRA has shown significant improvements in various tasks with compressed models, demonstrating its effectiveness in enhancing model deployment under different constraints."}, 'zh': {'title': 'çµæ´»çš„æ¨¡å‹å‹ç¼©è¡¥å¿è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡å°†æ¨¡å‹å‹ç¼©é—®é¢˜é‡æ–°å®šä¹‰ä¸ºå®šåˆ¶è¡¥å¿é—®é¢˜ï¼šåœ¨ç»™å®šå‹ç¼©æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ—¨åœ¨å¼•å…¥æ®‹å·®ä½ç§©è·¯å¾„ï¼Œä»¥è¡¥å¿å‹ç¼©è¯¯å·®ï¼Œæ»¡è¶³ç”¨æˆ·çš„å®šåˆ¶éœ€æ±‚ï¼ˆå¦‚ä»»åŠ¡ã€å‹ç¼©æ¯”ï¼‰ï¼Œä»è€Œåœ¨ä¸å—ç‰¹å®šå‹ç¼©æ ¼å¼é™åˆ¶çš„æƒ…å†µä¸‹ï¼Œçµæ´»è°ƒæ•´æ•´ä½“å®¹é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„ç‰¹å¾ç©ºé—´ä½ç§©è¿‘ä¼¼æ–¹æ³•ï¼ˆEoRAï¼‰ï¼Œè¯¥æ–¹æ³•ç›´æ¥æœ€å°åŒ–å‹ç¼©å¼•èµ·çš„è¯¯å·®ï¼Œæ— éœ€åŸºäºæ¢¯åº¦çš„è®­ç»ƒï¼Œä½¿ç”¨å°‘é‡æ ¡å‡†æ•°æ®å³å¯åœ¨å‡ åˆ†é’Ÿå†…å®ç°å¿«é€Ÿä¼˜åŒ–ã€‚EoRAå°†å‹ç¼©è¯¯å·®æŠ•å½±åˆ°è¾“å…¥æ¿€æ´»çš„ç‰¹å¾ç©ºé—´ä¸­ï¼Œåˆ©ç”¨ç‰¹å¾å€¼æœ‰æ•ˆä¼˜å…ˆé‡å»ºé«˜é‡è¦æ€§çš„è¯¯å·®æˆåˆ†ã€‚æ­¤å¤–ï¼ŒEoRAå¯ä»¥ä¸å¾®è°ƒå’Œé‡åŒ–æ— ç¼é›†æˆï¼Œè¿›ä¸€æ­¥æé«˜æ•ˆæœå’Œæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.19100', 'title': 'VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks', 'url': 'https://huggingface.co/papers/2410.19100', 'abstract': 'Videos are often used to learn or extract the necessary information to complete tasks in ways different than what text and static imagery alone can provide. However, many existing agent benchmarks neglect long-context video understanding, instead focusing on text or static image inputs. To bridge this gap, we introduce VideoWebArena (VideoWA), a benchmark for evaluating the capabilities of long-context multimodal agents for video understanding. VideoWA consists of 2,021 web agent tasks based on manually crafted video tutorials, which total almost four hours of content. For our benchmark, we define a taxonomy of long-context video-based agent tasks with two main areas of focus: skill retention and factual retention. While skill retention tasks evaluate whether an agent can use a given human demonstration to complete a task efficiently, the factual retention task evaluates whether an agent can retrieve instruction-relevant information from a video to complete a task. We find that the best model achieves 13.3% success on factual retention tasks and 45.8% on factual retention QA pairs, far below human performance at 73.9% and 79.3%, respectively. On skill retention tasks, long-context models perform worse with tutorials than without, exhibiting a 5% performance decrease in WebArena tasks and a 10.3% decrease in VisualWebArena tasks. Our work highlights the need to improve the agentic abilities of long-context multimodal models and provides a testbed for future development with long-context video agents.', 'score': 6, 'issue_id': 330, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 24', 'zh': '10æœˆ24æ—¥'}, 'hash': '82af04969ba93bff', 'authors': ['Lawrence Jang', 'Yinheng Li', 'Charles Ding', 'Justin Lin', 'Paul Pu Liang', 'Dan Zhao', 'Rogerio Bonatti', 'Kazuhito Koishida'], 'affiliations': ['Carnegie Mellon University', 'Massachusetts Institute of Technology', 'Microsoft', 'New York University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.19100.jpg', 'data': {'categories': ['#benchmark', '#video', '#multimodal', '#interpretability', '#games', '#agents', '#long_context'], 'emoji': 'ğŸ¥', 'ru': {'title': 'VideoWebArena: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VideoWebArena (VideoWA) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. VideoWA Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2,021 Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ÑƒÑ€Ğ¾ĞºĞ¾Ğ², Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ²Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ 13.3% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ¶Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Long-Context Video Understanding for Agents', 'desc': 'This paper introduces VideoWebArena (VideoWA), a new benchmark designed to evaluate long-context multimodal agents specifically for video understanding. It consists of 2,021 tasks based on video tutorials, focusing on two main areas: skill retention and factual retention. The study reveals that current models struggle with these tasks, achieving significantly lower success rates compared to human performance. The findings emphasize the necessity for advancements in the capabilities of long-context video agents to enhance their effectiveness in real-world applications.'}, 'zh': {'title': 'æå‡é•¿ä¸Šä¸‹æ–‡è§†é¢‘ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†VideoWebArenaï¼ˆVideoWAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ä»£ç†åœ¨è§†é¢‘ç†è§£èƒ½åŠ›çš„åŸºå‡†ã€‚VideoWAåŒ…å«2021ä¸ªåŸºäºæ‰‹å·¥åˆ¶ä½œè§†é¢‘æ•™ç¨‹çš„ç½‘ç»œä»£ç†ä»»åŠ¡ï¼Œæ€»æ—¶é•¿æ¥è¿‘å››å°æ—¶ã€‚æˆ‘ä»¬å®šä¹‰äº†é•¿ä¸Šä¸‹æ–‡è§†é¢‘ä»£ç†ä»»åŠ¡çš„åˆ†ç±»ï¼Œä¸»è¦å…³æ³¨æŠ€èƒ½ä¿ç•™å’Œäº‹å®ä¿ç•™ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨äº‹å®ä¿ç•™ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ä»…ä¸º13.3%ï¼Œè¿œä½äºäººç±»çš„73.9%ï¼Œè¿™è¡¨æ˜éœ€è¦æå‡é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€æ¨¡å‹çš„ä»£ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.20672', 'title': 'Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA', 'url': 'https://huggingface.co/papers/2410.20672', 'abstract': 'Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit "layer tying" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller "Recursive Transformers" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original "full-size" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.', 'score': 5, 'issue_id': 330, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'ca8b51b9816b76db', 'authors': ['Sangmin Bae', 'Adam Fisch', 'Hrayr Harutyunyan', 'Ziwei Ji', 'Seungyeon Kim', 'Tal Schuster'], 'affiliations': ['Google DeepMind', 'Google Research', 'KAIST AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20672.jpg', 'data': {'categories': ['#small_models', '#inference', '#optimization', '#training', '#transfer_learning', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ´Ğ¸Ğ½ Ğ±Ğ»Ğ¾Ğº ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ², Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞµĞ¼Ñ‹Ğ¹ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ·. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ñ€ĞµĞ»Ğ°ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ LoRA. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ĞºĞ°Ğº Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Efficient LLMs through Recursive Transformers', 'desc': "This paper explores a method to reduce the size and cost of large language models (LLMs) by using parameter sharing through a technique called 'layer tying' in Transformers. The authors introduce 'Recursive Transformers', which utilize a single set of unique layers that are repeated, allowing for a more compact model with minimal performance loss. They enhance this approach with 'Relaxed Recursive Transformers' that incorporate low-rank adaptation modules, maintaining efficiency while improving performance. The results demonstrate that these recursive models outperform similarly sized models and can achieve performance close to larger models, while also introducing a new inference method that significantly boosts throughput."}, 'zh': {'title': 'é€’å½’å˜æ¢å™¨ï¼šé«˜æ•ˆå…±äº«å‚æ•°çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å‚æ•°å…±äº«æ¥å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è§„æ¨¡å’Œæˆæœ¬ã€‚æˆ‘ä»¬é‡æ–°å®¡è§†äº†åœ¨å˜æ¢å™¨ä¸­ä½¿ç”¨çš„â€œå±‚ç»‘å®šâ€æŠ€æœ¯ï¼Œå¹¶æå‡ºäº†å°†ç°æœ‰LLMsè½¬åŒ–ä¸ºæ›´å°çš„â€œé€’å½’å˜æ¢å™¨â€çš„æ–°æ–¹æ³•ï¼Œè¿™äº›å˜æ¢å™¨åœ¨å±‚ä¹‹é—´å…±äº«å‚æ•°ï¼Œä¸”æ€§èƒ½æŸå¤±æœ€å°ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ”¾æ¾é€’å½’å˜æ¢å™¨ï¼Œé€šè¿‡æ·±åº¦ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¨¡å—å¢åŠ çµæ´»æ€§ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„ç´§å‡‘æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é€’å½’æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†åŒç­‰è§„æ¨¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶èƒ½æ¢å¤å¤§éƒ¨åˆ†åŸå§‹å…¨å°ºå¯¸æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.18481', 'title': 'Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction', 'url': 'https://huggingface.co/papers/2410.18481', 'abstract': 'Efficiently deriving structured workflows from unannotated dialogs remains an underexplored and formidable challenge in computational linguistics. Automating this process could significantly accelerate the manual design of workflows in new domains and enable the grounding of large language models in domain-specific flowcharts, enhancing transparency and controllability. In this paper, we introduce Dialog2Flow (D2F) embeddings, which differ from conventional sentence embeddings by mapping utterances to a latent space where they are grouped according to their communicative and informative functions (i.e., the actions they represent). D2F allows for modeling dialogs as continuous trajectories in a latent space with distinct action-related regions. By clustering D2F embeddings, the latent space is quantized, and dialogs can be converted into sequences of region/action IDs, facilitating the extraction of the underlying workflow. To pre-train D2F, we build a comprehensive dataset by unifying twenty task-oriented dialog datasets with normalized per-turn action annotations. We also introduce a novel soft contrastive loss that leverages the semantic information of these actions to guide the representation learning process, showing superior performance compared to standard supervised contrastive loss. Evaluation against various sentence embeddings, including dialog-specific ones, demonstrates that D2F yields superior qualitative and quantitative results across diverse domains.', 'score': 5, 'issue_id': 328, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 24', 'zh': '10æœˆ24æ—¥'}, 'hash': '7ddeda6e7b60deaa', 'authors': ['Sergio Burdisso', 'Srikanth Madikeri', 'Petr Motlicek'], 'affiliations': ['Brno University of Technology, Brno, Czech Republic', 'Department of Computational Linguistics, University of Zurich, Zurich, Switzerland', 'Idiap Research Institute, Martigny, Switzerland'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18481.jpg', 'data': {'categories': ['#reasoning', '#multilingual', '#cv', '#optimization', '#data', '#training', '#dataset', '#transfer_learning'], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'D2F: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ°Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¸Ğ· Ğ½ĞµĞ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ embeddings Dialog2Flow (D2F). D2F Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ¾ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ğ² 20 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ soft contrastive loss. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ D2F Ğ½Ğ°Ğ´ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ sentence embeddings Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ….'}, 'en': {'title': 'Transforming Dialogs into Actionable Workflows with D2F Embeddings', 'desc': 'This paper addresses the challenge of creating structured workflows from unannotated dialogs in computational linguistics. It presents Dialog2Flow (D2F) embeddings, which uniquely map dialog utterances into a latent space based on their communicative functions. By clustering these embeddings, the authors can convert dialogs into sequences of action IDs, effectively extracting workflows. The study also introduces a new soft contrastive loss for better representation learning, showing that D2F outperforms existing sentence embeddings in various tasks.'}, 'zh': {'title': 'ä»å¯¹è¯ä¸­æå–å·¥ä½œæµçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ä»æœªæ ‡æ³¨å¯¹è¯ä¸­é«˜æ•ˆæå–ç»“æ„åŒ–å·¥ä½œæµçš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†Dialog2Flow (D2F) åµŒå…¥ï¼Œå®ƒé€šè¿‡å°†å¯¹è¯æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ï¼ŒæŒ‰å…¶äº¤æµå’Œä¿¡æ¯åŠŸèƒ½è¿›è¡Œåˆ†ç»„ã€‚D2Fä½¿å¾—å¯¹è¯å¯ä»¥åœ¨æ½œåœ¨ç©ºé—´ä¸­å»ºæ¨¡ä¸ºè¿ç»­è½¨è¿¹ï¼Œå¹¶é€šè¿‡èšç±»D2FåµŒå…¥æ¥é‡åŒ–æ½œåœ¨ç©ºé—´ï¼Œä»è€Œæå–åº•å±‚å·¥ä½œæµã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è½¯å¯¹æ¯”æŸå¤±ï¼Œåˆ©ç”¨åŠ¨ä½œçš„è¯­ä¹‰ä¿¡æ¯æ¥æŒ‡å¯¼è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œæ˜¾ç¤ºå‡ºä¼˜äºä¼ ç»Ÿç›‘ç£å¯¹æ¯”æŸå¤±çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.20220', 'title': 'Neural Fields in Robotics: A Survey', 'url': 'https://huggingface.co/papers/2410.20220', 'abstract': "Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable rendering, Neural Fields encompass both continuous implicit and explicit neural representations enabling high-fidelity 3D reconstruction, integration of multi-modal sensor data, and generation of novel viewpoints. This survey explores their applications in robotics, emphasizing their potential to enhance perception, planning, and control. Their compactness, memory efficiency, and differentiability, along with seamless integration with foundation and generative models, make them ideal for real-time applications, improving robot adaptability and decision-making. This paper provides a thorough review of Neural Fields in robotics, categorizing applications across various domains and evaluating their strengths and limitations, based on over 200 papers. First, we present four key Neural Fields frameworks: Occupancy Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian Splatting. Second, we detail Neural Fields' applications in five major robotics domains: pose estimation, manipulation, navigation, physics, and autonomous driving, highlighting key works and discussing takeaways and open challenges. Finally, we outline the current limitations of Neural Fields in robotics and propose promising directions for future research. Project page: https://robonerf.github.io", 'score': 4, 'issue_id': 323, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': '4666404a494d69c8', 'authors': ['Muhammad Zubair Irshad', 'Mauro Comi', 'Yen-Chen Lin', 'Nick Heppert', 'Abhinav Valada', 'Rares Ambrus', 'Zsolt Kira', 'Jonathan Tremblay'], 'affiliations': ['Georgia Institute of Technology', 'Nvidia', 'Toyota Research Institute', 'University of Bristol', 'University of Freiburg'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20220.jpg', 'data': {'categories': ['#science', '#cv', '#graphs', '#multimodal', '#robotics', '#3d', '#survey', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞĞ½Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, 3D-ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¸Ğ· 2D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 200 Ñ€Ğ°Ğ±Ğ¾Ñ‚, ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Robotics with Neural Fields for 3D Understanding', 'desc': "Neural Fields are a new way to represent 3D scenes using data from 2D images, which helps in understanding the shape, meaning, and movement of objects in a scene. They use a technique called differentiable rendering to create detailed 3D models and can combine information from different types of sensors. This paper reviews how Neural Fields can improve robots' ability to see, plan, and act in their environments, making them more efficient and adaptable. It also discusses various frameworks and applications of Neural Fields in robotics, while identifying their current challenges and future research opportunities."}, 'zh': {'title': 'ç¥ç»åœºï¼šæå‡æœºå™¨äººæ„ŸçŸ¥ä¸å†³ç­–çš„å…³é”®æŠ€æœ¯', 'desc': 'ç¥ç»åœºæ˜¯ä¸€ç§æ–°å…´çš„3Dåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿä»2Dæ•°æ®ä¸­å‡†ç¡®æ¨æ–­å‡ ä½•å½¢çŠ¶ã€3Dè¯­ä¹‰å’ŒåŠ¨æ€ä¿¡æ¯ã€‚é€šè¿‡å¯å¾®æ¸²æŸ“ï¼Œç¥ç»åœºç»“åˆäº†è¿ç»­éšå¼å’Œæ˜¾å¼ç¥ç»è¡¨ç¤ºï¼Œå®ç°é«˜ä¿çœŸåº¦çš„3Dé‡å»ºå’Œå¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®çš„æ•´åˆã€‚æœ¬æ–‡å›é¡¾äº†ç¥ç»åœºåœ¨æœºå™¨äººé¢†åŸŸçš„åº”ç”¨ï¼Œå¼ºè°ƒå…¶åœ¨æ„ŸçŸ¥ã€è§„åˆ’å’Œæ§åˆ¶æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†ç¥ç»åœºçš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2406.10615', 'title': 'Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation', 'url': 'https://huggingface.co/papers/2406.10615', 'abstract': "Given the high cost of collecting robotic data in the real world, sample efficiency is a consistently compelling pursuit in robotics. In this paper, we introduce SGRv2, an imitation learning framework that enhances sample efficiency through improved visual and action representations. Central to the design of SGRv2 is the incorporation of a critical inductive bias-action locality, which posits that robot's actions are predominantly influenced by the target object and its interactions with the local environment. Extensive experiments in both simulated and real-world settings demonstrate that action locality is essential for boosting sample efficiency. SGRv2 excels in RLBench tasks with keyframe control using merely 5 demonstrations and surpasses the RVT baseline in 23 of 26 tasks. Furthermore, when evaluated on ManiSkill2 and MimicGen using dense control, SGRv2's success rate is 2.54 times that of SGR. In real-world environments, with only eight demonstrations, SGRv2 can perform a variety of tasks at a markedly higher success rate compared to baseline models. Project website: http://sgrv2-robot.github.io", 'score': 2, 'issue_id': 327, 'pub_date': '2024-06-15', 'pub_date_card': {'ru': '15 Ğ¸ÑĞ½Ñ', 'en': 'June 15', 'zh': '6æœˆ15æ—¥'}, 'hash': 'cc1e4c7a45b1c9ab', 'authors': ['Tong Zhang', 'Yingdong Hu', 'Jiacheng You', 'Yang Gao'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Qi Zhi Institute', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2406.10615.jpg', 'data': {'categories': ['#synthetic', '#rl', '#benchmark', '#cv', '#optimization', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ - ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'SGRv2 - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‰ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ ĞµĞ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼. SGRv2 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 5-8 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Boosting Robotics with Sample-Efficient Imitation Learning', 'desc': "This paper presents SGRv2, an advanced imitation learning framework aimed at improving sample efficiency in robotics. It introduces a key concept called action locality, which suggests that a robot's actions are mainly determined by the target object and its local environment interactions. Through extensive testing in both simulated and real-world scenarios, the authors demonstrate that SGRv2 significantly outperforms existing models, achieving higher success rates with fewer demonstrations. The results indicate that SGRv2 is particularly effective in complex tasks, showcasing its potential for practical applications in robotics."}, 'zh': {'title': 'æå‡æ ·æœ¬æ•ˆç‡çš„æ¨¡ä»¿å­¦ä¹ æ–°æ¡†æ¶SGRv2', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSGRv2çš„æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººæ•°æ®æ”¶é›†çš„æ ·æœ¬æ•ˆç‡ã€‚SGRv2é€šè¿‡æ”¹è¿›è§†è§‰å’ŒåŠ¨ä½œè¡¨ç¤ºï¼Œç»“åˆäº†å…³é”®çš„å½’çº³åç½®â€”â€”åŠ¨ä½œå±€éƒ¨æ€§ï¼Œå¼ºè°ƒæœºå™¨äººåŠ¨ä½œä¸»è¦å—ç›®æ ‡ç‰©ä½“åŠå…¶ä¸ç¯å¢ƒçš„äº’åŠ¨å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠ¨ä½œå±€éƒ¨æ€§å¯¹äºæå‡æ ·æœ¬æ•ˆç‡è‡³å…³é‡è¦ï¼ŒSGRv2åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸç‡æ˜¾è‘—é«˜äºåŸºçº¿æ¨¡å‹ã€‚è¯¥æ¡†æ¶åœ¨çœŸå®ç¯å¢ƒä¸­ä»…éœ€å…«ä¸ªæ¼”ç¤ºå°±èƒ½å®Œæˆå¤šç§ä»»åŠ¡ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.20636', 'title': 'Language Models And A Second Opinion Use Case: The Pocket Professional', 'url': 'https://huggingface.co/papers/2410.20636', 'abstract': "This research tests the role of Large Language Models (LLMs) as formal second opinion tools in professional decision-making, particularly focusing on complex medical cases where even experienced physicians seek peer consultation. The work analyzed 183 challenging medical cases from Medscape over a 20-month period, testing multiple LLMs' performance against crowd-sourced physician responses. A key finding was the high overall score possible in the latest foundational models (>80% accuracy compared to consensus opinion), which exceeds most human metrics reported on the same clinical cases (450 pages of patient profiles, test results). The study rates the LLMs' performance disparity between straightforward cases (>81% accuracy) and complex scenarios (43% accuracy), particularly in these cases generating substantial debate among human physicians. The research demonstrates that LLMs may be valuable as generators of comprehensive differential diagnoses rather than as primary diagnostic tools, potentially helping to counter cognitive biases in clinical decision-making, reduce cognitive loads, and thus remove some sources of medical error. The inclusion of a second comparative legal dataset (Supreme Court cases, N=21) provides added empirical context to the AI use to foster second opinions, though these legal challenges proved considerably easier for LLMs to analyze. In addition to the original contributions of empirical evidence for LLM accuracy, the research aggregated a novel benchmark for others to score highly contested question and answer reliability between both LLMs and disagreeing human practitioners. These results suggest that the optimal deployment of LLMs in professional settings may differ substantially from current approaches that emphasize automation of routine tasks.", 'score': 2, 'issue_id': 323, 'pub_date': '2024-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': 'b1a78251a22af319', 'authors': ['David Noever'], 'affiliations': ['PeopleTec, 4901-D Corporate Drive, Huntsville, AL, USA, 35805'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20636.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#multilingual', '#healthcare', '#dataset', '#alignment'], 'emoji': 'ğŸ©º', 'ru': {'title': 'LLM ĞºĞ°Ğº Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºÑƒÑ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· 183 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 80% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑĞ½Ñ‹Ğ¼ Ğ¼Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ…, Ğ½Ğ¾ Ğ¼ĞµĞ½ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» LLM ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ² Ğ±Ğ¾Ñ€ÑŒĞ±Ğµ Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹.'}, 'en': {'title': 'LLMs: Enhancing Medical Decision-Making with Second Opinions', 'desc': 'This research investigates how Large Language Models (LLMs) can serve as second opinion tools in complex medical decision-making. By analyzing 183 challenging medical cases, the study found that LLMs achieved over 80% accuracy, outperforming many human physicians. However, the models struggled with complex cases, showing only 43% accuracy, indicating their strength lies in generating differential diagnoses rather than making primary diagnoses. The findings suggest that LLMs could help reduce cognitive biases and errors in clinical settings, while also providing a new benchmark for evaluating AI performance against human practitioners.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼šåŒ»ç–—å†³ç­–ä¸­çš„ç¬¬äºŒæ„è§åŠ©æ‰‹', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸“ä¸šå†³ç­–ä¸­çš„ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚åŒ»ç–—æ¡ˆä¾‹ä¸­ä½œä¸ºæ­£å¼çš„ç¬¬äºŒæ„è§å·¥å…·ã€‚ç ”ç©¶åˆ†æäº†183ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»ç–—æ¡ˆä¾‹ï¼Œæ¯”è¾ƒäº†å¤šç§LLMsçš„è¡¨ç°ä¸åŒ»ç”Ÿçš„ç¾¤ä½“æ„è§ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ€æ–°çš„åŸºç¡€æ¨¡å‹åœ¨è¿™äº›æ¡ˆä¾‹ä¸­çš„å‡†ç¡®ç‡è¶…è¿‡80%ï¼Œé«˜äºå¤§å¤šæ•°äººç±»åŒ»ç”Ÿçš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMsåœ¨ç”Ÿæˆå…¨é¢çš„é‰´åˆ«è¯Šæ–­æ–¹é¢å¯èƒ½æ›´æœ‰ä»·å€¼ï¼Œè€Œä¸æ˜¯ä½œä¸ºä¸»è¦çš„è¯Šæ–­å·¥å…·ï¼Œèƒ½å¤Ÿå¸®åŠ©å‡å°‘ä¸´åºŠå†³ç­–ä¸­çš„è®¤çŸ¥åå·®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.01968', 'title': 'Bi-Level Motion Imitation for Humanoid Robots', 'url': 'https://huggingface.co/papers/2410.01968', 'abstract': 'Imitation learning from human motion capture (MoCap) data provides a promising way to train humanoid robots. However, due to differences in morphology, such as varying degrees of joint freedom and force limits, exact replication of human behaviors may not be feasible for humanoid robots. Consequently, incorporating physically infeasible MoCap data in training datasets can adversely affect the performance of the robot policy. To address this issue, we propose a bi-level optimization-based imitation learning framework that alternates between optimizing both the robot policy and the target MoCap data. Specifically, we first develop a generative latent dynamics model using a novel self-consistent auto-encoder, which learns sparse and structured motion representations while capturing desired motion patterns in the dataset. The dynamics model is then utilized to generate reference motions while the latent representation regularizes the bi-level motion imitation process. Simulations conducted with a realistic model of a humanoid robot demonstrate that our method enhances the robot policy by modifying reference motions to be physically consistent.', 'score': 1, 'issue_id': 331, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '61a874c3e9900dab', 'authors': ['Wenshuai Zhao', 'Yi Zhao', 'Joni Pajarinen', 'Michael Muehlebach'], 'affiliations': ['Aalto University, Finland', 'Max Planck Institute for Intelligent Systems, Germany'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.01968.jpg', 'data': {'categories': ['#rl', '#optimization', '#dataset', '#robotics', '#games', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Optimizing Robot Imitation with Physically Feasible Motion', 'desc': "This paper presents a new approach to train humanoid robots using imitation learning from human motion capture (MoCap) data. The challenge arises from the differences in robot and human body structures, which can make it difficult to directly replicate human movements. To solve this, the authors introduce a bi-level optimization framework that optimizes both the robot's movement policy and the MoCap data. By using a generative latent dynamics model, the framework ensures that the generated motions are physically feasible, improving the robot's ability to imitate human actions effectively."}, 'zh': {'title': 'ä¼˜åŒ–æ¨¡ä»¿å­¦ä¹ ï¼Œæå‡ç±»äººæœºå™¨äººè¡¨ç°', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºåŒå±‚ä¼˜åŒ–çš„æ¨¡ä»¿å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒç±»äººæœºå™¨äººã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–æœºå™¨äººç­–ç•¥å’Œç›®æ ‡è¿åŠ¨æ•æ‰æ•°æ®ï¼Œè§£å†³äº†äººç±»è¡Œä¸ºåœ¨æœºå™¨äººä¸­æ— æ³•ç²¾ç¡®å¤åˆ¶çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç”Ÿæˆæ½œåœ¨åŠ¨æ€æ¨¡å‹ï¼Œåˆ©ç”¨è‡ªä¸€è‡´æ€§è‡ªç¼–ç å™¨å­¦ä¹ ç¨€ç–å’Œç»“æ„åŒ–çš„è¿åŠ¨è¡¨ç¤ºã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œç”Ÿæˆçš„å‚è€ƒè¿åŠ¨ä¸æ½œåœ¨è¡¨ç¤ºç›¸ç»“åˆï¼Œç¡®ä¿äº†æœºå™¨äººç­–ç•¥çš„ç‰©ç†ä¸€è‡´æ€§ï¼Œä»è€Œæå‡äº†æœºå™¨äººçš„è¡¨ç°ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (3)', '#agi (1)', '#alignment (5)', '#architecture (11)', '#audio (1)', '#benchmark (10)', '#cv (8)', '#data (3)', '#dataset (6)', '#diffusion (4)', '#ethics (1)', '#games (5)', '#graphs (1)', '#hallucinations', '#healthcare (1)', '#inference (6)', '#interpretability (1)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation (1)', '#math (1)', '#multilingual (4)', '#multimodal (6)', '#open_source (5)', '#optimization (13)', '#plp', '#rag (1)', '#reasoning (5)', '#rl (3)', '#rlhf (2)', '#robotics (3)', '#science (4)', '#security (1)', '#small_models (5)', '#story_generation', '#survey (3)', '#synthetic (3)', '#training (9)', '#transfer_learning (4)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-10-29 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-29 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-29 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    