{
    "date": {
        "ru": "5 Ğ¸ÑĞ½Ñ",
        "en": "June 5",
        "zh": "6æœˆ5æ—¥"
    },
    "time_utc": "2025-06-05 07:12",
    "weekday": 3,
    "issue_id": 4138,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.03569",
            "title": "MiMo-VL Technical Report",
            "url": "https://huggingface.co/papers/2506.03569",
            "abstract": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
            "score": 34,
            "issue_id": 4133,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "cb568276c7e799cb",
            "authors": [
                "Xiaomi LLM-Core Team",
                ":",
                "Zihao Yue",
                "Zhenru Lin",
                "Yifan Song",
                "Weikun Wang",
                "Shuhuai Ren",
                "Shuhao Gu",
                "Shicheng Li",
                "Peidian Li",
                "Liang Zhao",
                "Lei Li",
                "Kainan Bao",
                "Hao Tian",
                "Hailin Zhang",
                "Gang Wang",
                "Dawei Zhu",
                "Cici",
                "Chenhong He",
                "Bowen Ye",
                "Bowen Shen",
                "Zihan Zhang",
                "Zihan Jiang",
                "Zhixian Zheng",
                "Zhichao Song",
                "Zhenbo Luo",
                "Yue Yu",
                "Yudong Wang",
                "Yuanyuan Tian",
                "Yu Tu",
                "Yihan Yan",
                "Yi Huang",
                "Xu Wang",
                "Xinzhe Xu",
                "Xingchen Song",
                "Xing Zhang",
                "Xing Yong",
                "Xin Zhang",
                "Xiangwei Deng",
                "Wenyu Yang",
                "Wenhan Ma",
                "Weiwei Lv",
                "Weiji Zhuang",
                "Wei Liu",
                "Sirui Deng",
                "Shuo Liu",
                "Shimao Chen",
                "Shihua Yu",
                "Shaohui Liu",
                "Shande Wang",
                "Rui Ma",
                "Qiantong Wang",
                "Peng Wang",
                "Nuo Chen",
                "Menghang Zhu",
                "Kangyang Zhou",
                "Kang Zhou",
                "Kai Fang",
                "Jun Shi",
                "Jinhao Dong",
                "Jiebao Xiao",
                "Jiaming Xu",
                "Huaqiu Liu",
                "Hongshen Xu",
                "Heng Qu",
                "Haochen Zhao",
                "Hanglong Lv",
                "Guoan Wang",
                "Duo Zhang",
                "Dong Zhang",
                "Di Zhang",
                "Chong Ma",
                "Chang Liu",
                "Can Cai",
                "Bingquan Xia"
            ],
            "affiliations": [
                "Xiaomi"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03569.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#multimodal",
                    "#rlhf",
                    "#benchmark",
                    "#dataset",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜: MiMo-VL ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ MiMo-VL-7B-SFT Ğ¸ MiMo-VL-7B-RL, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ MiMo-VL-7B-RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Qwen2.5-VL-7B Ğ² 35 Ğ¸Ğ· 40 Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 59.4 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OlympiadBench. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ»Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 2.4 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (MORL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ² ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Vision-Language Models with MiMo-VL",
                    "desc": "The paper introduces two advanced vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, which excel in visual understanding and multimodal reasoning tasks. MiMo-VL-7B-RL demonstrates superior performance, outperforming other models on a majority of evaluated tasks and achieving high scores on benchmark datasets. The training methodology involves a four-stage pre-training process using a massive dataset and incorporates Mixed On-policy Reinforcement Learning to enhance model performance through diverse reward signals. Additionally, the authors emphasize the significance of high-quality reasoning data and provide a comprehensive evaluation suite to facilitate reproducibility in future research."
                },
                "zh": {
                    "title": "å¼€åˆ›è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ–°æ ‡å‡†",
                    "desc": "æˆ‘ä»¬å¼€æºäº†MiMo-VL-7B-SFTå’ŒMiMo-VL-7B-RLï¼Œè¿™ä¸¤ä¸ªå¼ºå¤§çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ä¸€èˆ¬è§†è§‰ç†è§£å’Œå¤šæ¨¡æ€æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚MiMo-VL-7B-RLåœ¨40ä¸ªè¯„ä¼°ä»»åŠ¡ä¸­æœ‰35ä¸ªè¶…è¶Šäº†Qwen2.5-VL-7Bï¼Œå¹¶åœ¨OlympiadBenchä¸Šå¾—åˆ†59.4ï¼Œè¶…è¿‡äº†å‚æ•°é«˜è¾¾78Bçš„æ¨¡å‹ã€‚åœ¨GUIå®šä½åº”ç”¨ä¸­ï¼Œå®ƒåœ¨OSWorld-Gä¸Šä»¥56.1çš„åˆ†æ•°è®¾å®šäº†æ–°æ ‡å‡†ï¼Œç”šè‡³è¶…è¶Šäº†ä¸“é—¨æ¨¡å‹UI-TARSã€‚æˆ‘ä»¬çš„è®­ç»ƒç»“åˆäº†å››é˜¶æ®µçš„é¢„è®­ç»ƒï¼ˆ24ä¸‡äº¿ä¸ªæ ‡è®°ï¼‰å’Œæ··åˆåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆMORLï¼‰ï¼Œå¹¶å¼ºè°ƒäº†åœ¨é¢„è®­ç»ƒé˜¶æ®µèå…¥é«˜è´¨é‡æ¨ç†æ•°æ®å’Œé•¿é“¾æ€ç»´çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02921",
            "title": "A Controllable Examination for Long-Context Language Models",
            "url": "https://huggingface.co/papers/2506.02921",
            "abstract": "LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing frameworks for evaluating long-context language models (LCLM) can be broadly categorized into real-world and synthetic tasks. Despite their utility, both approaches are accompanied by certain intrinsic limitations. Real-world tasks are too complex to interpret or characterize and are susceptible to data contamination. In contrast, synthetic tasks often adopt the needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the \"needle\" and the \"haystack\" compromises their validity as proxies for realistic applications. In response to these challenges, we posit that an ideal long-context evaluation framework should be characterized by three essential features: seamless context, controllable setting, and sound evaluation. This study introduces LongBioBench, a novel benchmark that utilizes artificially generated biographies as a controlled environment for assessing LCLMs across dimensions of understanding, reasoning, and trustworthiness. Our experimental evaluation, which includes 18 LCLMs in total, demonstrates that most models still exhibit deficiencies in semantic understanding and elementary reasoning over retrieved results and are less trustworthy as context length increases. Our further analysis indicates some design choices employed by existing synthetic benchmarks, such as contextual non-coherence, numerical needles, and the absence of distractors, rendering them vulnerable to test the model long-context capabilities. Moreover, we also reveal that long-context continual pretraining primarily adjusts RoPE embedding to accommodate extended context lengths. To sum up, compared to previous synthetic benchmarks, LongBioBench achieves a better trade-off between mirroring authentic language tasks and maintaining controllability, and is highly interpretable and configurable.",
            "score": 23,
            "issue_id": 4134,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "073ae66fedf9c141",
            "authors": [
                "Yijun Yang",
                "Zeyu Huang",
                "Wenhao Zhu",
                "Zihan Qiu",
                "Fei Yuan",
                "Jeff Z. Pan",
                "Ivan Titov"
            ],
            "affiliations": [
                "Nanjing University",
                "Qwen Team, Alibaba Group",
                "Shanghai Artificial Intelligence Laboratory",
                "University of Amsterdam",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02921.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#synthetic",
                    "#long_context",
                    "#reasoning",
                    "#interpretability"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "LongBioBench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "LongBioBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸. ĞĞ½ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ¼ĞµĞ½ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "LongBioBench: A New Standard for Evaluating Long-Context Language Models",
                    "desc": "LongBioBench is a new benchmark designed to evaluate long-context language models (LCLMs) using artificially generated biographies. It addresses the limitations of existing evaluation frameworks by providing a controlled environment that emphasizes understanding, reasoning, and trustworthiness. The study reveals that many LCLMs struggle with semantic understanding and reasoning as context length increases, highlighting the need for better evaluation methods. LongBioBench offers a more coherent and interpretable approach compared to previous synthetic benchmarks, making it a valuable tool for assessing LCLMs."
                },
                "zh": {
                    "title": "LongBioBenchï¼šè¯„ä¼°é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹çš„æ–°åŸºå‡†",
                    "desc": "LongBioBench æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œåˆ©ç”¨äººå·¥ç”Ÿæˆçš„ä¼ è®°æ¥è¯„ä¼°é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ï¼ˆLCLMï¼‰åœ¨ç†è§£ã€æ¨ç†å’Œå¯ä¿¡åº¦æ–¹é¢çš„è¡¨ç°ï¼Œè§£å†³äº†ç°æœ‰æ¡†æ¶çš„å±€é™æ€§ã€‚ç°æœ‰çš„è¯„ä¼°æ¡†æ¶åˆ†ä¸ºçœŸå®ä¸–ç•Œä»»åŠ¡å’Œåˆæˆä»»åŠ¡ï¼Œä½†ä¸¤è€…éƒ½æœ‰å†…åœ¨çš„ç¼ºé™·ã€‚çœŸå®ä¸–ç•Œä»»åŠ¡å¤æ‚ä¸”æ˜“å—æ•°æ®æ±¡æŸ“ï¼Œè€Œåˆæˆä»»åŠ¡å¸¸å¸¸ç¼ºä¹è¿è´¯æ€§ï¼Œå½±å“å…¶ä½œä¸ºç°å®åº”ç”¨çš„æœ‰æ•ˆæ€§ã€‚LongBioBench æä¾›äº†ä¸€ä¸ªå—æ§ç¯å¢ƒï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼° LCLM çš„èƒ½åŠ›ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå¤§å¤šæ•°æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’ŒåŸºæœ¬æ¨ç†ä¸Šä»å­˜åœ¨ä¸è¶³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04207",
            "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2506.04207",
            "abstract": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025.",
            "score": 22,
            "issue_id": 4135,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "61521f9ed974c930",
            "authors": [
                "Shuang Chen",
                "Yue Guo",
                "Zhaochen Su",
                "Yafu Li",
                "Yulun Wu",
                "Jiacheng Chen",
                "Jiayu Chen",
                "Weijie Wang",
                "Xiaoye Qu",
                "Yu Cheng"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai AI Laboratory",
                "Soochow University",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04207.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#multimodal",
                    "#training",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ MLLM: Ğ¾Ñ‚ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ RL",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ° Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑ‚Ğ°Ğ³Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¼ GRPO Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ RL. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ReVisual-R1, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ÑˆĞ°Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… 7B MLLM Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Reasoning in MLLMs with Smart Training Strategies",
                    "desc": "This paper explores how to improve reasoning in Multimodal Large Language Models (MLLMs) by analyzing their training processes. It identifies that starting with well-chosen text data can significantly boost reasoning capabilities, even before applying multimodal reinforcement learning (RL). The authors also highlight that traditional gradient-based methods in multimodal RL can lead to stagnation, negatively impacting training effectiveness. By implementing a staged training approach that combines text-only RL after multimodal RL, they introduce ReVisual-R1, which sets new performance records on various complex benchmarks."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè‰¯å¥½çš„å†·å¯åŠ¨åˆå§‹åŒ–å¯¹äºå¢å¼ºMLLMçš„æ¨ç†è‡³å…³é‡è¦ï¼Œå•ç‹¬ä½¿ç”¨ç²¾å¿ƒé€‰æ‹©çš„æ–‡æœ¬æ•°æ®å³å¯è¶…è¶Šè®¸å¤šè¿‘æœŸçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚æ ‡å‡†çš„GRPOåœ¨å¤šæ¨¡æ€RLä¸­å­˜åœ¨æ¢¯åº¦åœæ»çš„é—®é¢˜ï¼Œå½±å“äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚é€šè¿‡åˆ†é˜¶æ®µçš„è®­ç»ƒæ–¹æ³•ï¼Œç»“åˆæ–‡æœ¬å’Œå¤šæ¨¡æ€RLï¼Œæå‡ºäº†ReVisual-R1ï¼Œè¾¾åˆ°äº†å¼€æº7B MLLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æ–°çŠ¶æ€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04180",
            "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
            "url": "https://huggingface.co/papers/2506.04180",
            "abstract": "Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation.",
            "score": 18,
            "issue_id": 4133,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "3f52b337c5fa3683",
            "authors": [
                "Yuhao Wu",
                "Yushi Bai",
                "Zhiqiang Hu",
                "Juanzi Li",
                "Roy Ka-Wei Lee"
            ],
            "affiliations": [
                "Singapore University of Technology and Design, Singapore",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04180.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#agents",
                    "#long_context",
                    "#rlhf",
                    "#benchmark",
                    "#dataset",
                    "#story_generation"
                ],
                "emoji": "âœï¸",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "SuperWriter-Agent - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ 7B-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SuperWriter-LM Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (DPO) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SuperWriter-LM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Elevating Long-Form Text Generation with Structured Thinking",
                    "desc": "This paper presents SuperWriter-Agent, a novel framework aimed at improving long-form text generation by large language models (LLMs). It introduces structured thinking through planning and refinement stages, which helps the model generate more coherent and logically consistent text. The framework is supported by a supervised fine-tuning dataset for training a 7B parameter model called SuperWriter-LM. Additionally, a hierarchical Direct Preference Optimization (DPO) method is employed, utilizing Monte Carlo Tree Search to enhance the quality of generated text, leading to superior performance on various benchmarks."
                },
                "zh": {
                    "title": "æå‡é•¿æ–‡æœ¬ç”Ÿæˆè´¨é‡çš„æ™ºèƒ½ä»£ç†",
                    "desc": "é•¿æ–‡æœ¬ç”Ÿæˆæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢ä¸´çš„é‡è¦æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¿æŒè¿è´¯æ€§ã€é€»è¾‘ä¸€è‡´æ€§å’Œæ–‡æœ¬è´¨é‡æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SuperWriter-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä»£ç†çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é•¿æ–‡æœ¬ç”Ÿæˆçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡è§„åˆ’å’Œç²¾ç‚¼é˜¶æ®µå¼•å…¥æ˜ç¡®çš„ç»“æ„åŒ–æ€ç»´ï¼ŒæŒ‡å¯¼æ¨¡å‹éµå¾ªæ›´æœ‰æ„è¯†å’Œè®¤çŸ¥åŸºç¡€çš„è¿‡ç¨‹ï¼Œç±»ä¼¼äºä¸“ä¸šä½œå®¶çš„å†™ä½œæ–¹å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSuperWriter-LMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†åˆ†å±‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç»“æ„åŒ–æ€ç»´æ­¥éª¤çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01320",
            "title": "Î¨-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models",
            "url": "https://huggingface.co/papers/2506.01320",
            "abstract": "We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.",
            "score": 15,
            "issue_id": 4136,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "6441248200df695a",
            "authors": [
                "Taehoon Yoon",
                "Yunhong Min",
                "Kyeongmin Yeo",
                "Minhyuk Sung"
            ],
            "affiliations": [
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01320.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#alignment",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Psi-Sampler - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ (SMC) Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ† Ğ¸Ğ· Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸Ğ· Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ preconditioned Crank-Nicolson Langevin (pCNL). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´."
                },
                "en": {
                    "title": "Enhancing Reward Alignment with Psi-Sampler",
                    "desc": "The paper presents Psi-Sampler, a framework that enhances reward alignment during inference by using Sequential Monte Carlo (SMC) methods. It addresses the limitations of traditional particle initialization from Gaussian priors, which often fail to capture important reward-related areas. By employing a reward-aware posterior for initialization, the framework significantly boosts sampling efficiency and alignment performance. Additionally, the introduction of the preconditioned Crank-Nicolson Langevin (pCNL) algorithm allows for effective sampling in complex, high-dimensional spaces, leading to improved results in various generative tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆçš„å¥–åŠ±å¯¹é½ï¼šPsi-Sampleræ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPsi-Samplerçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºåºåˆ—è’™ç‰¹å¡æ´›ï¼ˆSMCï¼‰æ–¹æ³•ï¼Œå¹¶ç»“åˆäº†åŸºäºå¥–åŠ±çš„åˆå§‹ç²’å­é‡‡æ ·ï¼Œä»¥å®ç°ä¸åŸºäºåˆ†æ•°çš„ç”Ÿæˆæ¨¡å‹çš„æœ‰æ•ˆæ¨ç†æ—¶é—´å¥–åŠ±å¯¹é½ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºåˆ†æ•°çš„ç”Ÿæˆæ¨¡å‹åœ¨æ¨ç†æ—¶é—´å¥–åŠ±å¯¹é½æ–¹é¢å—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œæ ‡å¿—ç€ä»é¢„è®­ç»ƒåˆ°åè®­ç»ƒä¼˜åŒ–çš„èŒƒå¼è½¬å˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»é«˜æ–¯å…ˆéªŒåˆå§‹åŒ–ç²’å­ï¼Œè¿™ä¸è¶³ä»¥æ•æ‰ä¸å¥–åŠ±ç›¸å…³çš„åŒºåŸŸï¼Œå¯¼è‡´é‡‡æ ·æ•ˆç‡é™ä½ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä»å¥–åŠ±æ„ŸçŸ¥åéªŒåˆå§‹åŒ–æ˜¾è‘—æé«˜äº†å¯¹é½æ€§èƒ½ï¼Œå¹¶å¼•å…¥äº†é¢„æ¡ä»¶Crank-Nicolson Langevinï¼ˆpCNLï¼‰ç®—æ³•ï¼Œä»¥å®ç°é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­çš„åéªŒé‡‡æ ·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03139",
            "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
            "url": "https://huggingface.co/papers/2506.03139",
            "abstract": "SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) and Multimodal LLMs have shown promising capabilities for SVG processing, yet existing benchmarks suffer from limited real-world coverage, lack of complexity stratification, and fragmented evaluation paradigms. We introduce SVGenius, a comprehensive benchmark comprising 2,377 queries across three progressive dimensions: understanding, editing, and generation. Built on real-world data from 24 application domains with systematic complexity stratification, SVGenius evaluates models through 8 task categories and 18 metrics. We assess 22 mainstream models spanning different scales, architectures, training paradigms, and accessibility levels. Our analysis reveals that while proprietary models significantly outperform open-source counterparts, all models exhibit systematic performance degradation with increasing complexity, indicating fundamental limitations in current approaches; however, reasoning-enhanced training proves more effective than pure scaling for overcoming these limitations, though style transfer remains the most challenging capability across all model types. SVGenius establishes the first systematic evaluation framework for SVG processing, providing crucial insights for developing more capable vector graphics models and advancing automated graphic design applications. Appendix and supplementary materials (including all data and code) are available at https://zju-real.github.io/SVGenius.",
            "score": 11,
            "issue_id": 4137,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "c1fdc3559598aa68",
            "authors": [
                "Siqi Chen",
                "Xinyu Dong",
                "Haolei Xu",
                "Xingyu Wu",
                "Fei Tang",
                "Hang Zhang",
                "Yuchen Yan",
                "Linjuan Wu",
                "Wenqi Zhang",
                "Guiyang Hou",
                "Yongliang Shen",
                "Weiming Lu",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Zhejiang University Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03139.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#optimization",
                    "#multimodal",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "SVGenius: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¹",
                    "desc": "SVGenius - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ SVG. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2377 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ñ‚Ñ€ĞµÑ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ…: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ SVG. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾ 8 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 18 Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 24 Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ, Ğ½Ğ¾ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "SVGenius: Unveiling SVG Processing Potential in LLMs",
                    "desc": "SVGenius is a benchmark designed to evaluate the performance of Large Language Models (LLMs) and Multimodal LLMs in processing Scalable Vector Graphics (SVG). It assesses models across three key dimensions: understanding, editing, and generation, using a total of 2,377 queries derived from real-world applications. The evaluation framework includes 8 task categories and 18 metrics, highlighting the strengths and weaknesses of 22 different models. Findings indicate that while proprietary models excel, all models struggle with complex tasks, suggesting a need for improved training methods, particularly in reasoning, to enhance their capabilities."
                },
                "zh": {
                    "title": "SVGeniusï¼šå…¨é¢è¯„ä¼° SVG å¤„ç†èƒ½åŠ›çš„åŸºå‡†å·¥å…·",
                    "desc": "SVGenius æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€ LLM åœ¨ SVG å¤„ç†èƒ½åŠ›çš„åŸºå‡†å·¥å…·ã€‚å®ƒé€šè¿‡ç†è§£ã€ç¼–è¾‘å’Œç”Ÿæˆä¸‰ä¸ªç»´åº¦ï¼Œä½¿ç”¨ 2,377 ä¸ªæŸ¥è¯¢æ¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡ä¸“æœ‰æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºå¼€æºæ¨¡å‹ï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨å¤æ‚æ€§å¢åŠ æ—¶è¡¨ç°æ™®éä¸‹é™ã€‚SVGenius æä¾›äº†ä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„çŸ¢é‡å›¾å½¢æ¨¡å‹å’Œæ¨è¿›è‡ªåŠ¨åŒ–å›¾å½¢è®¾è®¡åº”ç”¨æä¾›äº†é‡è¦è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04228",
            "title": "LayerFlow: A Unified Model for Layer-aware Video Generation",
            "url": "https://huggingface.co/papers/2506.04228",
            "abstract": "LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LayerFlow, a unified solution for layer-aware video generation. Given per-layer prompts, LayerFlow generates videos for the transparent foreground, clean background, and blended scene. It also supports versatile variants like decomposing a blended video or generating the background for the given foreground and vice versa. Starting from a text-to-video diffusion transformer, we organize the videos for different layers as sub-clips, and leverage layer embeddings to distinguish each clip and the corresponding layer-wise prompts. In this way, we seamlessly support the aforementioned variants in one unified framework. For the lack of high-quality layer-wise training videos, we design a multi-stage training strategy to accommodate static images with high-quality layer annotations. Specifically, we first train the model with low-quality video data. Then, we tune a motion LoRA to make the model compatible with static frames. Afterward, we train the content LoRA on the mixture of image data with high-quality layered images along with copy-pasted video data. During inference, we remove the motion LoRA thus generating smooth videos with desired layers.",
            "score": 10,
            "issue_id": 4134,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "1e8f6532d8b54b21",
            "authors": [
                "Sihui Ji",
                "Hao Luo",
                "Xi Chen",
                "Yuanpeng Tu",
                "Yiyang Wang",
                "Hengshuang Zhao"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group, China",
                "Hupan Laboratory, China",
                "The University of Hong Kong",
                "The University of Hong Kong, Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04228.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#synthetic",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "LayerFlow: Ğ£Ğ¼Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼",
                    "desc": "LayerFlow - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾ĞµĞ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ². ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°, Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ğ½Ğ° Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ÑĞ»Ğ¾ĞµĞ². LayerFlow Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ LoRA Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "LayerFlow: Unified Layer-Aware Video Generation",
                    "desc": "LayerFlow is a comprehensive framework designed for generating videos that are aware of different layers, such as foreground and background. It utilizes a text-to-video diffusion transformer to create videos based on specific prompts for each layer, allowing for various video generation tasks. The framework can decompose blended videos or generate backgrounds for given foregrounds, making it versatile. To address the challenge of limited high-quality training data, LayerFlow employs a multi-stage training strategy that begins with low-quality videos and progressively incorporates high-quality layered images."
                },
                "zh": {
                    "title": "LayerFlowï¼šç»Ÿä¸€çš„å±‚æ„ŸçŸ¥è§†é¢‘ç”Ÿæˆæ¡†æ¶",
                    "desc": "LayerFlowæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå±‚æ„ŸçŸ¥çš„è§†é¢‘ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£å˜æ¢å™¨å’Œå±‚åµŒå…¥ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§è§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬é€æ˜å‰æ™¯ã€å¹²å‡€èƒŒæ™¯å’Œæ··åˆåœºæ™¯çš„è§†é¢‘ç”Ÿæˆã€‚é€šè¿‡å°†è§†é¢‘æŒ‰å±‚ç»„ç»‡ä¸ºå­å‰ªè¾‘ï¼Œå¹¶åˆ©ç”¨å±‚åµŒå…¥æ¥åŒºåˆ†æ¯ä¸ªå‰ªè¾‘åŠå…¶å¯¹åº”çš„å±‚çº§æç¤ºï¼ŒLayerFlowå®ç°äº†å¤šç§è§†é¢‘ç”Ÿæˆå˜ä½“ã€‚ä¸ºäº†å…‹æœé«˜è´¨é‡å±‚çº§è®­ç»ƒè§†é¢‘çš„ç¼ºä¹ï¼ŒLayerFlowè®¾è®¡äº†å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç»“åˆé™æ€å›¾åƒå’Œé«˜è´¨é‡å±‚æ³¨é‡Šè¿›è¡Œè®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24500",
            "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence",
            "url": "https://huggingface.co/papers/2505.24500",
            "abstract": "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.",
            "score": 10,
            "issue_id": 4133,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 Ğ¼Ğ°Ñ",
                "en": "May 30",
                "zh": "5æœˆ30æ—¥"
            },
            "hash": "ee580986393d0b7e",
            "authors": [
                "Guiyang Hou",
                "Xing Gao",
                "Yuchuan Wu",
                "Xiang Huang",
                "Wenqi Zhang",
                "Zhe Zheng",
                "Yongliang Shen",
                "Jialu Du",
                "Fei Huang",
                "Yongbin Li",
                "Weiming Lu"
            ],
            "affiliations": [
                "Nanjing University",
                "Tongyi Lab, Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24500.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#rl",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ÑƒÑ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ TimeHC-RL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ System 2 RL. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ TimeHC-RL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Boosting LLMs' Social Intelligence with TimeHC-RL",
                    "desc": "This paper presents a new approach called Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) to improve the social intelligence of Large Language Models (LLMs). Unlike traditional methods that focus on logical reasoning, TimeHC-RL incorporates different cognitive processes, including intuitive and deliberate thinking, to better navigate social contexts. The authors conducted experiments across various datasets and compared TimeHC-RL with existing reinforcement learning methods, demonstrating its superior performance. The findings suggest that enhancing LLMs' cognitive abilities in social domains can significantly elevate their overall intelligence and effectiveness."
                },
                "zh": {
                    "title": "æå‡ç¤¾äº¤æ™ºèƒ½çš„æ—¶é—´æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºæ—¶é—´æ„ŸçŸ¥å±‚æ¬¡è®¤çŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆTimeHC-RLï¼‰ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾äº¤é¢†åŸŸçš„æ™ºèƒ½ã€‚ä¸æ•°å­¦ç­‰ä¾èµ–ç³»ç»Ÿ2è®¤çŸ¥çš„é¢†åŸŸä¸åŒï¼Œç¤¾äº¤é¢†åŸŸéœ€è¦æ›´ä¸°å¯Œçš„è®¤çŸ¥æ¨¡å¼ï¼ŒåŒ…æ‹¬ç›´è§‰ååº”å’Œè¡¨å±‚æ€ç»´ã€‚é€šè¿‡å¯¹å…«ä¸ªä¸åŒæ•°æ®é›†çš„å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†TimeHC-RLæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨ç¤¾äº¤æ™ºèƒ½æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„ç³»ç»Ÿ2å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿å¾—7BåŸºç¡€æ¨¡å‹çš„è¡¨ç°æ¥è¿‘äºæ›´å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚DeepSeek-R1å’ŒOpenAI-O3ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03295",
            "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem",
            "url": "https://huggingface.co/papers/2506.03295",
            "abstract": "Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs.",
            "score": 8,
            "issue_id": 4135,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "03df39687a4bdf5a",
            "authors": [
                "Yubo Wang",
                "Ping Nie",
                "Kai Zou",
                "Lijun Wu",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Independent",
                "Netmind.AI",
                "Shanghai AI Lab",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03295.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Critique Fine-Tuning (CFT) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). CFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ñ… Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CFT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ CFT ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Unlocking Reasoning Power with Efficient Critique Fine-Tuning",
                    "desc": "This paper introduces Critique Fine-Tuning (CFT) as a method to enhance the reasoning abilities of large language models (LLMs) like Qwen and Llama. By focusing on a single problem, CFT generates critique data from various model-generated solutions, which are then used to fine-tune the models. The results show that this approach leads to significant performance improvements on reasoning tasks with much lower computational costs compared to traditional reinforcement learning methods. The findings suggest that CFT is a robust and efficient strategy for maximizing the reasoning potential of LLMs."
                },
                "zh": {
                    "title": "æ‰¹è¯„å¾®è°ƒï¼šé«˜æ•ˆé‡Šæ”¾è¯­è¨€æ¨¡å‹æ¨ç†æ½œåŠ›çš„åˆ©å™¨",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ‰¹è¯„å¾®è°ƒï¼ˆCritique Fine-Tuning, CFTï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨é«˜æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹å•ä¸€é—®é¢˜è¿›è¡Œå¾®è°ƒï¼ŒCFTèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—æˆæœ¬ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨CFTæ–¹æ³•ï¼Œæ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº†15%åˆ°16%ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒCFTåœ¨è®¡ç®—èµ„æºä¸Šæ›´åŠ é«˜æ•ˆï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºä¸€ç§ç®€å•ä¸”é€šç”¨çš„æ¨ç†èƒ½åŠ›æå‡ç­–ç•¥çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04158",
            "title": "Image Editing As Programs with Diffusion Models",
            "url": "https://huggingface.co/papers/2506.04158",
            "abstract": "While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at https://github.com/YujiaHu1109/IEAP.",
            "score": 7,
            "issue_id": 4133,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "e5a32d484bb427f3",
            "authors": [
                "Yujia Hu",
                "Songhua Liu",
                "Zhenxiong Tan",
                "Xingyi Yang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04158.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#benchmark",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ˜Ğ˜-Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ IEAP (Image Editing As Programs). Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Diffusion Transformer Ğ¸ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ DiT. IEAP Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Programmatic Precision",
                    "desc": "This paper addresses the limitations of diffusion models in instruction-driven image editing, particularly when it comes to making significant layout changes. The authors propose a new framework called Image Editing As Programs (IEAP), which utilizes the Diffusion Transformer (DiT) architecture to break down complex editing tasks into simpler, atomic operations. Each operation is executed by a lightweight adapter that specializes in a specific type of edit, allowing for more flexible and accurate transformations. The framework shows improved performance over existing methods, achieving higher accuracy and semantic fidelity in various editing scenarios, especially for complex instructions."
                },
                "zh": {
                    "title": "å›¾åƒç¼–è¾‘çš„æ–°æ–¹æ³•ï¼šå°†å¤æ‚æŒ‡ä»¤è½¬åŒ–ä¸ºç®€å•æ“ä½œ",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œç§°ä¸ºå›¾åƒç¼–è¾‘ä½œä¸ºç¨‹åºï¼ˆIEAPï¼‰ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹åœ¨æŒ‡ä»¤é©±åŠ¨çš„å›¾åƒç¼–è¾‘ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚IEAPåŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¶æ„ï¼Œé€šè¿‡å°†å¤æ‚çš„ç¼–è¾‘æŒ‡ä»¤åˆ†è§£ä¸ºä¸€ç³»åˆ—åŸå­æ“ä½œæ¥å®ç°ã€‚æ¯ä¸ªæ“ä½œç”±è½»é‡çº§é€‚é…å™¨å®ç°ï¼Œä¸“é—¨é’ˆå¯¹ç‰¹å®šç±»å‹çš„ç¼–è¾‘ï¼Œèƒ½å¤Ÿæ”¯æŒä»»æ„å’Œç»“æ„ä¸ä¸€è‡´çš„å˜æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIEAPåœ¨å„ç§ç¼–è¾‘åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤æŒ‡ä»¤æ—¶è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03517",
            "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models",
            "url": "https://huggingface.co/papers/2506.03517",
            "abstract": "Direct Preference Optimization (DPO) has recently been applied as a post-training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain fewer visual artifacts. In this work, we introduce DenseDPO, a method that addresses these shortcomings by making three contributions. First, we create each video pair for DPO by denoising corrupted copies of a ground truth video. This results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias. Second, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding a denser and more precise learning signal. With only one-third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency. Finally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels.",
            "score": 6,
            "issue_id": 4137,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "5be6b9aa57297fb6",
            "authors": [
                "Ziyi Wu",
                "Anil Kag",
                "Ivan Skorokhodov",
                "Willi Menapace",
                "Ashkan Mirzaei",
                "Igor Gilitschenski",
                "Sergey Tulyakov",
                "Aliaksandr Siarohin"
            ],
            "affiliations": [
                "Snap Research",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03517.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#rlhf"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "DenseDPO: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "DenseDPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ²Ğ¸Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. DenseDPO ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ° Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¿Ğ¸Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ…, Ğ° Ğ½Ğµ Ğ½Ğ° Ñ†ĞµĞ»Ñ‹Ñ… ĞºĞ»Ğ¸Ğ¿Ğ°Ñ…. DenseDPO Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ DPO, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ñ€ĞµÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Video Generation with DenseDPO: Precision and Efficiency in Preference Learning",
                    "desc": "This paper introduces DenseDPO, an improved method for Direct Preference Optimization (DPO) in text-to-video diffusion models. DenseDPO addresses the limitations of traditional DPO by creating video pairs from denoised versions of a ground truth video, allowing for better alignment and reducing bias towards low-motion clips. It also enables preference labeling on shorter video segments, which provides a more detailed learning signal while using less labeled data. Additionally, DenseDPO facilitates automatic preference annotation through Vision Language Models, achieving performance comparable to human-labeled data."
                },
                "zh": {
                    "title": "DenseDPOï¼šæå‡è§†é¢‘ç”Ÿæˆçš„åå¥½ä¼˜åŒ–æ–¹æ³•",
                    "desc": "ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æœ€è¿‘è¢«åº”ç”¨äºæ–‡æœ¬åˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹åè®­ç»ƒæŠ€æœ¯ã€‚æˆ‘ä»¬æå‡ºçš„DenseDPOæ–¹æ³•é€šè¿‡ä¸‰é¡¹è´¡çŒ®è§£å†³äº†DPOçš„ä¸è¶³ä¹‹å¤„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å»å™ªçœŸå®è§†é¢‘çš„æŸåå‰¯æœ¬æ¥åˆ›å»ºè§†é¢‘å¯¹ï¼Œä»è€Œæ¶ˆé™¤äº†è¿åŠ¨åå·®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨æ—¶é—´å¯¹é½æ¥æ ‡è®°çŸ­ç‰‡æ®µçš„åå¥½ï¼Œä½¿å­¦ä¹ ä¿¡å·æ›´åŠ å¯†é›†å’Œç²¾ç¡®ï¼Œæœ€ç»ˆDenseDPOåœ¨è¿åŠ¨ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»ŸDPOï¼ŒåŒæ—¶åœ¨æ–‡æœ¬å¯¹é½ã€è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢è¡¨ç°ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03099",
            "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models",
            "url": "https://huggingface.co/papers/2506.03099",
            "abstract": "TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/",
            "score": 5,
            "issue_id": 4134,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "eff27ca5fef5cdcf",
            "authors": [
                "Chetwin Low",
                "Weimin Wang"
            ],
            "affiliations": [
                "Character AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03099.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#inference",
                    "#games",
                    "#audio",
                    "#video",
                    "#optimization"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞĞ¶Ğ¸Ğ²Ğ»ÑĞµĞ¼ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹: Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "TalkingMachines - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ DiT Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ 18 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½ÑƒÑ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. TalkingMachines Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ÑĞ´ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "Transforming Audio into Real-Time Avatar Animation",
                    "desc": "TalkingMachines is a novel framework that converts existing image-to-video models into real-time, audio-responsive avatar generators. It combines a large language model (LLM) with a video generation foundation model to create engaging conversational avatars. The framework features a significant adaptation of a state-of-the-art (SOTA) image-to-video model, allowing for efficient infinite video streaming through advanced knowledge distillation techniques. Additionally, it incorporates engineering optimizations to enhance performance, such as distributing processing across devices and minimizing computation delays."
                },
                "zh": {
                    "title": "å®æ—¶éŸ³é¢‘é©±åŠ¨çš„è§’è‰²åŠ¨ç”»ç”Ÿæˆå™¨",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†TalkingMachinesï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ¡†æ¶ï¼Œå°†é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è½¬å˜ä¸ºå®æ—¶çš„éŸ³é¢‘é©±åŠ¨è§’è‰²åŠ¨ç”»ç”Ÿæˆå™¨ã€‚é€šè¿‡å°†éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹ç»“åˆï¼ŒTalkingMachinesèƒ½å¤Ÿå®ç°è‡ªç„¶çš„å¯¹è¯ä½“éªŒã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šå°†ä¸€ä¸ªé¢„è®­ç»ƒçš„æœ€å…ˆè¿›çš„å›¾åƒåˆ°è§†é¢‘æ¨¡å‹é€‚é…ä¸ºä¸€ä¸ªå…·æœ‰180äº¿å‚æ•°çš„éŸ³é¢‘é©±åŠ¨å¤´åƒç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠé€šè¿‡ä¸å¯¹ç§°çŸ¥è¯†è’¸é¦å®ç°æ— é™è§†é¢‘æµçš„ç”Ÿæˆã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªé«˜ååé‡ã€ä½å»¶è¿Ÿçš„æ¨ç†ç®¡é“ï¼Œç»“åˆäº†å¤šé¡¹å…³é”®çš„å·¥ç¨‹ä¼˜åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02592",
            "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments",
            "url": "https://huggingface.co/papers/2506.02592",
            "abstract": "The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at https://github.com/zhiyuanc2001/self-preference.",
            "score": 5,
            "issue_id": 4134,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "ccdb2761a23fe0c8",
            "authors": [
                "Zhi-Yuan Chen",
                "Hao Wang",
                "Xinyu Zhang",
                "Enrui Hu",
                "Yankai Lin"
            ],
            "affiliations": [
                "Beijing Key Laboratory of Research on Large Models and Intelligent Governance",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Huawei Poisson Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02592.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#hallucinations",
                    "#interpretability",
                    "#ethics",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "DBG: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ DBG, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ñ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ LLM Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Measuring Self-Preference Bias with the DBG Score",
                    "desc": "This paper introduces the DBG score, a new metric designed to measure self-preference bias in large language models (LLMs) while accounting for response quality. Traditional methods for assessing this bias often confuse it with the quality of the responses, as higher quality can lead to misleading score differences. By using gold judgments as benchmarks for response quality, the DBG score effectively isolates self-preference bias from quality effects. The authors conduct experiments across various LLMs and examine factors that influence bias, providing insights into the mechanisms behind self-preference in model responses."
                },
                "zh": {
                    "title": "å¼•å…¥DBGè¯„åˆ†ï¼Œç²¾å‡†æµ‹é‡è‡ªæˆ‘åå¥½åå·®",
                    "desc": "æœ¬æ–‡æå‡ºäº†DBGè¯„åˆ†ï¼Œç”¨äºæµ‹é‡å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è‡ªæˆ‘åå¥½åå·®ã€‚é€šè¿‡ä½¿ç”¨é‡‘æ ‡å‡†åˆ¤æ–­ä½œä¸ºå“åº”è´¨é‡çš„ä»£ç†ï¼ŒDBGè¯„åˆ†è§£å†³äº†å“åº”è´¨é‡å¯¹åå·®æµ‹é‡çš„æ··æ·†æ•ˆåº”ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨è¯„ä¼°è‡ªæˆ‘åå¥½åå·®æ—¶ï¼Œå¾€å¾€å°†å…¶ä¸å“åº”è´¨é‡æ··ä¸ºä¸€è°ˆã€‚æˆ‘ä»¬é€šè¿‡å®éªŒè¯„ä¼°äº†ä¸åŒç‰ˆæœ¬ã€è§„æ¨¡å’Œæ¨ç†èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘åå¥½åå·®ï¼Œå¹¶æ¢è®¨äº†å½±å“è¯¥åå·®çš„å› ç´ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03956",
            "title": "Adapt before Continual Learning",
            "url": "https://huggingface.co/papers/2506.03956",
            "abstract": "Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Continual Learning (CL) seeks to enable neural networks to incrementally acquire new knowledge (plasticity) while retaining existing knowledge (stability). While pre-trained models (PTMs) have become pivotal in CL, prevailing approaches freeze the PTM backbone to preserve stability, limiting their plasticity, particularly when encountering significant domain gaps in incremental tasks. Conversely, sequentially finetuning the entire PTM risks catastrophic forgetting of generalizable knowledge, exposing a critical stability-plasticity trade-off. To address this challenge, we propose Adapting PTMs before the core CL process (ACL), a novel framework that refines the PTM backbone through a plug-and-play adaptation phase before learning each new task with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by aligning embeddings with their original class prototypes while distancing them from others, theoretically and empirically shown to balance stability and plasticity. Extensive experiments demonstrate that ACL significantly improves CL performance across benchmarks and integrated methods, offering a versatile solution for PTM-based CL.",
            "score": 4,
            "issue_id": 4138,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "33f8cb4049923c1f",
            "authors": [
                "Aojun Lu",
                "Tao Feng",
                "Hangjie Yuan",
                "Chunhui Ding",
                "Yanan Sun"
            ],
            "affiliations": [
                "College of Computer Science Sichuan University Chengdu, China",
                "College of Computer Science and Technology Zhejiang University Hangzhou, China",
                "Department of Computer Science and Technology Tsinghua University Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03956.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Continual Learning) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ACL (Adapting Pre-trained Models before the core CL process), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ACL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ñ Ğ¸Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ACL Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Learning Flexibility with Pre-trained Models",
                    "desc": "This paper introduces a new method called Adapting Pre-trained Models before the core Continual Learning (CL) process, which aims to improve how neural networks learn new information while keeping what they already know. The authors highlight the common issue where pre-trained models are often frozen to maintain stability, which limits their ability to adapt to new tasks. Their approach involves refining the pre-trained model before learning new tasks, allowing for better alignment of knowledge and reducing the risk of forgetting previous information. The results show that this method enhances the performance of CL systems, making it a promising solution for integrating pre-trained models in continual learning scenarios."
                },
                "zh": {
                    "title": "æå‡æŒç»­å­¦ä¹ çš„å¯å¡‘æ€§ä¸ç¨³å®šæ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºåœ¨æ ¸å¿ƒæŒç»­å­¦ä¹ è¿‡ç¨‹ä¹‹å‰è°ƒæ•´é¢„è®­ç»ƒæ¨¡å‹ï¼ˆACLï¼‰ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æé«˜ç¥ç»ç½‘ç»œçš„å¯å¡‘æ€§ï¼ŒåŒæ—¶ä¿æŒå…¶ç¨³å®šæ€§ï¼Œä»¥ä¾¿åœ¨å¢é‡å­¦ä¹ ä¸­æ›´å¥½åœ°é€‚åº”æ–°çŸ¥è¯†ã€‚é€šè¿‡åœ¨å­¦ä¹ æ¯ä¸ªæ–°ä»»åŠ¡ä¹‹å‰å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé€‚åº”æ€§è°ƒæ•´ï¼ŒACLèƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹é½åµŒå…¥ä¸åŸå§‹ç±»åˆ«åŸå‹ï¼Œä»è€Œå‡å°‘ç¾éš¾æ€§é—å¿˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒACLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æŒç»­å­¦ä¹ çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03106",
            "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
            "url": "https://huggingface.co/papers/2506.03106",
            "abstract": "Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration.",
            "score": 4,
            "issue_id": 4134,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "99f5fb3b08ab4205",
            "authors": [
                "Xiaoying Zhang",
                "Hao Sun",
                "Yipeng Zhang",
                "Kaituo Feng",
                "Chaochao Lu",
                "Chao Yang",
                "Helen Meng"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong, HCCL",
                "The Chinese University of Hong Kong, MMLab",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03106.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Critique-GRPO: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Critique-GRPO - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²ÑƒÑ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Critique-GRPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Critique-GRPO: A Dual Feedback Approach",
                    "desc": "Critique-GRPO is a reinforcement learning (RL) framework that enhances the reasoning abilities of large language models (LLMs) by combining numerical and natural language feedback. It addresses challenges faced by traditional RL methods that rely solely on numerical feedback, such as performance plateaus and ineffective self-reflection. By incorporating critiques in natural language, Critique-GRPO allows models to refine their responses and improve their performance on difficult tasks. Experimental results show that this approach significantly outperforms existing fine-tuning methods, achieving better results in various reasoning tasks."
                },
                "zh": {
                    "title": "Critique-GRPOï¼šè‡ªç„¶è¯­è¨€ä¸æ•°å€¼åé¦ˆçš„å®Œç¾ç»“åˆ",
                    "desc": "Critique-GRPOæ˜¯ä¸€ç§ç»“åˆæ•°å€¼åé¦ˆå’Œè‡ªç„¶è¯­è¨€åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä»…ä¾èµ–æ•°å€¼åé¦ˆæ—¶é‡åˆ°çš„æ€§èƒ½åœæ»ã€è‡ªæˆ‘åæ€æ•ˆæœæœ‰é™å’ŒæŒç»­å¤±è´¥ç­‰æŒ‘æˆ˜ã€‚é€šè¿‡åˆ©ç”¨è‡ªç„¶è¯­è¨€åé¦ˆï¼ŒCritique-GRPOèƒ½å¤Ÿåœ¨æ¨¡å‹è¡¨ç°åœæ»æ—¶ï¼Œç”Ÿæˆæ­£ç¡®çš„æ”¹è¿›å»ºè®®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCritique-GRPOåœ¨å¤šä¸ªå¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å¹³å‡é€šè¿‡ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04142",
            "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
            "url": "https://huggingface.co/papers/2506.04142",
            "abstract": "A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient (rho) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation",
            "score": 3,
            "issue_id": 4137,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "4799bf9d7a1cb57f",
            "authors": [
                "Kejian Zhu",
                "Shangqing Tu",
                "Zhuoran Jin",
                "Lei Hou",
                "Juanzi Li",
                "Jun Zhao"
            ],
            "affiliations": [
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04142.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#hallucinations",
                    "#benchmark",
                    "#ethics",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ‚Ñ‡Ğ¸Ğ½Ğ³ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ²-ÑˆĞ¾Ñ€Ñ‚ĞºĞ°Ñ‚Ğ¾Ğ²",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ 'shortcut neuron patching' Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹-ÑˆĞ¾Ñ€Ñ‚ĞºĞ°Ñ‚Ñ‹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ¼ MixEval, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¡Ğ¿Ğ¸Ñ€Ğ¼ĞµĞ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ 0,95. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞµĞ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸."
                },
                "en": {
                    "title": "Suppressing Shortcut Neurons for Trustworthy Evaluations",
                    "desc": "This paper presents a method called shortcut neuron patching, which aims to identify and suppress shortcut neurons in language models to improve the reliability of evaluations. The authors highlight that current evaluation methods often suffer from data contamination, leading to unfair assessments of model performance. By analyzing the internal mechanisms of contaminated models, they find that shortcut solutions during training contribute to overestimation of model capabilities. Their proposed method effectively mitigates these issues, showing strong correlation with established trustworthy benchmarks, thus ensuring more accurate evaluations of language models."
                },
                "zh": {
                    "title": "æŠ‘åˆ¶å¿«æ·ç¥ç»å…ƒï¼Œæå‡è¯„ä¼°å¯ä¿¡åº¦",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¿«æ·ç¥ç»å…ƒä¿®è¡¥çš„æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«å’ŒæŠ‘åˆ¶è¯­è¨€æ¨¡å‹ä¸­çš„å¿«æ·ç¥ç»å…ƒï¼Œä»¥å‡è½»æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œä»è€Œæé«˜è¯„ä¼°çš„å¯ä¿¡åº¦ã€‚å½“å‰çš„è¯„ä¼°æ–¹æ³•å¤§å¤šä¾èµ–å…¬å…±åŸºå‡†ï¼Œä½†è¿™äº›åŸºå‡†å®¹æ˜“å—åˆ°æ•°æ®æ±¡æŸ“çš„å½±å“ï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å…¬å¹³ã€‚æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒå’Œå› æœåˆ†æï¼Œå‘ç°è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹å‚æ•°å¯èƒ½ä¼šè·å¾—å¿«æ·è§£å†³æ–¹æ¡ˆï¼Œä»è€Œå¯¼è‡´å¯¹æ±¡æŸ“æ¨¡å‹çš„è¿‡é«˜ä¼°è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡è½»æ±¡æŸ“æ–¹é¢æœ‰æ•ˆï¼Œå¹¶ä¸”ä¸MixEvalåŸºå‡†çš„è¯„ä¼°ç»“æœå…·æœ‰å¾ˆå¼ºçš„çº¿æ€§ç›¸å…³æ€§ï¼ŒSpearmanç³»æ•°è¶…è¿‡0.95ï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤ŸçœŸå®åæ˜ æ¨¡å‹çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04141",
            "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos",
            "url": "https://huggingface.co/papers/2506.04141",
            "abstract": "A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.  \t\t\t\t\tAI-generated summary \t\t\t\t The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as \"question frame\") and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities.",
            "score": 3,
            "issue_id": 4137,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "a12411c7424fd2a7",
            "authors": [
                "Kejian Zhu",
                "Zhuoran Jin",
                "Hongbang Yuan",
                "Jiachun Li",
                "Shangqing Tu",
                "Pengfei Cao",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao"
            ],
            "affiliations": [
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04141.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#long_context",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "MMR-V: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MMR-V Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ², ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ ÑƒĞ¿Ğ¾Ğ¼ÑĞ½ÑƒÑ‚Ñ‹Ñ… Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞµ, Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ - Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸ÑˆÑŒ 52.5%. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñƒ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "MMR-V: Elevating Multimodal Reasoning in Videos",
                    "desc": "The paper introduces MMR-V, a new benchmark designed to test the capabilities of multimodal large language models (MLLMs) in video analysis. It emphasizes the need for long-range, multi-frame reasoning, where models must analyze evidence that is not immediately adjacent to the question frame. Unlike existing benchmarks that focus on simple understanding tasks, MMR-V requires models to reason about hidden information and avoid shortcuts through distractor annotations. The findings show that current models struggle with these challenges, achieving only modest accuracy, highlighting the need for further research in enhancing multimodal reasoning skills."
                },
                "zh": {
                    "title": "MMR-Vï¼šæ¨åŠ¨å¤šæ¨¡æ€æ¨ç†çš„æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†MMR-Vï¼Œæ—¨åœ¨æŒ‘æˆ˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ä¸­çš„é•¿è·ç¦»ã€å¤šå¸§æ¨ç†å’Œéšè—ä¿¡æ¯å¤„ç†èƒ½åŠ›ã€‚ç°æœ‰çš„è§†é¢‘åŸºå‡†ä¸»è¦é›†ä¸­åœ¨ç†è§£ä»»åŠ¡ä¸Šï¼Œè€ŒMMR-Vè¦æ±‚æ¨¡å‹è¿›è¡Œæ›´å¤æ‚çš„æ¨ç†ï¼Œåˆ†æä¸é—®é¢˜å¸§ç›¸è·è¾ƒè¿œçš„è¯æ®å¸§ã€‚è¯¥åŸºå‡†åŒ…å«317ä¸ªè§†é¢‘å’Œ1257ä¸ªä»»åŠ¡ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå½“å‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œæœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡ä»…ä¸º52.5%ã€‚æˆ‘ä»¬å¸Œæœ›MMR-Vèƒ½å¤Ÿæ¿€å‘è¿›ä¸€æ­¥ç ”ç©¶ï¼Œä»¥æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04108",
            "title": "Rectified Sparse Attention",
            "url": "https://huggingface.co/papers/2506.04108",
            "abstract": "Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42times end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM.",
            "score": 3,
            "issue_id": 4134,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "ff7222f16cd2bf28",
            "authors": [
                "Yutao Sun",
                "Tianzhu Ye",
                "Li Dong",
                "Yuqing Xia",
                "Jian Chen",
                "Yizhao Gao",
                "Shijie Cao",
                "Jianyong Wang",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04108.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#long_context",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "ĞœĞµÑ‚Ğ¾Ğ´ Rectified Sparse Attention (ReSA) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾-Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºÑ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ReSA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ğº Ğ±ĞµĞ·Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 2,42 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 256 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Boosting Efficiency in Long-Sequence Generation with ReSA",
                    "desc": "Rectified Sparse Attention (ReSA) enhances the efficiency of generating long sequences in Large Language Models by integrating block-sparse attention with periodic dense rectification. This approach addresses the issue of KV cache misalignment that can lead to errors and reduced quality in generated outputs. By periodically refreshing the KV cache through a dense forward pass, ReSA minimizes error accumulation and maintains alignment with the model's pretraining data. Experimental results show that ReSA not only preserves high-quality generation but also achieves significant speed improvements, making it a viable option for long-context tasks."
                },
                "zh": {
                    "title": "é«˜æ•ˆé•¿åºåˆ—ç”Ÿæˆçš„æ–°æ–¹æ³•ï¼šReSA",
                    "desc": "Rectified Sparse Attentionï¼ˆReSAï¼‰æ˜¯ä¸€ç§æé«˜å¤§å‹è¯­è¨€æ¨¡å‹é•¿åºåˆ—ç”Ÿæˆæ•ˆç‡çš„æ–¹æ³•ã€‚å®ƒç»“åˆäº†å—ç¨€ç–æ³¨æ„åŠ›å’Œå‘¨æœŸæ€§å¯†é›†æ•´æµï¼Œèƒ½å¤Ÿä¿æŒé«˜è´¨é‡çš„ç”Ÿæˆæ•ˆæœã€‚é€šè¿‡åœ¨å›ºå®šé—´éš”å†…ä½¿ç”¨å¯†é›†å‰å‘ä¼ é€’åˆ·æ–°KVç¼“å­˜ï¼ŒReSAé™åˆ¶äº†è¯¯å·®ç´¯ç§¯ï¼Œå¹¶ä¿æŒä¸é¢„è®­ç»ƒåˆ†å¸ƒçš„å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒReSAåœ¨æ•°å­¦æ¨ç†ã€è¯­è¨€å»ºæ¨¡å’Œæ£€ç´¢ä»»åŠ¡ä¸­å®ç°äº†æ¥è¿‘æ— æŸçš„ç”Ÿæˆè´¨é‡ï¼Œå¹¶åœ¨256Kåºåˆ—é•¿åº¦ä¸‹æä¾›äº†é«˜è¾¾2.42å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03448",
            "title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions",
            "url": "https://huggingface.co/papers/2506.03448",
            "abstract": "RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editing single, prominent objects but significantly struggle when applied to complex scenes containing multiple entities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous real-world benchmark rooted in RefCOCO, where even baselines trained on millions of samples perform poorly. To overcome this limitation, we introduce RefEdit -- an instruction-based editing model trained on our scalable synthetic data generation pipeline. Our RefEdit, trained on only 20,000 editing triplets, outperforms the Flux/SD3 model-based baselines trained on millions of data. Extensive evaluations across various benchmarks demonstrate that our model not only excels in referring expression tasks but also enhances performance on traditional benchmarks, achieving state-of-the-art results comparable to closed-source methods. We release data \\& checkpoint for reproducibility.",
            "score": 3,
            "issue_id": 4134,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "717f877ff02ce882",
            "authors": [
                "Bimsara Pathiraja",
                "Maitreya Patel",
                "Shivam Singh",
                "Yezhou Yang",
                "Chitta Baral"
            ],
            "affiliations": [
                "Arizona State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03448.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#synthetic",
                    "#cv",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "RefEdit: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "RefEdit - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RefEdit-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. RefEdit, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 20 000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Instruction-Based Learning",
                    "desc": "RefEdit is a new model designed for editing images based on instructions, specifically focusing on complex scenes with multiple objects. Unlike previous models that struggle with such tasks, RefEdit is trained on a unique synthetic data generation pipeline, allowing it to learn effectively from a smaller dataset of 20,000 editing examples. The model significantly outperforms existing baselines, which were trained on millions of samples, in both referring expression tasks and traditional editing benchmarks. This advancement highlights the potential of instruction-based editing in achieving high performance in challenging image editing scenarios."
                },
                "zh": {
                    "title": "RefEditï¼šå¤æ‚åœºæ™¯ç¼–è¾‘çš„æ–°çªç ´",
                    "desc": "RefEditæ˜¯ä¸€ç§åŸºäºæŒ‡ä»¤çš„ç¼–è¾‘æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹å¤æ‚åœºæ™¯ä¸­çš„ç¼–è¾‘ä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒRefEditåœ¨å¤„ç†å¤šä¸ªå®ä½“çš„å¤æ‚åœºæ™¯æ—¶è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†RefEdit-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºRefCOCOçš„çœŸå®ä¸–ç•ŒåŸºå‡†ï¼Œç”¨äºé‡åŒ–ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚é€šè¿‡ä½¿ç”¨åˆæˆæ•°æ®ç”Ÿæˆç®¡é“ï¼ŒRefEditåœ¨ä»…ä½¿ç”¨20,000ä¸ªç¼–è¾‘ä¸‰å…ƒç»„çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†åŸºäºFlux/SD3æ¨¡å‹çš„åŸºçº¿ï¼Œå±•ç¤ºäº†å…¶åœ¨æŒ‡ä»£è¡¨è¾¾ä»»åŠ¡å’Œä¼ ç»ŸåŸºå‡†ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02945",
            "title": "Quantitative LLM Judges",
            "url": "https://huggingface.co/papers/2506.02945",
            "abstract": "LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve the score of the original judge by using the judge's textual evaluation and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in most applications of our work. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can effectively improve the predictive power of existing judges through post-hoc modeling.",
            "score": 3,
            "issue_id": 4133,
            "pub_date": "2025-06-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ½Ñ",
                "en": "June 3",
                "zh": "6æœˆ3æ—¥"
            },
            "hash": "de4ea9c8e4abb76a",
            "authors": [
                "Aishwarya Sahoo",
                "Jeevana Kruthi Karnuthala",
                "Tushar Parmanand Budhwani",
                "Pranchal Agarwal",
                "Sankaran Vaidyanathan",
                "Alexa Siu",
                "Franck Dernoncourt",
                "Jennifer Healey",
                "Nedim Lipka",
                "Ryan Rossi",
                "Uttaran Bhattacharya",
                "Branislav Kveton"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Massachusetts Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02945.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#alignment",
                    "#rlhf",
                    "#dataset"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "LLM-ÑÑƒĞ´ÑŒĞ¸: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº LLM-as-a-judge, Ğ³Ğ´Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… LLM-ÑÑƒĞ´ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑƒĞ´ĞµĞ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑƒĞ´ÑŒĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑÑƒĞ´ÑŒĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¸Ğ»Ñƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑƒĞ´ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ."
                },
                "en": {
                    "title": "Enhancing LLM Evaluation with Quantitative Judges",
                    "desc": "The paper introduces a framework called LLM-as-a-judge, where a large language model (LLM) assesses the outputs of another LLM. It focuses on creating quantitative LLM judges that align their evaluation scores with human assessments using regression models. These models enhance the original judge's scoring by leveraging its textual evaluations and scores. The framework is shown to be more computationally and statistically efficient than traditional supervised fine-tuning, especially when human feedback is scarce, and is validated through experiments on multiple datasets."
                },
                "zh": {
                    "title": "åˆ©ç”¨LLMæå‡è¯„ä¼°æ•ˆç‡çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLLM-as-a-judgeçš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è¯„ä¼°å¦ä¸€ä¸ªLLMçš„è¾“å‡ºã€‚æˆ‘ä»¬å¼•å…¥äº†å®šé‡LLMè¯„ä¼°è€…ï¼Œé€šè¿‡å›å½’æ¨¡å‹å°†ç°æœ‰è¯„ä¼°è€…çš„è¯„åˆ†ä¸äººç±»è¯„åˆ†å¯¹é½ã€‚è¯¥æ¨¡å‹é€šè¿‡ä½¿ç”¨è¯„ä¼°è€…çš„æ–‡æœ¬è¯„ä»·å’Œè¯„åˆ†æ¥æé«˜åŸå§‹è¯„ä¼°è€…çš„è¯„åˆ†ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¼˜äºç›‘ç£å¾®è°ƒï¼Œå¹¶ä¸”åœ¨äººå·¥åé¦ˆæœ‰é™çš„æƒ…å†µä¸‹ï¼Œç»Ÿè®¡æ•ˆç‡æ›´é«˜ï¼Œé€‚ç”¨äºå¤§å¤šæ•°åº”ç”¨åœºæ™¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.00482",
            "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation",
            "url": "https://huggingface.co/papers/2506.00482",
            "abstract": "BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.",
            "score": 3,
            "issue_id": 4137,
            "pub_date": "2025-05-31",
            "pub_date_card": {
                "ru": "31 Ğ¼Ğ°Ñ",
                "en": "May 31",
                "zh": "5æœˆ31æ—¥"
            },
            "hash": "0f7e970118d80f26",
            "authors": [
                "Eunsu Kim",
                "Haneul Yoo",
                "Guijin Son",
                "Hitesh Patel",
                "Amit Agarwal",
                "Alice Oh"
            ],
            "affiliations": [
                "KAIST",
                "OnelineAI",
                "Oracle",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.00482.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#survey",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "BenchHub: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "BenchHub - ÑÑ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ¾ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ 303 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· 38 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. BenchHub Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ¸Ğ»Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ğ¼ĞµĞ½-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ğ½Ğ³Ğ°."
                },
                "en": {
                    "title": "BenchHub: Streamlining Domain-Specific Evaluations for LLMs",
                    "desc": "BenchHub is a repository designed to organize and classify datasets specifically for evaluating large language models (LLMs). It addresses the challenge of scattered and hard-to-manage datasets, which complicate domain-specific evaluations. By aggregating 303K questions across 38 benchmarks, BenchHub allows for flexible and customizable assessments tailored to various domains. The paper highlights the importance of domain-aware benchmarking, showing that model performance can vary significantly based on the specific dataset used."
                },
                "zh": {
                    "title": "BenchHubï¼šæå‡è¯­è¨€æ¨¡å‹è¯„ä¼°çš„åŠ¨æ€åŸºå‡†åº“",
                    "desc": "BenchHubæ˜¯ä¸€ä¸ªåŠ¨æ€åŸºå‡†åº“ï¼Œä¸“é—¨ç”¨äºèšåˆå’Œåˆ†ç±»å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°æ®é›†ï¼Œæ—¨åœ¨ä¿ƒè¿›ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°å¹¶æ”¹å–„æ¨¡å‹æ¯”è¾ƒã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸æ–­è¿›æ­¥ï¼Œæ›´æ–°å’Œç»„ç»‡è‰¯å¥½çš„åŸºå‡†å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚BenchHubé›†æˆäº†æ¥è‡ª38ä¸ªåŸºå‡†çš„303Ké—®é¢˜ï¼Œæ”¯æŒæŒç»­æ›´æ–°å’Œå¯æ‰©å±•çš„æ•°æ®ç®¡ç†ï¼Œå…è®¸æ ¹æ®ä¸åŒé¢†åŸŸæˆ–ç”¨ä¾‹è¿›è¡Œçµæ´»çš„è¯„ä¼°ã€‚é€šè¿‡å¯¹ä¸åŒè¯­è¨€æ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ¨¡å‹æ€§èƒ½åœ¨ç‰¹å®šé¢†åŸŸå­é›†ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ï¼Œå¼ºè°ƒäº†é¢†åŸŸæ„ŸçŸ¥åŸºå‡†çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.21541",
            "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers",
            "url": "https://huggingface.co/papers/2505.21541",
            "abstract": "DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose.",
            "score": 3,
            "issue_id": 4133,
            "pub_date": "2025-05-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ",
                "en": "May 24",
                "zh": "5æœˆ24æ—¥"
            },
            "hash": "cda6015909393ad0",
            "authors": [
                "Zitong Wang",
                "Hang Zhao",
                "Qianyu Zhou",
                "Xuequan Lu",
                "Xiangtai Li",
                "Yiren Song"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.21541.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#open_source",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "DiffDecompose - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¸. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒĞ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞ»Ğ¸ÑÑŒ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ AlphaBlend Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. DiffDecompose Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Image Layer Decomposition with DiffDecompose",
                    "desc": "This paper introduces DiffDecompose, a novel framework that uses diffusion Transformers to decompose images into their individual layers, particularly focusing on transparent and semi-transparent layers. The authors highlight the limitations of existing methods in handling complex occlusions and propose a new dataset called AlphaBlend, which is designed to support various real-world image decomposition tasks. DiffDecompose employs In-Context Decomposition to predict multiple layers without needing direct supervision for each layer, enhancing its ability to generalize across different scenarios. The framework's effectiveness is validated through extensive experiments on the AlphaBlend dataset and the public LOGO dataset, showcasing its potential in image processing applications."
                },
                "zh": {
                    "title": "é€æ˜å±‚åˆ†è§£çš„æ–°çªç ´ï¼šDiffDecompose",
                    "desc": "DiffDecompose æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£ Transformer çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†å›¾åƒåˆ†è§£ä¸ºç»„æˆå±‚ï¼Œå¹¶ä½¿ç”¨è¯­ä¹‰æç¤ºæ¥è§£å†³é€æ˜å±‚åˆ†è§£ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é’ˆå¯¹åŠé€æ˜å’Œé€æ˜å›¾å±‚çš„éçº¿æ€§é®æŒ¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼šé€å±‚åˆ†è§£ alpha åˆæˆå›¾åƒã€‚ä¸ºäº†è§£å†³å±‚æ¨¡ç³Šã€æ³›åŒ–èƒ½åŠ›å’Œæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œç ”ç©¶è€…ä»¬é¦–æ¬¡å¼•å…¥äº† AlphaBlend æ•°æ®é›†ï¼Œæ”¯æŒå¤šç§å®é™…åº”ç”¨åœºæ™¯ã€‚DiffDecompose é€šè¿‡ä¸Šä¸‹æ–‡åˆ†è§£çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰é€å±‚ç›‘ç£çš„æƒ…å†µä¸‹é¢„æµ‹ä¸€ä¸ªæˆ–å¤šä¸ªå±‚ï¼Œå±•ç¤ºäº†å…¶åœ¨å›¾åƒåˆ†è§£ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04133",
            "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
            "url": "https://huggingface.co/papers/2506.04133",
            "abstract": "A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy/security each contextualized for agentic LLMs. We identify unique threat vectors and introduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with a roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment.",
            "score": 2,
            "issue_id": 4133,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "a0a258935ed39508",
            "authors": [
                "Shaina Raza",
                "Ranjan Sapkota",
                "Manoj Karkee",
                "Christos Emmanouilidis"
            ],
            "affiliations": [
                "Cornell University, USA",
                "University of Groningen, Netherlands",
                "Vector Institute, Toronto, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04133.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#survey",
                    "#agents",
                    "#multimodal",
                    "#security",
                    "#alignment",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ğµ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸ĞµĞ¼, Ñ€Ğ¸ÑĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ (TRiSM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, ModelOps Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ/Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑƒĞ³Ñ€Ğ¾Ğ· Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² LLM."
                },
                "en": {
                    "title": "Navigating Trust and Security in Agentic AI Systems",
                    "desc": "This paper reviews the management of trust, risk, and security in multi-agent systems that use large language models (LLMs). It discusses how these agentic AI systems differ from traditional AI, focusing on their ability to operate autonomously and collaboratively. The authors outline four key areas of Trust, Risk, and Security Management (TRiSM): governance, explainability, ModelOps, and privacy/security, providing a framework for understanding the unique challenges these systems face. The paper also highlights the importance of building trust and ensuring transparency in these systems, while proposing future research directions for responsible deployment."
                },
                "zh": {
                    "title": "æ„å»ºå®‰å…¨é€æ˜çš„ä»£ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿ",
                    "desc": "æœ¬æ–‡å›é¡¾äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„ä¿¡ä»»ã€é£é™©å’Œå®‰å…¨ç®¡ç†ï¼ˆTRiSMï¼‰ã€‚æˆ‘ä»¬åˆ†æäº†ä»£ç†äººå·¥æ™ºèƒ½çš„æ¦‚å¿µåŸºç¡€åŠå…¶ä¸ä¼ ç»Ÿäººå·¥æ™ºèƒ½ä»£ç†çš„æ¶æ„å·®å¼‚ï¼Œå¹¶æ¢è®¨äº†æ”¯æŒå¯æ‰©å±•è‡ªä¸»æ€§çš„ç³»ç»Ÿè®¾è®¡ã€‚æ–‡ç« è¯¦ç»†é˜è¿°äº†TRiSMçš„å››ä¸ªæ”¯æŸ±ï¼šæ²»ç†ã€å¯è§£é‡Šæ€§ã€æ¨¡å‹æ“ä½œå’Œéšç§/å®‰å…¨ï¼Œå¹¶ä¸ºä»£ç†LLMæä¾›äº†å…·ä½“çš„èƒŒæ™¯ã€‚æœ€åï¼Œæå‡ºäº†è´Ÿè´£ä»»çš„ä»£ç†äººå·¥æ™ºèƒ½çš„è·¯çº¿å›¾ï¼Œå»ºè®®ç ”ç©¶æ–¹å‘ä»¥ç¡®ä¿æ–°å…´å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å®‰å…¨ã€é€æ˜å’Œè´Ÿè´£ä»»çš„éƒ¨ç½²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.04034",
            "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2506.04034",
            "abstract": "Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings.",
            "score": 2,
            "issue_id": 4133,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "9d3dcbdd5158f101",
            "authors": [
                "Qing Jiang",
                "Xingyu Chen",
                "Zhaoyang Zeng",
                "Junzhi Yu",
                "Lei Zhang"
            ],
            "affiliations": [
                "International Digital Economy Academy (IDEA)",
                "Peking University",
                "South China University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.04034.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#hallucinations",
                    "#interpretability",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾Ğµ Ñ€ĞµÑ„ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ„ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Rex-Thinker. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ HumanRef-CoT Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Rex-Thinker Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Rex-Thinker: Grounded Object Referring with Explainable Reasoning",
                    "desc": "This paper introduces Rex-Thinker, a model designed to enhance object referring in images by incorporating explainable and trustworthy reasoning. Unlike traditional methods that focus solely on bounding box predictions, Rex-Thinker employs a Chain of Thought (CoT) reasoning approach to evaluate candidate objects against natural language descriptions. The model is trained on a new dataset, HumanRef-CoT, which facilitates structured reasoning through a systematic planning and summarization process. Results indicate that Rex-Thinker not only improves precision and interpretability but also effectively rejects irrelevant predictions, showcasing its robustness in various scenarios."
                },
                "zh": {
                    "title": "Rex-Thinkerï¼šå¯è§£é‡Šçš„ç‰©ä½“æŒ‡ä»£æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç‰©ä½“æŒ‡ä»£æ¨¡å‹Rex-Thinkerï¼Œæ—¨åœ¨é€šè¿‡æ˜ç¡®çš„é“¾å¼æ¨ç†ä»»åŠ¡æ¥æ£€æµ‹ä¸è‡ªç„¶è¯­è¨€æè¿°åŒ¹é…çš„å›¾åƒä¸­çš„æ‰€æœ‰ç‰©ä½“ã€‚è¯¥æ¨¡å‹å¼ºè°ƒå¯éªŒè¯æ€§å’Œå¯ä¿¡æ€§ï¼Œç¡®ä¿å…¶é¢„æµ‹èƒ½å¤Ÿè§£é‡Šå¹¶ä¸è§†è§‰è¯æ®ç›¸è¿ã€‚Rex-Thinkeré€šè¿‡é€æ­¥æ¨ç†å€™é€‰ç‰©ä½“å®ä¾‹ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ç¬¦åˆç»™å®šçš„æè¿°ï¼Œä»è€Œåšå‡ºæœ€ç»ˆé¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç²¾ç¡®åº¦å’Œå¯è§£é‡Šæ€§æ–¹é¢ä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œå¹¶åœ¨æ‹’ç»è™šå‡è¾“å‡ºå’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23807",
            "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
            "url": "https://huggingface.co/papers/2505.23807",
            "abstract": "A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.  \t\t\t\t\tAI-generated summary \t\t\t\t Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.",
            "score": 2,
            "issue_id": 4133,
            "pub_date": "2025-05-27",
            "pub_date_card": {
                "ru": "27 Ğ¼Ğ°Ñ",
                "en": "May 27",
                "zh": "5æœˆ27æ—¥"
            },
            "hash": "a817afc0cdd35d8d",
            "authors": [
                "Yuli Chen",
                "Bo Cheng",
                "Jiale Han",
                "Yingying Zhang",
                "Yingting Li",
                "Shuhao Zhang"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology, Hong Kong, China",
                "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23807.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#training"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ (DLP) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. DLP Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ²ĞµÑĞ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑÑ…. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ DLP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Dynamic Layerwise Pruning: Smart Sparsity for Language Models",
                    "desc": "This paper introduces a new method called Dynamic Layerwise Pruning (DLP) that improves the efficiency of large language models (LLMs) by adaptively determining the importance of each layer. Unlike traditional pruning methods that apply uniform strategies, DLP combines model weights and activation data to assign specific pruning rates to different layers. This approach helps maintain model performance even at high levels of sparsity, which is crucial for effective model compression. Experimental results demonstrate that DLP significantly enhances accuracy and reduces perplexity in LLMs compared to existing techniques."
                },
                "zh": {
                    "title": "åŠ¨æ€å‰ªæï¼Œæ™ºèƒ½ä¿æŒæ€§èƒ½ï¼",
                    "desc": "åŠ¨æ€å±‚çº§å‰ªææ–¹æ³•é€šè¿‡ç»“åˆæ¨¡å‹æƒé‡å’Œæ¿€æ´»ä¿¡æ¯ï¼Œè‡ªé€‚åº”åœ°ç¡®å®šæ¯ä¸€å±‚çš„é‡è¦æ€§ï¼Œä»è€Œåœ¨é«˜ç¨€ç–æ€§ä¸‹ä¿æŒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„å‰ªææŠ€æœ¯é€šå¸¸é‡‡ç”¨å‡åŒ€å±‚çº§å‰ªæç­–ç•¥ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨é«˜ç¨€ç–æ€§æ°´å¹³ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚åŠ¨æ€å±‚çº§å‰ªæï¼ˆDLPï¼‰æ–¹æ³•å…‹æœäº†è¿™ä¸€é™åˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥æ¿€æ´»ä¿¡æ¯åŠ¨æ€è°ƒæ•´å‰ªæç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDLPåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸­æœ‰æ•ˆåœ°ä¿æŒäº†é«˜ç¨€ç–æ€§ä¸‹çš„æ¨¡å‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.03614",
            "title": "VLMs Can Aggregate Scattered Training Patches",
            "url": "https://huggingface.co/papers/2506.03614",
            "abstract": "VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t One way to mitigate risks in vision-language models (VLMs) is to remove dangerous samples in their training data. However, such data moderation can be easily bypassed when harmful images are split into small, benign-looking patches, scattered across many training samples. VLMs may then learn to piece these fragments together during training and generate harmful responses at inference, either from full images or text references. For instance, if trained on image patches from a bloody scene paired with the descriptions \"safe,\" VLMs may later describe, the full image or a text reference to the scene, as \"safe.\" We define the core ability of VLMs enabling this attack as visual stitching -- the ability to integrate visual information spread across multiple training samples that share the same textual descriptions. In our work, we first demonstrate visual stitching abilities in common open-source VLMs on three datasets where each image is labeled with a unique synthetic ID: we split each (image, ID) pair into {(patch, ID)} pairs at different granularity for finetuning, and we find that tuned models can verbalize the correct IDs from full images or text reference. Building on this, we simulate the adversarial data poisoning scenario mentioned above by using patches from dangerous images and replacing IDs with text descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can evade moderation in patches and later be reconstructed through visual stitching, posing serious VLM safety risks. Code is available at https://github.com/ZHZisZZ/visual-stitching.",
            "score": 1,
            "issue_id": 4137,
            "pub_date": "2025-06-04",
            "pub_date_card": {
                "ru": "4 Ğ¸ÑĞ½Ñ",
                "en": "June 4",
                "zh": "6æœˆ4æ—¥"
            },
            "hash": "180b48fc19d50b80",
            "authors": [
                "Zhanhui Zhou",
                "Lingjie Chen",
                "Chao Yang",
                "Chaochao Lu"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.03614.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#data",
                    "#dataset",
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#security",
                    "#ethics"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ: ÑĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ 'Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ' Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº VLM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…, Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²Ğ¸Ğ´ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ±Ñ€Ğ¾ÑĞ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ñƒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ VLM Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹."
                },
                "en": {
                    "title": "Visual Stitching: A Hidden Risk in Vision-Language Models",
                    "desc": "This paper discusses a vulnerability in vision-language models (VLMs) known as visual stitching, which allows these models to reconstruct harmful content from fragmented visual information. The authors show that when dangerous images are divided into small patches and mixed with benign data, VLMs can still learn to piece them together during training. This leads to a situation where the models can generate harmful outputs by associating safe descriptions with dangerous images. The study highlights the risks of data moderation being bypassed and emphasizes the need for improved safety measures in VLMs."
                },
                "zh": {
                    "title": "è§†è§‰æ‹¼æ¥ï¼šVLMsçš„å®‰å…¨éšæ‚£",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„è§†è§‰æ‹¼æ¥èƒ½åŠ›ï¼Œè¿™ç§èƒ½åŠ›ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•´åˆåˆ†æ•£çš„è§†è§‰ä¿¡æ¯ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“æœ‰å®³å›¾åƒè¢«åˆ†å‰²æˆå°çš„ã€çœ‹ä¼¼æ— å®³çš„ç‰‡æ®µæ—¶ï¼Œæ•°æ®çš„å®¡æŸ¥å¯ä»¥è¢«è½»æ˜“ç»•è¿‡ã€‚VLMsåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šå­¦ä¹ å°†è¿™äº›ç‰‡æ®µæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œä»è€Œåœ¨æ¨ç†æ—¶ç”Ÿæˆæœ‰å®³çš„å“åº”ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒå±•ç¤ºäº†è¿™ä¸€ç°è±¡ï¼Œå¹¶æ¨¡æ‹Ÿäº†å¯¹æŠ—æ€§æ•°æ®ä¸­æ¯’çš„åœºæ™¯ï¼Œæ­ç¤ºäº†VLMsåœ¨å®‰å…¨æ€§æ–¹é¢çš„æ½œåœ¨é£é™©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.02294",
            "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation",
            "url": "https://huggingface.co/papers/2506.02294",
            "abstract": "A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.  \t\t\t\t\tAI-generated summary \t\t\t\t Large foundation models trained on extensive datasets demonstrate strong zero-shot capabilities in various domains. To replicate their success when data and model size are constrained, knowledge distillation has become an established tool for transferring knowledge from foundation models to small student networks. However, the effectiveness of distillation is critically limited by the available training data. This work addresses the common practical issue of covariate shift in knowledge distillation, where spurious features appear during training but not at test time. We ask the question: when these spurious features are unknown, yet a robust teacher is available, is it possible for a student to also become robust to them? We address this problem by introducing a novel diffusion-based data augmentation strategy that generates images by maximizing the disagreement between the teacher and the student, effectively creating challenging samples that the student struggles with. Experiments demonstrate that our approach significantly improves worst group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art diffusion-based data augmentation baselines",
            "score": 1,
            "issue_id": 4133,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "740d99ccc158d514",
            "authors": [
                "Niclas Popp",
                "Kevin Alexander Laube",
                "Matthias Hein",
                "Lukas Schott"
            ],
            "affiliations": [
                "Bosch Center for Artificial Intelligence",
                "University of TÃ¼bingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.02294.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#transfer_learning",
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ´Ğ²Ğ¸Ğ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ¸Ñ…ÑƒĞ´ÑˆĞ¸Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ… Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ğ¼ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… CelebA Ğ¸ SpuCo Birds. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Boosting Student Robustness with Diffusion Data Augmentation",
                    "desc": "This paper presents a new data augmentation method using diffusion processes to enhance knowledge distillation. The approach generates challenging samples that help student networks learn to be more robust against spurious features that may not appear during testing. By maximizing the disagreement between a robust teacher model and the student model, the method effectively prepares the student for real-world scenarios where data may shift. Experiments show that this strategy improves accuracy and resilience against spurious features in various datasets, outperforming existing methods."
                },
                "zh": {
                    "title": "åŸºäºæ‰©æ•£çš„æ•°æ®å¢å¼ºæå‡çŸ¥è¯†è’¸é¦é²æ£’æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œä»¥æé«˜çŸ¥è¯†è’¸é¦ä¸­çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œå¢å¼ºäº†å­¦ç”Ÿç½‘ç»œå¯¹è™šå‡ç‰¹å¾çš„æŠµæŠ—åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨CelebAå’ŒSpuCo Birdsæ•°æ®é›†ä¸Šï¼Œè¯¥ç­–ç•¥æ˜¾è‘—æé«˜äº†æœ€å·®ç»„å’Œå¹³å‡ç»„çš„å‡†ç¡®ç‡ã€‚é€šè¿‡æœ€å¤§åŒ–æ•™å¸ˆå’Œå­¦ç”Ÿä¹‹é—´çš„åˆ†æ­§ï¼Œæœ¬æ–‡æœ‰æ•ˆåœ°è§£å†³äº†çŸ¥è¯†è’¸é¦ä¸­çš„åå˜é‡åç§»é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.01344",
            "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents",
            "url": "https://huggingface.co/papers/2506.01344",
            "abstract": "Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our proposed FlowExplainBench dataset.",
            "score": 1,
            "issue_id": 4133,
            "pub_date": "2025-06-02",
            "pub_date_card": {
                "ru": "2 Ğ¸ÑĞ½Ñ",
                "en": "June 2",
                "zh": "6æœˆ2æ—¥"
            },
            "hash": "788495117e4bf1d7",
            "authors": [
                "Manan Suri",
                "Puneet Mathur",
                "Nedim Lipka",
                "Franck Dernoncourt",
                "Ryan A. Rossi",
                "Vivek Gupta",
                "Dinesh Manocha"
            ],
            "affiliations": [
                "Adobe",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.01344.jpg",
            "data": {
                "categories": [
                    "#graphs",
                    "#cv",
                    "#reasoning",
                    "#agents",
                    "#hallucinations",
                    "#multimodal",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° FlowPathAgent Ğ´Ğ»Ñ ĞµĞµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼Ñƒ, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ĞµĞµ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº FlowExplainBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¹ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FlowPathAgent ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼Ğ°Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 10-14%."
                },
                "en": {
                    "title": "Enhancing Flowchart Interpretation with Fine-grained Attribution",
                    "desc": "This paper addresses the challenges of interpreting flowcharts using large language models (LLMs) due to their complex structures and potential for hallucination. It introduces Fine-grained Flowchart Attribution, a method that links LLM responses to specific components of flowcharts, enhancing the reliability and explainability of automated processing. The authors present FlowPathAgent, a neurosymbolic agent that utilizes graph-based reasoning to segment flowcharts and create structured symbolic graphs for dynamic interaction. Experimental results demonstrate that FlowPathAgent significantly reduces hallucinations in LLM outputs, achieving improved performance on the newly introduced FlowExplainBench benchmark."
                },
                "zh": {
                    "title": "æå‡æµç¨‹å›¾è§£æçš„å¯é æ€§ä¸å¯è§£é‡Šæ€§",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œç§°ä¸ºç»†ç²’åº¦æµç¨‹å›¾å½’å› ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†æµç¨‹å›¾æ—¶çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬æå‡ºäº†FlowPathAgentï¼Œè¿™æ˜¯ä¸€ç§ç¥ç»ç¬¦å·ä»£ç†ï¼Œé€šè¿‡å›¾å½¢æ¨ç†è¿›è¡Œç»†ç²’åº¦çš„åæœŸå½’å› ã€‚è¯¥ä»£ç†é¦–å…ˆå¯¹æµç¨‹å›¾è¿›è¡Œåˆ†å‰²ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºç»“æ„åŒ–çš„ç¬¦å·å›¾ï¼Œå¹¶åŠ¨æ€ä¸å›¾è¿›è¡Œäº¤äº’ï¼Œä»¥ç”Ÿæˆå½’å› è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowPathAgentåœ¨æµç¨‹å›¾é—®ç­”ä¸­å‡å°‘äº†è§†è§‰å¹»è§‰ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿æé«˜äº†10-14%çš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-04.html",
    "link_next": "2025-06-06.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "04.06",
        "en": "06/04",
        "zh": "6æœˆ4æ—¥"
    },
    "short_date_next": {
        "ru": "06.06",
        "en": "06/06",
        "zh": "6æœˆ6æ—¥"
    },
    "categories": {
        "#dataset": 9,
        "#data": 4,
        "#benchmark": 15,
        "#agents": 3,
        "#cv": 6,
        "#rl": 6,
        "#rlhf": 7,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 0,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 9,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 16,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 6,
        "#reasoning": 10,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 3,
        "#security": 2,
        "#optimization": 14,
        "#survey": 2,
        "#diffusion": 5,
        "#alignment": 3,
        "#story_generation": 1,
        "#hallucinations": 4,
        "#long_context": 4,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "VS-Benchæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„ç­–ç•¥æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚ç°æœ‰çš„åŸºå‡†å¤§å¤šå±€é™äºå•æ™ºèƒ½ä½“æˆ–ä»…æ–‡æœ¬ç¯å¢ƒï¼Œè€ŒVS-BenchåŒ…å«å…«ä¸ªåŸºäºè§†è§‰çš„ç¯å¢ƒï¼Œæ¶µç›–åˆä½œã€ç«äº‰å’Œæ··åˆåŠ¨æœºçš„äº’åŠ¨ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œå½’ä¸€åŒ–å›æŠ¥æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚VS-Benchæ—¨åœ¨æ ‡å‡†åŒ–è¯„ä¼°å¹¶æŒ‡å‡ºç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œæ¨åŠ¨æœªæ¥ç ”ç©¶ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨https://vs-bench.github.ioè·å–ã€‚",
        "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments",
        "pinyin": "ä»¥ä¸‹æ˜¯æ‚¨æä¾›çš„æ–‡æœ¬çš„æ‹¼éŸ³è½¬å†™ï¼š\n\nVS-Bench shÃ¬ yÄ«gÃ¨ duÅ mÃ³ tÃ i jÄ«zhÇ”n, yÃ²ngyÃº pÃ­nggÇ” shÃ¬juÃ© yÇ”yÃ¡n mÃ³xÃ­ng zÃ i fÃ¹zÃ¡ dÅu zhÃ¬nÃ©ngtÇ huÃ¡njÃ¬ng zhÅng de cÃ¨lÃ¼Ã¨ tuÄ«lÇ hÃ© juÃ©cÃ¨ nÃ©nglÃ¬. XiÃ n yÇ’u de jÄ«zhÇ”n dÃ duÅ jÃºxÃ¬an zÃ i dÄn zhÃ¬nÃ©ngtÇ huÃ² jÇn wÃ©nbÄ›n huÃ¡njÃ¬ng, Ã©r VS-Bench hÃ¡n yÇ’u bÄ gÃ¨ jÄ«chÇ” shÃ¬juÃ© de huÃ¡njÃ¬ng, hÃ¡nfÃ¹ hÃ©zuÃ², jÃ¬ngzhÄ“ng hÃ© hÃ¹nhÃ© dÃ²ngjÄ« de hÃ¹dÃ²ng. YÃ¡njiÅ« fÄxiÃ n, xiÃ n yÇ’u mÃ³xÃ­ng zÃ i yÃ¹cÃ¨ zhÇ”nquÃ¨xÃ¬ng hÃ© guÄ«yÄ«huÃ  huÃ­bÃ o fÄngmiÃ n cÃºnzÃ i xiÇnzhÃ¹ chÄjÃ¹. VS-Bench zhÇ yÃº biÄozhÇ”nhuÃ  pÃ­nggÇ” Ã©r zhÇchÅ« xiÃ n yÇ’u mÃ³xÃ­ng de jÃºxÃ¬anxÃ¬ng, tuÄ«dÃ²ng wÃ¨ilÃ¡i yÃ¡njiÅ«. DÃ imÇ hÃ© shÃ¹jÃ¹ kÄ› zÃ i https://vs-bench.github.io huÃ²qÇ”.",
        "vocab": "[{'word': 'å¤šæ¨¡æ€', 'pinyin': 'duÅ mÃ³ tÃ i', 'trans': 'multimodal'}, {'word': 'åŸºå‡†', 'pinyin': 'jÄ« zhÇ”n', 'trans': 'benchmark'}, {'word': 'è§†è§‰è¯­è¨€æ¨¡å‹', 'pinyin': 'shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'vision-language model'}, {'word': 'å¤æ‚', 'pinyin': 'fÃ¹ zÃ¡', 'trans': 'complex'}, {'word': 'å¤šæ™ºèƒ½ä½“', 'pinyin': 'duÅ zhÃ¬ nÃ©ng tÇ', 'trans': 'multi-agent'}, {'word': 'ç¯å¢ƒ', 'pinyin': 'huÃ¡n jÃ¬ng', 'trans': 'environment'}, {'word': 'ç­–ç•¥æ¨ç†', 'pinyin': 'cÃ¨ lÃ¼Ã¨ tuÄ« lÇ', 'trans': 'strategic reasoning'}, {'word': 'å†³ç­–', 'pinyin': 'juÃ© cÃ¨', 'trans': 'decision-making'}, {'word': 'èƒ½åŠ›', 'pinyin': 'nÃ©ng lÃ¬', 'trans': 'ability'}, {'word': 'å±€é™äº', 'pinyin': 'jÃº xiÃ n yÃº', 'trans': 'limited to'}, {'word': 'å•æ™ºèƒ½ä½“', 'pinyin': 'dÄn zhÃ¬ nÃ©ng tÇ', 'trans': 'single-agent'}, {'word': 'ä»…', 'pinyin': 'jÇn', 'trans': 'only'}, {'word': 'æ–‡æœ¬', 'pinyin': 'wÃ©n bÄ›n', 'trans': 'text'}, {'word': 'æ¶µç›–', 'pinyin': 'hÃ¡n gÃ i', 'trans': 'cover'}, {'word': 'åˆä½œ', 'pinyin': 'hÃ© zuÃ²', 'trans': 'cooperation'}, {'word': 'ç«äº‰', 'pinyin': 'jÃ¬ng zhÄ“ng', 'trans': 'competition'}, {'word': 'æ··åˆ', 'pinyin': 'hÃ¹n hÃ©', 'trans': 'hybrid'}, {'word': 'åŠ¨æœº', 'pinyin': 'dÃ²ng jÄ«', 'trans': 'motivation'}, {'word': 'äº’åŠ¨', 'pinyin': 'hÃ¹ dÃ²ng', 'trans': 'interaction'}, {'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}, {'word': 'å‘ç°', 'pinyin': 'fÄ xiÃ n', 'trans': 'find'}, {'word': 'é¢„æµ‹', 'pinyin': 'yÃ¹ cÃ¨', 'trans': 'prediction'}, {'word': 'å‡†ç¡®æ€§', 'pinyin': 'zhÇ”n quÃ¨ xÃ¬ng', 'trans': 'accuracy'}, {'word': 'å½’ä¸€åŒ–', 'pinyin': 'guÄ« yÄ« huÃ ', 'trans': 'normalization'}, {'word': 'å›æŠ¥', 'pinyin': 'huÃ­ bÃ o', 'trans': 'reward'}, {'word': 'æ–¹é¢', 'pinyin': 'fÄng miÃ n', 'trans': 'aspect'}, {'word': 'å­˜åœ¨', 'pinyin': 'cÃºn zÃ i', 'trans': 'exist'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'}, {'word': 'å·®è·', 'pinyin': 'chÄ jÃ¹', 'trans': 'gap'}, {'word': 'æ—¨åœ¨', 'pinyin': 'zhÇ zÃ i', 'trans': 'aim to'}, {'word': 'æ ‡å‡†åŒ–', 'pinyin': 'biÄo zhÇ”n huÃ ', 'trans': 'standardize'}, {'word': 'è¯„ä¼°', 'pinyin': 'pÃ­ng gÅ«', 'trans': 'evaluate'}, {'word': 'æŒ‡å‡º', 'pinyin': 'zhÇ chÅ«', 'trans': 'point out'}, {'word': 'å±€é™æ€§', 'pinyin': 'jÃº xiÃ n xÃ¬ng', 'trans': 'limitation'}, {'word': 'æ¨åŠ¨', 'pinyin': 'tuÄ« dÃ²ng', 'trans': 'promote'}, {'word': 'æœªæ¥', 'pinyin': 'wÃ¨i lÃ¡i', 'trans': 'future'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'}, {'word': 'æ•°æ®', 'pinyin': 'shÃ¹ jÃ¹', 'trans': 'data'}, {'word': 'è·å–', 'pinyin': 'huÃ² qÇ”', 'trans': 'obtain'}]",
        "trans": "VS-Bench is a multimodal benchmark designed to evaluate the strategic reasoning and decision-making capabilities of vision-language models in complex multi-agent environments. Existing benchmarks are largely limited to single-agent or text-only environments, whereas VS-Bench includes eight visually-based environments that encompass cooperative, competitive, and mixed-motive interactions. Research has revealed significant gaps in the predictive accuracy and normalized returns of existing models. VS-Bench aims to standardize evaluations and highlight the limitations of current models, driving future research. The code and data are available at https://vs-bench.github.io.",
        "update_ts": "2025-06-04 09:12"
    }
}