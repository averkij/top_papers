
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. November 26.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">26 ноября</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-11-25.html">⬅️ <span id="prev-date">25.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-11-27.html">➡️ <span id="next-date">27.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-11.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '26 ноября', 'en': 'November 26', 'zh': '11月26日'};
        let feedDateNext = {'ru': '27.11', 'en': '11/27', 'zh': '11月27日'};
        let feedDatePrev = {'ru': '25.11', 'en': '11/25', 'zh': '11月25日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2411.15138', 'title': 'Material Anything: Generating Materials for Any 3D Object via Diffusion', 'url': 'https://huggingface.co/papers/2411.15138', 'abstract': 'We present Material Anything, a fully-automated, unified diffusion framework designed to generate physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to objects under diverse lighting conditions. Our approach leverages a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss to improve stability and material quality. Additionally, we introduce confidence masks as a dynamic switcher within the diffusion model, enabling it to effectively handle both textured and texture-less objects across varying lighting conditions. By employing a progressive material generation strategy guided by these confidence masks, along with a UV-space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate our approach outperforms existing methods across a wide range of object categories and lighting conditions.', 'score': 23, 'issue_id': 776, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '34b8f6718115f1e3', 'authors': ['Xin Huang', 'Tengfei Wang', 'Ziwei Liu', 'Qing Wang'], 'affiliations': ['Northwestern Polytechnical University', 'S-Lab, Nanyang Technological University', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.15138.jpg', 'data': {'categories': ['#3d', '#architecture', '#optimization', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Универсальная генерация материалов для 3D-объектов с помощью диффузии', 'desc': 'В статье представлен Material Anything - полностью автоматизированный унифицированный фреймворк диффузии для генерации физически корректных материалов для 3D-объектов. В отличие от существующих методов, он предлагает надежное сквозное решение, адаптируемое к объектам в различных условиях освещения. Подход использует предобученную модель диффузии изображений с тройной архитектурой и функцией потерь рендеринга для улучшения стабильности и качества материалов. Также вводятся маски уверенности как динамический переключатель в модели диффузии, позволяющий эффективно обрабатывать объекты с текстурами и без них в различных условиях освещения.'}, 'en': {'title': 'Automating Realistic Material Generation for 3D Objects', 'desc': 'Material Anything is a novel framework that automates the generation of realistic materials for 3D objects using a unified diffusion approach. It simplifies the material creation process by eliminating the need for complex workflows and optimizations tailored to specific cases. The framework utilizes a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss, to ensure high-quality and stable material outputs. By incorporating confidence masks, it dynamically adapts to different object types and lighting scenarios, resulting in consistent and UV-ready materials across various conditions.'}, 'zh': {'title': '全自动材料生成，适应多种光照条件', 'desc': '本文介绍了一种名为Material Anything的全自动统一扩散框架，旨在为3D物体生成基于物理的材料。与现有方法依赖复杂流程或特定优化不同，Material Anything提供了一种稳健的端到端解决方案，适应不同光照条件下的物体。我们的方法利用了预训练的图像扩散模型，并通过三头架构和渲染损失来提高稳定性和材料质量。此外，我们引入了置信掩码作为扩散模型中的动态切换器，使其能够有效处理有纹理和无纹理的物体。'}}}, {'id': 'https://huggingface.co/papers/2411.15466', 'title': 'Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator', 'url': 'https://huggingface.co/papers/2411.15466', 'abstract': 'Subject-driven text-to-image generation aims to produce images of a new subject within a desired context by accurately capturing both the visual characteristics of the subject and the semantic content of a text prompt. Traditional methods rely on time- and resource-intensive fine-tuning for subject alignment, while recent zero-shot approaches leverage on-the-fly image prompting, often sacrificing subject alignment. In this paper, we introduce Diptych Prompting, a novel zero-shot approach that reinterprets as an inpainting task with precise subject alignment by leveraging the emergent property of diptych generation in large-scale text-to-image models. Diptych Prompting arranges an incomplete diptych with the reference image in the left panel, and performs text-conditioned inpainting on the right panel. We further prevent unwanted content leakage by removing the background in the reference image and improve fine-grained details in the generated subject by enhancing attention weights between the panels during inpainting. Experimental results confirm that our approach significantly outperforms zero-shot image prompting methods, resulting in images that are visually preferred by users. Additionally, our method supports not only subject-driven generation but also stylized image generation and subject-driven image editing, demonstrating versatility across diverse image generation applications. Project page: https://diptychprompting.github.io/', 'score': 20, 'issue_id': 777, 'pub_date': '2024-11-23', 'pub_date_card': {'ru': '23 ноября', 'en': 'November 23', 'zh': '11月23日'}, 'hash': '288600e8c54930f4', 'authors': ['Chaehun Shin', 'Jooyoung Choi', 'Heeseung Kim', 'Sungroh Yoon'], 'affiliations': ['AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University', 'Data Science and AI Laboratory, ECE, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2411.15466.jpg', 'data': {'categories': ['#synthetic', '#cv', '#multimodal', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Diptych Prompting: точная генерация изображений без дополнительного обучения', 'desc': 'Статья представляет новый метод генерации изображений под названием Diptych Prompting. Этот подход использует свойство диптиха в крупномасштабных моделях text-to-image для точного воспроизведения субъекта в желаемом контексте. Метод интерпретирует задачу как инпейнтинг, размещая исходное изображение в левой части диптиха и генерируя правую часть на основе текстового промпта. Diptych Prompting превосходит существующие zero-shot методы и поддерживает различные приложения генерации изображений.'}, 'en': {'title': 'Diptych Prompting: Zero-Shot Image Generation with Subject Precision', 'desc': 'This paper presents Diptych Prompting, a new method for generating images from text prompts while maintaining accurate subject alignment. Unlike traditional methods that require extensive fine-tuning, this zero-shot approach treats the task as inpainting, using a diptych format with a reference image. By focusing on the left panel for the reference and performing text-conditioned inpainting on the right, the method enhances detail and prevents unwanted content from leaking into the generated image. The results show that Diptych Prompting not only improves visual quality but also allows for versatile applications in stylized image generation and editing.'}, 'zh': {'title': 'Diptych Prompting：精准的主题驱动图像生成新方法', 'desc': '本文提出了一种新的零-shot方法，称为Diptych Prompting，旨在实现主题驱动的文本到图像生成。该方法通过将生成任务重新解释为图像修补，确保了主题的精确对齐。Diptych Prompting利用大型文本到图像模型的双联生成特性，左侧面板展示参考图像，右侧面板进行文本条件的修补。实验结果表明，该方法在视觉效果上优于传统的零-shot图像提示方法，且支持多种图像生成应用。'}}}, {'id': 'https://huggingface.co/papers/2411.15611', 'title': 'Knowledge Transfer Across Modalities with Natural Language Supervision', 'url': 'https://huggingface.co/papers/2411.15611', 'abstract': 'We present a way to learn novel concepts by only using their textual description. We call this method Knowledge Transfer. Similarly to human perception, we leverage cross-modal interaction to introduce new concepts. We hypothesize that in a pre-trained visual encoder there are enough low-level features already learned (e.g. shape, appearance, color) that can be used to describe previously unknown high-level concepts. Provided with a textual description of the novel concept, our method works by aligning the known low-level features of the visual encoder to its high-level textual description. We show that Knowledge Transfer can successfully introduce novel concepts in multimodal models, in a very efficient manner, by only requiring a single description of the target concept. Our approach is compatible with both separate textual and visual encoders (e.g. CLIP) and shared parameters across modalities. We also show that, following the same principle, Knowledge Transfer can improve concepts already known by the model. Leveraging Knowledge Transfer we improve zero-shot performance across different tasks such as classification, segmentation, image-text retrieval, and captioning.', 'score': 10, 'issue_id': 784, 'pub_date': '2024-11-23', 'pub_date_card': {'ru': '23 ноября', 'en': 'November 23', 'zh': '11月23日'}, 'hash': '117dd5d3b47d2583', 'authors': ['Carlo Alberto Barbano', 'Luca Molinaro', 'Emanuele Aiello', 'Marco Grangetto'], 'affiliations': ['Politecnico di Torino', 'University of Turin'], 'pdf_title_img': 'assets/pdf/title_img/2411.15611.jpg', 'data': {'categories': ['#cv', '#transfer_learning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Обучение новым концепциям через текст: революция в машинном обучении', 'desc': "Статья представляет метод под названием 'Передача знаний' (Knowledge Transfer), позволяющий обучать модели машинного обучения новым концепциям, используя только их текстовое описание. Авторы предполагают, что в предобученном визуальном энкодере уже содержатся низкоуровневые признаки, которые можно использовать для описания ранее неизвестных высокоуровневых концепций. Метод работает путем выравнивания известных низкоуровневых признаков визуального энкодера с их высокоуровневым текстовым описанием. Исследователи показывают, что этот подход эффективен для мультимодальных моделей и применим в различных задачах, таких как классификация, сегментация, поиск изображений по тексту и генерация подписей."}, 'en': {'title': 'Learn New Concepts with Just Words!', 'desc': "This paper introduces a method called Knowledge Transfer, which allows machine learning models to learn new concepts using only their textual descriptions. The approach utilizes a pre-trained visual encoder that has already learned basic features like shape and color, which can be aligned with high-level textual descriptions of new concepts. By doing this, the model can efficiently incorporate novel concepts into its understanding without needing extensive retraining. Additionally, Knowledge Transfer enhances the model's performance on existing concepts and improves its zero-shot capabilities across various tasks such as classification and image-text retrieval."}, 'zh': {'title': '知识迁移：通过文本描述学习新概念', 'desc': '我们提出了一种通过文本描述学习新概念的方法，称为知识迁移。该方法利用跨模态交互，类似于人类的感知，来引入新概念。我们假设预训练的视觉编码器中已经学习了足够的低级特征（如形状、外观、颜色），可以用来描述之前未知的高级概念。通过对目标概念的单一文本描述进行对齐，我们的方法能够高效地在多模态模型中引入新概念，并提升模型在分类、分割、图像-文本检索和标题生成等任务上的零-shot性能。'}}}, {'id': 'https://huggingface.co/papers/2411.16594', 'title': 'From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge', 'url': 'https://huggingface.co/papers/2411.16594', 'abstract': 'Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the "LLM-as-a-judge" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. We begin by giving detailed definitions from both input and output perspectives. Then we introduce a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area. Paper list and more resources about LLM-as-a-judge can be found at https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge and https://llm-as-a-judge.github.io.', 'score': 10, 'issue_id': 778, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '56883eb77dcb5fa3', 'authors': ['Dawei Li', 'Bohan Jiang', 'Liangjie Huang', 'Alimohammad Beigi', 'Chengshuai Zhao', 'Zhen Tan', 'Amrita Bhattacharjee', 'Yuxuan Jiang', 'Canyu Chen', 'Tianhao Wu', 'Kai Shu', 'Lu Cheng', 'Huan Liu'], 'affiliations': ['Arizona State University', 'Emory University', 'Illinois Institute of Technology', 'University of California, Berkeley', 'University of Illinois Chicago', 'University of Maryland, Baltimore County'], 'pdf_title_img': 'assets/pdf/title_img/2411.16594.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#survey'], 'emoji': '⚖️', 'ru': {'title': 'LLM как судья: новая парадигма оценки в AI и NLP', 'desc': "Статья представляет собой обзор использования больших языковых моделей (LLM) в качестве судей для оценки и ранжирования в задачах искусственного интеллекта и обработки естественного языка. Авторы предлагают подробную таксономию подхода 'LLM-as-a-judge', рассматривая что, как и где оценивать. В работе также представлены бенчмарки для оценки эффективности LLM в роли судей. Статья завершается обсуждением ключевых проблем и перспективных направлений исследований в этой области."}, 'en': {'title': 'Harnessing LLMs for Enhanced AI Evaluation', 'desc': 'This paper discusses the challenges of assessment and evaluation in artificial intelligence and natural language processing, particularly focusing on the limitations of traditional methods. It introduces the innovative concept of using Large Language Models (LLMs) as judges to score, rank, or select outputs in various tasks. The authors provide a detailed framework that categorizes LLM-based judgment into three key areas: what to judge, how to judge, and where to judge. Additionally, the paper compiles benchmarks for evaluating these models and identifies future research directions to enhance the effectiveness of LLMs in assessment tasks.'}, 'zh': {'title': '大型语言模型：评判的新力量', 'desc': '本论文探讨了大型语言模型（LLM）在评估和判断中的应用，提出了“LLM作为评判者”的新范式。传统的评估方法往往无法有效判断细微的属性，而LLM能够在多种任务中进行打分、排名和选择。我们从输入和输出的角度详细定义了评判的概念，并建立了一个全面的分类法，探讨了评判的内容、方式和场所。最后，我们编制了评估LLM作为评判者的基准，并强调了关键挑战和未来研究的方向。'}}}, {'id': 'https://huggingface.co/papers/2411.16205', 'title': 'MH-MoE:Multi-Head Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2411.16205', 'abstract': 'Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by using the multi-head mechanism to collectively attend to information from various representation spaces within different experts. In this paper, we present a novel implementation of MH-MoE that maintains both FLOPs and parameter parity with sparse Mixture of Experts models. Experimental results on language models show that the new implementation yields quality improvements over both vanilla MoE and fine-grained MoE models. Additionally, our experiments demonstrate that MH-MoE is compatible with 1-bit Large Language Models (LLMs) such as BitNet.', 'score': 9, 'issue_id': 778, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': 'b684b6d745cb66ff', 'authors': ['Shaohan Huang', 'Xun Wu', 'Shuming Ma', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2411.16205.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Мультиголовая смесь экспертов: эффективность без компромиссов', 'desc': 'Статья представляет новую реализацию мультиголовой смеси экспертов (MH-MoE), которая сохраняет паритет по FLOP и параметрам с разреженными моделями смеси экспертов. Эксперименты на языковых моделях показывают, что новая реализация улучшает качество по сравнению с обычными MoE и мелкозернистыми MoE моделями. MH-MoE использует механизм мультиголовности для совместного внимания к информации из различных пространств представлений в разных экспертах. Исследование также демонстрирует совместимость MH-MoE с 1-битными большими языковыми моделями, такими как BitNet.'}, 'en': {'title': 'Unlocking Performance with Multi-Head Mixture-of-Experts', 'desc': 'The Multi-Head Mixture-of-Experts (MH-MoE) model enhances performance by allowing multiple heads to focus on different aspects of data from various experts. This paper introduces a new way to implement MH-MoE that keeps the same computational cost and number of parameters as traditional sparse Mixture of Experts models. Experiments conducted on language models reveal that this new approach provides better results compared to standard MoE and fine-grained MoE models. Furthermore, the findings indicate that MH-MoE can effectively work with 1-bit Large Language Models like BitNet.'}, 'zh': {'title': '多头混合专家：提升模型性能的新方法', 'desc': '多头混合专家模型（MH-MoE）通过多头机制，能够同时关注来自不同专家的多种表示空间的信息，从而展现出优越的性能。本文提出了一种新颖的MH-MoE实现，能够在计算量（FLOPs）和参数数量上与稀疏混合专家模型保持一致。实验结果表明，该新实现相较于传统的MoE和细粒度MoE模型在语言模型上有显著的质量提升。此外，我们的实验还表明MH-MoE与1位大语言模型（如BitNet）兼容。'}}}, {'id': 'https://huggingface.co/papers/2411.14522', 'title': 'GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI', 'url': 'https://huggingface.co/papers/2411.14522', 'abstract': "Despite significant advancements in general artificial intelligence, such as GPT-4, their effectiveness in the medical domain (general medical AI, GMAI) remains constrained due to the absence of specialized medical knowledge. To address this challenge, we present GMAI-VL-5.5M, a comprehensive multimodal medical dataset created by converting hundreds of specialized medical datasets into meticulously constructed image-text pairs. This dataset features comprehensive task coverage, diverse modalities, and high-quality image-text data. Building upon this multimodal dataset, we propose GMAI-VL, a general medical vision-language model with a progressively three-stage training strategy. This approach significantly enhances the model's ability by integrating visual and textual information, thereby improving its ability to process multimodal data and support accurate diagnosis and clinical decision-making. Experimental evaluations demonstrate that GMAI-VL achieves state-of-the-art results across a wide range of multimodal medical tasks, such as visual question answering and medical image diagnosis. Our contributions include the development of the GMAI-VL-5.5M dataset, the introduction of the GMAI-VL model, and the establishment of new benchmarks in multiple medical domains. Code and dataset will be released at https://github.com/uni-medical/GMAI-VL.", 'score': 8, 'issue_id': 778, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'eb0e262f1661d5c8', 'authors': ['Tianbin Li', 'Yanzhou Su', 'Wei Li', 'Bin Fu', 'Zhe Chen', 'Ziyan Huang', 'Guoan Wang', 'Chenglong Ma', 'Ying Chen', 'Ming Hu', 'Yanjun Li', 'Pengcheng Chen', 'Xiaowei Hu', 'Zhongying Deng', 'Yuanfeng Ji', 'Jin Ye', 'Yu Qiao', 'Junjun He'], 'affiliations': ['East China Normal University', 'Fudan University', 'Monash University', 'Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Sciences', 'Stanford University', 'University of Cambridge', 'University of Washington', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14522.jpg', 'data': {'categories': ['#agi', '#benchmark', '#multimodal', '#optimization', '#science', '#healthcare', '#dataset'], 'emoji': '🏥', 'ru': {'title': 'GMAI-VL: Мощная мультимодальная модель для медицинского ИИ', 'desc': 'Исследователи представили GMAI-VL-5.5M - обширный мультимодальный медицинский датасет, созданный путем преобразования сотен специализированных медицинских наборов данных в пары изображение-текст. На основе этого датасета была разработана модель GMAI-VL - общая медицинская модель компьютерного зрения и обработки естественного языка, обученная по трехэтапной стратегии. GMAI-VL достигает передовых результатов в широком спектре мультимодальных медицинских задач, таких как визуальные вопросно-ответные системы и диагностика медицинских изображений. Работа вносит вклад в развитие искусственного интеллекта в медицине, предоставляя новый датасет, модель и бенчмарки.'}, 'en': {'title': 'Empowering Medical AI with Multimodal Learning', 'desc': "This paper introduces GMAI-VL-5.5M, a new multimodal medical dataset designed to enhance general medical AI (GMAI) by combining various specialized medical datasets into image-text pairs. The dataset supports a wide range of medical tasks and includes high-quality data that improves the model's understanding of both visual and textual information. The authors propose a three-stage training strategy for the GMAI-VL model, which significantly boosts its performance in tasks like visual question answering and medical image diagnosis. Experimental results show that GMAI-VL sets new benchmarks in the medical domain, demonstrating its effectiveness in aiding clinical decision-making."}, 'zh': {'title': '医学领域的多模态智能突破', 'desc': '尽管通用人工智能（如GPT-4）取得了显著进展，但在医学领域的有效性仍然受到限制，因为缺乏专业的医学知识。为了解决这个问题，我们提出了GMAI-VL-5.5M，这是一个通过将数百个专业医学数据集转换为精心构建的图像-文本对而创建的综合多模态医学数据集。基于这个多模态数据集，我们提出了GMAI-VL，一个具有逐步三阶段训练策略的通用医学视觉-语言模型。实验评估表明，GMAI-VL在视觉问答和医学图像诊断等多种多模态医学任务中达到了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2411.16681', 'title': 'Factorized Visual Tokenization and Generation', 'url': 'https://huggingface.co/papers/2411.16681', 'abstract': 'Visual tokenizers are fundamental to image generation. They convert visual data into discrete tokens, enabling transformer-based models to excel at image generation. Despite their success, VQ-based tokenizers like VQGAN face significant limitations due to constrained vocabulary sizes. Simply expanding the codebook often leads to training instability and diminishing performance gains, making scalability a critical challenge. In this work, we introduce Factorized Quantization (FQ), a novel approach that revitalizes VQ-based tokenizers by decomposing a large codebook into multiple independent sub-codebooks. This factorization reduces the lookup complexity of large codebooks, enabling more efficient and scalable visual tokenization. To ensure each sub-codebook captures distinct and complementary information, we propose a disentanglement regularization that explicitly reduces redundancy, promoting diversity across the sub-codebooks. Furthermore, we integrate representation learning into the training process, leveraging pretrained vision models like CLIP and DINO to infuse semantic richness into the learned representations. This design ensures our tokenizer captures diverse semantic levels, leading to more expressive and disentangled representations. Experiments show that the proposed FQGAN model substantially improves the reconstruction quality of visual tokenizers, achieving state-of-the-art performance. We further demonstrate that this tokenizer can be effectively adapted into auto-regressive image generation. https://showlab.github.io/FQGAN', 'score': 7, 'issue_id': 780, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '966d673404fb7a77', 'authors': ['Zechen Bai', 'Jianxiong Gao', 'Ziteng Gao', 'Pichao Wang', 'Zheng Zhang', 'Tong He', 'Mike Zheng Shou'], 'affiliations': ['Amazon', 'Fudan University', 'Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.16681.jpg', 'data': {'categories': ['#optimization', '#architecture', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Факторизованная квантизация: новый шаг в масштабируемой токенизации изображений', 'desc': 'Статья представляет новый подход к визуальным токенизаторам для генерации изображений - Факторизованную Квантизацию (FQ). Этот метод разбивает большой кодбук на несколько независимых под-кодбуков, что позволяет эффективно масштабировать процесс токенизации. Авторы предлагают регуляризацию для уменьшения избыточности и повышения разнообразия между под-кодбуками. Интеграция предобученных моделей компьютерного зрения, таких как CLIP и DINO, обогащает семантическое представление токенов.'}, 'en': {'title': 'Revitalizing Image Generation with Factorized Quantization', 'desc': 'This paper presents a new method called Factorized Quantization (FQ) to improve visual tokenizers used in image generation. Traditional VQ-based tokenizers struggle with limited vocabulary sizes, which can hinder performance and scalability. FQ addresses this by breaking down a large codebook into smaller, independent sub-codebooks, reducing complexity and enhancing efficiency. Additionally, the method incorporates disentanglement regularization and representation learning to ensure diverse and rich semantic representations, leading to better image generation results.'}, 'zh': {'title': '因子化量化：提升视觉标记器的效率与表现', 'desc': '视觉标记器是图像生成的基础，它将视觉数据转换为离散的标记，使基于变换器的模型在图像生成方面表现出色。尽管VQGAN等基于VQ的标记器取得了一定成功，但由于词汇量有限，它们面临着显著的局限性。我们提出了一种新方法——因子化量化（FQ），通过将大型代码本分解为多个独立的子代码本，来解决这一问题，从而提高了视觉标记化的效率和可扩展性。实验表明，FQGAN模型显著提高了视觉标记器的重建质量，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.16657', 'title': 'DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation', 'url': 'https://huggingface.co/papers/2411.16657', 'abstract': "Storytelling video generation (SVG) has recently emerged as a task to create long, multi-motion, multi-scene videos that consistently represent the story described in the input text script. SVG holds great potential for diverse content creation in media and entertainment; however, it also presents significant challenges: (1) objects must exhibit a range of fine-grained, complex motions, (2) multiple objects need to appear consistently across scenes, and (3) subjects may require multiple motions with seamless transitions within a single scene. To address these challenges, we propose DreamRunner, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DreamRunner presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DreamRunner with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DreamRunner exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate multi-object interactions with qualitative examples.", 'score': 7, 'issue_id': 777, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '02cf3312e8d1f6ca', 'authors': ['Zun Wang', 'Jialu Li', 'Han Lin', 'Jaehong Yoon', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.16657.jpg', 'data': {'categories': ['#multimodal', '#3d', '#video', '#story_generation'], 'emoji': '🎬', 'ru': {'title': 'DreamRunner: От сценария к видео с помощью ИИ', 'desc': 'DreamRunner - это новый метод генерации видео по текстовому сценарию, который использует большую языковую модель для структурирования входных данных. Он применяет адаптацию на основе извлечения информации для захвата целевых движений объектов в каждой сцене. DreamRunner также предлагает новый модуль пространственно-временного 3D-внимания и внедрения приоров для точного связывания объектов и движений. Метод демонстрирует передовые результаты в согласованности персонажей, соответствии тексту и плавных переходах.'}, 'en': {'title': 'DreamRunner: Crafting Seamless Storytelling Videos from Text', 'desc': 'This paper introduces DreamRunner, a new method for generating storytelling videos from text scripts. It addresses challenges in creating complex motions and maintaining object consistency across scenes by using a large language model for scene planning and a retrieval-augmented approach for motion customization. The method incorporates a novel spatial-temporal region-based attention module to ensure precise object-motion binding and semantic control in video frames. DreamRunner outperforms existing models in character consistency and smooth transitions, showcasing its effectiveness in generating multi-object interactions and adhering to compositional text prompts.'}, 'zh': {'title': 'DreamRunner：创新的故事视频生成方法', 'desc': '故事视频生成（SVG）是一项新兴任务，旨在根据输入文本脚本创建长篇、多动作、多场景的视频。该方法面临着多个挑战，包括对象需要展现复杂的细微动作，以及多个对象在不同场景中的一致性。为了解决这些问题，我们提出了DreamRunner，这是一种新颖的故事到视频生成方法，利用大型语言模型（LLM）进行场景规划和对象布局。DreamRunner还引入了空间-时间区域基础的3D注意力机制，能够实现细粒度的对象动作绑定和逐帧语义控制，展现出在角色一致性和文本对齐方面的先进性能。'}}}, {'id': 'https://huggingface.co/papers/2411.16034', 'title': 'VisualLens: Personalization through Visual History', 'url': 'https://huggingface.co/papers/2411.16034', 'abstract': "We hypothesize that a user's visual history with images reflecting their daily life, offers valuable insights into their interests and preferences, and can be leveraged for personalization. Among the many challenges to achieve this goal, the foremost is the diversity and noises in the visual history, containing images not necessarily related to a recommendation task, not necessarily reflecting the user's interest, or even not necessarily preference-relevant. Existing recommendation systems either rely on task-specific user interaction logs, such as online shopping history for shopping recommendations, or focus on text signals. We propose a novel approach, VisualLens, that extracts, filters, and refines image representations, and leverages these signals for personalization. We created two new benchmarks with task-agnostic visual histories, and show that our method improves over state-of-the-art recommendations by 5-10% on Hit@3, and improves over GPT-4o by 2-5%. Our approach paves the way for personalized recommendations in scenarios where traditional methods fail.", 'score': 6, 'issue_id': 781, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '5eece5978da5fe1d', 'authors': ['Wang Bill Zhu', 'Deqing Fu', 'Kai Sun', 'Yi Lu', 'Zhaojiang Lin', 'Seungwhan Moon', 'Kanika Narang', 'Mustafa Canim', 'Yue Liu', 'Anuj Kumar', 'Xin Luna Dong'], 'affiliations': ['Meta', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2411.16034.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Персонализированные рекомендации через призму визуальной истории пользователя', 'desc': 'Статья представляет новый подход VisualLens для персонализации рекомендаций на основе визуальной истории пользователя. Авторы предлагают метод извлечения, фильтрации и уточнения представлений изображений для улучшения рекомендаций. Они создали два новых бенчмарка с задаче-агностичными визуальными историями. Результаты показывают улучшение на 5-10% по метрике Hit@3 по сравнению с современными методами рекомендаций.'}, 'en': {'title': 'Unlocking Personalization Through Visual History', 'desc': "This paper introduces VisualLens, a new method for enhancing personalized recommendations by utilizing a user's visual history of images. The authors argue that these images, which reflect daily life, can provide insights into user interests and preferences, despite challenges like noise and irrelevant content. VisualLens processes and refines image representations to filter out unhelpful data, improving the quality of recommendations. The results show that this approach outperforms existing systems, achieving a 5-10% increase in recommendation accuracy on new benchmarks."}, 'zh': {'title': '利用视觉历史实现个性化推荐', 'desc': '我们假设用户的视觉历史可以提供关于他们兴趣和偏好的重要信息，这些信息可以用于个性化推荐。当前的推荐系统通常依赖于特定任务的用户交互记录，或者关注文本信号，而忽视了视觉信息的潜力。我们提出了一种新方法，VisualLens，通过提取、过滤和优化图像表示，利用这些信号进行个性化推荐。我们的实验表明，该方法在无任务视觉历史的基准测试中，推荐效果比现有技术提高了5-10%。'}}}, {'id': 'https://huggingface.co/papers/2411.16489', 'title': 'O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?', 'url': 'https://huggingface.co/papers/2411.16489', 'abstract': "This paper presents a critical examination of current approaches to replicating OpenAI's O1 model capabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation techniques. While our previous work explored the fundamental technical path to O1 replication, this study reveals how simple distillation from O1's API, combined with supervised fine-tuning, can achieve superior performance on complex mathematical reasoning tasks. Through extensive experiments, we show that a base model fine-tuned on simply tens of thousands of samples O1-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics Examination (AIME) with minimal technical complexity. Moreover, our investigation extends beyond mathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse tasks: hallucination, safety and open-domain QA. Notably, despite training only on mathematical problem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became significantly less susceptible to sycophancy after fine-tuning. We deliberately make this finding public to promote transparency in AI research and to challenge the current trend of obscured technical claims in the field. Our work includes: (1) A detailed technical exposition of the distillation process and its effectiveness, (2) A comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based on their technical transparency and reproducibility, (3) A critical discussion of the limitations and potential risks of over-relying on distillation approaches, our analysis culminates in a crucial bitter lesson: while the pursuit of more capable AI systems is important, the development of researchers grounded in first-principles thinking is paramount.", 'score': 5, 'issue_id': 780, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '9d7613aad6cae404', 'authors': ['Zhen Huang', 'Haoyang Zou', 'Xuefeng Li', 'Yixiu Liu', 'Yuxiang Zheng', 'Ethan Chern', 'Shijie Xia', 'Yiwei Qin', 'Weizhe Yuan', 'Pengfei Liu'], 'affiliations': ['Generative AI Research Lab (GAIR)', 'NYU', 'SII', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2411.16489.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#data', '#math', '#benchmark', '#transfer_learning', '#hallucinations', '#training'], 'emoji': '🧠', 'ru': {'title': 'Дистилляция знаний: скрытый путь к репликации передовых языковых моделей', 'desc': 'Эта статья представляет критический анализ текущих подходов к репликации возможностей модели O1 от OpenAI, уделяя особое внимание широко распространенному, но часто нераскрываемому использованию методов дистилляции знаний. Исследование показывает, как простая дистилляция из API O1 в сочетании с контролируемой тонкой настройкой может достичь превосходной производительности в сложных математических задачах. Эксперименты демонстрируют, что базовая модель, настроенная на десятках тысяч образцов дистиллированных из O1 цепочек рассуждений, превосходит O1-preview на американском математическом экзамене AIME при минимальной технической сложности. Авторы также исследуют возможности обобщения моделей, дистиллированных из O1, на различные задачи, включая галлюцинации, безопасность и открытые вопросно-ответные системы.'}, 'en': {'title': 'Unlocking AI Potential: The Power of Transparent Distillation', 'desc': "This paper critically analyzes the methods used to replicate the capabilities of OpenAI's O1 model, emphasizing the often hidden use of knowledge distillation techniques. The authors demonstrate that fine-tuning a base model with data distilled from O1's API can lead to better performance on complex mathematical reasoning tasks compared to the original O1 model. Their experiments reveal that models trained on O1-distilled data not only excel in math but also generalize well to other tasks, showing reduced biases and improved safety. The study advocates for transparency in AI research and highlights the importance of foundational understanding in developing advanced AI systems."}, 'zh': {'title': '知识蒸馏：提升AI模型性能的关键', 'desc': '本文对当前复制OpenAI O1模型能力的方法进行了深入分析，特别关注知识蒸馏技术的广泛使用。研究表明，通过简单的从O1 API进行蒸馏，并结合监督微调，可以在复杂的数学推理任务上实现优越的性能。我们的实验显示，经过微调的基础模型在美国邀请数学考试（AIME）中表现优于O1预览，且技术复杂性较低。此外，尽管模型仅在数学问题解决数据上进行训练，但在开放式问答任务中也展现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2411.16318', 'title': 'One Diffusion to Generate Them All', 'url': 'https://huggingface.co/papers/2411.16318', 'abstract': 'We introduce OneDiffusion, a versatile, large-scale diffusion model that seamlessly supports bidirectional image synthesis and understanding across diverse tasks. It enables conditional generation from inputs such as text, depth, pose, layout, and semantic maps, while also handling tasks like image deblurring, upscaling, and reverse processes such as depth estimation and segmentation. Additionally, OneDiffusion allows for multi-view generation, camera pose estimation, and instant personalization using sequential image inputs. Our model takes a straightforward yet effective approach by treating all tasks as frame sequences with varying noise scales during training, allowing any frame to act as a conditioning image at inference time. Our unified training framework removes the need for specialized architectures, supports scalable multi-task training, and adapts smoothly to any resolution, enhancing both generalization and scalability. Experimental results demonstrate competitive performance across tasks in both generation and prediction such as text-to-image, multiview generation, ID preservation, depth estimation and camera pose estimation despite relatively small training dataset. Our code and checkpoint are freely available at https://github.com/lehduong/OneDiffusion', 'score': 4, 'issue_id': 782, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '2719da6b8a88249f', 'authors': ['Duong H. Le', 'Tuan Pham', 'Sangho Lee', 'Christopher Clark', 'Aniruddha Kembhavi', 'Stephan Mandt', 'Ranjay Krishna', 'Jiasen Lu'], 'affiliations': ['AI2', 'University of California, Irvine', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.16318.jpg', 'data': {'categories': ['#cv', '#diffusion', '#training', '#multimodal', '#open_source'], 'emoji': '🎨', 'ru': {'title': 'Универсальная диффузионная модель для синтеза и анализа изображений', 'desc': 'OneDiffusion - это универсальная крупномасштабная диффузионная модель, поддерживающая двунаправленный синтез и понимание изображений для различных задач. Модель позволяет выполнять условную генерацию на основе текста, глубины, позы, макета и семантических карт, а также решать задачи устранения размытия, увеличения масштаба и обратные процессы. OneDiffusion использует единый подход к обучению, рассматривая все задачи как последовательности кадров с различными уровнями шума. Экспериментальные результаты демонстрируют конкурентоспособную производительность модели в задачах генерации и предсказания, несмотря на относительно небольшой набор данных для обучения.'}, 'en': {'title': 'OneDiffusion: Unifying Image Synthesis and Understanding with Versatile Diffusion', 'desc': 'OneDiffusion is a powerful diffusion model designed for various image synthesis and understanding tasks. It can generate images based on different inputs like text and depth, and also perform tasks such as image deblurring and segmentation. The model treats all tasks as sequences of frames with different noise levels, allowing flexibility during inference. Its unified training approach enhances scalability and generalization, achieving strong performance across multiple tasks with a relatively small dataset.'}, 'zh': {'title': 'OneDiffusion：多任务图像生成与理解的统一模型', 'desc': 'OneDiffusion是一种多功能的大规模扩散模型，能够支持双向图像合成和理解。它可以根据文本、深度、姿态、布局和语义图等输入进行条件生成，同时处理图像去模糊、放大和深度估计等任务。该模型通过将所有任务视为具有不同噪声尺度的帧序列进行训练，简化了训练过程，并允许在推理时使用任意帧作为条件图像。实验结果表明，OneDiffusion在生成和预测任务中表现出色，尽管训练数据集相对较小。'}}}, {'id': 'https://huggingface.co/papers/2411.16443', 'title': 'SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis', 'url': 'https://huggingface.co/papers/2411.16443', 'abstract': "Text-based generation and editing of 3D scenes hold significant potential for streamlining content creation through intuitive user interactions. While recent advances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time rendering, existing methods are often specialized and task-focused, lacking a unified framework for both generation and editing. In this paper, we introduce SplatFlow, a comprehensive framework that addresses this gap by enabling direct 3DGS generation and editing. SplatFlow comprises two main components: a multi-view rectified flow (RF) model and a Gaussian Splatting Decoder (GSDecoder). The multi-view RF model operates in latent space, generating multi-view images, depths, and camera poses simultaneously, conditioned on text prompts, thus addressing challenges like diverse scene scales and complex camera trajectories in real-world settings. Then, the GSDecoder efficiently translates these latent outputs into 3DGS representations through a feed-forward 3DGS method. Leveraging training-free inversion and inpainting techniques, SplatFlow enables seamless 3DGS editing and supports a broad range of 3D tasks-including object editing, novel view synthesis, and camera pose estimation-within a unified framework without requiring additional complex pipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K datasets, demonstrating its versatility and effectiveness in various 3D generation, editing, and inpainting-based tasks.", 'score': 4, 'issue_id': 779, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '8447f5d110a89105', 'authors': ['Hyojun Go', 'Byeongjun Park', 'Jiho Jang', 'Jin-Young Kim', 'Soonwoo Kwon', 'Changick Kim'], 'affiliations': ['KAIST', 'Twelve Labs'], 'pdf_title_img': 'assets/pdf/title_img/2411.16443.jpg', 'data': {'categories': ['#3d'], 'emoji': '🎨', 'ru': {'title': 'Универсальная система для интуитивного создания и редактирования 3D-сцен', 'desc': 'SplatFlow - это комплексная система для генерации и редактирования 3D-сцен на основе текстовых запросов. Она состоит из двух основных компонентов: многоракурсной модели выпрямленного потока и декодера гауссовского сплаттинга. Система позволяет генерировать многоракурсные изображения, карты глубины и положения камер одновременно, а затем эффективно преобразовывать их в 3D-представления. SplatFlow поддерживает широкий спектр задач 3D-моделирования, включая редактирование объектов, синтез новых ракурсов и оценку положения камеры, в рамках единой системы.'}, 'en': {'title': 'SplatFlow: Unifying 3D Scene Generation and Editing', 'desc': 'This paper presents SplatFlow, a unified framework for generating and editing 3D scenes using 3D Gaussian Splatting (3DGS). It features a multi-view rectified flow model that generates images, depths, and camera poses from text prompts, addressing challenges in scene diversity and camera movement. The framework also includes a Gaussian Splatting Decoder that converts latent outputs into 3DGS representations efficiently. SplatFlow supports various 3D tasks like object editing and novel view synthesis, demonstrating its effectiveness on multiple datasets without the need for complex pipelines.'}, 'zh': {'title': 'SplatFlow：统一的3D生成与编辑框架', 'desc': '本文介绍了一种名为SplatFlow的综合框架，旨在简化3D场景的生成和编辑。SplatFlow结合了多视角整流流模型和高斯点云解码器，能够根据文本提示同时生成多视角图像、深度和相机姿态。该框架通过无训练反演和修补技术，实现了无缝的3D高斯点云编辑，支持多种3D任务，如物体编辑和新视角合成。我们在MVImgNet和DL3DV-7K数据集上验证了SplatFlow的能力，展示了其在多种3D生成和编辑任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.16085', 'title': 'Cautious Optimizers: Improving Training with One Line of Code', 'url': 'https://huggingface.co/papers/2411.16085', 'abstract': "AdamW has been the default optimizer for transformer pretraining. For many years, our community searches for faster and more stable optimizers with only constraint positive outcomes. In this work, we propose a single-line modification in Pytorch to any momentum-based optimizer, which we rename Cautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing speed-up on Llama and MAE pretraining up to 1.47times. Code is available at https://github.com/kyleliang919/C-Optim", 'score': 4, 'issue_id': 777, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '48a2e1454fdde298', 'authors': ['Kaizhao Liang', 'Lizhang Chen', 'Bo Liu', 'Qiang Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.16085.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Осторожная оптимизация: простое изменение, большой результат', 'desc': 'Статья представляет модификацию оптимизаторов на основе импульса, названную Cautious Optimizer. Авторы предлагают простое изменение в коде PyTorch, которое сохраняет функцию Гамильтона для Adam и гарантирует сходимость по анализу Ляпунова. Теоретический анализ раскрывает целое семейство новых оптимизаторов. Эмпирические эксперименты показывают ускорение предобучения Llama и MAE до 1.47 раз.'}, 'en': {'title': 'Cautious Optimizer: Speeding Up Transformer Pretraining!', 'desc': 'This paper introduces a new optimizer called the Cautious Optimizer, which is a simple modification of existing momentum-based optimizers like AdamW and Lion. The modification preserves the Hamiltonian function of Adam, ensuring that the convergence properties remain intact according to Lyapunov stability analysis. The authors demonstrate that this new optimizer can significantly speed up the pretraining of models like Llama and MAE, achieving improvements of up to 1.47 times. Additionally, the research opens the door to a new family of optimizers, expanding the options available for machine learning practitioners.'}, 'zh': {'title': '提升变换器预训练速度的新优化器', 'desc': '本文提出了一种新的优化器，称为Cautious Optimizer，旨在提高变换器预训练的速度和稳定性。通过对现有的动量优化器进行简单的修改，我们的理论分析表明，这种修改保持了Adam的哈密顿函数，并且在Lyapunov分析下不破坏收敛性保证。我们还揭示了一系列新的优化器，并选择了其中最简单的进行实证实验，结果显示在Llama和MAE预训练中速度提升可达1.47倍。相关代码已在GitHub上发布。'}}}, {'id': 'https://huggingface.co/papers/2411.14486', 'title': 'The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz', 'url': 'https://huggingface.co/papers/2411.14486', 'abstract': "This research introduces a novel evaluation framework designed to assess large language models' (LLMs) ability to acknowledge uncertainty on 675 fundamentally unsolvable problems. Using a curated dataset of graduate-level grand challenge questions with intentionally unknowable answers, we evaluated twelve state-of-the-art LLMs, including both open and closed-source models, on their propensity to admit ignorance rather than generate plausible but incorrect responses. The best models scored in 62-68% accuracy ranges for admitting the problem solution was unknown in fields ranging from biology to philosophy and mathematics. We observed an inverse relationship between problem difficulty and model accuracy, with GPT-4 demonstrating higher rates of uncertainty acknowledgment on more challenging problems (35.8%) compared to simpler ones (20.0%). This pattern indicates that models may be more prone to generate speculative answers when problems appear more tractable. The study also revealed significant variations across problem categories, with models showing difficulty in acknowledging uncertainty in invention and NP-hard problems while performing relatively better on philosophical and psychological challenges. These results contribute to the growing body of research on artificial general intelligence (AGI) assessment by highlighting the importance of uncertainty recognition as a critical component of future machine intelligence evaluation. This impossibility test thus extends previous theoretical frameworks for universal intelligence testing by providing empirical evidence of current limitations in LLMs' ability to recognize their own knowledge boundaries, suggesting new directions for improving model training architectures and evaluation approaches.", 'score': 4, 'issue_id': 777, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': 'c30a94b30cace49a', 'authors': ['David Noever', 'Forrest McKee'], 'affiliations': ['PeopleTec, Inc., Huntsville, AL'], 'pdf_title_img': 'assets/pdf/title_img/2411.14486.jpg', 'data': {'categories': ['#architecture', '#training', '#benchmark', '#interpretability', '#agi', '#dataset'], 'emoji': '🤔', 'ru': {'title': 'Признание незнания: ключевой аспект оценки искусственного интеллекта', 'desc': 'Это исследование представляет новую систему оценки способности больших языковых моделей (LLM) признавать неопределенность на 675 принципиально нерешаемых проблемах. Двенадцать современных LLM были оценены на их склонность признавать незнание, а не генерировать правдоподобные, но неверные ответы. Лучшие модели показали точность 62-68% в признании неизвестности решения проблемы в различных областях. Исследование выявило обратную зависимость между сложностью проблемы и точностью модели, а также значительные различия между категориями проблем.'}, 'en': {'title': 'Evaluating Uncertainty: A New Benchmark for Language Models', 'desc': "This research presents a new framework to evaluate how well large language models (LLMs) recognize their own uncertainty when faced with unsolvable problems. By testing twelve advanced LLMs on a set of graduate-level questions that have no answers, the study found that the best models could admit ignorance 62-68% of the time. Interestingly, the models were more likely to acknowledge uncertainty on harder problems, with GPT-4 showing a 35.8% acknowledgment rate on difficult questions. The findings emphasize the need for better training and evaluation methods to enhance LLMs' ability to recognize their knowledge limits, which is crucial for advancing artificial general intelligence (AGI)."}, 'zh': {'title': '承认不确定性：评估大型语言模型的新视角', 'desc': '本研究提出了一种新的评估框架，用于评估大型语言模型（LLMs）在675个根本无法解决的问题上承认不确定性的能力。我们使用了一组经过精心挑选的研究生级别的重大挑战问题数据集，评估了包括开源和闭源模型在内的十二个最先进的LLMs，观察它们承认无知的倾向。结果显示，最佳模型在承认问题解决方案未知的准确率范围为62%到68%，并且在更具挑战性的问题上（如生物学、哲学和数学）表现出更高的不确定性承认率。研究还发现，不同问题类别之间存在显著差异，模型在承认发明和NP难题的不确定性时表现较差，而在哲学和心理学挑战中表现相对较好。'}}}, {'id': 'https://huggingface.co/papers/2411.16035', 'title': 'Predicting Emergent Capabilities by Finetuning', 'url': 'https://huggingface.co/papers/2411.16035', 'abstract': 'A fundamental open challenge in modern LLM scaling is the lack of understanding around emergent capabilities. In particular, language model pretraining loss is known to be highly predictable as a function of compute. However, downstream capabilities are far less predictable -- sometimes even exhibiting emergent jumps -- which makes it challenging to anticipate the capabilities of future models. In this work, we first pose the task of emergence prediction: given access to current LLMs that have random few-shot accuracy on a task, can we predict whether future models (GPT-N+1) will have non-trivial accuracy on that task? We then discover a simple insight for this problem: finetuning LLMs on a given task can shift the point in scaling at which emergence occurs towards less capable models. To operationalize this insight, we can finetune LLMs with varying amounts of data and fit a parametric function that predicts when emergence will occur (i.e., "emergence laws"). We validate this approach using four standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and CoLA). Using only small-scale LLMs, we find that, in some cases, we can accurately predict whether models trained with up to 4x more compute have emerged. Finally, we present a case study of two realistic uses for emergence prediction.', 'score': 3, 'issue_id': 779, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': 'a3e818d78a8bea58', 'authors': ['Charlie Snell', 'Eric Wallace', 'Dan Klein', 'Sergey Levine'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2411.16035.jpg', 'data': {'categories': ['#optimization', '#training', '#agi', '#benchmark', '#open_source'], 'emoji': '🔮', 'ru': {'title': 'Предсказание будущего ИИ: раскрытие тайн эмерджентности в языковых моделях', 'desc': 'Статья посвящена проблеме предсказания возникновения новых способностей у крупномасштабных языковых моделей (LLM). Авторы предлагают метод предсказания эмерджентности, основанный на дообучении моделей на конкретных задачах. Они обнаружили, что дообучение может сдвинуть точку возникновения эмерджентности в сторону менее мощных моделей. Используя этот подход, исследователи смогли точно предсказать эмерджентность для моделей, обученных с использованием в 4 раза больше вычислительных ресурсов.'}, 'en': {'title': 'Predicting Emergence: Unlocking Future LLM Capabilities', 'desc': 'This paper addresses the challenge of predicting emergent capabilities in large language models (LLMs) as they scale. While the pretraining loss of LLMs is predictable based on compute resources, their downstream performance can show unexpected jumps in capability. The authors introduce the concept of emergence prediction, which involves fine-tuning LLMs on specific tasks to determine when these emergent capabilities will appear. They validate their approach using standard NLP benchmarks and demonstrate that it is possible to predict the emergence of capabilities in future models based on the performance of smaller models.'}, 'zh': {'title': '预测语言模型能力的突破', 'desc': '本研究探讨了大型语言模型（LLM）在扩展时出现的能力预测问题。我们发现，通过对特定任务进行微调，可以提前预测未来模型在该任务上的表现。研究表明，微调可以将能力出现的临界点向能力较低的模型移动。我们在多个自然语言处理基准上验证了这一方法，并展示了如何利用这一预测能力进行实际应用。'}}}, {'id': 'https://huggingface.co/papers/2411.16341', 'title': 'From CISC to RISC: language-model guided assembly transpilation', 'url': 'https://huggingface.co/papers/2411.16341', 'abstract': "The transition from x86 to ARM architecture is becoming increasingly common across various domains, primarily driven by ARM's energy efficiency and improved performance across traditional sectors. However, this ISA shift poses significant challenges, mainly due to the extensive legacy ecosystem of x86 software and lack of portability across proprietary ecosystems and software stacks. This paper introduces CRT, a lightweight LLM-based transpiler that automatically converts x86 assembly to ARM assembly. Our approach bridges the fundamental architectural gap between x86's CISC-based and ARM's RISC-based computing paradigms while preserving program semantics and optimizing performance. We evaluate CRT on diverse real-world applications, achieving 79.25% translation accuracy from x86 to ARMv5 on our comprehensive test suite, and an 88.68% accuracy from x86 to RISC-V. In practical deployments on Apple M2 hardware (ARMv8), our transpiled code achieves 1.73times speedup compared to Apple's Rosetta 2 virtualization engine, while delivering 2.41times memory efficiency and 1.47times better energy consumption. Through testing and analysis, we show that CRT successfully navigates the CISC/RISC divide and generates correctly executable RISC code despite machine ``language'' barriers. We release our code, models, training datasets, and benchmarks at: https://ahmedheakl.github.io/asm2asm/.", 'score': 3, 'issue_id': 778, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '1d28eaae89074f91', 'authors': ['Ahmed Heakl', 'Chaimaa Abi', 'Rania Hossam', 'Abdulrahman Mahmoud'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE'], 'pdf_title_img': 'assets/pdf/title_img/2411.16341.jpg', 'data': {'categories': ['#open_source', '#dataset', '#architecture', '#benchmark'], 'emoji': '🔄', 'ru': {'title': 'Преодоление барьера CISC/RISC: автоматическая трансляция x86 в ARM с помощью ИИ', 'desc': 'Статья представляет CRT - лёгкий транспилятор на основе большой языковой модели, который автоматически конвертирует ассемблерный код x86 в ассемблер ARM. Этот подход преодолевает фундаментальный архитектурный разрыв между вычислительными парадигмами CISC (x86) и RISC (ARM), сохраняя семантику программ и оптимизируя производительность. Авторы оценивают CRT на различных реальных приложениях, достигая 79.25% точности перевода с x86 на ARMv5. В практических развертываниях на оборудовании Apple M2 (ARMv8) транспилированный код достигает ускорения в 1.73 раза по сравнению с виртуализационным движком Apple Rosetta 2.'}, 'en': {'title': 'Bridging the CISC/RISC Divide with CRT Transpiler', 'desc': "This paper presents CRT, a lightweight LLM-based transpiler designed to convert x86 assembly code into ARM assembly code. The transition from x86 to ARM architecture is challenging due to the differences in their instruction set architectures (ISAs), specifically x86's Complex Instruction Set Computing (CISC) and ARM's Reduced Instruction Set Computing (RISC). CRT effectively bridges this gap while maintaining the original program's functionality and optimizing performance. The evaluation shows that CRT achieves high translation accuracy and outperforms existing solutions in terms of speed, memory efficiency, and energy consumption on ARM hardware."}, 'zh': {'title': '轻松跨越架构鸿沟，提升性能与效率', 'desc': '本论文介绍了一种名为CRT的轻量级LLM基础的转译器，能够自动将x86汇编代码转换为ARM汇编代码。该方法解决了x86的复杂指令集（CISC）与ARM的精简指令集（RISC）之间的架构差异，同时保持程序语义并优化性能。通过在真实应用上的评估，CRT在x86到ARMv5的转换准确率达到79.25%，在x86到RISC-V的转换准确率达到88.68%。在Apple M2硬件上的实际部署中，转译后的代码相比于Apple的Rosetta 2虚拟化引擎实现了1.73倍的速度提升和2.41倍的内存效率。'}}}, {'id': 'https://huggingface.co/papers/2411.15671', 'title': 'Best of Both Worlds: Advantages of Hybrid Graph Sequence Models', 'url': 'https://huggingface.co/papers/2411.15671', 'abstract': 'Modern sequence models (e.g., Transformers, linear RNNs, etc.) emerged as dominant backbones of recent deep learning frameworks, mainly due to their efficiency, representational power, and/or ability to capture long-range dependencies. Adopting these sequence models for graph-structured data has recently gained popularity as the alternative to Message Passing Neural Networks (MPNNs). There is, however, a lack of a common foundation about what constitutes a good graph sequence model, and a mathematical description of the benefits and deficiencies in adopting different sequence models for learning on graphs. To this end, we first present Graph Sequence Model (GSM), a unifying framework for adopting sequence models for graphs, consisting of three main steps: (1) Tokenization, which translates the graph into a set of sequences; (2) Local Encoding, which encodes local neighborhoods around each node; and (3) Global Encoding, which employs a scalable sequence model to capture long-range dependencies within the sequences. This framework allows us to understand, evaluate, and compare the power of different sequence model backbones in graph tasks. Our theoretical evaluations of the representation power of Transformers and modern recurrent models through the lens of global and local graph tasks show that there are both negative and positive sides for both types of models. Building on this observation, we present GSM++, a fast hybrid model that uses the Hierarchical Affinity Clustering (HAC) algorithm to tokenize the graph into hierarchical sequences, and then employs a hybrid architecture of Transformer to encode these sequences. Our theoretical and experimental results support the design of GSM++, showing that GSM++ outperforms baselines in most benchmark evaluations.', 'score': 2, 'issue_id': 779, 'pub_date': '2024-11-23', 'pub_date_card': {'ru': '23 ноября', 'en': 'November 23', 'zh': '11月23日'}, 'hash': '540358181ae0274e', 'authors': ['Ali Behrouz', 'Ali Parviz', 'Mahdi Karami', 'Clayton Sanford', 'Bryan Perozzi', 'Vahab Mirrokni'], 'affiliations': ['Google Research', 'New Jersey Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2411.15671.jpg', 'data': {'categories': ['#optimization', '#math', '#architecture', '#benchmark', '#graphs'], 'emoji': '🕸️', 'ru': {'title': 'Унифицированный подход к обработке графов с помощью последовательностных моделей', 'desc': 'Статья представляет унифицированный фреймворк Graph Sequence Model (GSM) для применения последовательностных моделей к графовым данным. GSM состоит из трех этапов: токенизация графа в набор последовательностей, локальное кодирование окрестностей узлов и глобальное кодирование для захвата дальних зависимостей. Авторы теоретически оценивают репрезентативную мощность трансформеров и современных рекуррентных моделей для графовых задач. На основе этого анализа предлагается гибридная модель GSM++, использующая иерархическую кластеризацию для токенизации и архитектуру трансформера для кодирования.'}, 'en': {'title': 'Unifying Sequence Models for Enhanced Graph Learning', 'desc': 'This paper introduces the Graph Sequence Model (GSM), a framework that integrates sequence models with graph-structured data. It consists of three steps: tokenization of graphs into sequences, local encoding of node neighborhoods, and global encoding to capture long-range dependencies. The authors evaluate the strengths and weaknesses of different sequence models, particularly Transformers and recurrent models, in graph tasks. They also propose GSM++, a hybrid model that enhances performance by using hierarchical tokenization and a Transformer architecture, demonstrating superior results in benchmark tests.'}, 'zh': {'title': '图序列模型：结合序列与图的力量', 'desc': '现代序列模型（如变换器和线性递归神经网络）在深度学习框架中占据主导地位，因其高效性、表示能力和捕捉长距离依赖的能力。将这些序列模型应用于图结构数据的研究逐渐受到关注，作为消息传递神经网络（MPNNs）的替代方案。本文提出了图序列模型（GSM），这是一个统一框架，包含图的标记化、局部编码和全局编码三个主要步骤。我们还提出了GSM++，一种快速混合模型，利用层次亲和聚类算法对图进行分层序列标记，并采用变换器的混合架构进行编码，实验结果表明GSM++在大多数基准评估中优于其他模型。'}}}, {'id': 'https://huggingface.co/papers/2411.14525', 'title': 'SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image Segmentation', 'url': 'https://huggingface.co/papers/2411.14525', 'abstract': 'Computed Tomography (CT) is one of the most popular modalities for medical imaging. By far, CT images have contributed to the largest publicly available datasets for volumetric medical segmentation tasks, covering full-body anatomical structures. Large amounts of full-body CT images provide the opportunity to pre-train powerful models, e.g., STU-Net pre-trained in a supervised fashion, to segment numerous anatomical structures. However, it remains unclear in which conditions these pre-trained models can be transferred to various downstream medical segmentation tasks, particularly segmenting the other modalities and diverse targets. To address this problem, a large-scale benchmark for comprehensive evaluation is crucial for finding these conditions. Thus, we collected 87 public datasets varying in modality, target, and sample size to evaluate the transfer ability of full-body CT pre-trained models. We then employed a representative model, STU-Net with multiple model scales, to conduct transfer learning across modalities and targets. Our experimental results show that (1) there may be a bottleneck effect concerning the dataset size in fine-tuning, with more improvement on both small- and large-scale datasets than medium-size ones. (2) Models pre-trained on full-body CT demonstrate effective modality transfer, adapting well to other modalities such as MRI. (3) Pre-training on the full-body CT not only supports strong performance in structure detection but also shows efficacy in lesion detection, showcasing adaptability across target tasks. We hope that this large-scale open evaluation of transfer learning can direct future research in volumetric medical image segmentation.', 'score': 2, 'issue_id': 778, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'eb68ad946d69ba56', 'authors': ['Jin Ye', 'Ying Chen', 'Yanjun Li', 'Haoyu Wang', 'Zhongying Deng', 'Ziyan Huang', 'Yanzhou Su', 'Chenglong Ma', 'Yuanfeng Ji', 'Junjun He'], 'affiliations': ['East China Normal University', 'Shanghai AI Laboratory', 'Stanford University', 'University of Cambridge', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14525.jpg', 'data': {'categories': ['#benchmark', '#healthcare', '#open_source', '#training', '#transfer_learning', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Универсальность предобученных КТ-моделей в медицинской сегментации', 'desc': "Статья посвящена исследованию переноса обучения моделей, предобученных на полноразмерных КТ-изображениях, на другие задачи сегментации медицинских изображений. Авторы создали масштабный бенчмарк из 87 публичных датасетов для оценки эффективности такого переноса. Результаты показывают, что предобученные модели хорошо адаптируются к другим модальностям (например, МРТ) и различным целевым задачам. Исследование выявило эффект 'бутылочного горлышка' в зависимости от размера датасета при тонкой настройке моделей."}, 'en': {'title': 'Unlocking Transfer Learning in Medical Imaging with CT Datasets', 'desc': 'This paper investigates the transferability of models pre-trained on full-body CT images for various medical segmentation tasks. It highlights the effectiveness of the STU-Net model in adapting to different imaging modalities, such as MRI, and diverse anatomical targets. The study reveals that dataset size impacts fine-tuning performance, with both small and large datasets yielding better results than medium-sized ones. Overall, the findings emphasize the potential of using large-scale CT datasets to enhance model performance in medical image segmentation tasks.'}, 'zh': {'title': '全身CT预训练模型的迁移学习能力研究', 'desc': '计算机断层扫描（CT）是医学成像中最常用的技术之一。本文探讨了在不同条件下，基于全身CT预训练模型的迁移学习能力，特别是在其他成像模态和多样化目标的分割任务中。我们收集了87个公共数据集进行评估，结果表明，数据集大小对微调有瓶颈效应，且全身CT预训练模型在迁移到其他模态（如MRI）时表现良好。我们的研究希望为未来的体积医学图像分割研究提供指导。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents', '#agi (3)', '#alignment', '#architecture (7)', '#audio', '#benchmark (9)', '#cv (5)', '#data (1)', '#dataset (5)', '#diffusion (2)', '#ethics', '#games', '#graphs (1)', '#hallucinations (1)', '#healthcare (2)', '#inference', '#interpretability (1)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (2)', '#multilingual', '#multimodal (6)', '#open_source (5)', '#optimization (8)', '#plp', '#rag', '#reasoning (1)', '#rl', '#rlhf', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation (1)', '#survey (1)', '#synthetic (1)', '#training (7)', '#transfer_learning (3)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-11-26 10:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-11-26 10:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-11-26 10:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    