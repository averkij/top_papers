{
    "date": {
        "ru": "24 декабря",
        "en": "December 24",
        "zh": "12月24日"
    },
    "time_utc": "2024-12-24 05:10",
    "weekday": 1,
    "issue_id": 1283,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.17256",
            "title": "B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners",
            "url": "https://huggingface.co/papers/2412.17256",
            "abstract": "In the absence of extensive human-annotated data for complex reasoning tasks, self-improvement -- where models are trained on their own outputs -- has emerged as a primary method for enhancing performance. However, the critical factors underlying the mechanism of these iterative self-improving methods remain poorly understood, such as under what conditions self-improvement is effective, and what are the bottlenecks in the current iterations. In this work, we identify and propose methods to monitor two pivotal factors in this iterative process: (1) the model's ability to generate sufficiently diverse responses (exploration); and (2) the effectiveness of external rewards in distinguishing high-quality candidates from lower-quality ones (exploitation). Using mathematical reasoning as a case study, we begin with a quantitative analysis to track the dynamics of exploration and exploitation, discovering that a model's exploratory capabilities rapidly deteriorate over iterations, and the effectiveness of exploiting external rewards diminishes as well. Motivated by these findings, we introduce B-STaR, a Self-Taught Reasoning framework that autonomously adjusts configurations across iterations to Balance exploration and exploitation, thereby optimizing the self-improving effectiveness based on the current policy model and available rewards. Our experiments on mathematical reasoning, coding, and commonsense reasoning demonstrate that B-STaR not only enhances the model's exploratory capabilities throughout training but also achieves a more effective balance between exploration and exploitation, leading to superior performance.",
            "score": 15,
            "issue_id": 1281,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 декабря",
                "en": "December 23",
                "zh": "12月23日"
            },
            "hash": "1a1ee4818597feae",
            "authors": [
                "Weihao Zeng",
                "Yuzhen Huang",
                "Lulu Zhao",
                "Yijun Wang",
                "Zifei Shan",
                "Junxian He"
            ],
            "affiliations": [
                "BAAI",
                "Tencent",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17256.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#math",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Баланс исследования и эксплуатации для эффективного самосовершенствования ИИ",
                    "desc": "Эта статья посвящена самосовершенствованию моделей машинного обучения в отсутствие большого количества аннотированных данных. Авторы исследуют два ключевых фактора в итеративном процессе самообучения: способность модели генерировать разнообразные ответы (исследование) и эффективность внешних наград в различении высококачественных кандидатов (эксплуатация). На основе анализа они предлагают фреймворк B-STaR, который автоматически балансирует исследование и эксплуатацию для оптимизации процесса самосовершенствования. Эксперименты показывают, что B-STaR улучшает исследовательские возможности модели и достигает лучшего баланса, приводя к повышению производительности в задачах математических рассуждений, кодирования и здравого смысла."
                },
                "en": {
                    "title": "Balancing Exploration and Exploitation for Self-Improvement in ML Models",
                    "desc": "This paper addresses the challenge of improving machine learning models in the absence of large human-annotated datasets, focusing on self-improvement through iterative training on their own outputs. It identifies two key factors that influence this process: the model's ability to explore diverse responses and the effectiveness of external rewards in selecting high-quality outputs. The authors introduce B-STaR, a framework that dynamically adjusts training configurations to maintain a balance between exploration and exploitation. Experiments show that B-STaR enhances exploratory capabilities and improves overall model performance in reasoning tasks."
                },
                "zh": {
                    "title": "自我改进：平衡探索与利用的关键",
                    "desc": "在缺乏大量人工标注数据的复杂推理任务中，自我改进成为提升模型性能的主要方法。本文探讨了自我改进过程中的两个关键因素：模型生成多样化响应的能力（探索）和外部奖励在区分高质量候选项与低质量候选项中的有效性（利用）。通过对数学推理的案例研究，我们发现模型的探索能力在迭代过程中迅速下降，而外部奖励的有效性也随之减弱。为了解决这些问题，我们提出了B-STaR框架，能够在迭代中自我调整配置，以平衡探索与利用，从而优化自我改进的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17451",
            "title": "Diving into Self-Evolving Training for Multimodal Reasoning",
            "url": "https://huggingface.co/papers/2412.17451",
            "abstract": "Reasoning ability is essential for Large Multimodal Models (LMMs). In the absence of multimodal chain-of-thought annotated data, self-evolving training, where the model learns from its own outputs, has emerged as an effective and scalable approach for enhancing reasoning abilities. Despite its growing usage, a comprehensive understanding of self-evolving training, particularly in the context of multimodal reasoning, remains limited. In this paper, we delve into the intricacies of self-evolving training for multimodal reasoning, pinpointing three key factors: Training Method, Reward Model, and Prompt Variation. We systematically examine each factor and explore how various configurations affect the training's effectiveness. Our analysis leads to a set of best practices for each factor, aimed at optimizing multimodal reasoning. Furthermore, we explore the Self-Evolution Dynamics during training and the impact of automatic balancing mechanisms in boosting performance. After all the investigations, we present a final recipe for self-evolving training in multimodal reasoning, encapsulating these design choices into a framework we call MSTaR (Multimodal Self-evolving Training for Reasoning), which is universally effective for models with different sizes on various benchmarks, e.g., surpassing the pre-evolved model significantly on 5 multimodal reasoning benchmarks without using additional human annotations, as demonstrated on MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B). We believe this study fills a significant gap in the understanding of self-evolving training for multimodal reasoning and offers a robust framework for future research. Our policy and reward models, as well as the collected data, is released to facilitate further investigation in multimodal reasoning.",
            "score": 14,
            "issue_id": 1281,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 декабря",
                "en": "December 23",
                "zh": "12月23日"
            },
            "hash": "2866001b23a585e1",
            "authors": [
                "Wei Liu",
                "Junlong Li",
                "Xiwen Zhang",
                "Fan Zhou",
                "Yu Cheng",
                "Junxian He"
            ],
            "affiliations": [
                "Helixon Research",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17451.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Самосовершенствование мультимодальных моделей в искусстве рассуждения",
                    "desc": "Статья исследует самоэволюционирующее обучение для улучшения способностей мультимодальных моделей к рассуждению. Авторы выделяют три ключевых фактора: метод обучения, модель вознаграждения и вариации промптов. На основе анализа предлагается набор лучших практик для каждого фактора. Исследователи представляют фреймворк MSTaR для самоэволюционирующего обучения мультимодальному рассуждению. Результаты показывают значительное улучшение производительности на нескольких бенчмарках без использования дополнительных человеческих аннотаций."
                },
                "en": {
                    "title": "Unlocking Reasoning in Multimodal Models with Self-Evolving Training",
                    "desc": "This paper focuses on improving reasoning abilities in Large Multimodal Models (LMMs) through a method called self-evolving training, which allows models to learn from their own outputs. The authors identify three critical factors that influence the effectiveness of this training: the Training Method, Reward Model, and Prompt Variation. They provide a detailed analysis of how different configurations of these factors can optimize multimodal reasoning performance. The study culminates in the development of a framework named MSTaR, which demonstrates significant improvements in reasoning tasks across various model sizes and benchmarks without requiring additional human annotations."
                },
                "zh": {
                    "title": "自我进化训练：提升多模态推理能力的关键",
                    "desc": "本文探讨了自我进化训练在多模态推理中的应用，强调了推理能力对大型多模态模型的重要性。我们识别了影响训练效果的三个关键因素：训练方法、奖励模型和提示变体，并系统地分析了这些因素的不同配置。研究结果提供了一套最佳实践，旨在优化多模态推理的训练过程。此外，我们提出了MSTaR框架，展示了自我进化训练在不同规模模型上的普遍有效性，显著超越了预先进化模型的表现。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14922",
            "title": "RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response",
            "url": "https://huggingface.co/papers/2412.14922",
            "abstract": "Supervised fine-tuning (SFT) plays a crucial role in adapting large language models (LLMs) to specific domains or tasks. However, as demonstrated by empirical experiments, the collected data inevitably contains noise in practical applications, which poses significant challenges to model performance on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT framework to enhance model capabilities in downstream tasks. To address this challenge, we introduce a robust SFT framework (RobustFT) that performs noise detection and relabeling on downstream task data. For noise identification, our approach employs a multi-expert collaborative system with inference-enhanced models to achieve superior noise detection. In the denoising phase, we utilize a context-enhanced strategy, which incorporates the most relevant and confident knowledge followed by careful assessment to generate reliable annotations. Additionally, we introduce an effective data selection mechanism based on response entropy, ensuring only high-quality samples are retained for fine-tuning. Extensive experiments conducted on multiple LLMs across five datasets demonstrate RobustFT's exceptional performance in noisy scenarios.",
            "score": 12,
            "issue_id": 1281,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "9cc4a703e686ea87",
            "authors": [
                "Junyu Luo",
                "Xiao Luo",
                "Kaize Ding",
                "Jingyang Yuan",
                "Zhiping Xiao",
                "Ming Zhang"
            ],
            "affiliations": [
                "Northwestern University",
                "Peking University",
                "University of California, Los Angeles",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14922.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧼",
                "ru": {
                    "title": "Чистая дообучка: RobustFT для устойчивых языковых моделей",
                    "desc": "Статья представляет новый фреймворк RobustFT для устойчивой дообучения больших языковых моделей (LLM) на зашумленных данных. RobustFT использует систему экспертов для обнаружения шума и переразметки данных. Фреймворк применяет контекстно-улучшенную стратегию для генерации надежных аннотаций и механизм отбора данных на основе энтропии ответов. Эксперименты на пяти наборах данных показали высокую эффективность RobustFT в сценариях с шумом."
                },
                "en": {
                    "title": "Enhancing Language Models with Robust Fine-Tuning",
                    "desc": "This paper presents a new framework called RobustFT for supervised fine-tuning (SFT) of large language models (LLMs) that addresses the issue of noisy data in training. The framework includes a multi-expert system for detecting noise in the data, which helps improve the quality of the training process. It also features a context-enhanced strategy for relabeling data, ensuring that only the most relevant and reliable information is used for fine-tuning. Experiments show that RobustFT significantly enhances model performance on downstream tasks, even in the presence of noise."
                },
                "zh": {
                    "title": "构建抗噪声的微调框架，提升模型性能",
                    "desc": "监督微调（SFT）在将大型语言模型（LLMs）适应特定领域或任务中起着重要作用。然而，实际应用中收集的数据不可避免地包含噪声，这对模型在下游任务上的表现造成了重大挑战。因此，迫切需要一种抗噪声的SFT框架，以增强模型在下游任务中的能力。为了解决这个问题，我们提出了一种稳健的SFT框架（RobustFT），该框架在下游任务数据上执行噪声检测和重新标注。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16926",
            "title": "Revisiting In-Context Learning with Long Context Language Models",
            "url": "https://huggingface.co/papers/2412.16926",
            "abstract": "In-Context Learning (ICL) is a technique by which language models make predictions based on examples provided in their input context. Previously, their context window size imposed a limit on the number of examples that can be shown, making example selection techniques crucial for identifying the maximally effective set of examples. However, the recent advent of Long Context Language Models (LCLMs) has significantly increased the number of examples that can be included in context, raising an important question of whether ICL performance in a many-shot regime is still sensitive to the method of sample selection. To answer this, we revisit these approaches in the context of LCLMs through extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we observe that sophisticated example selection techniques do not yield significant improvements over a simple random sample selection method. Instead, we find that the advent of LCLMs has fundamentally shifted the challenge of ICL from that of selecting the most effective examples to that of collecting sufficient examples to fill the context window. Specifically, in certain datasets, including all available examples does not fully utilize the context window; however, by augmenting the examples in context with a simple data augmentation approach, we substantially improve ICL performance by 5%.",
            "score": 8,
            "issue_id": 1281,
            "pub_date": "2024-12-22",
            "pub_date_card": {
                "ru": "22 декабря",
                "en": "December 22",
                "zh": "12月22日"
            },
            "hash": "764c013157322e0d",
            "authors": [
                "Jinheon Baek",
                "Sun Jae Lee",
                "Prakhar Gupta",
                "Geunseob",
                "Oh",
                "Siddharth Dalmia",
                "Prateek Kolhar"
            ],
            "affiliations": [
                "Google DeepMind",
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16926.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#long_context"
                ],
                "emoji": "📏",
                "ru": {
                    "title": "Больше примеров - лучше результат: новый взгляд на обучение в контексте для языковых моделей",
                    "desc": "Статья исследует эффективность техник выбора примеров для обучения языковых моделей с длинным контекстом (LCLM) методом обучения в контексте (ICL). Авторы провели эксперименты на 18 наборах данных и обнаружили, что сложные методы выбора примеров не дают значительных улучшений по сравнению с простым случайным выбором. Основной вызов теперь заключается не в выборе наиболее эффективных примеров, а в сборе достаточного количества примеров для заполнения контекстного окна. Простая техника аугментации данных позволила улучшить производительность ICL на 5% в некоторых наборах данных."
                },
                "en": {
                    "title": "Maximizing ICL Performance with Long Contexts: Simplicity Over Sophistication",
                    "desc": "In-Context Learning (ICL) allows language models to make predictions based on examples in their input. With the introduction of Long Context Language Models (LCLMs), the number of examples that can be included has increased, prompting a reevaluation of example selection methods. The study finds that complex selection techniques do not significantly outperform simple random sampling in many-shot scenarios. Instead, the focus shifts to ensuring enough examples fill the context window, and using data augmentation can enhance ICL performance by 5%."
                },
                "zh": {
                    "title": "长上下文模型下的示例选择新挑战",
                    "desc": "本文探讨了在长上下文语言模型（LCLMs）中，示例选择对上下文学习（ICL）性能的影响。研究发现，尽管复杂的示例选择技术并未显著提高性能，但简单的随机选择方法在许多情况下表现良好。随着LCLMs的出现，ICL的挑战已从选择最有效的示例转变为收集足够的示例以填充上下文窗口。通过简单的数据增强方法，我们在某些数据集上提高了ICL性能，提升幅度达到5%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17153",
            "title": "Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching",
            "url": "https://huggingface.co/papers/2412.17153",
            "abstract": "Autoregressive (AR) models have achieved state-of-the-art performance in text and image generation but suffer from slow generation due to the token-by-token process. We ask an ambitious question: can a pre-trained AR model be adapted to generate outputs in just one or two steps? If successful, this would significantly advance the development and deployment of AR models. We notice that existing works that try to speed up AR generation by generating multiple tokens at once fundamentally cannot capture the output distribution due to the conditional dependencies between tokens, limiting their effectiveness for few-step generation. To address this, we propose Distilled Decoding (DD), which uses flow matching to create a deterministic mapping from Gaussian distribution to the output distribution of the pre-trained AR model. We then train a network to distill this mapping, enabling few-step generation. DD doesn't need the training data of the original AR model, making it more practical.We evaluate DD on state-of-the-art image AR models and present promising results on ImageNet-256. For VAR, which requires 10-step generation, DD enables one-step generation (6.3times speed-up), with an acceptable increase in FID from 4.19 to 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an 217.8times speed-up with a comparable FID increase from 4.11 to 11.35. In both cases, baseline methods completely fail with FID>100. DD also excels on text-to-image generation, reducing the generation from 256 steps to 2 for LlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to demonstrate the possibility of one-step generation for image AR models, DD challenges the prevailing notion that AR models are inherently slow, and opens up new opportunities for efficient AR generation. The project website is at https://imagination-research.github.io/distilled-decoding.",
            "score": 3,
            "issue_id": 1283,
            "pub_date": "2024-12-22",
            "pub_date_card": {
                "ru": "22 декабря",
                "en": "December 22",
                "zh": "12月22日"
            },
            "hash": "b1968a6263a19386",
            "authors": [
                "Enshu Liu",
                "Xuefei Ning",
                "Yu Wang",
                "Zinan Lin"
            ],
            "affiliations": [
                "Department of EE, Tsinghua University",
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17153.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Революция в скорости: генерация изображений за один шаг",
                    "desc": "Статья представляет новый метод под названием Distilled Decoding (DD) для ускорения генерации авторегрессионных (AR) моделей. DD использует технику flow matching для создания детерминированного отображения от гауссовского распределения к выходному распределению предобученной AR модели. Этот подход позволяет генерировать выходные данные за один или два шага вместо пошагового процесса, значительно ускоряя работу AR моделей. Эксперименты на задачах генерации изображений показывают, что DD может достичь ускорения в 6-217 раз с приемлемым ухудшением качества по метрике FID."
                },
                "en": {
                    "title": "Revolutionizing Autoregressive Generation: Fast and Efficient with Distilled Decoding!",
                    "desc": "This paper introduces Distilled Decoding (DD), a novel approach to enhance the efficiency of autoregressive (AR) models in generating text and images. Traditional AR models generate outputs token-by-token, which can be slow, but DD aims to enable generation in just one or two steps by creating a deterministic mapping from a Gaussian distribution to the AR model's output distribution. By training a network to distill this mapping, DD allows for rapid generation without requiring the original training data, making it more practical for real-world applications. The results show significant speed-ups in generation times while maintaining acceptable quality, challenging the belief that AR models are inherently slow."
                },
                "zh": {
                    "title": "蒸馏解码：加速自回归模型生成的革命性方法",
                    "desc": "自回归（AR）模型在文本和图像生成方面表现出色，但由于逐个生成的过程，速度较慢。本文提出了一种名为蒸馏解码（DD）的方法，旨在将预训练的AR模型适应为仅需一步或两步生成输出。DD通过流匹配创建从高斯分布到AR模型输出分布的确定性映射，从而实现快速生成。实验结果表明，DD在多个图像AR模型上显著提高了生成速度，同时保持了可接受的生成质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16686",
            "title": "NILE: Internal Consistency Alignment in Large Language Models",
            "url": "https://huggingface.co/papers/2412.16686",
            "abstract": "As a crucial step to enhance LLMs alignment with human intentions, Instruction Fine-Tuning (IFT) has a high demand on dataset quality. However, existing IFT datasets often contain knowledge that is inconsistent with LLMs' internal knowledge learned from the pre-training phase, which can greatly affect the efficacy of IFT. To address this issue, we introduce NILE (iNternal consIstency aLignmEnt) framework, aimed at optimizing IFT datasets to unlock LLMs' capability further. NILE operates by eliciting target pre-trained LLM's internal knowledge corresponding to instruction data. The internal knowledge is leveraged to revise the answer in IFT datasets. Additionally, we propose a novel Internal Consistency Filtering (ICF) method to filter training samples, ensuring its high consistency with LLM's internal knowledge. Our experiments demonstrate that NILE-aligned IFT datasets sharply boost LLM performance across multiple LLM ability evaluation datasets, achieving up to 66.6% gain on Arena-Hard and 68.5% on Alpaca-Eval V2. Further analysis confirms that each component of the NILE}framework contributes to these substantial performance improvements, and provides compelling evidence that dataset consistency with pre-trained internal knowledge is pivotal for maximizing LLM potential.",
            "score": 3,
            "issue_id": 1282,
            "pub_date": "2024-12-21",
            "pub_date_card": {
                "ru": "21 декабря",
                "en": "December 21",
                "zh": "12月21日"
            },
            "hash": "0d1e640729e131e5",
            "authors": [
                "Minda Hu",
                "Qiyuan Zhang",
                "Yufei Wang",
                "Bowei He",
                "Hongru Wang",
                "Jingyan Zhou",
                "Liangyou Li",
                "Yasheng Wang",
                "Chen Ma",
                "Irwin King"
            ],
            "affiliations": [
                "City University of Hong Kong",
                "Huawei Noahs Ark Lab",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16686.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#optimization",
                    "#data",
                    "#alignment"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Согласование данных с внутренними знаниями LLM для улучшения инструктивной тонкой настройки",
                    "desc": "Статья представляет фреймворк NILE для оптимизации наборов данных для инструктивной тонкой настройки (IFT) языковых моделей. NILE использует внутренние знания предварительно обученной модели для пересмотра ответов в наборах данных IFT. Авторы также предлагают метод фильтрации для обеспечения высокой согласованности данных с внутренними знаниями модели. Эксперименты показывают значительное улучшение производительности модели на различных наборах данных для оценки способностей LLM."
                },
                "en": {
                    "title": "Aligning IFT Datasets for Enhanced LLM Performance",
                    "desc": "This paper addresses the challenge of aligning large language models (LLMs) with human intentions through Instruction Fine-Tuning (IFT). It highlights the problem of existing IFT datasets containing inconsistent knowledge that conflicts with the LLMs' pre-trained knowledge, which can hinder their performance. To solve this, the authors introduce the NILE framework, which optimizes IFT datasets by aligning them with the internal knowledge of the LLMs. The framework includes a method called Internal Consistency Filtering (ICF) to ensure high consistency, leading to significant performance improvements in LLM evaluations."
                },
                "zh": {
                    "title": "优化数据集，提升LLM性能的关键",
                    "desc": "本论文提出了一种名为NILE的框架，旨在优化指令微调（IFT）数据集，以提高大型语言模型（LLMs）与人类意图的一致性。现有的IFT数据集常常包含与LLMs预训练阶段学习的内部知识不一致的信息，这会影响IFT的效果。NILE通过提取目标预训练LLM的内部知识来修正IFT数据集中的答案，并引入了一种新的内部一致性过滤（ICF）方法，以确保训练样本与LLM的内部知识高度一致。实验结果表明，NILE对IFT数据集的优化显著提升了LLM在多个评估数据集上的表现，证明了数据集与预训练内部知识一致性的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17747",
            "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
            "url": "https://huggingface.co/papers/2412.17747",
            "abstract": "Techniques enabling large language models (LLMs) to \"think more\" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. In this work, we demonstrate that a frozen LLM can be augmented with an offline coprocessor that operates on the model's key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. We train this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. We show experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, our experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks.",
            "score": 1,
            "issue_id": 1283,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 декабря",
                "en": "December 23",
                "zh": "12月23日"
            },
            "hash": "b3ee8264ebbee41e",
            "authors": [
                "Luyang Liu",
                "Jonas Pfeiffer",
                "Jiaxing Wu",
                "Jun Xie",
                "Arthur Szlam"
            ],
            "affiliations": [
                "Google DeepMind"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17747.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#reasoning",
                    "#inference",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Офлайн-сопроцессор: Улучшение языковых моделей без изменения декодера",
                    "desc": "Статья представляет новый подход к улучшению работы больших языковых моделей (LLM) с помощью офлайн-сопроцессора. Этот сопроцессор работает с кэшем ключ-значение модели, добавляя в него латентные эмбеддинги для повышения точности декодирования. Обучение сопроцессора происходит на стандартных данных предобучения, используя функцию потерь языкового моделирования декодера. Эксперименты показывают, что данный метод снижает перплексию и улучшает производительность на задачах, требующих рассуждений, без необходимости в специфическом обучении под задачу."
                },
                "en": {
                    "title": "Enhancing LLMs with Offline Cache Augmentation for Better Reasoning",
                    "desc": "This paper presents a method to enhance large language models (LLMs) by using an offline coprocessor that improves the model's key-value (kv) cache. The coprocessor adds latent embeddings to the cache, which helps the model generate better responses by refining its reasoning process. By training the coprocessor with language modeling loss while keeping the main decoder unchanged, the system can learn to optimize its computations without increasing latency. Experimental results show that this cache augmentation leads to lower perplexity and better performance on various reasoning tasks, even without specific training for those tasks."
                },
                "zh": {
                    "title": "增强缓存，提升推理能力！",
                    "desc": "本文探讨了一种增强大型语言模型（LLM）推理能力的方法。通过引入一个离线协处理器，该协处理器在模型的键值缓存上操作，从而提高后续解码的准确性。我们的方法允许模型以端到端可微分的方式学习如何将额外的计算提炼到其缓存中。实验结果表明，增强缓存后，解码器在多个后续标记上表现出更低的困惑度，且在推理密集型任务中性能显著提升。"
                }
            }
        }
    ],
    "link_prev": "2024-12-23.html",
    "link_next": "2024-12-25.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "23.12",
        "en": "12/23",
        "zh": "12月23日"
    },
    "short_date_next": {
        "ru": "25.12",
        "en": "12/25",
        "zh": "12月25日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 3,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "自回归模型在视觉生成中表现出强大的性能，但由于其顺序逐标记预测的过程，推理速度较慢。本文提出了一种简单有效的并行自回归视觉生成方法，提高生成效率，同时保留自回归建模的优势。我们的关键洞见是并行生成依赖于视觉标记的依赖关系：弱依赖的标记可以并行生成，而强依赖的相邻标记难以一起生成，因为它们的独立采样可能导致不一致。基于此观察，我们开发了一种并行生成策略，并行生成弱依赖的远距离标记，同时保持强依赖的局部标记的顺序生成。我们的方法可以无缝集成到标准自回归模型中，无需修改架构或标记器。在ImageNet和UCF-101上的实验表明，我们的方法在图像和视频生成任务中实现了最高9.5倍的加速，同时质量损失最小。我们希望这项工作能激发未来在高效视觉生成和统一自回归建模方面的研究。项目页面：https://epiphqny.github.io/PAR-project。",
        "title": "Parallelized Autoregressive Visual Generation",
        "pinyin": "Zì huíguī móxíng zài shìjué shēngchéng zhōng biǎoxiàn chū qiángdà de xìngnéng, dàn yóuyú qí shùnxù zhú biāojì yùcè de guòchéng, tuīlǐ sùdù jiào màn. Běnwén tíchū le yīzhǒng jiǎndān yǒuxiào de bìngxíng zìhuíguī shìjué shēngchéng fāngfǎ, tígāo shēngchéng xiàolǜ, tóngshí bǎoliú zìhuíguī jiànmó de yōushì. Wǒmen de guǎnjiàn dòngjiàn shì bìngxíng shēngchéng yīlài yú shìjué biāojì de yīlài guānxì: ruò yīlài de biāojì kěyǐ bìngxíng shēngchéng, ér qiáng yīlài de xiānglín biāojì nán yǐ yīqǐ shēngchéng, yīnwèi tāmen de dúlì cǎiyǎng kěnéng dǎozhì bù yīzhì. Jīyú cǐ guānchá, wǒmen kāifā le yīzhǒng bìngxíng shēngchéng cèlüè, bìngxíng shēngchéng ruò yīlài de yuǎn jùlí biāojì, tóngshí bǎochí qiáng yīlài de júbù biāojì de shùnxù shēngchéng. Wǒmen de fāngfǎ kěyǐ wúfèng jíchéng dào biāozhǔn zìhuíguī móxíng zhōng, wúxū xiūgǎi jiàgòu huò biāojìqì. Zài ImageNet hé UCF-101 shàng de shíyàn biǎomíng, wǒmen de fāngfǎ zài túxiàng hé shìpǐn shēngchéng rènwù zhōng shíxiàn le zuìgāo 9.5 bèi de jiāsù, tóngshí zhìliàng sǔnshī zuìshǎo. Wǒmen xīwàng zhè jiàn gōngzuò néng jīfā wèilái zài gāoxiào shìjué shēngchéng hé tǒngyī zìhuíguī jiànmó fāngmiàn de yánjiū. Xiàngmù yèmiàn: https://epiphqny.github.io/PAR-project.",
        "vocab": "[{'word': '自回归', 'pinyin': 'zì huí guī', 'trans': 'autoregressive'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},\n{'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generation'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '强大', 'pinyin': 'qiáng dà', 'trans': 'powerful'},\n{'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'},\n{'word': '顺序', 'pinyin': 'shùn xù', 'trans': 'sequential'},\n{'word': '逐', 'pinyin': 'zhú', 'trans': 'gradual'},\n{'word': '标记', 'pinyin': 'biāo jì', 'trans': 'token'},\n{'word': '预测', 'pinyin': 'yù cè', 'trans': 'prediction'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'},\n{'word': '速度', 'pinyin': 'sù dù', 'trans': 'speed'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': '并行', 'pinyin': 'bìng xíng', 'trans': 'parallel'},\n{'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'},\n{'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'},\n{'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'},\n{'word': '效率', 'pinyin': 'xiào lǜ', 'trans': 'efficiency'},\n{'word': '保留', 'pinyin': 'bǎo liú', 'trans': 'retain'},\n{'word': '优势', 'pinyin': 'yōu shì', 'trans': 'advantage'},\n{'word': '关键', 'pinyin': 'guǎn jiàn', 'trans': 'key'},\n{'word': '洞见', 'pinyin': 'dòng jiàn', 'trans': 'insight'},\n{'word': '依赖', 'pinyin': 'yī lài', 'trans': 'dependency'},\n{'word': '关系', 'pinyin': 'guān xì', 'trans': 'relationship'},\n{'word': '弱', 'pinyin': 'ruò', 'trans': 'weak'},\n{'word': '强', 'pinyin': 'qiáng', 'trans': 'strong'},\n{'word': '相邻', 'pinyin': 'xiāng lín', 'trans': 'adjacent'},\n{'word': '难以', 'pinyin': 'nán yǐ', 'trans': 'difficult'},\n{'word': '独立', 'pinyin': 'dú lì', 'trans': 'independent'},\n{'word': '采样', 'pinyin': 'cǎi yàng', 'trans': 'sampling'},\n{'word': '不一致', 'pinyin': 'bù yī zhì', 'trans': 'inconsistency'},\n{'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'},\n{'word': '远距离', 'pinyin': 'yuǎn jù lí', 'trans': 'long-distance'},\n{'word': '局部', 'pinyin': 'jú bù', 'trans': 'local'},\n{'word': '无缝', 'pinyin': 'wú fèng', 'trans': 'seamless'},\n{'word': '集成', 'pinyin': 'jí chéng', 'trans': 'integrate'},\n{'word': '标准', 'pinyin': 'biāo zhǔn', 'trans': 'standard'},\n{'word': '架构', 'pinyin': 'jià gòu', 'trans': 'architecture'},\n{'word': '修改', 'pinyin': 'xiū gǎi', 'trans': 'modify'},\n{'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'},\n{'word': '图像', 'pinyin': 'tú xiàng', 'trans': 'image'},\n{'word': '视频', 'pinyin': 'shì pín', 'trans': 'video'},\n{'word': '任务', 'pinyin': 'rèn wù', 'trans': 'task'},\n{'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'},\n{'word': '加速', 'pinyin': 'jiā sù', 'trans': 'acceleration'},\n{'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'},\n{'word': '损失', 'pinyin': 'sǔn shī', 'trans': 'loss'},\n{'word': '激发', 'pinyin': 'jī fā', 'trans': 'inspire'},\n{'word': '未来', 'pinyin': 'wèi lái', 'trans': 'future'},\n{'word': '高效', 'pinyin': 'gāo xiào', 'trans': 'efficient'},\n{'word': '统一', 'pinyin': 'tǒng yī', 'trans': 'unified'},\n{'word': '项目', 'pinyin': 'xiàng mù', 'trans': 'project'},\n{'word': '页面', 'pinyin': 'yè miàn', 'trans': 'page'}]",
        "trans": "Autoregressive models have demonstrated strong performance in visual generation, but their sequential, token-by-token prediction process results in slow inference speeds. This paper proposes a simple and effective parallel autoregressive visual generation method that improves generation efficiency while retaining the advantages of autoregressive modeling. Our key insight is that parallel generation depends on the dependency relationships among visual tokens: weakly dependent tokens can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together because their independent sampling may lead to inconsistencies. Based on this observation, we developed a parallel generation strategy that generates weakly dependent distant tokens in parallel while maintaining the sequential generation of strongly dependent local tokens. Our method can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 show that our method achieves up to a 9.5x speedup in image and video generation tasks with minimal quality loss. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.",
        "update_ts": "2024-12-23 09:11"
    }
}