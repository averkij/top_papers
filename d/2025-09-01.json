{
    "date": {
        "ru": "1 сентября",
        "en": "September 1",
        "zh": "9月1日"
    },
    "time_utc": "2025-09-01 19:10",
    "weekday": 0,
    "issue_id": 5654,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.21113",
            "title": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning",
            "url": "https://huggingface.co/papers/2508.21113",
            "abstract": "R-4B, an auto-thinking multimodal large language model, uses bi-mode annealing and Bi-mode Policy Optimization to adaptively decide on problem-solving strategies, achieving state-of-the-art performance with lower computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost.",
            "score": 80,
            "issue_id": 5638,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 августа",
                "en": "August 28",
                "zh": "8月28日"
            },
            "hash": "e3b0726caba25eb1",
            "authors": [
                "Jie Jiang",
                "Qi Yang",
                "Bolin Ni",
                "Shiming Xiang",
                "Han Hu",
                "Houwen Peng"
            ],
            "affiliations": [
                "Institute of Automation, CAS",
                "Tencent Hunyuan Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21113.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#dataset",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "R-4B: Адаптивное мышление для эффективного решения задач",
                    "desc": "R-4B - это мультимодальная большая языковая модель с возможностью автоматического мышления. Она использует двухрежимный отжиг и двухрежимную оптимизацию политики для адаптивного выбора стратегии решения задач. Модель способна определять, когда нужно активировать процесс мышления в зависимости от сложности проблемы. R-4B достигает передовых результатов на 25 сложных бенчмарках при меньших вычислительных затратах по сравнению с более крупными моделями."
                },
                "en": {
                    "title": "R-4B: Smart Thinking for Efficient Problem Solving",
                    "desc": "R-4B is an innovative multimodal large language model (MLLM) that enhances problem-solving efficiency by using bi-mode annealing and Bi-mode Policy Optimization. This model intelligently decides when to engage in complex reasoning, avoiding unnecessary computations for simpler tasks. By training on a diverse dataset that includes both thinking and non-thinking examples, R-4B learns to optimize its responses based on the complexity of the problem. The results demonstrate that R-4B achieves top performance on 25 benchmarks while maintaining lower computational costs compared to larger models."
                },
                "zh": {
                    "title": "R-4B：智能思考与高效解决的结合",
                    "desc": "R-4B是一种自动思考的多模态大型语言模型，能够根据问题的复杂性自适应地决定何时进行思考。它采用双模退火和双模策略优化技术，以提高模型在解决问题时的效率和准确性。通过在多样化的数据集上进行训练，R-4B能够在简单问题上避免冗余的思考过程，从而降低计算成本。实验结果表明，R-4B在25个具有挑战性的基准测试中表现优异，超越了许多现有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21112",
            "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control",
            "url": "https://huggingface.co/papers/2508.21112",
            "abstract": "EO-Robotics, comprising EO-1 model and EO-Data1.5M dataset, advances multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models.",
            "score": 54,
            "issue_id": 5638,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 августа",
                "en": "August 28",
                "zh": "8月28日"
            },
            "hash": "5bbbfa48bbd5fb7c",
            "authors": [
                "Delin Qu",
                "Haoming Song",
                "Qizhi Chen",
                "Zhaoqing Chen",
                "Xianqiang Gao",
                "Xinyi Ye",
                "Qi Lv",
                "Modi Shi",
                "Guanghui Ren",
                "Cheng Ruan",
                "Maoqing Yao",
                "Haoran Yang",
                "Jiacheng Bao",
                "Bin Zhao",
                "Dong Wang"
            ],
            "affiliations": [
                "AgiBot",
                "Fudan University",
                "Northwestern Polytechnical University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21112.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#agents",
                    "#multimodal",
                    "#agi",
                    "#reasoning",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Революция в мультимодальном ИИ для роботов: EO-Robotics объединяет зрение, текст и действие",
                    "desc": "EO-Robotics представляет собой систему, состоящую из модели EO-1 и датасета EO-Data1.5M, которая продвигает мультимодальное воплощенное рассуждение и управление роботами через смешанное предобучение на основе зрения, текста и действий. Модель EO-1 использует унифицированную архитектуру для обработки мультимодальных входных данных и обучается на массивном высококачественном датасете EO-Data1.5M, содержащем более 1,5 миллиона образцов. Обучение модели происходит с использованием авторегрессивного декодирования и денойзинга методом сопоставления потоков. Эксперименты демонстрируют эффективность смешанного обучения на основе зрения, текста и действий для понимания открытого мира и обобщения на различные задачи манипуляции."
                },
                "en": {
                    "title": "Empowering Robots with Multimodal Reasoning and Control",
                    "desc": "EO-Robotics introduces a new model called EO-1, which enhances robot control and reasoning by integrating vision, text, and action in its training process. The EO-Data1.5M dataset, containing over 1.5 million samples, supports this model by providing diverse multimodal data for better understanding and interaction. EO-1 utilizes a unified architecture that processes various inputs simultaneously, allowing for more flexible and human-like responses in real-world scenarios. The research demonstrates that interleaved learning of vision, text, and action significantly improves the robot's ability to perform complex tasks and adapt to different environments."
                },
                "zh": {
                    "title": "提升机器人控制的多模态推理新突破",
                    "desc": "EO-Robotics是一个新模型，包含EO-1模型和EO-Data1.5M数据集，旨在通过交替的视觉-文本-动作预训练来提升多模态的具身推理和机器人控制能力。EO-1模型能够处理图像、文本、视频和动作等多种输入，展现出在多模态具身推理和机器人控制方面的优越性能。该模型的训练依赖于一个包含超过150万样本的高质量数据集，强调交替的视觉-文本-动作理解。通过大量实验，验证了交替学习在开放世界理解和泛化中的有效性，提供了构建先进具身基础模型的宝贵见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.18106",
            "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
            "url": "https://huggingface.co/papers/2508.18106",
            "abstract": "A.S.E is a benchmark for evaluating the security of code generated by large language models using real-world repositories and expert-defined rules, revealing insights into model performance and decoding strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing adoption of large language models (LLMs) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks are inadequate, as they focus on isolated code snippets, employ unstable evaluation methods that lack reproducibility, and fail to connect the quality of input context with the security of the output. To address these gaps, we introduce A.S.E (AI Code Generation Security Evaluation), a benchmark for repository-level secure code generation. A.S.E constructs tasks from real-world repositories with documented CVEs, preserving full repository context like build systems and cross-file dependencies. Its reproducible, containerized evaluation framework uses expert-defined rules to provide stable, auditable assessments of security, build quality, and generation stability. Our evaluation of leading LLMs on A.S.E reveals three key findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The security gap between proprietary and open-source models is narrow; Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise, ``fast-thinking'' decoding strategies consistently outperform complex, ``slow-thinking'' reasoning for security patching.",
            "score": 46,
            "issue_id": 5638,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "7b691aaa7b52bcfd",
            "authors": [
                "Keke Lian",
                "Bin Wang",
                "Lei Zhang",
                "Libo Chen",
                "Junjie Wang",
                "Ziming Zhao",
                "Yujiu Yang",
                "Haotong Duan",
                "Haoran Zhao",
                "Shuang Liao",
                "Mingda Guo",
                "Jiazheng Quan",
                "Yilu Zhong",
                "Chenhao He",
                "Zichuan Chen",
                "Jie Wu",
                "Haoling Li",
                "Zhaoxuan Li",
                "Jiongchi Yu",
                "Hui Li",
                "Dong Zhang"
            ],
            "affiliations": [
                "Fudan University",
                "Institute of Information Engineering, Chinese Academy of Sciences",
                "Peking University",
                "Shanghai Jiao Tong University",
                "Singapore Management University",
                "Tencent",
                "Tsinghua University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.18106.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#security",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "A.S.E: Новый стандарт оценки безопасности ИИ-генерируемого кода",
                    "desc": "A.S.E - это новый бенчмарк для оценки безопасности кода, генерируемого большими языковыми моделями (LLM). Он использует реальные репозитории и правила, определенные экспертами, что позволяет получить более точные результаты по сравнению с существующими методами. A.S.E сохраняет полный контекст репозитория, включая системы сборки и зависимости между файлами. Исследование показало, что Claude-3.7-Sonnet демонстрирует лучшую общую производительность, а разрыв в безопасности между проприетарными и открытыми моделями невелик."
                },
                "en": {
                    "title": "A.S.E: Elevating Security Evaluation for AI-Generated Code",
                    "desc": "The paper introduces A.S.E, a benchmark designed to evaluate the security of code generated by large language models (LLMs) in a more realistic context. Unlike previous benchmarks that only assess isolated code snippets, A.S.E uses real-world repositories and incorporates expert-defined rules to ensure reproducibility and stability in evaluations. It focuses on repository-level secure code generation, taking into account the entire context, including build systems and cross-file dependencies. The findings indicate that certain models excel in security performance, and simpler decoding strategies are more effective for generating secure code patches."
                },
                "zh": {
                    "title": "A.S.E：提升代码生成安全性的基准评估",
                    "desc": "A.S.E是一个用于评估大型语言模型生成代码安全性的基准，利用真实世界的代码库和专家定义的规则。现有的基准测试方法存在不足，无法有效连接输入上下文的质量与输出的安全性。A.S.E通过构建真实代码库中的任务，保留完整的上下文信息，提供可重复的安全评估。我们的评估结果显示，Claude-3.7-Sonnet在整体表现上最佳，而Qwen3-235B-A22B-Instruct在安全性评分上表现突出。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20470",
            "title": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation",
            "url": "https://huggingface.co/papers/2508.20470",
            "abstract": "Using video data to provide commonsense priors enhances 3D asset generation, enabling spatial consistency and semantic plausibility in 3D content creation.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: https://dropletx.github.io/.",
            "score": 31,
            "issue_id": 5642,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 августа",
                "en": "August 28",
                "zh": "8月28日"
            },
            "hash": "2bbd88d7f14f5a78",
            "authors": [
                "Xiaochuan Li",
                "Guoguang Du",
                "Runze Zhang",
                "Liang Jin",
                "Qi Jia",
                "Lihua Lu",
                "Zhenhua Guo",
                "Yaqian Zhao",
                "Haiyang Liu",
                "Tianqi Wang",
                "Changsheng Li",
                "Xiaoli Gong",
                "Rengang Li",
                "Baoyu Fan"
            ],
            "affiliations": [
                "IEIT System Co., Ltd.",
                "Nankai University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20470.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#3d",
                    "#multimodal",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Видео как источник здравого смысла для 3D-генерации",
                    "desc": "Эта статья исследует применение видеоданных для улучшения генерации 3D-объектов. Авторы представляют датасет Droplet3D-4M с аннотациями многоракурсных видео и модель Droplet3D, способную генерировать 3D-контент по изображениям и текстовым описаниям. Использование видео позволяет улучшить пространственную согласованность и семантическую правдоподобность создаваемых 3D-активов. Эксперименты подтверждают эффективность подхода и его потенциал для применения в генерации сцен."
                },
                "en": {
                    "title": "Enhancing 3D Asset Generation with Video Commonsense Priors",
                    "desc": "This paper discusses how using video data can improve the generation of 3D assets by providing commonsense knowledge that helps maintain spatial consistency and semantic accuracy. It highlights the challenge of limited 3D data compared to other media types, like text and images, and proposes leveraging videos as a solution. The authors introduce Droplet3D-4M, a large dataset with multi-view annotations, and a generative model called Droplet3D that can process both images and text. Their experiments show that this approach not only enhances the quality of 3D content but also allows for broader applications in scene generation."
                },
                "zh": {
                    "title": "利用视频数据提升3D生成的空间与语义一致性",
                    "desc": "本论文探讨了如何利用视频数据来增强3D资产生成，提供空间一致性和语义合理性。由于3D领域的数据稀缺，视频中的常识先验成为了一种有效的替代监督信号。视频捕捉的多视角信息为3D生成提供了空间一致性，而丰富的语义信息则使生成的内容更符合文本提示。我们介绍了Droplet3D-4M数据集和Droplet3D生成模型，实验结果表明该方法在3D内容生成中表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.13618",
            "title": "TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head\n  Synthesis",
            "url": "https://huggingface.co/papers/2508.13618",
            "abstract": "TalkVid, a large-scale, high-quality, and diverse dataset, improves audio-driven talking head synthesis by enhancing generalization across human diversity and revealing subgroup performance disparities.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-driven talking head synthesis has achieved remarkable photorealism, yet state-of-the-art (SOTA) models exhibit a critical failure: they lack generalization to the full spectrum of human diversity in ethnicity, language, and age groups. We argue that this generalization gap is a direct symptom of limitations in existing training data, which lack the necessary scale, quality, and diversity. To address this challenge, we introduce TalkVid, a new large-scale, high-quality, and diverse dataset containing 1244 hours of video from 7729 unique speakers. TalkVid is curated through a principled, multi-stage automated pipeline that rigorously filters for motion stability, aesthetic quality, and facial detail, and is validated against human judgments to ensure its reliability. Furthermore, we construct and release TalkVid-Bench, a stratified evaluation set of 500 clips meticulously balanced across key demographic and linguistic axes. Our experiments demonstrate that a model trained on TalkVid outperforms counterparts trained on previous datasets, exhibiting superior cross-dataset generalization. Crucially, our analysis on TalkVid-Bench reveals performance disparities across subgroups that are obscured by traditional aggregate metrics, underscoring its necessity for future research. Code and data can be found in https://github.com/FreedomIntelligence/TalkVid",
            "score": 14,
            "issue_id": 5639,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 августа",
                "en": "August 19",
                "zh": "8月19日"
            },
            "hash": "8baf01eb014bc50c",
            "authors": [
                "Shunian Chen",
                "Hejin Huang",
                "Yexin Liu",
                "Zihan Ye",
                "Pengcheng Chen",
                "Chenghao Zhu",
                "Michael Guan",
                "Rongsheng Wang",
                "Junying Chen",
                "Guanbin Li",
                "Ser-Nam Lim",
                "Harry Yang",
                "Benyou Wang"
            ],
            "affiliations": [
                "Sun Yat-sen University",
                "The Chinese University of Hong Kong, Shenzhen",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.13618.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#ethics",
                    "#transfer_learning",
                    "#dataset",
                    "#cv",
                    "#data"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "TalkVid: большой и разнообразный датасет для улучшения синтеза говорящих голов",
                    "desc": "Представлен новый набор данных TalkVid для улучшения синтеза говорящих голов на основе аудио. Этот датасет содержит 1244 часа видео от 7729 уникальных дикторов и отличается высоким качеством и разнообразием. Модель, обученная на TalkVid, превосходит аналоги по обобщающей способности на различных демографических группах. Также создан стратифицированный набор данных TalkVid-Bench для оценки производительности на различных подгруппах."
                },
                "en": {
                    "title": "Bridging the Diversity Gap in AI with TalkVid",
                    "desc": "The paper introduces TalkVid, a new dataset designed to improve audio-driven talking head synthesis by addressing the generalization gap in existing models. This gap arises from the lack of diversity in training data, which often fails to represent various ethnicities, languages, and age groups. TalkVid consists of 1244 hours of video from 7729 unique speakers, curated through a rigorous process to ensure high quality and diversity. The study also presents TalkVid-Bench, an evaluation set that highlights performance disparities among different demographic groups, emphasizing the importance of diverse datasets in machine learning research."
                },
                "zh": {
                    "title": "TalkVid：提升虚拟人头合成的多样性与泛化能力",
                    "desc": "TalkVid是一个大规模、高质量和多样化的数据集，旨在改善基于音频的虚拟人头合成技术。现有的模型在处理不同种族、语言和年龄群体时存在泛化能力不足的问题，这主要是由于训练数据的规模和多样性不足。为了解决这个问题，TalkVid包含了1244小时来自7729个独特说话者的视频，经过严格的多阶段自动化筛选，确保了数据的稳定性和美学质量。我们的实验表明，基于TalkVid训练的模型在跨数据集泛化能力上优于以往的数据集，同时也揭示了不同子群体之间的性能差异。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21148",
            "title": "A Survey of Scientific Large Language Models: From Data Foundations to\n  Agent Frontiers",
            "url": "https://huggingface.co/papers/2508.21148",
            "abstract": "Sci-LLMs are evolving through a co-development with scientific data, addressing unique challenges like multimodal and domain-specific information, and are moving towards autonomous, closed-loop systems in scientific research.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.",
            "score": 12,
            "issue_id": 5645,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 августа",
                "en": "August 28",
                "zh": "8月28日"
            },
            "hash": "3097c905f2f36541",
            "authors": [
                "Ming Hu",
                "Chenglong Ma",
                "Wei Li",
                "Wanghan Xu",
                "Jiamin Wu",
                "Jucheng Hu",
                "Tianbin Li",
                "Guohang Zhuang",
                "Jiaqi Liu",
                "Yingzhou Lu",
                "Ying Chen",
                "Chaoyang Zhang",
                "Cheng Tan",
                "Jie Ying",
                "Guocheng Wu",
                "Shujian Gao",
                "Pengcheng Chen",
                "Jiashi Lin",
                "Haitao Wu",
                "Lulu Chen",
                "Fengxiang Wang",
                "Yuanyuan Zhang",
                "Xiangyu Zhao",
                "Feilong Tang",
                "Encheng Su",
                "Junzhi Ning",
                "Xinyao Liu",
                "Ye Du",
                "Changkai Ji",
                "Cheng Tang",
                "Huihui Xu",
                "Ziyang Chen",
                "Ziyan Huang",
                "Jiyao Liu",
                "Pengfei Jiang",
                "Yizhou Wang",
                "Chen Tang",
                "Jianyu Wu",
                "Yuchen Ren",
                "Siyuan Yan",
                "Zhonghua Wang",
                "Zhongxing Xu",
                "Shiyan Su",
                "Shangquan Sun",
                "Runkai Zhao",
                "Zhisheng Zhang",
                "Yu Liu",
                "Fudi Wang",
                "Yuanfeng Ji",
                "Yanzhou Su",
                "Hongming Shan",
                "Chunmei Feng",
                "Jiahao Xu",
                "Jiangtao Yan",
                "Wenhao Tang",
                "Diping Song",
                "Lihao Liu",
                "Yanyan Huang",
                "Lequan Yu",
                "Bin Fu",
                "Shujun Wang",
                "Xiaomeng Li",
                "Xiaowei Hu",
                "Yun Gu",
                "Ben Fei",
                "Zhongying Deng",
                "Benyou Wang",
                "Yuewen Cao",
                "Minjie Shen",
                "Haodong Duan",
                "Jie Xu",
                "Yirong Chen",
                "Fang Yan",
                "Hongxia Hao",
                "Jielan Li",
                "Jiajun Du",
                "Yanbo Wang",
                "Imran Razzak",
                "Chi Zhang",
                "Lijun Wu",
                "Conghui He",
                "Zhaohui Lu",
                "Jinhai Huang",
                "Yihao Liu",
                "Fenghua Ling",
                "Yuqiang Li",
                "Aoran Wang",
                "Qihao Zheng",
                "Nanqing Dong",
                "Tianfan Fu",
                "Dongzhan Zhou",
                "Yan Lu",
                "Wenlong Zhang",
                "Jin Ye",
                "Jianfei Cai",
                "Wanli Ouyang",
                "Yu Qiao",
                "Zongyuan Ge",
                "Shixiang Tang",
                "Junjun He",
                "Chunfeng Song",
                "Lei Bai",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Beijing Institute of Heart, Lung and Blood Vessel Diseases",
                "China Pharmaceutical University",
                "Chinese Academy of Sciences",
                "Fudan University",
                "Fuzhou University",
                "Monash University",
                "Purdue University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "South China University",
                "Stanford University",
                "The Chinese University of Hong Kong",
                "The Hong Kong Polytechnic University",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong",
                "UNC-Chapel Hill",
                "University College Dublin",
                "University College London",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21148.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#survey",
                    "#multimodal",
                    "#data",
                    "#dataset",
                    "#science"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Sci-LLMs: эволюция искусственного интеллекта в научном познании",
                    "desc": "Научные большие языковые модели (Sci-LLMs) развиваются в тесной связи с научными данными, решая уникальные задачи обработки мультимодальной и специализированной информации. В статье представлен комплексный обзор развития Sci-LLMs, включая анализ более 270 наборов данных для предобучения и дообучения моделей. Авторы рассматривают переход от статических тестов к оценке, ориентированной на процесс и открытия, с использованием продвинутых протоколов. Обсуждается парадигма автономных систем на основе Sci-LLMs, способных экспериментировать и вносить вклад в развивающуюся базу знаний."
                },
                "en": {
                    "title": "Transforming Science with Autonomous Language Models",
                    "desc": "This paper discusses the evolution of Scientific Large Language Models (Sci-LLMs) and their interaction with scientific data. It highlights the unique challenges posed by scientific datasets, which are often multimodal and domain-specific, requiring specialized approaches compared to general natural language processing. The authors propose a taxonomy of scientific data and review various Sci-LLMs, emphasizing the need for models that can handle heterogeneous and uncertain information. Ultimately, the paper advocates for the development of autonomous systems that can actively engage in scientific research, contributing to a dynamic knowledge base."
                },
                "zh": {
                    "title": "科学研究中的智能合作伙伴",
                    "desc": "科学大型语言模型（Sci-LLMs）正在通过与科学数据的共同发展而不断演变，解决多模态和特定领域信息等独特挑战。这项研究提出了一种以数据为中心的综合框架，将Sci-LLMs的发展视为模型与其基础数据之间的共同进化。我们建立了科学数据的统一分类法和科学知识的层次模型，强调了科学语料库与一般自然语言处理数据集之间的区别。最后，我们展望了向闭环系统的转变，强调基于Sci-LLMs的自主代理如何积极实验、验证并为不断发展的知识库做出贡献。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21365",
            "title": "Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models",
            "url": "https://huggingface.co/papers/2508.21365",
            "abstract": "Think in Games (TiG) framework enables large language models to develop procedural knowledge through interactive game environments, achieving competitive performance with reduced data and computational demands while providing transparent explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at complex reasoning tasks such as mathematics and coding, yet they frequently struggle with simple interactive tasks that young children perform effortlessly. This discrepancy highlights a critical gap between declarative knowledge (knowing about something) and procedural knowledge (knowing how to do something). Although traditional reinforcement learning (RL) agents can acquire procedural knowledge through environmental interaction, they often operate as black boxes and require substantial training data. In contrast, LLMs possess extensive world knowledge and reasoning capabilities, but are unable to effectively convert this static knowledge into dynamic decision-making in interactive settings. To address this challenge, we propose Think in Games (TiG), a novel framework that empowers LLMs to develop procedural understanding through direct interaction with game environments, while retaining their inherent reasoning and explanatory abilities. Specifically, TiG reformulates RL-based decision-making as a language modeling task: LLMs generate language-guided policies, which are refined iteratively through online reinforcement learning based on environmental feedback. Our experimental results show that TiG successfully bridges the gap between declarative and procedural knowledge, achieving competitive performance with dramatically lower data and computational demands compared to conventional RL methods. Moreover, TiG provides step-by-step natural language explanations for its decisions, greatly improving transparency and interpretability in complex interactive tasks.",
            "score": 9,
            "issue_id": 5639,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 августа",
                "en": "August 29",
                "zh": "8月29日"
            },
            "hash": "cbcbd196468063e9",
            "authors": [
                "Yi Liao",
                "Yu Gu",
                "Yuan Sui",
                "Zining Zhu",
                "Yifan Lu",
                "Guohua Tang",
                "Zhongqian Sun",
                "Wei Yang"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21365.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#games",
                    "#optimization",
                    "#reasoning",
                    "#interpretability",
                    "#multimodal",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Обучение ИИ через игры: от знаний к умениям",
                    "desc": "Предложена новая система Think in Games (TiG), позволяющая большим языковым моделям развивать процедурные знания через взаимодействие с игровыми средами. TiG преобразует задачу принятия решений на основе обучения с подкреплением в задачу языкового моделирования. Система достигает конкурентоспособных результатов при значительно меньших требованиях к данным и вычислительным ресурсам по сравнению с традиционными методами обучения с подкреплением. Кроме того, TiG обеспечивает прозрачность, предоставляя пошаговые объяснения своих решений на естественном языке."
                },
                "en": {
                    "title": "Empowering Language Models to Learn by Playing",
                    "desc": "The Think in Games (TiG) framework allows large language models (LLMs) to learn how to perform tasks through interaction with game environments, enhancing their procedural knowledge. This approach addresses the gap between knowing facts (declarative knowledge) and knowing how to apply them (procedural knowledge), which LLMs often struggle with in interactive scenarios. By treating decision-making in reinforcement learning as a language modeling task, TiG enables LLMs to create and refine policies based on feedback from their environment. The results show that TiG not only requires less data and computation than traditional methods but also offers clear explanations for its actions, making it more transparent and interpretable."
                },
                "zh": {
                    "title": "通过游戏思维提升AI的学习能力",
                    "desc": "Think in Games (TiG) 框架使大型语言模型能够通过互动游戏环境发展程序性知识。与传统的强化学习方法相比，TiG 在数据和计算需求上显著降低，同时保持了模型的推理和解释能力。该框架将基于强化学习的决策过程重新定义为语言建模任务，使得模型能够生成语言指导的策略，并通过环境反馈进行迭代优化。实验结果表明，TiG 成功弥补了声明性知识与程序性知识之间的差距，提升了复杂互动任务的透明度和可解释性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17677",
            "title": "TiKMiX: Take Data Influence into Dynamic Mixture for Language Model\n  Pre-training",
            "url": "https://huggingface.co/papers/2508.17677",
            "abstract": "Dynamic adjustment of data mixture based on Group Influence metric improves language model performance by adapting to evolving learning preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t The data mixture used in the pre-training of a language model is a cornerstone of its final performance. However, a static mixing strategy is suboptimal, as the model's learning preferences for various data domains shift dynamically throughout training. Crucially, observing these evolving preferences in a computationally efficient manner remains a significant challenge. To address this, we propose TiKMiX, a method that dynamically adjusts the data mixture according to the model's evolving preferences. TiKMiX introduces Group Influence, an efficient metric for evaluating the impact of data domains on the model. This metric enables the formulation of the data mixing problem as a search for an optimal, influence-maximizing distribution. We solve this via two approaches: TiKMiX-D for direct optimization, and TiKMiX-M, which uses a regression model to predict a superior mixture. We trained models with different numbers of parameters, on up to 1 trillion tokens. TiKMiX-D exceeds the performance of state-of-the-art methods like REGMIX while using just 20% of the computational resources. TiKMiX-M leads to an average performance gain of 2% across 9 downstream benchmarks. Our experiments reveal that a model's data preferences evolve with training progress and scale, and we demonstrate that dynamically adjusting the data mixture based on Group Influence, a direct measure of these preferences, significantly improves performance by mitigating the underdigestion of data seen with static ratios.",
            "score": 7,
            "issue_id": 5639,
            "pub_date": "2025-08-25",
            "pub_date_card": {
                "ru": "25 августа",
                "en": "August 25",
                "zh": "8月25日"
            },
            "hash": "8c118ab21cea1eb2",
            "authors": [
                "Yifan Wang",
                "Binbin Liu",
                "Fengze Liu",
                "Yuanfan Guo",
                "Jiyao Deng",
                "Xuecheng Wu",
                "Weidong Zhou",
                "Xiaohuan Zhou",
                "Taifeng Wang"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17677.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Динамическая оптимизация данных для эффективного обучения языковых моделей",
                    "desc": "Статья представляет TiKMiX - метод динамической корректировки смеси данных для предобучения языковых моделей. Авторы вводят метрику Group Influence для эффективной оценки влияния доменов данных на модель. TiKMiX оптимизирует распределение данных, максимизируя эту метрику, что позволяет адаптироваться к меняющимся предпочтениям модели в процессе обучения. Эксперименты показывают, что TiKMiX превосходит современные методы, используя меньше вычислительных ресурсов и улучшая производительность на нисходящих задачах."
                },
                "en": {
                    "title": "Dynamic Data Mixing for Enhanced Language Model Performance",
                    "desc": "This paper introduces TiKMiX, a novel method for dynamically adjusting the data mixture used in training language models. It leverages a metric called Group Influence to assess how different data domains impact the model's learning preferences, which change over time. By optimizing the data mixture based on these evolving preferences, TiKMiX significantly enhances model performance while being computationally efficient. The results show that TiKMiX outperforms existing methods and achieves better results with fewer resources, demonstrating the importance of adaptive data strategies in machine learning."
                },
                "zh": {
                    "title": "动态调整数据混合，提升语言模型性能",
                    "desc": "本文提出了一种名为TiKMiX的方法，用于根据模型的学习偏好动态调整数据混合，以提高语言模型的性能。传统的静态数据混合策略无法适应模型在训练过程中不断变化的偏好。TiKMiX引入了Group Influence这一高效指标，用于评估不同数据领域对模型的影响，从而优化数据混合的分布。通过TiKMiX-D和TiKMiX-M两种方法，我们实现了在计算资源使用上更高效的模型训练，同时在多个基准测试中取得了显著的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21767",
            "title": "UItron: Foundational GUI Agent with Advanced Perception and Planning",
            "url": "https://huggingface.co/papers/2508.21767",
            "abstract": "UItron, an open-source foundational model for GUI agents, enhances visual understanding and task planning through advanced perception, grounding, and planning capabilities, achieving superior performance in Chinese app scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI agent aims to enable automated operations on Mobile/PC devices, which is an important task toward achieving artificial general intelligence. The rapid advancement of VLMs accelerates the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the availability of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develop a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. To this end, we manually collect over one million steps of operation trajectories across the top 100 most popular apps, and build the offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese app scenarios, propelling GUI agents one step closer to real-world application.",
            "score": 6,
            "issue_id": 5639,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 августа",
                "en": "August 29",
                "zh": "8月29日"
            },
            "hash": "4e413654a21d562e",
            "authors": [
                "Zhixiong Zeng",
                "Jing Huang",
                "Liming Zheng",
                "Wenkang Han",
                "Yufeng Zhong",
                "Lei Chen",
                "Longrong Yang",
                "Yingjie Chu",
                "Yuzhi He",
                "Lin Ma"
            ],
            "affiliations": [
                "Meituan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21767.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#open_source",
                    "#training",
                    "#optimization",
                    "#reasoning",
                    "#agi",
                    "#dataset",
                    "#data",
                    "#agents"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "UItron: ИИ-агент для автоматизации графических интерфейсов",
                    "desc": "UItron - это модель машинного обучения с открытым исходным кодом для автоматизации работы с графическим интерфейсом. Она улучшает визуальное понимание и планирование задач с помощью продвинутых возможностей восприятия, привязки к контексту и планирования. UItron демонстрирует превосходную производительность в сценариях работы с китайскими приложениями. Модель использует стратегии инженерии данных и интерактивную инфраструктуру для повышения эффективности обучения."
                },
                "en": {
                    "title": "UItron: Advancing GUI Agents for Real-World Applications",
                    "desc": "UItron is an open-source foundational model designed to enhance the capabilities of GUI agents, focusing on visual understanding and task planning. It addresses challenges in developing GUI agents, such as limited operation trajectories and the need for interactive infrastructure. By employing advanced perception, grounding, and planning techniques, UItron systematically improves training through data engineering and a curriculum reinforcement learning framework. The model demonstrates exceptional performance in Chinese app scenarios, filling a gap in existing solutions and moving closer to practical applications of GUI agents."
                },
                "zh": {
                    "title": "UItron：推动图形用户界面代理的未来",
                    "desc": "UItron是一个开源的基础模型，专为图形用户界面（GUI）代理设计，提升了视觉理解和任务规划能力。该模型通过先进的感知、定位和规划功能，在中文应用场景中表现出色。UItron强调了系统数据工程和交互基础设施在GUI代理开发中的重要性，并通过监督微调和强化学习框架来增强训练效果。实验结果表明，UItron在中文应用场景中取得了显著进展，使GUI代理更接近实际应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21290",
            "title": "Efficient Code Embeddings from Code Generation Models",
            "url": "https://huggingface.co/papers/2508.21290",
            "abstract": "Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.  \t\t\t\t\tAI-generated summary \t\t\t\t jina-code-embeddings is a novel code embedding model suite designed to retrieve code from natural language queries, perform technical question-answering, and identify semantically similar code snippets across programming languages. It makes innovative use of an autoregressive backbone pre-trained on both text and code, generating embeddings via last-token pooling. We outline the training recipe and demonstrate state-of-the-art performance despite the relatively small size of the models, validating this approach to code embedding model construction.",
            "score": 5,
            "issue_id": 5640,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 августа",
                "en": "August 29",
                "zh": "8月29日"
            },
            "hash": "484a770fa8f460fc",
            "authors": [
                "Daria Kryvosheieva",
                "Saba Sturua",
                "Michael Günther",
                "Scott Martens",
                "Han Xiao"
            ],
            "affiliations": [
                "Jina AI GmbH",
                "Massachusetts Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21290.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#transfer_learning",
                    "#data",
                    "#dataset",
                    "#games",
                    "#plp",
                    "#small_models",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Умные эмбеддинги для эффективной работы с кодом",
                    "desc": "Jina-code-embeddings - это новая модель встраивания кода, основанная на авторегрессионной архитектуре, предобученной на тексте и коде. Модель генерирует эмбеддинги для поиска кода, ответов на вопросы и определения семантически похожих фрагментов кода. Несмотря на относительно небольшой размер, модель демонстрирует передовые результаты в различных задачах, связанных с кодом. Авторы описывают методику обучения и валидируют эффективность данного подхода к созданию моделей встраивания кода."
                },
                "en": {
                    "title": "Revolutionizing Code Retrieval with Smart Embeddings",
                    "desc": "Jina-code-embeddings is a new model that creates embeddings for code, which helps in finding code based on natural language questions and identifying similar code snippets. It uses an autoregressive backbone that has been trained on both text and code, allowing it to understand and generate relevant embeddings effectively. The model employs a technique called last-token pooling to produce these embeddings, which enhances its performance. Despite being smaller in size compared to other models, it achieves state-of-the-art results in code retrieval and question-answering tasks."
                },
                "zh": {
                    "title": "创新代码嵌入模型，提升代码检索与问答能力",
                    "desc": "Jina-code-embeddings 是一种新型的代码嵌入模型，旨在通过自然语言查询检索代码、进行技术问答以及识别不同编程语言中语义相似的代码片段。该模型创新性地使用了一个在文本和代码上预训练的自回归骨干网络，通过最后一个标记的池化生成嵌入。我们详细介绍了训练方法，并展示了尽管模型相对较小，但仍能实现最先进的性能。这验证了这种代码嵌入模型构建方法的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21456",
            "title": "Morae: Proactively Pausing UI Agents for User Choices",
            "url": "https://huggingface.co/papers/2508.21456",
            "abstract": "Morae, a UI agent, enhances accessibility for BLV users by involving them in decision-making processes during task execution, using large multimodal models to interpret user queries and UI elements.  \t\t\t\t\tAI-generated summary \t\t\t\t User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences.",
            "score": 3,
            "issue_id": 5639,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 августа",
                "en": "August 29",
                "zh": "8月29日"
            },
            "hash": "3d7bd580c525eaa6",
            "authors": [
                "Yi-Hao Peng",
                "Dingzeyu Li",
                "Jeffrey P. Bigham",
                "Amy Pavel"
            ],
            "affiliations": [
                "Adobe Research",
                "Carnegie Mellon University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21456.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#agi",
                    "#multimodal",
                    "#healthcare",
                    "#agents"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Morae: ИИ-ассистент, расширяющий возможности пользователей с нарушениями зрения",
                    "desc": "Статья представляет Morae - агента пользовательского интерфейса, который улучшает доступность для пользователей с нарушениями зрения. Morae использует большие мультимодальные модели для интерпретации запросов пользователей и элементов интерфейса. В отличие от существующих агентов, Morae вовлекает пользователей в процесс принятия решений во время выполнения задач. Исследование показало, что Morae помогает пользователям выполнять больше задач и выбирать варианты, которые лучше соответствуют их предпочтениям."
                },
                "en": {
                    "title": "Empowering BLV Users with Interactive Decision-Making",
                    "desc": "Morae is a user interface (UI) agent designed to improve accessibility for blind and low-vision (BLV) users by actively involving them in decision-making during task execution. Unlike traditional UI agents that operate autonomously, Morae identifies key decision points and pauses to allow users to make informed choices, enhancing their agency. It leverages large multimodal models to interpret user queries and UI elements, ensuring that users are aware of their options. In studies, Morae demonstrated improved task completion and user satisfaction compared to standard agents, showcasing a mixed-initiative approach that balances automation with user preference expression."
                },
                "zh": {
                    "title": "Morae：让盲人和低视力用户参与决策的智能代理",
                    "desc": "Morae是一种用户界面代理，旨在通过让盲人和低视力用户参与决策过程来提高可访问性。它利用大型多模态模型来理解用户查询和用户界面元素，并在任务执行中自动识别决策点，以便用户可以做出选择。与传统的全自动代理不同，Morae在关键时刻暂停，提示用户进行澄清，从而增强用户的自主性。研究表明，Morae帮助用户完成更多任务，并选择更符合他们偏好的选项。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21376",
            "title": "AHELM: A Holistic Evaluation of Audio-Language Models",
            "url": "https://huggingface.co/papers/2508.21376",
            "abstract": "AHELM is a comprehensive benchmark for audio-language models that evaluates multiple aspects including fairness, safety, and reasoning across various datasets and models.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluations of audio-language models (ALMs) -- multimodal models that take interleaved audio and text as input and output text -- are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets -- including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering -- to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness (p=0.01) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with one ranking 5th overall despite having only speech-to-text capabilities. For transparency, all raw prompts, model generations, and outputs are available on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is intended to be a living benchmark and new datasets and models will be added over time.",
            "score": 1,
            "issue_id": 5639,
            "pub_date": "2025-08-29",
            "pub_date_card": {
                "ru": "29 августа",
                "en": "August 29",
                "zh": "8月29日"
            },
            "hash": "fccdd3f91aa4aebd",
            "authors": [
                "Tony Lee",
                "Haoqin Tu",
                "Chi Heem Wong",
                "Zijun Wang",
                "Siwei Yang",
                "Yifan Mai",
                "Yuyin Zhou",
                "Cihang Xie",
                "Percy Liang"
            ],
            "affiliations": [
                "Hitachi America, Ltd.",
                "Stanford University",
                "University of California, Santa Cruz"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21376.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#ethics",
                    "#reasoning",
                    "#multimodal",
                    "#audio",
                    "#dataset"
                ],
                "emoji": "🎧",
                "ru": {
                    "title": "AHELM: Всесторонняя оценка аудио-языковых моделей",
                    "desc": "AHELM - это комплексный бенчмарк для оценки аудио-языковых моделей (ALM). Он измеряет 10 аспектов, включая восприятие аудио, рассуждение, обнаружение эмоций и безопасность, используя различные наборы данных. Бенчмарк стандартизирует промпты, параметры вывода и метрики оценки для справедливого сравнения моделей. Результаты показывают, что Gemini 2.5 Pro лидирует в 5 из 10 аспектов, но проявляет групповую несправедливость в задачах ASR."
                },
                "en": {
                    "title": "AHELM: A Holistic Benchmark for Evaluating Audio-Language Models",
                    "desc": "AHELM is a new benchmark designed to evaluate audio-language models (ALMs) on multiple important aspects such as fairness, safety, and reasoning. It combines various datasets, including two new ones, PARADE and CoRe-Bench, to provide a comprehensive assessment of ALMs across ten critical dimensions. By standardizing evaluation methods and metrics, AHELM allows for fair comparisons between different models, addressing the limitations of previous benchmarks. The initial testing of 14 ALMs reveals insights into their performance, highlighting both strengths and weaknesses in areas like bias and group fairness."
                },
                "zh": {
                    "title": "AHELM：音频语言模型的全面评估基准",
                    "desc": "AHELM是一个全面的基准测试，用于评估音频语言模型（ALMs），涵盖公平性、安全性和推理等多个方面。该基准整合了多种数据集，包括两个新的合成音频-文本数据集PARADE和CoRe-Bench，以全面测量ALMs在音频感知、知识、推理等10个重要方面的表现。通过标准化提示、推理参数和评估指标，AHELM确保了模型之间的公平比较。我们的测试结果显示，尽管Gemini 2.5 Pro在10个方面中有5个排名第一，但在ASR任务上表现出群体不公平性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21188",
            "title": "Model-Task Alignment Drives Distinct RL Outcomes",
            "url": "https://huggingface.co/papers/2508.21188",
            "abstract": "Reinforcement learning applied to large language models shows counterintuitive results that depend on pre-existing model-task alignment, with standard RL methods remaining robust in challenging scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in applying reinforcement learning (RL) to large language models (LLMs) have led to substantial progress. In particular, a series of remarkable yet often counterintuitive phenomena have been reported in LLMs, exhibiting patterns not typically observed in traditional RL settings. For example, notable claims include that a single training example can match the performance achieved with an entire dataset, that the reward signal does not need to be very accurate, and that training solely with negative samples can match or even surpass sophisticated reward-based methods. However, the precise conditions under which these observations hold - and, critically, when they fail - remain unclear. In this work, we identify a key factor that differentiates RL observations: whether the pretrained model already exhibits strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated task. Through a systematic and comprehensive examination of a series of counterintuitive claims, supported by rigorous experimental validation across different model architectures and task domains, our findings show that while standard RL training remains consistently robust across settings, many of these counterintuitive results arise only when the model and task already exhibit strong model-task alignment. In contrast, these techniques fail to drive substantial learning in more challenging regimes, where standard RL methods remain effective.",
            "score": 1,
            "issue_id": 5648,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 августа",
                "en": "August 28",
                "zh": "8月28日"
            },
            "hash": "f7ee68376b3660e0",
            "authors": [
                "Haoze Wu",
                "Cheng Wang",
                "Wenshuo Zhao",
                "Junxian He"
            ],
            "affiliations": [
                "HKUST",
                "National University of Singapore",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21188.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rlhf",
                    "#reasoning",
                    "#alignment",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Неожиданные эффекты обучения с подкреплением в ИИ зависят от начальной подготовки модели",
                    "desc": "Исследование применения обучения с подкреплением к большим языковым моделям выявило неожиданные результаты, зависящие от предварительного соответствия модели задаче. Авторы обнаружили, что многие контринтуитивные эффекты проявляются только при сильном начальном соответствии модели и задачи. В более сложных сценариях эти техники оказываются неэффективными, в то время как стандартные методы обучения с подкреплением остаются надежными. Результаты были подтверждены экспериментально на различных архитектурах моделей и типах задач."
                },
                "en": {
                    "title": "Understanding RL Success in Language Models: The Role of Model-Task Alignment",
                    "desc": "This paper explores the application of reinforcement learning (RL) to large language models (LLMs) and reveals unexpected results that depend on the alignment between the model and the task. It highlights that certain counterintuitive phenomena, such as achieving high performance with minimal training data or inaccurate reward signals, occur primarily when there is strong model-task alignment. The authors conduct rigorous experiments to validate these findings across various model architectures and tasks, demonstrating that standard RL methods are robust in challenging scenarios. However, they also note that the effectiveness of these counterintuitive results diminishes when the model-task alignment is weak."
                },
                "zh": {
                    "title": "强化学习与模型任务对齐的奥秘",
                    "desc": "本研究探讨了强化学习（RL）在大型语言模型（LLM）中的应用，发现了一些反直觉的现象。这些现象的出现与预训练模型与任务之间的对齐程度密切相关。研究表明，当模型与任务具有强对齐时，某些训练方法可以取得意想不到的效果，但在更具挑战性的环境中，传统的RL方法仍然有效。通过系统的实验验证，我们揭示了这些反直觉结果的条件和局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.20085",
            "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data\n  for Mobile Dexterous Manipulation",
            "url": "https://huggingface.co/papers/2508.20085",
            "abstract": "HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.",
            "score": 1,
            "issue_id": 5640,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "ad1271c7a1097c84",
            "authors": [
                "Zhecheng Yuan",
                "Tianming Wei",
                "Langzhe Gu",
                "Pu Hua",
                "Tianhai Liang",
                "Yuanpei Chen",
                "Huazhe Xu"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai Qi Zhi Institute",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.20085.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agents",
                    "#optimization",
                    "#transfer_learning",
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "От движений человека к умелым рукам робота",
                    "desc": "HERMES - это система обучения роботов на основе данных о движениях человеческих рук. Она использует обучение с подкреплением и перенос из симуляции в реальность для создания универсальных манипуляционных навыков. HERMES способна адаптировать движения к различным условиям окружающей среды. Система включает в себя навигационную модель с механизмом локализации для автономной работы в неструктурированных средах."
                },
                "en": {
                    "title": "Bridging Human Motion and Robotic Dexterity with HERMES",
                    "desc": "HERMES is a framework that helps robots learn to mimic human hand movements for better manipulation tasks. It uses reinforcement learning to convert various human motions into actions that robots can perform, even with complex hand structures. The framework also addresses the challenge of transferring learned behaviors from simulated environments to real-world applications. By incorporating advanced localization techniques, HERMES enables robots to operate effectively in diverse and unpredictable settings."
                },
                "zh": {
                    "title": "HERMES：人机协作的灵巧操作新框架",
                    "desc": "HERMES是一个人机学习框架，旨在将人类手部动作转化为机器人行为。该框架利用强化学习和仿真到现实的转移技术，帮助机器人在多样化环境中进行灵活的操作。HERMES能够将来自多个来源的人类手部动作统一转化为可行的机器人行为，并通过深度图像实现更好的现实适应性。实验结果表明，HERMES在各种复杂的移动双手灵巧操作任务中表现出色，具有良好的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17380",
            "title": "Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula\n  Discovery",
            "url": "https://huggingface.co/papers/2508.17380",
            "abstract": "VIPER-R1, a multimodal model combining visual perception, trajectory data, and symbolic reasoning, discovers physical laws with higher accuracy and interpretability than existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated discovery of physical laws from observational data in the real world is a grand challenge in AI. Current methods, relying on symbolic regression or LLMs, are limited to uni-modal data and overlook the rich, visual phenomenological representations of motion that are indispensable to physicists. This \"sensory deprivation\" severely weakens their ability to interpret the inherent spatio-temporal patterns within dynamic phenomena. To address this gap, we propose VIPER-R1, a multimodal model that performs Visual Induction for Physics-based Equation Reasoning to discover fundamental symbolic formulas. It integrates visual perception, trajectory data, and symbolic reasoning to emulate the scientific discovery process. The model is trained via a curriculum of Motion Structure Induction (MSI), using supervised fine-tuning to interpret kinematic phase portraits and to construct hypotheses guided by a Causal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration (RGSC) to refine the formula structure with reinforcement learning. During inference, the trained VIPER-R1 acts as an agent: it first posits a high-confidence symbolic ansatz, then proactively invokes an external symbolic regression tool to perform Symbolic Residual Realignment (SR^2). This final step, analogous to a physicist's perturbation analysis, reconciles the theoretical model with empirical data. To support this research, we introduce PhysSymbol, a new 5,000-instance multimodal corpus. Experiments show that VIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy and interpretability, enabling more precise discovery of physical laws. Project page: https://jiaaqiliu.github.io/VIPER-R1/",
            "score": 1,
            "issue_id": 5648,
            "pub_date": "2025-08-24",
            "pub_date_card": {
                "ru": "24 августа",
                "en": "August 24",
                "zh": "8月24日"
            },
            "hash": "87afb7a989f84636",
            "authors": [
                "Jiaqi Liu",
                "Songning Lai",
                "Pengze Li",
                "Di Yu",
                "Wenjie Zhou",
                "Yiyang Zhou",
                "Peng Xia",
                "Zijun Wang",
                "Xi Chen",
                "Shixiang Tang",
                "Lei Bai",
                "Wanli Ouyang",
                "Mingyu Ding",
                "Huaxiu Yao",
                "Aoran Wang"
            ],
            "affiliations": [
                "Fudan University",
                "HKUST (Guangzhou)",
                "Nankai University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Innovation Institute",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "UC Santa Cruz",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17380.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#reasoning",
                    "#rl",
                    "#multimodal",
                    "#interpretability",
                    "#dataset",
                    "#science"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "VIPER-R1: Мультимодальный ИИ для открытия законов физики",
                    "desc": "VIPER-R1 - это мультимодальная модель для автоматического открытия физических законов, сочетающая визуальное восприятие, данные о траекториях и символические рассуждения. Модель обучается с помощью курса индукции структуры движения и символической калибровки с подкреплением. VIPER-R1 превосходит существующие методы по точности и интерпретируемости при обнаружении физических законов. Для поддержки исследования был создан новый мультимодальный корпус PhysSymbol с 5000 примеров."
                },
                "en": {
                    "title": "Unlocking Physical Laws with Multimodal Insights",
                    "desc": "VIPER-R1 is a multimodal model designed to discover physical laws by integrating visual perception, trajectory data, and symbolic reasoning. Unlike traditional methods that focus on single data types, VIPER-R1 utilizes a combination of visual and motion data to better understand complex physical phenomena. The model employs a structured training approach, including Motion Structure Induction and reinforcement learning, to refine its symbolic formulas. Experimental results demonstrate that VIPER-R1 achieves higher accuracy and interpretability compared to existing models, making it a significant advancement in automated scientific discovery."
                },
                "zh": {
                    "title": "VIPER-R1：多模态模型助力物理定律发现",
                    "desc": "VIPER-R1是一种多模态模型，结合了视觉感知、轨迹数据和符号推理，能够以更高的准确性和可解释性发现物理定律。现有方法主要依赖于符号回归或大型语言模型，通常只处理单一模态数据，忽视了运动的丰富视觉表征。VIPER-R1通过运动结构归纳（MSI）和奖励引导的符号校准（RGSC）来训练，模拟科学发现过程。实验表明，VIPER-R1在准确性和可解释性上均优于现有的最先进模型，能够更精确地发现物理定律。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.14197",
            "title": "CLIPSym: Delving into Symmetry Detection with CLIP",
            "url": "https://huggingface.co/papers/2508.14197",
            "abstract": "CLIPSym, a vision-language model using CLIP, enhances symmetry detection through a rotation-equivariant decoder and semantic-aware prompting, outperforming existing methods on standard datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Symmetry is one of the most fundamental geometric cues in computer vision, and detecting it has been an ongoing challenge. With the recent advances in vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP model can aid symmetry detection by leveraging the additional symmetry cues found in the natural image descriptions. We propose CLIPSym, which leverages CLIP's image and language encoders and a rotation-equivariant decoder based on a hybrid of Transformer and G-Convolution to detect rotation and reflection symmetries. To fully utilize CLIP's language encoder, we have developed a novel prompting technique called Semantic-Aware Prompt Grouping (SAPG), which aggregates a diverse set of frequent object-based prompts to better integrate the semantic cues for symmetry detection. Empirically, we show that CLIPSym outperforms the current state-of-the-art on three standard symmetry detection datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations verifying the benefits of CLIP's pre-training, the proposed equivariant decoder, and the SAPG technique. The code is available at https://github.com/timyoung2333/CLIPSym.",
            "score": 1,
            "issue_id": 5642,
            "pub_date": "2025-08-19",
            "pub_date_card": {
                "ru": "19 августа",
                "en": "August 19",
                "zh": "8月19日"
            },
            "hash": "fe61d1db34c28a8d",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#optimization",
                    "#multimodal",
                    "#cv",
                    "#games"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "CLIPSym: Улучшенное обнаружение симметрии с помощью языковой модели",
                    "desc": "CLIPSym - это новая модель для обнаружения симметрии, использующая предобученную модель CLIP. Она сочетает энкодеры изображений и текста CLIP с ротационно-эквивариантным декодером на основе трансформера и G-свертки. Модель использует технику семантически-осведомленной группировки промптов для лучшего учета семантических подсказок при обнаружении симметрии. CLIPSym превосходит современные методы на стандартных наборах данных для обнаружения симметрии."
                },
                "en": {
                    "title": "Enhancing Symmetry Detection with CLIPSym",
                    "desc": "CLIPSym is a vision-language model that improves the detection of symmetry in images by using a rotation-equivariant decoder and a new prompting technique. It builds on the capabilities of the CLIP model, which combines image and text understanding, to enhance symmetry detection by incorporating semantic information from image descriptions. The model employs a hybrid architecture that integrates Transformer and G-Convolution to effectively recognize both rotation and reflection symmetries. Experimental results demonstrate that CLIPSym surpasses existing methods on multiple standard datasets, confirming the effectiveness of its innovative approaches."
                },
                "zh": {
                    "title": "CLIPSym：提升对称性检测的新方法",
                    "desc": "CLIPSym是一种基于CLIP的视觉-语言模型，旨在提高对称性检测的能力。它采用了一种旋转等变解码器和语义感知提示技术，能够更好地利用自然图像描述中的对称性线索。通过结合Transformer和G-卷积的混合结构，CLIPSym能够有效检测旋转和反射对称性。实验结果表明，CLIPSym在三个标准对称性检测数据集上超越了现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.21172",
            "title": "Deep Residual Echo State Networks: exploring residual orthogonal\n  connections in untrained Recurrent Neural Networks",
            "url": "https://huggingface.co/papers/2508.21172",
            "abstract": "Deep Residual Echo State Networks (DeepResESNs) enhance long-term temporal modeling and memory capacity through hierarchical untrained residual layers, outperforming traditional shallow and deep reservoir computing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Echo State Networks (ESNs) are a particular type of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular for their fast and efficient learning. However, traditional ESNs often struggle with long-term information processing. In this paper, we introduce a novel class of deep untrained RNNs based on temporal residual connections, called Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a hierarchy of untrained residual recurrent layers significantly boosts memory capacity and long-term temporal modeling. For the temporal residual connections, we consider different orthogonal configurations, including randomly generated and fixed-structure configurations, and we study their effect on network dynamics. A thorough mathematical analysis outlines necessary and sufficient conditions to ensure stable dynamics within DeepResESN. Our experiments on a variety of time series tasks showcase the advantages of the proposed approach over traditional shallow and deep RC.",
            "score": 0,
            "issue_id": 5650,
            "pub_date": "2025-08-28",
            "pub_date_card": {
                "ru": "28 августа",
                "en": "August 28",
                "zh": "8月28日"
            },
            "hash": "a0a4fea9c9a49fe6",
            "authors": [
                "Matteo Pinna",
                "Andrea Ceni",
                "Claudio Gallicchio"
            ],
            "affiliations": [
                "Department of Computer Science, University of Pisa, 56127 Pisa, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.21172.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#math",
                    "#optimization",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Глубокие остаточные сети для улучшенной обработки временных рядов",
                    "desc": "В этой статье представлены Глубокие Остаточные Эхо-Государственные Сети (DeepResESNs), новый класс глубоких необученных рекуррентных нейронных сетей. Они используют иерархию необученных остаточных рекуррентных слоев для улучшения долгосрочного временного моделирования и емкости памяти. Авторы рассматривают различные ортогональные конфигурации для временных остаточных соединений и проводят математический анализ условий стабильной динамики сети. Эксперименты показывают преимущества предложенного подхода над традиционными поверхностными и глубокими методами резервуарных вычислений в различных задачах временных рядов."
                },
                "en": {
                    "title": "Boosting Memory with Deep Residual Echo State Networks",
                    "desc": "Deep Residual Echo State Networks (DeepResESNs) are a new type of untrained Recurrent Neural Network designed to improve the handling of long-term temporal data. They use a structure of hierarchical residual layers that do not require training, which enhances their memory capacity. This paper explores how different configurations of these residual connections can affect the network's performance and stability. The results demonstrate that DeepResESNs outperform traditional reservoir computing methods in various time series tasks."
                },
                "zh": {
                    "title": "深残差网络，提升记忆与建模能力",
                    "desc": "深残差回声状态网络（DeepResESNs）通过层次化的未训练残差层增强了长期时间建模和记忆能力，超越了传统的浅层和深层水库计算方法。回声状态网络（ESNs）是一种特殊类型的未训练递归神经网络（RNN），在水库计算框架中因其快速高效的学习而受到欢迎。然而，传统的ESNs在处理长期信息时常常面临挑战。本文提出了一种基于时间残差连接的新型深度未训练RNN，展示了利用未训练的残差递归层的层次结构显著提升了记忆容量和长期时间建模能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.19600",
            "title": "Quantization Robustness to Input Degradations for Object Detection",
            "url": "https://huggingface.co/papers/2508.19600",
            "abstract": "Post-training quantization of YOLO models is evaluated for robustness to real-world degradations, with a focus on the effectiveness of a degradation-aware calibration strategy for Static INT8 quantization.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training quantization (PTQ) is crucial for deploying efficient object detection models, like YOLO, on resource-constrained devices. However, the impact of reduced precision on model robustness to real-world input degradations such as noise, blur, and compression artifacts is a significant concern. This paper presents a comprehensive empirical study evaluating the robustness of YOLO models (nano to extra-large scales) across multiple precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8 (TensorRT). We introduce and evaluate a degradation-aware calibration strategy for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix of clean and synthetically degraded images. Models were benchmarked on the COCO dataset under seven distinct degradation conditions (including various types and levels of noise, blur, low contrast, and JPEG compression) and a mixed-degradation scenario. Results indicate that while Static INT8 TensorRT engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop (~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did not yield consistent, broad improvements in robustness over standard clean-data calibration across most models and degradations. A notable exception was observed for larger model scales under specific noise conditions, suggesting model capacity may influence the efficacy of this calibration approach. These findings highlight the challenges in enhancing PTQ robustness and provide insights for deploying quantized detectors in uncontrolled environments. All code and evaluation tables are available at https://github.com/AllanK24/QRID.",
            "score": 0,
            "issue_id": 5649,
            "pub_date": "2025-08-27",
            "pub_date_card": {
                "ru": "27 августа",
                "en": "August 27",
                "zh": "8月27日"
            },
            "hash": "ab400b3d8dc110c8",
            "authors": [
                "Toghrul Karimov",
                "Hassan Imani",
                "Allan Kazakov"
            ],
            "affiliations": [
                "Bahcesehir University, Baku, Azerbaijan",
                "Bahcesehir University, Istanbul, Turkey"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.19600.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#security",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Квантизация YOLO: баланс между эффективностью и устойчивостью",
                    "desc": "Статья исследует влияние пост-тренировочной квантизации моделей YOLO на их устойчивость к искажениям изображений в реальном мире. Авторы оценивают эффективность стратегии калибровки с учетом искажений для статической INT8 квантизации. Эксперименты проводились на наборе данных COCO с различными типами искажений, включая шум, размытие и JPEG-сжатие. Результаты показывают, что предложенная стратегия калибровки не дает последовательных улучшений устойчивости по сравнению со стандартной калибровкой на чистых данных."
                },
                "en": {
                    "title": "Enhancing YOLO Robustness with Degradation-Aware Calibration",
                    "desc": "This paper investigates the post-training quantization (PTQ) of YOLO object detection models to assess their robustness against real-world image degradations. It specifically focuses on a degradation-aware calibration strategy for Static INT8 quantization, which aims to improve model performance when faced with various input distortions like noise and blur. The study evaluates different precision formats and benchmarks the models on the COCO dataset under multiple degradation scenarios. Results show that while Static INT8 quantization improves processing speed, the proposed calibration method does not consistently enhance robustness, particularly for smaller models, although larger models may benefit under certain conditions."
                },
                "zh": {
                    "title": "提升YOLO模型鲁棒性的量化策略",
                    "desc": "本文研究了YOLO模型的后训练量化（PTQ）在真实世界退化下的鲁棒性，特别关注静态INT8量化的退化感知校准策略的有效性。研究表明，虽然静态INT8 TensorRT引擎在干净数据上提供了显著的速度提升，但在大多数模型和退化条件下，退化感知校准并未带来一致的鲁棒性改善。对于特定噪声条件下的大型模型，校准效果有所不同，表明模型容量可能影响校准方法的有效性。该研究为在不受控环境中部署量化检测器提供了重要见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.17008",
            "title": "EduRABSA: An Education Review Dataset for Aspect-based Sentiment\n  Analysis Tasks",
            "url": "https://huggingface.co/papers/2508.17008",
            "abstract": "EduRABSA is a public dataset and ASQE-DPT is a tool for aspect-based sentiment analysis in education reviews, addressing the lack of resources in this domain.  \t\t\t\t\tAI-generated summary \t\t\t\t Every year, most educational institutions seek and receive an enormous volume of text feedback from students on courses, teaching, and overall experience. Yet, turning this raw feedback into useful insights is far from straightforward. It has been a long-standing challenge to adopt automatic opinion mining solutions for such education review text data due to the content complexity and low-granularity reporting requirements. Aspect-based Sentiment Analysis (ABSA) offers a promising solution with its rich, sub-sentence-level opinion mining capabilities. However, existing ABSA research and resources are very heavily focused on the commercial domain. In education, they are scarce and hard to develop due to limited public datasets and strict data protection. A high-quality, annotated dataset is urgently needed to advance research in this under-resourced area. In this work, we present EduRABSA (Education Review ABSA), the first public, annotated ABSA education review dataset that covers three review subject types (course, teaching staff, university) in the English language and all main ABSA tasks, including the under-explored implicit aspect and implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool), an offline, lightweight, installation-free manual data annotation tool that generates labelled datasets for comprehensive ABSA tasks from a single-task annotation. Together, these resources contribute to the ABSA community and education domain by removing the dataset barrier, supporting research transparency and reproducibility, and enabling the creation and sharing of further resources. The dataset, annotation tool, and scripts and statistics for dataset processing and sampling are available at https://github.com/yhua219/edurabsa_dataset_and_annotation_tool.",
            "score": 0,
            "issue_id": 5649,
            "pub_date": "2025-08-23",
            "pub_date_card": {
                "ru": "23 августа",
                "en": "August 23",
                "zh": "8月23日"
            },
            "hash": "22b921b4352ed731",
            "authors": [
                "Yan Cathy Hua",
                "Paul Denny",
                "Jörg Wicker",
                "Katerina Taskova"
            ],
            "affiliations": [
                "School of Computer Science, University of Auckland, New Zealand"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.17008.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#low_resource",
                    "#data"
                ],
                "emoji": "🎓",
                "ru": {
                    "title": "Новые инструменты для анализа мнений в образовательных отзывах",
                    "desc": "EduRABSA - это публичный датасет для анализа тональности по аспектам в образовательных отзывах. ASQE-DPT - инструмент для разметки данных, генерирующий размеченные наборы для задач ABSA из однозадачной аннотации. Эти ресурсы восполняют пробел в данных для ABSA в образовательной сфере. Датасет охватывает отзывы о курсах, преподавателях и университетах на английском языке."
                },
                "en": {
                    "title": "Empowering Education Insights with EduRABSA and ASQE-DPT",
                    "desc": "EduRABSA is a newly introduced public dataset specifically designed for aspect-based sentiment analysis (ABSA) in educational reviews, addressing the scarcity of resources in this area. The dataset includes annotated reviews covering various subjects such as courses, teaching staff, and universities, facilitating detailed opinion mining. Additionally, the ASQE-DPT tool allows for efficient manual data annotation, enabling researchers to create labeled datasets for comprehensive ABSA tasks. This work aims to enhance research in education by providing essential resources that promote transparency and reproducibility in sentiment analysis."
                },
                "zh": {
                    "title": "推动教育评论情感分析的资源创新",
                    "desc": "EduRABSA是一个公共数据集，专注于教育评论的基于方面的情感分析（ABSA），解决了该领域资源匮乏的问题。该数据集涵盖了课程、教学人员和大学三种评论主题，并支持多种ABSA任务，包括隐含方面和隐含意见的提取。ASQE-DPT是一个轻量级的手动数据注释工具，可以从单一任务注释生成标记数据集，促进了教育领域的研究透明性和可重复性。通过这些资源，我们希望推动教育评论的情感分析研究，填补现有的研究空白。"
                }
            }
        }
    ],
    "link_prev": "2025-08-29.html",
    "link_next": "2025-09-02.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "29.08",
        "en": "08/29",
        "zh": "8月29日"
    },
    "short_date_next": {
        "ru": "02.09",
        "en": "09/02",
        "zh": "9月2日"
    },
    "categories": {
        "#dataset": 12,
        "#data": 6,
        "#benchmark": 7,
        "#agents": 6,
        "#cv": 2,
        "#rl": 5,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 1,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 9,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 8,
        "#robotics": 1,
        "#agi": 3,
        "#games": 3,
        "#interpretability": 2,
        "#reasoning": 7,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 3,
        "#security": 2,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 1
    }
}